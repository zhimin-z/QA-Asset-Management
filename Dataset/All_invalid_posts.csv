Question_title,Question_body,Question_answer_count,Question_comment_count,Question_creation_date,Question_favorite_count,Question_last_edit_date,Question_score,Question_tags,Question_view_count,Owner_creation_date,Owner_last_access_date,Owner_location,Owner_reputation,Owner_up_votes,Owner_down_votes,Owner_views,Answer_body,Answer_comment_count,Answer_creation_date,Answer_last_edit_date,Answer_score,Question_valid_tags
Why do you use Git over Mercurial? (Or vice versa.),"<p>I currently use Git, and am generally happy with it, but I'd like to know more about Mercurial.  Does it have any advantages over Git?  What are the advantages of Git over Mercurial?</p>

<p>I realize there are already detailed comparisons of the two, but that's not what I'm asking for.  I don't want dispassionate information, but rather impassioned (but polite!!!) reasons why you think one is better/easier/faster/smarter/stronger/etc.  </p>",11,3,2010-11-28 22:13:13.270000 UTC,3.0,,27,git|mercurial|dvcs,5887,2009-12-30 20:07:02.193000 UTC,2022-03-05 23:31:04.920000 UTC,"Grand Rapids, MI, United States",19010,1528,131,1945,"<p>(A) Top 3 <em>real</em> reasons I use Git rather than Mercurial by choice:</p>

<ol>
<li>historical stochasticity / inertia</li>
<li>historical stochasticity / inertia</li>
<li>historical stochasticity / inertia</li>
</ol>

<p>(B) Given the above, the top 3 things I like about Git over Mercurial:</p>

<ol>
<li>the index</li>
<li>lightweight branches</li>
<li>sane tagging (a tag is not a commit that follows the commit that is being tagged)</li>
</ol>

<p>(C) The three things Mercurial has for it over Git:</p>

<ol>
<li>better <em>native</em> documentation (emphasis on <em>native</em>, because the <a href=""http://git-scm.com/book"">ProGit book</a> is simply outstanding, and anyone who has any complaints about Git or the Git model being too complex or confusing should go directly there)</li>
<li>sequential commit numbering (local repository scope only, but I imagine this might sometimes be more convenient than cutting-and-pasting SHA-1's when fiddling about?)</li>
<li>nicer logo/name?</li>
</ol>

<p>I think that all (most?) of the items in (B) are either a result of me being used to the Git model due to (A) or can be ""fixed"" in Mercurial through plug-ins. But the fact remains that Git works the way I want it to out of the box, and I can live without any of the (C) items [given that C(1) is rendered a non-issue due to the excellent ProGit book]. And, so, I continue to use Git over Mercurial.</p>",4.0,2010-11-29 17:05:36.920000 UTC,2012-05-12 11:00:17.347000 UTC,15.0,[]
Is using a central repository going against GIT's purpose?,"<p>If you are in a <em>corporate</em> setting with many people working on a particular application, is it going against the grain of a <a href=""http://en.wikipedia.org/wiki/Distributed_revision_control"" rel=""noreferrer"">distributed version control</a> system to have an official central repository?</p>

<p>Sometimes I struggle to understand the concept of a distributed version control system such as <a href=""http://git-scm.com/"" rel=""noreferrer"">GIT</a> in a corporate environment.  If you didn't have a central repository, wouldn't it be a PITA to figure out who had the latest updated version to pull from, who has feature x or bug fix y that everyone needs to grab, etc, etc.</p>

<p>Is it defeating the purpose of <a href=""http://git-scm.com/"" rel=""noreferrer"">GIT</a> to use it in a similar fashion to <a href=""http://subversion.tigris.org/"" rel=""noreferrer"">SVN</a>, with a central repository that everyone pushes/pulls from?  Every time I think about doing that I feel like I'm missing the point of everything.</p>

<p>Could someone enlighten me?</p>",7,0,2009-01-20 08:49:22.967000 UTC,9.0,2009-01-20 18:41:25.783000 UTC,33,svn|git|dvcs,5400,2008-08-23 19:46:51.740000 UTC,2021-06-21 14:19:48.533000 UTC,United States,88329,1748,576,7706,"<p>Not really. DCVS just allows more freedom in how to interact between developers without involving the central repository. The official repository is only official <strong>by consensus</strong>. Linux also has a central repository, the one from which the ""official"" kernel-releases are created, but there is no physical difference between the central, ""official"", repository and client repositories as it is in centralized VCS.</p>",1.0,2009-01-20 08:55:16.310000 UTC,,29.0,[]
Moving from Visual Sourcesafe to Mercurial,<p>What's the best way to move a Visual Sourcesafe repository to Mercurial (I'm interested in retaining all history)?</p>,6,0,2009-06-07 13:22:01.850000 UTC,11.0,2012-03-30 12:24:27.683000 UTC,19,mercurial|migration|dvcs|visual-sourcesafe,5105,2009-06-05 14:56:54.277000 UTC,2018-10-29 19:11:13.397000 UTC,"Denver, CO",5351,341,2,414,"<p>While I haven't made that particular conversion, I have gone from VSS to SVN using (IIRC) <a href=""http://www.pumacode.org/projects/vss2svn"" rel=""noreferrer"">this script</a>.  You'll probably want to look into tailor and do a search for vss2hg.  Also keep in mind that it may make sense to go through an intermediate step like vss2svn + svn2hg or similar.</p>

<p>The primary bit of advice I'd give though is: script the conversion so you can re-run it easily.  That will let you run nightly conversions from VSS to Hg and make sure that everything is converting correctly before you pull the trigger on it.</p>",0.0,2009-06-07 13:53:26.763000 UTC,,14.0,[]
Is a DVCS useful for one developer?,"<p>Or would a conventional client-server VCS be more appropriate? I'm currently using TortoiseSVN, but I'm interested in a DVCS, but I'm not sure if it's even a good idea to try to use something like that solo.</p>",6,0,2008-10-07 15:53:48.963000 UTC,5.0,2011-12-09 19:03:35.160000 UTC,16,version-control|dvcs,1016,2008-08-06 20:56:54.547000 UTC,2022-03-03 14:53:21.437000 UTC,"Cambridge, MA, United States",110624,2947,204,15263,"<p>Since you can still push to another machine also running Git/Mercurial/Bzr/etc you still have the multi-computer backup safety, which you'd hopefully have either way.  However if you ever code while traveling, having full repository access can be a huge plus, then just resync to your server when you have a net connection again/get home/etc.</p>",1.0,2008-10-07 15:56:30.487000 UTC,,15.0,[]
Can Mercurial repositories be nested?,"<p>What happens if there is already a Mercurial repository at</p>

<pre><code>/User/peter/development
</code></pre>

<p>and now I want to add a repository for</p>

<pre><code>/User/peter
</code></pre>

<p>because I also want to version <code>.bashrc</code>, <code>.profile</code>, or maybe <code>/User/peter/notes</code> as well.  Will having a repository above an already existing repository create conflicts for Mercurial?</p>",2,0,2010-08-30 21:12:26.233000 UTC,2.0,2010-08-31 20:14:09.243000 UTC,11,mercurial|dvcs,1129,2009-05-09 15:50:29.477000 UTC,2022-03-04 09:41:10.460000 UTC,,137341,1445,39,12817,"<p>Everything will be okay.</p>

<p>It seems that Mercurial is smart enough to ignore subdirectories which already have repositories in them. Here's a conversation with it:</p>

<pre><code>$ mkdir outer
$ mkdir outer/inner
$ mkdir outer/sub
$ echo red &gt;outer/red.txt
$ echo blue &gt;outer/inner/blue.txt
$ echo green &gt;outer/sub/green.txt
$ cd outer/inner/
$ hg init
$ hg add
adding blue.txt
$ hg commit -m ""create inner""
$ cd ..
$ hg init
$ hg add
adding red.txt
adding sub/green.txt
$ hg commit -m ""create outer""
$ hg status
A red.txt
A sub/green.txt
$ hg commit -m ""create outer""
</code></pre>

<p>As you can see, when i add to the outer repository, it ignores the inner directory.</p>

<p>If you wanted to be extra sure, you could add the inner directory to your <code>.hgignore</code>.</p>",3.0,2010-08-30 21:44:19.780000 UTC,,10.0,[]
"Can GIT, Mercurial, SVN, or other version control tools work well when project tree has binary files?","<p>Sometimes our project tree can have binary files, such as jpg, png, doc, xls, or pdf.  Can GIT, Mercurial, SVN, or other tools do a good job when only part of a binary file is changed?</p>
<p>For example, if the spec is written in .doc and it is part of the repository, then if it is 4MB, and edited 100 times but just for 1 or 2 lines, and checked in 100 times during the year, then it is 400MB.</p>
<p>If it is 100 different .doc and .xls files, then it is 40GB... not a size that is easy to manage.</p>
<p>I have tried GIT and Mercurial and see that they both seem to add a big size of data even when 1 line is changed in a .doc or .pdf.  Is there other way inside of GIT or Mercurial or SVN that can do the job?</p>
<p>P.S. I tried Dropbox and I could have a 7MB file, and then I highlight a couple of places in the .PDF file, and Dropbox seemed to be able to upload the change in 1 second. My uplink is only about 200kb/s, so I think Dropbox did a pretty good job diff'ing my file. So we can use Dropbox, except there is no version control this way.</p>",5,0,2010-06-06 08:57:51.260000 UTC,1.0,2021-01-30 06:54:10.690000 UTC,10,svn|git|mercurial|dvcs|binaryfiles,6191,2009-05-09 15:50:29.477000 UTC,2022-03-04 09:41:10.460000 UTC,,137341,1445,39,12817,"<p>In general, version control systems work better with text files.  The whole merge/conflict concept is really based around source code.  However, SVN works pretty well for binary files. (We use it to version CAD drawings.)</p>

<p>I will point out that the file locking (svn:needs-lock) are pretty much a must-have when there are multiple people working on a common binary file.  Without file locking, it is possible for 2 people to work on a binary file at once.  Someone commits their changes first.  Guess what happens to the person that didn't commit.  All of that binary/unmergable work they did is effectively lost.  File-locking serializes work on the file.  You do lose the ""concurrent"" access capabilities of a version control system, but you still have the benefits of a commit log, rolling back to a previous version, etc.</p>

<p>The TortoieSVN client is smart enough to use MS Word's built in merge tool to diff a doc/docx file.  It also has configuration options to let you specify alternate diff tools based on file extension, which is pretty cool.  (It's a shame no one has made a diff tool for our CAD package).</p>

<p>Current-generation DVCSes like Git or Hg tend to suck with binary files though.  They don't have any sort of mechanism for file locking.</p>",0.0,2010-06-06 18:05:24.673000 UTC,2010-06-07 13:11:30.173000 UTC,13.0,[]
commit-pull-merge-push or pull-merge-commit-push?,"<p>We started using Mercurial a several weeks ago. Most developers follow this workflow:</p>

<ul>
<li>work on a feature</li>
<li>commit -m ""Worked on feature ABC""</li>
<li>pull -u</li>
<li>If branch

<ul>
<li>merge</li>
<li>commit -m ""Merge""</li>
<li>push</li>
</ul></li>
</ul>

<p>Today, one of our developer suggested that we do:</p>

<ul>
<li>work on a feature</li>
<li>pull -u</li>
<li>if branch

<ul>
<li>merge</li>
</ul></li>
<li>commit -m ""Worked on feature ABC""</li>
<li>push</li>
</ul>

<p>That way, we have a lot less ""Merge"" changesets in the log.</p>

<p>Some of us think it's just a matter preference. Some of us think one is better than the other. We don't have much experience and don't want to live the downsides of misusing the tool. So if one approach is more advisable then the other, please let me know why.</p>",3,1,2010-07-13 20:04:50.793000 UTC,13.0,2010-07-13 20:17:33.837000 UTC,25,mercurial|dvcs,14599,2009-05-06 15:46:26.067000 UTC,2022-02-17 21:52:55.770000 UTC,Canada,18831,342,15,1235,"<p>I like your original procedure more, but reasonable people can certainly disagree.  I consider merging an actual piece of software development work and like having it be a first class citizen in our process.</p>

<p>In your second/suggested procedure the risk is that the pull does some stuff you really don't want and then you have a very hard time separating it from the work you've already done.</p>

<p>For people who just can't stand branchy history the usual preferred workflow is:</p>

<ul>
<li>work on a feature</li>
<li>commit</li>
<li>pull --rebase</li>
<li>push</li>
</ul>

<p>where the <code>--rebase</code> option appears on pull after you enable the <a href=""https://www.mercurial-scm.org/wiki/RebaseExtension"" rel=""nofollow noreferrer"">rebase extension</a>.  I'm not a fan of rebase because it's technically rewriting history which is antithetical to how mercurial is supposed to work, but I'm in a rapidly shrinking minority on that point.</p>

<p>Bottom line, if you really don't want a branchy history use rebase -- don't update into uncommitted changes as it's hard to undo.</p>",1.0,2010-07-13 20:21:48.463000 UTC,2018-02-22 10:48:16.757000 UTC,21.0,[]
Plotting arbitrary data for repository,"<p>I'm looking for a way to visualize arbitrary information about my repository over time, which might be some version-dependent number, such as:</p>

<ul>
<li>lines of code</li>
<li>number of lines in a latex document</li>
<li>time between commits</li>
<li>anything that can be output by a script</li>
</ul>

<p>What is the best way to visualize this information?</p>

<p>More specifically, I'm using mercurial and would ideally like something with a decent interface, with plot resizing/scrolling/etc...  Jenkins' plot plugin is decent but not great, but more importantly it's not possible to visualize <strong>past</strong> data (say, after adding a new metric).</p>",1,1,2012-09-19 17:31:52.933000 UTC,1.0,2012-09-19 18:20:26.353000 UTC,7,version-control|jenkins|dvcs,343,2010-01-22 14:34:29.733000 UTC,2022-02-09 01:51:55.040000 UTC,"Lausanne, Switzerland",1804,483,1,115,"<p>I would suggest to split your task to simplify everything a little bit. It is likely you will need several different tools in order to collect and visualize all required information. Historical view seems to be another big challenge.</p>

<p><strong>Lines of code</strong></p>

<p>There are several plugins available for Jenkins, but almost all are highly specialized. <a href=""https://wiki.jenkins-ci.org/display/JENKINS/SLOCCount+Plugin"" rel=""noreferrer"">SLOCCount plug-in</a> seems to be most universal, but it does not provide any graphical output.</p>

<p><img src=""https://i.stack.imgur.com/eQwF5.png"" alt=""enter image description here""></p>

<ul>
<li><a href=""https://wiki.jenkins-ci.org/display/JENKINS/NSIQ+Collector+Plugin"" rel=""noreferrer"">NSIQ Collector Plugin</a></li>
<li><a href=""https://wiki.jenkins-ci.org/display/JENKINS/SLOCCount+Plugin"" rel=""noreferrer"">SLOCCount plug-in</a></li>
<li><a href=""https://wiki.jenkins-ci.org/display/JENKINS/JavaNCSS+Plugin"" rel=""noreferrer"">JavaNCSS Plugin</a></li>
</ul>

<p>There might be some other option for your language. For example, <a href=""https://wiki.jenkins-ci.org/display/JENKINS/CCCC+Plugin"" rel=""noreferrer"">CCCC</a> will provide required information for C and C++ code:</p>

<p><img src=""https://i.stack.imgur.com/f2GV5.png"" alt=""enter image description here""></p>

<p><strong>Number of lines in a latex document</strong>
I see several options to achieve that:</p>

<ul>
<li>adapt existing solution/plugin</li>
<li>use repository statistics tool (<a href=""http://scm-pepper.sourceforge.net/"" rel=""noreferrer"">Pepper</a>, for example, can do the trick)</li>
<li>use simple shell script to count lines and report it</li>
</ul>

<p><a href=""http://scm-pepper.sourceforge.net/"" rel=""noreferrer"">Pepper</a> will generate something like the following:</p>

<p><img src=""https://i.stack.imgur.com/6sMCC.png"" alt=""enter image description here""></p>

<p>Please check Pepper <a href=""http://scm-pepper.sourceforge.net/gallery/"" rel=""noreferrer"">gallery</a>. There are another tools, for example: <a href=""https://bitbucket.org/mg/hgchart/wiki/Home"" rel=""noreferrer"">hgchart</a></p>

<p><strong>Time between commits</strong></p>

<p>The simplest solution is to let a commit to trigger some trivial job, so Jenkins will provide all information as part of build history (with a timeline, etc).</p>

<p><img src=""https://i.stack.imgur.com/Q5WMF.png"" alt=""enter image description here""></p>

<p>Another solution is to use repository statistics tool once again:</p>

<p><img src=""https://i.stack.imgur.com/foNte.png"" alt=""enter image description here""></p>

<p><strong>Anything that can be output by a script</strong></p>

<p>There are several good plug-ins for that.</p>

<ul>
<li><a href=""https://wiki.jenkins-ci.org/display/JENKINS/Plot+Plugin"" rel=""noreferrer"">Plot plugin</a> can visualize multiple values provided as properties or csv file.</li>
<li><a href=""https://wiki.jenkins-ci.org/display/JENKINS/Measurement+Plots+Plugin"" rel=""noreferrer"">Measurement Plots Plugin</a> scans the output in order to find values to be visualized</li>
</ul>

<p><img src=""https://i.stack.imgur.com/rSqdd.png"" alt=""enter image description here""></p>

<p>Happy continuous integration.</p>",1.0,2013-01-09 15:04:27.247000 UTC,,9.0,[]
How do you remove big files from history in mercurial?,"<p>I have just created a mercurial repo created from a heterogeneous ecosystems of other repos. Before I publish it to my co-workers, I want to clean it as much as possible. To this end, I'd like to entirely remove a few big old files from history (pretend they never existed), so repo will be smaller.</p>

<p>Is this possible with mercurial?</p>",1,2,2012-04-11 09:26:29.073000 UTC,11.0,,34,version-control|mercurial|dvcs,4522,2009-08-27 12:21:50.197000 UTC,2022-03-04 16:32:49.647000 UTC,,49595,2869,185,1996,"<p>Check out the <code>convert</code> extension, particularly the <code>--filemap</code> option.</p>

<p>Enable by adding the following to <code>mercurial.ini</code>:</p>

<pre><code>[extensions]
convert =
</code></pre>

<p>Create a map of files to exclude:</p>

<pre><code>exclude path/to/file1
exclude path/to/file2
</code></pre>

<p>Then convert the repo:</p>

<pre><code>hg convert srcrepo destrepo --filemap &lt;map&gt;
</code></pre>

<p>Note there is a bug in Mercurial 2.1.1 causing an error with the above command:</p>

<pre><code>initializing destination destrepo repository
abort: invalid mode ('r') or filename
</code></pre>

<p>Just add the <code>--splicemap &lt;nonexistent file&gt;</code> option to fix the problem.</p>",10.0,2012-04-11 12:58:54.283000 UTC,2012-04-11 13:05:07.383000 UTC,35.0,[]
"In Mercurial (hg), how do you see a list of files that will be pushed if an ""hg push"" is issued?","<p>We can see all the changesets and the files involved using</p>

<pre><code>hg outgoing -v
</code></pre>

<p>but the filenames are all scattered in the list of changesets.</p>

<p>Is there a way to just see a list of all the files that will go out if <code>hg push</code> is issued?</p>",5,0,2010-06-14 23:20:02.517000 UTC,7.0,2012-03-30 12:09:09.923000 UTC,21,mercurial|push|dvcs,17967,2009-05-09 15:50:29.477000 UTC,2022-03-04 09:41:10.460000 UTC,,137341,1445,39,12817,"<p>First, create a file with this content:</p>

<pre><code>changeset = ""{files}""
file = ""{file}\n""
</code></pre>

<p>Let's say you call it out-style.txt and put it in your home directory.  Then you can give this command:</p>

<pre><code>hg -q outgoing --style ~/out-style.txt | sort -u
</code></pre>",5.0,2010-06-15 00:00:13.840000 UTC,,10.0,[]
Verify if a tag was done in the master branch,"<p>In this project I am working, we do deployments based on tags. Whilst it's obligatory that the tags are done against the master branch (after you merge the release there), sometimes by mistake someone can tag against a dev or release branch, which is incorrect. That causes several problems.</p>

<p>In our deployment script, there is a step in which we do clone a specific tag from git, using a process like the one described in this question: <a href=""https://stackoverflow.com/questions/791959/download-a-specific-tag-with-git"">Download a specific tag with Git</a></p>

<pre><code>$ git clone
$ git checkout tags/&lt;tag_name&gt;
</code></pre>

<p>How can I amend this script to check if this tag was actually done against the master branch? I would like to then stop the deployment and throw an error if the branch is not the master.</p>

<p>Thanks.</p>",4,3,2018-04-05 05:56:25.437000 UTC,1.0,,8,git|version-control|repository|dvcs|git-tag,5872,2013-01-22 21:31:18.980000 UTC,2018-04-16 01:01:49.397000 UTC,,1781,1,0,63,"<p>As several people noted, your question cannot really be answered until it is reformulated.  This is because a Git tag or branch name simply identifies <em>one specific commit.</em>  The desired effect of a branch name is to identify the <em>tip</em> commit of a branch, which changes over time, so that the specific commit it identifies also changes over time.  The desired effect of a tag name is to identify one specific commit forever, without changing.  So if someone tags <code>master</code>, there will be some moments in time during which parsing the name <code>master</code> produces commit hash <em>H</em>, and parsing the tag name <em>also</em> produces commit hash <em>H:</em></p>

<pre><code>if test $(git rev-parse master) = $(git rev-parse $tag^{commit}); then
    echo ""master and $tag both identify the same commit""
else
    echo ""master and $tag identify two different commits""
fi
</code></pre>

<p>This particular test is valid until someone makes the branch name <code>master</code> advance, after which it is no longer useful.  If we draw this the way I typically like to draw Git commit graphs on StackOverflow, we can see this as:</p>

<pre><code>          tag
           |
           v
...--o--o--H   &lt;-- master
          /
 ...--o--o   &lt;-- develop
</code></pre>

<p>Currently the names <code>tag</code> and <code>master</code> both identify commit H, which is a merge commit.  As soon as someone creates a <em>new</em> commit on <code>master</code>, though, the graph becomes:</p>

<pre><code>          tag
           |
           v
...--o--o--H--I   &lt;-- master
          /
 ...--o--o   &lt;-- develop
</code></pre>

<p>Now <code>master</code> identifies new commit <code>I</code>, so doing the <code>rev-parse tag^{commit}</code> will find <code>H</code> while doing the <code>rev-parse master</code> will find <code>I</code> and they won't be equal and the test will fail.</p>

<p>(I drew commit <code>I</code> as an ordinary commit here, but it could be a merge commit with a second parent.  If so, imagine a second backwards-pointing line / arrow emerging from <code>I</code>, pointing to some other earlier commit.)</p>

<p><a href=""https://stackoverflow.com/a/49666571/1256452"">Philippe's answer</a> comes in two forms and answers a slightly different question.  Since branch names <em>do</em> move over time, we can use <code>git branch --contains</code> to find <em>all</em> branch names that make the tagged commit reachable, and see if one of these is <code>master</code>.  This will give a true / yes answer for the case above.  Unfortunately, it will also tell you that the tag <code>error</code> is contained within <code>master</code>—which is true!—for <em>this</em> graph:</p>

<pre><code>          tag
           |
           v
...--o--o--H   &lt;-- master
          /
 ...--o--G   &lt;-- develop
         ^
         |
       error
</code></pre>

<p>This is because the tag <code>error</code> identifies commit <code>G</code>, and commit <code>G</code> is <em>reachable from</em> commit <code>H</code> (by following the second parent of <code>H</code>).  In fact any tag along the bottom row, pointing to any commit contained within the <code>develop</code> branch, identifies a commit contained within the <code>master</code> branch, since <em>every commit currently on <code>develop</code> is also on <code>master</code></em>.</p>

<p>(Incidentally, the difference between using <code>git rev-parse your-tag</code> and using <code>git rev-list -n 1 your-annotated-tag</code> is covered by using <code>git rev-parse $tag^{commit}</code>.  The issue here is that an annotated tag has an actual repository object, of type ""annotated tag"", to which the name points; using <code>git rev-parse your-annotated-tag</code> alone finds the <em>tag</em> object rather than its commit.  The <code>^{commit}</code> suffix syntax is described in <a href=""https://www.kernel.org/pub/software/scm/git/docs/gitrevisions.html"" rel=""noreferrer"">the gitrevisions documentation</a>.)</p>

<p>There <em>is</em> a way to tell whether the commit to which any given tag points is in the history of <code>master</code> that occurs only on the first-parent chain.  It is not the prettiest: <code>git branch --contains</code> and <code>git merge-base --is-ancestor</code> are the usual building blocks for finding reachability but both follow <em>all</em> parents.  To follow only <em>first</em> parents, we need to use <code>git rev-list --first-parent</code> to enumerate all the commits reachable from the name <code>master</code> when following only first parents.  Then we simply check whether the tagged revision is in that list.  Here's a bad way to do it that makes clear what we are doing:</p>

<pre><code>tagged_commit=$(git rev-parse $tag^{commit}) ||
    fatal ""tag $tag does not exist or does not point to a commit object""
found=false
for hash in $(git rev-list --first-parent master); do
    test $hash == $tagged_commit &amp;&amp; found=true
done
</code></pre>

<p>To make this much faster, it would be better to pipe the <code>git rev-list</code> output through a <code>grep</code> that searches for <code>$tagged_commit</code> (with grep's output discarded since we only care about the status):</p>

<pre><code>if git rev-list --first-parent master | grep $tagged_commit &gt;/dev/null; then
    echo ""$tag points to a commit reachable via first-parent from master""
else
    echo ""$tag does not point to a commit reachable via first-parent from master""
fi
</code></pre>

<p>for instance.  (One flaw here is that <code>git rev-list</code> will run all the way through every reachable commit; in a large repository, this can take <em>seconds</em>.)</p>",0.0,2018-04-05 15:04:25.657000 UTC,,10.0,[]
How to delete graph in amazon Neptune,"<p>How to delete Neptune graph or delete all vertex and edges from the graph. </p>

<p>Is there also way from the gremlin. Rather than iterating all the nodes and delete single vertex </p>",5,0,2018-06-28 09:23:52.620000 UTC,3.0,,5,gremlin|amazon-neptune,4544,2013-07-25 08:12:20.183000 UTC,2022-02-08 15:52:29.637000 UTC,"Pune, Maharashtra, India",279,1,0,57,"<p>Gremlin Code will be
g.V().limit(100000).drop().iterate();</p>

<p>Update: 
g.V().drop().iterate() is sufficient, as drop all queries have now been optimised in Neptune.</p>",5.0,2018-06-28 09:31:26.730000 UTC,2018-10-24 07:31:42.187000 UTC,8.0,[]
"How can I learn to understand how Mercurial works, as an experienced git user?","<p>I've been using git for a while now, and since it is the only DVCS I have ever used, I've learned to rely a lot on the way it works for my workflow.</p>

<p>I'm in a new company now, and they use Mercurial, so I need to understand Mercurial's model and how it differs from git, to adapt my workflow and avoid making costly mistakes.</p>

<p>So what resources can I use for this?</p>",1,4,2011-09-07 09:48:26.050000 UTC,4.0,,5,git|version-control|mercurial|dvcs,549,2009-08-27 12:21:50.197000 UTC,2022-03-04 16:32:49.647000 UTC,,49595,2869,185,1996,"<p>Quite extended <a href=""http://mercurial.selenic.com/wiki/GitConcepts"" rel=""nofollow noreferrer"">concept differences from Mercurial's official wiki</a>.</p>

<p>Another question from <em>stackoverflow</em>: <a href=""https://stackoverflow.com/questions/1450348/git-equivalents-of-most-common-mercurial-commands"">Git equivalents of most common Mercurial commands?</a></p>

<h3>Comment follow up:</h3>

<p>If I resume: ""<em>model</em>"", ""<em>differences</em>"", ""<em>philosophy behind the differences</em>"" and influences on the ""<em>workflow</em>"". In the differences I can think of, there is:</p>

<ul>
<li>For the workflow, except the lack of staging area, you should be able to stick on your old practices (when to commit, when to branch, when to merge,...).</li>
<li>The main difference of philosophy about branches is that Git clones only the specified branch when Mercurial clones all branches (if you want only one branch, it is possible, but it needs configuration).</li>
<li>The merge philosophy is not really different. Git can merge two or more heads, but Mercurial can only merge two heads (did you ever merge more?).</li>
<li>The philosophy about renames differs greatly and each designer did defend his point of view. As Mercurial tracks renames, merge can be easier. Git tries to guess renames at merge time if you use the strategy ""recursive"".</li>
<li><p>Storage differs greatly in implementation, but not in concepts (as you asked for ""model"", I will put more details):</p>

<ul>
<li><p>Git stores data as <a href=""http://book.git-scm.com/1_the_git_object_model.html"" rel=""nofollow noreferrer"">""objects""</a> (""commit object"", ""tree object"", ""blob object"" or ""tag object"", stored as file uniquely identified by there name which is a SHA1 hash). This is some kind of ""filesystem hash table"". With recent versions, those objects can be packed to have less small files under the <code>.git/objects</code> directory. I will not go further, my understanding stop here as I never found use to know how bits are laid. You can have a pretty printing in the object's content with:</p>

<pre><code>git show -s --format=raw &lt;commitid&gt; # changeset content
git ls-tree &lt;treeid&gt; # list tree content
git show &lt;fileid&gt; # blob content
</code></pre></li>
<li><p>Mercurial <a href=""http://hgbook.red-bean.com/read/behind-the-scenes.html#x8-640004"" rel=""nofollow noreferrer"">stores</a> history of each files individually as ""filelog"" in the <a href=""http://mercurial.selenic.com/wiki/Revlog"" rel=""nofollow noreferrer"">""revlog(NG)"" format</a>. You can manually inspect file names under <code>.hg/store/data</code> (revlogNG). Note that special and uppercase characters are ""tilda-underscore encoded"".</p>

<p>You can list revisions of a file with:</p>

<pre><code>hg debugindex .hg/store/data/&lt;file&gt;.i # hg debugindex &lt;file&gt; also works but you see less of internals
</code></pre>

<p>You have already noted that the <code>nodeid</code>s are not the one in <code>hg log</code>.</p>

<p>And now, inspect the contents with:</p>

<pre><code>hg debugdata .hg/store/data/&lt;file&gt;.i &lt;nodeid&gt;
</code></pre>

<p>The revision history (more or less what you see with <code>hg log</code>) is stored in <code>.hg/store/00changelog.i</code> (inspect it with <code>hg debugindex .hg/store/00changelog.i</code>, you will see the same IDs as the one in <code>hg log</code> in the <code>nodeid</code> column). To show one raw history entry with id <code>XXXX</code>, type <code>hg debugdata .hg/store/00changelog.i XXXX</code> in a terminal. (look at first line, it will be used later as <code>YYYY</code>)</p>

<p>The state of the tree is stored in <code>.hg/store/00manifest.i</code>. The corresponding <code>nodeid</code> in the manifest is <code>YYYY</code>.</p>

<pre><code>hg debugdata .hg/store/00manifest.i YYYY
</code></pre>

<p>This will show a list of ""filename+nodeid"" appended. Let's choose file <code>foo/bar</code> and note the <code>nodeid</code> appended to it and consider it is <code>ZZZZ</code> (line <code>foo/barZZZZ</code>).</p>

<p>Last step, access to the content of the <code>foo/bar</code> file:</p>

<pre><code>hg debugdata .hg/store/data/foo/bar.i ZZZZ
</code></pre></li>
<li><p>For the differences in philosophy clearly visible from this basic data storage analysis:</p>

<ul>
<li><p>When Git commits, it make (potentially a lot of) new files (which can be packed later). When Mercurial commits, it appends to existing files.</p></li>
<li><p>In Git a <code>blobid</code> can collide with a <code>treeid</code> (or <code>commitid</code> or <code>tagid</code>) but that is highly improbable. In Mercurial, a <code>changesetid</code> can collide only with another <code>changesetid</code> (ditto for manifests (tree) and files (blob)) which is even more improbable.</p></li>
</ul></li>
</ul></li>
<li>In Git, tags are special objects, in Mercurial, it's just a list in a file in the repository (with some <a href=""http://mercurial.selenic.com/wiki/Tag#How_do_tags_work_with_multiple_heads.3F"" rel=""nofollow noreferrer"">rules</a> to know which modified copy of the same tag wins).</li>
<li>In Mercurial, there is no ""amend"" or ""rebase"" by default and it is a design choice (philosophy?), always append, never remove content, as it can cause concurrency problem. But it is possible with extensions.</li>
</ul>",2.0,2011-09-07 10:38:42.570000 UTC,2017-05-23 12:19:03.503000 UTC,8.0,[]
Git remove directory,"<p>I've got a repository on GitHub (<a href=""http://github.com/hrickards/PHP-Crypto"" rel=""noreferrer"">http://github.com/hrickards/PHP-Crypto</a>) for a little project me and a couple of others are working on. My development environment is Aptana Studio, and I use the EGit plugin as Aptana is basically Eclipse underneath. Today the designer sent the HTML and CSS for the website with the images in a folder named img. Previously the images were in a folder called images. Thinking nothing of it and being too lazy to update the CSS and HTML, I simply kept the images in the img directory and commited to Git. However, the GitHub web interface shows both the img and images directories, with the images directory being empty. I've tried deleting the images directory with <code>git rm -r images</code> and <code>git rm images</code>, and even <code>mkdir images; git add images; git rm -r images</code> but whatever I try I get the same result:
<code>fatal: pathspec 'images' did not match any files</code>.</p>

<p>Has anyone got any advice on how to remove images, or am I misunderstanding Git or something?</p>",4,0,2009-12-22 16:22:15.273000 UTC,6.0,2010-03-29 02:56:25.833000 UTC,46,git|version-control|github|dvcs,61845,2009-12-07 17:35:41.523000 UTC,2016-04-15 17:13:23.553000 UTC,"England, United Kingdom",1031,60,1,67,"<p>Git does not store any information about the directory, just the files within it. So, you cannot add or remove directories themselves; if you say <code>git add images</code>, Git will add all of the files within that directory (that don't match the ignore list).</p>

<p>Generally, the only way for there to be an empty directory is if it actually contains a file, usually a hidden file like <code>.gitignore</code>.</p>

<p>You said that you see an <code>images</code> directory on GitHub, but I'm not seeing it in the repo that you linked to. Are you sure it's there, and not just an empty directory on your disk? If it's just an empty directory on your disk, you can just remove it using <code>rm images</code>; Git doesn't need to know about it.</p>",2.0,2009-12-22 17:05:17.123000 UTC,,41.0,[]
Git for Web Development (procedure method),"<p>I am wondering what is your procedure method of a web development using Git?</p>

<ol>
<li><p>When you finish coding, do you just overwrite the files on the FTP to the live server? </p></li>
<li><p>How does git handle number of version of same project? like v1, v1.5, etc</p></li>
<li><p>Let say 2 people working on the project locally at work (same office), how do you work together? Do I have to keep asking them to give me a source ready (save on USB?) for merge? </p></li>
<li><p>Can two people work on the same project on the same server? Wouldn't this be easier than question 3?</p></li>
</ol>",3,1,2011-05-03 22:13:57.920000 UTC,4.0,2011-05-03 23:23:29.443000 UTC,8,php|git|version-control|dvcs,670,2011-02-18 00:35:52.813000 UTC,2011-08-15 15:52:33.957000 UTC,,2278,129,8,235,"<p>The idea behind git is that it actually takes care of all that for you.</p>

<ol>
<li>When you write code you commit your code and you can push it out to the server. Git tracks the changes so its easy to rollback to a previous version.</li>
<li>It tracks the versions of files as they change so you can easily undo any changes that was made in the past, see <a href=""http://book.git-scm.com/3_git_tag.html"" rel=""nofollow"">tags</a> for more details.</li>
<li>NO. You can push your changes to the server and the other person can pull these changes. Some merging will have to occur but its quite easy with git. No need to transfer files from one dev to another. <a href=""http://book.git-scm.com/3_basic_branching_and_merging.html"" rel=""nofollow"">Branching and merging is discussed here</a>.</li>
<li>Yes. Thats the idea.</li>
</ol>

<p>To better understand the concepts behind a distributed version control system you can read <a href=""http://hginit.com/"" rel=""nofollow"">this tutorial by Joel Spolsky</a>. It is about Mercurial, but you will find the concepts very similar and this is probably the best tutorial written about this subject on the web.</p>",5.0,2011-05-03 22:19:30.110000 UTC,2011-05-03 22:52:09.343000 UTC,8.0,[]
is `hg forget` the same as `git rm --cached`?,<p>i'm having a hard time understanding <code>hg forget</code>. is it the same as <code>git rm --cached</code>? i.e. will remove the file with the next commit and stop tracking it?</p>,1,1,2010-11-09 15:27:19.513000 UTC,2.0,,9,git|mercurial|dvcs,2679,2009-05-27 07:53:46.773000 UTC,2022-03-05 19:20:55.647000 UTC,Austria,212209,3080,544,7572,"<p><code>hg forget</code> tells mercurial to stop tracking a file, but doesn't alter the file in your working directory.</p>

<p>After your commit it will behave as if you never did a <code>hg add</code> (though of course history will still exist).  New clones won't have that file in their working dir, but it won't be deleted in your working dir.</p>

<p>If you want to have a file in the workingdir/manifest, but want to ignore future changes, there's no easy way to do it (because it's generally considered a bad idea), though you can fake it by aliasing things to use <code>-I</code> on <code>hg commit</code>.</p>

<p>The better way to do that is to commit a sample of the file you want in the repo but whose changes you want ignored and then have your build system (or your instructions) copy that to its non-sample location.  For example have a file <code>config-file.sample</code> that is tracked in the repo, and then have setup/installation/build do a <code>cp config-file.sample config-file</code> if <code>config-file</code> doesnt' exist.  Include <code>config-file</code> in your <code>.hgignore</code> so it doesn't accidentally get added.  That gives you a tracked baseline but doesn't risk committing and pushing your local customizations.  This is very commonly done for things like database paths.</p>",5.0,2010-11-09 15:57:53.563000 UTC,2010-11-09 16:52:44.140000 UTC,9.0,[]
Is there a distributed VCS that can manage large files?,"<p>Is there a distributed version control system (git, bazaar, mercurial, darcs etc.) that can handle files larger than available RAM?</p>

<p>I need to be able to commit large binary files (i.e. datasets, source video/images, archives), but I don't need to be able to diff them, just be able to commit and then update when the file changes.</p>

<p>I last looked at this about a year ago, and none of the obvious candidates allowed this, since they're all designed to diff in memory for speed. That left me with a VCS for managing code and something else (""asset management"" software or just rsync and scripts) for large files, which is pretty ugly when the directory structures of the two overlap.</p>",7,0,2008-09-16 08:35:24.337000 UTC,5.0,2010-03-29 03:05:05.253000 UTC,14,version-control|dvcs|large-files,5283,2008-09-16 07:58:12.710000 UTC,2021-03-22 02:56:11.570000 UTC,"Washington, DC",10779,141,16,365,"<p>It's been 3 years since I asked this question, but, as of version 2.0 Mercurial includes the <a href=""https://www.mercurial-scm.org/wiki/LargefilesExtension"" rel=""nofollow noreferrer"">largefiles extension</a>, which accomplishes what I was originally looking for:</p>

<blockquote>
  <p>The largefiles extension allows for tracking large, incompressible binary files in Mercurial without requiring excessive bandwidth for clones and pulls. Files added as largefiles are not tracked directly by Mercurial; rather, their revisions are identified by a checksum, and Mercurial tracks these checksums. This way, when you clone a repository or pull in changesets, the large files in older revisions of the repository are not needed, and only the ones needed to update to the current version are downloaded. This saves both disk space and bandwidth.</p>
</blockquote>",0.0,2011-11-03 08:00:57.900000 UTC,2018-02-22 12:53:47.417000 UTC,12.0,[]
Git: merging public and private branches while while keeping certain files intact in both branches,"<p>I've read a few git questions here, but could not find an answer to this one:</p>

<p>I have a public and a private branches where I want to allow certain files to diverge.</p>

<p>Those are configuration files with passwords and my local customizations. </p>

<p>I do want to be able to merge the branches both ways: from private to public and back, but I do not want to ever have those specific files merged automatically.</p>

<p>Is there a way to set up git this way? I'd love to find an automated solution :) - so that merging could be done as usual.</p>

<hr>

<p><strong>EDIT:</strong> here's the solution that worked for me (Thanks to VonC for the advice on gitattribute)</p>

<p>the only unexpected thing for me was that ""merge protection"" starts working only after files have diverged in the two branches, not immediately after the following configuration was applied</p>

<p>.gitattributes (track with git if you want to share this) or .git/info/attributes:</p>

<pre><code>file1      merge=keepmine
path/file2     merge=keepmine
</code></pre>

<p>keepmine is the named custom merge manager that is just a do-nothing command called instead of the internal merge driver on selected files, which is set up below</p>

<p>When merging from private to public branch I usually do <code>git merge --squash private</code>. That way private edits won't get into the git history on the public branch.</p>

<p>.git/config:</p>

<pre><code>#public repository
[remote ""origin""]
    fetch = +refs/heads/*:refs/remotes/origin/*
    url = &lt;public repo git url&gt; 

#private repository
#has to set up with git init and populated with the initial commit to branch mybranch
[remote ""private""]
    push = +:
    url = /path/to/local/private/repo 
[merge ""keepmine""]
    name = dont_merge_selected_files
    driver = echo %O %A %B 
[branch ""master""]
    remote = origin
    merge = refs/heads/master 

#private branch settings
[branch ""mybranch""]
    remote = private
    merge = refs/heads/mybranch
</code></pre>

<p>if there's a way to improve this please comment</p>",4,0,2009-11-27 06:07:21.070000 UTC,11.0,2009-12-01 19:14:53.183000 UTC,24,git|merge|dvcs,3899,2009-05-21 01:30:44.783000 UTC,2022-03-05 19:47:50.243000 UTC,"Valparaiso, Chile",10426,1400,18,1763,"<p>To be on the safe side, you can add a <a href=""http://git-scm.com/docs/gitattributes"" rel=""nofollow noreferrer""><code>git attribute</code></a> (see <a href=""https://stackoverflow.com/questions/928646/how-do-i-tell-git-to-always-select-my-local-version-for-conflicted-merges-on-a-sp/930495#930495"">here for an example</a>) for those private files.</p>

<p>That way, you can define a script (a ""merge manager"") which will ensure the file including private informations will remain empty (or with a public content) if merged on the public branch, while keeping its local content if merged to the private branch.<br>
It means you can merge/rebase without thinking about that file.</p>",1.0,2009-11-27 08:10:06.530000 UTC,2017-05-23 12:16:55.457000 UTC,14.0,[]
"How can I view the output of `git show` in a diff viewer like meld, kdiff3, etc","<p>There are many SO questions that show how to view the output of a <code>git diff</code> command in a diff viewer like meld using <code>git difftool</code> or otherwise.  I am not asking about <code>git diff</code> though.</p>

<p>I want to see the output of <code>git show &lt;previous commit sha1&gt;</code> in a diff viewer like meld. How can I do this?</p>",6,1,2013-07-09 21:07:51.080000 UTC,4.0,2013-07-09 21:15:58.023000 UTC,55,git|version-control|diff|dvcs,13055,2011-01-06 18:21:21.230000 UTC,2022-02-09 22:42:03.430000 UTC,,8527,1290,10,645,"<p>You can use <code>git difftool</code> to show a single commit.</p>

<p>Say you want to see the commit with the sha1 <code>abc123</code>:</p>

<pre><code>git difftool abc123~1 abc123
</code></pre>

<p>(<code>~1</code> tells git to move to the previous commit, so <code>abc123~1</code> is the commit before <code>abc123</code>)</p>

<p>If you use this regularly, you could make a custom git command to make it easier:</p>

<ol>
<li><p>Create a file called <code>git-showtool</code> somewhere on your <code>$PATH</code> with the following contents:</p>

<pre><code>git difftool $1~1 $1
</code></pre></li>
<li><p>Give that file execute permissions:</p>

<pre><code>chmod +x ~/path/to/git-showtool
</code></pre></li>
<li><p>Use the command <code>git showtool &lt;sha1 or tag or ...&gt;</code></p></li>
<li>Profit.</li>
</ol>",5.0,2013-07-09 21:37:07.347000 UTC,,64.0,[]
Moving master head to a branch,"<p>I have several feature branches and a master branch. Feature2 is done. Normally I would rebase (working with a remote SVN repo and would like to keep the history, so no regular merge) and ff-merge. But since master hasnt changed since I branched, I would like to move the master head (at <code>E</code>) to <code>G</code>. Using <code>git branch -f master G</code> does not result in any visible changes, I assumed this is because <code>G</code> is on a different branch. </p>

<p>Is it safe to use <code>git update-ref -f master G</code> here instead? Should I stick with rebase/ff-merge? Something even better? </p>

<pre><code>feature1      C-D  
             /
master    A-B-E            
               \                      
feature2        F-G  
</code></pre>

<p>Thank you.</p>",5,5,2012-05-25 11:05:53.440000 UTC,2.0,,14,git|version-control|branch|dvcs|branching-and-merging,22090,2009-11-21 09:36:27.417000 UTC,2022-03-02 09:17:37.630000 UTC,"Halle, Germany",58111,2728,180,4096,"<p>A regular merge of G into master will do the trick, no need to rebase:</p>

<pre><code>    feature1      C-D  
                 /
    master    A-B-E            
                   \                      
    feature2        F-G

git checkout master
git merge feature2

    feature1              C-D  
                         /
    master, feature2  A-B-E-F-G
</code></pre>",0.0,2012-05-25 22:29:16.030000 UTC,,10.0,[]
How to get the schema definition from a dataframe in PySpark?,"<p>In PySpark it you can define a schema and read data sources with this pre-defined schema, e. g.:</p>

<pre class=""lang-py prettyprint-override""><code>Schema = StructType([ StructField(""temperature"", DoubleType(), True),
                      StructField(""temperature_unit"", StringType(), True),
                      StructField(""humidity"", DoubleType(), True),
                      StructField(""humidity_unit"", StringType(), True),
                      StructField(""pressure"", DoubleType(), True),
                      StructField(""pressure_unit"", StringType(), True)
                    ])
</code></pre>

<p>For some datasources it is possible to infer the schema from the data-source and get a dataframe with this schema definition.</p>

<p>Is it possible to get the schema definition (in the form described above) from a dataframe, where the data has been inferred before?</p>

<p><code>df.printSchema()</code> prints the schema as a tree, but I need to reuse the schema, having it defined as above,so I can read a data-source with this schema that has been inferred before from another data-source.</p>",4,0,2019-02-03 12:49:03.320000 UTC,6.0,2019-02-03 13:12:05.497000 UTC,26,apache-spark|dataframe|pyspark|schema|azure-databricks,57882,2018-04-21 13:17:15.767000 UTC,2022-03-03 07:35:28.907000 UTC,"Stuttgart, Deutschland",2187,1076,4,223,"<p>Yes it is possible. Use <a href=""https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=schema#pyspark.sql.DataFrame.schema"" rel=""noreferrer""><code>DataFrame.schema</code></a> <a href=""https://docs.python.org/3.7/library/functions.html#property"" rel=""noreferrer""><code>property</code></a></p>

<blockquote>
  <p><code>schema</code></p>
  
  <p>Returns the schema of this DataFrame as a pyspark.sql.types.StructType.</p>

<pre><code>&gt;&gt;&gt; df.schema
StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))
</code></pre>
  
  <p>New in version 1.3.</p>
</blockquote>

<p>Schema <a href=""https://stackoverflow.com/q/32028149/11008525"">can be also exported to JSON and imported back</a> if needed.</p>",0.0,2019-02-03 13:06:43.670000 UTC,,34.0,[]
git command to add/stage modified files in folders only,"<p>I've modified several files in a folder and subfolders. I could add the top folder recursively but then i have several subfolders that are untracked, so every time I do the recursive add to stage changes, I have to untrack the subfolders with <code>git rm -r --cached</code>. Is there a better way to just stage the modifications that show up in git status, without tracking already explicitly untracked files?</p>

<p>meta-question: is it really a good idea that <code>git add</code> means two (or more) things? in this context, if the command to track files (<code>git add</code>) were not the same used for stage modifications (<code>git add</code>) then i would not have this problem in the first place</p>",2,0,2010-10-10 16:23:35.583000 UTC,1.0,2019-10-23 08:50:02.207000 UTC,8,git|dvcs,12967,2009-09-09 00:31:24.100000 UTC,2022-03-04 16:20:55.223000 UTC,Panama,24332,1870,143,1043,<p><code>git add -u</code> adds only modifications. Also consider adding files which you don't want to track to <code>.gitignore</code> file.</p>,3.0,2010-10-10 16:29:26.993000 UTC,,22.0,[]
How to see all tags in TortoiseHg. Any handy way of switching and computing diffs between them?,"<p>Is there a way to see all tags in Tortoise Mercurial?  For example, is there a way to filter the visualization of all my commits and show only those that have tags in them?</p>

<p>Besides viewing only commits with tags, does TortoiseHg support any way of quickly switching or visualizing diffs between them?</p>",1,0,2012-02-01 18:36:15.257000 UTC,2.0,2012-02-02 07:44:53.880000 UTC,6,mercurial|dvcs|tortoisehg,1426,2010-02-28 23:05:44.127000 UTC,2021-12-11 19:26:47.753000 UTC,,81126,4017,14,3421,"<ul>
<li>Did you try to explore THG interface (namely - workbench toolbar buttons)? Fourth from left button do any filters (""tagged()"" is predefined)</li>
</ul>

<p><img src=""https://i.stack.imgur.com/d1w7n.png"" alt=""Tags filter""></p>

<ul>
<li>To view the differences between two tags, right click on the two selected tags</li>
</ul>

<p><img src=""https://i.stack.imgur.com/c3UGg.png"" alt=""Context menu""></p>",0.0,2012-02-01 20:41:49.087000 UTC,2012-08-16 17:12:10.263000 UTC,15.0,[]
Check in single file with Mercurial?,<p>Let's say you do <code>hg status</code> and you have three files modified.  I know how to check in all three files (<code>hg commit</code>).  But how can you check in (and then <code>hg push</code>) just <em>one</em> of the modified files?</p>,3,1,2010-05-25 20:02:57.863000 UTC,2.0,2012-01-31 22:15:31.060000 UTC,29,version-control|mercurial|dvcs,15032,2008-12-18 03:16:49.993000 UTC,2022-03-03 19:59:46.803000 UTC,"New York, NY",52509,2554,17,3816,"<p>Please check <a href=""http://www.selenic.com/mercurial/hg.1.html#commit"" rel=""noreferrer"">the output of <code>hg help commit</code></a> which reveals that you can do</p>

<pre><code>hg commit foo.c
</code></pre>

<p>if you just want to commit a single file. This is just like Subversion and many other systems — no hocus-pocus :-)</p>",2.0,2010-05-26 07:54:50.720000 UTC,2011-12-07 08:07:13.513000 UTC,44.0,[]
Can artists realistically cope with (distributed) version control in an open source environment?,"<p>Heya, I'm working in the project management department of an open source game. Right now we're using SVN for version control and store code and assets in the same repository. Source versions of assets (models, textures) reside in a separate media branch, while the rendered versions of the assets (we're working on an isometric 2d game, so we actually use rendered 2d images of the 3d models in the game) reside close to the code, as they're needed to be in place to run the game.</p>

<p>Our artists had a hard time to get started with using Subversion and to wrap their head around the concept of version control in general. Right now the project mostly consists of programmers and we're considering to move from SVN to distributed version control to ease working with branches (and the associated merging process) and sending in patches. We haven't made a decision about which DVCS to use yet, but we will most likely end up using either Mercurial or Git.</p>

<p>While distributed version control is great for developers with a technical background, it might seem overly complex and complicated for artist and other prolly less tech savvy devs.</p>

<p>So I'm looking for all kinds of advice how we could ease the version control workflow for artists. Keep in mind that using something like Perforce, regardless of how suited it might be for the job, is not option for a free of charge open source project. So I'm pretty much rather looking for advice, tutorials, project tools that make it easy for artists to wrap their head around distributed version control, especially Hg and/or Git.</p>

<p>Is it even worth going down that route and try to get the artists using distributed version control? We could continue to store the source versions of assets (textures, models) in our existing SVN repository. But we would still have to find a solution for the assets that are needed to run the game, as they should reside close to the code in version control.</p>

<p>There are a bunch of great DVCS guides out there, e.g. the <a href=""http://hginit.com/"">Hginit tutorial</a>. However the ones I've found were all written for programmers. It's great that they can now easily locally commit, use the full potential of branches and merge back their changes without too much hassle. But this might not be beneficial for artists but rather overly complex and scary to them. Do you happen to know a DVCS tutorial that was written for artists as the primary target audience?</p>

<p>We're also using Trac for project management purposes, so if you know of a Trac plugin that is artist friendly, let me know as well :-)</p>",6,1,2010-12-08 16:06:54.160000 UTC,4.0,,16,git|open-source|mercurial|project-management|dvcs,3505,2010-12-08 15:55:28.730000 UTC,2011-02-18 20:29:00.463000 UTC,,163,3,0,8,"<p>I'm not even sure programmers can realistically cope with distributed version control.  For proof, I offer the number of distributed version control questions on Stack Overflow.</p>

<p>My suggestion would be to give the artists two checklists or cheat sheets.  One to commit art work, and one to retrieve art work.  These cheat sheets would be specific to your work environment.</p>

<p>If an artist wants to understand the what of source control, one of the programmers can explain the details.</p>

<p>It's been my experience that most people want to get their work done, and don't care about the what.  They just care about how.</p>",1.0,2010-12-08 17:29:41.350000 UTC,,12.0,[]
Comparison between Centralized and Distributed Version Control Systems,"<p>What are the <strong>benefits and drawbacks</strong> with using <strong>Centralized versus Distributed</strong> Version Control Systems (DVCS)? Have you run into any problems in DVCS and how did you safeguard against these problems? <em>Keep the discussion tool agnostic and flaming to minimum.</em></p>

<p>For those wondering what DVCS tools are available, here is a list of the best known free/open source DVCSs:</p>

<ul>
<li><a href=""http://git.or.cz/"" rel=""noreferrer"">Git</a>, (written in C) used by the <a href=""http://git.or.cz/gitwiki/GitProjects"" rel=""noreferrer"">Linux Kernel and Ruby on Rails</a>.</li>
<li><a href=""https://www.mercurial-scm.org"" rel=""noreferrer"">Mercurial</a>, (written in Python) used by <a href=""https://www.mercurial-scm.org/wiki/ProjectsUsingMercurial"" rel=""noreferrer"">Mozilla and OpenJDK</a>.</li>
<li><a href=""http://bazaar-vcs.org/"" rel=""noreferrer"">Bazaar</a>, (written in Python) used by <a href=""https://wiki.ubuntu.com/UbuntuDevelopment"" rel=""noreferrer"">Ubuntu developers</a>.</li>
<li><a href=""http://darcs.net/"" rel=""noreferrer"">Darcs</a>, (written in Haskell).</li>
</ul>",15,2,2008-09-21 13:36:45.700000 UTC,41.0,2017-06-20 09:56:51.280000 UTC,78,version-control|comparison|dvcs,65665,2008-08-30 09:06:27.857000 UTC,2022-01-10 10:24:26.840000 UTC,"Lund, Sweden",115720,2486,36,4308,"<p>From <a href=""https://stackoverflow.com/questions/77485/what-are-the-relative-strengths-and-weaknesses-of-git-mercurial-and-bazaar#77834"">my answer</a> to a different <a href=""https://stackoverflow.com/questions/77485/what-are-the-relative-strengths-and-weaknesses-of-git-mercurial-and-bazaar"">question</a>:</p>

<blockquote>
  <p>Distributed version control systems
  (DVCSs) solve different problems than
  Centralized VCSs.  Comparing them is
  like comparing hammers and
  screwdrivers.</p>
  
  <p><a href=""http://en.wikipedia.org/wiki/Revision_control"" rel=""noreferrer"">Centralized VCS</a> systems are
  designed with the intent that there is
  One True Source that is Blessed, and
  therefore Good.  All developers work
  (checkout) from that source, and then
  add (commit) their changes, which then
  become similarly Blessed.  The only
  real difference between CVS,
  Subversion, ClearCase, Perforce,
  VisualSourceSafe and all the other
  CVCSes is in the workflow,
  performance, and integration that each
  product offers.</p>
  
  <p><a href=""http://en.wikipedia.org/wiki/Distributed_revision_control"" rel=""noreferrer"">Distributed VCS</a> systems are
  designed with the intent that one
  repository is as good as any other,
  and that merges from one repository to
  another are just another form of
  communication.  Any semantic value as
  to which repository should be trusted
  is imposed from the outside by
  process, not by the software itself.</p>
  
  <p>The real choice between using one type
  or the other is organizational -- if
  your project or organization wants
  centralized control, then a DVCS is a
  non-starter.  If your developers are
  expected to work all over the
  country/world, without secure
  broadband connections to a central
  repository, then DVCS is probably your
  salvation.  If you need both, you're
  fsck'd.</p>
</blockquote>",7.0,2008-09-21 13:55:21.117000 UTC,2017-05-23 12:18:09.537000 UTC,57.0,[]
What is the Fork & Pull Model in GitHub?,<p>I have heard this term used for managing a code base in GitHub. What does it mean?</p>,2,0,2012-07-20 16:02:55.410000 UTC,3.0,2012-08-26 19:20:29.290000 UTC,14,git|github|terminology|dvcs,7174,2010-06-18 20:47:37.817000 UTC,2022-02-27 23:24:45.020000 UTC,"San Francisco, CA, USA",12431,1882,153,1957,"<p>As noted <a href=""https://help.github.com/articles/about-collaborative-development-models/"" rel=""nofollow noreferrer"">within the GitHub docs</a>:</p>

<blockquote>
  <p>The <em>Fork &amp; Pull Model</em> lets anyone fork an existing repository and push changes to their personal fork without requiring access be granted to the source repository. The changes must then be pulled into the source repository by the project maintainer. This model reduces the amount of friction for new contributors and is popular with open source projects because it allows people to work independently without upfront coordination. [...] Pull requests are especially useful in the Fork &amp; Pull Model because they provide a way to notify project maintainers about changes in your fork.</p>
</blockquote>",1.0,2012-07-20 16:02:55.410000 UTC,2017-09-10 15:27:03.990000 UTC,16.0,[]
Hg: How to do a rebase like git's rebase,"<p>In Git I can do this:</p>

<pre>
1. Start working on new feature:
$ git co -b newfeature-123  # (a local feature development branch)
do a few commits (M, N, O)

master A---B---C
                \
newfeature-123   M---N---O

2. Pull new changes from upstream master:
$ git pull
(master updated with ff-commits)

master A---B---C---D---E---F
                \
newfeature-123   M---N---O

3. Rebase off master so that my new feature 
can be developed against the latest upstream changes:
(from newfeature-123)
$ git rebase master

master A---B---C---D---E---F
                            \
newfeature-123               M---N---O
</pre>

<p><br /></p>

<p>I want to know how to do the same thing in Mercurial, and I've scoured the web for an answer, but the best I could find was: <a href=""https://www.mercurial-scm.org/pipermail/mercurial/2007-June/013393.html"" rel=""noreferrer"">git rebase - can hg do that</a></p>

<p>That link provides 2 examples:<br />
1. I'll admit that this: (replacing the revisions from the example with those from my own example)</p>

<pre>
hg up -C F  
hg branch -f newfeature-123  
hg transplant -a -b newfeature-123 
</pre>

<p>is not too bad, except that it leaves behind the pre-rebase M-N-O as an unmerged head and creates 3 new commits M',N',O' that represent them branching off the updated mainline.</p>

<p>Basically the problem is that I end up with this:</p>

<pre>
master A---B---C---D---E---F
                \           \
newfeature-123   \           M'---N'---O'
                  \
newfeature-123     M---N---O
</pre>

<p>this is not good because it leaves behind local, unwanted commits that should be dropped.</p>

<ol start=""2"">
<li>The other option from the same link is </li>
</ol>

<pre>
hg qimport -r M:O
hg qpop -a
hg up F
hg branch newfeature-123
hg qpush -a
hg qdel -r qbase:qtip
</pre>

<p>and this does result in the desired graph:</p>

<pre>
master A---B---C---D---E---F
                            \
newfeature-123               M---N---O
</pre>

<p>but these commands (all 6 of them!) seem so much more complicated than </p>

<pre>
$ git rebase master
</pre>

<p>I want to know if this is the only equivalent in Hg or if there is some other way available that is simple like Git.</p>",5,4,2010-04-20 03:34:23.237000 UTC,66.0,2017-06-29 11:52:13.023000 UTC,210,git|mercurial|dvcs|rebase,80747,2009-10-20 03:39:35.510000 UTC,2021-09-27 23:28:23.793000 UTC,"Los Angeles, CA",14514,1003,35,714,"<p>VonC has <a href=""https://stackoverflow.com/questions/2672351/hg-how-to-do-a-rebase-like-gits-rebase/2672429#2672429"">the answer you're looking for</a>, the Rebase Extension.  It is, however, worth spending a second or two thinking about why neither mq nor rebase are enabled by default in mercurial: because mercurial is all about indelible changesets.  When I work in the manner you're describing, which is nearly daily, here's the pattern I take:</p>

<pre><code>1. Start working on a new feature:
$ hg clone mainline-repo newfeature-123
do a few commits (M, N, O)

master A---B---C
                \
newfeature-123   M---N---O

2. Pull new changes from upstream mainline:
$ hg pull

master A---B---C---D---E---F
                \
newfeature-123   M---N---O

3. merge master into my clone so that my new feature 
can be developed against the latest upstream changes:
(from newfeature-123)
$ hg merge F

master A---B---C---D---E---F
                \           \
newfeature-123   M---N---O---P
</code></pre>

<p>and that's really all that's necessary.  I end up with a newfeature-123 clone I can easily push back to the mainline when I'm happy with it.  Most importantly, however, I <em>never changed history</em>.  Someone can look at my csets and see what they were originally coded against and how I reacted to changes in the mainline throughout my work.  Not everyone thinks that has value, but I'm a firm believer that it's the job of source control to show us not what we wished had happened, but what actually happened -- every deadend and every refactor should leave an indelible trace, and rebasing and other history editing techniques hide that.</p>

<p>Now go pick VonC's answer while I put my soapbox away. :)</p>",7.0,2010-04-20 04:14:15.697000 UTC,2017-05-23 12:18:14.363000 UTC,237.0,[]
Using Mercurial in a Large Organization,"<p>I've been using Mercurial for my own personal projects for a while, and I love it.  My employer is considering a switch from CVS to SVN, but I'm wondering whether I should push for Mercurial (or some other DVCS) instead.</p>

<p>One wrinkle with Mercurial is that it seems to be designed around the idea of having a single repository per ""project"".  In this organization, there are dozens of different executables, DLLs, and other components in the current CVS repository, hierarchically organized.  There are a lot of generic reusable components, but also some customer-specific components, and customer-specific configurations.  The current build procedures generally get some set of subtrees out of the CVS repository.</p>

<p>If we move from CVS to Mercurial, what is the best way to organize the repository/repositories?  Should we have one huge Mercurial repository containing everything?  If not, how fine-grained should the smaller repositories be?  I think people will find it very annoying if they have to pull and push updates from a lot of different places, but they will also find it annoying if they have to pull/push the entire company codebase.</p>

<p>Anybody have experience with this, or advice?</p>

<hr>

<p>Related questions:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/2383826/git-based-source-control-in-the-enterprise-suggested-tools-and-practices"">Git-Based Source Control in the Enterprise: Suggested Tools and Practices?</a></li>
<li><a href=""https://stackoverflow.com/questions/2476356/distributed-version-control-for-huge-projects-is-it-feasible"">Distributed version control for HUGE projects - is it feasible?</a></li>
</ul>",3,0,2010-03-19 17:23:42.817000 UTC,39.0,2017-05-23 10:28:52.513000 UTC,63,mercurial|dvcs,8489,2008-08-13 10:21:54.053000 UTC,2022-03-04 15:36:39.060000 UTC,"Cumming, GA, USA",78753,8387,679,5906,"<p>AFAICS most of the resistance to any of the DVCSes comes from people not understanding how to use them. The oft-repeated statement that ""there is no central repository"" is very scary to people who have been locked into the CVS/SVN model since time immemorial and can't imagine anything else, especially so for management and senior (experienced and/or cynical) developers who want strong source code tracking and reproducibility (and perhaps also if you have to satisfy certain standards regarding your development processes, like we did at a place I once worked). Well, you can have a central ""blessed"" repo; you just aren't shackled to it. It's easy for a subteam to set up an internal playground repo on one of their workstations for a while, for example.</p>

<p>There are so many ways to skin the proverbial cat that it will pay you to sit down and think carefully about your workflow. Think about your current practices and the power that nearly-free cloning and branching gives you. It's likely that some of what you currently do will have evolved to work around the limitations of the CVS-type model; be prepared to break the mould. You will probably need to appoint a champion or two to ease everybody through the transition; with a big team you probably want to think about restricting commit access to <em>blessed</em>.</p>

<p>At my work (small software house) we moved from CVS to hg and wouldn't go back. We're using it in a mostly-centralised way. Converting our main (ancient and very large) repo was painful, but it will be whatever way you go, and when it's done it's done - it'll be a lot easier to change VCS later. (We found a number of situations where the CVS conversion tools just can't figure out what happened; where somebody's commit only partially succeeded and they didn't notice for days; resolving vendor branches; general madness and insanity caused by time appearing to go backwards, not helped by commit timestamps in local time from different timezones...)</p>

<p>The great benefit I've found of a DVCS is the ability to commit early and commit often and only push when it's ready. As I reach various work-in-progress milestones I like to lay down a line in the sand so that I have somewhere I can rewind to if need be - but these are not commits which should be exposed to the team as they are manifestly incomplete in myriad ways. (I do this mostly with mercurial queues.) It's all about the workflow; I could never have done this with CVS.</p>

<p>I guess you already know this, but if you're contemplating moving away from CVS, you can do so much better than SVN...</p>

<hr>

<p>To monolith, or to module? Any paradigm shift is going to be tricky whatever VCS you work with, distributed or not; the CVS model is quite special in how it allows you to commit on a file by file basis without checking whether the rest of the repo is up to date (and let's not mention the headache that module aliases have been known to cause).</p>

<ul>
<li>Dealing with monolithic repositories can be pretty slow. Your vcs client has to scan your copy of the entire universe for changes, as opposed to just a single module. (If you're working on Linux, look into the hg inotify extension if you haven't already done so.)</li>
<li>A monolithic repo also causes unnecessary race conditions when committing(pushing). It's like the CVS up-to-date check, but applied across the entire repo: if you have many active developers, committing frequently, this one will bite you.</li>
</ul>

<p>I'd suggest that it's worth the effort to stay away from monolithic, but beware that it will impose its own overhead in terms of added complexity in your build system. (Side note: If you find something a tiresome chore, automate it! We programmers are lazy creatures, after all.) Splitting your repo out into all its component modules may be too extreme; there may be a halfway house to be found with related components grouped together among a small number of repositories. You may also find it useful to look into mercurial's submodule support - <a href=""https://www.mercurial-scm.org/wiki/NestedRepositories"" rel=""nofollow noreferrer"">Nested Repositories</a> and the <a href=""https://www.mercurial-scm.org/wiki/ForestExtension"" rel=""nofollow noreferrer"">Forest Extension</a> (both of which I ought to try and get my head around).</p>

<p>At a former workplace we had several dozen components which were kept as independent CVS modules with a fairly regimented metastructure. Components declared what they depended on and which built parts were to be exported where; the build system automatically wrote make fragments so that what you were working on would pick up what it needed. It generally worked very well and it was quite rare to fail the CVS up-to-date check. (There was also a fiendishly complicated but extremely powerful build bot with a least-effort attitude to dependency resolution: it wouldn't rebuild a component if there was already one which met your requirements. Add to that meta-components which assembled installers and whole ISO images, and you have a good recipe for easy start-to-finish builds and for things going Sorcerers Apprentice. Somebody ought to write a book about it...)</p>",0.0,2010-03-19 21:50:06.133000 UTC,2017-06-19 10:19:56.850000 UTC,43.0,[]
"Why Kiln is based on Mercurial, and not other (D)VCS","<p>What were the reason for chosing Mercurial as a basis of FogCreek <a href=""http://fogcreek.com/kiln/"" rel=""noreferrer""><strong>Kiln</strong></a>, a source control management system with tightly integrated code review, and FogBugz integration?</p>

<p>Why Mercurial, and not other (distributed) version control system, like Bazaar, Git or Monotone, or creating own version control system like <a href=""http://www.fossil-scm.org/"" rel=""noreferrer"">Fossil</a> (distributed software configuration management, including bug tracking and wiki) did?</p>

<p>What were features that make FogCreek choose Mercurial as Kiln engine?</p>",6,7,2009-11-09 23:05:38.107000 UTC,8.0,2009-11-10 00:29:07.643000 UTC,41,version-control|mercurial|dvcs|kiln,10509,2008-12-14 01:08:47.257000 UTC,2022-03-03 14:02:57.650000 UTC,"Toruń, Kuiavia-Pomerania Poland",284455,1440,41,15713,"<p>Here's an answer from one of the Kiln developers.</p>

<ul>
<li>It provides real branching.</li>
<li>It's easy to use.</li>
<li>Windows support is very good.</li>
<li>It's fast.</li>
<li>It's powerful.</li>
<li>It's easily extensible.</li>
</ul>

<p>Check out the full details <a href=""https://web.archive.org/web/20130801182155/http://kiln.stackexchange.com/questions/187/why-is-kiln-supporting-mercurial-over-git-or-another-scm"" rel=""nofollow noreferrer"">here</a>.  They explained themselves quite thoroughly.</p>",9.0,2009-11-10 00:23:12.007000 UTC,2015-10-29 14:28:41.297000 UTC,75.0,[]
Fossil: Deleting missing files in one command?,"<p>Is there a way to delete ""MISSING"" files explicitly without having to delete them with <code>fossil rm</code> one by one?</p>

<p>I've recently created a repository with a Flash XFL folder in it and Flash seems to delete and create some binary files somewhat arbitrarily. Having to delete them manually every time I commit is a bit annoying.</p>",1,0,2011-01-12 14:37:02.813000 UTC,2.0,,7,dvcs|fossil,1473,2009-04-17 09:38:35.843000 UTC,2021-11-29 15:29:46.377000 UTC,"Rheda-Wiedenbrück, Germany",10594,560,59,979,"<p>Nevermind. I'm an idiot. Fossil has me covered with <code>fossil addremove</code>. Seems like this will replace <code>fossil add .</code> in my workflow now.</p>

<p>It really <em>does</em> include everything but the kitchen sink. Thanks, drh.</p>",1.0,2011-01-12 14:43:11.967000 UTC,,12.0,[]
Locking binary files using git version control system,"<p>For one and a half years, I have been keeping my eyes on the git community in hopes of making the switch away from SVN.  One particular issue holding me back is the inability to lock binary files. Throughout the past year I have yet to see developments on this issue.  I understand that locking files goes against the fundamental principles of distributed source control, but I don't see how a web development company can take advantage of git to track source code and image file changes when there is the potential for binary file conflicts.</p>

<p>To achieve the effects of locking, a ""central"" repository must be identified.  Regardless of the distributed nature of git, most companies will have a ""central"" repository for a software project.  We should be able to mark a file as requiring a lock from the governing git repository at a specified address.  Perhaps this is made difficult because git tracks file contents not files?</p>

<p>Do any of you have experience in dealing with git and binary files that should be locked before modification?</p>

<p>NOTE: It looks like Source Gear's new open source distributed version control project, Veracity, has locking as one of its goals.</p>",17,0,2008-09-23 06:46:41.270000 UTC,23.0,2018-02-13 11:47:01.630000 UTC,76,git|version-control|binaryfiles|dvcs,49924,2008-08-06 05:36:48.180000 UTC,2022-03-01 16:31:29.740000 UTC,"Waterloo, ON, Canada",2209,118,0,234,"<p><a href=""https://github.com/blog/2328-git-lfs-2-0-0-released"" rel=""noreferrer"">Git LFS 2.0</a> has added support for file locking.</p>
<blockquote>
<p>With Git LFS 2.0.0 you can now lock files that you're actively working on, preventing others from pushing to the Git LFS server until you unlock the files again.</p>
<p>This will prevent merge conflicts as well as lost work on non-mergeable files at the filesystem level. While it may seem to contradict the distributed and parallel nature of Git, file locking is an important part of many software development workflows—particularly for larger teams working with binary assets.</p>
</blockquote>",0.0,2017-04-26 03:05:45.250000 UTC,2020-06-20 09:12:55.060000 UTC,9.0,[]
Git Submodule to a subfolder,"<p>In svn you can link a repository to any folder in another svn repository.  I'm wondering if there is a similar feature for git?  Basically I want a git submodule inside my repository, but I want the submodule to be a pointer to a subfolder of another git repository, not the whole repository.  Is this possible?</p>",3,0,2009-07-13 18:28:34.280000 UTC,5.0,,32,git|dvcs|git-submodules,11239,2008-09-15 15:24:28.200000 UTC,2022-03-03 10:51:25.503000 UTC,"Cape Town, Western Cape, South Africa",20108,725,8,1298,"<p>Git does not support partial checkouts, so the <a href=""https://schacon.github.com/git/git-submodule.html"" rel=""noreferrer"">submodule</a> must point to a complete repository.</p>

<p><a href=""http://kerneltrap.org/mailarchive/git/2007/10/15/344049"" rel=""noreferrer"">This thread on the Git mail list</a> provides some background information.</p>

<p><a href=""https://web.archive.org/web/20090207160050/http://panthersoftware.com/articles/view/3/svn-s-svn-externals-to-git-s-submodule-for-rails-plugins"" rel=""noreferrer"">This article from Panther Software</a> offers some insight from someone else trying to accomplish a similar goal (i.e. replicate <code>svn:externals</code> using Git).</p>

<p>If both projects are under your control, then I suggest you isolate the ""subfolder"" that you interested in to its own standalone repo.  Then both projects can create submodules that link to it.</p>",2.0,2009-07-13 19:00:25.293000 UTC,2019-12-31 21:22:10.280000 UTC,25.0,[]
Merging with MercurialEclipse has conflicts that are resolved automatically when merged at the command line,"<p>I am playing around with Mercurial to see if it is suitable for use in our company.  One of the big selling points of it is the merging capabilities.  So I have been playing around with creating branches and merging them back into the default line.  The tested involved simply adding a new method (methodA) to a single Java file in one branch, and adding a different method (methodB) in a completely different place in the same file in another branch.</p>

<p>When I first tried it in Eclipse using the team-> merge option I found that the first merge worked fine (i.e. it added method A).  When I try to merge the second branch now it tells me there is a conflict that I must resolve.  This is very unfortunate as I thought this simple kind of merge was exactly the kind of thing Mercurial was supposed to handle with ease?</p>

<p>I tried the exact same test using the command line, and this time it worked fine, i.e. both of the merges were successful with no need to resolve conflicts.  Looking at the console output in eclipse it is using the following command to perform the merge:</p>

<pre><code>hg -y merge --config ui.merge=internal:fail -r 611ca2784593525cdafd3082b17d3310037a5d58 -f
</code></pre>

<p>whereas when I run it myself from the command line I simply do:</p>

<pre><code>hg merge -r 1234
</code></pre>

<p>Is the use of the merge strategy 'internal:fail' causing this to happen within Eclipse??  And if so is it possible to change the default behaviour so that it works the same way as it does at the command line?</p>",1,1,2011-01-10 12:26:15.343000 UTC,1.0,2011-01-10 14:15:40.673000 UTC,5,java|eclipse|version-control|mercurial|dvcs,3094,2009-07-31 09:15:19.333000 UTC,2022-01-27 19:09:50.710000 UTC,"Glasgow, United Kingdom",9865,513,158,838,"<p>The <code>internal:fail</code> tells Mercurial to not to try merge anything at all. See <a href=""https://www.mercurial-scm.org/wiki/MergeToolConfiguration?highlight=(internal:fail)"" rel=""nofollow noreferrer"">the mercurial wiki</a> for more details.</p>

<p>It seems whomever wrote the Eclipse plugin for mercurial felt they could better control the merge in eclipse by having Mercurial automatically fail to merge everything and then to do the merge in Eclipse, and presumably call <code>hg resolve --mark ...</code> as files are merged.</p>

<p>From the Mercurial command line you're getting both the <code>premerge</code> behavior which handles trivial merges, and then if there are still conflicts the invocation of tool from your <a href=""https://www.mercurial-scm.org/wiki/MergeToolConfiguration?highlight=(internal:fail)"" rel=""nofollow noreferrer"">MergeToolConfiguration</a> that has the highest priority for that file type and is available on your local system.</p>",1.0,2011-01-10 15:15:40.213000 UTC,2017-06-30 12:03:27.367000 UTC,8.0,[]
Is there a way to optimize this Gremlin query?,"<p>I have a graph database which looks like this (simplified) diagram:</p>

<p><img src=""https://i.stack.imgur.com/GIBZR.png"" alt=""Graph diagram""></p>

<p>Each unique ID has many properties, which are represented as edges from the ID to unique values of that property. Basically that means that if two ID nodes have the same email, then their <code>has_email</code> edges will both point to the same node. In the diagram, the two shown IDs share both a first name and a last name.</p>

<p>I'm having difficulty writing an efficient Gremlin query to find matching IDs, for a given set of ""matching rules"". A matching rule will consist of a set of properties which must all be the same for IDs to be considered to have come from the same person. The query I'm currently using to match people based on their first name, last name, and email looks like:</p>

<pre><code>g.V().match(
    __.as(""id"").hasId(""some_id""),
    __.as(""id"")
        .out(""has_firstName"")
        .in(""has_firstName"")
        .as(""firstName""),
    __.as(""id"")
        .out(""has_lastName"")
        .in(""has_lastName"")
        .as(""lastName""),
    __.as(""id"")
        .out(""has_email"")
        .in(""has_email"")
        .as(""email""),
    where(""firstName"", eq(""lastName"")),
    where(""firstName"", eq(""email"")),
    where(""firstName"", neq(""id""))
).select(""firstName"")
</code></pre>

<p>The query returns a list of IDs which match the input <code>some_id</code>.</p>

<p>When this query tries to match an ID with a particularly common first name, it becomes very, very slow. I suspect that the <code>match</code> step is the problem, but I've struggled to find an alternative with no luck so far.</p>",1,0,2019-11-11 12:10:14.667000 UTC,2.0,2019-12-30 14:22:02.860000 UTC,3,graph-databases|gremlin|amazon-neptune,730,2016-08-22 15:59:29.100000 UTC,2022-03-03 17:32:11.513000 UTC,,33,3,0,2,"<p>The performance of this query will depend on the edge degrees in your graph. Since many people share the same first name, you will most likely have a huge amount of edge going into a specific <code>firstName</code> vertex.
You can make assumptions, like: there are fewer people with the same last name than people with the same first name. And of course, there should be even fewer people who share the same email address. With that knowledge you just can start to traverse to the vertices with the lowest degree first and then filter from there:</p>

<pre><code>g.V().hasId(""some_id"").as(""id"").
  out(""has_email"").in(""has_email"").where(neq(""id"")).
  filter(out(""has_lastName"").where(__.in(""has_lastName"").as(""id""))).
  filter(out(""has_firstName"").where(__.in(""has_firstName"").as(""id"")))
</code></pre>

<p>With that, the performance will mostly depend on the vertex with the lowest edge degree.</p>",4.0,2019-11-11 15:40:07.327000 UTC,,8.0,[]
Are Mercurial's bundled extensions considered part of it's core feature set and approach to version control?,"<p>I'm currently trying to evaluate Mercurial, to get a feel for the philosophy the system tries to promote - but one thing that's got me confused is the presence of the bundled 'extensions' and how they fit into the mix. </p>

<p>In the core package, Mercurial ships with a bunch of functionality that is implemented as extensions but is disabled by default. (See: <a href=""https://www.mercurial-scm.org/wiki/UsingExtensions#Extensions_Bundled_with_Mercurial"" rel=""nofollow noreferrer"">https://www.mercurial-scm.org/wiki/UsingExtensions#Extensions_Bundled_with_Mercurial</a>)</p>

<p>Here's the thing I'm confused about: </p>

<ul>
<li><p>Are these extensions considered first class citizens by the Mercurial dev team and therefore part of the overall Mercurial approach to DVCS? </p></li>
<li><p>Why are they implemented outside of the default features and disabled by default?</p></li>
</ul>

<p>I don't need info on how activate extensions, that's pretty straight forward - it's the logic behind the separation that I'm curious about.</p>

<p>The reason I'm trying to get my head around this is because I don't really want to try and crowbar an opposing approach into Mercurial via extensions if it differs from the overall philosophy of the project.</p>",2,0,2009-11-16 08:50:28.857000 UTC,,2017-06-16 14:27:54.747000 UTC,6,mercurial|dvcs,170,2009-05-11 03:31:23.613000 UTC,2022-03-05 10:27:04.930000 UTC,"Brisbane, Australia",399,69,1,45,"<blockquote>
  <p>Are these extensions considered first class citizens by the Mercurial dev team and therefore part of the overall Mercurial approach to DVCS?</p>
</blockquote>

<p>Yes, although we won't generally advocate their use to new users, they are very useful for advanced usage. I guess everybody in the dev team has extension enabled (at least mq, patchbomb, and sometimes record).</p>

<p>Extension accepted in <code>hgext/</code> are reviewed priori to inclusion, and we generally require them to provides tests. But they are often <em>owned</em> by outside contributors and aren't updated by the dev team except for API changes within core hg.</p>

<blockquote>
  <p>Why are they implemented outside of the default features and disabled by default?</p>
</blockquote>

<p>We generally think that hg should stay simple and adding more commands might confuse users (e.g. if you have a simple workflow you don't need to learn about mq). But if a command is deemed useful for most users, it can migrate from an extension into core (that was the case for bisect, and it is the case of the subrepo functionality).</p>",1.0,2009-11-16 09:23:02.200000 UTC,,8.0,[]
hg equivalant of git revert,"<p>I have a commit on a public repository.  I would like this commit to not be there (I've moved that work off to a branch), I obviously don't want to destroy the branch history, basically just do an inverse of that commit.  In git this is just <code>git revert</code>, but I'm not using git :)</p>",1,0,2011-01-03 19:00:25.750000 UTC,3.0,2011-01-03 19:17:39.937000 UTC,13,git|mercurial|dvcs,1783,2008-11-13 00:04:49.993000 UTC,2022-03-03 17:23:51.153000 UTC,"Washington, DC, USA",13418,132,16,2894,"<p><code>hg backout</code></p>

<blockquote>
  <p>hg backout [OPTION]... [-r] REV</p>
  
  <p>reverse effect of earlier changeset</p>

<pre><code>Commit the backed out changes as a new changeset. The new
changeset is a child of the backed out changeset.

If you backout a changeset other than the tip, a new head is
created. This head will be the new tip and you should merge this
backout changeset with another head.
</code></pre>
</blockquote>",0.0,2011-01-03 19:16:00.553000 UTC,,16.0,[]
Sell me distributed revision control,"<p>I know 1000s of similar topics floating around. I read at lest 5 threads here in SO But why am I still not convinced about DVCS?</p>

<p>I have only following questions (note that I am selfishly worried only about Java projects)</p>

<ul>
<li>What is the advantage or value of
committing locally? What? really? All
modern IDEs allows you to keep track
of your changes? and if required you
can restore a particular change.
Also, they have a feature to label
your changes/versions at IDE level!?</li>
<li>what if I crash my hard drive? where
did my local repository go? (so how is it cool compared to checking in to a central repo?)</li>
<li>Working offline or in an air plane.
What is the big deal?In order for me
to build a release with my changes, I
must eventually connect to the
central repository. Till then it does not matter how I track my changes locally.</li>
<li>Ok Linus Torvalds gives his life to
Git and <em>hates</em> everything else.
Is that enough to blindly sing
praises? Linus lives in a different
world compared to offshore developers
in my mid-sized project?</li>
</ul>

<p>Pitch me!</p>",10,12,2010-04-01 21:33:06.507000 UTC,9.0,2014-04-12 12:18:43.067000 UTC,21,svn|cvs|dvcs|centralized,3486,2009-11-16 16:52:56.947000 UTC,2022-03-04 09:14:47.797000 UTC,"Bangalore, Karnataka, India",19585,490,16,1509,"<h1>Reliability</h1>

<p>If your harddisk silently starts corrupting data, you damn well want to know about it. Git takes SHA1 hashes of everything you commit. You have 1 central repo with SVN and if its bits get silently modified by a faulty HDD controller you won't know about it till it's too late.</p>

<p>And since you have 1 central repo, <em>you just blew your only lifeline</em>.</p>

<p>With git, <em>everyone</em> has an identical repo, complete with change history, and its content can be fully trusted due to SHA1's of its complete image. So if you back up your 20 byte SHA1 of your HEAD you can be certain that when you clone from some untrusted mirror, you have the exact same repo you lost!</p>

<h1>Branching (and namespace pollution)</h1>

<p>When you use a centralised repo, all the branches are there for the world to see. You can't make private branches. You have to make some branch that doesn't already collide with some other global name. </p>

<blockquote>
  <p>""<code>test123</code> -- damn, there's already a
  <code>test123</code>. Lets try <code>test124</code>.""</p>
</blockquote>

<p>And everyone has to see all these branches with stupid names. You have to succumb to company policy that might go along the lines of ""don't make branches unless you <em>really</em> need to"", which prevents a lot of freedoms you get with git.</p>

<p>Same with committing. When you commit, you better be <em>really</em> sure your code works. Otherwise you break the build. No intermediate commits. 'Cause they all go to the central repo.</p>

<p>With git you have none of this nonsense. Branch and commit locally all you want. When you're ready to expose your changes to the rest of the world, you ask them to pull from you, or you push it to some ""main"" git repo.</p>

<h1>Performance</h1>

<p>Since your repo is local, all the VCS operations are <em>fast</em> and don't require round trips and transfer from the central server! <code>git log</code> doesn't have to go over the network to find a change history. SVN does. Same with all other commands, since all the important stuff is stored in <em>one location</em>!</p>

<p>Watch <a href=""http://www.youtube.com/watch?v=4XpnKHJAok8"" rel=""noreferrer"">Linus' talk</a> for these and other benefits over SVN.</p>",2.0,2010-04-03 09:40:33.127000 UTC,2010-04-03 09:58:45.137000 UTC,13.0,[]
Showing renames in hg status?,"<p>I know that Mercurial can track renames of files, but how do I get it to <em>show</em> me renames instead of adds/removes when I do <code>hg status</code>? For instance, instead of:</p>

<pre><code>A bin/extract-csv-column.pl
A bin/find-mirna-binding.pl
A bin/xls2csv-separate-sheets.pl
A lib/Text/CSV/Euclid.pm
R src/extract-csv-column.pl
R src/find-mirna-binding.pl
R src/modules/Text/CSV/Euclid.pm
R src/xls2csv-separate-sheets.pl
</code></pre>

<p>I want some indication that four files have been moved.</p>

<p>I think I read somewhere that the output is like this to preserve backward-compatibility with something-or-other, but I'm not worried about that.</p>",2,2,2010-04-20 23:56:56.100000 UTC,8.0,2010-04-21 01:17:38.900000 UTC,30,mercurial|dvcs|rename,3327,2009-06-19 18:58:29.557000 UTC,2022-03-02 21:07:49.493000 UTC,"La Jolla, CA",38506,516,0,1376,"<p>There are several ways to do this.</p>

<p>Before you commit, you can use <code>hg diff --git</code> to show what was renamed:</p>

<pre><code>$ hg diff --git
diff --git a/theTest.txt b/aTest.txt
rename from theTest.txt
rename to aTest.txt
</code></pre>

<p>Note that this only works if you used <code>hg mv</code>, <code>hg rename</code>, or <code>mv</code> and <code>hg addremove --similarity 100</code>.</p>

<p>After you commit, you can still use <code>hg diff</code>, but you'll have to specify the change using <code>-r</code>:</p>

<pre><code>$ hg diff -r 0 -r 1 --git
diff --git a/test.txt b/theTest.txt
rename from test.txt
rename to theTest.txt
</code></pre>

<p>For both <code>hg status</code> and <code>hg log</code>, use the -C command-line flag to see the source that a file was copied from.</p>

<pre><code>$ hg status -C
A aTest.txt
  theTest.txt
R theTest.txt
</code></pre>

<p>The line just below aTest.txt indicates the source it was copied from (theTest.txt).</p>

<pre><code>$ hg log -v -C
changeset:   1:4d7b42489d9f
tag:         tip
user:        jhurne
date:        Tue Apr 20 20:57:07 2010 -0400
files:       test.txt theTest.txt
copies:      theTest.txt (test.txt)
description:
Renamed test.txt
</code></pre>

<p>You can see the files that were affected (test.txt and theTest.txt), and that ""theTest.txt"" was copied from test.txt.</p>",0.0,2010-04-21 01:17:11.337000 UTC,,34.0,[]
git untracked files - how to ignore,"<p>When I run <code>git status</code>, I see the ""Untracked files"" section has many files (some with a ""."" extension.)</p>

<p>I don't think I have to do anything, but it doesnt look good to see these files whenever I run <code>git status</code>. Is there any way to not to see these files?</p>",5,3,2011-06-09 15:33:45.633000 UTC,8.0,2018-11-22 22:06:14.113000 UTC,16,git|version-control|dvcs|gitignore|git-status,44587,2010-04-07 22:13:03.567000 UTC,2021-06-28 21:53:30.380000 UTC,,2254,40,1,152,"<p>You need to create one or several <code>.gitignore</code> files. They can be stored in git itself, or just kept local.</p>",2.0,2011-06-09 15:35:44.163000 UTC,,16.0,[]
Managing aesthetic code changes in git,"<p>I find that I make a lot of small changes to my source code, often things that have almost no functional effect. For example:</p>

<ul>
<li>Refining or correcting comments.</li>
<li>Moving function definitions within a class for a more natural reading order.</li>
<li>Spacing and lining up some declarations for readability.</li>
<li>Collapsing something using multiple lines on to one.</li>
<li>Removing an old piece of commented-out code.</li>
<li>Correcting some inconsistent whitespace.</li>
</ul>

<p>I guess I have a formidable attention to detail in my code. But the problem is I don't know what to do about these changes and they make it difficult to switch between branches etc. in git. I find myself not knowing whether to commit the minor changes, stash them, or put them in a separate branch of little tweaks and merge that in later. None those options seems ideal. </p>

<p>The main problem is that these sort of changes are unpredictable. If I was to commit these there would be so many commits with the message ""Minor code aesthetic change."", because, the second I make such a commit I notice another similar issue. What should I do when I make a minor change, a significant change, and then another minor change? I'd like to merge the three minor changes into one commit. It's also annoying seeing files as modified in <code>git status</code> when the change barely warrants my attention. I know about <code>git commit --amend</code> but I also know that's bad practice as it makes my repo inconsistent with remotes.</p>",6,3,2009-09-24 11:27:46.607000 UTC,3.0,2010-04-16 15:59:01.160000 UTC,28,git|version-control|dvcs,2271,2009-05-27 09:19:00.517000 UTC,2020-12-24 08:06:14.927000 UTC,,7207,188,15,533,"<p>On one hand I don't think moderately frequent minor changes do a lot of harm to anything or anyone. I always commit minor cosmetic changes right away and in our team, we have agreed that we just use the word cosmetic in our commit message and that's it.</p>

<p>What I wouldn't recommend is commiting cosmetics together with other changes!!</p>

<p>On the other hand, if it is a problem for you and you would like to change something I would suggest doing cosmetic sessions where you fix all kind of things from the list you mentioned at once. It might also be more efficient to concentrate on cosmetics only at a certain point of time. Otherwise cosmetic changes can become a procrastinational activity.</p>",0.0,2009-09-24 11:51:39.367000 UTC,2009-09-24 12:00:54.463000 UTC,20.0,[]
Show all commits whose diff contain specific string,"<p>As the title says, I want to find every commit whose diff contains specific string.</p>

<p>At the moment, I use   </p>

<pre><code>git log -p 'filename'
</code></pre>

<p>Which shows less like interface of every diff, where I search for the string.
Then I backtrace to find the actual commit msg.</p>

<p>Simple alternative might be to pipe git log -p into grep, but I can not find the commit id or message that way.</p>",7,1,2013-08-21 17:39:13.020000 UTC,6.0,2013-08-21 18:29:13.713000 UTC,25,git|dvcs,9030,2009-05-12 09:34:03.417000 UTC,2022-01-31 15:42:39.677000 UTC,"Dublin, Ireland",2059,583,5,189,"<p>Here's a one-liner shell script (split into more than one line for formatting purposes) that extracts the rev-numbers of current-branch-reachable commits affecting <code>path</code> where <code>git show -p</code> contains the given <code>pattern</code>.  It's not perfect (it will match commit messages as well as diffs) but it should be easy to tweak however you like, from here.</p>

<pre><code>git rev-list HEAD -- path |
while read rev; do
    if git show -p $rev | grep pattern &gt;/dev/null; then
        echo $rev
    fi
done
</code></pre>

<p>Note that you can replace <code>git show</code> with, e.g., <code>git diff $rev^ $rev</code> (note that this only compares against first-parent if it's a merge), or <code>git whatchanged $rev</code>, or whatever you like.  The main trick is to start with <code>git rev-list</code> to extract all the candidates (commits affecting the given path; omit the <code>-- path</code> part to get all commits starting from <code>HEAD</code>).  See <a href=""https://www.kernel.org/pub//software/scm/git/docs/git-rev-list.html"">git-rev-list(1)</a> for lots of other things you can do with <code>git rev-list</code>.</p>",2.0,2013-08-21 18:35:21.970000 UTC,,15.0,[]
What are the advantages of a rebase over a merge in git?,"<p>In <a href=""http://eagain.net/articles/git-for-computer-scientists/"" rel=""nofollow noreferrer"">this article</a>, the author explains rebasing with this diagram:</p>

<p><a href=""https://i.stack.imgur.com/XodLC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XodLC.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p><strong>Rebase:</strong> If you have not yet published your
  branch, or have clearly communicated
  that others should not base their work
  on it, you have an alternative. You
  can rebase your branch, where instead
  of merging, your commit is replaced by
  another commit with a different
  parent, and your branch is moved
  there.</p>
</blockquote>

<p>while a normal merge would have looked like this:</p>

<p><a href=""https://i.stack.imgur.com/C75pG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/C75pG.png"" alt=""enter image description here""></a></p>

<p>So, if you <strong>rebase</strong>, you are just losing a history state (which would be garbage collected sometime in the future). <strong>So, why would someone want to do a rebase at all?</strong> What am I missing here?</p>",2,1,2010-05-20 17:56:00.927000 UTC,7.0,2019-05-13 02:16:03.473000 UTC,21,algorithm|git|version-control|dvcs,5873,2009-05-27 12:28:44.067000 UTC,2022-03-02 21:59:36.230000 UTC,,84151,2083,117,3472,"<p>There are variety of situations in which you might want to rebase.</p>

<ul>
<li><p>You develop a few parts of a feature on separate branches, then realize they're in reality a linear progression of ideas. Rebase them into that configuration.</p></li>
<li><p>You fork a topic from the wrong place. Maybe it's too early (you need something from later), maybe it's too late (it actually applies to previous versions as well). Move it to the right place. The ""too late"" case actually can't be fixed by a merge, so rebase is critical.</p></li>
<li><p>You want to test the interaction of a branch with another branch, but for some reason don't want to merge. For example, you might want to see what conflicts crop up commit-by-commit, instead of all at once.</p></li>
</ul>

<p>The general theme here is that excessive merging clutters up the history, and rebasing is a way to avoid it if you didn't get your branch/merge plan right at first. Too many merges can make it hard for a human to follow the history, and also can make it harder to use tools like <code>git-bisect</code>.</p>

<p>There are also all the many cases which prompt an interactive rebase:</p>

<ul>
<li><p>Multiple commits should've been one commit.</p></li>
<li><p>A commit (not the current one) should've been multiple commits.</p></li>
<li><p>A commit (not the current one) had a mistake in it or its message.</p></li>
<li><p>A commit (not the current one) should be removed.</p></li>
<li><p>Commits should be reordered (e.g. to flow more logically).</p></li>
</ul>

<p>While it's true that you ""lose history"" doing these things, the reality is that you want to only publish clean work. If something is still unpublished, it's okay to rebase it in order to transform it to the way you <em>should have</em> committed it. This means that the final version in the public repository will be logical and easy to follow, not preserving any of the hiccups a developer had along the way.</p>",4.0,2010-05-20 18:10:35.167000 UTC,,37.0,[]
Undo working copy modifications of one file in Git?,"<p>After the last commit, I modified a bunch of files in my working copy, but I want to undo the changes to one of those files, as in reset it to the same state as the most recent commit.</p>

<p>However, I only want to undo the working copy changes of just that one file alone, nothing else with it.</p>

<p>How do I do that?</p>",14,0,2009-03-28 05:09:02.883000 UTC,406.0,2018-01-11 10:49:47.223000 UTC,1852,git|file|version-control|dvcs|undo,767807,2008-11-07 04:08:19.003000 UTC,2022-03-05 11:19:29.007000 UTC,"Tokyo, Japan",153743,3476,596,7311,"<p>You can use</p>

<pre><code>git checkout -- file
</code></pre>

<p>You can do it without the <code>--</code> (as suggested by nimrodm), but if the filename looks like a branch or tag (or other revision identifier), it may get confused, so using <code>--</code> is best.</p>

<p>You can also check out a particular version of a file:</p>

<pre><code>git checkout v1.2.3 -- file         # tag v1.2.3
git checkout stable -- file         # stable branch
git checkout origin/master -- file  # upstream master
git checkout HEAD -- file           # the version from the most recent commit
git checkout HEAD^ -- file          # the version before the most recent commit
</code></pre>",15.0,2009-03-28 06:12:51.403000 UTC,2014-05-30 02:30:59.827000 UTC,2490.0,[]
What Mercurial DVCS hosting would you recommend for a small open source project?,"<p>I'm looking around for free Mercurial hosting for a small-scale open-source project.<br>
If you've ever used such a service, who is doing the hosting, and would you recommend them?</p>

<p>I know SF.net can be <strong><a href=""http://www.selenic.com/mercurial/wiki/index.cgi/MercurialOnSourceforge"" rel=""nofollow noreferrer"">set up</a></strong> to host HG repos, but it looks like a lot of trouble (for the benefit of having a big, known, service that's unlikely to go down anytime soon).</p>

<p>There's also the <a href=""http://www.selenic.com/mercurial/wiki/index.cgi/MercurialHosting"" rel=""nofollow noreferrer"">list of free HG hosts</a> right in Mercurial's official documentation, but I'd like to hear from those that actually got down and dirty with it :-)</p>",7,0,2009-02-28 10:11:31.657000 UTC,,2009-02-28 17:09:51.747000 UTC,9,mercurial|hosting|dvcs,1228,2008-09-16 09:04:00.697000 UTC,2022-03-03 01:10:51.487000 UTC,"London, UK",32957,1051,24,2036,"<p>[update] <strong><a href=""https://bitbucket.org/blog/sunsetting-mercurial-support-in-bitbucket"" rel=""nofollow noreferrer"">Bitbucket stopped hosting Mercurial</a>.</strong></p>
<p><a href=""http://bitbucket.org/"" rel=""nofollow noreferrer"">BitBucket</a> is certainly the most popular. I've experimented it for while, then I jumped into git.</p>",3.0,2009-02-28 10:21:47.250000 UTC,2020-10-31 18:25:06.107000 UTC,14.0,[]
Is it good to commit files often if using Mercurial or Git?,"<p>It seems that it is suggested we can commit often to keep track of intermediate changes of code we wrote… such as on hginit.com, when using Mercurial or Git.</p>

<p>However, let's say if we work on a project, and we commit files often.  Now for one reason or another, the manager wants part of the feature to go out, so we need to do a push, but I heard that on Mercurial or Git, there is no way to push individual files or a folder… either everything committed gets pushed or nothing get pushed.  So we either have to revert all those files we don't want to push, or we just never should commit until before we push -- right after commit, we push?</p>",4,0,2010-06-10 18:47:32.023000 UTC,,,3,git|mercurial|dvcs,827,2009-05-09 15:50:29.477000 UTC,2022-03-04 09:41:10.460000 UTC,,137341,1445,39,12817,"<p>The best way to manage this (whether you are using Mercurial, Git
or any other revision control system) is to make sure your work
is done on branches which correspond to these ""parts of
features"". If there is even a small chance that some portion of
work will need to be released independently from other work, it
should have its own branch from the start.</p>

<p>This gives you the flexibility to push just the ""part of the
feature"", and is much better suited in the case where the ""part
of feature"" and some other ""part of the feature"" both contain
changes to the same file.</p>

<p>The nice thing about using Mercurial or Git here is that managing
these branches is trivial, so the cost to creating and using
them (even if they turn out not to be necessary) is minimal.</p>

<p>Now, you can't always foresee everything. If you do end up stuck
in the situation you describe, though, it's easy to get out
of. Say you had 100 changesets locally (not yet on the server) and you wanted to just
push the current contents of 1 file. Create a clone of the
repository you're working on back to the server revision, copy
the file over, commit, push, and integrate back. In Mercurial
this would look something like the following:</p>

<pre><code>$ cd ~/project
$ hg clone http://server/project/trunk trunk-oops
$ cp trunk/shouldve-branched trunk-oops/shouldve-branched
$ cd trunk-oops; hg addrem; hg ci -m ""We need to organize better!""; hg push
$ cd ../trunk; hg fetch ../trunk-oops
</code></pre>",4.0,2010-06-10 19:00:34.010000 UTC,,11.0,[]
"Unable to create a new fossil repo, fossil beginner","<p>I figured I would check out fossil for some small apps I have to complete. I am not new to DVCS as I have used CVS, subversion, mercurial, and git. After installing on my Mac (10.7), using homebrew, I am left with the following when attempting to run:</p>

<pre><code>-&gt; % fossil new ../FOSSIL/project.fossil
fossil: SQLITE_CONSTRAINT: abort at 20 in [INSERT INTO user(login,pw,cap,info) VALUES('developer','','dei','Dev');]: column login is not unique
fossil: column login is not unique
INSERT INTO user(login,pw,cap,info) VALUES('anonymous',hex(randomblob(8)),'hmncz','Anon');INSERT INTO user(login,pw,cap,info) VALUES('nobody','','gjor','Nobody');INSERT INTO user(login,pw,cap,info) VALUES('developer','','dei','Dev');INSERT INTO user(login,pw,cap,info) VALUES('reader','','kptw','Reader');

If you have recently updated your fossil executable, you might need to run ""fossil all rebuild"" to bring the repository schemas up to date.
</code></pre>

<p>I have attempted to run <code>fossil init  ../FOSSIL/project.fossil</code> as well as <code>fossil clone <a href=""http://www.fossil-scm.org/"" rel=""nofollow"">http://www.fossil-scm.org/</a> myclone.fossil</code> which yields the same results as seen above.</p>

<p>I then tried <code>fossil user list</code> as it looks like fossil is having issues with logins and users, however fossil expects a repo argument. Searching for the default configuration (something like ~/.gitconfig with git) yielded no results.</p>

<p>I have also tried <code>rm -rf ~/.fossil</code> and re-running the above commands as well as using <code>brew install sqlite</code> to get the updated version (3.7.7 as of this writing), but neither seem to help.</p>

<p>What do I need to do in order to get fossil functioning properly?</p>",1,0,2011-08-03 05:33:02.097000 UTC,,,4,repository|dvcs|fossil,418,2011-03-03 11:36:20.613000 UTC,2012-04-03 04:48:31.327000 UTC,"Springfield, IL",494,28,2,50,"<p>Silly question... is your own username on the system ""developer""? Since that would clash with the name of one of the auto-created users.</p>

<p>Try doing ""fossil init -A admin (filename)"" to use a different name (""admin"" in my example) for the repository owner and see if it works.</p>",1.0,2011-08-23 23:05:42.560000 UTC,,9.0,[]
Any project hosting with a in-browser code editor like Google Code?,"<p><a href=""http://code.google.com/hosting/"" rel=""noreferrer"">Google Code Project Hosting</a> recently adapted CodeMirror to allow for <a href=""http://google-opensource.blogspot.com/2011/01/make-quick-fixes-quicker-on-google.html"" rel=""noreferrer"">quick in-browser editing of code in a repository</a>. This seems extremely useful! Unfortunately not all of my projects are open source.</p>

<p>Are there any other DVCS (Mercurial or Git) hosts that also have this feature but support closed-source projects? In particular, free would be nice (since I'd like to host personal projects with it and be able to tweak them in the browser); I'm a student, so if there's a discount or free mode due to that, that's cool too!</p>",5,2,2011-02-22 17:35:29.023000 UTC,1.0,2011-02-22 20:16:45.560000 UTC,11,git|mercurial|hosting|dvcs,2842,2008-12-18 17:55:48.877000 UTC,2022-02-11 00:35:24.777000 UTC,"Saint Louis, MO, United States",31972,505,121,2488,"<p>GitHub can do that. There is a “edit this file” button next to each code viewer.</p>

<p>See also the “Edit Online” section on <a href=""https://github.com/features/hosting"" rel=""nofollow noreferrer"">Features / Hosting</a>.</p>

<h3>Update (May 2013)</h3>

<p>Bitbucket <a href=""http://blog.bitbucket.org/2013/05/14/edit-your-code-in-the-cloud-with-bitbucket/"" rel=""nofollow noreferrer"">recently introduced</a> a online code editor, just like the one GitHub provides. As Bitbucket also offers free private repositories (up to 5 collaborators) this is definitely a good alternative to GitHub.</p>",3.0,2011-02-22 17:42:21.377000 UTC,2013-05-17 12:03:12.767000 UTC,13.0,[]
Mercurial - cannot commit merge with missing files error,"<p>I have done a 'hg merge' however when I attempt to do a 'hg commit -m ""my msg.."" I get the following error message :</p>

<pre><code>abort: cannot commit merge with missing files
</code></pre>

<p>Can anyone explain how to fix this to allow the commit go through?</p>",3,0,2012-08-22 09:33:57.333000 UTC,2.0,,35,mercurial|dvcs,17086,2009-11-25 15:42:47.513000 UTC,2022-03-05 18:36:38.047000 UTC,,13182,929,4,1278,"<p>Try <code>hg status</code> and look for files in state <code>!</code> (missing).</p>

<p>The cause is that one of the files which is part of the merge has been deleted. Undelete the file and try again.</p>",2.0,2012-08-22 10:19:13.800000 UTC,2012-09-21 12:28:37.273000 UTC,56.0,[]
Show branches that do not contain commit,"<p><code>git branch -a --contains &lt;hash&gt;</code> gives me all those branches containing <code>hash</code>. what I want is <code>git branch -a --no-contains &lt;hash&gt;</code>. Unfortunately, there doesn't seem to be a command to accomplish this, so I'm thinking the solution is something like:</p>

<p><code>git branch -a | grep -v output of(git branch -a --contains)</code> but my bash isn't up to the task.</p>

<p><a href=""https://stackoverflow.com/questions/10187302/show-all-branches-that-commit-a-is-on-and-commit-b-is-not-on"">Show all branches that commit A is on and commit B is not on?</a> would seem to apply, but the approach seems more complicated than necessary.</p>

<p>What is the best/most simple approach to accomplish the above?</p>",3,1,2012-05-22 17:55:57.953000 UTC,4.0,2017-05-23 12:10:36.343000 UTC,9,git|branch|dvcs,1532,2011-04-15 14:05:59.043000 UTC,2019-12-21 02:49:19.683000 UTC,,2440,63,3,136,"<p><code>grep</code> has a <code>-F</code> option which matches fixed strings. Would be useful for what you're doing.</p>

<pre><code>git branch -a | grep -vF ""$(git branch -a --contains &lt;hash&gt;)""
</code></pre>

<p>Unfortunately, <code>-F</code> will filter out branches names that have a partial match. As suggested by antak, we can use <code>comm</code> instead for a more reliable diff.</p>

<pre><code>git branch -a | sort | comm -3 - &lt;(git branch -a --contains &lt;hash&gt; | sort)
</code></pre>",2.0,2012-05-22 19:17:56.413000 UTC,2015-12-01 20:47:15.580000 UTC,12.0,[]
Is 'pull' a synonym for 'clone' in a Mercurial source-control repository?,<p>I'm seeing the command 'pull' and wondering how that's different from a 'clone'. Both terms seem to imply retrieving code from some remote repository. Is there some subtle distinction here?</p>,3,3,2009-09-09 05:13:23.983000 UTC,3.0,2012-03-10 11:31:35.093000 UTC,6,version-control|mercurial|clone|dvcs|pull,3086,2009-09-09 03:05:42.310000 UTC,2009-09-16 15:52:43.143000 UTC,,1039,2,0,47,"<p><code>hg clone</code> is how you make a local copy of a remote repository.  The Subversion equivalent is <code>svn checkout</code>.</p>

<p><code>hg pull</code> pulls changes from another repository.  <code>hg update</code> applies those changes to the local repository.  <code>hg pull -u</code> is equivalent to <code>hg pull; hg update</code>.  The Subversion equivalent to <code>hg pull -u</code> is <code>svn update</code>.</p>",0.0,2009-09-09 05:47:43.703000 UTC,,11.0,[]
Connect to Neptune on AWS from local machine,"<p>I am trying to connect to Neptune DB in AWS Instance from my local machine in office,like connecting RDS from office.Is it possible to connect Neptune db from local machine?. Is Neptune db publicly available.Is there any way  developer can connect neptune db from office.</p>",4,0,2018-09-30 07:25:08.610000 UTC,4.0,2018-11-07 06:39:16.677000 UTC,19,amazon-web-services|amazon-neptune,9630,2015-09-29 06:07:44.427000 UTC,2021-04-07 12:08:41.040000 UTC,"Ernakulam, Kerala, India",447,33,1,108,"<p>Neptune does not support public endpoints (endpoints that are accessible from outside the VPC). However, there are few architectural options using which you can access your Neptune instance outside your VPC. All of them have the same theme: setup a proxy (EC2 machine, or ALB, or something similar, or a combination of these) that resides inside your VPC, and make that proxy accessible from outside your VPC. </p>

<p>It seems like you want to talk to your instance purely for development purposes. The easiest option for that would be to spin up an ALB, and create a target group that points to your instance's IP. </p>

<p>Brief Steps (These are intentionally not in detail, please refer to AWS Docs for detailed instructions):</p>

<ol>
<li><p><code>dig +short &lt;your cluster endpoint&gt;</code>
This would give you the current master's IP address.</p></li>
<li><p>Create an ALB (See AWS Docs on how to do this). </p></li>
<li>Make your ALB's target group point to the IP Address obtained for step #1. By the end of this step, you should have an ALB listening on <code>PORT-A</code>, that would forward requests to <code>IP:PORT</code>, where <code>IP</code> is your database IP (from Step 1) and <code>PORT</code> is your database port (default is <code>8182</code>). </li>
<li>Create a security group that allows inbound traffic from everywhere. i.e. Inbound TCP rule for <code>0.0.0.0</code> on <code>PORT-A</code>.</li>
<li>Attach the security group to your ALB</li>
</ol>

<p>Now from your developer boxes, you can connect to your ALB endpoint at PORT-A, which would internally forward the request to your Neptune instance.</p>

<p>Do checkout ALB docs for details around how you can create it and the concepts around it. If you need me to elaborate any of the steps, feel free to ask.</p>

<p>NOTE: This is not a recommended solution for a production setup. IP's used by Neptune instances are bound to change with failovers and host replacements. Use this solution only for testing purposes. If you want a similar setup for production, feel free to ask a question and we can discuss options.</p>",7.0,2018-10-03 07:44:59.540000 UTC,,17.0,[]
Get tip changeset of remote Mercurial repository,"<p>My .hg/hgrc file has the line:</p>

<p><code>default = http://some/remote/repository</code></p>

<p>Is there a quick command to print the tip revision of that repository (which may or may not be inside my local repository)?</p>",3,0,2012-01-19 17:56:02.233000 UTC,,2012-02-03 21:35:14.157000 UTC,7,mercurial|dvcs,1916,2010-11-12 02:35:45.083000 UTC,2022-02-05 20:08:19.120000 UTC,"Auckland, New Zealand",4035,33,8,190,"<p>You can use the <a href=""http://www.selenic.com/mercurial/hg.1.html#identify"">identify command</a> like this:</p>

<pre><code>$ hg identify $(hg paths default)
</code></pre>

<p>This is one of the few commands that can operate on a remote repository. If you need more information about the remote repository, then I suggest you take a look at <code>hg incoming</code>.</p>",1.0,2012-01-19 18:02:19.563000 UTC,,11.0,[]
How to move bugfixes across branches in DVCS?,"<p>If we discover bug in some branche we fix it (check <em>New</em> release on picture). But also we interesting in moving this fix to <em>Old</em> release and <em>Main</em> development branch:</p>

<pre>
a-->b-->c              (Old release)
|    
A-->B-->C-->D           (Main release)
    |
    1-->2-->bugfix-->4  (New release)
</pre>

<p><strong>svn</strong> remember in svn:merge properties (from svn 1.6, 2009) which revision merged to which branches. So next time if you merge region of revisions previously merged revisions was skipped from merge patch.</p>

<p>How deal with modern DVCS?</p>

<p>I need make plain patch and apply it to each branches or there are exist some help from DVCS?</p>

<p><strong>Note</strong>: I can not just merge <em>New</em> branche to <em>Main</em> as previous changesets move also to <em>Main</em> branche.</p>

<p>Rebase also not possible as <em>New</em> release come to many developers.</p>

<p>I interesting in answer for named braches schema and for multirepository schema.</p>

<p><strong>PS.</strong> <em>Andy</em> suggest find common parent to all branches for which bug have affect, update to it, apply fix to the bug and move fix to affected branches.</p>

<p>By updating to old changeset and making changes you create new branch. I recommend create named branch (name it as <strong>bugID</strong>), so later you can easy back to it.</p>

<p>There are problem on finding common parent to all branches in which we have interest to fix bug.</p>

<p>First solution (that suggest <em>Andy</em>) is using <strong>$ hg/git/bzr blame</strong> and carefully check output for all affected files. This involve first fix bug on <em>some newest</em> changeset before you find with <strong>blame</strong> what changeset introduce a bug. Then you need <strong>rebase</strong> fix (patch) to common parent changeset.</p>

<p>Another solution is using <strong>$ hg/git/bzr bisect</strong> (you can also manually perform updates to find first revision in which bug introduced). This can be expansive but more <em>true</em> solution as allow populate bugfix to <strong>any</strong> branches in which bug present.</p>

<p>I think it is better first find <em>first BAD</em> changeset and then fix a bug, instead of first fix a bug and then find <em>first BAD</em> changeset (except case when you already know how fix bug). Also having a diff that introduce can help in understanding why it occur.</p>

<p><strong>PPS.</strong> With bugs it is clear which branch effected to allow merge changes to any effected branch.</p>

<p>Interesting question come if ask how backport feature from development branch to release branch. As you can see you must commit feature changesets starting with changeset that before release branch. But when you develop feature you can don't know where you need backport feature. With <strong>svn:merge</strong> VCS remember for you all backports. How about DVCS?</p>",3,5,2011-07-15 15:23:00.307000 UTC,1.0,2011-07-15 17:18:57.990000 UTC,8,git|mercurial|branch|dvcs|bazaar,492,2009-09-14 13:42:30.747000 UTC,2022-02-28 11:34:20.310000 UTC,"Dnipro, Ukraine",40399,19176,27,5468,"<p>You could:</p>

<ol>
<li><p>Find common parent. <em>Thanks to Novelocrat for this step.</em><br>
<code>git merge-base branch1 branch2 branch3 ...</code></p></li>
<li><p>Checkout the common parent<br>
<code>git checkout A</code></p></li>
<li><p>create new branch to fix bugs on<br>
<code>git checkout -b bugFix</code> </p></li>
<li><p>Make bug fixes and commit them to bugFix branch</p></li>
<li><p>Checkout branch that needs the bug fix<br>
<code>git checkout branchToApplyFixTo</code></p></li>
<li><p>Merge the bugFix into the branch<br>
<code>git merge bugFix</code></p></li>
</ol>",4.0,2011-07-15 15:34:18.270000 UTC,2011-07-16 15:20:43.797000 UTC,9.0,[]
Seeing what revision goes with a tag in Git,"<p>Beginner <a href=""http://en.wikipedia.org/wiki/Git_%28software%29"" rel=""nofollow noreferrer"">Git</a> question:</p>

<p>In the <a href=""http://en.wikipedia.org/wiki/Mercurial"" rel=""nofollow noreferrer"">Mercurial</a> world, <code>hg tags</code> gives me a list of tags and the corresponding revisions, whereas <code>git tag</code> only lists the tag names. How do I see what the matching rev number/hash is?</p>",2,0,2009-07-28 14:14:47.473000 UTC,,2014-03-10 06:22:23.263000 UTC,14,git|version-control|dvcs|git-tag,8388,2008-10-10 13:35:04.687000 UTC,2022-02-22 17:18:34.010000 UTC,,2792,481,17,220,"<p>For full information associated with that tag, use command</p>

<pre><code>git show v1.5
</code></pre>

<p>Or you can see the lightweight information, skipping details, by command</p>

<pre><code>git show v1.5 -lw
</code></pre>",2.0,2009-07-28 14:18:11.023000 UTC,2012-03-13 17:29:31.057000 UTC,21.0,[]
"Gremlin, javascript: where is the function ""valueMap()"" imported from?","<p>I am using es6 on nodejs, and am trying to execute the project() step in a gremlin query.</p>

<p>As part of the projection, I want to extract the properties.</p>

<p>Using gremlin console I would use valueMap() to get the properties.</p>

<p>However, when I attempt this from javascript, I get the expected error ""valueMap is not a function"".  </p>

<p>Question 1: where do I import this function from?</p>

<p>Question 2: where can I read about all of the various gremlin objects available for importing in javascript?</p>

<p>===========================</p>

<p>I am using Gremlin 3.3 connecting to an AWS Neptune instance.</p>

<p>Here is my gremlin code:</p>

<pre><code>g.V('test-id')  
    .bothE()  
    .limit(10)  
    .project('id', 'properties', 'out', 'in')  
    .by(id)  
    .by(valueMap())  
    .by(outV().id())  
    .by(inV().id())  
</code></pre>",1,3,2018-10-12 13:30:07.673000 UTC,,2018-12-17 23:30:04.937000 UTC,5,javascript|node.js|gremlin|amazon-neptune,1121,2016-09-29 13:51:20.553000 UTC,2022-03-05 13:29:43.827000 UTC,United States,1128,1481,1,73,"<p><code>valueMap()</code>, <code>outV()</code> and similar traversals are spawned <a href=""http://tinkerpop.apache.org/docs/current/tutorials/gremlins-anatomy/#_anonymous_traversals"" rel=""noreferrer"">anonymously</a> from a <a href=""http://tinkerpop.apache.org/docs/current/reference/#_static_enums_and_methods_3"" rel=""noreferrer"">double underscore class</a> - <code>__</code> - so your code could be re-written as:</p>

<pre><code>const gremlin = require('gremlin');
const __ = gremlin.process.statics;

g.V('test-id')  
    .bothE()  
    .limit(10)  
    .project('id', 'properties', 'out', 'in')  
    .by(id)  
    .by(__.valueMap())  
    .by(__.outV().id())  
    .by(__.inV().id()) 
</code></pre>",0.0,2018-10-12 15:23:43.273000 UTC,,9.0,[]
How do I prevent Mercurial patches from being pulled?,"<p>So far I haven't been able to find a clear answer, though it's possible that the answer is ""change your workflow"".</p>

<p>I've just started playing around with Mercurial's patch queue and I can see some serious power in it. It seems pretty awesome. In my tests, I've discovered that if you have a patch queue in <code>repo1</code>, and you pull from <code>repo2</code>, you can do some bad things. For example:</p>

<ol>
<li>Create repos 1, and clone it.</li>
<li>Enable the queue on <code>repo1</code></li>
<li>Make some commits and some patches on <code>repo1</code></li>
<li>Pull changes to <code>repo2</code></li>
<li>On <code>repo1</code> un-apply(pop?) all your patches</li>
<li>Pull changes to <code>repo2</code></li>
</ol>

<p>Now you'll see two different branches - which makes sense from a certain viewpoint. However, since my patches aren't a part of <code>repo1</code>'s history (at least until they're applied), it seems like there should be a way to tell mercurial that my patches are off-limits, and only provide what's in the ""official history"".</p>

<p>Is there a way to do this?</p>",1,2,2012-06-06 16:52:38.773000 UTC,,2012-06-07 21:53:40.387000 UTC,4,version-control|mercurial|dvcs|patch|mercurial-queue,208,2010-05-01 16:35:26.073000 UTC,2022-03-04 02:11:31.000000 UTC,"Greenwood, AR",44718,2079,37,4751,"<p>Mercurial <a href=""https://www.mercurial-scm.org/repo/hg/help/phases"" rel=""nofollow noreferrer"">phases</a> may be the answer to this.</p>

<p>Starting with Mercurial v2.1, you can configure <code>mq</code> changesets to automatically be marked <code>secret</code>.  <code>secret</code> changesets are ignored by <code>incoming/pull</code> and <code>outgoing/push</code> commands.</p>

<p>To enable this behavior, you need to add the following to your config:</p>

<pre><code>[mq]
secret = True
</code></pre>

<p>Once enabled, it behaves as follows:</p>

<pre><code>$ hg qpush --all
applying my-patch
now at: my-patch

$ hg phase -r .
16873: secret

$hg outgoing
comparing with https://www.mercurial-scm.org/repo/hg
searching for changes
no changes found (ignored 1 secret changesets)
</code></pre>",1.0,2012-06-06 17:58:46.500000 UTC,2017-06-19 12:42:25.853000 UTC,8.0,[]
Does git have an equivalent for the bazaar automv plugin?,<p>The only distributed revision control system I've used on my projects is bazaar. I don't know much about git and mercurial except how to do checkouts. The reason I like bazaar is the automv plugin. It detects when I've moved/deleted files manualy (from command line/ide etc.) which I tend to do alot when I'm in a hurry. But bazaar is really slow and I'm thinking of moving to git. Does git have something similar to this functionality? </p>,3,1,2008-12-23 07:28:00.977000 UTC,0.0,2009-02-27 21:00:31.730000 UTC,2,git|dvcs|bazaar,350,2008-09-15 14:37:57.347000 UTC,2022-03-04 17:20:48.087000 UTC,"Skopje, Macedonia",34248,752,25,1864,"<p>The way Git works in the move/rename situation is quite different. Git tracks only content, so it knows that file X had SHA1 hash <code>abc123...</code> at one commit, and file Y happened to have the same hash <code>abc123...</code> at the next commit. So the Git tools, when viewing history or whatever, conclude that file X must have been renamed to filename Y at that point.</p>

<p>In this way, Git reconstructs the actions taken to move from one commit to the next, without having to have that knowledge at commit time. It can event detect situations where you rename a file <em>and</em> change something within the file (of course it doesn't do this by comparing SHA1 hashes, but by doing a text compare between files that were changed in a commit).</p>",5.0,2008-12-23 07:47:08.287000 UTC,,8.0,[]
Should checkins be small steps or complete features?,"<p>Two uses of version control seem to dictate different checkin styles.</p>

<ul>
<li><p><strong>distribution centric</strong>: changesets will generally reflect a complete feature.  In general these checkins will be larger.  This style is more user/maintainer friendly.</p></li>
<li><p><strong>rollback centric</strong>: changesets will be individual small steps so the history can function like an incredibly powerful undo.  In general these checkins will be smaller.  This style is more developer friendly.</p></li>
</ul>

<p>I like to use my version control as really powerful undo while while I banging away at some stubborn code/bug.  In this way I'm not afraid to make drastic changes just to try out a possible solution.  However, this seems to give me a fragmented file history with lots of ""well that didn't work"" checkins.</p>

<p>If instead I try to have my changeset reflect complete features I loose the use of my version control software for experimentation.  However, it is much easier for user/maintainers to figure out how the code is evolving.  Which has great advantages for code reviews, managing multiple branches, etc.</p>

<p>So what's a developer to do? Checkin small steps or complete features?</p>",7,2,2010-06-14 22:41:08.847000 UTC,14.0,2010-06-14 22:58:04.797000 UTC,26,git|mercurial|dvcs|bazaar,1062,2008-10-17 04:26:41.420000 UTC,2022-02-10 02:14:02.977000 UTC,"Woodinville, WA",53865,3531,277,4724,"<p>The beauty of DVCS systems is that you can have <em>both</em>, because in a DVCS unlike a CVCS, <em>publishing</em> is orthogonal to <em>committing</em>. In a CVCS, every commit is automatically published, but it in a DVCS, commits are only published when they are <em>pushed</em>.</p>

<p>So, <em>commit</em> small steps, but only <em>publish</em> working features.</p>

<p>If you are worried about polluting your history, then you can rewrite it. You might have heard that rewriting history is evil, but that is not true: only rewriting <em>published</em> history is evil, but again, since publishing and committing are different, you can rewrite your unpublished history before publishing it.</p>

<p>This is how Linux development works, for example. Linus Torvalds is very concerned with keeping the history clean. In one of the very early e-mails about Git, he said that the published history should look not like you <em>actually</em> developed it, but how you <em>would have</em> developed it, if you were omniscient, could see into the future and never made any mistakes.</p>

<p>Now, Linux is a little bit special: it has commits going in at a rate of 1 commit every 11 minutes for 24 hours a day, 7 days a week, 365 days a year, including nights, weekends, holidays and natural disasters. And that rate is still increasing. Just imagine how much more commits there would be if every single typo and brainfart would result in a commit, too.</p>

<p>But the developers themselves in their private repositories commit however often they want.</p>",1.0,2010-06-14 23:09:55.800000 UTC,,20.0,[]
Bazaar (bzr) predefined locations,"<p>Bazaar has a Launchpad pseudo-protocol (lp:) that able the user to operate in remote branchs without write full Launchpad location, I'm searching a way to create my own pseudo-protocols in a way like this (similar to GIT):</p>

<pre><code>bzr remote my sftp://myuser@myserver.com/home/myuser/myrepo/
bzr push my:mybranch
bzr push my:otherbranch
bzr push my:anotherbranch
</code></pre>

<p>...instead of...</p>

<pre><code>bzr push sftp://myuser@myserver.com/home/myuser/myrepo/mybranch
bzr push sftp://myuser@myserver.com/home/myuser/myrepo/otherbranch
bzr push sftp://myuser@myserver.com/home/myuser/myrepo/anotherbranch
</code></pre>

<p>...that is a pain in the ass.</p>

<p>I readed about bazaar.conf but it seems only able you to set a globals push locations for each branch/repo. Another way I saw is to use alias, but has the same problem, is very ugly when is needed to create new branches.</p>

<p>Is posible to do this without modify the BZR code? This should be obvious...</p>",2,0,2009-12-21 01:00:57.047000 UTC,2.0,2010-03-29 03:31:31.983000 UTC,5,version-control|dvcs|bazaar,262,2009-12-21 00:32:51.310000 UTC,2020-07-09 17:12:31.300000 UTC,"Paron, France",5363,32,7,108,"<p>The my:xxx syntax called <code>directory services</code> in bzr. You can define your own directory service via plugin (write in Python).</p>

<p>Good example of such plugins is <a href=""http://doc.bazaar.canonical.com/plugins/en/bookmarks-plugin.html"" rel=""nofollow noreferrer"">bookmark plugin</a>. Check it.</p>

<p>Maybe this plugin itself will be enough for your needs.</p>",1.0,2009-12-21 04:59:06.097000 UTC,2010-09-01 04:17:33.353000 UTC,8.0,[]
"Why does Mercurial need to talk to the server, to list outgoing (non-pushed) commits?","<p>When I type <code>hg outgoing</code>, I get a response like this:</p>

<pre><code>comparing with ssh://server:1234/path/to/repo
</code></pre>

<p>and a delay while it communicates over the network.</p>

<p>Why is this network traffic necessary? Is there something fundamental about Mercurial which means it can't remember which commits have been pushed, and which haven't?</p>

<p>Is there an alternative command which can give me similar information without having to communicate over the network?</p>",2,1,2012-03-12 17:11:33.653000 UTC,,2012-03-12 22:47:43.700000 UTC,9,mercurial|dvcs,194,2010-07-07 11:39:54.190000 UTC,2022-03-04 15:46:31.253000 UTC,,58695,1277,392,4554,"<p>As Mercurial is a distributed system, there are multiple ways for your changes to get from your local repo to the remote repo.  </p>

<p>For example:</p>

<ul>
<li>it is possible for someone to pull changes from you and then push those changes to the remote repo</li>
<li>you could actually just copy your local repo using whatever operating system you have and Mercurial would be totally unaware of that.  You could then push the changes in this copy to the remote repo.</li>
</ul>

<p>However, if you have Mercurial 2.1 or later you can use <code>hg phase</code> to determine which changesets have been pushed. Assuming you don't use <code>hg phase</code> to change the status of any changesets then the changesets with a phase of <code>draft</code> or <code>secret</code> have not been pushed and those with a phase of <code>public</code> have. Use</p>

<pre><code>$ hg log -r ""not public()""
</code></pre>

<p>to see unpublished changesets.</p>

<p>It won't catch the two examples I gave above but it will probably be good enough if you just want to know which changesets you have not pushed.</p>

<p>Look <a href=""http://www.selenic.com/mercurial/hg.1.html#phases"">here</a> or check <code>hg help phases</code> for instructions on how to work with phases.</p>",0.0,2012-03-12 17:40:30.010000 UTC,2012-03-12 20:20:12.327000 UTC,14.0,[]
Can I clone part of a Mercurial repository?,"<p>Is it possible to clone part of a Mercurial repository? Let's say the repository is quite large, or contains multiple projects, or multiple branches. Can I clone only part of the repository?</p>

<p>E.g. in Subversion, you might have <code>trunk</code> and <code>branches</code>. If I only want to get trunk (or one of the branches) I can just request <code>[project]/trunk</code>. If I clone the hg repo I'll get trunk and all of the branches. This might be a lot of information I don't want. Can I avoid getting this?</p>

<p>Alternatively, if I want to have multiple projects in one hg repo, how should I do this? I.e. so that I might just get one of the projects and ignore the others.</p>",8,2,2009-11-16 21:34:33.500000 UTC,11.0,2012-01-20 08:31:22.290000 UTC,45,mercurial|dvcs,18082,2008-08-27 13:54:44.423000 UTC,2022-03-05 18:51:59.960000 UTC,"London, United Kingdom",26658,785,217,1132,"<p>Yes you can.  I'm sure you've moved on, but for the sake of those who will wander here later, I followed the docs at <a href=""https://www.mercurial-scm.org/wiki/ConvertExtension"" rel=""nofollow noreferrer"">ConvertExtension</a>, and wrote a simple batch script:</p>

<pre><code>@echo off
echo Converting %1
REM Create the file map
echo include %1 &gt; ~myfilemap               
echo rename %1 . &gt;&gt; ~myfilemap 
REM Run the convert process
hg convert --filemap ~myfilemap .\ ..\%1   
REM Delete the file map
del ~myfilemap                             
cd ..\%1
REM update the new repo--to create the files
hg update                                  
</code></pre>

<p>Name it something like <code>split.cmd</code>, and put it in the directory for the repo you want to split.  Say for example you have <code>C:\repos\ReallyBigProject</code>, and a subfolder is <code>C:\repos\ReallyBigProject\small-project</code>.  At the command prompt, run:</p>

<pre><code>cd\repos\ReallyBigProject
split.cmd small-project
</code></pre>

<p>This will create <code>C:\repos\small-project</code> with a slice of the relevant history of revisions from the larger project.</p>

<p>The <code>convert</code> is not enabled by default.  You'll need to make sure the following lines exist in your <code>.hg\hgrc</code> file (<code>c:\repos\ReallyBigProject\.hg\hgrc</code> in my example):</p>

<pre><code>[extensions]
hgext.convert=
</code></pre>",3.0,2011-02-14 20:13:26.563000 UTC,2017-05-31 11:39:12.000000 UTC,37.0,[]
"Is there a difference between `bzr clone`, `bzr branch` and `bzr checkout`?","<p>Obviously <code>bzr clone</code>, <code>bzr branch</code> and <code>bzr checkout</code> all do the same thing when given an URL as parameter and executed in a non-bzr directory.</p>

<p>Is there any difference for later Bazaar workflow? i.e. <code>bzr commit</code>, <code>bzr update</code> and friends.</p>",3,1,2010-06-05 15:44:53.883000 UTC,1.0,2011-09-27 20:20:43.217000 UTC,10,version-control|dvcs|bazaar,5250,2009-05-27 07:53:46.773000 UTC,2022-03-05 19:20:55.647000 UTC,Austria,212209,3080,544,7572,"<p>Branching and cloning are the same, but branching and checkouts are <strong>not</strong> the same.</p>

<pre>
$ bzr help branch
...
Aliases:  get, clone
See also: checkout
</pre>

<p>Looking over the <a href=""http://wiki.bazaar.canonical.com/CheckoutTutorial"" rel=""noreferrer"">Checkout Tutorial</a>, you'll see that a checkout essentially binds every action you take directly to the branch.  So anything you do is essentially <code>push</code>ed when you do it -- obviously that's a huge workflow difference.</p>

<blockquote>
  <h2>Difference between a Branch and a Checkout</h2>
  
  <p>Let's start by saying there is nothing
  you can do with a Checkout that you
  can't do with plain Branches. A
  Checkout just enables different
  defaults and workflow helpers. </p>
  
  <h2>What does Checkout do</h2>
  
  <p>With a Checkout, whenever you create
  new entries in a local Branch, it also
  creates them in a remote Branch. This
  corresponds to commands like <code>bzr
  commit</code> and <code>bzr pull</code>. If you attempt to
  commit a new changes, and the remote
  Branch has a different state than the
  local one, it will prevent you, and
  let you know that you are out of date.
  You can use <code>bzr update</code> to apply the
  remote changes locally.</p>
</blockquote>",4.0,2010-06-05 15:50:21.993000 UTC,2010-06-05 15:57:01.180000 UTC,14.0,[]
www.bitbucket.org certificate with fingerprint not verified issue while pushing changes,"<p>I am trying to push changes to a project I am contributing to. I have permissions to commit to the project and has a valid account in bitbucket. I am getting the below error while pushing the changes. Appreciate help on fixing this.</p>

<p><img src=""https://i.stack.imgur.com/XO71X.png"" alt=""enter image description here""></p>",2,1,2011-12-29 19:58:24.227000 UTC,1.0,2011-12-29 20:08:40.870000 UTC,10,ssl|mercurial|ssl-certificate|dvcs|bitbucket,3783,2009-06-23 03:11:55.290000 UTC,2022-03-03 15:33:17.520000 UTC,"Cumming, GA",76193,2696,43,6231,"<p>It started working after adding the below to repo specific hgrc file</p>

<pre><code>[hostfingerprints]
bitbucket.org = 24:9c:45:8b:9c:aa:ba:55:4e:01:6d:58:ff:e4:28:7d:2a:14:ae:3b
</code></pre>

<p>based on the suggestion <a href=""https://bitbucket.org/site/master/issue/2780/getting-warning-while-using-https-and-ssh"" rel=""nofollow noreferrer"">here</a></p>

<p><a href=""http://twitter.com/#!/bitbucket/status/160066801319948288"" rel=""nofollow noreferrer"">certificate was changed on jan 19, 2012</a></p>

<p><a href=""http://blog.bitbucket.org/2014/03/06/ssl-certificates-are-changing/"" rel=""nofollow noreferrer"">Bitbucket’s SSL certificates are changing on March 6, 2014</a></p>",0.0,2011-12-30 01:22:53.930000 UTC,2014-03-08 05:35:45.703000 UTC,18.0,[]
How to make sure my git repo code is safe?,"<p>If our organisation were to switch from a central-server VCS like subversion to a distributed VCS like git, how do I make sure that all my code is safe from hardware failure?</p>

<p>With a central-server VCS I just need to backup the repository every day. If we were using a DVCS then there'd be loads of code branches on all the developer machines, and if that hardware were to fail (or a dev were to lose his laptop or have it stolen) then we wouldn't have any backups.</p>

<p>Note that I don't consider it a good option to ""make the developers push branches to a server"" -- that's <a href=""http://www.mattblodgett.com/2008/02/matt-blodgett-first-law-of-software.html"" rel=""nofollow noreferrer"">tedious</a> and the developers will end up not doing it.</p>

<p>Is there a common way around this problem?</p>

<p><strong>Some clarification:</strong></p>

<p>With a natively-central-server VCS then <em>everything</em> has to be on the central server except the developer's most recent changes. So, for example, if a developer decides to branch to do a bugfix, that branch is on the central server and available for backup immediately.</p>

<p>If we're using a DVCS then the developer can do a local branch (and in fact many local branches). None of those branches are on the central server and available for backup until the developer thinks, ""oh yeah, I should push that to the central server"".</p>

<p>So the difference I'm seeing (correct me if I'm wrong!): Half-implemented features and bugfixes will probably not available for backup on the central server if we're using a DVCS, but are with a normal VCS. How do I keep that code safe?</p>",7,0,2008-09-21 05:25:29.767000 UTC,3.0,2008-09-21 06:20:20.617000 UTC,7,git|backup|dvcs,2200,2008-09-15 09:32:34.613000 UTC,2022-02-20 08:54:28.413000 UTC,Australia,13883,345,38,981,"<p>I think that you will find that in practice developers will prefer to use a central repository than pushing and pulling between each other's local repositories. Once you've cloned a central repository, while working on any tracking branches, fetching and pushing are trivial commands. Adding half a dozen remotes to all your colleagues' local repositories is a pain and these repositories may not always be accessible (switched off, on a laptop taken home, etc.).</p>

<p>At some point, if you are all working on the same project, all the work needs to be integrated. This means that you need an integration branch where all the changes come together. This naturally needs to be somewhere accessible by all the developers, it doesn't belong, for example, on the lead developer's laptop.</p>

<p>Once you've set up a central repository you can use a cvs/svn style workflow to check in and update. cvs update becomes git fetch and rebase if you have local changes or just git pull if you don't. cvs commit becomes git commit and git push.</p>

<p>With this setup you are in a similar position with your fully centralized VCS system. Once developers submit their changes (git push), which they need to do to be visible to the rest of the team, they are on the central server and will be backed up.</p>

<p>What takes discipline in both cases is preventing developers keeping long running changes out of the central repository. Most of us have probably worked in a situation where one developer is working on feature 'x' which needs a fundamental change in some core code. The change will cause everyone else to need to completely rebuild but the feature isn't ready for the main stream yet so he just keeps it checked out until a suitable point in time.</p>

<p>The situation is very similar in both situations although there are some practical differences. Using git, because you get to perform local commits and can manage local history, the need to push to the central repository may not be felt as much by the individual developer as with something like cvs.</p>

<p>On the other hand, the use of local commits can be used as an advantage. Pushing all local commits to a safe place on the central repository should not be very difficult. Local branches can be stored in a developer specific tag namespace.</p>

<p>For example, for Joe Bloggs, An alias could be made in his local repository to perform something like the following in response to (e.g.) <code>git mybackup</code>.</p>

<pre><code>git push origin +refs/heads/*:refs/jbloggs/*
</code></pre>

<p>This is a single command that can be used at any point (such as the end of the day) to make sure that all his local changes are safely backed up.</p>

<p>This helps with all sorts of disasters. Joe's machine blows up and he can use another machine and fetch is saved commits and carry on from where he left off. Joe's ill? Fred can fetch Joe's branches to grab that 'must have' fix that he made yesterday but didn't have a chance to test against master.</p>

<p>To go back to the original question. Does there need to be a difference between dVCS and centralized VCS? You say that half-implemented features and bugfixes will not end up on the central repository in the dVCS case but I would contend that there need be no difference.</p>

<p>I have seen many cases where a half-implemented feature stays on one developers working box when using centralized VCS. It either takes a policy that allows half written features to be checked in to the main stream or a decision has to be made to create a central branch.</p>

<p>In the dVCS the same thing can happen, but the same decision should be made. If there is important but incomplete work, it needs to be saved centrally. The advantage of git is that creating this central branch is almost trivial.</p>",0.0,2008-09-21 09:21:16.097000 UTC,,12.0,[]
Mercurial Bookmarks and 'Git like branching',"<p>I am not having any luck using Bookmarks in Mercurial for Git like branching.</p>

<p>From the article: <a href=""https://www.mercurial-scm.org/wiki/BookmarksExtension"" rel=""nofollow noreferrer"">https://www.mercurial-scm.org/wiki/BookmarksExtension</a>, I've set ""track.current"" to true in my .hgrc file. </p>

<p>Excerpt below:</p>

<blockquote>
  <p>By default, when several bookmarks
  point to the same changeset, they will
  all move forward together. It is
  possible to obtain a more Git-like
  experience by adding the following
  configuration option to your .hgrc</p>

<pre><code>[bookmarks]
track.current = True
</code></pre>
</blockquote>

<p>However, as soon as I start trying to do parallel / independent development on more than one bookmark, then switch back and forth between the bookmarks, I run into the following:</p>

<pre><code>abort: crosses branches (use 'hg merge' or 'hg update -C')
</code></pre>

<p>Example to reproduce:</p>

<pre><code># Make a new directory and Mercurial repository
$ mkdir bookmark
$ cd bookmark
$ hg init

# Create two bookmarks
$ hg bookmark bk1
$ hg bookmark bk2

# Checkout bk1
$ hg update bk1
0 files updated, 0 files merged, 0 files removed, 0 files unresolved

# Create and commit a file to bk1
$ touch bk1.txt
$ hg add
adding bk1.txt
$ hg commit -m ""bk1 file""

# Checkout bk2
$ hg update bk2
0 files updated, 0 files merged, 1 files removed, 0 files unresolved

# Create and commit a file to bk2
$ touch bk2.txt
$ hg add
adding bk2.txt
$ hg commit -m ""bk2 file""
created new head

# Checkout bk1
$ hg up bk1
abort: crosses branches (use 'hg merge' or 'hg update -C')
</code></pre>

<p>Is this normal behavior, for there to be ""crosses branches"" forcing a merge or file overwrite, when moving between bookmarks?</p>

<p>For a 'Git-like experience' I would expect to be able to flick back and forth between bk1 and bk2, committing and developing on either, merging if and when I needed to.</p>",2,1,2009-11-22 12:40:04.757000 UTC,1.0,2017-06-16 15:15:16.250000 UTC,6,git|version-control|mercurial|dvcs,2614,2009-05-11 03:31:23.613000 UTC,2022-03-05 10:27:04.930000 UTC,"Brisbane, Australia",399,69,1,45,"<p>Please upgrade to Mercurial 1.4, which was released last week. Then you will be able to switch between heads on a branch without warning.</p>",4.0,2009-11-22 15:29:38.773000 UTC,,16.0,[]
Is it possible to export from Fossil SCM into some other DVCS?,"<p>For many years I used Darcs as my only DVCS...some time ago I explored Monotone which I like as well.</p>

<p>However, the main problem with both is the lack of complete hosting solutions.</p>

<p><a href=""http://www.fossil-scm.org/index.html/doc/tip/www/index.wiki"" rel=""noreferrer"" title=""Fossil SCM"">Fossil</a> looks as interesting option (although we don't like its use of non-standard wiki markup) considering it's very light in regard to hosting requirements and incorporates distributed bug tracker which eliminates the need for some public hosting solution as in the scenario with darcs &amp; monotone.</p>

<p>However, Fossil is not very widely adopted and still very young project, so I'm curious how does it operate with other, more established DVCS-es (bzr,git,hg)...</p>

<p>Based on the info I've gathered from docs, I see it can only import from CVS although it seems there is some tool which can do import from git.</p>

<p>However, the more serious problem is that, afaict, there is no tool to migrate from Fossil into something else by using e.g. 'standard' fast-export/import toolchain.</p>

<p>In the archive I saw that Dr Hipp mentioned 'deconstruct' command, but not being familiar-enough with Fossil, I'm curious how it can be used for exporting Fossil's artifacts into some other DVCS or if I'm not aware of some other tool/converter for such task?</p>

<p>This is serious issue and the question is if there is no such tool, whether it's better to just use bzr/hg (we are not fan of Git and want that tool operates nicely on Linux/Mac/Win for an open-source project) instead along with their public hostings like LP &amp; Bitbucket?</p>",1,0,2010-11-07 20:05:33.663000 UTC,2.0,2010-11-09 12:57:17.520000 UTC,11,version-control|migration|dvcs|fossil,1337,2010-10-05 11:14:53.220000 UTC,2016-11-09 06:06:40.830000 UTC,Croatia,971,32,0,82,"<p>Interestingly enough, but recently (after I've asked this question), Fossil got <em>import</em> &amp; <em>export</em> commands, so that now it's possible to use Git's <em>fast-import/export</em> protocol. (See <a href=""http://fossil-scm.org/index.html/doc/trunk/www/inout.wiki"" rel=""noreferrer"" title=""Import And Export"">Import And Export""</a>.) :-)</p>

<p>No need to tell, but it makes Fossil perfect tool for my DVCS needs and the above dilemma is resolved to my upmost satisfaction. ;)</p>",3.0,2010-11-14 07:16:38.570000 UTC,,12.0,[]
Git interoperability with a Mercurial Repository,"<p>I use GIT on a Mac. Enough said. I have the tools, I have the experience. And I want to continue to use it. No wars here...</p>

<p>The problem is always with interoperability. Most people use SVN, which is great for me. Git SVN works out of the box, and is a no frills solution. People can continue happily use SVN and I don't lose my workflow and neither my tools.</p>

<p>Now... Some guys come along with Mercurial. Fine for them: they have their reasons. But I can't find any GIT HG out-of-the-box. I don't want to switch to HG, but I still need to interoperate with their repository.</p>

<p>Any of you guys know a simple solution for this?</p>",11,2,2009-05-19 15:26:51.250000 UTC,121.0,2012-09-10 21:58:16.060000 UTC,200,git|version-control|mercurial|interop|dvcs,46919,2008-11-28 17:43:32.153000 UTC,2022-03-02 22:39:09.597000 UTC,"Porto, Portugal",8807,194,29,1067,"<p>Update from June 2012. Currently there seem to be the following methods for Git/Hg interoperability when the developer wants to work from the git side:</p>
<ol>
<li><p>Install Mercurial and the <a href=""https://hg-git.github.io/"" rel=""nofollow noreferrer""><strong>hg-git extension</strong></a>. You can do the latter using your package manager, or with <code>easy_install hg-git</code>. Then make sure the following is in your ~/.hgrc:</p>
<pre><code>[extensions]
hggit = 
</code></pre>
</li>
</ol>
<p>You may see some references that talk about specifying the <code>bookmarks</code> extension here too, but that has been built into Mercurial since v 1.8. Here are <a href=""http://candidcode.com/2010/01/12/a-guide-to-converting-from-mercurial-hg-to-git-on-a-windows-client/"" rel=""nofollow noreferrer"">some tips about installing hg-git on Windows</a>.</p>
<p>Once you have hg-git, you can use commands roughly like <a href=""https://stackoverflow.com/a/1089221/674840"">Abderrahim Kitouni posted above</a>. This method has been <a href=""http://traviscline.com/blog/2010/04/27/using-hg-git-to-work-in-git-and-push-to-hg/"" rel=""nofollow noreferrer"">refined and tweaked</a> since 2009 though, and there is a friendly wrapper: <a href=""https://github.com/abourget/git-hg-again"" rel=""nofollow noreferrer""><strong>git-hg-again</strong></a>. This uses the toplevel directory as a working directory for both Mercurial and Git at the same time. It creates a Mercurial bookmark that it keeps in synch with the tip of the <code>default</code> (unnamed) branch in the Mercurial repository, and it updates a local Git branch from that bookmark.</p>
<ol start=""2"">
<li><p><a href=""https://github.com/rfk/git-remote-hg"" rel=""nofollow noreferrer""><strong>git-remote-hg</strong></a> is a different wrapper, also based on the Mercurial <code>hg-git</code> extension. This additionally makes use of the <code>git-remote-helpers</code> protocols (hence its name). It uses the toplevel directory only for a Git working directory; it keeps its Mercurial repository bare. It also maintains a second bare Git repository to make synching between Git and Mercurial safer and more idiomatically gitlike.</p>
</li>
<li><p>The <a href=""https://github.com/cosmin/git-hg"" rel=""nofollow noreferrer""><strong>git-hg</strong></a> script (formerly maintained <a href=""https://github.com/offbytwo/git-hg"" rel=""nofollow noreferrer"">here</a>) uses a different method, based on <code>hg-fast-export</code> from the <a href=""http://repo.or.cz/w/fast-export.git"" rel=""nofollow noreferrer""><strong>fast-export project</strong></a>. Like method 2, this also keeps a bare Mercurial repository and an additional bare Git repository.</p>
</li>
</ol>
<p>For pulling, this tool ignores Mercurial bookmarks and instead imports every named Mercurial branch into a Git branch, and the default (unnamed) Mercurial branch into master.</p>
<p>Some commentary discusses this tool as being hg-&gt;git only, but it claims to have merged in git-&gt;hg push support on 7 Dec 2011. As I explain in <a href=""https://github.com/dubiousjim/yagh/blob/master/Evaluation.md"" rel=""nofollow noreferrer"">a review of these tools</a>, though, the way this tool tries to implement push support doesn't seem to be workable.</p>
<ol start=""4"">
<li><p>There's also <a href=""https://github.com/msysgit/msysgit/wiki/Guide-to-git-remote-hg"" rel=""nofollow noreferrer""><strong>another project called git-remote-hg</strong></a>. Unlike the version listed above, this one doesn't rely on hg-git, but instead directly accesses the Mercurial Python API. At the moment, using it also requires a patched version of git. I haven't tried this yet.</p>
</li>
<li><p>Finally, <a href=""http://progetti.arstecnica.it/tailor"" rel=""nofollow noreferrer""><strong>Tailor</strong></a> is a project that incrementally converts between a variety of different VCSs. It sounds like development of this won't be aggressively continued.</p>
</li>
</ol>
<p>The first three of these approaches looked lightweight enough to persuade me to investigate. I needed to tweak them in some ways to get them to run on my setup, and I saw some ways to tweak them further to improve them, and then I tweaked them still further to make them behave more like each other so that I could evaluate them more effectively. Then I thought others might like to have these tweaks too, to do the same evaluation. So I've <a href=""https://github.com/dubiousjim/yagh"" rel=""nofollow noreferrer"">made a source package</a> that will enable you to install my versions of any of the first three tools. It should also take care of installing the needed <code>hg-fast-export</code> pieces. (You need to install <code>hg-git</code> on your own.)</p>
<p>I encourage you to try them out and decide for yourself what works best. I'll be glad to hear about cases where these tools break. I'll try to keep them in synch with upstream changes, and to make sure the upstream authors are aware of the tweaks I think are useful.</p>
<p>As I mentioned above, in evaluating these tools, I came to the conclusion that <code>git-hg</code> is only usable for pulling from Mercurial, not for pushing.</p>
<p>Relatedly, here are some useful comparisons/translation manuals between Git and Mercurial, in some cases targetted at users who already know Git:</p>
<ul>
<li><a href=""https://www.mercurial-scm.org/wiki/GitConcepts"" rel=""nofollow noreferrer"">Mercurial for Git users</a></li>
<li><a href=""https://stackoverflow.com/questions/1598759/git-and-mercurial-compare-and-contrast"">Git and Mercurial - Compare and Contrast</a></li>
<li><a href=""https://stackoverflow.com/questions/35837/what-is-the-difference-between-mercurial-and-git"">What is the difference between Mercurial and Git</a></li>
<li><a href=""http://alblue.bandlem.com/2011/03/mercurial-and-git-technical-comparison.html"" rel=""nofollow noreferrer"">Mercurial and Git: a technical comparison</a></li>
<li><a href=""https://github.com/sympy/sympy/wiki/Git-hg-rosetta-stone"" rel=""nofollow noreferrer"">Git hg rosetta stone</a></li>
<li><a href=""http://quirkygba.blogspot.com/2009/04/mercurial.html"" rel=""nofollow noreferrer"">Homebrew Coding: Mercurial</a></li>
<li><a href=""http://francisoud.blogspot.com/2010/07/git-vs-mercurial.html"" rel=""nofollow noreferrer"">Francisoud's Blog: Git vs Mercurial (hg)</a></li>
<li><a href=""http://www.wikivs.com/wiki/Git_vs_Mercurial"" rel=""nofollow noreferrer"">Git vs Mercurial</a></li>
</ul>",2.0,2012-06-24 15:31:43.540000 UTC,2021-12-01 19:09:49.917000 UTC,63.0,[]
GIT: Appending a patch made after a few commits,"<p>I use git to keep track of changes made by our development team and committed into our central cvs-style repository.  Since its cvs, it keeps track of files and not commits, making it sometimes difficult to tell exactly what files constitute the full patch for a bug fix.  I just came across one and did the following:</p>

<p>1) trolling along, checking CVS logs and committing them to git as full patches</p>

<pre><code>A--B--C--D
</code></pre>

<p>2) Found another file change that was actually for ticket (B), so I reset the current branch to B with </p>

<pre><code>git reset --soft &lt;sha1 ID for commit B&gt;
</code></pre>

<p>3) I copy in the change, and append it to commit (B) with</p>

<pre><code>git commit --amend
</code></pre>

<p>4) to my surprise, the tree now reads</p>

<pre><code>A--B
</code></pre>

<p>with commits (C) and (D) only in the working tree.  Their details are gone from the logs and I don't think I can get them back.  Where did I go wrong?  Is my only option to make an additional commit on top of (D), and just know that its really part of (B)?</p>",2,1,2009-10-09 17:33:05.743000 UTC,6.0,2009-10-09 19:12:57.367000 UTC,8,git|version-control|dvcs,2946,2008-11-10 16:16:24.243000 UTC,2022-03-04 22:04:26.037000 UTC,"Schenectady, NY",1555,317,1,370,"<h2>What happened</h2>

<p>You mean amend, not append, right?  I'm going to pretend this was on a branch called master, for convenience. This is what your repository looks like now:</p>

<pre><code>A---B' (master)
 \
  \-B---C---D
</code></pre>

<p>Git commits explicitly depend on their parents - the same patch on top of a different parent is a different commit.</p>

<h2>How to recover</h2>

<p>You can recover the previous position of  a few ways. There's a nice shorthand for previous positions, which you can use to directly check it out or create a branch:</p>

<pre><code>git checkout master@{1}
git branch oldmaster master@{1}
</code></pre>

<p>This is assuming it's the first previous position. It might be the second (<code>master@{2}</code>)... or if you know when it was, you can use <code>master@{7:35}</code> or <code>master@{23.hours.ago}</code>. For a summary of these forms, see the ""specifying revisions"" section of <code>man git-rev-parse</code> (<a href=""http://www.kernel.org/pub/software/scm/git/docs/git-rev-parse.html#_specifying_revisions"" rel=""noreferrer"">online here</a>).</p>

<p>If you're not sure exactly how to get to it, try</p>

<pre><code>git reflog show master
</code></pre>

<p>This will give you a list of previous positions of <code>master</code>, and you should be able to tell from the descriptions which one you want (or maybe try a few). You can simply copy hashes from the list, and use <code>git checkout</code> or <code>git branch</code> as above.</p>

<h2>What you should have done</h2>

<p>Warning: editing history is a bad idea if it's been published already - in that case, you should simply commit the fix. Yes, it's kind of ugly having it split into two commits in the repository, but other users have to be able to trust what they've seen in the public repo not to change!</p>

<p>That said, to do this particular kind of history editing, you want interactive rebase:</p>

<pre><code>git rebase -i master~4 master
</code></pre>

<p><code>master~4</code> represents the commit four commits before the tip of master. You can use any form you want here - maybe it's another branch, maybe a commit hash - whatever works.</p>

<p>This will open up in an editor a list of the commits you're playing with:</p>

<pre><code>pick &lt;hash-A&gt;  &lt;message-A&gt;
pick &lt;hash-B&gt;  &lt;message-B&gt;
pick &lt;hash-C&gt;  &lt;message-C&gt;
pick &lt;hash-D&gt;  &lt;message-D&gt;

# Rebase &lt;hash-A^&gt;..&lt;hash-D&gt; onto &lt;hash-A^&gt;
#
# Commands:
#  p, pick = use commit
#  e, edit = use commit, but stop for amending
#  s, squash = use commit, but meld into previous commit
#
# If you remove a line here THAT COMMIT WILL BE LOST.
# However, if you remove everything, the rebase will be aborted.
#
</code></pre>

<p>The commented-out help text there is pretty self-explanatory. In this case, you want to change 'pick' to 'edit' on commit B's line, save, and quit. The rebase will start, and it'll pause after applying <code>B</code> to let you make changes. You'll do what you need to, add, use <code>git commit --amend</code>, and then <code>git rebase --continue</code>. It'll apply <code>C</code> and <code>D</code>, and you'll be done. If anything goes wrong in the middle, use <code>git rebase --abort</code> to get back to where you started.</p>

<p>Rebasing can be kind of scary - for example, don't accidentally remove lines in that list! If you're not comfortable with it yet, it's a good idea to make heavy use of <code>gitk</code> and backup branch names.</p>",3.0,2009-10-09 17:57:46.557000 UTC,,14.0,[]
How does git track source code moved between files?,"<p>Apparently, when you move a function from one source code file to another, the git revision log (for the new file) can show you where that code fragment was originally coming from (see for example the Viewing History section in <a href=""http://nathanj.github.com/gitguide/tour.html"" rel=""noreferrer"">this tutorial</a>).</p>

<p>How does this work?</p>",2,0,2009-11-13 12:26:50.173000 UTC,1.0,,31,git|version-control|dvcs,8982,2008-09-17 04:23:55.480000 UTC,2022-03-04 16:59:34.417000 UTC,Singapore,248194,11159,747,16118,"<p>It doesn't track them. That's the beauty of it.</p>

<p>Git only records snapshots of the entire project tree: here's what <em>all files</em> looked like before the commit and here's how they look like after. <em>How</em> we got from here to there, Git doesn't care.</p>

<p>This allows intelligent tools to be written <em>after</em> a commit has already happened, to extract information from that commit. For example, rename detection in Git is done by comparing all deleted files against all new files and comparing pairwise similarity metrics. If the similarity metric is greater than <code>x</code>, they are considered renamed, if it is between <code>y</code> and <code>x</code> (<code>y &lt; x</code>), it is considered to be a rename+edit, and if it is below <code>y</code>, they are considered independent. The cool thing is that you, as a ""commit archaeologist"", can specify after the fact, what <code>x</code> and <code>y</code> should be. This would not work if the commit simply recorded ""this file is a rename of that file"".</p>

<p>Detecting moved content works similar: you slice every file into pieces, compute similarity metrics between all the slices and can then deduce that this slice which was deleted over here and this very similar slice which was added over there are actually the same slice that was moved from here to there.</p>

<p>However, as tonfa mentioned in his answer, this is <em>very</em> expensive, so it is not normally done. But it <em>could</em> be done, and that's the point.</p>

<p>BTW: this is pretty much the exact opposite of the Operational Transformation model used by Google Wave, EtherPad, Gobby, SubEthaEdit, ACE and Co.</p>",8.0,2009-11-13 17:55:33.507000 UTC,,45.0,[]
Mercurial with multiple projects,"<p>I have a couple of projects with different release cycles sitting in my SVN repository. Releases are created by using the classic tags structure in SVN. When there are bugs to fix in releases a branch is created from a tag, the bug is fixed and then merged from there into trunk.</p>

<p>Now, for multiple reasons, I want to switch from SVN to mercurial with a central push site.</p>

<p>Question: Which is the best way in mercurial to organize multiple projects that share little code between them? Should I create multiple push sites, one for each project?</p>

<p>Please include in the answer a description on how to recreate my release-tag, bugfix branch, ... with your preferred version of repository design.</p>

<p>Edit: I would like to install as little extensions as possible.</p>

<p>Edit2:</p>

<p>Given this SVN layout:</p>

<pre><code>.
|-- project-a
|   |-- branches
|   |   |-- 1.x
|   |   `-- feature-1
|   |-- tags
|   `-- trunk
`-- project-b
    |-- branches
    |-- tags
    |   |-- 1.0
    |   `-- 1.1
    `-- trunk
</code></pre>

<p>(thanks @bendin! :) )</p>

<p>Is it better to work with multiple hg push repositories</p>

<pre><code>project_a-trunk
project_a-1.x
project_a-feature-1
project_b-trunk
</code></pre>

<p>for the branches. Tags are folded into the appropriate branch.</p>

<p>Or would you rather go with two push repositories in this example</p>

<pre><code>project_a
project_b
</code></pre>

<p>with named branches and therefore multiple heads within one repo.</p>

<p>The advantage I see with the multiple heads repos is that I don't have to go hunt for a tag in multiple repos. The disadvantage I see is that the hg book seems to discourage multiple head repos. What would/do you do?</p>",3,2,2009-05-30 10:35:06.460000 UTC,12.0,2015-07-03 10:20:00.393000 UTC,32,version-control|mercurial|dvcs|repository-design,8094,2008-11-04 00:25:25.990000 UTC,2022-02-22 15:19:55.123000 UTC,"Unterschleissheim, Bavaria, Germany",2732,79,5,413,"<p>Some subversion repositories will group logically unrelated things (i.e. projects with different version numbers and release cycles) under one trunk:</p>

<pre><code>.
|-- branches
|   |-- project-a-1.x
|   `-- project-a-feature-1
|-- tags
|   |-- project-a-1.0
|   |-- project-b-1.0
|   `-- project-b-1.1
`-- trunk
    |-- project-a
    `-- project-b
</code></pre>

<p>This kind of layout has no direct analog in mercurial. Each project which has its own release cycle and own version numbers should have its own repository. </p>

<p>Some subversion repositories are structured to do this by giving each project its own trunk, tags and branches: </p>

<pre><code>.
|-- project-a
|   |-- branches
|   |   |-- 1.x
|   |   `-- feature-1
|   |-- tags
|   `-- trunk
`-- project-b
    |-- branches
    |-- tags
    |   |-- 1.0
    |   `-- 1.1
    `-- trunk
</code></pre>

<p>You can think of each project as a logical repository within your physical subversion repository. Each project has its own trunk, tags and branches.  This also has the benefit that you can keep tag and branch names shorter because you already know which project they belong to.</p>

<p>This layout is also trivial to express with a tool like mercurial. Each ""project"" becomes a mercurial repository. Tags and branches within that repository are tags and branches of that project.</p>",3.0,2009-05-30 10:59:57.393000 UTC,,12.0,[]
"In git, what are some good conventions to format multiple comments to a single commit","<p>I was wondering how people normally separate out the multiple comments in a single commit. Stars? Commas? Separate lines?  Just wondering what you find to be the best.</p>

<p>I'm using this now when I add comments via Emacs, but not sure I like it:</p>

<pre><code>Added error messaging
Cleaned up sign-up UI
Added recaptcha

# Please enter the commit message for your changes. Lines starting
# with '#' will be ignored, and an empty message aborts the commit.
#
# Committer: Conrad Chu &lt;chuboy@conradlaptop.local&gt;
#
# On branch master
# Changes to be committed:
#   (use ""git reset HEAD &lt;file&gt;..."" to unstage)
#
#       modified:   app/controllers/api_controller.rb
#       modified:   app/controllers/users_controller.rb
#       modified:   app/models/user.rb
#       modified:   app/views/users/new.html.erb
#       modified:   app/views/users/show.html.erb
#       modified:   config/environment.rb
#       modified:   db/migrate/20090923001219_create_users.rb
#       modified:   db/schema.rb
#       modified:   doc/README
#       modified:   public/stylesheets/master.css
#       new file:   vendor/plugins/recaptcha/.gitignore
#       new file:   vendor/plugins/recaptcha/CHANGELOG
</code></pre>",4,0,2009-10-02 22:50:35.130000 UTC,3.0,2009-10-02 23:38:13.127000 UTC,5,git|dvcs,1719,2009-08-26 23:11:22.510000 UTC,2021-06-24 01:38:34.233000 UTC,"Alameda, CA",609,8,0,42,"<p>Git has very strong conventions on log messages, but the rules are simple:</p>

<ol>
<li>First line is a summary of the commit</li>
<li>First line may have a scope description prefix ""module:""</li>
<li>Second line is empty</li>
<li>Then follows a discussion in paragraphs, as long as it needs be</li>
</ol>

<p>As a start you should use these conventions, as the presentation tools even rely on them (The second line being empty is important, and in many situations, you will only see the first line summary.)</p>

<p>With git, commits should be small so the first answer is of course, you shouldn't modify many things in one commit. You should have three commits, not one.</p>

<p>But, you can write a whole essay in the commit log, and there you can describe their changes in all detail (motivation, discarded designs, ideas). If the three changes truly belong together, this text will make it clear why.</p>

<p>I found <a href=""http://www.tpope.net/node/106"" rel=""noreferrer"">more instructions</a> describing the same Git Commit message conventions, with examples for which git commands depend on the particular format. (Most of it is all based on an existing convention: Patches sent by emails.)</p>",0.0,2009-10-02 22:57:44.450000 UTC,2009-10-05 17:51:26.117000 UTC,18.0,[]
git push to multiple repositories simultaneously,"<p>How can I make <code>git push</code> to push not only to <code>origin</code> but also another remote repository?</p>

<p>as <code>git push</code> is only an alias for <code>git push origin</code>, can I alias git push to push to 2 remote repositories at once (with just that one command)?</p>

<p>I’m not looking for a non-git script here but would like to set this up for my local repository in git.</p>

<p>When I tried it with post-push scripts I failed.</p>",3,2,2010-11-23 12:10:19.923000 UTC,53.0,2013-01-19 18:10:44.877000 UTC,91,git|version-control|dvcs,46441,2010-07-15 11:28:51.210000 UTC,2022-03-04 09:57:09.383000 UTC,"Offenburg, Germany",8453,1313,175,735,"<p>I don't think you can do it just by setting a flag on git, but you can modify a config file that will allow you to push to multiple remote repositories without manually typing them all in (well only typing them in the first time and not after)</p>

<p>In the <code>.git/config</code> file you can add multiple urls to a defined remote:</p>

<pre><code>[remote ""all""]
    url=ssh://user@server/repos/g0.git
    url=ssh://user@server/repos/g1.git
</code></pre>

<p>If you <code>git push all</code> now you push to all the remote urls.</p>",6.0,2010-11-23 12:18:30.673000 UTC,2014-07-07 12:28:49.943000 UTC,141.0,[]
Does a Distributed Version Control System really have no centralised repository?,"<p>It might seem a silly question, but how do you get a working drectory set up without a server to check out from? And how does a business keep a safe backed up copy of the repo?</p>

<p>I assume then there must be a central repo... but then how exactly is it 'distributed'? I always thought of a server-client (SVN) Vs peer-2-peer (GIT) distinction, but I don't believe that can be correct unless tools like GIT are dependent on torrent-style technology?</p>",7,0,2010-03-19 09:39:05.887000 UTC,4.0,,10,version-control|dvcs,649,2009-08-06 19:20:13.693000 UTC,2022-03-02 16:53:16.720000 UTC,,56045,1022,155,3750,"<p>Re: 'torrent-style technology' - you're confusing 2 issues, one of network topology (peer to peer vs. server/client) and one of server authority. This is understandable because the terms are almost identical. But there's nothing about distributed source control that makes any requirements on the network connection model - you could be distributing changesets via email if you prefer. The important thing with distributed version control is that each person essentially runs their own server and merges changes in from the other servers. Of course, you need to be able to get your initial clone from somewhere, and how you know where that 'somewhere' is falls outside of the scope of the system itself. There is no 'tracker' program or anything - typically someone has a public repository somewhere with the address published on a web site. But once you've cloned it, your copy is a full one that is capable of being the basis for someone else's clone.</p>",0.0,2010-03-19 10:01:02.587000 UTC,,8.0,[]
NuGet and distributed version control (DVCS),"<p>I wonder if it is possible to use nuget to only store references to the required packages in version control (only the package.config and ignore the packages folder).</p>

<p>Is there a way to tell nuget to (re)download all the referenced packages in the various package.config files? Or something similar which could be put into a build script.</p>

<p><strong>Update:</strong></p>

<p>Seems that I'm not the only one who requested this feature: <a href=""http://nupack.codeplex.com/workitem/165"" rel=""noreferrer"">See this work item</a> (thanks to PHeiberg for the hint)</p>

<p><strong>Update 2:</strong></p>

<p>NuGet now has this feature builtin. See <a href=""http://docs.nuget.org/docs/workflows/using-nuget-without-committing-packages#Using_NuGet_without_committing_packages_to_source_control"" rel=""noreferrer"">Using NuGet without committing packages to source control</a> for details. All left is to add the packages directory to .gitignore or some equivalent of your VCS (<code>/packages/</code> will do the trick if you have it in the root of your repository and are using git).</p>",3,1,2010-10-16 18:12:01.303000 UTC,11.0,2012-04-11 18:17:53.903000 UTC,52,git|version-control|dvcs|nuget,6176,2008-09-24 09:12:19.993000 UTC,2022-03-04 18:10:59.013000 UTC,Austria,10365,152,5,451,"<p>I just found out about NuGetPowerTools: <a href=""https://github.com/davidfowl/NuGetPowerTools"" rel=""noreferrer"">https://github.com/davidfowl/NuGetPowerTools</a></p>

<p>Also see: <a href=""http://blog.davidebbo.com/2011/08/easy-way-to-set-up-nuget-to-restore.html"" rel=""noreferrer"">http://blog.davidebbo.com/2011/08/easy-way-to-set-up-nuget-to-restore.html</a></p>

<p>Update: NuGet 1.6 now supports Package Restore: <a href=""http://docs.nuget.org/docs/release-notes/nuget-1.6"" rel=""noreferrer"">http://docs.nuget.org/docs/release-notes/nuget-1.6</a></p>",0.0,2011-08-23 08:00:15.680000 UTC,2011-12-17 21:21:16.453000 UTC,19.0,[]
"In bazaar, how do I change the default target of ""bzr push""?","<p><code>bzr push</code> defaults to the first target you pushed to. If I now want to push to a different location, but don't want to manually enter it every time, how do I change the default?</p>",1,0,2010-01-01 06:17:15.067000 UTC,2.0,2010-03-29 03:33:06.350000 UTC,13,version-control|dvcs|bazaar,1218,2009-05-12 01:13:48.413000 UTC,2022-03-04 20:11:31.030000 UTC,"Baltimore, MD, USA",26697,1230,81,1172,"<p>I figured it out. You do:</p>

<pre><code>bzr push --remember [location]
</code></pre>",1.0,2010-01-01 06:39:24.880000 UTC,,27.0,[]
Named Branches vs Multiple Repositories,"<p>We're currently using subversion on a relatively large codebase. Each release gets its own branch, and fixes are performed against the trunk and migrated into release branches using <code>svnmerge.py</code></p>

<p>I believe the time has come to move on to better source control, and I've been toying with Mercurial for a while.</p>

<p>There seems to be two schools of though on managing such a release structure using Mercurial.  Either each release gets its own repo, and fixes are made against the release branch and pushed to the main branch (and any other newer release branches.)  OR using named branches within a single repository (or multiple matching copies.)</p>

<p>In either case it seems like I might be using something like transplant to cherrypick changes for inclusion in the release branches.</p>

<p>I ask of you; what are the relative merits of each approach?</p>",6,0,2009-05-20 23:20:48.120000 UTC,82.0,2019-02-07 09:54:45.540000 UTC,130,version-control|mercurial|branch|dvcs,20015,2009-02-15 20:28:31.737000 UTC,2021-07-20 23:40:19.387000 UTC,"Smithers, Canada",3929,54,1,156,"<p>The biggest difference is how the branch names are recorded in the history. With named branches the branch name is <em>embedded</em> in each changeset and will thus become an immutable part of the history. With clones there will be <em>no permanent</em> record of where a particular changeset came from.</p>

<p>This means that clones are great for quick experiments where you don't want to record a branch name, and named branches are good for long term branches (""1.x"", ""2.x"" and similar).</p>

<p>Note also that a single repository can easily accommodate multiple light-weight branches in Mercurial. Such in–repository branches can be bookmarked so that you can easily find them again. Let's say that you have cloned the company repository when it looked like this:</p>

<pre><code>[a] --- [b]
</code></pre>

<p>You hack away and make <code>[x]</code> and <code>[y]</code>:</p>

<pre><code>[a] --- [b] --- [x] --- [y]
</code></pre>

<p>Mean while someone puts <code>[c]</code> and <code>[d]</code> into the repository, so when you pull you get a history graph like this:</p>

<pre>
            [x] --- [y]
           /
[a] --- [b] --- [c] --- [d]
</pre>

<p>Here there are two heads in a single repository. Your working copy will always reflect a single changeset, the so-called working copy parent changeset. Check this with:</p>

<pre><code>% hg parents
</code></pre>

<p>Let's say that it reports <code>[y]</code>. You can see the heads with</p>

<pre><code>% hg heads
</code></pre>

<p>and this will report <code>[y]</code> and <code>[d]</code>. If you want to update your repository to a clean checkout of <code>[d]</code>, then simply do (substitute <code>[d]</code> with the revision number for <code>[d]</code>):</p>

<pre><code>% hg update --clean [d]
</code></pre>

<p>You will then see that <code>hg parents</code> report <code>[d]</code>. This means that your next commit will have <code>[d]</code> as parent. You can thus fix a bug you've noticed in the main branch and create changeset <code>[e]</code>:</p>

<pre>
            [x] --- [y]
           /
[a] --- [b] --- [c] --- [d] --- [e]
</pre>

<p>To push changeset <code>[e]</code> only, you need to do</p>

<pre><code>% hg push -r [e]
</code></pre>

<p>where <code>[e]</code> is the changeset hash. By default <code>hg push</code> will simply compare the repositories and see that <code>[x]</code>, <code>[y]</code>, and <code>[e]</code> are missing, but you might not want to share <code>[x]</code> and <code>[y]</code> yet.</p>

<p>If the bugfix also effects you, you want to merge it with your feature branch:</p>

<pre><code>% hg update [y]
% hg merge
</code></pre>

<p>That will leave your repository graph looking like this:</p>

<pre>
            [x] --- [y] ----------- [z]
           /                       /
[a] --- [b] --- [c] --- [d] --- [e]
</pre>

<p>where <code>[z]</code> is the merge between <code>[y]</code> and <code>[e]</code>. You could also have opted to throw the branch away:</p>

<pre><code>% hg strip [x]
</code></pre>

<p><strong>My main point of this story is this:</strong> a single clone can easily represent several tracks of development. This has always been true for ""plain hg"" without using any extensions. The <a href=""https://www.mercurial-scm.org/wiki/BookmarksExtension"" rel=""nofollow noreferrer"">bookmarks extension</a> is a great help, though. It will allow you to assign names (bookmarks) to changesets. In the case above you will want a bookmark on your development head and one on the upstream head. Bookmarks can be <strong>pushed and pulled</strong> with Mercurial 1.6 and have become a built-in feature in Mercurial 1.8.</p>

<p>If you had opted to make two clones, your development clone would have looked like this after making <code>[x]</code> and <code>[y]</code>:</p>

<pre><code>[a] --- [b] --- [x] --- [y]
</code></pre>

<p>And your upstream clone will contain:</p>

<pre><code>[a] --- [b] --- [c] --- [d]
</code></pre>

<p>You now notice the bug and fix it. Here you don't have to <code>hg update</code> since the upstream clone is ready to use. You commit and create <code>[e]</code>:</p>

<pre><code>[a] --- [b] --- [c] --- [d] --- [e]
</code></pre>

<p>To include the bugfix in your development clone you pull it in there:</p>

<pre>
[a] --- [b] --- [x] --- [y]
           \
            [c] --- [d] --- [e]
</pre>

<p>and merge:</p>

<pre>
[a] --- [b] --- [x] --- [y] --- [z]
           \                   /
            [c] --- [d] --- [e]
</pre>

<p>The graph might looks different, but it has the same structure and the end result is the same. Using the clones you had to do a little less mental bookkeeping.</p>

<p>Named branches didn't really come into the picture here because they are quite optional. Mercurial itself was developed using two clones for years before we switched to using named branches. We maintain a branch called 'stable' in addition to the 'default' branch and make our releases based on the 'stable' branch. See the <a href=""https://www.mercurial-scm.org/wiki/StandardBranching"" rel=""nofollow noreferrer"">standard branching</a> page in the wiki for a description of the recommended workflow.</p>",4.0,2009-05-21 00:27:38.467000 UTC,2018-02-22 09:52:09.350000 UTC,129.0,[]
"Decoding git objects / ""Block length does not match with its complement"" error","<p>I'm stuck with very simple but annoying issue, and cannot find answer on the Internet. Hope you will be able to point me, what I've done wrong.</p>

<p>I'm trying to decode object from Git repository. According to <a href=""http://progit.org/book/ch9-2.html"" rel=""noreferrer"">ProGit</a>, file name and it's contents have been deflated during commit.</p>

<p>I'm using C# to read object indicated by SHA1 into a stream, inflate it and convert into byte array. Here is the code:</p>

<pre><code>using System.IO.Compression;

static internal byte[] GetObjectBySha(string storagePath, string sha)
{
    string filePath = Path.Combine(storagePath, ""objects"", sha.Substring(0, 2), sha.Substring(2, 38));
    byte[] fileContent = null;

    using (FileStream fs = new FileStream(filePath, FileMode.Open, FileAccess.Read))
    {
        using (MemoryStream ms = new MemoryStream())
        {
            using (DeflateStream gs = new DeflateStream(fs, CompressionMode.Decompress))
            {
                gs.CopyTo(ms);
            }

            fileContent = ms.ToArray();
        }
    }

    return fileContent;
}
</code></pre>

<p>When <code>gs.CopyTo(ms);</code> is reached the runtime error occurs: <strong>Block length does not match with its complement.</strong></p>

<p>Why so? </p>

<p>Regarding the content of the file I'm trying to read... It's binary and it was created by git executable. The original file name is <code>testfile.txt</code>, it's content is <code>Sample text.</code> the SHA1 is <code>51d0be227ecdc0039698122a1513421ce35c1dbe</code>.</p>

<p>Any idea would be greatly appreciated!</p>",2,0,2011-12-02 10:18:33.443000 UTC,2.0,2011-12-02 10:24:59.777000 UTC,7,c#|git|dvcs|low-level,6065,2011-11-15 14:11:57.343000 UTC,2022-02-22 16:05:09.347000 UTC,"Stockhom, Sweden",8508,1688,4,1263,"<p><code>DeflateStream</code> and <code>zlib</code> are two different things as explained in <a href=""https://stackoverflow.com/a/6283224/29407"">this answer</a>:</p>

<blockquote>
  <p>There is no ZlibStream in the .NET base class library - nothing that
  produces or consumes ZLIB</p>
</blockquote>

<p>So what you need is a ZLIB consumer. The <a href=""http://dotnetzip.codeplex.com/"" rel=""nofollow noreferrer"">DotNetZip</a> library provides one:</p>

<pre><code>static internal byte[] GetObjectBySha(string storagePath, string sha)
{
    string filePath = Path.Combine(storagePath, ""objects"", sha.Substring(0, 2), sha.Substring(2, 38));
    byte[] compressed = File.ReadAllBytes(filePath);
    return Ionic.Zlib.ZlibStream.UncompressBuffer(compressed);
}
</code></pre>",1.0,2011-12-02 11:13:43.193000 UTC,2017-05-23 12:09:20.917000 UTC,8.0,[]
Git and Mercurial - Compare and Contrast,"<p>For a while now I've been using subversion for my personal projects.</p>

<p>More and more I keep hearing great things about Git and Mercurial, and DVCS in general.</p>

<p>I'd like to give the whole DVCS thing a whirl, but I'm not too familiar with either option.</p>

<p>What are some of the differences between Mercurial and Git?</p>

<p>Note: I'm <strong>not</strong> trying to find out which one is ""best"" or even which one I should start with.  I'm mainly looking for key areas where they are similar, and where they are different, because I am interested to know how they differ in terms of implementation and philosophy.</p>",9,2,2009-10-21 04:46:35.957000 UTC,318.0,2018-11-15 13:40:46.477000 UTC,523,git|version-control|mercurial|dvcs,40949,2008-09-16 16:30:55.053000 UTC,2022-03-05 05:12:19.973000 UTC,"Oakland, CA, United States",101338,840,127,2928,"<p><strong>Disclaimer:</strong> <em>I use Git, follow Git development on git mailing list, and even contribute a bit to Git (gitweb mainly). I know Mercurial from documentation and some from discussion on #revctrl IRC channel on FreeNode.</em></p>
<p><em>Thanks to all people on on #mercurial IRC channel who provided help about Mercurial for this writeup</em></p>
<hr />
<hr />
<h2>Summary</h2>
<p><sub><em>Here it would be nice to have some syntax for table, something like in PHPMarkdown / MultiMarkdown / Maruku extension of Markdown</em></sub></p>
<ul>
<li><strong>Repository structure:</strong> Mercurial doesn't allow octopus merges (with more than two parents), nor tagging non-commit objects.</li>
<li><strong>Tags:</strong> Mercurial uses versioned <code>.hgtags</code> file with special rules for per-repository tags, and has also support for local tags in <code>.hg/localtags</code>; in Git tags are refs residing in <code>refs/tags/</code> namespace, and by default are autofollowed on fetching and require explicit pushing.</li>
<li><strong>Branches:</strong> In Mercurial basic workflow is based on <em>anonymous heads</em>; Git uses lightweight named branches, and has special kind of branches (<em>remote-tracking branches</em>) that follow branches in remote repository.</li>
<li><strong>Revision naming and ranges:</strong> Mercurial provides <em>revision numbers</em>, local to repository, and bases relative revisions (counting from tip, i.e. current branch) and revision ranges on this <em>local</em> numbering; Git provides a way to refer to revision relative to branch tip, and revision ranges are topological (based on graph of revisions)</li>
<li>Mercurial uses <strong>rename tracking</strong>, while Git uses <strong>rename detection</strong> to deal with file renames</li>
<li><strong>Network:</strong> Mercurial supports SSH and HTTP &quot;smart&quot; protocols, and static HTTP protocol; modern Git supports SSH, HTTP  and GIT &quot;smart&quot; protocols, and HTTP(S) &quot;dumb&quot; protocol.  Both have support for bundles files for off-line transport.</li>
<li>Mercurial uses <strong>extensions</strong> (plugins) and established API; Git has <strong>scriptability</strong> and established formats.</li>
</ul>
<hr />
<p>There are a few things that differ Mercurial from Git, but there are other things that make them similar.  Both projects borrow ideas from each other.  For example <code>hg bisect</code> command in Mercurial (formerly <a href=""https://www.mercurial-scm.org/wiki/BisectExtension"" rel=""noreferrer"">bisect extension</a>) was inspired by <code>git bisect</code> command in Git, while idea of <code>git bundle</code> was inspired by <code>hg bundle</code>.</p>
<h2>Repository structure, storing revisions</h2>
<p>In Git there are four types of objects in its object database: <em>blob</em> objects which contain contents of a file, hierarchical <em>tree</em> objects which store directory structure, including file names and relevant parts of file permissions (executable permission for files, being a symbolic link), <em>commit</em> object which contain authorship info, pointer to snapshot of state of repository at revision represented by a commit (via a tree object of top directory of project) and references to zero or more parent commits, and <em>tag</em> objects which reference other objects and can be signed using PGP / GPG.</p>
<p>Git uses two ways of storing objects: <em>loose</em> format, where each object is stored in a separate file (those files are written once, and never modified), and <em>packed</em> format where many objects are stored delta-compressed in a single file.  Atomicity of operations is provided by the fact, that reference to a new object is written (atomically, using create + rename trick) after writing an object.</p>
<p>Git repositories require periodic maintenance using <code>git gc</code> (to reduce disk space and improve performance), although nowadays Git does that automatically.  (This method provides better compression of repositories.)</p>
<p>Mercurial (as far as I understand it) stores history of a file in a <em>filelog</em> (together, I think, with extra metadata like rename tracking, and some helper information); it uses flat structure called <em>manifest</em> to store directory structure, and structure called <em>changelog</em> which store information about changesets (revisions), including commit message and zero, one or two parents.</p>
<p>Mercurial uses <em>transaction journal</em> to provide atomicity of operations, and relies on <em>truncating</em> files to clean-up after failed or interrupted operation.  Revlogs are append-only.</p>
<p>Looking at repository structure in Git versus in Mercurial, one can see that Git is more like object database (or a content-addressed filesystem), and Mercurial more like traditional fixed-field relational database.</p>
<p><strong>Differences:</strong><br />
<sup>In Git the <em>tree</em> objects form a <strong>hierarchical</strong> structure; in Mercurial <em>manifest</em> file is <strong>flat</strong> structure.  In Git <em>blob</em> object store <strong>one version</strong> of a contents of a file; in Mercurial <em>filelog</em> stores <strong>whole history of a single file</strong> (if we do not take into account here any complications with renames). This means that there are different areas of operations where Git would be faster than Mercurial, all other things considered equal (like merges, or showing history of a project), and areas where Mercurial would be faster than Git (like applying patches, or showing history of a single file). <em>This issue might be not important for end user.</em> </sup></p>
<p>Because of the fixed-record structure of Mercurial's <em>changelog</em> structure, commits in Mercurial can have only <strong>up to two parents</strong>; commits in Git can have more than two parents (so called &quot;octopus merge&quot;).  While you can (in theory) replace octopus merge by a series of two-parent merges, this might cause complications when converting between Mercurial and Git repositories.</p>
<p>As far as I know Mercurial doesn't have equivalent of <strong>annotated tags</strong> (tag objects) from Git. A special case of annotated tags are <strong>signed tags</strong> (with PGP / GPG signature); equivalent in Mercurial can be done using <a href=""https://www.mercurial-scm.org/wiki/GpgExtension"" rel=""noreferrer"">GpgExtension</a>, which extension is being distributed along with Mercurial.  You can't <strong>tag non-commit object</strong> in Mercurial like you can in Git, but that is not very important, I think (some git repositories use tagged blob to distribute public PGP key to use to verify signed tags).</p>
<h2>References: branches and tags</h2>
<p>In Git references (branches, remote-tracking branches and tags) reside outside DAG of commits (as they should).  References in <code>refs/heads/</code> namespace (<strong>local branches</strong>) point to commits, and are usually updated by &quot;git commit&quot;; they point to the tip (head) of branch, that's why such name.  References in <code>refs/remotes/&lt;remotename&gt;/</code> namespace (<strong>remote-tracking branches</strong>) point to commit, follow branches in remote repository <code>&lt;remotename&gt;</code>, and are updated by &quot;git fetch&quot; or equivalent.  References in <code>refs/tags/</code> namespace (<strong>tags</strong>) point usually to commits (lightweight tags) or tag objects (annotated and signed tags), and are not meant to change.</p>
<h3>Tags</h3>
<p>In Mercurial you can give persistent name to revision using <strong>tag</strong>; tags are stored similarly to the ignore patterns.  It means that globally visible tags are stored in revision-controlled <code>.hgtags</code> file in your repository. That has two consequences: first, Mercurial has to use special rules for this file to get current list of all tags and to update such file (e.g. it reads the most recently committed  revision of the file, not currently checked out version); second, you have to commit changes to this file to have new tag visible to other users / other repositories (as far as I understand it).</p>
<p>Mercurial also supports <em>local tags</em>, stored in <code>hg/localtags</code>, which are not visible to others (and of course are not transferable)</p>
<p>In Git tags are fixed (constant) named references to other objects (usually tag objects, which in turn point to commits) stored in <code>refs/tags/</code> namespace.  By default when fetching or pushing a set of revision, git automatically fetches or pushes tags which point to revisions being fetched or pushed.  Nevertheless you can <em>control</em> to some extent <em>which tags are fetched</em> or pushed.</p>
<p>Git treats lightweight tags (pointing directly to commits) and annotated tags (pointing to tag objects, which contain tag message which optionally includes PGP signature, which in turn point to commit) slightly differently, for example by default it considers only annotated tags when describing commits using &quot;git describe&quot;.</p>
<p>Git doesn't have a strict equivalent of local tags in Mercurial. Nevertheless git best practices recommend to setup separate public bare repository, into which you push ready changes, and from which others clone and fetch. This means that tags (and branches) that you don't push, are private to your repository. On the other hand you can also use namespace other than <code>heads</code>, <code>remotes</code> or <code>tags</code>, for example <code>local-tags</code> for local tags.</p>
<p><sup><strong>Personal opinion:</strong> In my opinion tags should reside outside revision graph, as they are external to it (they are pointers into graph of revisions).  Tags should be non-versioned, but transferable.  Mercurial's choice of using a mechanism similar to the one for ignoring files, means that it either has to treat <code>.hgtags</code> specially (file in-tree is transferable, but ordinary it is versioned), or have tags which are local only (<code>.hg/localtags</code> is non-versioned, but untransferable).</sup></p>
<h3>Branches</h3>
<p>In Git <strong>local branch</strong> (branch tip, or branch head) is a named reference to a commit, where one can grow new commits.  Branch can also mean active line of development, i.e. all commits reachable from branch tip.  Local branches reside in <code>refs/heads/</code> namespace, so e.g. fully qualified name of 'master' branch is 'refs/heads/master'.</p>
<p>Current branch in Git (meaning checked out branch, and branch where new commit will go) is the branch which is referenced by the HEAD ref. One can have HEAD pointing directly to a commit, rather than being symbolic reference; this situation of being on an anonymous unnamed branch is called <em>detached HEAD</em> (&quot;git branch&quot; shows that you are on '(no branch)').</p>
<p>In Mercurial there are anonymous branches (branch heads), and one can use bookmarks (via <a href=""https://www.mercurial-scm.org/wiki/BookmarksExtension"" rel=""noreferrer"">bookmark extension</a>).  Such <strong>bookmark branches</strong> are purely local, and those names were (up to version 1.6) not transferable using Mercurial. You can use rsync or scp to copy the <code>.hg/bookmarks</code> file to a remote repository. You can also use <code>hg id -r &lt;bookmark&gt; &lt;url&gt;</code> to get the revision id of a current tip of a bookmark.</p>
<p>Since 1.6 bookmarks can be pushed/pulled. The <a href=""https://www.mercurial-scm.org/wiki/BookmarksExtension"" rel=""noreferrer"">BookmarksExtension</a> page has a section on <a href=""https://www.mercurial-scm.org/wiki/BookmarksExtension#Working_With_Remote_Repositories"" rel=""noreferrer"">Working With Remote Repositories</a>.  There is a difference in that in Mercurial bookmark names are <em>global</em>, while definition of 'remote' in Git describes also <em>mapping of branch names</em> from the names in remote repository to the names of local remote-tracking branches; for example <code>refs/heads/*:refs/remotes/origin/*</code> mapping means that one can find state of 'master' branch ('refs/heads/master') in the remote repository in the 'origin/master' remote-tracking branch ('refs/remotes/origin/master').</p>
<p>Mercurial has also so called <em>named branches</em>, where the branch name is <strong>embedded</strong> in a commit (in a changeset).  Such name is global (transferred on fetch).  Those branch names are permanently recorded as part of the changeset\u2019s metadata.  With modern Mercurial you can close &quot;named branch&quot; and stop recording branch name.  In this mechanism tips of branches are calculated on the fly.</p>
<p>Mercurial's &quot;named branches&quot; should in my opinion be called <em><strong>commit labels</strong></em> instead, because it is what they are.  There are situations where &quot;named branch&quot; can have multiple tips (multiple childless commits), and can also consist of several disjoint parts of graph of revisions.</p>
<p>There is no equivalent of those Mercurial &quot;embedded branches&quot; in Git; moreover Git's philosophy is that while one can say that branch includes some commit, it doesn't mean that a commit belongs to some branch.</p>
<p>Note that Mercurial documentation still proposes to use separate clones (separate repositories) at least for long-lived branches (single branch per repository workflow), aka <em>branching by cloning</em>.</p>
<h3>Branches in pushing</h3>
<p>Mercurial by default pushes <strong>all heads</strong>.  If you want to push a single branch (<strong>single head</strong>), you have to specify tip revision of the branch you want to push.  You can specify branch tip by its revision number (local to repository), by revision identifier, by bookmark name (local to repository, doesn't get transferred), or by embedded branch name (named branch).</p>
<p>As far as I understand it, if you push a range of revisions that contain commits marked as being on some &quot;named branch&quot; in Mercurial parlance, you will have this &quot;named branch&quot; in the repository you push to.  This means that names of such embedded branches (&quot;named branches&quot;) are <em>global</em> (with respect to clones of given repository / project).</p>
<p>By default (subject to <code>push.default</code> configuration variable) &quot;git push&quot; or &quot;git push &lt;<i>remote</i>&gt;&quot; Git would push <strong>matching branches</strong>, i.e. only those local branches that have their equivalent already present in remote repository you push into.  You can use <code>--all</code> option to git-push (&quot;git push --all&quot;) to push <strong>all branches</strong>, you can use &quot;git push &lt;<i>remote</i>&gt; &lt;<i>branch</i>&gt;&quot; to push a <strong>given single branch</strong>, and you can use &quot;git push &lt;<i>remote</i>&gt; HEAD&quot; to push <strong>current branch</strong>.</p>
<p>All of the above assumes that Git isn't configured which branches to push via <code>remote.&lt;remotename&gt;.push</code>  configuration variables.</p>
<h3>Branches in fetching</h3>
<p><i><strong>Note:</strong> here I use Git terminology where &quot;fetch&quot; means downloading changes from remote repository <em>without</em> integrating those changes with local work.  This is what &quot;<code>git fetch</code>&quot; and &quot;<code>hg pull</code>&quot; does.</i></p>
<p>If I understand it correctly, by default Mercurial fetches <strong>all heads</strong> from remote repository, but you can specify branch to fetch via &quot;<code>hg pull --rev &lt;rev&gt; &lt;url&gt;</code>&quot; or &quot;<code>hg pull &lt;url&gt;#&lt;rev&gt;</code>&quot; to get <strong>single branch</strong>.  You can specify &lt;rev&gt; using revision identifier, &quot;named branch&quot; name (branch embedded in changelog), or bookmark name.  Bookmark name however (at least currently) doesn't get transferred.  All &quot;named branches&quot; revisions you get belong to get transferred.  &quot;hg pull&quot; stores tips of branches it fetched as anonymous, unnamed heads.</p>
<p>In Git by default (for 'origin' remote created by &quot;git clone&quot;, and for remotes created using &quot;git remote add&quot;)  &quot;<code>git fetch</code>&quot; (or &quot;<code>git fetch &lt;remote&gt;</code>&quot;) gets <strong>all branches</strong> from remote repository (from <code>refs/heads/</code> namespace), and stores them in <code>refs/remotes/</code> namespace.  This means for example that branch named 'master' (full name: 'refs/heads/master') in remote 'origin' would get stored (saved) as 'origin/master' <em>remote-tracking branch</em> (full name: 'refs/remotes/origin/master').</p>
<p>You can fetch <strong>single branch</strong> in Git by using <code>git fetch &lt;remote&gt; &lt;branch&gt;</code> - Git would store requested branch(es) in FETCH_HEAD, which is something similar to Mercurial unnamed heads.</p>
<p>Those are but examples of default cases of powerful <strong>refspec</strong> Git syntax: with refspecs you can specify and/or configure which branches one want to fetch, and where to store them.  For example default &quot;fetch all branches&quot; case is represented by '+refs/heads/*:refs/remotes/origin/*' wildcard refspec, and &quot;fetch single branch&quot; is shorthand for 'refs/heads/&lt;branch&gt;:'.  Refspecs are used to map names of branches (refs) in remote repository to local refs names.  But you don't need to know (much) about refspecs to be able to work effectively with Git (thanks mainly to &quot;git remote&quot; command).</p>
<p><sup><strong>Personal opinion:</strong> I personally think that &quot;named branches&quot; (with branch names embedded in changeset metadata) in Mercurial are misguided design with its global namespace, especially for a <em>distributed</em> version control system.  For example let's take case where both Alice and Bob have &quot;named branch&quot; named 'for-joe' in their repositories, branches which have nothing in common. In Joe's repository however those two branches would be mistreated as a single branch. So you have somehow come up with convention protecting against branch name clashes. This is not problem with Git, where in Joe's repository 'for-joe' branch from Alice would be 'alice/for-joe', and from Bob it would be 'bob/for-joe'. See also <a href=""http://bitbucket.org/abuehl/hgext-branchident/wiki/BranchIdentityPlan"" rel=""noreferrer"">Separating branch name from branch identity</a> issue raised on Mercurial wiki.</p>
<p>Mercurial's &quot;bookmark branches&quot; currently lack in-core distribution mechanism.</sup></p>
<p><strong>Differences:</strong><br />
This area is one of the main differences between Mercurial and Git, as <a href=""https://stackoverflow.com/questions/1598759/git-and-mercurial-compare-and-contrast/1598877#1598877"">james woodyatt</a> and <a href=""https://stackoverflow.com/questions/1598759/git-and-mercurial-compare-and-contrast/1600841#1600841"">Steve Losh</a> said in their answers. Mercurial, by default, uses anonymous lightweight codelines, which in its terminology are called &quot;heads&quot;.  Git uses lightweight named branches, with injective mapping to map names of branches in remote repository to names of remote-tracking branches.  Git &quot;forces&quot; you to name branches (well, with exception of single unnamed branch, situation called detached HEAD), but I think this works better with branch-heavy workflows such as topic branch workflow, meaning multiple branches in single repository paradigm.</p>
<h2>Naming revisions</h2>
<p>In Git there are many ways of naming revisions (described e.g. in <a href=""http://www.kernel.org/pub/software/scm/git/docs/git-rev-parse.html"" rel=""noreferrer"">git rev-parse</a> manpage):</p>
<ul>
<li>The full SHA1 object name (40-byte hexadecimal string), or a substring of such that is unique within the repository</li>
<li>A symbolic ref name,  e.g. 'master' (referring to 'master' branch), or 'v1.5.0' (referring to tag), or 'origin/next' (referring to remote-tracking branch)</li>
<li>A suffix <code>^</code> to revision parameter means the first parent of a commit object, <code>^n</code> means n-th parent of a merge commit.  A suffix <code>~n</code> to revision parameter means n-th ancestor of a commit in straight first-parent line. Those suffixes can be combined, to form revision specifier following path from a symbolic reference, e.g. 'pu~3^2~3'</li>
<li>Output of &quot;git describe&quot;, i.e. a closest tag, optionally followed by a dash and a number of commits, followed by a dash, a 'g', and an abbreviated object name, for example 'v1.6.5.1-75-g5bf8097'.</li>
</ul>
<p>There are also revision specifiers involving reflog, not mentioned here.  In Git each object, be it commit, tag, tree or blob has its SHA-1 identifier; there is special syntax like e.g. 'next:Documentation' or 'next:README' to refer to tree (directory) or blob (file contents) at specified revision.</p>
<p>Mercurial also has many ways of naming changesets (described e.g. in <a href=""http://www.selenic.com/mercurial/hg.1.html"" rel=""noreferrer"">hg</a> manpage):</p>
<ul>
<li>A plain integer is treated as a revision number.  One need to remember that revision numbers are <em>local to given repository</em>; in other repository they can be different.</li>
<li>Negative integers are treated as sequential offsets from the tip, with -1 denoting the tip, -2 denoting the revision prior to the tip, and so forth. They are also <em>local</em> to repository.</li>
<li>An unique revision identifier (40-digit hexadecimal string) or its unique prefix.</li>
<li>A tag name (symbolic name associated with given revision), or a bookmark name (with extension: symbolic name associated with given head, local to repository), or a &quot;named branch&quot; (commit label; revision given by &quot;named branch&quot; is tip (childless commit) of all commits with given commit label, with largest revision number if there are more than one such tip)</li>
<li>The reserved name &quot;tip&quot; is a special tag that always identifies the most recent revision.</li>
<li>The reserved name &quot;null&quot; indicates the null revision.</li>
<li>The reserved name &quot;.&quot; indicates the working directory parent.</li>
</ul>
<p><strong>Differences</strong><br />
As you can see comparing above lists Mercurial offers revision numbers, local to repository, while Git doesn't.  On the other hand Mercurial offers relative offsets only from 'tip' (current branch), which are local to repository (at least without <a href=""https://www.mercurial-scm.org/wiki/ParentrevspecExtension"" rel=""noreferrer"">ParentrevspecExtension</a>), while Git allows to specify any commit following from any tip.</p>
<p>The most recent revision is named HEAD in Git, and &quot;tip&quot; in Mercurial; there is no null revision in Git. Both Mercurial and Git can have many root (can have more than one parentless commits; this is usually result of formerly separate projects joining).</p>
<p><strong>See also:</strong> <a href=""http://blogs.gnome.org/newren/2008/03/31/many-different-kinds-of-revision-specifiers/"" rel=""noreferrer"">Many different kinds of revision specifiers</a> article on Elijah's Blog (newren's).</p>
<p><sup><strong>Personal opinion:</strong> I think that <em>revision numbers</em> are overrated (at least for distributed development and/or nonlinear / branchy history).  First, for a distributed version control system they have to be either local to repository, or require treating some repository in a special way as a central numbering authority.  Second, larger projects, with longer history, can have number of revisions in 5 digits range so they are offer only slight advantage over shortened to 6-7 character revision identifiers, and imply strict ordering while revisions are only partially ordered (I mean here that revisions n and n+1 doesn't need to be parent and child).</sup></p>
<h3>Revision ranges</h3>
<p>In Git revision ranges are <strong>topological</strong>.  Commonly seen <code>A..B</code> syntax, which for linear history means revision range starting at A (but excluding A), and ending at B (i.e. range is <em>open from below</em>), is shorthand (&quot;syntactic sugar&quot;) for <code>^A B</code>, which for history traversing commands mean all commits reachable from B, excluding those reachable from A.  This means that the behavior of <code>A..B</code> range is entirely predictable (and quite useful) even if A is not ancestor of B: <code>A..B</code> means then range of revisions from common ancestor of A and B (merge base) to revision B.</p>
<p>In Mercurial revision ranges are based on range of <strong>revision numbers</strong>. Range is specified using <code>A:B</code> syntax, and contrary to Git range acts as a <em>closed interval</em>.  Also range B:A is the range A:B in reverse order, which is not the case in Git (but see below note on <code>A...B</code> syntax).  But such simplicity comes with a price: revision range A:B makes sense only if A is ancestor of B or vice versa, i.e. with linear history; otherwise (I guess that) the range is unpredictable, and the result is local to repository (because revision numbers are local to repository).</p>
<p>This is fixed with Mercurial 1.6, which has new <strong>topological revision range</strong>, where 'A..B' (or 'A::B') is understood as the set of changesets that are both descendants of X and ancestors of Y.  This is, I guess, equivalent to '--ancestry-path A..B' in Git.</p>
<p>Git also has notation <code>A...B</code> for symmetric difference of revisions; it means <code>A B --not $(git merge-base A B)</code>, which means all commits reachable from either A or B, but excluding all commits reachable from both of them (reachable from common ancestors).</p>
<h2>Renames</h2>
<p>Mercurial uses <strong>rename tracking</strong> to deal with file renames.  This means that the information about the fact that a file was renamed is saved at the commit time; in Mercurial this information is saved in the &quot;enhanced diff&quot; form in <em>filelog</em> (file revlog) metadata.  The consequence of this is that you have to use <code>hg rename</code> / <code>hg mv</code>... or you need to remember to run <code>hg addremove</code> to do similarity based rename detection.</p>
<p>Git is unique among version control systems in that it uses <strong>rename detection</strong> to deal with file renames.  This means that the fact that file was renamed is detected at time it is needed: when doing a merge, or when showing a diff (if requested / configured).  This has the advantage that rename detection algorithm can be improved, and is not frozen at time of commit.</p>
<p>Both Git and Mercurial require using <code>--follow</code> option to follow renames when showing history of a single file.  Both can follow renames when showing line-wise history of a file in <code>git blame</code> / <code>hg annotate</code>.</p>
<p>In Git the <code>git blame</code> command is able to follow code movement, also moving (or copying) code from one file to the other, even if the code movement is not part of wholesome file rename.  <em>As far as I know this feature is unique to Git (at the time of writing, October 2009).</em></p>
<h2>Network protocols</h2>
<p>Both Mercurial and Git have support for fetching from and pushing to repositories on the same filesystem, where repository URL is just a filesystem path to repository.  Both also have support for fetching from <em>bundle files</em>.</p>
<p>Mercurial support fetching and pushing via SSH and via HTTP protocols.  For SSH one needs an accessible shell account on the destination machine and a copy of hg installed / available.  For HTTP access the <code>hg-serve</code> or Mercurial CGI script running is required, and Mercurial needs to be installed on server machine.</p>
<p>Git supports two kinds of protocols used to access remote repository:</p>
<ul>
<li><em>&quot;smart&quot; protocols</em>, which include access via SSH and via custom git:// protocol (by <code>git-daemon</code>), require having git installed on server.  The exchange in those protocols consist of client and server negotiating about what objects they have in common, and then generating and sending a packfile.  Modern Git includes support for &quot;smart&quot; HTTP protocol.</li>
<li><em>&quot;dumb&quot; protocols</em>, which include HTTP and FTP (only for fetching), and HTTPS (for pushing via WebDAV), do not require git installed on server, but they do require that repository contains extra information generated by <code>git update-server-info</code> (usually run from a hook).  The exchange consist of client walking the commit chain and downloading loose objects and packfiles as needed.  The downside is that it downloads more than strictly required (e.g. in corner case when there is only single packfile it would get downloaded whole even when fetching only a few revisions), and that it can require many connections to finish.</li>
</ul>
<h2>Extending: scriptability vs extensions (plugins)</h2>
<p>Mercurial is implemented in <strong>Python</strong>, with some core code written in C for performance.  It provides API for writing <strong>extensions</strong> (plugins) as a way of adding extra features.  Some of functionality, like &quot;bookmark branches&quot; or signing revisions, is provided in extensions distributed with Mercurial and  requires turning it on.</p>
<p>Git is implemented in <strong>C</strong>, <strong>Perl</strong> and <strong>shell scripts</strong>.  Git provides many low level commands (<em>plumbing</em>) suitable to use in scripts.  The usual way of introducing new feature is to write it as Perl or shell script, and when user interface stabilizes rewrite it in C for performance, portability, and in the case of shell script avoiding corner cases (this procedure is called <em>builtinification</em>).</p>
<p>Git relies and is built around [repository] formats and [network] protocols.  Instead of language bindings there are (partial or complete) <em>reimplementations</em> of Git in other languages (some of those are partially reimplementations, and partially wrappers around git commands): JGit (Java, used by EGit, Eclipse Git Plugin), Grit (Ruby), Dulwich (Python), git# (C#).</p>
<hr />
<p>TL;DR</p>",30.0,2009-10-21 10:23:42.910000 UTC,2020-06-20 09:12:55.060000 UTC,457.0,[]
Is there any value in including SQLite in VCS's,"<p>Having an argument with my team. We are developing an application using SQLite and some want to add it to the repo (GIT) and some don't. Previously with RDBMS system there has been no perceived benefit of using VCS on the DB. However SQLite is a self contained file with no external dependencies so i assume, even though it is binary, that a commit of the project code + the SQLite  file will give an accurate snapshot of the state of play at that point.</p>

<p>I also assume that a branch and merge would work as well.</p>

<p>Has anyone actually done this and if so does it work?</p>",2,0,2010-12-14 12:21:06.437000 UTC,2.0,,9,sqlite|version-control|dvcs,993,2009-08-27 17:43:29.450000 UTC,2021-09-24 14:42:25.563000 UTC,"Nottingham, United Kingdom",6492,142,34,912,<p>You'd get more benefit from GIT's versioning facilities if you stored a dump of the SQLite database (i.e. commands required to create it) rather than the database file itself. That way you could look at the history of the dump file and see tables or data being added etc.</p>,1.0,2010-12-14 12:40:31.170000 UTC,,10.0,[]
What files will be changed vs added when I do an hg pull and hg update,"<p>So in Subversion when I do an <code>svn up</code> I get a list of files that were added, modified, deleted, and conflicted.</p>

<p>When I do an <code>hg pull</code> then <code>hg up -v</code> it just displays a list of: <code>getting file.ext</code> but I have no way of know if that file is new or already existed. Is there a way to have Mercurial display the same sort of meta about if the file was added, modified, or deleted?</p>

<p>Does Mercurial offer any ability to do what I am asking?</p>",4,0,2010-07-18 21:31:13.343000 UTC,6.0,2012-03-30 12:27:36.107000 UTC,14,version-control|mercurial|dvcs|pull,4836,2010-07-18 21:31:13.283000 UTC,2017-09-01 20:06:36.693000 UTC,,271,3,0,19,"<p>Use the status command to list changes in file status between the working copy and its parent revision or <strong>between any two revisions</strong>. It gives you output like this:</p>

<pre><code>$ hg status --rev .:tip
M hgext/keyword.py
M mercurial/cmdutil.py
M mercurial/commands.py
M mercurial/context.py
M mercurial/patch.py
A tests/test-encoding-align
A tests/test-encoding-align.out
</code></pre>

<p>which corresponds to this update:</p>

<pre><code>$ hg update -v
resolving manifests
getting hgext/keyword.py
getting mercurial/cmdutil.py
getting mercurial/commands.py
getting mercurial/context.py
getting mercurial/patch.py
getting tests/test-encoding-align
getting tests/test-encoding-align.out
7 files updated, 0 files merged, 0 files removed, 0 files unresolved
</code></pre>

<p><strong>Edit:</strong> You can create a <code>preupdate</code> hook to always get this information as part of your updates. I happen to be on Windows right now, and here this hook works:</p>

<pre><code>[hooks]
preupdate = hg status --rev .:%HG_PARENT1%
</code></pre>

<p>Replace <code>%HG_PARENT1%</code> with <code>$HG_PARENT1</code> on Unix-like systems. This should make the Mercurial experience even more Subversion-like :-)</p>",3.0,2010-07-20 11:15:09.727000 UTC,2010-07-21 10:03:16.187000 UTC,17.0,[]
Renaming directories with the Fossil DVCS,"<p>Is it possible to rename directories with Fossil?  I've tried the obvious command:</p>

<pre><code>fossil mv oldname newname
</code></pre>

<p>Fossil then informs me that it has done something:</p>

<pre><code>RENAME oldname newname
</code></pre>

<p>However, calling ""fossil changes"" results in an empty list.  As far as I can tell, renaming directories is either not supported, not yet implemented or just broken.  Is there a trick to this?</p>",1,0,2009-12-15 14:30:32.077000 UTC,2.0,2010-03-16 10:29:20.270000 UTC,12,dvcs|fossil,1454,2008-08-21 13:59:43.713000 UTC,2022-02-22 22:22:55.027000 UTC,"Mountain View, CA",4810,1138,6,281,"<p>After some research I've found that it can be done, but it is counter-intuitive.  Fossil doesn't really care what happens to directories; all it cares about is the location of the files within them.</p>

<p>When renaming a directory, Fossil appears to:</p>

<ul>
<li>loop through the repository's list of files for the old directory;</li>
<li>locate the file in the new directory on the filesystem;</li>
<li>update the files' metadata so that they are listed as being part of the new directory.</li>
</ul>

<p>If the new directory does not exist, this fails.  There are no files in the new location so Fossil cannot match up the old with the new, so no changes are made.</p>

<p>In short: You <em>must</em> rename the folder via the filesystem before you try to make the change to Fossil.  If you don't, Fossil ignores you.</p>

<p>Now that I think about it, this makes sense, though I'd prefer it if Fossil would just update the filesystem itself instead of forcing a two-step process on its users.</p>

<p>As an addendum, it appears to be impossible to add an empty directory to Fossil.  I assume that internally it stores just files; folders are considered to be metadata.  An empty folder is metadata describing nothing, so adding them makes no sense.</p>",1.0,2009-12-15 16:21:59.247000 UTC,,17.0,[]
How can I tell Emacs my git branch has changed?,"<p>At the bottom of my Emacs 23 editor, I notice that Emacs is aware that I am working in a directory that is under version control, what that version control system is, and what branch I am currently on. Pretty cool! But say I am on the master branch, and from the command line I do a <code>git commit</code>, followed by a <code>git checkout &lt;branch&gt;</code>. Emacs still shows me being on the master branch. How do I refresh Emacs so that it reflects the branch I am currently on without closing down all my buffers and restarting it? </p>",3,3,2013-06-11 18:07:54.010000 UTC,1.0,,13,git|emacs|version-control|dvcs,2713,2012-08-06 15:45:17.707000 UTC,2022-03-04 20:24:48.253000 UTC,"Cambridge, MA",2922,328,4,303,"<p><kbd>M-x</kbd> <code>revert-buffer</code> but I suggest you to use <a href=""http://www.emacswiki.org/emacs/Magit""><code>magit-mode</code></a> to manage your git repos in Emacs.</p>

<p>For the record, I use to bind <code>revert-buffer</code> to <kbd>F5</kbd>:</p>

<pre class=""lang-lisp prettyprint-override""><code>(global-set-key [f5] 'revert-buffer)
</code></pre>",2.0,2013-06-11 18:10:01.237000 UTC,2014-02-26 00:35:27.270000 UTC,15.0,[]
How do I get changes from my trunk into a branch?,"<p>I've just started using Git and find that whilst I am implementing a feature in a branch, I will encounter some bug that really needs to be pushed to the trunk as soon as possible. In order to do so, I use checkout to switch back to the trunk, make my changes and commit them. I now have a trunk that is bug free.</p>

<p>Unfortunately, my branch needs to have this bug fixed as well. As the feature is not complete, I can't just merge the branch back into the trunk. How can I alter my branch so that it receives the changes I made to the trunk?</p>

<p>If it matters, I am developing on my own and so only have a single repository to worry about.</p>

<p>I am using TortoiseGit so instructions specific to that would be helpful but not necessary.</p>",2,0,2010-09-05 12:31:12.107000 UTC,3.0,,10,git|dvcs|tortoisegit,17211,2009-12-22 05:03:17.873000 UTC,2022-03-04 19:47:16.310000 UTC,United Kingdom,12450,407,21,429,"<p>Make sure you have your branch checked out (<code>git checkout branch-name</code>) and run</p>

<p><code>git rebase master</code></p>

<p>And resolve any conflicts that arrive.</p>

<p>If you aren't sure what these commands do, try not using TortoiseGit and use the terminal. It will help you really understand the commands.</p>

<p>WARNING: This assumes a local branch. If you have shared the branch, do not run the rebase (because it modifies history). Run</p>

<p><code>git merge master</code></p>

<p>while you are on your other branch. This has less clean history, but can be used.</p>

<p>The difference is:</p>

<ul>
<li>Rebase - rewrites the branch ontop of master, replaying all the changes</li>
<li>Merge - a normal merge, creating a commit with two parents</li>
</ul>",10.0,2010-09-05 12:40:06.007000 UTC,,25.0,[]
Submitting Hg changes back to SVN,"<p>I've began work in an SVN repository. I've cloned a subfolder of it into a local Hg repo with <code>hg clone</code>.</p>

<p>Afterwards, I wanted to share this with a colleague who does not have access to the SVN repository. I've created a private BitBucket repository, and we occasionally pushed the changes, and hence I had to pull them. </p>

<p><code>hgsubversion</code> does some nasty things to changesets, such as changing their committer (and I believe even the hash). When I tried pushing and pulling to the BitBucket repo, I had to do a merge.</p>

<p>Now I am unable to push changes back into the Subversion repository due to our beloved friend, <code>abort: Sorry, can't find svn parent of a merge revision.</code>. </p>

<p>How would one pull the BitBucket-targetting Mercurial repository with the svn-targetting Mercurial repository, while staying compatible with <code>hgsubversion</code> (that is, without importing the merge revisions)?</p>

<p>Some automated way to do this would be appreciated, of course, but if there is no such thing/easy way to do this, I'd be grateful for any solution.</p>

<p>I use <code>hgsubversion</code>, not <code>hgsvn</code>; that is, the extension in which one does <code>hg clone svn://repo/url</code>. I am open to switching, though, if necessary.</p>",3,2,2010-10-21 21:55:13.417000 UTC,7.0,2012-03-30 12:36:07.077000 UTC,16,svn|mercurial|dvcs|hgsubversion,2531,2008-11-22 20:51:10.823000 UTC,2022-03-05 05:01:09.643000 UTC,"Dublin, Ireland",9384,1239,48,2087,"<p>When you use Mercurial on a subversion repository, you have to still think like SVN does, so a lot of features part of the basic mercurial workflow just won't work. Merging the way mercurial does it is impossible on a svn depot. If you have merged your work with the pulled svn branch, you'll get the infamous about message you're getting now :(</p>

<p>I suggest you read durin42's answer to <a href=""https://stackoverflow.com/q/2503152/454556"">this question</a>.</p>

<p>EDIT : To get out of your current mess, I suggest you create a patch (or a series of patches) from the point you checkout from the SVN repo. Get a new fresh copy from the subversion repository and apply the patch(es). I am not sure you'll be able to do it from your current repo. You could explore the hg diff command.</p>

<pre><code>hg diff -g -r tip -r XXX &gt; patch
</code></pre>

<p>with XXX being your original SVN checkout (I haven't tested it yet.)</p>",3.0,2010-10-23 14:47:17.207000 UTC,2017-05-23 12:24:49.367000 UTC,21.0,[]
Sorting out a Git mess,"<p>I've just inherited a project which was maintained using Git. At one point, the code was deployed onto 3 separate systems and each system maintained their own decentralised Git respository.</p>

<p>Each of the 3 systems extended the original base system in 3 different directions. None of the 3 systems have been synchronised against each other. Some changes are on master branch, others are on new branches.</p>

<p>How can I bring the 3 different sources together so that I can:</p>

<ol>
<li>find a common base to work with;</li>
<li>find out which changes are bug fixes which should be applied across all 3 systems; and</li>
<li>maintain the 3 systems in a sane way so that there is only one common branch and separate out the customisations required for the 3 different systems?</li>
</ol>",2,0,2009-03-03 09:20:48.603000 UTC,4.0,2009-03-03 12:40:53.007000 UTC,9,git|version-control|branch|dvcs,1687,2009-02-19 22:30:12.157000 UTC,2010-09-05 18:23:37.173000 UTC,,3811,2,0,127,"<p>I would probably start by pushing all the repositories to separate branches in a central repository, from which I can rebase, merge etc between branches easily.</p>

<p>A good visualization tool such as <a href=""http://wiki.github.com/krig/git-age"" rel=""nofollow noreferrer"">git-age</a>, <a href=""http://wiki.github.com/Caged/gitnub"" rel=""nofollow noreferrer"">gitnub</a>, <a href=""http://gitx.frim.nl/"" rel=""nofollow noreferrer"">gitx</a>, <a href=""http://live.gnome.org/giggle"" rel=""nofollow noreferrer"">giggle</a> can work wonders, but your task will probably be rather tedious unless you can find the branching points. If there are similar patches applied to all branches you can use (interactive) <a href=""http://git-scm.com/docs/git-rebase"" rel=""nofollow noreferrer"">rebase</a> to reorder your commits such that they are in the same order. Then you can start 'zipping up' your branches, moving the branch-point upwards by putting commits into master. A nice description on how to reorder commits using rebase can be found <a href=""http://gitready.com/advanced/2009/03/20/reorder-commits-with-rebase.html"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Chances are the actions you need to take are described in the links provided by the <a href=""http://www.kernel.org/pub/software/scm/git/docs/howto-index.html"" rel=""nofollow noreferrer"">Git Howto Index</a>. A good <a href=""http://zrusin.blogspot.com/2007/09/git-cheat-sheet.html"" rel=""nofollow noreferrer"">cheat sheet</a> is always nice to have within reach. Also, I suspect the followup to Eric Sinks post ""<a href=""http://www.ericsink.com/entries/dvcs_dag_1.html"" rel=""nofollow noreferrer"">DVCS and DAGs, Part 1</a>"" will contain something useful (It didn't, but was an interesting read nontheless).</p>

<p>Additional good-to-have links are: <a href=""http://www-cs-students.stanford.edu/~blynn/gitmagic/"" rel=""nofollow noreferrer"">Git Magic</a>, <a href=""http://gitready.com/"" rel=""nofollow noreferrer"">Git Ready</a> and <a href=""http://www.sourcemage.org/Git_Guide"" rel=""nofollow noreferrer"">SourceMage Git Guide</a></p>

<p>I hope all the repos had good commit messages that tell you the purpose of each patch, it's that or code review :)</p>

<p>As for how to maintain customizations we've had luck with the following:</p>

<p>We started by separating (or keeping separate) the customized code from the generic code. Then we've tried two approaches; both which worked fine:</p>

<ol>
<li>All deployments got their own repositories where the customization was kept.</li>
<li>All deployments got their own branch in a 'customization'-repository.</li>
</ol>

<p>After the first deployment and seeing that the second was a fact we spent some time trying to foresee future customization/cutting points to reduce duplication across the customized repos (alt. 1, which is the approach we currently use) and in the base/core repo.</p>

<p>And yes, we do try to refactor mercilessly whenever we notice the core/customization split slipping :)</p>",5.0,2009-03-03 09:29:10.113000 UTC,2009-04-20 08:40:43.023000 UTC,13.0,[]
Magento workflow process with git,"<p>I've been coding <strong>Magento</strong> sites using cron jobs to handle backups but it gets a little clunky when trying to revert back to previous versions of the site. I essentially have to dig through the backups and manually revert things back. I've read up on git and I like the idea of being able to document my changes through the development cycle and revert back to just the specific things I change with a few commands. </p>

<p>Currently I have two branches in my local repo (master, develop). I essentially do all my development in the develop branch and merge everything over to the master branch when I'm done with a specific set of tasks and push things up to my gitbucket repo for backup purposes and just to get into the habbit of using a remote repo. </p>

<p>I'm new to git so does this sound sufficient for my scenario or can anyone recommend a good workflow process for a one man shop using git with Magento? Thanks.</p>",1,0,2012-09-02 22:59:33.830000 UTC,7.0,,3,git|magento|workflow|dvcs|repository,4825,2010-01-02 18:01:05.097000 UTC,2016-08-24 02:27:04.730000 UTC,"Orlando, FL",533,9,0,55,"<p>We use the <a href=""http://nvie.com/posts/a-successful-git-branching-model/"">GitFlow</a> model for our Magento development.  It's similar to yours but with an extra branch for a staging site.  Larger development tasks/fixes are also completed in separate branches to ensure develop is always reasonably stable.  Our staging sites are a git repo with the staging branch checked out and production sites have the master branch checked out.  When we're ready to deploy changes to staging we merge them to staging on our local machines, and push to a central git repo, then pull on the server.  Works well, but you need to be careful of app/etc/local.xml as this should be different in each environment.  You'll also want to make sure things like media and var folders do not end up in your git repo.</p>

<p><a href=""https://github.com/github/gitignore/blob/master/Magento.gitignore"">Github publishes a .gitignore</a> for Magento, but the issue I have with it is that it excludes all of the Magento core files. You end up with a nice clean repository that contains only your changes, but not something that's deployable. We use a much simpler .gitignore which basically just excludes media var and app/etc/local.xml. We generally use a single remote (e.g. Github) and deploy from there. </p>

<p>On the servers we're deploying to, we generally have shell access and git installed so deployment is a matter of ssh'ing in and running a git pull (or fetch &amp; merge) from the normal remote.</p>

<p>We maintain staging sites as a separate branch in git, and deploy them the same way.  So our process looks like Develop in feature branch and merge into ""develop"" when done.  When integration testing is complete, merge ""develop"" (or cherry-pick changes) to ""staging"" and deploy.  When ready for production merge ""staging"" (or cherry-pick individual changes) to the ""master"" branch.  This is basically <a href=""http://nvie.com/posts/a-successful-git-branching-model/"">GitFlow</a> but out ""preparation for release"" branch is long lived.</p>

<p>As the <a href=""http://www.sonassi.com/knowledge-base/our-magento-git-guide-and-work-flow/#git-create-staging-environment"">Sonassi tutorial</a> linked in the comments points out, the issue with symlinking media is that if you delete a media file from production you get a broken link in staging and vice versa.  Instead of linking the two, we update staging code from git, but otherwise run the staging and production sites separately.  If we need fresh data in staging, for any reason we'll take a copy of the production media and database and restore it to the staging site.</p>

<p>The Gitflow model uses version numbers for release tags, if you have version numbers which the team agrees on for releases, use them.  Otherwise, if you have some sort of milestone plan, or sprint system you're working to and can identify a release that way, that would work also.  If all else fails we use the date/time the deployment occurred. e.g.</p>

<pre><code>git tag -a deployed-`date +%Y%m%d-%H%M` -m ""Code release at `date`""
</code></pre>",10.0,2012-09-03 00:36:30.430000 UTC,2012-09-04 23:17:08.337000 UTC,11.0,[]
Mercurial — revert back to old version and continue from there,"<p>I'm using Mercurial locally for a project (it's the only repo there's no pushing/pulling to/from anywhere else).</p>

<p>To date it's got a linear history.  However, the current thing I'm working on I've now realized is a terrible approach and I want to go back to the version before I started it and implement it a different way.</p>

<p>I'm a bit confused with the <code>branch</code> / <code>revert</code> / <code>update -C</code> commands in Mercurial.  Basically I want to revert to version 38 (currently on 45) and have my next commits have 38 as a parent and carry on from there.  I don't care if revisions 39-45 are lost for ever or end up in a dead-end branch of their own.</p>

<p>Which command / set of commands do I need?</p>",7,1,2010-03-29 18:53:43.253000 UTC,80.0,2013-12-13 11:54:24.167000 UTC,249,version-control|mercurial|branch|dvcs|revert,175386,2009-12-03 12:46:37.193000 UTC,2022-03-03 17:01:10.573000 UTC,,21298,427,24,791,"<pre><code>hg update [-r REV]
</code></pre>

<p>If later you commit, you will effectively create a new branch. Then you might continue working only on this branch or eventually merge the existing one into it.</p>",4.0,2010-03-29 18:56:50.313000 UTC,,151.0,[]
Mercurial: Merging one file between branches in one repo,"<p>When I have two branches in Hg repo, how to merge only one file with another branch, without having all other files from changeset merged?</p>

<p>Is it possible to merge only certain files, instead of whole changeset?</p>",4,0,2009-07-03 11:09:22.183000 UTC,18.0,2012-03-30 12:24:09.040000 UTC,51,version-control|mercurial|dvcs|branching-and-merging,18592,2008-11-07 09:59:08.750000 UTC,2019-09-24 13:43:01.497000 UTC,Serbia,7002,448,19,301,"<p>WARNING: such a &quot;dummy merge&quot;, as is <a href=""https://stackoverflow.com/a/1079497/3195477"">recommended by @Martin_Geisler</a>, can really mess you up, if later you want to do a true merge of the two branches. The dummy merge will be recorded, and say that you merge into the branch you did the dummy merge to -- you will not see the changes. Or if you merge into the other branch, the changes on that other branch will be undone.</p>
<p>If all you want is to copy an entire file from one branch to another, you can simply do:</p>
<pre><code>   hg update -r to-branch
   hg revert -r from-branch file
   hg ci -m 'copied single file from from-branch to to-branch
</code></pre>
<p>If you want to select different parts of that file, then <code>&quot;hg record&quot;</code> is useful.</p>
<p>I just did this on my home directory .hgignore.</p>
<p>If both branches have made changes to a file that you want to keep, a dirty trick would be to create a merge of the two branches using hg merge, possibly/probably on still another branch, check that in, and then copy a single file between the merge and the to-branch:</p>
<pre><code>   hg update -r to-branch
   branch merge-branch
   hg merge -r from-branch
   hg ci -m 'temp merge to be discarded&quot;
   hg update -r to-branch
   hg revert -r merge-branch single-file
   hg ci -m 'merged single-file from from-branch to to-branch&quot;
   hg strip merge-branch
</code></pre>
<p>It is worth mentioning: the way to &quot;copy a single file between branches&quot; (or revisions, or from revision to merge, or....) is &quot;hg revert&quot;. I.e.</p>
<pre><code>   hg update -r Where-you-want-to-copy-to
   hg revert -r Where-you-want-to-copy-from file-you-want-to-copy
   ...
   hg ci
</code></pre>
<p>For some reason I, and some of my coworkers, find this <em>VERY</em> confusing.   &quot;revert&quot;==&quot;copy&quot; ...  makes sense for some usage patterns, but not all.</p>",11.0,2012-10-26 04:19:10.437000 UTC,2021-06-08 12:29:49.697000 UTC,59.0,[]
Combine DVCS with Visual Source Safe,"<p>I'm forced to use Visual Source Safe 2005 at work. I'd like to combine that with a DVCS, so that I can check in files locally without disrupting my co-workers if there's a bug or it doesn't compile.</p>

<p>In my attempts with Mercurial, it works, but causes a few weird issues. Namely, it thinks someone else has checked out the files I have checked out.</p>

<p>Here's my thoughts on how I should manage it:</p>

<ol>
<li>Disable auto-checkout.</li>
<li>Work locally in Mercurial</li>
<li>When I'm ready to push my changes...

<ol>
<li>Clone my Mercurial repository.</li>
<li>Update my Visual Source Safe repository</li>
<li>Pull and merge the two repositories using Mercurial.</li>
<li>Check everything into Visual Source Safe.</li>
</ol></li>
</ol>

<p>Does this sound reasonable? I'm always hearing bad things about VSS, is this just asking for me to see those problems firsthand?</p>",1,4,2009-05-01 16:00:54.717000 UTC,11.0,2010-05-23 04:26:35.767000 UTC,16,mercurial|visual-sourcesafe|dvcs|visual-sourcesafe-2005,1687,2008-10-08 14:10:23.583000 UTC,2019-12-08 01:06:30.620000 UTC,"Asheville, North Carolina United States",2613,59,1,80,"<p>WBlasko</p>

<p>I've found the same problem. I wanted to change files and merge them when needed instead of waiting for some other developer to unlock it. The solution that worked for me was:</p>

<p>1) Get the latest version of a VSS project (I placed all VSS projects under vss):</p>

<pre><code>c:\vss\projectA
</code></pre>

<p>2A) Initialize with Mercurial</p>

<pre><code>cd vss\projectA
C:\vss\projectA&gt;hg init
</code></pre>

<p>2B) Clone the project to the place where it could be changed at will</p>

<pre><code>hg clone vss\projectA myProjects\projectA
</code></pre>

<p>3) Grab the latest changes from the VSS copy (skip if you came from 1 and 2)</p>

<pre><code>C:\myProjects\projectA&gt;hg pull
C:\myProjects\projectA&gt;hg update
(solve conflicts if any)
</code></pre>

<p>4) Work at will with the cloned version. Later, push your work to the vss copy:</p>

<pre><code>C:\myProjects\projectA&gt;hg push
(don't run hg update yet, wait for VSS latestes version)
</code></pre>

<p>5) Now, perform a checkout of all files to the VSS project</p>

<p>6) Run ""hg update"" on the VSS project to merge your changes to the latest VSS changes.</p>

<pre><code>C:\vss\projectA&gt;hg update
(if there are conflicts, resolve them)
</code></pre>

<p>7) Commit the changes</p>

<pre><code>C:\vss\projectA&gt;hg commit
</code></pre>

<p>8) Perform a VSS checkin (releasing the locks to the other folks)
Go back to step 3. repeat steps 3-8 forever then... ;-)</p>

<p>This way you can work with a good version control system while still being able ""talk"" to legacy projects. You will be also be able to enjoy:
a) No problem with locked files
b) you can share your repository with others that know how to use Hg
c) make branches , etc</p>

<p>Just be carefull to first update/solve conflicts, test and then perform VSS checkin</p>

<p>Cheers,
Luis</p>",4.0,2009-06-04 21:53:43.453000 UTC,,13.0,[]
How to deal with Git submodules on a repo that is converted to Mercurial,"<p>Here goes:</p>

<pre><code>$ cat .gitmodules 
[submodule ""utils/external/firepython""]
    path = utils/external/firepython
    url = git://github.com/darwin/firepython.git
[submodule ""utils/external/textile""]
    path = utils/external/textile
    url = git://github.com/jsamsa/python-textile.git
</code></pre>

<p>While this was still a Git repo, I needed to run <code>git submodule init</code>, after which some magic happens. Since I've now converted the repo to Mercurial (using <code>hgext.git</code> extension), I don't know what to do. Is there an equivalent process (I need those 2 Git modules in my Mercurial repo)?</p>",1,0,2011-04-10 14:16:35.003000 UTC,1.0,2013-04-06 17:28:10.877000 UTC,10,git|mercurial|dvcs|git-submodules|mercurial-subrepos,1571,2010-04-20 21:30:46.187000 UTC,2022-03-02 01:27:48.800000 UTC,Johannesburg,11290,3210,378,26893,"<p>Mercurial supports <a href=""https://www.mercurial-scm.org/wiki/Subrepository"" rel=""nofollow noreferrer"">subrepositories</a> of different kinds: Mercurial, Subversion, and Git. So you can create a <code>.hgsub</code> file with</p>

<pre><code>utils/external/firepython = [git]git://github.com/darwin/firepython.git
utils/external/textile    = [git]git://github.com/jsamsa/python-textile.git
</code></pre>

<p>and that will inform Mercurial to make a clone of your Git repositories when the Mercurial repository is cloned. You need to make the Git clones yourself the first time, or copy them from somewhere else on your disk:</p>

<pre><code>$ git clone git://github.com/darwin/firepython.git utils/external/firepython
$ git clone git://github.com/jsamsa/python-textile.git utils/external/textile
$ hg add .hgsub
$ hg commit -m 'Added Git subrepositories'
</code></pre>

<p>You will then note that Mercurial has added a <code>.hgsubstate</code> file to your repository where it stores information about the Git subrepositories. This file is needed so that Mercurial knows which revision to checkout from your subrepositories when you make a new Mercurial clone.</p>

<p>A colleague of mine has written a <a href=""http://mercurial.aragost.com/kick-start/en/subrepositories/"" rel=""nofollow noreferrer"">subrepository guide</a> that you might find useful.</p>",0.0,2011-04-10 16:08:55.493000 UTC,2018-02-22 12:53:05.460000 UTC,13.0,[]
How can I get the same results from Amazon Neptune that I do from Cosmos DB?,"<p>Using Gremlin.Net 3.3.2 I am getting very different results from Neptune and Cosmos DB. Graph data is the same on both platforms. Cosmos DB gives me everything I need (vertex id, label and properties). </p>

<p>When query is made to Neptune using Gremlin.Net I only get the vertex Id and label. Is this a bug with Neptune and Gremlin.net? Bug with Neptune? </p>

<p>If execute the query in the gremlin console Neptune returns all the data so problem appears to be confined to Gremlin.Net.</p>

<p>query = g.V().has('name',within('wind'))</p>

<pre><code>Amazon Neptune results
{
  ""Id"": ""14b15642-842f-888a-a28e-3ed117a07d5b"",
  ""Label"": ""keyword""
}

Cosmos DB results
{
  ""id"": ""wind"",
  ""label"": ""keyword"",
  ""type"": ""vertex"",
  ""properties"": {
    ""popularity"": [
      {
        ""id"": ""8f9967f1-cead-41d6-a432-de025d9dc14b"",
        ""value"": ""16""
      }
    ],
    ""name"": [
      {
        ""id"": ""fb90af3f-828b-4cc0-b9f8-b571a30c6b14"",
        ""value"": ""wind""
      }
    ]
  }
}
</code></pre>",1,0,2018-04-11 02:38:17.830000 UTC,,,1,gremlin|amazon-neptune,702,2018-02-05 20:35:56.270000 UTC,2022-02-11 20:58:28.557000 UTC,,49,6,0,5,"<p>Neptune is a bit more in line with the expected output that TinkerPop itself would provide, whereas CosmosDB returns an older approach. TinkerPop recommends the return of ""references"" to graph elements (i.e. id and label and not properties) and that appears to be what Neptune provides. I don't know if Neptune can be configured to behave differently.</p>

<p>While it may not seem convenient, the reason TinkerPop recommends this approach is that users should only return the data that they request. For instance, you typically wouldn't do <code>SELECT * FROM table</code> for a SQL query - you would include the fields that you wanted returned in the <code>SELECT</code> clause. For the same reasons you do that in SQL, you would do that in Gremlin. </p>

<p>Also, returning all properties on an element could be massively expensive. It's hard for TinkerPop to recommend returning anything other than a reference because of multi-properties. If a <code>Vertex</code> can hold millions of properties, the last thing we'd want to see happen is for the element to default serialize with all of those properties.</p>

<p>Unfortunately, much of this thinking was not clear in the TinkerPop community when we started down the path of defining IO formats. OLAP was still a bit of an experiment, GLVs were not a thought, etc. and so the idea of ""reference elements as a default"" didn't come until in later releases. Hopefully we can make the IO more consistent for TinkerPop 4.x some day.</p>

<p>The way to get the same results would be to follow TinkerPop's recommendations and avoid returning graph elements. The best approach would probably be to use <code>project()</code> or <code>valueMap()</code> in some form:</p>

<pre><code>g.V().valueMap('popularity','name')
g.V().
  project('popularity','name').
    by('popularity').
    by('name')
</code></pre>

<p>Note while <code>project()</code> is a bit more verbose in the example it does provide a more compact output because it doesn't embed the value of each key in a <code>List</code> the way <code>valueMap()</code> does. The above will coerce results to <code>Map</code> so that they will be consistent across all platforms.</p>",0.0,2018-04-11 10:40:58.303000 UTC,2018-04-11 16:37:32.857000 UTC,8.0,[]
"How to collect all vertex and edge properties along path, with Gremlin","<p>Here's a really simple query:</p>

<pre><code>g.V('customerId').out().path()
</code></pre>

<p>The JSON output of this is </p>

<pre><code>{  
   ""requestId"":""96b26c1d-d032-2004-d36e-c700bd6db2a2"",
   ""status"":{  
      ""message"":"""",
      ""code"":200,
      ""attributes"":{  
         ""@type"":""g:Map"",
         ""@value"":[  

         ]
      }
   },
   ""result"":{  
      ""data"":{  
         ""@type"":""g:List"",
         ""@value"":[  
            {  
               ""@type"":""g:Path"",
               ""@value"":{  
                  ""labels"":{  
                     ""@type"":""g:List"",
                     ""@value"":[  
                        {  
                           ""@type"":""g:Set"",
                           ""@value"":[  

                           ]
                        },
                        {  
                           ""@type"":""g:Set"",
                           ""@value"":[  

                           ]
                        }
                     ]
                  },
                  ""objects"":{  
                     ""@type"":""g:List"",
                     ""@value"":[  
                        {  
                           ""@type"":""g:Vertex"",
                           ""@value"":{  
                              ""id"":""customerId"",
                              ""label"":""customer""
                           }
                        },
                        {  
                           ""@type"":""g:Vertex"",
                           ""@value"":{  
                              ""id"":""e:customerIdemail@email.com"",
                              ""label"":""email""
                           }
                        }
                     ]
                  }
               }
            }
         ]
      },
      ""meta"":{  
         ""@type"":""g:Map"",
         ""@value"":[  

         ]
      }
   }
}
</code></pre>

<p>Now, a customer vertex also contains the property name and age. What I would like to understand, is how to (simply, if possible) form my gremlin query such that it nests of the vertex properties within the graph. Note that when I just run g.V(""customerId""), the response does contain these properties.</p>",1,0,2018-07-26 20:20:53.747000 UTC,1.0,,4,gremlin|tinkerpop|amazon-neptune,8559,2013-05-03 19:22:22.747000 UTC,2022-03-04 22:46:12.183000 UTC,Seattle,167,3,0,28,"<p>You should always specific exactly the data that you want returned in a traversal. Even for something as simple as:</p>

<pre><code>g.V('customerId')
</code></pre>

<p>you should really prefer:</p>

<pre><code>g.V('customerId').valueMap('name','age')
</code></pre>

<p>It's really no different in SQL where you likely wouldn't do</p>

<pre><code>SELECT * FROM customer
</code></pre>

<p>but instead</p>

<pre><code>SELECT name, age FROM customer
</code></pre>

<p>As for your question, you just need to specify the data you want back, so use the <code>by()</code> modulator for the <code>path()</code>:</p>

<pre><code>g.V('customerId').
  out().
  path().
    by(valueMap('name','age'))
</code></pre>

<p>That of course assumes your <code>out()</code> is also a ""customer"", if not, just add a second <code>by()</code> with the specific fields required for that. The <code>by()</code> modulators are applied in round-robin fashion. If you'd like a slightly cleaner bit of JSON to deal with you might instead use <code>project()</code> like:</p>

<pre><code>g.V('customerId').
  out().
  path().
    by(project('name','age').
         by('name').
         by('age'))
</code></pre>

<p>as that will kill out the embedded lists that <code>valueMap()</code> adds in to properly account for multi-properties.</p>

<p>As of TinkerPop 3.4.4, you would also consider <code>elementMap()</code>-step which includes more of the structure of the graph element.</p>

<pre><code>gremlin&gt; g.V().has('person','name','marko').elementMap()
==&gt;[id:1,label:person,name:marko,age:29]
gremlin&gt; g.V().has('person','name','marko').elementMap('name')
==&gt;[id:1,label:person,name:marko]
gremlin&gt; g.V().has('person','name','marko').properties('name').elementMap()
==&gt;[id:0,key:name,value:marko]
gremlin&gt; g.E(11).elementMap()
==&gt;[id:11,label:created,IN:[id:3,label:software],OUT:[id:4,label:person],weight:0.4]
</code></pre>",5.0,2018-07-26 20:30:01.727000 UTC,2019-09-25 11:05:35.410000 UTC,15.0,[]
Git under windows: MSYS or Cygwin?,"<p>I plan to migrate my projects over to git, and I'm currently wondering which is the best and / or most stable option under windows.</p>

<p>From what I gather I basically have 2.5 options:</p>

<ol>
<li><a href=""http://code.google.com/p/msysgit/"" rel=""noreferrer"">MSYSgit</a></li>
<li>git under <a href=""http://www.cygwin.com/"" rel=""noreferrer"">Cygwin</a></li>
<li>(aka 2.5) MSYSgit from a Cygwin prompt (given that Cygwin git is already installed).</li>
</ol>

<p>Note: IMO Cygwin in itself is a big plus as you can have access to pretty much all the *nix command line tools, as where with MSYSgit bash, you only have access to a rather small subset of these tools.</p>

<p>Given that, what option would you suggest?</p>",13,0,2009-04-23 23:01:44.227000 UTC,36.0,2009-04-24 16:27:58.953000 UTC,80,git|cygwin|msysgit|dvcs,59048,2009-03-25 16:24:29.727000 UTC,2022-02-23 01:22:44.440000 UTC,,9136,1389,190,1710,"<p>Edit (2 more years later: October 2014)</p>
<p><a href=""http://episodes.gitminutes.com/2014/04/gitminutes-28-johannes-schindelin-on.html"" rel=""noreferrer"">Johannes Schindelin</a> <a href=""http://osdir.com/ml/msysgit/2014-10/msg00071.html"" rel=""noreferrer"">just explained</a> (Oct. 2014) that <strong>msysgit is phased out</strong>:</p>
<blockquote>
<p>we now have a <strong>light-weight Git for Windows SDK</strong> – which is essentially a standard MinGW/MSys system managed through the package manager mingw-get.</p>
<p>We decided to just phase out the name &quot;msysGit&quot; (as well as the <a href=""https://github.com/msysgit/msysgit/wiki/Relationship-to-Git-for-Windows"" rel=""noreferrer"">GitHub org of the same name</a>) and work on <a href=""http://git-for-windows.github.io/"" rel=""noreferrer""><strong>Git for Windows</strong></a> (with the <a href=""https://github.com/git-for-windows"" rel=""noreferrer"">corresponding GitHub org)</a>, and using the name &quot;Git for Windows&quot; for the installer aimed at &quot;end-users&quot; and &quot;Git for Windows SDK&quot; for the development environment targeting Git for Windows developers).</p>
</blockquote>
<hr />
<p>Edit (3 years later: April 2012)</p>
<p>MsysGit (now <a href=""https://github.com/msysgit/msysgit"" rel=""noreferrer"">on GitHub</a>) is the way to go if you want a light-weight fast start in Git world: unzip the archive, launch <a href=""https://github.com/msysgit/msysgit/blob/devel/git-cmd.bat"" rel=""noreferrer""><code>git-cmd.bat</code></a> or <a href=""https://github.com/msysgit/msysgit/blob/devel/msys.bat"" rel=""noreferrer""><code>git-bash.bat</code></a> and you are done.<br />
Its <a href=""http://code.google.com/p/msysgit/downloads/list"" rel=""noreferrer"">latest release (1.7.10, April 2012)</a> now includes <a href=""https://stackoverflow.com/a/5855213/6309"">support for UTF-8</a>, also <a href=""https://github.com/jbialobr/gitextensions/commit/50bf972442629c798be20a377a4702fef4a0f443"" rel=""noreferrer"">included in GitExtension</a>.
Don't forget to set your <a href=""https://stackoverflow.com/questions/2333424/distributing-git-configuration-with-the-code/2354278#2354278""><code>autocrlf</code> to false</a> though.</p>
<p>If you are really missing all the other unix commands not packages with msysgit, simply download a release of <a href=""https://github.com/bmatzelle/gow/wiki"" rel=""noreferrer"">GoW (Gnu on Windows)</a>, aptly named &quot;The lightweight alternative to Cygwin&quot;.<br />
I mean: <a href=""https://github.com/bmatzelle/gow/wiki/executables_list"" rel=""noreferrer""><strong>130</strong> unix command</a>s...</p>
<p>Cygwin should be a backup solution only for certain cases, like the transfer speed of <em>large</em> Git repositories, as mentioned below by <a href=""https://stackoverflow.com/users/1203340/incrementor"">incrementor</a> in <a href=""https://stackoverflow.com/a/9237853/6309"">his answer</a>.</p>
<hr />
<p>June 2012: if you want to interact with <strong>GitHub</strong>, you now have <strong><a href=""http://windows.github.com"" rel=""noreferrer"">http://windows.github.com</a></strong> (see also &quot;<a href=""https://github.com/blog/1151-designing-github-for-windows"" rel=""noreferrer"">Designing GitHub for Windows</a>&quot;).<br />
It not only will configure the ssh keys for you (and add the generated public key to your GitHub profile), but it will also install, for all git operations, a... MsysGit.</p>
<hr />
<p>Original answer (April 2009)</p>
<p>I have no problem with the latest version of MsysGit:<br />
I use the <a href=""https://stackoverflow.com/questions/623518/msysgit-on-windows-what-should-i-be-aware-of-if-any"">option 2</a>, which means I only add the git\bin directory to the PATH environment variable, but without overriding some built-in windows tools.</p>
<p>I managed to defined external tools like <a href=""https://stackoverflow.com/questions/10564/how-can-i-set-up-an-editor-to-work-with-git-on-windows/773973#773973"">Notepad++</a> and <a href=""https://stackoverflow.com/questions/780425/how-do-i-setup-diffmerge-with-msysgit-gitk/783667#783667"">WinMerge (or DiffMerge)</a></p>
<p>I did run successfully scripts like <a href=""https://stackoverflow.com/questions/572893/cloning-a-non-standard-svn-repository-with-git-svn/572898#572898"">svn2git</a> because, from MsysGit1.6.2+, it does include the '<code>git svn</code>' command.</p>
<p>And finally I look forward to MsysGit1.6.2.3 for it <a href=""http://groups.google.com/group/msysgit/browse_thread/thread/23d0046b697bdcce/aa746a2e3eab37fa?lnk=raot"" rel=""noreferrer"">will include finally '<code>git filter-branch</code>' command</a>! (a slow version of filter-branch, but still).</p>
<p>For the Unix command, I prefer to install a <a href=""https://stackoverflow.com/questions/247234/do-you-know-a-similar-program-for-wc-unix-word-count-command-on-windows"">GnuWin32 coreutils package</a>, and use them only when I need them.</p>
<p>So all in all, I do not use the Cygwin layer, and managed to run Git very well in its Windows-based release MsysGit.</p>",9.0,2009-04-24 06:29:59.313000 UTC,2020-06-20 09:12:55.060000 UTC,60.0,[]
Are there any good graphical Git and Hg/Mercurial clients on Mac OS X?,"<p>I'm searching for compelling Git and Mercurial clients on Mac OS X. The most clients I've found so far were less compelling as I expected. Some of the clients are programmed even in Ruby or Tcl/Tk, which IMO aren't good OS X citizens in regard of integration in the OS.</p>

<p>I have clients similar to Versions.app or Cornetstone in mind, which are Subversion-only clients. Perhaps somebody got an insider tip for me.</p>",13,0,2009-07-04 10:38:42.337000 UTC,16.0,2012-01-31 20:08:41.230000 UTC,29,git|version-control|mercurial|client|dvcs,21133,2009-05-02 15:52:42.683000 UTC,2022-03-04 15:12:44.477000 UTC,"Gießen, Deutschland",5791,697,25,547,"<p>I just thought I'd mention that <a href=""http://www.sourcetreeapp.com"" rel=""noreferrer"">SourceTree</a> is a Mac OS X client for <em>both</em> Mercurial and Git, in one tool. I wasn't sure if you were looking for that, or just mentioned both because you hadn't decided which to use yet; personally as an open source developer / user, having both available in one tool is very useful to me (that's why I wrote it :))</p>",1.0,2010-11-22 17:20:11.327000 UTC,,34.0,[]
Is there any harmful commands using GIT and HG,"<p>I'm teaching HG to my students, as it is a good playskool DVCS (not powerful as GIT but simple to start working with trivial concepts). I use HG because it seems very difficult to destroy previous entries (with the exception of <code>hg rollback</code>), so you have always a chance to get back on train without destroying important data. But i was recently wondering:</p>

<ul>
<li>is it true ?</li>
<li>does GIT offers the same protection (i read somewhere about <code>rebase</code> option which can be very dangerous)</li>
</ul>",3,2,2011-04-30 14:16:37.447000 UTC,1.0,2012-12-01 23:10:11.287000 UTC,5,git|mercurial|dvcs,935,2009-10-29 18:44:47.117000 UTC,2022-02-24 11:09:44.527000 UTC,"Villeneuve-d'Ascq, France",7385,3474,140,724,"<p>Both are DVCSs, meaning that you’re supposed to clone the repo and push your changes somewhere when you want a backup copy. If you do this diligently, then it becomes irrelevant which destructive tools are available to you, since backups are cheap and easy to maintain.</p>

<p>Now, be warned that I’m a 100% biased Git bigot.</p>

<p>Out of the box, Mercurial only ships with one destructive command, rollback. Everything else is relegated to extensions which must be manually enabled — strip, rebase, etc. These extensions are most certainly destructive in that they rewrite history <em>in-place</em>, or they <em>destroy</em> it. To avoid using these, most Mercurial users prefer to use the Mercurial Queues extension, which lets you maintain changesets as flexible patches before setting them in stone as commits. This essentially amounts to an entire VCS which sits on top of Mercurial. It gets the job done, but it must be consciously applied before the commits are written.</p>

<p>By contrast, Git ships with several commands that may be considered “destructive”, but with one crucial difference — nothing inside Git’s database is <em>ever</em> rewritten. Whenever you use the rebase, filter-branch, or reset commands, <em>new</em> objects are created in the database, and then the branch pointer is updated to point to these. Every time a branch pointer is updated, an entry is appended to its reflog, a history log which sits on top of Git and protects your branch pointers from unwanted updates; it’s <em>always</em> possible to revert a “destructive” command. It can be difficult, in fact, to permanently remove an object from Git’s database — it involves dropping the reflog entry and then pruning the unref’d objects.</p>

<p>In short:</p>

<ul>
<li>Mercurial is safe by default, but adding chainsaws can completely break it.</li>
<li>Git is built out of chainsaws from the ground up, increasing apparent danger, but there are safeties.</li>
</ul>

<p>For your use case of teaching, these differences are generally irrelevant anyway — you’ll be teaching the basic commands, and if someone wants to explore, the only way they’ll learn is by chainsawing their arm off. It is said that Mercurial is easier for beginners to learn, but I feel that that’s mostly because it doesn’t expose the index and so is more like Subversion. Complete neophytes to version control might not benefit from this similarity.</p>",4.0,2011-04-30 14:58:32.697000 UTC,2011-04-30 15:06:17.907000 UTC,13.0,[]
How do I git reset --hard HEAD on Mercurial?,"<p>I'm a Git user trying to use Mercurial.</p>

<p>Here's what happened: I did a <code>hg backout</code> on a changeset I wanted to revert. That created a new head, so hg instructed me to merge (back to ""default"", I assume). After the merge, it told me I still had to commit. Then I noticed something I did wrong when resolving a conflict in the merge, and decided I wanted to have everything as before the <code>hg backout</code>, that is, I want this uncommited merge to go away. On Git this uncommited stuff would be in the index and I'd just do a <code>git reset --hard HEAD</code> to wipe it out but, from what I've read, the index doesn't exist on Mercurial. So how do I back out from this?</p>",4,1,2010-04-20 04:03:05.100000 UTC,9.0,,78,git|mercurial|merge|dvcs,60927,2009-05-12 04:09:51.713000 UTC,2022-03-04 12:10:02.203000 UTC,"Campo Grande, State of Mato Grosso do Sul, Brazil",8557,2202,14,459,"<p>If you've not yet commited, and it sounds like you haven't you can undo all the merge work with <code>hg update --clean</code>.</p>

<p>However, in newer mercurial's there's a handy command to re-merge a single file: <code>hg resolve path/to/file.ext</code>.  From the <code>hg help resolve</code>:</p>

<blockquote>
<pre><code>The available actions are: ...

 4) discard your current attempt(s) at resolving conflicts and
</code></pre>
  
  <p>restart
           the merge from scratch: ""hg resolve file..."" (or ""-a"" for all
           unresolved files)</p>
</blockquote>",2.0,2010-04-20 04:21:56.617000 UTC,,97.0,[]
"How do I ""switch"" with Mercurial","<p>How do I do what <code>svn switch</code> does, in Mercurial?</p>

<p>That is change my working directory to switch to another branch in the repository?</p>",5,0,2009-07-20 21:48:46.623000 UTC,11.0,2012-01-26 09:11:29.607000 UTC,64,svn|version-control|mercurial|dvcs,43729,2009-07-20 21:45:21.030000 UTC,2022-02-17 13:52:13.157000 UTC,Denmark,783,69,5,28,"<pre><code>hg update -r&lt;REV&gt;
</code></pre>

<p>The <a href=""http://www.selenic.com/hg/help/update"" rel=""noreferrer"">update</a> command will switch to a specified revision.</p>",3.0,2009-07-20 21:51:58.897000 UTC,2015-03-25 18:36:19.187000 UTC,65.0,[]
Benefits of using git branches vs. multiple repositories,"<p>We are doing development for automation code.</p>

<p>Our code automates the company's products and is <strong>synced</strong> to a particular product version.</p>

<p>Currently, we have 1 big Git repository with multiple branches in it - v1.0, v1.1, v2.0 (automation for version 1.0 goes in v1.0 branch, and so on).</p>

<p><strong>What are the advantages and disadvantages of having these in a single repository with branches vs. keeping each version code in a separate repository ?</strong></p>

<p>Both solutions can work, the answer i'm looking for is a <strong>list of pros/cons for either approach.</strong></p>

<p>I know that many teams are using branches to isolate temporary stages in development, such as doing a bug fixes or a new feature, merging back the work into the main development branch in the end.</p>

<p>Other modes of work i know are having different branches for development, release, etc, to separate ""cleaner"" revisions of the code from dirty ones that are constantly being worked on.</p>

<p>None of these sound similar to what we are currently doing though.</p>

<p>*Note that some modifications in a particular version we make are relevant for all product versions, while some are not.</p>",3,2,2012-08-08 21:34:21.867000 UTC,1.0,2015-07-15 19:21:19.257000 UTC,13,git|version-control|dvcs|git-branch,9518,2010-11-01 23:01:19.087000 UTC,2022-03-02 08:59:13.857000 UTC,,18570,2260,20,1022,"<h2>Multiple branches</h2>

<h3>Pros:</h3>

<ul>
<li>only one repo to manage (your automate points to one remote)</li>
<li>comparison (diff) between branches possible directly from that one repo</li>
<li>you can pull any of those branches from that one repo to any other downstream repos which might need it</li>
</ul>

<h3>Cons:</h3>

<ul>
<li>branch cluttering (you need to manage/delete the sum of the branches)</li>
<li>tags are for <em>all</em> the repo (not just for ""some products""</li>
</ul>

<h2>Multiple repos</h2>

<h3>Pros</h3>

<ul>
<li>you can pull from the main repo only what you need and work from there</li>
<li>you can clean easily old ""branches"" (just delete that specific repo)</li>
</ul>

<h3>Cons</h3>

<ul>
<li>repo duplication (takes more space)</li>
<li>repo management (you need to point to the right remote(s))</li>
</ul>

<hr>

<p>I would argue the single repo approach is a simpler and more classic approach.<br>
That said, if you have <em>many</em> versions (which need to be cleaned up regularly), isolating those transient versions in their own repo can work too.</p>",3.0,2012-08-09 06:13:34.763000 UTC,,9.0,[]
Does Git work in Windows?,"<p>I work on Linux all the time and I'm clueless about Windows, not even having a Windows box. Is Git nowadays working on Windows? Or am I making problems for my Windows pals by using it?</p>",8,0,2008-09-23 22:09:28.713000 UTC,4.0,2015-07-02 20:10:49.713000 UTC,16,windows|git|version-control|dvcs,3959,2008-09-12 08:42:06.883000 UTC,2022-03-05 22:44:33.340000 UTC,United Kingdom,264618,974,104,13115,"<p>As far as I can tell <a href=""http://code.google.com/p/msysgit/"" rel=""nofollow noreferrer"">msysgit</a> works perfectly well under Windows Vista.</p>

<p>This after a whole 2-month experience checking out plugins and applications for Ruby on Rails :-)</p>

<p>Anyway, it was a breeze to install, no problem.</p>",5.0,2008-09-23 22:26:49.777000 UTC,,14.0,[]
Merging: Hg/Git vs. SVN,"<p>I often read that Hg (and Git and...) are better at merging than SVN but I have never seen practical examples of where Hg/Git can merge something where SVN fails (or where SVN needs manual intervention). Could you post a few step-by-step lists of branch/modify/commit/...-operations that show where SVN would fail while Hg/Git happily moves on? Practical, not highly exceptional cases please...</p>

<p>Some background: we have a few dozen developers working on projects using SVN, with each project (or group of similar projects) in its own repository. We know how to apply release- and feature-branches so we don't run into problems very often (i.e., we've been there, but we've learned to overcome <a href=""http://hginit.com/00.html"" rel=""noreferrer"">Joel's problems</a> of ""one programmer causing trauma to the whole team"" or ""needing six developers for two weeks to reintegrate a branch""). We have release-branches that are very stable and only used to apply bugfixes. We have trunks that should be stable enough to be able to create a release within one week. And we have feature-branches that single developers or groups of developers can work on. Yes, they are deleted after reintegration so they don't clutter up the repository. ;)</p>

<p>So I'm still trying to find the advantages of Hg/Git over SVN. I'd love to get some hands-on experience, but there aren't any bigger projects we could move to Hg/Git yet, so I'm stuck with playing with small artificial projects that only contain a few made up files. And I'm looking for a few cases where you can feel the impressive power of Hg/Git, since so far I have often read about them but failed to find them myself.</p>",6,6,2010-03-19 08:18:57.770000 UTC,70.0,2012-02-24 19:53:22.363000 UTC,144,git|svn|mercurial|merge|dvcs,23750,2010-02-18 11:34:13.103000 UTC,2022-03-04 15:05:09.783000 UTC,Austria,6452,1130,203,296,"<p>I do not use Subversion myself, but from the <a href=""http://subversion.apache.org/docs/release-notes/1.5.html#merge-tracking"" rel=""nofollow noreferrer"">release notes for Subversion 1.5: Merge tracking (foundational)</a> it looks like there are the following differences from how merge tracking work in full-<a href=""https://en.wikipedia.org/wiki/Directed_acyclic_graph"" rel=""nofollow noreferrer"">DAG</a> version control systems like Git or Mercurial.</p>

<ul>
<li><p>Merging trunk to branch is different from merging branch to trunk: for some reason merging trunk to branch requires <code>--reintegrate</code> option to <code>svn merge</code>.</p>

<p>In distributed version control systems like Git or Mercurial there is no <em>technical</em> difference between trunk and branch: all branches are created equal (there might be <em>social</em> difference, though).  Merging in either direction is done the same way.</p></li>
<li><p>You need to provide new <code>-g</code> (<code>--use-merge-history</code>) option to <code>svn log</code> and <code>svn blame</code> to take merge tracking into account.</p>

<p>In Git and Mercurial merge tracking is automatically taken into account when displaying history (log) and blame.  In Git you can request to follow first parent only with <code>--first-parent</code> (I guess similar option exists also for Mercurial) to ""discard"" merge tracking info in <code>git log</code>.</p></li>
<li><p>From what I understand <code>svn:mergeinfo</code> property stores per-path information about conflicts (Subversion is changeset-based), while in Git and Mercurial it is simply commit objects that can have more than one parent.</p></li>
<li><p><em>""Known Issues""</em> subsection for merge tracking in Subversion suggests that repeated / cyclic / reflective merge might not work properly.  It means that with the following histories second merge might not do the right thing ('A' can be trunk or branch, and 'B' can be branch or trunk, respectively):  </p>

<pre>
*---*---x---*---y---*---*---*---M2        &lt;-- A
         \       \             /
          --*----M1---*---*---/           &lt;-- B
</pre>

<p>In the case the above ASCII-art gets broken: Branch 'B' is created (forked) from branch 'A' at revision 'x', then later branch 'A' is merged at revision 'y' into branch 'B' as merge 'M1', and finally branch 'B' is merged into branch 'A' as merge 'M2'.  </p>

<pre>
*---*---x---*-----M1--*---*---M2          &lt;-- A
         \       /           / 
          \-*---y---*---*---/             &lt;-- B
</pre>  

<p>In the case the above ASCII-art gets broken: Branch 'B' is created (forked) from branch 'A' at revision 'x', it is merged into branch 'A' at 'y' as 'M1', and later merged again into branch 'A' as 'M2'.</p></li>
<li><p>Subversion might not support advanced case of <a href=""http://revctrl.org/CrissCrossMerge"" rel=""nofollow noreferrer"">criss-cross merge</a>.</p>

<pre>
*---b-----B1--M1--*---M3
     \     \ /        /
      \     X        /
       \   / \      /
        \--B2--M2--*
</pre>

<p>Git handles this situation just fine in practice using ""recursive"" merge strategy.  I am not sure about Mercurial.</p></li>
<li><p>In <em>""Known Issues""</em> there is warning that merge tracking migh not work with file renames, e.g. when one side renames file (and perhaps modifies it), and second side modifies file without renaming (under old name).</p>

<p>Both Git and Mercurial handle such case just fine in practice: Git using <strong>rename detection</strong>, Mercurial using <strong>rename tracking</strong>.</p></li>
</ul>

<p>HTH</p>",5.0,2010-03-19 12:14:33.577000 UTC,2018-08-13 11:54:17.570000 UTC,91.0,[]
How to completely remove .svn files in Git,"<p>I am trying to remove all .svn files and folders for my repo. I am using Git. But I downloaded the source from a server which came with all the nasty .svn files. I don't want to committ these. I have already created a .gitignore file to completely ignore .svn files but this isn't working. </p>

<p>I have already searched and read loads of answers on the web and on Stack Overflow. I tried using </p>

<pre><code>find . -type d -name '.svn' -exec git rm -rf {} \;
</code></pre>

<p>To completely remove .svn files but they still linger. I am using Source Tree and tried Git Tower, but all the .svn files still appear as pending. </p>

<p>It maybe that I have previously accidentally committed a .svn file so its still appearing. Any clues how to ignore them totall or remove them would be a god send. </p>",1,0,2012-04-12 20:05:39.510000 UTC,5.0,,14,git|svn|dvcs,3108,2010-11-19 17:01:07.303000 UTC,2016-08-18 12:59:34.953000 UTC,,283,13,0,26,"<pre><code>find . -name '.svn' -type d -print0 | xargs -0 rm -rf
git commit -a -m ""Deleting all .svn folders and files""
</code></pre>

<p>You can also update the .gitignore file afterwards.</p>",2.0,2012-04-12 20:12:51.760000 UTC,,21.0,[]
What is a practical workflow for keeping local changes uncommitted in git?,"<p>What I want to do is already described in <a href=""https://stackoverflow.com/questions/4377309/git-stashing-specific-files-before-a-commit"">that question</a>. But I want to solve that problem in a practical and more generic way. So the use-case is the following:</p>

<ul>
<li>I have several local changes in several files <code>web.config</code>, <code>createDb.sql</code> or in any others</li>
<li>I don't want to commit those files, since changes are specific to my local machine only</li>
<li>Those files <strong>must</strong> be version controlled and moreover changes are made pretty often to some of them (sql scripts in particular), so I want to receive updates for those files</li>
<li>I <strong>do</strong> want to commit all other files</li>
<li>I want to be able to do that without friction, in one command (using posh-git, so powershell is welcome)</li>
</ul>

<p>The linked-to solution said to use <code>git add -p</code> and that is <strong>not practical</strong>, it is boring to pick chunks manually all the time or maybe there is a more convenient way of doing that?</p>

<p>For instance, one of the following could work:</p>

<ul>
<li>if there is an ability to filter files that are in my working copy before adding them to index, something like <code>git -add -A -EXCEPT web.config crateDb.sql</code>. Then I can map a git alias to that and that's it.</li>
<li>if there is an ability to de-apply stash. I mean to remove changes that is contained in one specific stash from the working copy. So I will have to do something like <code>git stash -deapply</code> before each commit that is also fine.</li>
</ul>

<p>The problem is very common and it is weird that there is no solution currently. E.g. TortoiseSVN has ""ignore-on-commit"" feature, Perforce allows to store that type of local changes in a separate changelist and never submit it.</p>",1,2,2012-04-19 20:21:09.657000 UTC,2.0,2019-03-07 14:34:12.250000 UTC,10,git|version-control|dvcs,268,2009-06-08 22:24:22.947000 UTC,2022-02-15 03:20:04.853000 UTC,San Francisco,5747,1176,109,961,"<p>You could try doing the following before your git commit:</p>
<pre><code>git update-index --assume-unchanged web.config crateDb.sql
</code></pre>
<p>From git help:</p>
<blockquote>
<p><strong>--assume-unchanged</strong></p>
<p><strong>--no-assume-unchanged</strong></p>
<p>When these flags are specified, the object names recorded for the
paths are not updated. Instead, these options set and
unset the &quot;assume unchanged&quot; bit for the paths. When the &quot;assume
unchanged&quot; bit is on, git stops checking the working tree files for
possible modifications, so you need to manually unset the bit to tell
git when you change the working tree file. This is sometimes helpful
when working with a big project on a filesystem that has very slow
lstat(2) system call (e.g. cifs).</p>
<p><strong>This option can be also used as a coarse file-level mechanism to
ignore uncommitted changes in tracked files (akin to what .gitignore
does for untracked files).</strong> Git will fail (gracefully) in case it needs
to modify this file in the index e.g. when merging in a commit; thus,
in case the assumed-untracked file is changed upstream, you will need
to handle the situation manually.</p>
</blockquote>",3.0,2012-04-19 20:26:55.353000 UTC,2020-06-20 09:12:55.060000 UTC,10.0,[]
Git pull from another repository,"<p>I have a repository called <code>Generic</code>, which is a generic application. I have forked it into a repository called <code>Acme</code>, which just builds upon the application stored <code>Generic</code> repository and adds Acme Co branding to it.</p>

<p>If I make changes to the core functionality in <code>Generic</code>, I want to update the <code>Acme</code> repository with the latest changes I have made to the core functionality in <code>Generic</code>. How would I do that?</p>

<p>As far as I can tell, I am essentially trying to merge the changes made in an upstream repository into the current fork.</p>

<p>If it means anything, I'm trying to do this because I have a generic application that I then build upon and brand for individual clients (like <code>Acme</code> in this example). If there is a cleaner way of doing this, let me know.</p>",3,0,2014-07-18 01:17:13.707000 UTC,50.0,2018-03-19 15:44:41.553000 UTC,120,git|version-control|repository|dvcs,79353,2012-10-25 23:39:22.740000 UTC,2021-12-08 21:06:52.833000 UTC,"Toronto, Canada",1461,77,0,52,"<p>Issue the following command in your <code>Acme</code> repo. It adds a new remote repository named <code>upstream</code> that points to the <code>Generic</code> repo.</p>

<pre><code>git remote add upstream https://location/of/generic.git
</code></pre>

<p>You can then merge any changes made to <code>Generic</code> into the current branch in <code>Acme</code> with the following command:</p>

<pre><code>git pull upstream
</code></pre>

<p>If you just want it to download the changes without automatically merging, use <code>git fetch</code> instead of <code>git pull</code>.</p>

<p>If you want to disable pushing to that repository, set the push URL to an invalid URL using something like</p>

<pre><code>git config remote.upstream.pushurl ""NEVER GONNA GIVE YOU UP""
</code></pre>

<p>Git will now yell at you about not being able to find a repo if you try to push to <code>upstream</code> (and sorry about the Rickroll, but it was the first random string that popped into my head).</p>",7.0,2014-07-18 01:45:28.950000 UTC,2014-07-18 03:24:15.743000 UTC,169.0,[]
Git create branch from current checked out master?,"<p>There is a git controlled folder on a server  where the main branch is checked out and a whole pile of files have been modified and not committed. Is there a way for me to commit the changes to a separate branch so I can go back to a clean version?</p>

<p>ie  I want to effecitvely undo all this persons changes but store them in another chance so if that person wants their changes they can switch to that branch.</p>

<p>(Yes I know this is not how git is designed to work but that is my situation!) Any ideas very much appreciated.</p>",3,2,2009-09-21 06:41:09.750000 UTC,22.0,2012-01-31 01:20:58.223000 UTC,79,git|version-control|dvcs|git-branch,96409,2009-08-04 07:45:18.977000 UTC,2010-11-01 05:35:29.710000 UTC,Australia,6930,374,19,340,"<p>First of all moving to a different branch based in the current HEAD is performed like this:</p>

<pre><code>git checkout -b newbranch
</code></pre>

<p>Commit all the changes (assuming no newly added files, otherwise <code>git add</code> them):</p>

<pre><code>git commit -a
</code></pre>

<p>Go back to the master branch:</p>

<pre><code>git checkout master
</code></pre>

<p>The previously uncommitted changes will all be on the newbranch branch, and master will still be at the state it was without those changes.</p>",2.0,2009-09-21 06:52:28.887000 UTC,,138.0,[]
How do I keep an svn:external up to date using git-svn?,"<p>Treating my repository as a SVN repo, I get:</p>

<pre><code>svn co http://myrepo/foo/trunk foo
...
foo/
  bar/
  baz/ -&gt; http://myrepo/baz/trunk
</code></pre>

<p>Treating it as a Git repo, I get:</p>

<pre><code>git svn clone http://myrepo/foo --trunk=trunk --branches=branches --tags=tags
...
foo/
  bar/
</code></pre>

<p>I can clone baz to my local machine elsewhere and add a symlink, but that's just a hack.  Is there a way to have <code>git svn rebase</code> automatically pull in those changes when it updates everything else, just like <code>svn up</code> does?</p>",8,1,2008-12-08 19:52:06.773000 UTC,28.0,2010-08-16 20:47:42.123000 UTC,52,svn|git|version-control|dvcs,27193,2008-08-13 12:15:38.537000 UTC,2022-03-04 22:02:09.107000 UTC,United States,61968,1995,71,4220,"<p>The solution I ended up using was just to symlink to other <code>git-svn</code> clones on my local box.  This worked pretty well: it allows me to commit changes back, and it allows me to make local changes on project A just to get them into project B.</p>",0.0,2009-01-23 18:47:45.377000 UTC,,25.0,[]
How can I host a Mercurial repository for several users myself?,"<p>There are a number of hosting services for git, Mercurial, etc. I need to host my own, because some of the data we want to keep there is of a sensitive nature, and we cannot move it off-site.</p>

<p>I need to host a Mercurial repository on a server so that a number of people can access it to both pull changes and push their own work. There needs to be some kind of password protection, and it could also use ssh. I would prefer HTTP(S) as the transfer protocol (a la Google Code) though, if possible.</p>

<p>Can somebody point me to a description of how to do this? I have only found descriptions for hosting single-user repositories and hosting services so far, but nothing about doing the multi-user hosting myself.</p>",1,1,2009-05-19 18:06:22.357000 UTC,12.0,,21,mercurial|hosting|dvcs|collaboration,7716,2009-05-17 01:01:08.977000 UTC,2015-01-23 06:14:35.190000 UTC,,583,16,2,67,"<p>Have a look at the <a href=""http://www.selenic.com/mercurial/wiki/index.cgi/PublishingRepositories"" rel=""noreferrer"">PublishingRepositories</a> page on the mercurial site. It will tell you how to set up the <code>hgwebdir.cgi</code> script for serving multiple repos over https.</p>

<p>Once you've configured it, I would recommend adding this to your hgrc:</p>

<pre><code>[web]
style = gitweb
</code></pre>

<p>I find it's a bit nicer theme than the default.</p>",1.0,2009-05-19 18:19:45.750000 UTC,,22.0,[]
"Gremlin Python: How to use coalesce to get vertex if exists, else insert","<p>Is it possible to use the Gremlin <code>coalesce</code> step to select a vertex by id (or properties) if such a vertex exists, otherwise insert the vertex? I've attempted to do so with the following, but I get a <code>'list' object has no attribute 'coalesce'</code> error, which I can see is caused by <code>.fold().next()</code> returning a python list object:</p>

<pre><code>    my_vertex = g.V(1).fold().next().coalesce(__.unfold(), g.addV(
         'my_label'
        ).property(
            T.id,
            1
        ).property(
            'test', 'foo'
        ).property(
            'name', 'my_vertex_name'
        ).property(
            'created_timestamp', 1575480467
        ).next()
    )
</code></pre>

<p>Is there any performance benefit to doing it this way, or should I simply break this into an if/else on the hasNext() of the initial vertex query?</p>",2,0,2019-12-04 17:33:08.497000 UTC,,2019-12-04 18:25:20.090000 UTC,0,gremlin|amazon-neptune|gremlinpython,440,2013-06-11 22:07:42.193000 UTC,2022-02-17 20:21:00.920000 UTC,Michigan,3146,1124,49,222,"<p>I was able to accomplish the ""get if exists, else insert"" by moving the next() terminal step to the end of the traversal as shown below:</p>

<pre><code>my_vertex = g.V(1).fold().next().coalesce(__.unfold(), g.addV(
         'my_label'
        ).property(
            T.id,
            1
        ).property(
            'test', 'foo'
        ).property(
            'name', 'my_vertex_name'
        ).property(
            'created_timestamp', 1575480467
        )
    ).next() //notice that .next was simply moved to the close of the .coalesce step
</code></pre>",0.0,2019-12-12 21:35:52.897000 UTC,2020-02-11 18:39:19.710000 UTC,-1.0,[]
Is Git a good version control system for web development (HTML/CSS/Javascript) on a corporate project?,"<p>I'm currently involved in a large project - the redevelopment of a corporate website.</p>

<p>The project involves many staff across several teams (content, design, etc).</p>

<p>There is a small team (myself and another developer) in charge of the front-end/presentation layer of the system - that is - the development of templates using HTML, CSS and Javascript.</p>

<p>Code quality, iterative development and frequent testing are an important aspect of the project.</p>

<p>The back-end development team currently use CVS; however it's taking IT so long to give access to my team that I'm considering setting up our own version control, through say GitHub.</p>

<p>Would Git bring any particular benefits to front-end development in this environment?</p>

<p>(I understand the basic principles of Git; the other developer doesn't have any experience with it, but would be happy to pick it up.)</p>",3,4,2010-06-21 00:22:02.350000 UTC,1.0,,9,git|cvs|dvcs|corporate,1939,2008-09-29 04:41:59.153000 UTC,2022-03-01 11:30:35.353000 UTC,Australia,30748,984,25,2470,"<p>git will be no less appropriate for a corporate environment than something like CVS or SVN. There is nothing in the design or implementation that makes it inappropriate for the corporate environment.</p>

<p>You'll probably end up using a hybrid approach: each developer will have a local git repository and will push back to an internal origin (a central spoke) to keep with the corporation's backup policies and to give the the boss that warm and fuzzy, 'my code is safe' on the repository.corpration.com machine. As you mentioned, github can act as your origin.</p>

<p>Have a look at <a href=""http://en.wikipedia.org/wiki/Git_(software)#Projects_using_Git"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/Git_(software)#Projects_using_Git</a> for a list of large organizations (open source and non-open source) using git.</p>

<p>Also take a look at <a href=""http://www.youtube.com/watch?v=4XpnKHJAok8"" rel=""nofollow noreferrer"">http://www.youtube.com/watch?v=4XpnKHJAok8</a> and pay particular attention when Linus tells Google that their current revision control system sucks and that they'd be much better off with git.</p>",0.0,2010-06-21 02:14:29.663000 UTC,2010-06-21 02:53:10.380000 UTC,9.0,[]
How does the 3 way merge in Mercurial/Meld work?,"<p>I'm working on a project where I have a commit that introduced a feature with major problems that weren't discovered immediately. Now I want to completely remove that revision while keeping the work following it but i'm having a hard time wrapping my head around this 3 way merge. Here is a simplified graph of my project.</p>

<pre>
o  changeset:   134:7f81764aa03a
|  tag:         tip
|  parent:      128:451d8a19edea
|  summary:     Backed out changeset 451d8a19edea
|
| @  changeset:   133:5eefa40e2a29
| |  summary:     (Change I need to keep keep)
| |
*snip 3 commits*
| o  changeset:   129:5f6182a97d40
|/   summary:     (Change I need to keep keep)
|
o  changeset:   128:451d8a19edea
|  summary:     (Change that introduced a major problem)
|
o  changeset:   127:4f26dc55455d
|  summary:     (summary doesn't matter for this question)
</pre>

<p>If I understand this correctly, r127 and r134 are exactly the same. When I <code>hg up -C -r 133</code> and then run <code>hg merge</code>, Meld pops up with three forms of one of my files: local, base, and other. local seems to be r133 but i'm having a hard time wrapping my head around what ""base"" and ""other"" mean.</p>",2,0,2010-10-02 16:52:51.763000 UTC,11.0,2012-11-02 14:42:02.530000 UTC,33,mercurial|merge|dvcs|3-way-merge,15711,2008-11-06 20:05:44.813000 UTC,2020-08-24 22:07:09.323000 UTC,"Las Vegas, NV, United States",4765,178,13,326,"<p><strong>Local</strong> is r133</p>

<p><strong>Other</strong> is r134</p>

<p><strong>Base</strong> is r128 (the common ancestor to both r133 and r 134)</p>

<p>When you perform a 3 way merge it compares all three of those together to help you decide what to take and from where.  By seeing what change is in the other revision and what the common ancestor looked like you are able to make a much more informed decision about what to keep and what to change.</p>",1.0,2010-10-02 16:58:23.490000 UTC,2010-10-02 20:30:59.770000 UTC,35.0,[]
How do I synchronise two remote Git repositories?,"<p>I have two repository urls, and I want to synchronise them such that they both contain the same thing. In Mercurial, what I'm trying to do would be:</p>

<pre><code>hg pull {repo1}
hg pull {repo2}
hg push -f {repo1}
hg push -f {repo2}
</code></pre>

<p>This will result in two heads in both repos (I know it's not common to have two heads, but I'm doing this for synchornisation and it needs to be non-interactive. The heads will be merged manually from one of the repos and then the sync run again).</p>

<p>I'd like to do the same thing in Git. Eg., with no user interaction, get all of the changes into both repos, with multiple branches/heads/whatever to be merged later.
I'm trying to do this using urls in the commands, rather than adding remotes(?), as there could be a number of repos involved, and having aliases for them all will just make my script more complicated.</p>

<p>I'm currently cloning the repo using <code>git clone --bar {repo1}</code> however I'm struggling to ""update"" it. I've tried <code>get fetch {repo1}</code> but that doesn't seem to pull my changes down; <code>git log</code> still doesn't show the changeset that has been added in repo1.</p>

<p>I also tried using <code>--mirror</code> in my <code>push</code> and <code>clone</code>, but that seemed to remote changesets from repo2 that didn't exist locally, whereas I need to keep changes from both repos :/</p>

<p>What's the best way to do this?</p>

<p><strong>Edit:</strong> To make it a little clearer what I'm trying to do...</p>

<p>I have two repositories (eg. BitBucket and GitHub) and want people to be able to push to either (ultimately, one will be Git, one will be Mercurial, but let's assume they're both Git for now to simplify things). I need to be able to run a script that will ""sync"" the two repos in a way that they both contain both sets of changes, and may require merging manually later.</p>

<p>Eventually, this means I can just interact with one of the repos (eg. the Mercurial one), and my script will periodically pull in Git changes which I can merge in, and then they'll be pushed back.</p>

<p>In Mercurial this is <em>trivial</em>! I just pull from both repos, and push with <code>-f/--force</code> to allow pushing multiple heads. Then anybody can clone one of the repos, merge the heads, and push back. I want to know how to do the closest similar thing in Git. It must be 100% non-interactive, and must keep both repos in a state that the process can be repeated infinitely (that means no rewriting history/changing changesets etc).</p>",6,0,2013-02-24 20:33:38.440000 UTC,11.0,2013-02-24 21:08:23.193000 UTC,27,git|version-control|github|dvcs,54600,2008-10-04 15:14:46.937000 UTC,2022-03-02 22:08:36.157000 UTC,"England, United Kingdom",35819,544,88,2496,"<p>Git branches do not have ""heads"" in the Mercurial sense.  There is only one thing called <code>HEAD</code>, and it's effectively a symlink to the commit you currently have checked out.  In the case of hosted repositories like GitHub, there <em>is no</em> commit checked out—there's just the repository history itself.  (Called a ""bare"" repo.)</p>

<p>The reason for this difference is that Git branch names are completely arbitrary; they don't have to match between copies of a repository, and you can create and destroy them on a whim.[1]  Git branches are like Python variable names, which can be shuffled around and stuck to any value as you like; Mercurial branches are like C variables, which refer to fixed preallocated memory locations you then fill with data.</p>

<p>So when you pull in Mercurial, you have two histories for the same branch, because the branch name is a fixed meaningful thing in both repositories.  The leaf of each history is a ""head"", and you'd normally merge them to create a single head.</p>

<p>But in Git, fetching a remote branch doesn't actually affect your branch at all.  If you fetch the <code>master</code> branch from <code>origin</code>, it just goes into a branch called <code>origin/master</code>.[2]  <code>git pull origin master</code> is just thin sugar for two steps: fetching the remote branch into <code>origin/master</code>, and then merging that other branch into your current branch.  But they don't have to have the same name; your branch could be called <code>development</code> or <code>trunk</code> or whatever else.  You can pull or merge any other branch into it, and you can push it to any other branch.  Git doesn't care.</p>

<p>Which brings me back to your problem: you can't push a ""second"" branch head to a remote Git repository, because the concept doesn't exist.  You <em>could</em> push to branches with mangled names (<code>bitbucket_master</code>?), but as far as I'm aware, you can't update a remote's remotes remotely.</p>

<p>I don't think your plan makes a lot of sense, though, since with unmerged branches exposed to both repositories, you'd either have to merge them both, or you'd merge one and then mirror it on top of the other...  in which case you left the second repository in a useless state for no reason.</p>

<p>Is there a reason you can't just do this:</p>

<ol>
<li><p>Pick a repository to be canonical—I assume BitBucket.  Clone it.  It becomes <code>origin</code>.</p></li>
<li><p>Add the other repository as a remote called, say, <code>github</code>.</p></li>
<li><p>Have a simple script periodically fetch both remotes and attempt to merge the <code>github</code> branch(es) into the <code>origin</code> branches.  If the merge fails, abort and send you an email or whatever.  If the merge is trivial, push the result to both remotes.</p></li>
</ol>

<p>Of course, if you just do all your work on feature branches, this all becomes much less of a problem.  :)</p>

<hr>

<p>[1] It gets even better: you can merge together branches from different repositories that have <em>no history whatsoever</em> in common.  I've done this to consolidate projects that were started separatedly; they used different directory structures, so it works fine.  GitHub uses a similar trick for its Pages feature: the history of your Pages is stored in a branch called <code>gh-pages</code> that lives in the same repository but has absolutely no history in common with the rest of your project.</p>

<p>[2] This is a white lie.  The branch is still called <code>master</code>, but it belongs to the remote called <code>origin</code>, and the slash is syntax for referring to it.  The distinction can matter because Git has no qualms about slashes in branch names, so you could have a local branch named <code>origin/master</code>, and that would shadow the remote branch.</p>",9.0,2013-02-24 22:33:15.687000 UTC,2013-02-25 02:21:02.693000 UTC,30.0,[]
Getting started with Version Control System,"<p>I need to implement version control, even for just the developing I do at home.  I have read about how great subversion is for the past couple of years and was about to dedicate myself to learning this on the side until I heard about Git being the up and coming version control system.</p>
<p>Given the situation, should I hold off and see which one comes out on top? What are their relative advantages?</p>
<p>One issue I noticed with Git is, there are not many full featured GUIs, which is important to many users on my team.</p>
<p>Also, wouldn't mind suggestions on how to get started with one or the other. (tutorials, etc.)</p>",34,1,2008-08-05 18:29:14.283000 UTC,29.0,2022-02-23 12:27:23.230000 UTC,76,git|svn|version-control|language-agnostic|dvcs,9482,2008-08-05 13:38:24.543000 UTC,2021-11-13 17:28:27.220000 UTC,United States,1974,438,18,255,"<p>The most important thing about version control is:</p>
<p><em><strong>JUST START USING IT</strong></em></p>
<p>Not using version control is a horrible idea. If you are not using version control, stop reading right now and start using it.</p>
<p>It is very easy to convert from</p>
<pre><code>cvs&lt;-&gt;svn&lt;-&gt;git&lt;-&gt;hg
</code></pre>
<p>It doesn't matter which one you choose. Just pick the easiest one for you to use and start recording the history of your code. You can always migrate to another (D)VCS later.</p>
<p>If you are looking for a easy to use GUI look at <a href=""http://tortoisesvn.tigris.org/%22TortoiseSVN%22"" rel=""noreferrer"">TortoiseSVN (Windows)</a> and <a href=""http://www.versionsapp.com/"" rel=""noreferrer"">Versions (Mac)</a> (Suggested by <a href=""https://stackoverflow.com/questions/2658?sort=newest#2708"">codingwithoutcomments</a>)</p>
<hr />
<p>Edit:</p>
<blockquote>
<p><a href=""https://stackoverflow.com/questions/2658/#2672"">pix0r said:</a></p>
<p>Git has some nice features, but you won't be able to appreciate them unless you've already used something more standard like CVS or Subversion.</p>
</blockquote>
<p>This. Using git is pointless if you don't know what version control can do for you.</p>
<p>Edit 2:</p>
<p>Just saw this link on reddit: <a href=""http://www.addedbytes.com/cheat-sheets/subversion-cheat-sheet/"" rel=""noreferrer"">Subversion Cheat Sheet</a>. Good quick reference for the svn command line.</p>",1.0,2008-08-05 18:34:44.350000 UTC,2020-06-20 09:12:55.060000 UTC,82.0,[]
What is Mercurial bisect good for?,"<p>I've been reading about <code>hg bisect</code> and its interesting to be able to know which revision introduced a bug, but I'd like to know what people use this information for. The only thing I can think of is trying to narrow down which dates might need a data fix if it's a bug that results in some form of invalid data.</p>

<p><strong>update:</strong>
I think I completely misunderstood the purpose before I posted this. I was thinking that I would do the debugging and find which line(s) introduced the bug and then use bisect. It seems bisect is a way for me to not have to spend time guessing where the bug might be and placing breakpoints or logging. Instead I should write a small test that fails now, passes in a past revision and have bisect tell me where the problem originates.</p>",4,0,2010-08-20 19:14:40.310000 UTC,7.0,2015-11-10 16:00:48.510000 UTC,26,version-control|mercurial|dvcs|bisect,6016,2008-11-06 20:05:44.813000 UTC,2020-08-24 22:07:09.323000 UTC,"Las Vegas, NV, United States",4765,178,13,326,<p>To track down the changeset that introduced a bug. I think it's obvious that this is very useful. If your software suddenly fails and you don't know which change caused the bug bisect makes it easy to track down that change. I don't get at all what you're saying about dates.</p>,2.0,2010-08-20 19:20:14.700000 UTC,,8.0,[]
AWS Neptune: Custom vertex id in javascript,"<p>I'd like to specify my own vertex id's. According to the <a href=""https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-differences.html"" rel=""nofollow noreferrer"">docs</a> they are supported like so</p>

<p><code>g.addV().property(id, 'customid')</code></p>

<p>with no quotes around <code>id</code>. This causes JS to parse it as a variable, and it fails (undefined variable). Where do I import <code>id</code>?</p>",1,0,2018-07-28 23:18:17.060000 UTC,,,1,gremlin|amazon-neptune,767,2010-08-23 23:03:20.887000 UTC,2022-03-03 17:19:08.790000 UTC,,4656,718,14,204,"<p>try adding this to the code before you call the addV.</p>

<pre><code>const { t: { id } } = gremlin.process;
</code></pre>

<p>That will correctly import id from gremlin.process for you.</p>

<p>Hope this help,
Kelvin</p>",4.0,2018-07-28 23:37:10.243000 UTC,,8.0,[]
Can RDF model a labeled property graph with edge properties?,"<p>I wanted to model partner relationship like the following, which I expressed in the format of labeled property graph.</p>

<p><a href=""https://i.stack.imgur.com/FF7hT.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/FF7hT.png"" alt=""enter image description here""></a></p>

<p>I wanted to use RDF language to express the above graph, particularly I wanted to understand if I can express the label of the ""loves"" edge (which is an URI to an article/letter).</p>

<p>I am new to RDF, and I know the RDF can easily express the node properties in the LPG, but is it possible to conveniently express the edge properties?</p>

<p>A bit more context of this question: the reason I wanted use RDF (rather than Gremlin) is that I wanted to add some reasoning capability in the long run.</p>

<p><strong>Further added question</strong>: if we choose an RDF model to represent the above LPG, in plain English, I wanted to answer the following questions with SPARQL query:</p>

<ol>
<li>Is Bob in love with any one?</li>
<li>If so, who is he in love with and why?</li>
</ol>

<p>How complex would the SPARQL statement be to query out the <code>loveletters.com/123</code>? </p>",2,5,2019-04-28 00:11:42.730000 UTC,6.0,2019-04-28 23:16:21.423000 UTC,11,rdf|graph-databases|gremlin|amazon-neptune,1251,2009-08-08 03:46:23.860000 UTC,2022-02-14 09:14:33.393000 UTC,,4022,222,5,331,"<p>RDF doesn't support edge properties, so <strong>the brief answer is no</strong>. But of course there are ways to model this kind of thing in RDF.</p>

<h3>Plain RDF triple without edge properties</h3>

<p>If we didn't want to annotate the edge, the relationship between Bob and Mary would simply be a triple with Bob as Subject, Mary as object, and “loves” as predicate:</p>

<pre><code>PREFIX : &lt;http://example.org/ontology#&gt;
PREFIX person: &lt;http://example.org/data/person/&gt;

person:Bob :loves person:Mary.
</code></pre>

<p>So how can we add annotations?</p>

<h3>Option 1: Using RDF Reification</h3>

<p>RDF has a built-in solution called “RDF reification”. It allows making statements about statements:</p>

<pre><code>PREFIX : &lt;http://example.org/ontology#&gt;
PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;
PREFIX person: &lt;http://example.org/data/person/&gt;
PREFIX statement: &lt;http://example.org/data/statement/&gt;

person:Bob :loves person:Mary.

statement:1 a rdf:Statement;
    rdf:subject person:Bob;
    rdf:predicate :loves;
    rdf:object person:Mary;
    :reason &lt;http://loveletters.com/123&gt;.
</code></pre>

<p>So we say that there is a statement with Bob as subject, Mary as object, and “loves” as predicate. Then we can add properties to that statement. The downside is that it is kind of redundant. First we add the “loves” triple, then we add four more triples to replicate the “loves” triple.</p>

<h3>Option 2: Modelling relationships as entities</h3>

<p>Another approach is to change the model. Instead of considering “loves” an edge between people, we consider it a node in itself. A node that represents the relationship, and is connected to the two parties involved.</p>

<pre><code>PREFIX relationship: &lt;http://example.org/data/relationship/&gt;

relationship:1 a :LovesRelationship;
    :who person:Bob;
    :whom person:Mary;
    :reason &lt;http://loveletters.com/123&gt;.
</code></pre>

<p>So in our model we created a class <code>:LovesRelationship</code> that represents “loves”, and properties <code>:who</code> and <code>:whom</code> to indicate the two parties. The downside of this approach is that the graph structure no longer directly represents our social network. So when querying how two people are related, we always have to go through those relationship entities instead of just dealing with edges connecting people.</p>

<h3>Option 3: Using RDF*</h3>

<p>There is <a href=""http://blog.liu.se/olafhartig/2019/01/10/position-statement-rdf-star-and-sparql-star/"" rel=""noreferrer"">a proposal called RDF*</a> that addresses this problem quite nicely. (Sometimes it's called RDR or <em>Reification Done Right</em>.) RDF*/RDR adds new syntax that allows triples to be the subject of other triples:</p>

<pre><code>&lt;&lt;person:Bob :loves person:Mary&gt;&gt;
    :reason &lt;http://loveletters.com/123&gt;.
</code></pre>

<p>The downside is that it is non-standard and so far supported only by a few systems (<a href=""https://wiki.blazegraph.com/wiki/index.php/Reification_Done_Right"" rel=""noreferrer"">Blazegraph</a>, <a href=""https://docs.cambridgesemantics.com/anzograph/userdoc/lpgs.htm"" rel=""noreferrer"">AnzoGraph</a>, and <a href=""https://github.com/RDFstar/RDFstarTools"" rel=""noreferrer"">an extension for Jena</a>). As of April 2019, Neptune is not among them.</p>

<h3>Query: Is Bob in love with anyone?</h3>

<p>This is easy to do in the basic RDF version as well as in Option 1 and Option 3:</p>

<pre><code>ASK { person:Bob :loves ?anyone }
</code></pre>

<p>Option 2 requires a different query, because of the changed model:</p>

<pre><code>ASK {
   ?rel a :LovesRelationship;
       :who person:Bob.
}
</code></pre>

<p>This would match any <code>:LovesRelationship</code> where the <code>:who</code> property is Bob, regardless of the <code>:whom</code> and <code>:reason</code> properties.</p>

<h3>Query: Who is Bob in love with and why?</h3>

<p><strong>Option 1</strong>, RDF Reification:</p>

<pre><code>SELECT ?whom ?why {
    ?statement a rdf:Statement;
        rdf:subject person:Bob;
        rdf:predicate :loves;
        rdf:object ?whom;
        :reason ?why.
}
</code></pre>

<p>I find this query not very intuitive, because it talks about RDF statements, while we are really interested in people and relationships.</p>

<p><strong>Option 2</strong>, relationship modelled as entity:</p>

<pre><code>SELECT ?whom ?why {
    ?rel a :LovesRelationship;
        :who person:Bob;
        :whom ?whom;
        :reason ?why.
}
</code></pre>

<p>This is better in my eyes; once you have accepted that relationships are entities in this model, it becomes fairly intuitive.</p>

<p><strong>Option 3</strong>, RDF*, using SPARQL*:</p>

<pre><code>SELECT ?whom ?why {
    &lt;&lt;person:Bob :loves ?whom&gt;&gt;
        :reason ?why.
}
</code></pre>

<p>This is concise and intuitive, so it's a shame we can't currently use it in most SPARQL systems!</p>",3.0,2019-04-28 10:56:58.223000 UTC,2019-04-29 08:34:56.090000 UTC,19.0,[]
Distributed version control for HUGE projects - is it feasible?,"<p>We're pretty happy with SVN right now, but <a href=""http://hginit.com/"" rel=""nofollow noreferrer"">Joel's tutorial</a> intrigued me. So I was wondering - would it be feasible in our situation too?</p>

<p>The thing is - our SVN repository is HUGE. The software itself has a 15 years old legacy and has survived several different source control systems already. There are over 68,000 revisions (changesets), the source itself takes up over 100MB and I cant even begin to guess how many GB the whole repository consumes.</p>

<p>The problem then is simple - a clone of the whole repository would probably take ages to make, and would consume far more space on the drive that is remotely sane. And since the very point of distributed version control is to have a as many repositories as needed, I'm starting to get doubts.</p>

<p>How does Mercurial (or any other distributed version control) deal with this? Or are they unusable for such huge projects?</p>

<p><strong>Added:</strong> To clarify - the whole thing is one monolithic beast of a project which compiles to a single .EXE and cannot be split up.</p>

<p><strong>Added 2:</strong> Second thought - The Linux kernel repository uses git and is probably an order of magnitude or two bigger than mine. So how do they make it work?</p>",10,3,2010-03-19 10:07:37.907000 UTC,2.0,2010-03-19 10:53:44.183000 UTC,11,svn|mercurial|scalability|dvcs,2036,2008-11-27 12:59:33.427000 UTC,2022-03-04 16:51:07.947000 UTC,Latvia,100434,673,74,5416,"<p>100MB of source code is less than the Linux kernel. Changelog between Linux kernel 2.6.33 and 2.6.34-rc1 has 6604 commits. Your repository scale doesn't sound intimidating to me.</p>

<ul>
<li>Linux kernel 2.6.34-rc1 uncompressed from .tar.bz2 archive: 445MB</li>
<li>Linux kernel 2.6 head checked out from main Linus tree: 827MB</li>
</ul>

<p>Twice as much, but still peanuts with the big hard drives we all have.</p>",8.0,2010-03-19 10:20:23.017000 UTC,2010-03-19 10:33:48.217000 UTC,11.0,[]
Is it possible to visualize the output of a graph query (Gremlin or SPARQL) as nodes and edges in Amazon Neptune?,"<p>GREMLIN and SPARQL only define the APIs for graph queries. How do I use the API responses and and plot that as an actual graph, with edges and vertices? Is there something like MySQL Workbench for graphs?</p>",4,0,2018-10-17 05:47:52.717000 UTC,4.0,2020-09-09 01:55:48.057000 UTC,15,sparql|gremlin|graph-databases|tinkerpop3|amazon-neptune,8229,2013-12-05 11:08:35.323000 UTC,2022-03-01 18:38:12.630000 UTC,"Vancouver, BC, Canada",2409,244,19,377,"<blockquote>
<p>UPDATE: As of Nov 2019, Neptune launched Workbench, which is a Jupyter based visualization for Gremlin and SPARQL.</p>
</blockquote>
<blockquote>
<p>UPDATE: As of Aug 2020, Neptune Workbench extended support for visualizing graph data as nodes and edges in addition to tabular representation that was previously supported.</p>
</blockquote>
<p><a href=""https://aws.amazon.com/about-aws/whats-new/2019/12/amazon-neptune-workbench-provides-in-console-experience-to-query-your-graph/"" rel=""noreferrer"">https://aws.amazon.com/about-aws/whats-new/2019/12/amazon-neptune-workbench-provides-in-console-experience-to-query-your-graph/</a></p>
<p><a href=""https://aws.amazon.com/about-aws/whats-new/2020/08/amazon-neptune-announces-graph-visualization-in-neptune-workbench/"" rel=""noreferrer"">https://aws.amazon.com/about-aws/whats-new/2020/08/amazon-neptune-announces-graph-visualization-in-neptune-workbench/</a></p>
<p>Neptune Workbench basically is a Sagemaker instance preconfigured with extensions to help execute Gremlin and SPARQL queries, as well as other Neptune APIs like <code>/loader</code>, <code>/status</code> etc. You can easily create these notebooks from the Neptune console. There are no additional charges for the workbench, apart from the Sagemaker costs incurred by the notebook. These notebooks do support Start and Stop APIs, thereby making it possible for you to enable them only when you need it.</p>
<p>A very recent blog post walking you through some of the features: <a href=""https://aws.amazon.com/blogs/database/visualize-query-results-using-the-amazon-neptune-workbench/"" rel=""noreferrer"">https://aws.amazon.com/blogs/database/visualize-query-results-using-the-amazon-neptune-workbench/</a></p>
<p>SPARQL:
<a href=""https://i.stack.imgur.com/WMAPy.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/WMAPy.png"" alt=""SPARQL1"" /></a>
<a href=""https://i.stack.imgur.com/Czxh1.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Czxh1.png"" alt=""Sparql Query"" /></a>
GREMLIN:
<a href=""https://i.stack.imgur.com/M2rqn.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/M2rqn.png"" alt=""Gremlin1"" /></a>
<a href=""https://i.stack.imgur.com/CrGFS.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/CrGFS.png"" alt=""Gremlin2"" /></a>
<a href=""https://i.stack.imgur.com/snI2A.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/snI2A.png"" alt=""Gremlin Query"" /></a></p>",4.0,2018-10-17 05:47:52.717000 UTC,2020-09-03 18:43:56.203000 UTC,16.0,[]
Mercurial: Why would you maintain separate branches/versions of software in separate cloned repos rather than with tags?,"<p>I'm looking at the mercurial handbook, chapter 6 ""Working with multiple branches"". In there the author states that if you have separate versions/branches of the same software that it makes sense in an implied obvious way to host each branch of the software in a separate repository.</p>

<p>I know that Mercurial supports tags (which is the way that branches are done in subversion AFAIK). Why would you use different <em>repositories</em> instead of tags for branch managing?</p>",2,3,2009-11-04 08:57:57.407000 UTC,3.0,,2,mercurial|dvcs|branch,1104,2008-10-01 14:49:48.613000 UTC,2022-01-11 09:49:23.647000 UTC,South Africa,5479,389,17,373,"<p>Mercurial's tags are <em>not</em> the same as Subversion's branches.  In Subversion, making a branch creates a separate copy of the code (with hardlinks at least, I think) while tags in Mercurial just point to a specific commit.</p>

<p>A while ago I <a href=""http://stevelosh.com/blog/entry/2009/8/30/a-guide-to-branching-in-mercurial/"" rel=""noreferrer"">wrote about</a> Mercurial's branching models (of which branching-with-clones is one of four options), maybe you'd find it useful?</p>",0.0,2009-11-04 14:10:50.667000 UTC,,11.0,[]
Can I mark a branch as 'not going to push'?,"<p>I use named branches in Mercurial.</p>

<p>In doing so I have created one branch called playground where I can try out various wacky experiments.  I never intend to merge this branch into any others and I never want to push it to our main repository.</p>

<p>Since creating it, every time I do a push I am told I have added a new branch and I have to use the <code>--new-branch</code> flag. At this point <code>hg push -b default</code> (or whatever branch I'm pushing) works fine but it's annoying. Is there any way to suppress that message by letting Hg know that I am not interested in pushing that branch ever?</p>",3,0,2012-01-24 23:04:39.733000 UTC,6.0,2012-03-10 10:47:28.140000 UTC,35,mercurial|branch|push|dvcs,7835,2008-09-07 15:43:17.647000 UTC,2022-03-06 01:44:38.360000 UTC,United States,109334,3638,135,9422,"<p>Starting with Mercurial 2.1 (released in February 2012), you can mark your changesets <strong>secret</strong> to keep them from being pushed to another repository. You use the new <a href=""https://www.mercurial-scm.org/wiki/Phases"" rel=""nofollow noreferrer""><code>hg phase</code> command</a> to do this:</p>

<pre><code>$ hg phase --force --secret .
</code></pre>

<p>This mark the current working directory parent revision (<code>.</code>) as being in the <code>secret</code> phase. Secret changesets are local to your repository: they wont be pushed or pulled. Pushing now looks like this:</p>

<pre><code>$ hg push
pushing to /home/mg/tmp/repo
searching for changes
no changes to push but 2 secret changesets
</code></pre>

<p>There is no equivalent mechanism in older versions of Mercurial. There your best bet is to create a local clone for the changesets you don't want to push.</p>",3.0,2012-01-25 09:54:53.550000 UTC,2018-06-02 13:22:04.577000 UTC,57.0,[]
how to make pull requests *without* a github account?,"<p>One of the goals of git is to be decentralized. If Github is to be the ... hub of git, then maybe it could take into account that there are <em>other</em> hubs out there, and allow pull requests to happen on git URLs that are <em>not</em> hosted on github.</p>

<p>The <a href=""http://help.github.com/send-pull-requests/"" rel=""noreferrer"">documentation</a> is fairly clear about this: pull requests require that you have a Github account and that you fork a repository <em>on github</em>. Is this a real current technical limitation or is there a way to workaround that on Github?</p>

<p>If not, is it eventually planned to allow Github's ""pull requests"" tool to actually allow to pull from repositories outside of the Github silo?</p>",1,0,2012-03-09 08:09:09.147000 UTC,18.0,2016-07-07 08:35:33.250000 UTC,48,github|dvcs|pull|pull-request,12425,2012-01-28 04:28:52.817000 UTC,2021-09-02 19:14:27.283000 UTC,,4717,123,5,189,"<p>You can use <a href=""http://linux.die.net/man/1/git-request-pull"" rel=""noreferrer""><code>git request-pull</code></a> to achieve the same kind of workflow (which is improved with Git1.7.9+).</p>

<p>See the article ""<a href=""http://git-blame.blogspot.com/2012/01/using-signed-tag-in-pull-requests.html"" rel=""noreferrer"">using signed tag in pull-requests</a>""</p>

<blockquote>
  <p>A typical distributed workflow using Git is for a contributor to fork a project, build on it, publish the result to her public repository, and ask the ""upstream"" person (often the owner of the project where she forked from) to pull from her public repository. Requesting such a ""pull"" is made easy by the <code>git request-pull</code> command.</p>
  
  <p>Starting from Git release v1.7.9, a contributor can add a signed tag to the commit at the tip of the history and ask the integrator to pull that signed tag.<br>
  When the integrator runs <code>git pull</code>:</p>
  
  <ul>
  <li>the signed tag is automatically verified to assure that the history is not tampered with. </li>
  <li>In addition, the resulting merge commit records the content of the signed tag, so that other people can verify that the branch merged by the integrator was signed by the contributor, without fetching the signed tag used to validate the pull request separately and keeping it in the refs namespace.</li>
  </ul>
</blockquote>",6.0,2012-03-09 08:14:49.477000 UTC,,42.0,[]
Which DVCS work best with Subversion repositories,"<p>Subversion works great when we have access to central repository, but sometimes two or more developers work at client where they do not have connection to central repository. I am looking for DVCS that can help us where off-line.</p>

<p>It should:</p>

<ul>
<li>cooperate with Subversion repository so developers can checkout before leave, commit locally where off line and commit to central repository when they return</li>
<li>easy exchange code between developers working offline</li>
<li>work on Windows, GUI preferred; developers are used to TortoiseSVN</li>
</ul>

<p>Anybody uses Bazaar, Mercurial, git or maybe something else and can show its advantages and pitfalls?
So far I started (really returned to) testing Bazaar with Tortoise Bazaar.</p>",3,0,2010-10-20 11:08:46.403000 UTC,5.0,,8,svn|git|mercurial|dvcs|bazaar,539,2008-09-26 08:28:44.370000 UTC,2022-03-04 07:52:00.390000 UTC,Poland,50463,1579,2,1978,"<p>I have tried Git, Mercurial and Bazaar with a SVN repository and I have found that all three work pretty well (when using the their respective *-svn module).</p>

<p>I suggest you pick the DCVS you like most and use that one.  </p>

<p>(The modules are <a href=""http://git-scm.com/docs/git-svn"" rel=""nofollow noreferrer"">git-svn</a>, <a href=""http://doc.bazaar.canonical.com/latest/en/user-guide/svn_plugin.html"" rel=""nofollow noreferrer"">bzr-svn</a> and <a href=""https://www.mercurial-scm.org/wiki/HgSubversion"" rel=""nofollow noreferrer"">hgSubversion</a>)</p>",1.0,2010-10-20 11:19:31.800000 UTC,2018-02-22 12:52:17.650000 UTC,12.0,[]
Prevent commits in a local branch,"<p>In my local git tree I pull commits from the ""master"" branch in the repository, but all development is done in a different branch, and pushed in a different branch too.</p>

<p>I would like to avoid mistakes and prevent accidental commits in my local ""master"" branch, and allow only pull requests (then I'd rebase the developement branch to the updated master). Is this possible? How?</p>",1,0,2013-06-25 09:40:33.037000 UTC,4.0,2013-06-25 09:46:27.903000 UTC,19,git|version-control|dvcs,3379,2012-07-19 17:44:28.997000 UTC,2022-02-28 17:43:11.890000 UTC,,2140,30,1,113,"<p>You can use a <a href=""http://git-scm.com/book/en/Customizing-Git-Git-Hooks""><strong>pre-commit hook</strong></a>.</p>

<p>For example, place the following script as <code>.git/hooks/pre-commit</code>:</p>

<pre><code>#!/bin/bash
if test $(git rev-parse --abbrev-ref HEAD) = ""master"" ; then 
  echo ""Cannot commit on master""
  exit 1
fi
</code></pre>

<p>And set it as executable</p>

<pre><code>chmod +x .git/hooks/pre-commit
</code></pre>",5.0,2013-06-25 09:43:07.913000 UTC,2016-06-08 17:26:18.917000 UTC,31.0,[]
What is your workflow to coordinate Pivotal Tracker with Mercurial?,"<p>I want to use <a href=""http://www.pivotaltracker.com/"" rel=""noreferrer"">Pivotal Tracker</a> for a new project but I don't know how to use it with Mercurial to make it easy to go from one tool to the other.</p>

<p>What workflow do you use to link user stories/feature in Pivotal Tracker with your DVCS (Mercurial/Git)? </p>

<p>Thanks in advance for your advices.</p>",2,4,2010-04-19 15:02:08.197000 UTC,3.0,,6,mercurial|workflow|dvcs|pivotaltracker,1714,2009-07-08 14:05:35.713000 UTC,2020-12-12 00:30:57.097000 UTC,"Sherbrooke, QC, Canada",4621,256,8,565,"<p>If someone is still looking for an answer, there exists a service which allows mercurial users to connect to pivotal tracker using a syntax like <code>[#story_id finished]</code> in their commit messages. Bitbucket allows for this integration as well.</p>

<p>Links: <a href=""https://bitbucket.org/proppy/hgpivotal/src/tip/hgpivotal.py"" rel=""nofollow noreferrer"">https://bitbucket.org/proppy/hgpivotal/src/tip/hgpivotal.py</a></p>

<blockquote>
  <p><a href=""https://www.pivotaltracker.com/help/api?version=v5#Tracker_Updates_in_SCM_Post_Commit_Hooks"" rel=""nofollow noreferrer"">Note from Pivotal Tracker on the format</a>:</p>
  
  <p>The minimum commit message string that will allow Tracker to associate
  a source_commits POST with a story and create a comment is a single
  story ID enclosed in square brackets: '[#12345678]'. A more typical
  message, indicating that one commit completes two stories (which need
  not be in the same Tracker project), might look like this: 'finally
  [finished #12345678 #12345779], fixes client/server integration
  glitch'</p>
  
  <p>If an included story was not already started (it was in the ""not
  started"" state), an update to that story from /source_commits that
  doesn't contain any other state-change information will automatically
  start the story.</p>
  
  <p>To automatically finish a story by using a commit message, include
  ""fixed"", ""completed"", or ""finished"" in the square brackets in addition
  to the story ID. You may use different cases or forms of these verbs,
  such as ""Fix"" or ""FIXES"", and they may appear before or after the
  story ID. Note: For features, use of one of these keywords will put
  the story in the finished state. For chores, it will put the story in
  the accepted state.</p>
  
  <p>In some environments, code that is committed is automatically
  deployed. For this situation, use the keyword ""delivers"" and feature
  stories will be put in the delivered state.</p>
</blockquote>",2.0,2011-09-04 10:14:23.737000 UTC,2015-02-18 05:18:44.547000 UTC,8.0,[]
Git push won't do anything (everything up-to-date),"<p>I'm trying to update a Git repository on GitHub. I made a bunch of changes, added them, committed then attempted to do a <code>git push</code>. The response tells me that everything is up to date, but clearly it's not.</p>

<pre><code>git remote show origin
</code></pre>

<p>responds with the repository I'd expect.</p>

<p>Why is Git telling me the repository is up to date when there are local commits that aren't visible on the repository?</p>

<pre><code>  [searchgraph]  git status
# On branch develop
# Untracked files:
#   (use ""git add &lt;file&gt;..."" to include in what will be committed)
#
#       Capfile
#       config/deploy.rb
nothing added to commit but untracked files present (use ""git add"" to track)

  [searchgraph]  git add .

  [searchgraph]  git status
# On branch develop
# Changes to be committed:
#   (use ""git reset HEAD &lt;file&gt;..."" to unstage)
#
#       new file:   Capfile
#       new file:   config/deploy.rb
#

  [searchgraph]  git commit -m ""Added Capistrano deployment""
[develop 12e8af7] Added Capistrano deployment
 2 files changed, 26 insertions(+), 0 deletions(-)
 create mode 100644 Capfile
 create mode 100644 config/deploy.rb

  [searchgraph]  git push
Everything up-to-date

  [searchgraph]  git status
# On branch develop
nothing to commit (working directory clean)
</code></pre>",21,1,2010-05-29 21:27:41.727000 UTC,45.0,2018-08-17 17:51:38.050000 UTC,110,git|dvcs,211165,2010-03-29 03:14:32.237000 UTC,2019-11-01 16:11:22.970000 UTC,"Waterloo, Canada",17718,470,12,4507,"<p><code>git push</code> doesn't push all of your local branches: how would it know which remote branches to push them to? It only pushes local branches which have been configured to push to a particular remote branch.</p>

<p>On my version of Git (1.6.5.3), when I run <code>git remote show origin</code> it actually prints out which branches are configured for push:</p>

<pre><code>Local refs configured for 'git push':
  master pushes to master (up to date)
  quux   pushes to quux   (fast forwardable)
</code></pre>

<p><strong>Q. But I could push to <code>master</code> without worrying about all this!</strong></p>

<p>When you <code>git clone</code>, by default it sets up your local <code>master</code> branch to push to the remote's <code>master</code> branch (locally referred to as <code>origin/master</code>), so if you only commit on <code>master</code>, then a simple <code>git push</code> will always push your changes back.</p>

<p>However, from the output snippet you posted, you're on a branch called <code>develop</code>, which I'm guessing hasn't been set up to push to anything.  So <code>git push</code> without arguments won't push commits on that branch.</p>

<p>When it says ""Everything up-to-date"", it means ""all the branches you've told me how to push are up to date"".</p>

<p><strong>Q. So how can I push my commits?</strong></p>

<p>If what you want to do is put your changes from <code>develop</code> into <code>origin/master</code>, then you should probably merge them into your local <code>master</code> then push that:</p>

<pre><code>git checkout master
git merge develop
git push             # will push 'master'
</code></pre>

<p>If what you want is to create a <code>develop</code> branch on the remote, separate from <code>master</code>, then supply arguments to <code>git push</code>:</p>

<pre><code>git push origin develop
</code></pre>

<p>That will: create a new branch on the remote called <code>develop</code>; <em>and</em> bring that branch up to date with your local <code>develop</code> branch; <em>and</em> set <code>develop</code> to push to <code>origin/develop</code> so that in future, <code>git push</code> without arguments <em>will</em> push <code>develop</code> automatically.</p>

<p>If you want to push your local <code>develop</code> to a remote branch called something <em>other than</em> <code>develop</code>, then you can say:</p>

<pre><code>git push origin develop:something-else
</code></pre>

<p>However, that form <em>won't</em> set up <code>develop</code> to always push to <code>origin/something-else</code> in future; it's a one-shot operation.</p>",5.0,2010-05-29 22:07:57.763000 UTC,2018-08-17 17:52:37.807000 UTC,154.0,[]
Unification of DVCS commands,"<p>When working on multiple (open source) projects, multiple version controll systems start to be problematic. While they share common operations, I often make mistakes by typing <em>hg add</em> instead <em>git add</em>.</p>

<p>I remember seeing project some time ago that made access to different source control software in uniform way by providing basic commands <em>commit/ci</em> <em>add</em> etc. in shell. Depending on repository it would in turn call <em>hg add</em> or <em>git add</em> etc.</p>

<p>I've seen Amp: <a href=""http://amp.carboni.ca/"" rel=""nofollow noreferrer"">http://amp.carboni.ca/</a> (which seems to be down ATM) any other scripts like that?</p>",4,0,2010-09-05 21:51:43.753000 UTC,1.0,,3,git|mercurial|dvcs,168,2009-10-14 22:54:27.773000 UTC,2022-02-25 20:54:59.323000 UTC,,1391,110,25,209,"<p>I think unifications of two (in this case <em>completely</em> different) version control systems is not a sensible thing to attempt. While there are certain commonalities, <strong>the differences far outweigh the commonalities.</strong> More specifically, while you could map certain commands from one system to a similar command in another system, there will still be <strong>semantic differences because Mercurial and Git have completely different internal models.</strong> Just consider how branching is represented or git's staging area.</p>

<p>Instead of trying to unify both systems at the ""user"" level, I think it is much more desirable to stick with one version control system and, if required, backport your changes/history to another system using a bridge (similar to git-svn).</p>

<p>Typing <code>hg add</code> vs. <code>git add</code> is not something that can get really dangerous if your current directory is not a repository managed in both systems, so you'll get a meaningful error message. </p>",3.0,2010-09-05 22:23:43.233000 UTC,2010-09-05 22:39:44.050000 UTC,10.0,[]
Delete all local changesets and revert to tree,"<p>I'm using Mercurial and I've got into a terrible mess locally, with three heads. I can't push, and I just want to delete all my local changes and commits and start again with totally clean code and a clean history.</p>

<p>In other words, I want to end up with (a) exactly the same code locally as exists in the tip of the remote branch and (b) no history of any local commits. </p>

<p>I know <code>hg update -C</code> overwrites any local changes. But how do I delete any local commits?</p>

<p>To be clear, I have no interest in preserving any of the work I've done locally. I just want the simplest way to revert back to a totally clean local checkout.</p>",7,1,2010-01-26 11:58:06.903000 UTC,32.0,2015-02-03 13:30:22.147000 UTC,95,version-control|mercurial|dvcs|head,39934,2009-10-21 17:57:36.067000 UTC,2022-02-22 21:29:27.147000 UTC,,56595,407,16,2048,"<p>When the simplest way (a new <code>hg clone</code>) isn't practical, I use <code>hg strip</code>:</p>

<pre><code>% hg outgoing -l 1
% hg strip $rev # replace $rev with the revision number from outgoing
</code></pre>

<p>Repeat until <code>hg outgoing</code> stays quiet. Note that <code>hg strip $rev</code> obliterates <code>$rev</code> and all its descendants.</p>

<p>Note that you may have to <a href=""https://www.mercurial-scm.org/wiki/StripExtension"" rel=""noreferrer"">first enable <code>strip</code> in your Mercurial settings</a>.</p>

<p><strong>PS:</strong> an even smarter approach is to use the revset language, and do:</p>

<pre><code>% hg strip 'roots(outgoing())'
</code></pre>",4.0,2010-01-27 00:18:10.637000 UTC,2018-12-17 19:42:48.393000 UTC,131.0,[]
How does Mercurial stack up against GIT and SVN?,"<p>With Phil Haack and others recently tweeting about <a href=""http://blogs.msdn.com/codeplex/archive/2010/01/22/codeplex-now-supporting-native-mercurial.aspx"" rel=""noreferrer"">CodePlex's move to support Mercurial</a> as a DVCS, I thought it might be worth a look.</p>

<p>As someone who currently uses SVN for personal projects and TFS at The Office, how does Mercurial compare in terms of usability, features and what are some of the better Mercurial hosting services available?</p>",6,4,2010-01-24 08:16:48.110000 UTC,5.0,2010-01-24 08:59:54.547000 UTC,17,git|mercurial|comparison|dvcs,1922,2008-09-17 11:07:50.093000 UTC,2022-03-03 21:11:50.300000 UTC,New Zealand,16548,3092,15,1490,"<p>As far as comparing to Git, Google recently published an interesting comparison of Git and Mercurial based on their evaluation: <a href=""http://code.google.com/p/support/wiki/DVCSAnalysis"" rel=""nofollow noreferrer"">http://code.google.com/p/support/wiki/DVCSAnalysis</a></p>",3.0,2010-01-24 08:20:26.530000 UTC,,18.0,[]
How to develop on a branch in HG?,"<p>I would like to do some experimental work in a hg project. So I would like to create branch, commit to it. And if the experiment works, I can merge it back to main branch.</p>

<p>In git, I can do</p>

<pre><code>$ git branch experimental
$ git checkout experimental
(edit file)
$ git commit -a
$ git checkout master
</code></pre>

<p>I've read <a href=""http://stevelosh.com/blog/2009/08/a-guide-to-branching-in-mercurial/"" rel=""noreferrer"">A Guide to Branching in Mercurial</a>. It said <code>hg branch feature</code>. But what is next?
I don't follow.</p>",3,0,2010-04-25 23:32:14.440000 UTC,9.0,2012-01-19 21:57:33.383000 UTC,19,version-control|mercurial|branch|dvcs,7775,2010-03-05 03:13:31.317000 UTC,2015-06-12 07:17:56.663000 UTC,,98154,2,1,1773,"<pre>$ hg branch experimental

(edit file)
$ hg commit
$ hg update default
</pre>",4.0,2010-04-25 23:35:59.013000 UTC,,24.0,[]
Assignment of mercurial global changeset id,<p>Apparently Mercurial assigns a global changeset id to each change. How do they ensure that this is unique?</p>,2,0,2010-08-25 01:04:20.307000 UTC,,2012-03-30 12:27:11.287000 UTC,3,mercurial|dvcs|sha1|changeset,651,2009-08-30 02:12:42.007000 UTC,2022-02-21 14:03:20.663000 UTC,"Sydney, Australia",106336,2961,288,7010,"<p>As Zach says, the changeset ID is computed using the <a href=""http://en.wikipedia.org/wiki/SHA-1"" rel=""noreferrer"">SHA-1 hash function</a>. This is an example of a cryptographically secure hash function. Cryptographic hash functions take an input string of arbitrary length and produces a fixed-length digest from this string. In the case of SHA-1, the output length is fixed to 160 bit, of which Mercurial by default only shows you the first 48 bit (12 hexadecimal digits).</p>

<p>Cryptographic hash functions have the property that it is extremely difficult to find two different inputs that produce the same output, that is, it is hard to find strings <code>x != y</code> such that <code>H(x) == H(y)</code>. This is called collision resistance.</p>

<p>Since Mercurial uses the SHA-1 function to compute the changeset ID, you get the same changeset ID for identical inputs (identical changes, identical committer names and dates). However, if you use different inputs (<code>x != y</code>) when you will get different outputs (changeset IDs) because of the collision resistance.</p>

<p>Put differently, if you do not get different changeset IDs for different input, then you have found a collision for SHA-1! So far, nobody has ever found a collision for SHA-1, so this will be a major discovery.</p>

<hr>

<p>In more detail, the SHA-1 hash function is used in a recursive way in Mercurial. Each changeset hash is computed by concatenating:</p>

<ul>
<li>manifest ID</li>
<li>commit username</li>
<li>commit date</li>
<li>affected files</li>
<li>commit message</li>
<li>first parent <em>changeset ID</em></li>
<li>second parent <em>changeset ID</em></li>
</ul>

<p>and then running SHA-1 on all this (see <a href=""http://selenic.com/hg/file/c00f03a4982e/mercurial/changelog.py#l231"" rel=""noreferrer"">changelog.py</a> and <a href=""http://selenic.com/hg/file/c00f03a4982e/mercurial/revlog.py#l1107"" rel=""noreferrer"">revlog.py</a>). Because the hash function is used recursively, the changeset hash will fix the entire history all the way back to the root in the changeset graph.</p>

<p>This also means that you wont get the same changeset ID if you add the line <code>Hello World!</code> to two different projects at the same time with the same commit message -- when their histories are different (different parent changesets), the two new changesets will get different IDs.</p>",1.0,2010-08-25 08:02:05.180000 UTC,,8.0,[]
Gremlin: Date filters,"<p>Filter Graph DB based on date field: I searched <a href=""http://tinkerpop.apache.org/docs/current/reference/"" rel=""noreferrer"">http://tinkerpop.apache.org/docs/current/reference/</a> but did not find a documentation for the same.</p>

<p>Did some research and it seems <code>lt</code>, <code>gt</code>, etc are working. BUT is my below approach the proper way? or is there a official way to do it? 
Below code works on Neptune and NEO4J but is this Vendor Independent.</p>

<p><em>Also found a 4/5 yr <a href=""https://stackoverflow.com/a/20570937/1897935"">old post</a> where it was recommended to use long, but I think its pretty old.</em></p>

<hr>

<p><strong>Sample data:</strong> </p>

<pre><code>g.addV(""TestDate2"").property(""title"", ""Alpha"").property(""date"", ""01-19-2018"")
g.addV(""TestDate2"").property(""title"", ""Bravo"").property(""date"", ""02-20-2018"")
g.addV(""TestDate2"").property(""title"", ""Charlie"").property(""date"", ""03-13-2018"")
g.addV(""TestDate2"").property(""title"", ""Delta"").property(""date"", ""04-14-2018"")
g.addV(""TestDate2"").property(""title"", ""Echo"").property(""date"", ""05-15-2018"")
g.addV(""TestDate2"").property(""title"", ""Foxtrot"").property(""date"", ""06-16-2018"")
g.addV(""TestDate2"").property(""title"", ""Hotel"").property(""date"", ""07-17-2018"")
g.addV(""TestDate2"").property(""title"", ""India"").property(""date"", ""08-18-2018"")
</code></pre>

<p><strong>Queries</strong>:</p>

<p><em>(I formatted the data of output so it wont really match Gremlin output buts its more readability)</em></p>

<p><strong>Less than</strong></p>

<pre><code>g.V().has(""TestDate2"", ""date"", lt(""03-03-2018"")).valueMap()
{'date': ['02-20-2018'], 'title': ['Bravo']}
{'date': ['01-19-2018'], 'title': ['Alpha']}

g.V().has(""TestDate2"", ""date"", lt(""03-24-2018"")).valueMap()
{'date': ['03-13-2018'], 'title': ['Charlie']}
{'date': ['02-20-2018'], 'title': ['Bravo']}
{'date': ['01-19-2018'], 'title': ['Alpha']}
</code></pre>

<p><strong>Greater than</strong></p>

<pre><code>g.V().has(""TestDate2"", ""date"", gt(""06-16-2018"")).valueMap()
{'date': ['07-17-2018'], 'title': ['Hotel']}
{'date': ['08-18-2018'], 'title': ['India']}

g.V().has(""TestDate2"", ""date"", gte(""06-16-2018"")).valueMap()
{'date': ['07-17-2018'], 'title': ['Hotel']}
{'date': ['06-16-2018'], 'title': ['Foxtrot']}
{'date': ['08-18-2018'], 'title': ['India']}
</code></pre>

<p><strong>Between filter</strong></p>

<pre><code>g.V().has(""TestDate2"", ""date"", between(""04-01-2018"", ""07-01-2018"")).valueMap()
{'date': ['06-16-2018'], 'title': ['Foxtrot']}
{'date': ['04-14-2018'], 'title': ['Delta']}
{'date': ['05-15-2018'], 'title': ['Echo']}
</code></pre>

<p><strong>Fails, but its fine</strong></p>

<pre><code>g.V().has(""TestDate2"", ""date"", lt(""3-3-2018"")).valueMap()
{'date': ['03-13-2018'], 'title': ['Charlie']}
{'date': ['07-17-2018'], 'title': ['Hotel']}
{'date': ['02-20-2018'], 'title': ['Bravo']}
{'date': ['06-16-2018'], 'title': ['Foxtrot']}
{'date': ['04-14-2018'], 'title': ['Delta']}
{'date': ['08-18-2018'], 'title': ['India']}
{'date': ['01-19-2018'], 'title': ['Alpha']}
{'date': ['05-15-2018'], 'title': ['Echo']}
</code></pre>",1,9,2018-07-06 07:01:31.793000 UTC,1.0,,6,neo4j|gremlin|amazon-neptune,4913,2012-12-12 13:23:28.770000 UTC,2022-03-05 14:13:23.403000 UTC,"Mumbai, India",2244,881,12,922,"<p>If you are submitting Gremlin query as a String from Python (or any other language for that matter), Amazon Neptune has a custom syntax for specifying dates. You can use the function <code>datetime()</code> to specify your date as a String in a ISO8061 format. This syntax identifies the String as a Date object and processes it accordingly. Hence, you do not have to rely on lexicographic String comparison for comparing Dates. This is documented <a href=""https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-differences.html"" rel=""noreferrer"">here</a>.</p>

<p>For example, you can write your queries as String as follows:</p>

<pre><code>// Insert Date as property
String insertQuery = ""g.addV('TestDate2').property('title','Alpha').property('date', datetime('2018-01-19T00:00:00'))"";

// Query for the date added above
String query = ""g.V().hasLabel('TestDate2').has('date',gte(datetime('1929-01-01T00:00:00'))).valueMap()"";
</code></pre>

<p>Having said that, as others have mentioned, you can also use a GLV client (submit queries using Gremlin bytecode instead of String) and use the client language's native Date implementation itself.</p>",0.0,2018-07-06 22:14:39.747000 UTC,,8.0,[]
How are you structuring your Git repository workflow?,"<p>We've seen and watched the videos on how large distributed teams are using Git, but what about those of us who aren't distributed and who work in the office with the rest of our team?  How should we be structuring our repository(ies) and our workflow?</p>

<p>Think about the traditional office which has been using Subversion or CVS as the single point of authority.  Certainly these teams could each maintain their own Git repository and push/pull between each other as necessary, which would quickly turn into a nightmare in many situations.  Or, they could each maintain their own repository and sync with a single repository that's known as the ""master"" for the team.  Or, there could be any combination of workflows with the possibilities a DVCS opens up.</p>

<p>How does your team work?  What have you found to be a useful workflow?</p>",3,0,2009-01-21 17:12:23.573000 UTC,3.0,2009-03-02 07:25:34.893000 UTC,9,git|dvcs,1823,2009-01-18 18:31:37.583000 UTC,2022-02-25 22:02:21.693000 UTC,"Richmond, VA",3265,360,34,558,"<p>I like the way the Yahoo! User Interface (YUI) team appears to be working. I am not at Yahoo, nor am I on that team, but their git commit logs reveal a lot about their process.</p>

<p>The YUI team maintains a central repository where everyone on the team has commit access. Periodically after commits to this repository (it might be after every push, but I don't think so), the build system fires, rebuilds YUI and pushes a newly tagged commit to github, where the community can fork the code and work on it.</p>

<p>I am in favor of the central repository that represents the ""official"" status of the project. Certainly, if I want to share code with a co-worker, I can arrange for them to pull a branch from me, and we can collaborate that way.</p>

<p>The ""master"" repository offers other advantages as well, such as ease of continuous integration, as the push/pull triggers can be configured on the 'master' repository to fire off the unit tests and build system. It also ensures that everyone knows where the most recent 'known good' version of the repository is, so that if the project needs to be built, published, or tested, there can be reasonable assurances that the 'master' repository is ready for that.</p>

<p>Git will support almost any workflow you can think of, but even among a small team, you don't want there to be a question about where the 'official' repository is. The maintenance nightmare that could lead to, particularly as you approach a release, would be unpleasant. </p>",1.0,2009-01-21 17:28:42.730000 UTC,,17.0,[]
Do you use distributed version control?,"<p>I'd like to hear from people who are using distributed version control (aka distributed revision control, decentralized version control) and how they are finding it. What are you using, Mercurial, Darcs, Git, Bazaar? Are you still using it? If you've used client/server rcs in the past, are you finding it better, worse or just different? What could you tell me that would get me to jump on the bandwagon? Or jump off for that matter, I'd be interested to hear from people with negative experiences as well. </p>

<p>I'm currently looking at replacing our current source control system (Subversion) which is the impetus for this question.</p>

<p>I'd be especially interested in anyone who's used it with co-workers in other countries, where your machines may not be on at the same time, and your connection is very slow.</p>

<p>If you're not sure what distributed version control is, here are a couple articles:</p>

<p><a href=""http://betterexplained.com/articles/intro-to-distributed-version-control-illustrated/"" rel=""nofollow noreferrer"">Intro to Distributed Version Control</a></p>

<p><a href=""http://en.wikipedia.org/wiki/Distributed_revision_control"" rel=""nofollow noreferrer"">Wikipedia Entry</a></p>",18,0,2008-08-25 20:46:10.427000 UTC,10.0,2009-04-10 16:30:58.597000 UTC,37,version-control|dvcs|revision,2901,2008-08-25 03:16:36.083000 UTC,2022-02-25 19:47:19.313000 UTC,"Snohomish, WA",9139,146,10,342,"<p>I've been using Mercurial both at work and in my own personal projects, and I am really happy with it.  The advantages I see are:</p>

<ol>
<li><strong>Local version control.</strong> Sometimes I'm working on something, and I want to keep a version history on it, but I'm not ready to push it to the central repositories.  With distributed VCS, I can just commit to my local repo until it's ready, without branching.  That way, if other people make changes that I need, I can still get them and integrate them into my code.  When I'm ready, I push it out to the servers.</li>
<li><strong>Fewer merge conflicts.</strong> They still happen, but they seem to be less frequent, and are less of a risk, because all the code is checked in to my local repo, so even if I botch the merge, I can always back up and do it again.</li>
<li><strong>Separate repos as branches.</strong> If I have a couple development vectors running at the same time, I can just make several clones of my repo and develop each feature independently.  That way, if something gets scrapped or slipped, I don't have to pull pieces out.  When they're ready to go, I just merge them together.</li>
<li><strong>Speed.</strong> Mercurial is much faster to work with, mostly because most of your common operations are local.</li>
</ol>

<p>Of course, like any new system, there was some pain during the transition.  You have to think about version control differently than you did when you were using SVN, but overall I think it's very much worth it.</p>",3.0,2008-08-25 22:16:04.377000 UTC,2009-04-10 16:05:34.553000 UTC,30.0,[]
Reading avro data with Databricks from Azure Data Lake Gen1 generated by Azure EventHubs Capture fails,"<p>I am trying to read avro data from Azure Data Lake Gen1, generated from Azure EventHubs with Azure Event Hubs Capture enabled in Azure Databricks with pyspark:</p>

<pre><code>inputdata = ""evenhubscapturepath/*/*""
rawData = spark.read.format(""avro"").load(inputdata)
</code></pre>

<p>The following statement fails</p>

<pre><code>rawData.count()
</code></pre>

<p>with</p>

<pre><code>org.apache.spark.SparkException: Job aborted due to stage failure: Task 162 in stage 48.0 failed 4 times, most recent failure: Lost task 162.3 in stage 48.0 (TID 2807, 10.3.2.4, executor 1): java.io.IOException: Not an Avro data file
</code></pre>

<p>Is EventHub-Capture writing non-Avro data? Are there any best practices for reading EventHub captured data with Spark ?</p>",2,0,2019-12-01 15:39:14.253000 UTC,3.0,,1,azure|pyspark|azure-eventhub|azure-databricks|azure-eventhub-capture,1791,2017-01-19 13:13:39.920000 UTC,2021-04-10 08:52:48.693000 UTC,"Hebertsfelden, Germany",25,136,0,6,"<p>One pattern implementing a cold ingestion path is using <a href=""https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview"" rel=""noreferrer"">Event Hubs Capture</a>. EventHubs capturing writes one file per partition as defined with the <a href=""https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview#capture-windowing"" rel=""noreferrer"">windowing parameters</a>. The data is written in avro format and can be analyzed with Apache Spark. </p>

<p>So what are best practices using this functionality?</p>

<p><strong>1. Do not over-partition</strong></p>

<p>Often I have seen people using the default configuration which finally often results in many small files. If you want to consume the data ingested via EventHubs Capture with Spark, keep in mind the best practices for <a href=""https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-best-practices"" rel=""noreferrer"">file sizes in Azure Data Lake Store</a> and <a href=""https://github.com/Azure/AzureDatabricksBestPractices/blob/master/toc.md#partition-your-data"" rel=""noreferrer"">partitions</a> with Spark. File sizes should be ~256 MB and partitions between 10 and 50 GB. So finally the confguration depends on the number and sizes of the messages you are consuming. In most cases you are doing fine with just partitioning your data per ingest-date.</p>

<p><strong>2. Check ""Do not emit empty files option""</strong></p>

<p>You should check ""Do not emit empty files option"". If you want to consume the data with Spark that saves unnecessary file operations. </p>

<p><strong>3. Use the data origin in your file pathes</strong></p>

<p>With a streaming architecture your EventHub is what a <em>Landing Zone</em> would be in a batch oriented architecture approach. So you will ingest the data in a raw-data-layer. Good practice is to use data sources instead of the name of the EventHub in the directory path. So for example if you are ingesting telemetry data from robots in your factory this could be the directory path <em>/raw/robots/</em></p>

<p>The storage naming requires all attributes like {Namesapce}, {PartitionId} to be used. So finally the a good capture file format definition with an explicitly defined path, a daily partition and use of the remaining attributes for the filename in an Azure Data Lake Gen 2 could look like this: </p>

<pre><code> /raw/robots/ingest_date={Year}-{Month}-{Day}/{Hour}{Minute}{Second}-{Namespace}-{EventHub}-{PartitionId}
</code></pre>

<p><a href=""https://i.stack.imgur.com/QWZLI.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/QWZLI.jpg"" alt=""enter image description here""></a></p>

<p><strong>4. Think of a compaction job</strong></p>

<p>Captured data is not compressed and also might end up in to small files in your use case (as minimum write frequency is 15 minutes). So if necessary write a compaction job running once a day. Something like</p>

<pre><code>df.repartition(5).write.format(""avro"").save(targetpath)
</code></pre>

<p>will do this job.</p>

<p>So what are now the best practices for reading the captured data?</p>

<p><strong>5. Ignore non avro-files reading the data</strong></p>

<p>Azure EventHubs Capture writes temporary data to Azure Data Lake Gen1. Best practice is only to read data with avro-extension. You can easily achive this via a spark configuration:</p>

<pre><code>spark.conf.set(""avro.mapred.ignore.inputs.without.extension"", ""true"")
</code></pre>

<p><strong>6. Read only relevant partitions</strong></p>

<p>Consider reading only relevant partitions, e. g. filter the current ingestion day.</p>

<p><strong>7. Use shared metadata</strong></p>

<p>Reading the captured data works similar than reading the data directly from Azure EventHubs.
So you have to have a schema. Assuming that you also have jobs reading the data directly with Spark Structured Streaming a good pattern is to store the metadata and share it. You could just store this metadata in a Data Lake Store json file:</p>

<pre><code>[{""MeasurementTS"":""timestamp"",""Location"":""string"", ""Temperature"":""double""}]
</code></pre>

<p>and read it with this <a href=""https://stackoverflow.com/questions/32028149/pyspark-importing-schema-through-json-file"">simple parsing function</a>:</p>

<pre><code># parse the metadata to get the schema
from collections import OrderedDict 
from pyspark.sql.types import *
import json

ds = dbutils.fs.head (metadata)                                                 # read metadata file

items = (json
  .JSONDecoder(object_pairs_hook=OrderedDict)
  .decode(ds)[0].items())

#Schema mapping 
mapping = {""string"": StringType, ""integer"": IntegerType, ""double"" : DoubleType, ""timestamp"" : TimestampType, ""boolean"" : BooleanType}

schema = StructType([
    StructField(k, mapping.get(v.lower())(), True) for (k, v) in items])
</code></pre>

<p>So you could just reuse your schema:</p>

<pre><code>from pyspark.sql.functions import *

parsedData = spark.read.format(""avro"").load(rawpath). \
  selectExpr(""EnqueuedTimeUtc"", ""cast(Body as string) as json"") \
 .select(""EnqueuedTimeUtc"", from_json(""json"", schema=Schema).alias(""data"")) \
 .select(""EnqueuedTimeUtc"", ""data.*"")
</code></pre>",0.0,2019-12-08 13:09:52.363000 UTC,,9.0,[]
Mercurial color extension in Windows Powershell,"<p>Is there a way to enable color support for Mercurial in Powershell on Windows 7?  The <a href=""https://www.mercurial-scm.org/wiki/ColorExtension"" rel=""nofollow noreferrer"">ColorExtension</a> page says to add</p>

<pre><code>[color]
mode = win32
</code></pre>

<p>to your <code>.hgrc</code> file, but it doesn't seem to make a difference.</p>

<p>Running <code>hg status</code> shows several files that have not yet been added to the repository, and I believe they should have a pink color (based off other terminals I've seen).  This is what's displayed instead:</p>

<pre><code>←[0;35;1;4m? samplefile.php←[0m
←[0;35;1;4m? anotherfile.php←[0m
←[0;35;1;4m? derpderp.xml←[0m
←[0;35;1;4m? derp_model.php←[0m
←[0;35;1;4m? stillnocolor.php←[0m
</code></pre>",3,0,2010-10-21 03:08:04.297000 UTC,2.0,2018-02-22 12:10:52.667000 UTC,16,windows|powershell|mercurial|dvcs,1804,2010-01-11 02:57:16.937000 UTC,2022-03-04 21:07:06.743000 UTC,,16978,1799,144,1703,"<p>You probably need to update to the latest mercurial (1.6.4).  Version 1.5.4 in particular didn't do win32 color properly (and there have been quite a few color-related and win32-related bugs fixed in recent builds).</p>

<p>You also need to make sure you're not specifying ANSI color mode. You can force win32:</p>

<pre><code>[color]
mode = win32
</code></pre>",2.0,2010-10-25 14:53:49.760000 UTC,,15.0,[]
"Getting mercurial ""hg commit"" to work with Notepad++","<p>I just replaced notepad with notepad++ using <a href=""http://www.binaryfortress.com/NotepadReplacer/"">NotepadReplacer</a> and now my <code>hg commit</code> isn't working correctly.</p>

<p>Before replacing Notepad, <code>hg commit</code> would launch a new text file for me to enter my commit message into. With Notepad++, I'm prompted to create the file, and mercurial aborts the commit because of an empty commit message.</p>

<p>I know I can enter the message inline with the <code>hg commit</code> with the <code>-m</code> option, but I'd like to have the option of entering the commit message in the text file. I like having the status displayed in the text file as context for the commit message without having to type <code>hg stat</code> before <code>hg commit</code>.</p>

<p>How can I get this to work?</p>",1,1,2012-04-23 16:12:40.040000 UTC,1.0,,14,mercurial|notepad++|dvcs,1781,2009-07-15 19:32:05.583000 UTC,2022-03-05 22:41:39.500000 UTC,"Dallas, TX",20362,3439,7,2794,"<p>Try placing in your <code>%USERPROFILE%\.hgrc</code> (or <code>%USERPROFILE%\mercurial.ini</code>) something along the lines of:</p>

<pre><code>[ui]
editor = path/to/notepad++ -multiInst -nosession
</code></pre>

<p>Perhaps adding a <code>-notabbar</code> after <code>-nosession</code> could prove useful, too.</p>",3.0,2012-04-23 18:45:53.653000 UTC,,21.0,[]
"bitbucket, ""hg push"" and ""hg update""","<p>If I start out with a local mercurial repo, which I consider to be the ""main"" repo (pardon me my dvcs lords), and intend to use bitbucket as a backup and issue tracking facility, I can do all my changes in my local repo and do an ""hg push"" to send the changes back to bitbucket.</p>

<p>Don't I need to follow this ""hg push"" command run on my local machine with an ""hg update""?</p>",4,1,2009-07-27 12:16:54.950000 UTC,8.0,2009-07-27 12:24:56.217000 UTC,16,mercurial|dvcs|bitbucket,20424,2008-12-17 04:44:27.180000 UTC,2013-05-18 04:45:18.520000 UTC,,6628,26,4,452,"<p>Why do you care what's in the working directory on BitBucket's servers?  As long as you push the changes will be in the repository and visible on the BitBucket page.</p>

<p><strong>EDIT:</strong> OK, I'm going to edit this to be a useful answer.</p>

<p>Say you clone down one of my repositories like django-hoptoad on BitBucket.  You'll have a folder named <code>django-hoptoad</code> on your local machine and its content will look something like this:</p>

<pre><code>django-hoptoad/
 |
 +-- .hg/
 |
 +-- ... my code and other folders
</code></pre>

<p>All the data about the repository itself is stored in the <code>.hg/</code> folder.  That's where Mercurial keeps the data about which files were changed in which changesets, and lots of other stuff.</p>

<p>You can think of it like this (though it's an oversimplification):</p>

<pre><code>django-hoptoad/
 |
 +-- .hg/
 |    |
 |    +-- data about changeset 1
 |    +-- data about changeset 2
 |
 +-- ... my code and other folders as they appear in changeset 2
</code></pre>

<p>When you run <code>hg pull</code> and don't update, you pull any new changesets into the repository:</p>

<pre><code>django-hoptoad/
 |
 +-- .hg/
 |    |
 |    +-- data about changeset 1
 |    +-- data about changeset 2
 |    +-- data about changeset 3 (NEW)
 |    +-- data about changeset 4 (NEW)
 |
 +-- ... my code and other folders as they appear in changeset 2
</code></pre>

<p>If you don't update, the <code>... my code and other folders</code> will still be equivalent to whatever is in <code>changeset 2</code>, but the other changesets are still in the repository.</p>

<p>When you run <code>hg update</code> Mercurial will update the <code>... my code and other folders</code> to the contents of the newest changeset.</p>

<pre><code>django-hoptoad/
 |
 +-- .hg/
 |    |
 |    +-- data about changeset 1
 |    +-- data about changeset 2
 |    +-- data about changeset 3
 |    +-- data about changeset 4
 |
 +-- ... my code and other folders as they appear in changeset 4
</code></pre>

<p>Really, this means that what happens to be in <code>... my code and other folders</code> doesn't have to match what's in the repository.  You could just delete it and all the changesets would still be in the repository:</p>

<pre><code>django-hoptoad/
 |
 +-- .hg/
      |
      +-- data about changeset 1
      +-- data about changeset 2
      +-- data about changeset 3
      +-- data about changeset 4
</code></pre>

<p>If you committed right now, it would create a new changeset that basically says ""no files"".  You don't have to commit though.  People can still push and pull from you because the repository still has all the data about the changesets.</p>

<p>This is almost certainly what BitBucket is doing.  You're never going to log in to BitBucket's servers, edit your code and commit there -- you're only ever going to push/pull/clone.  That means the <code>... my code and other folders</code> will never actually be used, so I'd imagine Jesper has it set up to delete it to save the disk space.</p>

<p>Since <code>hg update</code> only really affects the working directory, and the working directory on BitBucket is never used, you don't need to run <code>hg update</code> after you push to BitBucket.</p>",1.0,2009-07-27 14:59:47.220000 UTC,2009-07-27 21:45:24.783000 UTC,38.0,[]
Select spark dataframe column with special character in it using selectExpr,"<p>I am in a scenario where my columns name is <code>Município</code> with accent on the letter <code>í</code>.</p>

<p>My <code>selectExpr</code> command is failing because of it. Is there a way to fix it? Basically I have something like the following expression:</p>

<pre><code>.selectExpr(""...CAST (Município as string) as Município..."")
</code></pre>

<p>What I really want is to be able to leave the column with the same name that it came, so in the future, I won't have this kind of problem on different tables/files.</p>

<p>How can I make spark dataframe accept accents or other special characters?</p>",1,0,2019-09-16 19:55:53.937000 UTC,1.0,2019-09-16 20:42:00.690000 UTC,4,pyspark|apache-spark-sql|special-characters|azure-databricks,6328,2015-09-15 08:29:24.193000 UTC,2021-08-26 20:22:38.807000 UTC,"São Paulo, State of São Paulo, Brazil",328,306,3,79,"<p>You can use wrap your column name in backticks. For example, if you had the following schema:</p>

<pre><code>df.printSchema()
#root
# |-- Município: long (nullable = true)
</code></pre>

<p>Express the column name with the special character wrapped with the backtick:</p>

<pre><code>df2 = df.selectExpr(""CAST (`Município` as string) as `Município`"")
df2.printSchema()
#root
# |-- Município: string (nullable = true)
</code></pre>",0.0,2019-09-16 20:40:25.033000 UTC,,9.0,[]
Source Control - Distributed Systems vs. Non Distributed - What's the difference?,"<p>I just read Spolsky's last piece about Distributed vs. Non-Distributed version control systems <a href=""http://www.joelonsoftware.com/items/2010/03/17.html"" rel=""noreferrer"">http://www.joelonsoftware.com/items/2010/03/17.html</a>. What's the difference between the two? Our company uses TFS. What camp does this fall in?</p>",6,1,2010-03-18 20:24:39.427000 UTC,3.0,2010-03-19 10:15:39.853000 UTC,14,version-control|tfs|mercurial|dvcs,7786,2009-08-26 13:45:27.383000 UTC,2022-03-06 05:00:13.047000 UTC,Ohio,44967,1364,130,2774,"<p>Simply speaking, a centralized VCS (including TFS) system has a central storage and each users gets and commits to this one location.</p>

<p>In distributed VCS, each user has the full repository and can make changes that are then synchronized to other repositories, a server is usually not really necessary.</p>",2.0,2010-03-18 20:31:16.713000 UTC,,12.0,[]
Reasons for not working on the master branch in Git,"<p>So, I'm fairly new to git and I've after a bit of reading around over the last couple of weeks I've read a few people saying that the master branch shouldn't be changed but rather branched from and then merged to.</p>

<p>I'm happy enough to work with branches but was wondering for the reasons behind not working on the master branch?</p>",5,0,2011-04-19 08:25:01.383000 UTC,8.0,,41,git|version-control|branch|dvcs,15913,2011-01-17 14:33:35.803000 UTC,2021-02-28 07:54:10.377000 UTC,"Shrewsbury, United Kingdom",1172,142,11,121,"<p>i guess the usual reasoning is, that the master branch should represent the 'stable' history of your code. use branches to experiment with new features, implement them, and when they have matured enough you can merge them back to master.</p>

<p>that way code in master will almost always build without problems, and can be mostly used directly for releases.</p>

<p>let's take git.git (the official git repository) as an example. there are several branches, most noticable:</p>

<ul>
<li><a href=""http://git.kernel.org/?p=git/git.git;a=shortlog;h=refs/heads/master"">master</a></li>
<li><a href=""http://git.kernel.org/?p=git/git.git;a=shortlog;h=refs/heads/next"">next</a></li>
<li><a href=""http://git.kernel.org/?p=git/git.git;a=shortlog;h=refs/heads/pu"">pu</a></li>
<li><a href=""http://git.kernel.org/?p=git/git.git;a=shortlog;h=refs/heads/maint"">maint</a></li>
</ul>

<p>so, <code>master</code> contains code which is very likely to end up in the next release of git. <code>next</code> contains tested code, which will potentially be merged into the <code>master</code> branch. <code>pu</code> (proposed updates, iirc) contains quite new (and probably) untested code.</p>

<p><code>pu</code> is considered unstable and will be reset and rebased to junio's liking. <code>next</code> might get reset after a release or during a release cycle, but this is less common. <code>master</code> is set in stone and never changed after it's been pushed and made publicly available.</p>

<p>you see, that changes will get merged from <code>pu</code> to <code>next</code> and from <code>next</code> to <code>master</code> if they are deemed worthy and don't break stuff.</p>

<p>the branch <code>maint</code> is used to make bugfixes which should also apply to older versions of git. <code>maint</code> is usually merged to <code>next</code> and/or <code>master</code>.</p>

<p>you can inspect the branches on <a href=""http://git.kernel.org/?p=git/git.git;a=summary"">http://git.kernel.org/?p=git/git.git;a=summary</a></p>",3.0,2011-04-19 08:31:08.490000 UTC,2011-04-19 08:40:41.867000 UTC,34.0,[]
AWS Neptune bulk delete data,"<p>I want to bulk delete nodes in the same way we can bulk load data using a curl operation in AWS Neptune. Is there an API convention for gremlin like I see for SPARQL? If so can you please post reference? </p>

<p>If there is no bulk delete via the API, how feasible is it to bulk delete with the gremlin python sdk? </p>",2,0,2020-03-09 15:44:12.130000 UTC,,,4,gremlin|amazon-neptune,1085,2015-12-25 04:06:14.027000 UTC,2022-02-07 19:20:17.893000 UTC,,127,7,0,4,"<p>There is an example of how to delete a graph using multi threaded Python at this location. The code could be further improved to work with very large graphs. That is discussed in the comments. I have successfully deleted graphs with 20+ million vertices using this code.</p>
<p><a href=""https://github.com/awslabs/amazon-neptune-tools/tree/master/drop-graph"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-neptune-tools/tree/master/drop-graph</a></p>
<p><strong>EDITED 2021-11-17</strong>
If you want to delete everything, Amazon Neptune now provides a &quot;fast reset&quot; API that allows you to efficiently delete all the data in a cluster.</p>
<p><a href=""https://docs.aws.amazon.com/neptune/latest/userguide/manage-console-fast-reset.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/neptune/latest/userguide/manage-console-fast-reset.html</a></p>",2.0,2020-03-09 15:50:42.143000 UTC,2021-11-17 15:24:51.390000 UTC,8.0,[]
Azure Databricks - Are there any best practices while creating folder structure in workspace and managing permissions on folders?,"<p>I have provisioned a new databricks workspace for production and want to create folders inside based on some best practices or pointers.</p>
<p>Also I want to manage the permissions to these folders.</p>
<p>What are the key points to take care of, while provisioning a clean production databricks workspace?</p>",1,1,2021-06-25 05:18:22.527000 UTC,,2021-06-28 11:59:25.220000 UTC,-1,azure|azure-databricks,462,2020-12-28 06:35:49.857000 UTC,2022-03-04 07:53:53.450000 UTC,,159,0,0,22,"<p>You can find azure Databricks best practices <a href=""https://docs.microsoft.com/en-in/azure/databricks/best-practices-index"" rel=""nofollow noreferrer"">here</a> for users and administrators.</p>
<p>Git Hub reference doc for <a href=""https://github.com/Azure/AzureDatabricksBestPractices/blob/master/toc.md"" rel=""nofollow noreferrer"">Azure Databricks Best Practices</a></p>
<p>Folders: Folders contain all static assets within a workspace: notebooks, libraries, experiments, and other folders. Icons indicate the type of the object contained in a folder. <a href=""https://docs.microsoft.com/en-us/azure/databricks/workspace/workspace-objects"" rel=""nofollow noreferrer"">Workspace objects</a> An Azure Databricks workspace has three special folders: Workspace, Shared, and Users.You cannot rename or move a special folder.</p>
<p>Permissions: By default, all users can create and modify workspace objects—including folders, notebooks, experiments, and models—unless an administrator enables workspace access control.You can assign five permission levels to folders: No Permissions, Read, Run, Edit, and Manage. Refer <a href=""https://docs.microsoft.com/en-us/azure/databricks/security/access-control/workspace-acl#--folder-permissions"" rel=""nofollow noreferrer"">this</a> for permissions. Access control is available only in the Azure Databricks Premium Plan.</p>",0.0,2021-06-25 09:42:52.007000 UTC,2021-06-25 09:52:26.523000 UTC,-1.0,[]
git: Simple solution for pushing between working copies,"<p><b>What I want to do:</b> On my (ssh remotely accessible) university machine I work on a project which I have put under git source control (<code>git init</code>, then <code>git commit -a</code> after every change, all works fine). Now I want to want to work on that project on my private machine at home. Should be easy, since git is a distributed vcs, right?</p>

<p>I read the <a href=""http://www.kernel.org/pub/software/scm/git/docs/gittutorial.html"" rel=""noreferrer"">git tutorial</a>, which suggests to do a <code>git pull</code> at university to get the changes done at home. That won't work, since my machine at home is not remotely accessible. So I thought that I'd do a <code>git push</code> at home. That works, but it's complicated (requires <code>git reset</code> at university afterwards, etc.), <a href=""http://www.gitready.com/advanced/2009/02/01/push-to-only-bare-repositories.html"" rel=""noreferrer"">since non-bare repositories are not designed for pushing</a>.</p>

<p><b>Question 1:</b> Is there an easier way than adding a additional bare repository to my setup (which would mean that I had: (1) the ""main"" bare repository, (2) the university working copy, (3) the home working copy)?<br />
<sub>&lt;Rant>If I really need that setup, I could have stayed with SVN.&lt;/Rant></sub></p>

<p><b>Question 2:</b> If that setup is really needed, how do I create that bare repository (<code>git clone --bare</code>, I guess) and <em>make it the ""main"" repository</em>, i.e., tell the working copies that <code>git push</code> is supposed to go <em>there</em>.</p>

<p><b>PS:</b> I know that there's a post-receive hook floating around that allows you to push into non-bare repositories. I tried it, but it didn't work well since the git version on the university machine is quite old (1.5.5.6) and misses some commands used by the hook. Updating is not an option, and I'd prefer a solution without third-party scripts anyway.</p>",3,2,2010-01-24 13:08:30.153000 UTC,4.0,,14,git|repository|dvcs|git-push,3538,2009-04-06 16:20:11.037000 UTC,2022-03-05 17:40:11.293000 UTC,"Vienna, Austria",156889,6951,312,5479,"<p>You really shouldn't push to the checked out branch as it effectively pulls the rug from under the remote working copy. It's then difficult to work out if the working tree is modified because the branch head has moved or if there were also local changes which would be lost by a <code>reset --hard</code>.</p>

<p>The simplest thing to do is to push to a different branch. You can then merge this into the working copy's checkout out branch (or rebase the local branch onto it) when you have access to the remote machine and need to work on it.</p>

<p>From home:</p>

<pre><code>git push origin HEAD:from-home
</code></pre>

<p>From 'work':</p>

<pre><code>git merge from-home
</code></pre>

<p>You can set up your config to default to a particular push refspec.</p>

<p>e.g.</p>

<pre><code>git config remote.origin.push +master:from-home
</code></pre>

<p>A bare repository is often more natural. You can either clone it from an existing repository or, what I usually do, initialize a new repository and push the master branch that I want to it from an existing repository.</p>

<p>Better still, if you're going to use working copies at each location, is to use this trick to <em>directly modify</em> the remote's remotes, rather than a speically renamed branch.</p>

<p>So, on origin, create a remote called 'home' -- you obviously can't fetch from it because of your network configuration.  That doesn't matter.</p>

<p>On home, tell it, ""When I push to origin, have it update <em>the origin's remote named home</em>:</p>

<pre><code>git config remote.origin.push +master:home/master
</code></pre>

<p>Now, things get really slick.  From home, run <code>git push origin</code>, and go to origin, and run <code>git status</code> or <code>git branch -a -v</code> -- What you will see is something like: ""master is behind home/master by 3 commits and can be fast forwarded.""</p>

<p>In other words, using home to push a change to origin's remote named home, is functionally the same as using origin to pull from home.</p>

<p>The one downside here is that you'll need to continually do new git config settings as you create additional branches on home.  That's the overhead you pay for your network setup.  Thankfully, it's simple and only happens once per branch create.</p>",3.0,2010-01-24 13:27:34.220000 UTC,2013-08-29 15:25:16.250000 UTC,14.0,[]
Mercurial error: repository is unrelated,"<p>I've just started with Mercurial, I have a 'central' repository on Bitbucket which I cloned onto one machine and made changes and committed and pushed. I then cloned from Bitbucket to another machine committed and pushed which was fine. I then came back to the first machine, made changes committed and attempted to push, but got the error message. What am I doing wrong? Should I have pulled first? How can I resolve the error and push? Any help is appreciated! </p>

<p>Darren.</p>",4,0,2011-09-05 22:16:37.130000 UTC,3.0,2012-03-30 12:09:46.477000 UTC,35,mercurial|dvcs|bitbucket,25685,2009-07-14 14:02:31.550000 UTC,2022-03-04 10:14:38.763000 UTC,"Manchester, United Kingdom",1033,12,3,132,"<p>A Mercurial repository gets its identity when you make the first commit in it. When you create a new repository on Bitbucket, you create an empty repository with no identity.</p>

<p>When you clone this repository to machine A and make a commit and push it back, then you brand the repository. If you have cloned the repository on the second machine <em>before</em> pushing from the first, then you can end up in the situation you describe.</p>

<p>Please run <code>hg paths</code> on the machine where you cannot push. Then make a separate clone of the repository it says it will push to. Now examine the first changeset in each repository with</p>

<pre><code>hg log -r 0
</code></pre>

<p>If the initial changesets are different, then you have two unrelated repositories, as we call it in Mercurial. You can then export the changes you cannot push as patches and import them in the other.</p>",5.0,2011-09-06 07:19:21.703000 UTC,,38.0,[]
Consequences of using graft in Mercurial,"<p>There've been several questions recently about skipping changes when maintaining release branches in Mercurial. For example:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/9532823/mercurial-branch-specific-changes-keep-coming-back-after-dummy-merge"">Mercurial: Branch specific changes keep coming back after dummy merge</a></li>
<li><a href=""https://stackoverflow.com/questions/9500399/why-are-mercurial-backouts-in-one-branch-affecting-other-branches"">Why are Mercurial backouts in one branch affecting other branches?</a></li>
</ul>

<p>Since it was introduced in 2.0, I've wondered about using <code>graft</code> to avoid this problem. Given a revision tree like this:</p>

<pre><code>A---B---C---D---E---F---G---H---I---J
</code></pre>

<p>Suppose we need to create a release branch that skips the Evil change <code>E</code>.</p>

<pre><code>hg update -r D
hg graft ""F::J""
</code></pre>

<p>giving us:</p>

<pre><code>A---B---C---D---E---F---G---H---I---J
             \
              --F'--G'--H'--I'--J'
</code></pre>

<ul>
<li>Q1: What just happened here? I can understand that <code>transplant</code> would have generated patches out of <code>F::J</code>, and then applied them onto <code>D</code>, but <code>graft</code> is said to use the 3-way merge rather than patches. So....... how does that work? Why is it better?</li>
</ul>

<p>Lets say I now fix <code>E</code>, and merge that into my release branch.</p>

<pre><code>                  --E2-----------------
                 /                     \
A---B---C---D---E---F---G---H---I---J---M1
             \                            \
              --F'--G'--H'--I'--J'---------M2--
</code></pre>

<p>M1 is a straight merge; nothing special there.
M2 is merging branches which have ""the same"" (or at least equivalent) changes on.</p>

<ul>
<li>Q2: Is this merge just a normal 3-way merge using <code>D</code>, <code>J'</code> and <code>M1</code>?</li>
<li>Q3: Has mercurial stored/used extra information about the graft operation to help it with the merge?</li>
</ul>

<p>And finally...</p>

<ul>
<li>Q4: What are the potential problems with a flow like this?</li>
</ul>",2,0,2012-03-07 09:24:09.133000 UTC,39.0,2017-05-23 11:45:43.127000 UTC,98,version-control|mercurial|branch|dvcs|cherry-pick,20567,2010-04-06 06:08:24.493000 UTC,2020-02-25 18:49:04.210000 UTC,"London, United Kingdom",7475,122,9,545,"<p>When you update to <code>D</code> and graft <code>F::J</code>, Mercurial runs a number of merges. It will start with this merge:</p>

<pre><code>M = three_way_merge(local=D, other=F, base=E)
</code></pre>

<p>If we write <code>+d</code> for the delta between the states <code>C</code> and <code>D</code>, then we start with:</p>

<pre><code>        +d     +e     +f
---- C ---- D ---- E ---- F ----
</code></pre>

<p>Turn the graph 90 degrees clockwise and the above three-way merge looks like this:</p>

<pre><code>    -e  
  .---- D
 /
E
 \
  '---- F
    +f
</code></pre>

<p>That is, we pretend that we started with <code>E</code> and applied the opposite of <code>-e</code> to get to <code>D</code>.  I think of as the reverse patch of <code>+e</code>. Starting in <code>E</code> we also went to state <code>F</code> with the normal delta <code>+f</code>. There's nothing strange here — we have all the states (<code>D</code>, <code>E</code>, and <code>F</code>) in the repository already. So seen like this, it's clear that we can merge <code>D</code> and <code>F</code>.</p>

<p>Merging is a matter of ""completing the diamond"". So we find a new state <code>M</code> that is a mix of <code>D</code> and <code>F</code> and where the difference from <code>D</code> to <code>M</code> is similar to <code>+f</code> and the difference from <code>F</code> to <code>M</code> is similar to <code>-e</code>. It looks like this:</p>

<pre><code>    -e     +f'
  .---- D ----.
 /             \
E               M
 \             /
  '---- F ----'
    +f     -e'
</code></pre>

<p>The <code>+f</code> delta became <code>+f'</code> and the <code>-e</code> delta became <code>-e'</code>. This is just a normal three-way merge, but the effect is interesting: we've applied <code>F</code> onto <code>D</code> instead of <code>E</code>!</p>

<p>After the merge, the second parent of <code>M</code> to <code>F</code> is dropped:</p>

<pre><code>    -e     +f'
  .---- D ----.
 /             \
E               M
 \
  '---- F
    +f
</code></pre>

<p>To reiterate: We have copied the ""effect"" of <code>F</code> onto <code>D</code>, that is, we have found a delta (<code>+f'</code>) that applied to <code>D</code> give the same effect as when <code>+f</code> was applied to <code>E</code>. We can straighten the graph a bit to get:</p>

<pre><code>       +f'
--- D ---- M
     \
      '---- E ---- F
        +e     +f
</code></pre>

<p>The result is that <code>F</code> is grafted onto <code>D</code> using the full three-way machinery.</p>

<ul>
<li><p><strong>Q1:</strong> What just happened here?  So....... how does that work? Why is it better?</p>

<p><strong>A1:</strong> Using merges is better than patches since the merge machinery takes things like renames into account.</p></li>
<li><p><strong>Q2:</strong> Is this merge just a normal 3-way merge using D, J' and M1?</p>

<p><strong>A2:</strong> Yes, grafting does not alter the topology of the graph.</p></li>
<li><p><strong>Q3:</strong> Has mercurial stored/used extra information about the graft operation to help it with the merge?</p>

<p><strong>A3:</strong> No.</p></li>
<li><p><strong>Q4:</strong> What are the potential problems with a flow like this?</p>

<p><strong>A4:</strong> From a merge perspective it should work okay. It will duplicate some history which might be confusing for people.</p></li>
</ul>",8.0,2012-03-07 16:24:43.000000 UTC,2014-12-04 14:26:07.120000 UTC,119.0,[]
What makes merging in DVCS easy?,"<p>I read at <a href=""http://www.joelonsoftware.com/items/2010/03/17.html"" rel=""nofollow noreferrer"">Joel on Software</a>:</p>

<blockquote>
  <p>With distributed version control, the
  distributed part is actually not the
  most interesting part.</p>
  
  <p>The interesting part is that these
  systems think in terms of changes, not
  in terms of versions.</p>
</blockquote>

<p>and at <a href=""http://hginit.com/00.html"" rel=""nofollow noreferrer"">HgInit</a>:</p>

<blockquote>
  <p>When we have to merge, Subversion
  tries to look at both revisions—my
  modified code, and your modified
  code—and it tries to guess how to
  smash them together in one big unholy
  mess. It usually fails, producing
  pages and pages of “merge conflicts”
  that aren’t really conflicts, simply
  places where Subversion failed to
  figure out what we did.</p>
  
  <p>By contrast, while we were working
  separately in Mercurial, Mercurial was
  busy keeping a series of changesets.
  And so, when we want to merge our code
  together, Mercurial actually has a
  whole lot more information: it knows
  what each of us changed and can
  reapply those changes, rather than
  just looking at the final product and
  trying to guess how to put it
  together.</p>
</blockquote>

<p>By looking at the SVN's repository folder, I have the impression that Subversion is maintaining each revisions as <em>changeset</em>. And from what I know, Hg is using both <em>changeset</em> and <em>snapshot</em> while Git is purely using <em>snapshot</em> to store the data.</p>

<p>If my assumption is correct, then there must be other ways that make merging in DVCS easy. What are those?</p>

<p><strong>* Update:</strong>  </p>

<ul>
<li>I am more interested in the technical perspective, but answers from non-technical perspective are acceptable</li>
<li>Corrections:

<ol>
<li>Git's <em>conceptual</em> model is purely based on snapshots. The snapshots can be stored as diffs of other snapshots, it's just that the diffs are purely for storage optimization. – <a href=""https://stackoverflow.com/users/12166/rafal-dowgird"">Rafał Dowgird</a>'s <a href=""https://stackoverflow.com/questions/2613525/what-makes-merging-in-dvcs-easy#comment2626335_2613525"">comment</a>  </li>
</ol></li>
<li>From non-technical perspective:

<ol>
<li>It's simply cultural: a DVCS wouldn't work at all if merging were hard, so DVCS developers invest a lot of time and effort into making merging easy. CVCS users OTOH are used to crappy merging, so there's no incentive for the developers to make it work. (Why make something good when your users pay you equally well for something crap?)<br>
...<br>
To recap: the whole point of a DVCS is to have many decentralized repositories and constantly merge changes back and forth. Without good merging, a DVCS simply is useless. A CVCS however, can still survive with crappy merging, especially if the vendor can condition its users to avoid branching. – <a href=""https://stackoverflow.com/users/2988/"">Jörg W Mittag</a>'s <a href=""https://stackoverflow.com/questions/2613525/what-makes-merging-in-dvcs-easy#2613595"">answer</a></li>
</ol></li>
<li>From technical perspective:

<ol>
<li>recording a real DAG of the history does help! I think the main difference is that CVCS didn't always record a merge as a changeset with several parents, losing some information. – <a href=""https://stackoverflow.com/users/148532/tonfa"">tonfa</a>'s <a href=""https://stackoverflow.com/questions/2613525/what-makes-merging-in-dvcs-easy#comment2624709_2613595"">comment</a></li>
<li>because of <strong><em>merge tracking</em></strong>, and the more fundamental fact that <strong><em>each revisions knows its parents</em></strong>. ... When each revision (each commit), including merge commits, know its parents (for merge commits that means having/remembering more than one parent, i.e. merge tracking), you can reconstruct diagram (DAG = Direct Acyclic Graph) of revision history. If you know graph of revisions, you can find common ancestor of the commits you want to merge. And when your DVCS knows itself how to <strong><em>find common ancestor</em></strong>, you don't need to provide it as an argument, as for example in CVS.<br>
.<br>
Note that there might be more than one common ancestor of two (or more) commits. Git makes use of so called ""recursive"" merge strategy, which merges merge bases (common ancestor), till you are left with one virtual / effective common ancestor (in some simplification), and can the do simple 3-way merge. – <a href=""https://stackoverflow.com/users/46058/jakub-narebski"">Jakub Narębski</a>'s <a href=""https://stackoverflow.com/questions/2613525/what-makes-merging-in-dvcs-easy#2614937"">answer</a></li>
</ol></li>
</ul>

<p>Check as well <a href=""https://stackoverflow.com/questions/2471606/how-and-or-why-is-merging-in-git-better-than-in-svn"">How and/or why is merging in Git better than in SVN?</a></p>",9,4,2010-04-10 13:25:11.373000 UTC,26.0,2017-05-23 12:00:20.527000 UTC,48,svn|git|version-control|mercurial|dvcs,3684,2009-05-20 05:46:36.720000 UTC,2022-03-04 09:19:07.773000 UTC,"East Jakarta, East Jakarta City, Jakarta, Indonesia",7078,2055,2,771,"<p>In Git and other DVCS merges are easy not because of some mystical <strong>series of changesets</strong> view (unless you are using Darcs, with its theory of patches, or some Darcs-inspired DVCS; they are minority, though) that Joel rambles about, but because of <em><strong>merge tracking</strong></em>, and the more fundamental fact that <strong>each revisions knows its parents</strong>.  For that you need (I think) whole-tree / full-repository commits... which unfortunately limits ability to do partial checkouts, and making a commit about only subset of files.</p>
<p>When each revision (each commit), including merge commits, know its parents (for merge commits that means having/remembering more than one parent, i.e. <em>merge tracking</em>), you can reconstruct diagram (DAG = Direct Acyclic Graph) of revision history.  If you know graph of revisions, you can find common ancestor of the commits you want to merge.  And when your DVCS knows itself how to <strong>find common ancestor</strong>, you don't need to provide it as an argument, as for example in CVS.</p>
<p>Note that there might be more than one common ancestor of two (or more) commits.  Git makes use of so called &quot;recursive&quot; merge strategy, which merges merge bases (common ancestor), till you are left with one virtual / effective common ancestor (in some simplification), and can the do simple 3-way merge.</p>
<p>Git use of <strong>rename detection</strong> was created to be able to deal with merges involving file renames.  (This supports <a href=""https://stackoverflow.com/questions/2613525/what-makes-merging-in-dvcs-easy/2613595#2613595"">Jörg W Mittag</a> argument that DVCS have better merge support because they had to have it, as merges are much more common than in CVCS with its merge hidden in 'update' command, in update-then-commit workflow, c.f. <a href=""http://www.catb.org/%7Eesr/writings/version-control/version-control.html"" rel=""noreferrer"" title=""Understanding Version-Control Systems (DRAFT)"">Understanding Version Control</a> (WIP) by Eric S. Raymond).</p>",8.0,2010-04-10 20:54:28.540000 UTC,2020-06-20 09:12:55.060000 UTC,23.0,[]
"Is it safe to use the same ignores file for Git, Mercurial, and Bazaar?","<p>Git, Mercurial, and Bazaar all seem to have similar formats for their ignore file (<a href=""http://linux.die.net/man/5/gitignore"" rel=""noreferrer""><code>.gitignore</code></a>, <a href=""http://www.selenic.com/mercurial/hgignore.5.html"" rel=""noreferrer""><code>.hgignore</code></a>, <a href=""http://doc.bazaar.canonical.com/beta/en/user-reference/ignore-help.html"" rel=""noreferrer""><code>.bzrignore</code></a> [see also <a href=""http://doc.bazaar.canonical.com/beta/en/user-reference/patterns-help.html"" rel=""noreferrer"">bzr patterns]</a>).</p>

<p>In order to improve synchronization of the global ignore files, would it be safe to use one as an actual file and simply symlink the other two to that file? In other words, is there any danger in making my <code>$HOME/.gitignore</code> the canonical ignores file and doing</p>

<pre><code>ln -s $HOME/.gitignore $HOME/.hgignore
ln -s $HOME/.gitignore $HOME/.bazaar/ignore
</code></pre>

<p>or is there some subtle difference among them that would bite me at some point?</p>",2,0,2012-03-14 03:23:56.760000 UTC,,2012-03-14 08:39:21.033000 UTC,10,git|mercurial|dvcs|bazaar|ignore,2703,2008-11-17 03:52:25.983000 UTC,2022-03-04 21:21:52.990000 UTC,"Escondido, CA, USA",36115,1423,12,2673,"<p>The syntax used in the ignore files is different from system to system:</p>

<ul>
<li><p><strong>Mercurial:</strong> list of regular expressions — can be changed with a <code>syntax: glob</code> line.</p></li>
<li><p><strong>Bazaar:</strong> list of shell glob patterns — prefixing with <code>RE:</code> to match as regular expression.</p></li>
<li><p><strong>Git:</strong> list of shell glob patterns.</p></li>
</ul>

<p>Furthermore, the exact rules for how the shell patterns and regular expressions are matched differ from tool to tool. All in all, this means that you can only hope to use this trick if your global ignore file is pretty simple. Otherwise the differences can come and bite you, as you say.</p>

<p>I tested it with this file:</p>

<pre><code>syntax: glob
.bzr
.git
.hg
*.o
*~
</code></pre>

<p>and it seems to work as intended across all three tools. To test it, I created this directory tree:</p>

<pre><code>$ tree
.
|-- foo.c
|-- foo.c.~1~
|-- foo.h
|-- foo.o
`-- src
    |-- bar.c
    |-- bar.c.~1~
    `-- bar.o
</code></pre>

<p>and ran the status command for each tool:</p>

<pre><code>$ for tool in hg git bzr; do echo ""== $tool status ==""; $tool status; done
== hg status ==
? foo.c
? foo.h
? src/bar.c
== git status ==
# On branch master
#
# Initial commit
#
# Untracked files:
#   (use ""git add &lt;file&gt;..."" to include in what will be committed)
#
#       foo.c
#       foo.h
#       src/
nothing added to commit but untracked files present (use ""git add"" to track)
== bzr status ==
unknown:
  foo.c
  foo.h
  src/
</code></pre>

<p>As you can see, this simple file works fine.</p>

<p>Technically, Git and Bazaar will now ignore a file called <code>syntax: glob</code>, but unless you plan to create a file with that weird name, this doesn't matter. Finally, note that Mercurial doesn't read a <code>$HOME/.hgignore</code> file by default. But you can make it read it by adding</p>

<pre><code>[ui]
ignore.my-default = ~/.hgignore
</code></pre>

<p>to your <code>$HOME/.hgrc</code> file.</p>",2.0,2012-03-14 08:38:05.960000 UTC,,13.0,[]
How do closed branches affect Mercurial performance?,"<p>I've noticed that <a href=""https://stackoverflow.com/a/9155138/203002"">some</a> <a href=""https://stackoverflow.com/a/8870937/203002"">answers</a> to questions about branch names <a href=""https://www.mercurial-scm.org/wiki/StandardBranching#Don.27t_treat_branch_names_as_disposable"" rel=""nofollow noreferrer"">quote the Mercurial wiki</a> to indicate that the branch-per-feature or branch-per-bug naming conventions may cause performance problems.</p>
<p>Does the ability to mark branches as closed with the <code>--close-branch</code> flag on commits have any effect on this performance claim?</p>",2,0,2012-02-07 00:51:24.147000 UTC,5.0,2021-12-16 09:42:21.677000 UTC,19,mercurial|branch|dvcs|branching-strategy,2745,2009-11-04 22:08:44.657000 UTC,2022-03-04 20:57:52.177000 UTC,,10390,405,21,209,"<blockquote>
  <p>Does the ability to mark branches as closed with the <code>--close-branch</code> flag on commits have any affect on this performance claim?</p>
</blockquote>

<p>Marking a branch closed with <code>hg commit --close-branch</code> merely creates a new changeset with a <code>close=1</code> marker in the changeset meta data. Commands like <code>hg branches</code> and <code>hg heads</code> will then know not to show this branch/head. These commands use a <em>branch cache</em> to speed things up and we expect that cache to scale well with the number of branches.</p>

<p>However, there are some operations that have a complexity that is linear in the number of <em>topological</em> heads. This includes the discovery protocol used before version 1.9. The new discovery protocol in version 1.9 will still exchange topological heads in its ""samples"", but the sample size is capped at 200 changesets.</p>

<p>There might be other code paths that still scale linearly in the number of heads and this is why we suggest close-before-merge:</p>

<pre><code>$ hg update bug-123
$ hg commit --close-branch -m ""All fixed""
$ hg update default
$ hg merge bug-123
</code></pre>

<p>instead merge-before-close:</p>

<pre><code>$ hg update default
$ hg merge bug-123
$ hg update bug-123
$ hg commit --close-branch -m ""All fixed""
</code></pre>

<p>The latter approach leaves a dangling head in the graph (a topological head).</p>",2.0,2012-02-07 09:03:52.860000 UTC,2012-02-08 08:28:44.537000 UTC,29.0,[]
"how to ignore files in kiln/mercurial using tortoise hg ""that are part of the repository""","<p>We use tortoise hg with Kiln. In my vs 2010 c# project there are some files that are part of the repository but I would like tortoise hg to ignore them when I make a commit.
For eg., say in a login screen I may hard code the userid, password for testing.  I dont really want this file considered during a commit. I understand .hgignore file but this really works for files that are not part of the repo. Any trick in tortoise hg to <strong>ignore files that are part of the repo</strong> ? (so they do not show up as modified (M) during a commit.) thanks</p>",4,0,2011-09-08 16:01:41.370000 UTC,,2012-04-13 14:36:44.820000 UTC,8,mercurial|dvcs|tortoisehg|kiln,2454,2011-03-18 17:55:26.140000 UTC,2022-03-05 17:13:44.690000 UTC,,3287,59,0,313,"<p>I always use a combination of .hgignore and <code>BeforeBuild</code> (in the .csproj file) for things like this.</p>

<p>In one of my pet projects, I have the following setup:</p>

<p><code>App.config</code> contains my real hardcoded user id and password for testing.<br>
<code>App.config.example</code> is identical, but with fake data like ""dummy_user"" and ""dummy_pw"".</p>

<p><code>App.config</code> is <strong>not</strong> part of the repository, <strong>and</strong> it's ignored (in <code>.hgignore</code>).<br>
<code>App.config.example</code> is part of the repository.</p>

<p>Then, I have the following in the <code>BeforeBuild</code> target in the .csproj file of my solution:</p>

<pre><code>&lt;Target Name=""BeforeBuild""&gt;
  &lt;Copy
    Condition=""!Exists('App.config')""
    SourceFiles=""App.config.example"" 
    DestinationFiles=""App.config""/&gt;
&lt;/Target&gt;
</code></pre>

<p>All this together has the following effect:</p>

<ul>
<li>the config file with the real data can never be accidentally committed to the repository, because it's ignored</li>
<li>the repository only contains the config file with the example data</li>
<li>if someone else clones the repository to his machine, he won't have the ""real"" config file...but if it's missing, it will be automatically created before the first build by Visual Studio / MSBuild by simply copying the <code>.example</code> file (and then he can just put <strong>his</strong> real login data into the newly created <code>App.config</code> file).</li>
<li>if an <code>App.config</code> with real hardcoded user data already exists, it won't be overwritten when building because the BeforeBuild event will only happen if <code>App.config</code> does <strong>not</strong> already exist</li>
</ul>",0.0,2011-09-08 16:21:18.563000 UTC,2011-09-08 16:39:56.363000 UTC,17.0,[]
Mercurial: Can I rename a branch?,"<p>We now have a ""stiging"" branch, where ""staging"" seems to be a far better semantic fit. What's a good strategy for handling this?</p>",5,0,2010-12-07 15:59:59.570000 UTC,52.0,,209,mercurial|branch|dvcs,64479,2008-09-16 17:19:33.190000 UTC,2021-11-19 03:09:22.120000 UTC,"Buffalo, NY",5665,202,18,681,"<p>Update to the <code>stiging</code> branch and create a new branch off of it. Then close the old branch.</p>

<p>In summary:</p>

<pre><code>hg update stiging
hg branch staging
hg commit -m""Changing stiging branch to staging.""
hg update stiging
hg commit --close-branch -m""This was a typo; use staging instead.""
hg push --new-branch
</code></pre>",7.0,2011-08-30 14:54:07.360000 UTC,2011-11-30 17:57:36.267000 UTC,226.0,[]
Open Source Clearcase Alternatives,"<p>I'm helping get the SCM set up for a new program, and we're currently in the process of deciding on a VCS.</p>

<p>The main contenders at this point are SVN, ClearCase, SVN+DVCS, and just a DVCS.</p>

<p>At the moment, the team is leaning towards either SVN or SVN+DVCS.  We want to avoid the expense and administration costs of ClearCase, but want the workflow and versioning options it offers.  I brought forward the option of using a DVCS as well, and the idea is being considered.</p>

<p>For the DVCS, we're considering Mercurial, Bazaar, and Git.  The team feels comfortable with SVN, but don't think it'll offer the versatility needed, which is why we're looking at the DVCS on top of SVN option.</p>

<p>Does anyone have any advice (e.g. existing tools, processes) for getting such a setup going?</p>

<p>Concerns include:</p>

<ul>
<li>Ease of setup</li>
<li>Setting up workflows (developing &lt;-> code reviews -> test -> trunk, then trunk -> integration test branch -> release with bug fixes put into the release branches), auditing (found a bug, when was it introduced)</li>
<li>Generating metrics</li>
<li>Reasonable learning curve for developers used to ClearCase.</li>
<li>Windows development</li>
<li>Issue tracker integration (probably going to be Redmine, though it isn't set in stone)</li>
</ul>",4,0,2009-06-19 17:40:03.703000 UTC,2.0,2009-06-22 13:00:36.527000 UTC,4,windows|svn|workflow|clearcase|dvcs,5036,2008-09-17 10:52:37.823000 UTC,2022-03-04 13:36:48.843000 UTC,United States,3924,1239,3,231,"<p>As an administrator of ClearCase, I would rule out that tool unless you have a complex merge workflow.</p>

<p>The <strong><a href=""https://stackoverflow.com/questions/216212#216228"">workflow</a></strong> you mention involves being able to branch, fix and then merge back easily, which should not be in favor of SVN.</p>

<p>The biggest trap when you are used to ClearCase (espacially non-UCM ClearCase) is the ""composition"" of a config spec.<br>
If you choose a DVCS (Mercurial, Bazaar -- or Git since it <a href=""https://stackoverflow.com/questions/783906/git-under-windows-msys-or-cygwin"">works really well on Windows</a>), you would <a href=""https://stackoverflow.com/questions/763099/flexible-vs-static-branching-git-vs-clearcase-accurev""><em>not</em> be able to get the same ""inheritance effect"" -- (when you are selecting different versions with different successive select rules --</a>: as said in that answer:</p>

<blockquote>
  <p>In a purely central VCS, you can define your workspace (in ClearCase, your ""view"" either snapshot or dynamic) with whatever rules you want.<br>
  That would not be practical in a DVCS (as in ""Distributed""): when you are making a branch with it, you need to do so with a starting point and a content clearly defined and easily replicated to other repositories.</p>
</blockquote>

<p>If you are using ClearCase UCM, that means identifying coherent sets of files, which can only be achieved through - for instance - <a href=""https://stackoverflow.com/questions/932586/middle-ground-between-submodules-and-branches"">Git submodules</a>.<br>
The complexity is higher with those DVCS, since they do not record the dependencies between modules (or ""set of files"") the way ClearCase UCM does.</p>

<p>So, to recap:</p>

<ul>
<li>Ease of setup: all the mentioned DVCS are easy to setup. What must be taken into account is the administration in terms of user access.  </li>
<li>Setting up workflows: a DVCS supports any kind of workflow, even a centralized one, or a public-private one, or.... Regarding finding a bug, they all support some kind of <a href=""https://stackoverflow.com/questions/959324/finding-bugs-by-bisecting-searching-revision-history-and-untestable-commits-r"">bisect process</a>.</li>
<li>Generating metrics: if by that you mean ""metrics about the code managed"", they all support some complete log system able to display a lot of information about what has changed.<br>
But in term of ""metrics about the tool"" (speed of a process or space taken by data), the DVCS tools mentioned are considered much faster than SVN (see <a href=""http://cafenate.wordpress.com/2008/05/21/more-subversion-vs-mercurial-metrics/"" rel=""nofollow noreferrer"">here as an example</a>).</li>
<li>Reasonable learning curve for developers used to ClearCase: the GUI can be a factor to alleviate the learning curve, but a DVCS is very different from ClearCase, as this ""<a href=""https://stackoverflow.com/questions/645008#645771"">core concepts</a>"" answer illustrates.</li>
<li>Windows development: they all work well on the Windows platform, with maybe a slight advantage (better integration) to Mercurial or Bazaar.</li>
<li>Issue tracker integration (probably going to be Redmine, though it isn't set in stone): Redmine supports now most of them (and not just SVN as it did at the beginning)</li>
</ul>",0.0,2009-06-19 17:51:20.880000 UTC,2017-05-23 10:27:36.633000 UTC,9.0,[]
Does merge direction matter in Mercurial?,"<p>Take a simple example: I'm working on the default branch, have some changesets committed locally, and I pulled a few more from the master repository. I've been working for a few days in my isolated local repository, so there's quite a few changes to merge before I can push my results back into master.</p>

<pre><code>default ---o-o-o-o-o-o-o-o-o-o-o  (pulled stuff)
            \
             o----o------------o  (my stuff)
</code></pre>

<p>I can do two things now.</p>

<p>Option #1:</p>

<pre><code>hg pull
hg merge
</code></pre>

<p>Result #1:</p>

<pre><code>default ---o-o-o-o-o-o-o-o-o-o-o
            \                   \
             o----o------------o-O
</code></pre>

<p>Option #2:</p>

<pre><code>hg pull
hg update
hg merge
</code></pre>

<p>Result #2:</p>

<pre><code>default ---o-o-o-o-o-o-o-o-o-o-o-O
            \                   /
             o----o------------o
</code></pre>

<p>These two results look isomorphic to me, but in practice it seems that option #2 results in way smaller changesets (because it only applies my few changes to the mainline instead of applying all the mainline changes to my few).</p>

<p>My question is: does this matter? Should I care about the direction of my merges? Am I saving space if I do this? (Doing <code>hg log --patch --rev tip</code> after the merge suggests so.)</p>",2,0,2011-02-09 15:30:19.787000 UTC,1.0,,16,mercurial|merge|dvcs,1105,2009-05-11 16:11:31.467000 UTC,2022-03-04 21:49:32.847000 UTC,"Budapest, Hungary",9454,319,3,289,"<p>They're (effectively) identical.  You see a difference in the <code>hg log --patch --rev X</code> output size because log shows the diff of the result and (arbitrarily) its 'left' parent (officially p1), but that's not how it's stored (Mercurial has a binary diff storage format that isn't patch/diff based) and it's now how it's computed (p1, p2, and most-recent-common-ancestor are all used).</p>

<p>The only real difference is, if you're using named branches, the branch name will be that of the left parent.</p>",0.0,2011-02-09 15:48:45.823000 UTC,,14.0,[]
Mercurial: a few questions all related to .hgignore,"<p>I've been working for a long time with a <code>.hgignore</code> file that was fine and recently added one new type of files to ignore.  When running ""hg status"", I noticed this:</p>

<pre><code>M .hgignore
</code></pre>

<p>So Mercurial considers the <code>.hgignore</code> to be a file that needs to be tracked (if it's a the root of the project).</p>

<p>Now I've read various docs but my points weren't specifically adressed so here are some very detailed questions which hopefully can help me figure this out (it would be great is someone answering could quote and address these three points [even with a simply yes/no answer for each question]):</p>

<ul>
<li><p>Should <code>.hgignore</code> be at the root of the project? (I guess it should, seen that a developer can potentially be working on several projects which would all have different .hgignore requirements)</p></li>
<li><p>Can <code>.hgignore</code> be ignored be Mercurial?</p></li>
<li><p>If it can be ignored, <em>should</em> <code>.hgignore</code> be ignored by Mercurial (which is different than the previous question)</p></li>
</ul>

<p>In the case where <code>.hgignore</code> should <em>not</em> be ignored, can't some really bad thing happens if you suddenly rollback way earlier, when a really old and incomplete <code>.hgignore</code> was used?</p>

<p>I think I saw weird things happening with certain per-user IDE project files (I'm not saying all IDEs project files are per-user only, but some definitely are) that were supposed to be ignored, but then the user rolls back to an old version, where an old <code>.hgignore</code> gets used, and then suddenly files supposed to be ignored are committed because the old <code>.hgignore</code> didn't exclude these.</p>",1,0,2010-03-24 17:39:25.737000 UTC,0.0,2010-05-20 05:21:59.790000 UTC,6,mercurial|dvcs|hgignore,1341,2010-01-23 11:01:56.643000 UTC,2011-09-29 20:43:04.880000 UTC,,26837,2188,656,3638,"<p>You don't generally ignore the .hgignore file because you want it to be part of the repository so all of your users end up with it.</p>

<p>As far as rolling far back and then accidentally committing files you shouldn't - well, that really could happen at any point anyway.  Commit responsibly.</p>

<p>To your other questions:</p>

<ul>
<li>Yes, .hgignore should be at the root of the repository.</li>
<li>Yes, .hgignore can ignore itself.</li>
<li>No, .hgignore should not be ignored (unless you decide you don't want it in the repository).</li>
</ul>",1.0,2010-03-24 17:51:42.913000 UTC,,13.0,[]
Centralized vs. Distributed version control security,"<p>As my company begins to further explore moving from centralized version control tools (CVS, SVN, Perforce and a host of others) to offering teams distributed version control tools (mercurial in our case) I've run into a problem:</p>

<p><strong>The Problem</strong></p>

<p>A manager has raised the concern that distributed version control may not be as secure as our CVCS options because the repo history is stored locally on the developer's machine.</p>

<p>It's been difficult to nail down his exact security concern but I've gathered that it centers on the fact that a malicious employee could steal not only the latest intellectual properly but our whole history of changes just by copying a single folder.</p>

<p><strong>The Question(s)</strong></p>

<ul>
<li>Do distributed version control system really introduce new security concerns for projects?</li>
<li>Is it easier to maliciously steal code?</li>
<li>Does the complete history represent an additional threat that the latest version of the code does not?</li>
</ul>

<p><strong>My Thoughts</strong></p>

<p>My take is that this may be a mistaken thought that the centralized model is more secure because the history <em>seems</em> to be safer as it is off on its own box.  Given that users with even read access to a centralized repo could selectively extract snapshots of the project at any key revision I'm not sure the DVCS model makes it all that easier.  Also, most CVCS tools allow you to extract the whole repo's history with a single command so that you can import them into other tools.</p>

<p>I think the other issue is just how important the history is compared to the latest version.  Granted someone could have checked in a top secret file, then deleted it and the history would pretty quickly be significant.  But even in that scenario a CVCS user could checkout that top secret version with a single command.</p>

<p>I'm sure I could be missing something or downplaying risks as I'm eager to see DVCS become a fully supported tool option.  Please contribute any ideas you have on security concerns.</p>",4,1,2011-10-03 21:01:07.287000 UTC,2.0,2011-10-04 02:51:59.220000 UTC,11,security|version-control|dvcs,1212,2009-02-13 01:00:57.107000 UTC,2022-03-04 17:47:39.967000 UTC,"Kansas City, MO",26803,2172,10,2307,"<p>If you have read access to a CVCS, you have enough permissions to convert the repo to a DVCS, which people do all the time.  No software tool is going to protect you from a disgruntled employee stealing your code, but a DVCS has many more options for dealing with untrusted contributors, such as a gatekeeper workflow.  Hence its widespread use in open source projects.</p>",2.0,2011-10-03 21:09:36.977000 UTC,,12.0,[]
Pushing to Git returning Error Code 403 fatal: HTTP request failed,"<p>I was able to clone a copy of this repo over HTTPS authenticated. I've made some commits and want to push back out to the GitHub server. Using Cygwin on Windows 7 x64.</p>

<pre><code>C:\cygwin\home\XPherior\Code\lunch_call&gt;git push
Password:
error: The requested URL returned error: 403 while accessing https://MichaelDrog
alis@github.com/derekerdmann/lunch_call.git/info/refs

fatal: HTTP request failed
</code></pre>

<p>Also set it up with verbose mode. I'm still pretty baffled.</p>

<pre><code>C:\cygwin\home\XPherior\Code\lunch_call&gt;set GIT_CURL_VERBOSE=1

C:\cygwin\home\XPherior\Code\lunch_call&gt;git push
Password:
* Couldn't find host github.com in the _netrc file; using defaults
* About to connect() to github.com port 443 (#0)
*   Trying 207.97.227.239... * 0x23cb740 is at send pipe head!
* Connected to github.com (207.97.227.239) port 443 (#0)
* successfully set certificate verify locations:
*   CAfile: C:\Program Files (x86)\Git/bin/curl-ca-bundle.crt
  CApath: none
* SSL connection using AES256-SHA
* Server certificate:
*        subject: 2.5.4.15=Private Organization; 1.3.6.1.4.1.311.60.2.1.3=US; 1.
3.6.1.4.1.311.60.2.1.2=California; serialNumber=C3268102; C=US; ST=California; L
=San Francisco; O=GitHub, Inc.; CN=github.com
*        start date: 2011-05-27 00:00:00 GMT
*        expire date: 2013-07-29 12:00:00 GMT
*        subjectAltName: github.com matched
*        issuer: C=US; O=DigiCert Inc; OU=www.digicert.com; CN=DigiCert High Ass
urance EV CA-1
*        SSL certificate verify ok.
&gt; GET /derekerdmann/lunch_call.git/info/refs?service=git-receive-pack HTTP/1.1
User-Agent: git/1.7.4.3282.g844cb
Host: github.com
Accept: */*
Pragma: no-cache

&lt; HTTP/1.1 401 Authorization Required
&lt; Server: nginx/1.0.4
&lt; Date: Thu, 15 Sep 2011 22:44:41 GMT
&lt; Content-Type: text/plain
&lt; Connection: keep-alive
&lt; Content-Length: 55
&lt; WWW-Authenticate: Basic realm=""GitHub""
&lt;
* Ignoring the response-body
* Expire cleared
* Connection #0 to host github.com left intact
* Issue another request to this URL: 'https://MichaelDrogalis@github.com/dereker
dmann/lunch_call.git/info/refs?service=git-receive-pack'
* Couldn't find host github.com in the _netrc file; using defaults
* Re-using existing connection! (#0) with host github.com
* Connected to github.com (207.97.227.239) port 443 (#0)
* 0x23cb740 is at send pipe head!
* Server auth using Basic with user 'MichaelDrogalis'
&gt; GET /derekerdmann/lunch_call.git/info/refs?service=git-receive-pack HTTP/1.1
Authorization: Basic XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
User-Agent: git/1.7.4.3282.g844cb
Host: github.com
Accept: */*
Pragma: no-cache

&lt; HTTP/1.1 401 Authorization Required
&lt; Server: nginx/1.0.4
&lt; Date: Thu, 15 Sep 2011 22:44:41 GMT
&lt; Content-Type: text/plain
&lt; Connection: keep-alive
&lt; Content-Length: 55
* Authentication problem. Ignoring this.
&lt; WWW-Authenticate: Basic realm=""GitHub""
* The requested URL returned error: 401
* Closing connection #0
* Couldn't find host github.com in the _netrc file; using defaults
* About to connect() to github.com port 443 (#0)
*   Trying 207.97.227.239... * 0x23cb740 is at send pipe head!
* Connected to github.com (207.97.227.239) port 443 (#0)
* successfully set certificate verify locations:
*   CAfile: C:\Program Files (x86)\Git/bin/curl-ca-bundle.crt
  CApath: none
* SSL re-using session ID
* SSL connection using AES256-SHA
* old SSL session ID is stale, removing
* Server certificate:
*        subject: 2.5.4.15=Private Organization; 1.3.6.1.4.1.311.60.2.1.3=US; 1.
3.6.1.4.1.311.60.2.1.2=California; serialNumber=C3268102; C=US; ST=California; L
=San Francisco; O=GitHub, Inc.; CN=github.com
*        start date: 2011-05-27 00:00:00 GMT
*        expire date: 2013-07-29 12:00:00 GMT
*        subjectAltName: github.com matched
*        issuer: C=US; O=DigiCert Inc; OU=www.digicert.com; CN=DigiCert High Ass
urance EV CA-1
*        SSL certificate verify ok.
* Server auth using Basic with user 'MichaelDrogalis'
&gt; GET /derekerdmann/lunch_call.git/info/refs HTTP/1.1
Authorization: Basic xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
User-Agent: git/1.7.4.3282.g844cb
Host: github.com
Accept: */*
Pragma: no-cache

* The requested URL returned error: 403
* Expire cleared
* Closing connection #0
error: The requested URL returned error: 403 while accessing https://MichaelDrog
alis@github.com/derekerdmann/lunch_call.git/info/refs

fatal: HTTP request failed
</code></pre>

<p>These are the versions of git and curl that I have:</p>

<pre><code>C:\Users\XPherior&gt;git --version
git version 1.7.4.msysgit.0

C:\Users\XPherior&gt;curl --version
curl 7.21.7 (amd64-pc-win32) libcurl/7.21.7 OpenSSL/0.9.8r zlib/1.2.5
Protocols: dict file ftp ftps gopher http https imap imaps ldap pop3 pop3s rtsp
smtp smtps telnet tftp
Features: AsynchDNS GSS-Negotiate Largefile NTLM SSL SSPI libz
</code></pre>",69,12,2011-09-15 22:45:59.773000 UTC,268.0,2014-04-01 23:31:16.800000 UTC,745,git|github|dvcs,1067739,2010-07-07 19:28:22.507000 UTC,2017-04-28 17:14:16.217000 UTC,"Rochester, NY",18299,3428,25,892,"<p>I just got the same problem and just figured out what's cause.</p>
<p>Github seems only supports ssh way to read&amp;write the repo, although https way also displayed 'Read&amp;Write'.</p>
<p>So you need to change your repo config on your PC to ssh way:</p>
<ol>
<li>edit <code>.git/config</code> file under your repo directory</li>
<li>find <code>url=</code>entry under section <code>[remote &quot;origin&quot;]</code></li>
<li>change it from <code>url=https://MichaelDrogalis@github.com/derekerdmann/lunch_call.git</code> to    <code>url=git@github.com/derekerdmann/lunch_call.git</code>. that is, change all the texts before <code>@</code> symbol to <code>ssh://git</code></li>
<li>Save <code>config</code> file and quit. now you could use <code>git push origin master</code> to sync your repo on GitHub</li>
</ol>",18.0,2011-10-14 18:26:33.860000 UTC,2020-09-18 08:17:20.177000 UTC,925.0,[]
Checking the version of Databricks Runtime in Azure,<p>Is it possible to check the version of Databricks Runtime in Azure?</p>,3,0,2018-12-12 10:28:13.227000 UTC,,,9,azure|version|azure-databricks,8499,2013-07-10 14:46:04.427000 UTC,2022-03-05 21:30:21.057000 UTC,"Copenhagen, Denmark",4659,527,3,294,"<p>Databricks Runtime is the set of core components that run on the clusters managed by Azure Databricks. It includes Apache Spark but also adds a number of components and updates that substantially improve the usability, performance, and security of big data analytics.</p>

<blockquote>
  <p>You can choose from among many supported runtime versions when you
  create a cluster.</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/kEYHo.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/kEYHo.jpg"" alt=""enter image description here""></a></p>

<blockquote>
  <p>If you want to know the version of Databricks runtime in Azure after
  creation:</p>
</blockquote>

<p>Go to <strong>Azure Data bricks portal</strong> => <strong>Clusters</strong> => <strong>Interactive Clusters</strong> => here you can find the run time version.</p>

<p><a href=""https://i.stack.imgur.com/3Cxva.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3Cxva.jpg"" alt=""enter image description here""></a></p>

<p>For more details, refer ""<a href=""https://docs.azuredatabricks.net/release-notes/runtime/databricks-runtime-ver.html#runtime-components"" rel=""noreferrer"">Azure Databricks Runtime versions</a>"".</p>

<p>Hope this helps.</p>",0.0,2018-12-12 10:56:22.940000 UTC,,9.0,[]
How to automatically remove all .orig files in Mercurial working tree?,"<p>During merges mercurial leaves .orig file for any unresolved file. But after manually resolving problems and marking a file correct it does not delete the .orig file. Can it be automatically removed by some command?</p>

<p>I work on a Mac so I can use something like:</p>

<pre><code>find . -iname '*.orig' -exec rm '{}' ';'
</code></pre>

<p>and alias it or something, but I'd rather use something like hg cleanup...</p>

<p>UPDATE:</p>

<p>Since some time now, <a href=""https://www.mercurial-scm.org/wiki/PurgeExtension"" rel=""nofollow noreferrer"">Purge extension</a> is bundled with Mercurial and solves this problem nicely.</p>",12,1,2009-07-01 07:10:20.623000 UTC,11.0,2018-02-22 10:08:37.413000 UTC,57,mercurial|dvcs|purge,27726,2008-11-12 20:23:29.237000 UTC,2022-02-25 00:52:45.613000 UTC,"Sunnyvale, CA",8029,932,43,464,"<p>Personally, I use</p>

<pre><code>$ rm **/*.orig
</code></pre>

<p>if I get tired of the <code>.orig</code> files. This works in Zsh and in Bash 4 after you run <code>shopt -s globstar</code>.</p>

<p>But if you use another shell or want a built-in solution, then maybe you'll like the <a href=""https://www.mercurial-scm.org/wiki/PurgeExtension"" rel=""noreferrer"">purge extension</a> (link updated 2016-08-25). That lets you remove all untracked files with</p>

<pre><code>$ hg purge
</code></pre>

<p>You can remove all untracked and ignored files with</p>

<pre><code>$ hg purge --all
</code></pre>

<p>An advantage of using <code>hg purge</code> is that this will also cleanup directories that become empty after removing the files. The <code>rm</code> command line will just leave the empty directories behind.</p>",4.0,2009-07-01 08:19:34.197000 UTC,2016-08-25 19:07:05.723000 UTC,70.0,[]
What directory structure makes sense for DVCS such as git?,"<h2>What I am used to:</h2>

<ul><li>Archives on the servers (NY, IN, NC)</li>
<li>On my development machine:
<ul><li>A directory named ~/work</li>
<li>Subdirectories named ~/work/NY/devproject, ~/work/NC/project etc</li>
<li>Not infrequently, subdirectories named ~/work/NY/release/1.3/project, ~/work/NY/test/1.3b/project etc</li><li>Sometimes directories named ~/proxy/NY, ~/proxy/NC etc which contain a disposable local cache of the archives in order to reduce network traffic for reads.  These directories can be deleted at any time.</ul><li>A scratch build that deletes ~/work/... and repopulates it from the archives</ul>

<hr>

<h2>But with DVCS that doesn't make sense</h2>

<ul><li>The archives are on my development machine, but a near-clone is on a remote machine for backup reasons.</li><li>Doing a scratch build would mean deleting and re-pulling the whole archive, which seems costly.</li><li>It looks like I have directories named ~/git/git.git/git which is a lot of gits.</li></ul>

<p>Do people do all their development in ~/git?  If you need to work with dev, test, release, and one-off-for-big-client versions, are these under ~/git, or can they be elsewhere in their own trees?  Where do third-party components go?  Is this too big for SO (do I need to read a book), or can it be answered with an ASCII tree diagram?</p>",2,0,2009-03-31 13:59:12.773000 UTC,7.0,,11,git|dvcs,3512,2008-10-19 15:33:00.297000 UTC,2019-02-09 18:17:19.667000 UTC,,13218,1020,52,1278,"<p>Because of the way Git works, you really don't want to place working directories for repositories (or branches) in a directory <em>under</em> the working directory for another repository. It would keep wanting to put the contents of your child directory into the parent's repository.</p>

<p>If you go with all branches being sibling directories, that would work just fine.</p>

<p>What I tend to do (regardless of using Git, cvs, or (ick) SourceSafe) is have a Development directory, and each project, branch, etc be a subdirectory in there.</p>",0.0,2009-03-31 14:47:09.193000 UTC,,8.0,[]
Found a swap file by the name,"<p>When I try to merge my branch with a remote branch:</p>

<pre><code>git merge feature/remote_branch
</code></pre>

<p>I got this message:</p>

<pre><code>E325: ATTENTION
Found a swap file by the name "".git/.MERGE_MSG.swp""
          owned by: xxxxxx   dated: Mon Nov 12 23:17:40 2012
         file name: ~xxxxxx/Desktop/My-ios-App/.git/MERGE_MSG
          modified: YES
         user name: xxxxxx   host name: unknown-b8-8d-12-22-27-72.lan
        process ID: 1639
While opening file "".git/MERGE_MSG""
             dated: Tue Nov 13 14:06:48 2012
      NEWER than swap file!

(1) Another program may be editing the same file.
    If this is the case, be careful not to end up with two
    different instances of the same file when making changes.
    Quit, or continue with caution.

(2) An edit session for this file crashed.
    If this is the case, use "":recover"" or ""vim -r .git/MERGE_MSG""
    to recover the changes (see "":help recovery"").
    If you did this already, delete the swap file "".git/.MERGE_MSG.swp""
    to avoid this message.

Swap file "".git/.MERGE_MSG.swp"" already exists!
[O]pen Read-Only, (E)dit anyway, (R)ecover, (D)elete it, (Q)uit, (A)bort:
</code></pre>

<p>How to deal with this?</p>",6,0,2012-11-13 13:39:41.827000 UTC,15.0,2021-11-12 19:38:43.930000 UTC,85,git|github|git-merge|dvcs|swapfile,146154,2011-02-03 21:21:02.247000 UTC,2022-03-06 00:15:02.417000 UTC,,13865,878,58,1673,"<p>Looks like you have an open <code>git commit</code> or <code>git merge</code> going on, and an editor is still open editing the commit message.</p>

<p>Two choices:</p>

<ol>
<li>Find the session and finish it (preferable).</li>
<li>Delete the <code>.swp</code> file (if you're sure the other git session has gone away).</li>
</ol>

<p>Clarification from comments:</p>

<ul>
<li>The <em>session</em> is the editing session.</li>
<li>You can see what <code>.swp</code> is being used by entering the command <code>:sw</code> within the editing session, but generally it's a hidden file in the same directory as the file you are using, with a <code>.swp</code> file suffix (i.e. <code>~/myfile.txt</code> would be <code>~/.myfile.txt.swp</code>).</li>
</ul>",9.0,2012-11-13 13:48:23.563000 UTC,2019-02-05 09:16:48.290000 UTC,90.0,[]
Can I add changes to staging area from another branch?,"<p>I have two branch,</p>

<ul>
<li>master</li>
<li>new-features</li>
</ul>

<p>New features have a commit I want to add to master but I want the control how much I want to merge.</p>

<p>I would like the option to add/reject changes for each line. Similar I can do with <code>git add -p</code> </p>

<p>I have searched a lot probably I was searching wrong term. This seems quite obvious task. </p>",2,6,2016-07-04 06:27:53.733000 UTC,0.0,2016-12-03 19:00:12.227000 UTC,11,git|shell|version-control|dvcs,2401,2010-12-14 09:11:50.447000 UTC,2022-03-04 18:58:01.663000 UTC,Bangladesh,2478,1213,33,757,"<p>Use <code>git cherry-pick</code> with the <code>-n|--no-commit</code> option and then interactively select what to commit:</p>

<blockquote>
  <p><strong>git cherry-pick</strong></p>
  
  <p>...</p>
  
  <p><code>-n</code>, <code>--no-commit</code></p>
  
  <p>Usually <code>git cherry-pick</code> automatically creates a sequence of commits. This
  flag applies the changes necessary to cherry-pick each named commit to your
  working tree and the index, <strong>without making any commit</strong>. In addition, when
  this option is used, your index does not have to match the HEAD commit. The
  cherry-pick is done against the beginning state of your index.</p>
  
  <p>This is useful when cherry-picking more than one commits' effect to your index
  in a row.</p>
</blockquote>

<p>So the sequence of commands will be the following:</p>

<pre><code>git cherry-pick -n &lt;commitid&gt;  # merge commitid into the index and working-tree
git reset                      # clear the index
git add -p                     # selectively add merged changes to the index
</code></pre>

<p>Alternatively, you can use <code>git reset -p</code> to <strong>remove</strong> undesired hunks from the staging area:</p>

<pre><code>git cherry-pick -n &lt;commitid&gt;  # merge commitid into the index and working-tree
git reset -p   # interactively remove from the index changes brought by commitid
</code></pre>",1.0,2016-07-04 07:09:44.240000 UTC,2016-07-04 07:17:14.040000 UTC,8.0,[]
How do you organize your git repositories?,"<p>""Traditional"" version control systems follow a ""Cathedral"" model -- all the code is stored in one main repository. </p>

<p>Distributed Version Control Systems like git allow a lot more flexibility in organizing your multiple repositories. You can ""push"" changes, ""pull"" changes, and ""clone"" repositories.</p>

<p>Have you organized your repositories along your workgroup or workflow lines? Have you noticed any patterns?</p>",2,1,2009-01-29 00:08:12.410000 UTC,17.0,2009-01-30 00:12:45.453000 UTC,16,git|version-control|dvcs|repository,17395,2009-01-11 10:34:49.370000 UTC,2011-07-14 23:17:30.230000 UTC,"Chicago, IL",303,31,8,125,"<p>Scott Chacon, whose git-fu is very strong, has some great slides on this in <a href=""http://en.oreilly.com/rails2008/public/asset/attachment/2816"" rel=""noreferrer"">Getting Git</a>. Check pages 474-501 for many excellent diagrams explaining three types of workflow: </p>

<ul>
<li>central repository model (svn style), </li>
<li>dictator and lieutenants model (linux kernel style), </li>
<li>integration manager model (github style).</li>
</ul>

<p>The full context for the referenced slides can be found here <a href=""http://git-scm.com/book/en/Distributed-Git-Distributed-Workflows"" rel=""noreferrer"">Pro Git - 5.1 Distributed Git - Distributed Workflows</a>.</p>",0.0,2009-01-29 00:27:17.850000 UTC,2012-11-16 19:26:54.410000 UTC,30.0,[]
How should I version control (sort of) unrelated scripts in the same path?,"<p>I've started using version control to better manage revisions to my PowerShell code. I decided to use Mercurial for 3 main reasons:</p>

<ol>
<li>As a DVCS, it doesn't require a server.</li>
<li>If I want to, I can store a private repository online for free (bitbucket.org)</li>
<li>It seemed simpler to use than Git.</li>
</ol>

<p>Mercurial works well for versioning PowerShell Modules since each module is contained in its own directory. However, I have some scripts that don't belong in a module, but I would still like to version them. These scripts are in a "".\Scripts"" directory that is added to <code>$env:PATH</code> so I can easily run them from the command line. Since these scripts aren't really related to each other, it doesn't make much sense to create a single repository for the Scripts directory.</p>

<p><strong>How should I version single scripts?</strong></p>

<p>I've thought of the following options:</p>

<ul>
<li>Create subdirectory for each script/related scripts.</li>
<li>Use temporary repositories until the script is ""stable"" then add the script to the main ""Scripts"" directory and version the collection of scripts as one. This would reduce the number of changesets introduced to the ""Scripts"" repository.</li>
</ul>

<p>Is there a tool that better handles versioning single files? Is there better way for versioning single files with Mercurial? Any other ideas?</p>",3,2,2011-08-15 20:34:20.180000 UTC,1.0,2012-01-07 21:49:11.410000 UTC,7,git|version-control|powershell|mercurial|dvcs,3525,2010-03-11 17:22:13.470000 UTC,2022-02-18 22:57:22.150000 UTC,,21732,578,43,527,"<p>Grouping of files based on their functionality should be based on </p>

<p>1) Name.</p>

<p>2) Folder they are in.</p>

<p>Just give a proper name for the scripts. If there are multiple related scripts group them into a folder. Having one script per folder makes no sense. You end up with almost same number of folders as scripts. </p>

<p>All this in a single repository. Generally, people have multiple projects in a single repo. Creating multiple repos, especially for a few files means lots of overhead. If the script is not ""stable"" use branches. That is what they are for and merge them back. </p>

<p>And don't worry how many ""changesets"" are there in the repo!</p>

<p>PS: Might seem a bit opinionated, but there is no real right or wrong answer for what you ask.</p>",6.0,2011-08-15 21:00:08.320000 UTC,2011-08-15 21:07:56.240000 UTC,11.0,[]
How to automatically merge .hgtags in Mercurial?,"<p>I have a script running some Mercurial commands in non-interactive mode on a build server. One of the commands merges two branches and there is always a conflict in the <code>.hgtags</code> file during the merge because of the way the build scripts are set up.</p>

<p>How can I force Mercurial to always merge the <code>.hgtags</code> file using changes from both files, first from one, then from the other?</p>

<p>For example, if I the files to merge were</p>

<pre><code>A
B
C
</code></pre>

<p>and</p>

<pre><code>A
B
D
</code></pre>

<p>I would like the result to be</p>

<pre><code>A
B
C
D
</code></pre>

<p>I am guessing I will need a custom merge tool. What tool provides this functionality?</p>",5,0,2012-03-20 06:41:57.100000 UTC,8.0,2012-03-20 12:06:37.057000 UTC,22,mercurial|merge|dvcs|merge-conflict-resolution|.hgtags,4857,2008-12-13 20:57:30.807000 UTC,2021-12-01 17:47:21.183000 UTC,"London, United Kingdom",1674,311,0,162,"<p><em>Please see the <a href=""https://stackoverflow.com/a/26012910/110204"">answer below</a> by Magras de La Mancha for better solution with Mercurial 3.1.</em> The below is a simpler and more naive solution for older versions of Mercurial.</p>

<hr>

<p>Yes, you need to configure a custom <a href=""https://www.mercurial-scm.org/wiki/MergeToolConfiguration"" rel=""nofollow noreferrer"">merge tool</a> for your <code>.hgtags</code> file. Mercurial doesn't  provide any special merge tool for <code>.hgtags</code>, you're expected to merge it by hand using your normal three-way merge tool.</p>

<p>Conflicts in the <code>.hgtags</code> file can have two types:</p>

<ul>
<li><p><strong>Silly conflicts:</strong> This is the situation you have and there is not really a conflict here. What happens is that one branch has</p>

<pre><code>f40273b0ad7b3a6d3012fd37736d0611f41ecf54 A
0a28dfe59f8fab54a5118c5be4f40da34a53cdb7 B
12e0fdbc57a0be78f0e817fd1d170a3615cd35da C
</code></pre>

<p>and the other branch has</p>

<pre><code>f40273b0ad7b3a6d3012fd37736d0611f41ecf54 A
0a28dfe59f8fab54a5118c5be4f40da34a53cdb7 B
979c049974485125e1f9357f6bbe9c1b548a64c3 D
</code></pre>

<p>Each tag refers to exactly one changeset, so there's no conflict here. The merge should of course be the <em>union</em> on of the two files:</p>

<pre><code>f40273b0ad7b3a6d3012fd37736d0611f41ecf54 A
0a28dfe59f8fab54a5118c5be4f40da34a53cdb7 B
12e0fdbc57a0be78f0e817fd1d170a3615cd35da C
979c049974485125e1f9357f6bbe9c1b548a64c3 D
</code></pre></li>
<li><p><strong>Real conflicts:</strong> There one branch has</p>

<pre><code>f40273b0ad7b3a6d3012fd37736d0611f41ecf54 A
0a28dfe59f8fab54a5118c5be4f40da34a53cdb7 B
12e0fdbc57a0be78f0e817fd1d170a3615cd35da C
</code></pre>

<p>and the other branch has</p>

<pre><code>f40273b0ad7b3a6d3012fd37736d0611f41ecf54 A
0a28dfe59f8fab54a5118c5be4f40da34a53cdb7 B
979c049974485125e1f9357f6bbe9c1b548a64c3 C
</code></pre>

<p>There is a real conflict here: <code>hg tag C</code> was done on both branches, but the tags refer to different changesets. Resolving this is a manual task.</p></li>
</ul>

<p>If you can guarantee that you'll only have <em>silly conflicts</em> and that you only have one tag per changeset, then you can use</p>

<pre><code>hg log -r ""tagged()"" --template ""{node} {tags}\n"" &gt; .hgtags
</code></pre>

<p>to generate a new <code>.hgtags</code> file. The key insight is that Mercurial <strong>knows how to merge tags</strong> internally! It does this all the time when you have two heads with different <code>.hgtags</code> files. The above template simply generates a new <code>.hgtags</code> file based on this internal merge.</p>

<p>If you might have more than one tag per changeset, then the above wont work — all the tags are printed on one line so you get a tag <code>foo bar</code> instead of two tags <code>foo</code> and <code>bar</code>. You can then use this <a href=""http://hgbook.red-bean.com/read/customizing-the-output-of-mercurial.html"" rel=""nofollow noreferrer"">style file</a> instead:</p>

<pre><code>changeset = ""{tags}""
tag = ""{node} {tag}\n""
</code></pre>

<p>It outputs one line per <em>tag</em>, not changeset. You save this style somewhere and configure a merge tool:</p>

<pre><code>[merge-tools]
hgtags.executable = hg
hgtags.args = log -r ""tagged()"" --style ~/tmp/tags-style &gt; $output
hgtags.premerge = False
hgtags.priority = -1000

[merge-patterns]
.hgtags = hgtags
</code></pre>

<p>You now have automatic tag merges. There are some caveats:</p>

<ol>
<li><p><strong>Three or more heads:</strong> The technique only works if you have <strong>two heads</strong> at the time of merging. If you have three heads or more, it's possible for a deleted tag to re-appear. If you have heads X, Y, and Z, and the tag <code>A</code> is deleted in X, then Mercurial is normally able to figure out that <code>A</code> is deleted overall. It does this based on a <code>000...0 A</code> line in the <code>.hgtags</code> file in X. However, if you merge X and Y to get W, then the approach suggested will not contain any such <code>000...0 A</code> line. The definition of <code>A</code> from Z will now suddenly take effect and re-introduce <code>A</code>.</p></li>
<li><p><strong>Real conflicts:</strong> If you have real conflicts in <code>.hgtags</code>, then the above method will <strong>silently pick the tag from the most recent head</strong> for you. The merge tool basically saves <code>hg tags</code> in <code>.hgtags</code>, and the behavior of <code>hg tags</code> with multiple heads is <a href=""https://www.mercurial-scm.org/wiki/Tag#How_do_tags_work_with_multiple_heads.3F"" rel=""nofollow noreferrer"">explained in the wiki</a>. Since <code>hg tags</code> unconditionally reads and silently merges the <code>.hgtags</code> file from all heads, there's nothing we can do about it with this simple approach. Dealing with this would require a bigger script that reads the two <code>.hgtags</code> files and detects the conflict.</p></li>
</ol>",8.0,2012-03-20 08:27:18.567000 UTC,2018-02-22 10:24:25.503000 UTC,31.0,[]
How to update to the last version in mercurial after adding new files?,"<p>I've created a repository in the project's directory. I've added all the files and made some commits. Then I added new files to project and to repository. After that I returned to earlier version and now I can't update to last version. After <code>hg update tip</code> I'm getting this message:</p>

<pre><code>abort: untracked file in working directory differs from file in requested
revision: '.DS_Store'
</code></pre>

<p>I'm new to mercurial. How can I fix this?</p>",1,0,2012-01-17 06:28:48.873000 UTC,,2012-03-10 11:30:06.670000 UTC,5,version-control|mercurial|dvcs,5388,2010-09-10 06:59:15.220000 UTC,2021-01-19 18:16:12.827000 UTC,"Moscow, Russia",23510,1148,114,1526,"<p>It means that Mercurial is unsure about what to do. You have a file with content <code>something</code> in the working copy. The file is not version controlled, so Mercurial will normally leave it alone when you update. However, the revision you're updating to <strong>also</strong> has a file with the same name, and the content <strong>differs</strong> from the <code>something</code> you already have in the file.</p>

<p>You can get this problem if you do</p>

<pre><code>$ hg init
$ echo ""a file"" &gt; a
$ hg add a
$ hg commit -m ""added a""
$ echo ""b file"" &gt; b
$ hg add b
$ hg commit -m ""added b""
</code></pre>

<p>You now have two revisions, the latest has files <code>a</code> and <code>b</code>. If you update back to the first revision and create a different file <code>b</code>, then you get trouble:</p>

<pre><code>$ hg update 0
$ echo ""other b file"" &gt; b
$ hg update
abort: untracked file in working directory differs from file in requested
revision: 'b'
</code></pre>

<p>The normal solution is to commit <em>before</em> updating. You should generally not update with a dirty working copy (""dirty"" means that <code>hg status</code> isn't empty). Mercurial does support such updates, but you should only use them if you know what you're doing.</p>

<p>So to continue the example above we can either commit the new <code>b</code> and then merge:</p>

<pre><code>$ hg add b
$ hg commit -m ""added new b""
$ hg merge
</code></pre>

<p>This gives a conflict in <code>b</code> since the two versions contain <code>b file</code> and <code>other b file</code>, respectively. Resolve the conflict and commit:</p>

<pre><code>$ hg commit -m ""merged two bs""
</code></pre>

<p>An alternative is to delete the file from the working copy. <strong>That is what I'll do in your case:</strong> <code>.DS_Store</code> files should not be tracked in the first place. They store some folder information on Mac OS X and this is not part of your source code. So you do</p>

<pre><code>$ rm .DS_Store
$ hg update
</code></pre>

<p>The update resurrected the <code>.DS_Store</code> file. You now want to tell Mercurial that it should stop tracking the file:</p>

<pre><code>$ hg forget .DS_Store
</code></pre>

<p>and you also want to tell Mercurial to ignore such files from now on:</p>

<pre><code>$ echo ""syntax: glob"" &gt;&gt; .hgignore
$ echo "".DS_Store"" &gt;&gt; .hgignore
$ hg commit -m ""removed .DS_Store file, ignored from now on""
</code></pre>",0.0,2012-01-17 06:47:02.503000 UTC,,12.0,[]
Branching in Mercurial,"<p>I have starting using Mercurial for my (our) versioning needs. I have now come to the point that I need to create a feature branch. However, now that I have started working on it -- and I try to push my changes, I keep getting a warning about new remote heads. That's stupid, I know that there will be new remote head(s) that's what a branch after all is?</p>

<p>How am I supposed to create branches and push them without this problem, without using force push as it is surely not the right way to go, right?</p>

<p>I thought about using separate repositories, but that feels just stupid especially for feature branches.</p>

<p>Any help welcome!</p>",2,1,2010-11-30 19:49:13.590000 UTC,2.0,2010-11-30 19:59:05.153000 UTC,8,mercurial|branch|dvcs,1812,2009-05-11 14:24:50.830000 UTC,2014-06-16 15:56:52.533000 UTC,,92393,1766,52,2413,"<p>To date, the best guide out there is Steve Losh's ""<a href=""http://stevelosh.com/blog/2009/08/a-guide-to-branching-in-mercurial/"" rel=""nofollow noreferrer"">A Guide to Branching in Mercurial</a>"".</p>

<p>Mercurial will always complain about creating new heads on the remote.  You must use either <code>--force</code> or <code>--new-branch</code> when creating a new head.</p>

<p>When using TortoiseHg, the same can be accomplished via the <code>Synchronize</code> view of the Workbench.  Click <code>Options</code> and then select the <code>Allow push of a new branch</code> or <code>Force push or pull</code> option, as needed.</p>

<p><img src=""https://i.stack.imgur.com/E6eAN.png"" alt=""TortoiseHg v2.x""></p>

<p>The reason it behaves this is way is that the Mercurial developers wanted to make it a conscious decision to create a new head on the remote.  Their view is that typical workflows should merge changes prior to pushing.</p>",3.0,2010-11-30 20:03:28.133000 UTC,2012-02-15 19:23:52.730000 UTC,13.0,[]
What is the Difference Between Mercurial and Git?,"<p>I've been using git for some time now on Windows (with msysGit) and I like the idea of distributed source control. Just recently I've been looking at Mercurial (hg) and it looks interesting. However, I can't wrap my head around the differences between hg and git.</p>

<p>Has anyone made a side-by-side comparison between git and hg? I'm interested to know what differs hg and git without having to jump into a fanboy discussion.</p>",25,0,2008-08-30 09:48:27.520000 UTC,624.0,2012-01-31 20:07:55.410000 UTC,727,git|version-control|mercurial|comparison|dvcs,632043,2008-08-30 09:06:27.857000 UTC,2022-01-10 10:24:26.840000 UTC,"Lund, Sweden",115720,2486,36,4308,"<p>These articles may help:</p>

<ul>
<li><a href=""http://importantshock.wordpress.com/2008/08/07/git-vs-mercurial/"" rel=""noreferrer"">Git vs. Mercurial: Please Relax</a> (Git is MacGyver and Mercurial is James Bond)</li>
<li><a href=""http://www.rockstarprogrammer.org/post/2008/apr/06/differences-between-mercurial-and-git/"" rel=""noreferrer"">The Differences Between Mercurial and Git</a></li>
</ul>

<p><strong>Edit</strong>: Comparing Git and Mercurial to celebrities seems to be a trend. Here's one more:</p>

<ul>
<li><a href=""http://www.ericsink.com/entries/hg_denzel.html"" rel=""noreferrer"">Git is Wesley Snipes, Mercurial is Denzel Washington</a></li>
</ul>",1.0,2008-08-30 09:57:01.963000 UTC,2011-11-29 11:36:50.483000 UTC,345.0,[]
"Is there an open source equivalent to piper, Google's version control tool?","<p>Google stores all its codebase in a single repository called piper [1][2][3].</p>

<p>It has an approach that is very different than open source alternatives do (centralized 'cloud' service) and aims at scaling to a repository with billions of files, thousands of developers and millions of commits [1].</p>

<p>It doesn't seem Google open-sourced it nor plan to do so (contrary to their build system blaze and some other tools [4]).</p>

<p>Are you aware of any open source version control system with an approach similar to piper ?</p>

<p>[1] <a href=""https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext"" rel=""noreferrer"">https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext</a></p>

<p>[2] <a href=""https://www.wired.com/2015/09/google-2-billion-lines-codeand-one-place/"" rel=""noreferrer"">https://www.wired.com/2015/09/google-2-billion-lines-codeand-one-place/</a></p>

<p>[3] <a href=""https://www.youtube.com/watch?v=W71BTkUbdqE"" rel=""noreferrer"">https://www.youtube.com/watch?v=W71BTkUbdqE</a></p>

<p>[4] <a href=""https://opensource.google.com/projects/list/developer-tools?page=3"" rel=""noreferrer"">https://opensource.google.com/projects/list/developer-tools?page=3</a></p>",4,5,2017-09-24 14:59:12.967000 UTC,9.0,2018-05-14 18:07:58.273000 UTC,47,version-control|dvcs,31366,2013-07-17 08:38:52.463000 UTC,2022-02-28 17:15:53.373000 UTC,Just behind you,1852,344,2,197,"<p>The short answer is no, it doesn't seem to exist.</p>

<p>As you can read in a <a href=""https://www.quora.com/What-version-control-system-does-Google-use-and-why"" rel=""noreferrer"">Quora article</a>, ""it’s hard to tell where the version control system ends, and where some of the other parts of the development toolchain begin"".</p>

<p>So, first, you need to be clear in what ""features"" you are interested in since you can be interested in a feature that is not Piper's responsibility.</p>

<p>Also, keep in mind that your server disk space and OS would limit the file count/size before the chosen VCS.</p>

<p>If you need a Centralized VCSs and billions of files, you could go with SVN or OpenCVS.</p>

<p>If you need a Distributed one with thousands of developers and millions of commits, take a look at Git, Bazaar, Bitbucket or Mercurial.</p>

<p>But do you <em>really</em> have all those requirements?</p>

<p>AFAIK there's no Piper's open source equivalent on the market.</p>

<p>In order to better understand Centralized and Distributed VCS, take a look at <a href=""https://stackoverflow.com/questions/111031/comparison-between-centralized-and-distributed-version-control-systems"">this Comparison between Centralized and Distributed Version Control Systems</a></p>

<p>Also, take a look at <a href=""https://softwareengineering.stackexchange.com/questions/41435/what-is-googles-repository-like"">what is Google's repository like?</a></p>",5.0,2017-10-03 18:10:21.863000 UTC,2018-06-05 18:42:03.163000 UTC,31.0,[]
"""bzr uncommit"" equivalent in Mercurial?","<p>Bazaar has a wonderful <code>uncommit</code> command, which simply undoes the last commit.  Is there any equivalent in Mercurial?</p>

<p><strong>Edit</strong>: Bazaar's uncommit command does not modify files – it removes the last commit and associated data (useful, for example, when you notice too late that there is a typo in a commit message or a file wasn't added that should have been).</p>

<p>For example:</p>

<pre><code>$ bzr ci -m ""Fixed a proooblem"" &lt;-- problem is misspelt
$ bzr uncommit
...
$ bzr ci -m ""Fixed a problem"" &lt;-- Exactly as if the first commit was correct.
</code></pre>",5,0,2009-05-28 01:43:26.943000 UTC,2.0,2012-01-26 09:12:35.080000 UTC,12,version-control|mercurial|dvcs|bazaar,3281,2009-02-26 18:36:33.130000 UTC,2022-03-04 22:59:50.527000 UTC,"Montreal, QC, Canada",137559,2272,352,8555,"<p>Maybe <code>hg backout tip</code>?  I recommend <a href=""http://hgbook.red-bean.com/read/finding-and-fixing-mistakes.html"" rel=""noreferrer"">http://hgbook.red-bean.com/read/finding-and-fixing-mistakes.html</a> for all details about <code>hg backout</code>, how it differs from <code>hg revert</code>, and many related topics, but if I undestand what <code>uncommit</code> does, it does seem exactly equivalent to <code>hg backout tip</code>.</p>

<p>Edit: in a comment you've now clarified you want to ""delete history"" -- that's hard (unless you're VERY fast on the draw, maybe;-)... per the red-bean book again:</p>

<blockquote>
  <p>Since Mercurial treats history as accumulative—every change builds on top of all changes that preceded it—you generally can't just make disastrous changes disappear. The one exception is when you've just committed a change, and it hasn't been pushed or pulled into another repository. That's when you can safely use the hg rollback command</p>
</blockquote>

<p>So if you just want to <em>try to</em> ""make it disappear"" (and you're lucky enough that it hasn't yet been pushed or pulled elsewhere), then <code>hg rollback</code> may be a better approach!</p>",3.0,2009-05-28 01:53:56.407000 UTC,2009-05-28 02:00:34.130000 UTC,8.0,[]
Is DVCS (Mercurial) not for me?,"<p>I work in a company where we create a lot of small customer-specific applications.
We are a few developers but most of the time there is only one developer per project.</p>

<pre><code>Customer1
    ProjectX
        App
        Tests
    ProjectY
        App
        Tests
Customer2
    Project2
Products
    Product1
Common
</code></pre>

<p>Today everything is stored in a single repository.</p>

<p>The process is simple.  </p>

<ol>
<li>A developer takes on a new project for a customer</li>
<li>Create a new folder for the project</li>
<li>Code in new project</li>
<li>Do some maintenance in another project</li>
<li>Check in updates to maintenance project</li>
<li>More work in new project</li>
<li>Check in new project</li>
<li>Deliver to customer</li>
</ol>

<p>There is no tagging nor branching. Earlier versions are checked out based on date.</p>

<p>This process has served well for many years but there are a few pain points with the current tool (CVS)  </p>

<ul>
<li>Slow. Checkout takes minutes even if nothing has changed. History is stored on the server so diffs takes too long time</li>
<li>Adding new projects. If you worked with CVS you know it is like: add folder, add files in folder, add next folder...</li>
<li>No way to back out obvious errors (checking in binaries etc)</li>
<li>No support for renaming which makes necessary refactoring even more painful.</li>
</ul>

<p>I have used Mercurial privately for some time and would like to extend it to all developers.</p>

<p>I might have got it all wrong but there are a few things that I don't understand how to implement in our organization.</p>

<p>CVS commits are current folder only but in mercurial they are repository wide.
In our case it means that committing maintenance work in one folder will also commit yet unfinished stuff in another folder.
(I assume we could do <code>hg ci ./**</code> in changed folders but that is not allowed on merge, at least that is what the documentation says <code>If you are committing the result of a merge, do not provide any filenames or -I/-X filters.</code>)</p>

<p>The common practice in Mercurial is to have one repository per project.</p>

<p>One repository per project is OK for us but it creates some other issues like:</p>

<p><strong>How to manage multiple repositories on the central server?</strong><br>
If a developer creates a new project he eventually need to push his changes.
Just doing</p>

<p><code>hg push <a href=""http://localhost:8000/Customer1/NewProject"" rel=""nofollow"">http://localhost:8000/Customer1/NewProject</a></code></p>

<p>crashes the hg-webserver with an ugly stack dump and hangs the client.</p>

<p>The way I understand it is that the developer need access to the server shell to add the new repository to a configuration file and restart hgweb</p>

<p>The alternative is to use SSH or a share 
(are there benefits using SSH instead of a file share?) </p>

<pre><code>cd Customer\NewProject
hg init
hg clone --noupdate --pull . //mercurialshare\Customer\Project
echo ""[paths]"" &gt;.hg\hgrc
echo ""default=//mercurialshare\Customer\Project"" &gt;&gt;.hg\hgrc

hg push
</code></pre>

<p>Works, but is a bit to complicated for some developers</p>

<p><strong>All developers need to have all projects.</strong><br>
(Not really all but many projects are linked so they need to be present and it is easiest to just have all)</p>

<p>With many existing projects and new ones added weekly we need a way to pull all projects in one go and also clone new ones.</p>

<p>I was thinking that subrepos could solve the ""global"" pull but the following
line in the documentation is a showstopper</p>

<p>""When we commit, Mercurial will attempt to create a consistent snapshot of the state of the entire project and its subrepos.
It does this by first attempting to commit in all modified subrepos and then recording the state of all subrepos.""</p>

<p>Back to the single repository problem of global commits.</p>

<p>(Tried a few variants of <code>hg ci .hgsub .hgsubstate &lt;subrepo&gt;</code> but .hgsubstate seem to only be updated on full commits. Other users will not see project changes without an explicit <code>hg pull --update</code> in the project folder)</p>

<p>My current thinking is to have a batch file in the root that pulls all projects</p>

<p>Any other ideas on how to use mercurial in our organization?</p>

<p><strong>Edit</strong></p>

<p>Thanks for the reply. I am currently evaluating how one repository per project will work for us. I put a batch file at the top level   </p>

<pre><code>FOR /F %%x IN (repolist.txt) DO (
    If EXIST .\%%x\.hg (
        ECHO Pull %%x
        hg pull --update --repository .\%%x
    ) ELSE (
        ECHO Clone %%x
        mkdir .\%%x
        hg clone --pull %1\%%x .\%%x
    )
)
</code></pre>",1,1,2010-11-07 20:10:00.220000 UTC,4.0,2010-11-09 08:14:08.777000 UTC,6,mercurial|dvcs|subrepos,394,2009-08-16 09:40:53.047000 UTC,2022-03-05 09:40:40.723000 UTC,Sweden,13950,195,6,898,"<p>Your right in saying that Mercurial is designed for one project per repo. It's also a lot nicer when you work like this because the history of different projects are kept separate.</p>

<p>Trying to have multiple projects in a DVCS repo just causes pain.</p>

<p>Personally I prefer serving projects via SSH rather than HTTP. One reason is the ability to...</p>

<pre><code># hg init blah
# hg clone blah ssh://server/blah
</code></pre>

<p>If you're serving via HTTP this doesn't work (as you've found out). I'm surprised it causes a hard crash though :-/</p>

<p>The sub-repos method of getting all projects isn't quite as you describe it. It's not that you're back to global commits (projects can be developed individually), but that the super-project stores the version of the sub-projects it depends on. This is exactly what you want if you have (for example) a library as a subproject, but the release depends on a specific version. Effectively a sub-repo link is a bookmark into another repo <strong>at a specific version</strong>.</p>

<p>Not really what you're after though.</p>

<p>Possibly, the common stuff should be a sub-repo of the projects that need it. Each project might then be frozen on a different version of the same code and you've got no problems. That would need a little thinking about.</p>

<p>Otherwise the script idea is probably easiest.</p>",0.0,2010-11-08 00:21:21.593000 UTC,,8.0,[]
Transferring legacy code base from cvs to distributed repository (e.g. git or mercurial). Suggestions needed for initial repository design,"<h2>Introduction and Background</h2>

<p>We are in the process of changing source control system and we are currently evaluating git and mercurial. The total code base is around 6 million lines of code, so not massive and not really small either.</p>

<p>Let me first start off with a very brief introduction to how the current repository design looks.</p>

<p>We have one base folder for the complete code base, and beneath that level there are all sorts modules used in several different contexts. For example “dllproject1” and “dllproject2” can be looked at as completely separate projects.</p>

<p>The software we are developing is something we call a configurator, which can be customized endlessly for different customer needs. At total we probably have 50 different versions of them. However, they have one thing in common. They all share a couple of mandatory modules (mandatory_module1 ..). These folders basically contain kernel/core code and common language resources etc. All customizations can then be any combination between the other modules (module1 ..).</p>

<p>Since we currently are using cvs we've added aliases in the CVSROOT/modules file. They might look something like:</p>

<pre><code>core –a mandatory_module1 mandatory_module2 mandatory_module3
project_x –a module1 module3 module5 core
</code></pre>

<p>So if someone decides to work on project_x, he/she can quickly checkout the modules needed by:</p>

<pre><code>base&gt;cvs co project_x
</code></pre>

<h2>Questions</h2>

<p>Intuitively it just feels wrong to have the base folder as a single repository. As a programmer you should be able to check out the exact code sub set needed for the current project you are working with. What are your thoughts on this?</p>

<p>On the other hand it feels more right to have each of these modules in separate repositories. But this makes it harder for programmers to check out the modules that they need. You should be able to do this by a single command. So my question is: Are there similar ways of defining aliases in git/mercurial?</p>

<p>Any other questions, suggestions, pointers are highly welcome!</p>

<p>PS. I have searched for similar questions but didn’t feel that any of them applied 100% to my situation.</p>",2,2,2009-05-22 18:42:13.643000 UTC,1.0,2015-09-22 16:45:22.233000 UTC,12,git|mercurial|cvs|dvcs,4604,2009-05-13 11:05:17.550000 UTC,2022-02-28 12:43:59.100000 UTC,,115611,1705,87,3087,"<p>Just a quick comment to remind you that:</p>

<ul>
<li>those migrations often offer the opportunity to reorganize the sources, not along modules (each with one repositories) but rather along a functional domain split (several modules for a same given functional domain being put in the same repository).</li>
</ul>

<p>Then <a href=""http://git-scm.com/docs/git-submodule"" rel=""nofollow noreferrer"">submodules</a> are to be used, as a way to define a <a href=""https://stackoverflow.com/questions/769786/vendor-branches-in-git/769941#769941"">configuration</a>.</p>

<ul>
<li>Git is alright, but from <a href=""http://osdir.com/ml/git/2009-05/msg00051.html"" rel=""nofollow noreferrer"">Linus's admission himself</a>, to put everything into one repository can be problematic.</li>
</ul>

<blockquote>
  <p>[...] CVS, ie it really ends up being pretty much oriented to a ""one file
  at a time"" model.</p>
  
  <p>Which is nice in that you can have a million files, and then only check
  out a few of them - you'll never even <em>see</em> the impact of the other
  999,995 files.</p>
  
  <p>Git
  fundamentally never really looks at less than the whole repo. Even if you
  limit things a bit (ie check out just a portion, or have the history go
  back just a bit), git ends up still always caring about the whole thing,
  and carrying the knowledge around.</p>
  
  <p>So git scales really badly if you force it to look at everything as one
  <em>huge</em> repository. I don't think that part is really fixable, although we
  can probably improve on it.</p>
  
  <p>And yes, then there's the ""big file"" issues. I really don't know what to
  do about huge files. We suck at them, I know. </p>
</blockquote>

<hr>

<p>Those two aforementioned points advocate for a more component-oriented approach for large system (and large legacy repository).</p>

<p>With <a href=""http://book.git-scm.com/5_submodules.html"" rel=""nofollow noreferrer"">Git submodule</a>, you can checkout them in your project (even if it is a two-steps process). You have however tools than can make the submodule management easier (<a href=""http://flavoriffic.blogspot.com/2008/05/managing-git-submodules-with-gitrake.html"" rel=""nofollow noreferrer"">git.rake</a> for instance).</p>

<hr>

<blockquote>
  <p>When I'm thinking of fixing a bug in a module that's shared between several projects, I just fix the bug and commit it and all just do their updates</p>
</blockquote>

<p>That is what I describe in the post <a href=""https://stackoverflow.com/questions/769786/vendor-branches-in-git/769941#769941"">Vendor Branch</a> as the ""system approach"": everyone works on the latest (HEAD) of everything, and it is effective for small number of projects.<br>
For a large number of modules though, the notion of ""module"" is still very useful, but its management is not the same with DVCS:</p>

<ul>
<li><p>for closely related modules (aka ""in the same functional domain"", like ""all modules related to PNL - Profit aNd Losses - or ""Risk analysis"", in a financial domain), you do need to work with the latest (HEAD) of all components involved.<br>
That would be achieved with the use of a <a href=""http://www.kernel.org/pub/software/scm/git/docs/howto/using-merge-subtree.html"" rel=""nofollow noreferrer"">subtree strategy</a>, not in order for you to publish (push) corrections on those other submodules, but to track works done by other teams.<br>
Git allows that with the extra-bonus that this ""tracking"" does not have to take place between your repository and one ""central"" repository, but can also take place between you and the local repository of the other team, allowing for a very quick back-and-forth integration and testing between projects of similar nature.</p></li>
<li><p>however, for modules which are not directly in your functional domain, submodules are a better option, because they refer to a fix version of a module (a commit):<br>
when a low-level framework changes, you do not want it to be propagated <em>instantaneously</em>, since it would impact all the other teams, which would then have to drop what they were doing to adapt their code to that new version (you do want though all the other teams to be aware of this new version, in order for them to not forget to update that low-level component or ""module"").<br>
That allows you to work only with official stable identified versions of other modules, and not potentially un-stabled or not fully tested HEADs. </p></li>
</ul>",2.0,2009-05-22 18:54:40.783000 UTC,2017-05-23 12:00:17.880000 UTC,13.0,[]
Automatically mirror a git repository,"<p>One of the side-effects of using an external Subversion repository was getting automatic offsite backups on every commit.</p>

<p>I'd like to achieve the same using Git. </p>

<p>i.e. every commit to my local repository automatically commits to an external one so the two repositories are always in sync.</p>

<p>I imagine that a post-commit hook would be the way to go. Does anyone have any specific examples of this?</p>",3,0,2010-08-27 09:56:10.967000 UTC,14.0,,29,git|version-control|dvcs,15653,2008-12-13 11:38:18.753000 UTC,2022-02-18 12:09:36.700000 UTC,"Brighton, United Kingdom",20249,355,110,927,"<p>I wrote a post-commit hook for just this purpose. The hook itself is simple; just add a file named <code>post-commit</code> to your <code>.git/hooks/</code> directory with the following contents:</p>

<pre><code>git push my_remote
</code></pre>

<p>The <code>post-commit</code> file should be executable. Also make sure that you add a suitable <a href=""http://www.kernel.org/pub/software/scm/git/docs/git-remote.html"" rel=""noreferrer"">remote</a> repository with the name <code>my_remote</code> for this this hook to work.</p>

<p>I also made a symlink named <code>post-merge</code> that points to <code>post-commit</code>. This is optional. If you do this you'll auto-sync after merges as well. </p>

<p><strong>UPDATE:</strong> If you want to ensure that your server, and your mirror do not get out of sync, and ensure that all branches are also backed up, your <code>post-commit</code> hook can use:</p>

<pre><code>git push my_remote -f --mirror
</code></pre>",7.0,2010-08-27 10:02:59.553000 UTC,2012-03-09 02:41:20.980000 UTC,27.0,[]
How do I show the changes which have been staged?,"<p>I staged a few changes to be committed; how can I see the diff of all files which are staged for the next commit? I'm aware of <a href=""http://git-scm.com/docs/git-status"" rel=""noreferrer"">git status</a>, but I'd like to see the actual diffs - not just the names of files which are staged.</p>
<p>I saw that the <a href=""http://git-scm.com/docs/git-diff"" rel=""noreferrer"">git-diff(1)</a> man page says</p>
<blockquote>
<p>git diff [--options] [--] […]</p>
<p>This form is to view the changes you made relative to the index (staging area for the next commit). In other words, the differences are what you <em>could</em> tell git to further add to the index but you still haven't. You can stage these changes by using git-add(1).</p>
</blockquote>
<p>Unfortunately, I can't quite make sense of this. There must be some handy one-liner which I could create an alias for, right?</p>",16,2,2009-10-19 09:57:27.417000 UTC,511.0,2020-06-20 09:12:55.060000 UTC,2395,git|diff|dvcs|git-diff|git-stage,981758,2009-04-16 17:54:19.293000 UTC,2022-03-05 10:19:24.890000 UTC,"Hamburg, Germany",85489,1500,73,2856,"<p>It should just be:</p>

<pre><code>git diff --cached
</code></pre>

<p><code>--cached</code> means show the changes in the cache/index (i.e. staged changes) against the current <code>HEAD</code>. <code>--staged</code> is a synonym for <code>--cached</code>.</p>

<p><code>--staged</code> and <code>--cached</code> does not point to <code>HEAD</code>, just difference with respect to <code>HEAD</code>. If you cherry pick what to commit using <code>git add --patch</code> (or <code>git add -p</code>), <code>--staged</code> will return what is staged.</p>",5.0,2009-10-19 10:07:10.820000 UTC,2018-02-28 23:35:53.407000 UTC,2910.0,[]
DVCS - How often and when to commit changes,"<p>There is another <a href=""https://stackoverflow.com/questions/107264/how-often-to-commit-changes-to-source-control"">thread</a> here on StackOverflow, dealing wih how often to commit changes to source control. I want to put that in the context of using a DVCS like git or mercurial.</p>

<ol>
<li><p>How often and when do you commit? </p></li>
<li><p>Do you only commit changes when they
build correctly?</p></li>
<li><p>How often and when do you push your changes (or file a pull request or similar)?</p></li>
<li><p>How do you approac developing a complex feature / doing a complex refactoring requiring many places to be touched? Are ""private commits"" that won't build ok? When finished, do you push them also to the master repository or do you bundle all your changes into a single changeset before pushing?</p></li>
</ol>",5,3,2009-09-26 07:59:26.387000 UTC,12.0,2017-05-23 12:23:07.427000 UTC,30,dvcs|commit,5735,2009-06-18 21:18:15.480000 UTC,2022-03-03 17:07:26.190000 UTC,Germany,34492,5156,90,4251,"<p>It depends on the nature of the branch (""line of development"") you are working on.</p>

<p>The main advantage with those DVCS (git or mercurial) is the ease you can:</p>

<ul>
<li>branch</li>
<li>merge</li>
</ul>

<p>So:</p>

<blockquote>
  <p>1/ How often and when do you commit?<br>
  2/ Do you only commit changes when they build correctly?</p>
</blockquote>

<p>As many time as necessary on a private branch (for instance, if it compiles).<br>
The practice to only commit if unit tests pass is a good one, but should only apply to an ""official"" (as in ""could be published or 'pushed'"") branch: in your private branch, you merge a gazillon times if you need to.<br>
The only thing is: do some merge --interactive to reorganize your many commits on your private branch, before replaying them on your main development branch, where you can pass some tests.</p>

<blockquote>
  <p>3/ How often and when do you push your changes (or file a pull request or similar)?  </p>
</blockquote>

<p>Publication is another matter and should be done with a ""clear"" history (coherent merges, representing a content which compile and pass some tests).<br>
The branch you publish should be one where the history is never rewritten, always updated.<br>
The pace of the publications depends on the nature of the remote branch and of the population pulling that branch. For instance, if it is for another team, you could push quite often. If it is for a system-wide integration testing team, you will push a lot less often.</p>

<blockquote>
  <p>4/ How do you approach developing a complex feature / doing a complex refactoring requiring many places to be touched? Are ""private commits"" that won't build ok? When finished, do you push them also to the master repository or do you bundle all your changes into a single changeset before pushing?</p>
</blockquote>

<p>See 1. and 2.: patch first in your own private branch, then reorganize your commits on an official (published) patch branch. One single commit is not always the best option if the patch involves several different ""activities"" (or bug fix).</p>",2.0,2009-09-26 08:16:43.337000 UTC,2009-09-26 11:08:41.633000 UTC,14.0,[]
Version-controlled extension configuration in Mercurial,"<p>Normally, I would enable extensions by adding the following to <code>.hg/hgrc</code>:</p>

<pre><code>[extensions]
hgext.win32text=
[encode]
** = cleverencode:
[decode]
** = cleverdecode:
</code></pre>

<p>However, I want this configuration to be versioned, i.e. part of the repository, so that it is enabled for anyone else (coworkers, build machines) cloning the repository.  Note that whomsoever clones the repository should not be required to do <em>anything</em> to enable these extensions.</p>

<p>It appears it is not possible from <a href=""http://www.selenic.com/mercurial/wiki/UsingExtensions"" rel=""nofollow noreferrer"">the documentation</a>, but does anyone know any neat tricks that can help me here?</p>",4,0,2009-05-13 07:18:46.467000 UTC,3.0,2012-03-30 12:37:52.333000 UTC,8,version-control|configuration|mercurial|dvcs|mercurial-extension,678,2008-09-15 12:42:54.360000 UTC,2018-06-25 11:15:32.393000 UTC,"Bergen, Norway",15658,913,328,918,"<p>You want mercurial to do something automatically when you clone a repo (update the hooks or config). <a href=""http://hgbook.red-bean.com/read/handling-repository-events-with-hooks.html"" rel=""noreferrer"">Documentation</a> says it is not possible and gives some very good reasons:</p>

<pre><code>Hooks do not propagate

In Mercurial, hooks are not revision controlled, and do not propagate when you clone,
or pull from, a repository. The reason for this is simple: a hook is a completely    
arbitrary piece of executable code. It runs under your user identity, with your 
privilege level, on your machine. No comments

It would be extremely reckless for any distributed revision control system to 
implement revision-controlled hooks, as this would offer an easily exploitable way to 
subvert the accounts of users of the revision control system. No comments
</code></pre>

<p>So clearly, mercurial itself won't solve your problem. You clearly state that you want nothing but mercurial to solve your problem, so the answer is: what you are asking is not possible.</p>

<p>The only way to solve your problem is that all your users will have to run/install at least once a given script that perform the actions you want, something like installing the right hooks.</p>

<p>If you want to be clever about this:</p>

<ul>
<li>create a one-time script to run that will install a hook to copy the right config into the .hg or the user</li>
<li>make sure that the hook, once installed, can update the script to distribute config updates to the users</li>
<li>make the hook add some special marking to commit messages</li>
<li>refuse on the central repository commit that do not carry the special message</li>
</ul>

<p>A bit complicated, but that's the closest I can imagine to your requirements:</p>

<ul>
<li>user run a script once and they forget</li>
<li>you can make sure that if the did not run it, they can not commit to your central repo</li>
</ul>",1.0,2009-05-13 08:09:58.613000 UTC,2009-05-13 10:46:47.657000 UTC,10.0,[]
Get the dates of pull and update in Mercurial,"<p>Is it possible to know when a certain commit was pulled from a distant repository and the files updated with Mercurial ?</p>

<p>More precisely, I made a <code>hg pull -u</code> a few days ago, and now I'd like to know if this pull downloaded only the last commit, or if there were some commits that had not been pulled yet, making my last pull getting them as well.</p>

<p><code>hg log</code> seems to give the dates of the commits, but nothing about the updates. Is this information anywhere ?</p>",2,0,2012-04-17 10:46:43.040000 UTC,3.0,2012-04-17 11:44:15.247000 UTC,6,mercurial|dvcs|pull,1235,2011-01-26 18:39:13.300000 UTC,2022-01-24 14:44:17.440000 UTC,"Geneva, Switzerland",4991,261,17,241,"<p>This information is not recorded by Mercurial. A Mercurial repository is just a container for changesets and Mercurial does not store how (or when) the changesets entered the repository.</p>

<p>You can setup hooks for this, though you would have to build the scripts yourself. A very rudimentary system would be</p>

<pre><code>[hooks]
pre-pull = (date; hg root; hg tip) &gt;&gt; ~/.pull-log
post-pull = hg tip &gt;&gt; ~/.pull-log
</code></pre>

<p>This would record the current date, the current repository, and the current tip in <code>~/.pull-log</code> just before every <code>hg pull</code>. After the pull the new tip is recorded. You could build scripts that parse the log file to extract information about what each pull did.</p>

<blockquote>
  <p><code>hg log</code> seems to give the dates of the commits, but nothing about the updates</p>
</blockquote>

<p>Yes, <code>hg log</code> is only concerned with the stored history (changesets) and working copy operations like updating is not part of recorded history.</p>

<p>Finally, let me mention that this is the first time I've seen someone ask for a ""pull log"". However, the opposite is quite common: there are scripts for maintaining a ""push log"" on a server to see who pushed what and when. This is done by <a href=""http://hg.mozilla.org/mozilla-central/pushloghtml"" rel=""nofollow"">Mozilla</a> among others. See <a href=""http://hg.mozilla.org/users/bsmedberg_mozilla.com/hgpoller/file/tip/README"" rel=""nofollow"">this README</a> for some starting instructions.</p>",2.0,2012-04-17 11:43:48.897000 UTC,2012-04-17 16:42:12.230000 UTC,8.0,[]
how does TFS's shelveset model compare to a DVCS?,"<p>Let's say a person is working in a small team SCRUM environment. Several teams are working on different branches of the same code base. In our team we usually split up into pairs to work on code. Occasionally people need to take time off etc. and it would be advantageous to be able to merge person A's code into person B's codebase. But they often don't want to commit at the end of the day for fear of breaking the build. </p>

<p>This led me to <strong>DVCS</strong> -- well suited to P2P merging and doesn't require ""the big commit"". A team member suggested that TFS's <strong>shelveset</strong> would be able to do this as well. <strong>How do the two approaches compare?</strong> </p>

<p>Team is currently using Subversion. </p>",2,2,2010-11-10 00:25:47.530000 UTC,1.0,,8,tfs|dvcs,1143,2008-10-23 19:56:12.523000 UTC,2022-03-03 17:21:16.433000 UTC,,39276,3628,113,3125,"<p>The TFS shelveset allows you to create something akin to a changeset, that isn't stored in the main repository. This shelveset is a backup, and can be ""pulled"" by another developer from another machine. This ""private repository"", and the ability to ""pull"" it is as far as the shelveset compares to DVCS. Shelvesets however are not true repositories; they are merely snapshots, they do not maintain history, and you cannot check in or manage changes as you would with the source control repository on your TFS.</p>

<p>A DVCS is a lot more than this; with a DVCS, you have a complete copy of the repository on your machine. You can commit locally as often as you want, storing each and every change you make; you can revert changes as much as you want, since your machine is for all intents and purposes, your source control.
And then you can push your changes to another machine (central repositor, build machine, or whatever). DVCS is a completely different and flexible paradigm, that allows but doesn't require the use of a central repository. It frees you of having to choose between stable and frequent check-ins (you can check-in locally whenever you want, and push when you're stable). </p>

<p>There's a lot more to it. If you like, you can checkout Joel Spolskey's excellent (and hillarious) intro to Mercurial DVCS at <a href=""http://hginit.com/"" rel=""noreferrer"">hginit.com</a>.</p>

<p>Hope this helps,<br>
Assaf.</p>",0.0,2010-11-14 16:30:34.573000 UTC,,14.0,[]
Git versus Mercurial for .NET developers?,"<p>I've been wondering what is the better DVCS for .NET developers?  From reading various information it has sounded like Mercurial works better on Windows, but other information claims that Git has caught up and surpassed Mercurial in offering quality Windows and Visual Studio tools.  Does anyone have good recent information or experience with trying both in a .NET development environment?</p>",7,0,2009-09-30 06:35:04.467000 UTC,10.0,2010-11-04 08:37:18.267000 UTC,29,.net|git|mercurial|dvcs,10341,2008-09-17 06:14:07.367000 UTC,2014-08-18 00:30:15.280000 UTC,,4038,163,68,275,"<p>I have been using Mercurial for over a year now for doing .NET development, and it has been working out great. I do admit I don't use any of the fancy tools (explorer add-ons and Visual studio tools), but they are available (e.g. TortoiseHg). I have found that using the command line tools is just as easy - just specify a few wildcards in .hgignore (as in <a href=""https://stackoverflow.com/questions/1496423/git-versus-mercurial-for-net-developers/1496472#1496472"">Blaenk's answer</a>) and you're good to go.</p>

<p>I'm not sure how well git integrates with hg, but for the reverse siutation I have used <a href=""http://hg-git.github.com/"" rel=""nofollow noreferrer"">hg-git</a> in the past and it worked fine. It is however still somewhat unstable as new releases of hg come out.</p>

<p>So you should be able to use github from mercurial, and for native mercurial repos there is always the (imo) equally nice <a href=""http://bitbucket.org/"" rel=""nofollow noreferrer"">bitbucket.org</a>. Edit: Note also that <a href=""http://www.codeplex.com"" rel=""nofollow noreferrer"">codeplex</a>, which is focused on .NET open source projects, now offers Git and Mercurial repositories.</p>

<p>And I should mention that ""popularity"" is a very difficult criterion to base your choice on. Either DVCS has high-profile users. Git has the linux kernel, of course, while notable hg users include the mozilla and Python projects.</p>

<p><strong>EDIT</strong>: Since this seems to get regular upvotes. Everything I wrote above was true at the time of writing, but I no longer agree with my former self bitbucket is as nice as GitHub. GitHub has better functionality, and from my point of view (mostly F# open source development) everyone else is there so you get much better network effects. I moved all my project from codeplex/bitbucket to GitHub a while ago and immediately started getting contributions, whereas on codeplex/bitbucket next to nothing happened.</p>",2.0,2009-09-30 07:10:08.743000 UTC,2017-05-23 11:55:13.833000 UTC,24.0,[]
git found out number of conflicts / list of conflicts in working folder,"<p>Is there any way to review list of conflicts (names of conflicting files and number of conflicts in it)?</p>

<p>The only thing I have discovered is to review pre-created <code>.git/MERGE_MSG</code> file... but this is not what I really searching for...</p>",4,0,2012-06-13 11:35:02.000000 UTC,2.0,,10,git|dvcs,3489,2011-11-15 14:11:57.343000 UTC,2022-02-22 16:05:09.347000 UTC,"Stockhom, Sweden",8508,1688,4,1263,"<p><strong>Edit</strong>: Of course, the easy, obvious and over-engineering free answer is <code>git status</code>, as <a href=""https://stackoverflow.com/users/720999/kostix"">kostix</a> <a href=""https://stackoverflow.com/a/11015036/220155"">notes</a>. The disadvantage of this is that <code>git status</code> checks the status of the index compared to the working copy, which is slow, whereas the below only checks the index, a much faster operation.</p>

<p>To get the names of the files that are conflicted, use <code>git ls-files --unmerged</code>.</p>

<pre><code>$ git ls-files --unmerged
100755 f50c20668c7221fa6f8beea26b7a8eb9c0ae36e4 1       path/to/conflicted_file
100755 d0f6000e67d81ad1909500a4abca6138d18139fa 2       path/to/conflicted_file
100755 4cb5ada73fbe1c314f68c905a62180c8e93af3ba 3       path/to/conflicted_file
</code></pre>

<p>For ease, I have the following in my <code>~/.gitconfig</code> file (I can't claim credit, but I can't remember the original source):</p>

<pre><code>[alias]
    conflicts = !git ls-files --unmerged | cut -f2 | sort -u
</code></pre>

<p>This gives me:</p>

<pre><code>$ git conflicts
path/to/conflicted_file
</code></pre>

<p>To work out the number of conflicts in a single file, I'd just use <code>grep</code> for the <code>=======</code> part of the conflict marker:</p>

<pre><code>$ grep -c '^=======$' path/to/conflicted_file
2
</code></pre>

<p>You could add the following to your <code>~/.gitconfig</code> as well as the <code>conflicts</code> line above:</p>

<pre><code>[alias]
    count-conflicts = !grep -c '^=======$'
    count-all-conflicts = !grep -c '^=======$' $(git conflicts)
</code></pre>

<p>This will give you:</p>

<pre><code>$ git conflicts
path/to/a/conflicted_file
path/to/another/different_conflicted_file

$ git count-conflicts path/to/a/conflicted_file
2

$ git count-all-conflicts
5
</code></pre>",0.0,2012-06-13 12:08:32.300000 UTC,2017-05-23 12:24:25.080000 UTC,20.0,[]
Why are Mercurial backouts in one branch affecting other branches?,"<p>This is a difficult situation to explain, so bear with me. I have a Mercurial repository with 2 main branches, <em>default</em> and <em>dev</em>.</p>

<p>Work is usually done in a named branch off of <em>dev</em> (a feature branch). There may be many feature branches at any one time. Once work is completed in that branch, it is merged back into <em>dev</em>.</p>

<p>When the time comes to prepare a release, another named branch is created off of <em>dev</em> (a release branch). Sometimes it is necessary to exclude entire features from a release. If that is the case, the merge changeset from where the feature branch was merged into <em>dev</em> is backed out of the new release branch.</p>

<p>Once a release branch is ready to be released, it is merged into <em>default</em> (so <em>default</em> always represents the state of the code in production). Work continues as normal on the <em>dev</em> branch and feature branches.</p>

<p>The problem occurs when the time comes to do another release, including the feature that was backed out in the previous release. A new release branch is created as normal (off of <em>dev</em>). This new release branch now contains the feature that was backed out of the previous release branch (since the backout was performed on the release branch, and the merge changeset remains on the <em>dev</em> branch).</p>

<p>This time, when the release branch is ready for release and is merged into <em>default</em>, any changes that were backed out as a result of the merge backout in the previous release branch are not merged into <em>default</em>. Why is this the case? Since the new release branch contains all of the feature branch changesets (nothing has been backed out), why does the <em>default</em> branch not receive all of these changesets too?</p>

<p>If all of the above is difficult to follow, here's a screenshot from TortoiseHg that shows the basic problem. ""branch1"" and ""branch2"" are feature branches, ""release"" and ""release2"" are the release branches:</p>

<p><img src=""https://i.stack.imgur.com/np1g1.jpg"" alt=""enter image description here""></p>",2,0,2012-02-29 13:54:38.677000 UTC,6.0,2012-02-29 15:20:13.087000 UTC,15,version-control|mercurial|branch|dvcs|three-way-merge,1446,2011-06-09 09:56:49.710000 UTC,2022-03-05 10:15:24.817000 UTC,"St Albans, United Kingdom",159997,924,745,8246,"<p>I believe the problem is that merges work differently than you think. You write</p>

<blockquote>
  <p>Since the new release branch contains all of the feature branch changesets (nothing has been backed out), why does the default branch not receive all of these changesets too?</p>
</blockquote>

<p>When you merge two branches, it's wrong to think of it as applying all changes from one branch onto another branch. So the <code>default</code> branch does not ""receive"" any changesets from <code>release2</code>. I know this is how we normally think of merges, but it's inaccurate.</p>

<p>What really happens when you merge two changesets is the following:</p>

<ol>
<li><p>Mercurial finds the common ancestor for the two changesets.</p></li>
<li><p>For each file that differ between the two changesets Mercurial runs a <a href=""http://en.wikipedia.org/wiki/Merge_(revision_control)"" rel=""nofollow noreferrer""><em>three-way merge</em> algorithm</a> using the ancestor file, the file in the first changeset and the file in the second changeset.</p></li>
</ol>

<p>In your case, you are merging revision 11 and 12. The least common ancestor is revision 8. This means that Mercurial will run a three-way merge between files from there revisions:</p>

<ul>
<li><p><strong>Revision 8:</strong> no backout</p></li>
<li><p><strong>Revision 11:</strong> feature branch has been backed out</p></li>
<li><p><strong>Revision 12:</strong> no backout</p></li>
</ul>

<p>In a three-way merge, a change always trumps no change. Mercurial sees that the files have been changed between 8 and 11 and it sees no change between 8 and 12. So it uses the changed version from revision 11 in the merge. This applies for any three-way merge algorithm. The full merge table looks like this where <code>old</code>, <code>new</code>, ... are the content of matching hunks in the three files:</p>

<pre><code>ancestor  local  other -&gt; merge
old       old    old      old (nobody changed the hunk)
old       old    new      new (they changed the hunk)
old       new    old      new (you changed the hunk)
old       new    new      new (hunk was cherry picked onto both branches)
old       foo    bar      &lt;!&gt; (conflict, both changed hunk but differently)
</code></pre>

<p>I'm afraid that a <a href=""https://www.mercurial-scm.org/wiki/Backout#Backout_of_a_Merge_Changeset"" rel=""nofollow noreferrer"">merge changeset shouldn't be backed out</a> at all because of this surprising merge behavior. Mercurial 2.0 and later will abort and complain if you try to backout a merge.</p>

<p>In general, one can say that the three-way merge algorithm assumes that <strong>all change is good</strong>. So if you merge <code>branch1</code> into <code>dev</code> and then later undo the merge with a backout, then the merge algorithm will think that the state is ""better"" than before. This means that you cannot just re-merge <code>branch1</code> into <code>dev</code> at a later point to get the backed-out changes back.</p>

<p>What you can do is to use a ""dummy merge"" when you merge into <code>default</code>. You simply merge and always keep the changes from the release branch you're merging into <code>default</code>:</p>

<pre><code>$ hg update default
$ hg merge release2 --tool internal:other -y
$ hg revert --all --rev release2
$ hg commit -m ""Release 2 is the new default""
</code></pre>

<p>That will side-step the problem and force <code>default</code> be just like <code>release2</code>. This assumes that absolutely no changes are made on <code>default</code> without being merged into a release branch. </p>

<p>If you must be able to make releases with skipped features, then the ""right"" way is to not merge those features at all. Merging is a strong commitment: you tell Mercurial that the merge changeset now has all the good stuff from both its ancestors. As long as Mercurial wont let you <a href=""https://stackoverflow.com/a/9430810/110204"">pick your own base revision when merging</a>, the three-way merge algorithm wont let you change your mind about a backout.</p>

<p>What you can do, however, is to <em>backout the backout</em>. This means that you re-introduce the changes from your feature branch onto your release branch. So you start with a graph like</p>

<pre><code>release: ... o --- o --- m1 --- m2
                        /      /
feature-A:   ... o --- o      /
                             /
feature-B:  ... o --- o --- o 
</code></pre>

<p>You now decided that the A feature was bad and you backout the merge:</p>

<pre><code>release: ... o --- o --- m1 --- m2 --- b1
                        /      /
feature-A:   ... o --- o      /
                             /
feature-B:  ... o --- o --- o 
</code></pre>

<p>You then merge another feature into your release branch:</p>

<pre><code>release: ... o --- o --- m1 --- m2 --- b1 --- m3
                        /      /             /
feature-A:   ... o --- o      /             /
                             /             /
feature-B:  ... o --- o --- o             /
                                         /
feature-C:  ... o --- o --- o --- o --- o 
</code></pre>

<p>If you now want to re-introduce the A feature, then you can backout <code>b1</code>:</p>

<pre><code>release: ... o --- o --- m1 --- m2 --- b1 --- m3 --- b2
                        /      /             /
feature-A:   ... o --- o      /             /
                             /             /
feature-B:  ... o --- o --- o             /
                                         /
feature-C:  ... o --- o --- o --- o --- o 
</code></pre>

<p>We can add the deltas to the graph to better show what changes where and when:</p>

<pre><code>                     +A     +B     -A     +C     --A
release: ... o --- o --- m1 --- m2 --- b1 --- m3 --- b2
</code></pre>

<p>After this second backout, you can merge again with <code>feature-A</code> in case new changesets have been added there. The graph you're merging looks like:</p>

<pre><code>release: ... o --- o --- m1 --- m2 --- b1 --- m3 --- b2
                        /      /             /
feature-A:   ... o -- a1 - a2 /             /
                             /             /
feature-B:  ... o --- o --- o             /
                                         /
feature-C:  ... o --- o --- o --- o --- o 
</code></pre>

<p>and you merge <code>a2</code> and <code>b2</code>. The common ancestor will be <code>a1</code>. This means that the only changes you'll need to consider in the three-way merge are those between <code>a1</code> and <code>a2</code> and <code>a1</code> and <code>b2</code>. Here <code>b2</code> already have the bulk of the changes ""in"" <code>a2</code> so the merge will be small.</p>",3.0,2012-02-29 14:20:24.317000 UTC,2018-02-22 10:36:00.843000 UTC,26.0,[]
How do I clone a sub-folder of a repository in Mercurial?,"<p>I have a Mercurial repository containing a handful of related projects. I want to branch just one of these projects to work on it elsewhere.</p>

<p>Is cloning just part of a repository possible, and is that the right way to achieve this?</p>",6,1,2009-05-28 11:18:47.360000 UTC,22.0,2012-01-20 08:31:49.090000 UTC,62,mercurial|clone|dvcs,20999,2008-09-16 11:37:23.590000 UTC,2022-02-28 13:57:39.743000 UTC,United Kingdom,13339,910,18,913,"<p>What you want is a <a href=""https://www.mercurial-scm.org/wiki/PartialClone"" rel=""nofollow noreferrer""><strong>narrow</strong> or <strong>partial</strong> clone</a>, but this is unfortunately not yet supported.</p>

<p>If you already have a big repository and you realize that it would make sense to split it into several smaller repositories, then you can use the <a href=""https://www.mercurial-scm.org/wiki/ConvertExtension#Converting_from_Mercurial"" rel=""nofollow noreferrer"">convert extension</a> to do a <strong>Mercurial to Mercurial conversion</strong>. Note that this creates a <strong>new</strong> repository <code>foo</code> and you cannot push/pull between <code>your-big-repo</code> and <code>foo</code>.</p>

<p>The <code>convert extension</code> is not enabled by default so add the following to your repo's <code>hgrc</code> file or your <code>mercurial.ini</code> file:</p>

<pre><code>[extensions]
hgext.convert=
</code></pre>

<p>Then create a <code>map.txt</code> file with</p>

<pre><code>include ""libs/foo""
rename ""libs/foo"" .
</code></pre>

<p>(note you can use forward slashes even on Windows) and run</p>

<pre><code>$ hg convert --filemap map.txt your-big-repo foo
</code></pre>

<p>That will make <code>foo</code> a repository with the full history of the <code>libs/foo</code> folder from <code>your-big-repo</code>. </p>

<p>If you want to delete all evidence of <code>foo</code> from <code>your-big-repo</code> you can make another conversion where you use <code>exclude libs/foo</code> to get rid of the directory. </p>

<p>When you have several repositories like that and you want to use them as a whole, then you should look at <a href=""https://www.mercurial-scm.org/wiki/Subrepository"" rel=""nofollow noreferrer"">subrepositories</a>. This feature lets you include other repositories in a checkout — similarly to how <code>svn:externals</code> work. Please follow the <a href=""https://www.mercurial-scm.org/wiki/Subrepository#Recommendations"" rel=""nofollow noreferrer"">recommendations</a> on that wiki page.</p>",4.0,2009-05-28 15:33:19.983000 UTC,2017-06-30 10:10:34.710000 UTC,53.0,[]
"Create a new branch, made a lot of changes, how to view list of files changed?","<p>So there was a new branch created where we made some breaking changes to the codebase.</p>

<p>Now we are going to merge, but before that I want to get a list of all the files that were changed in the branch.</p>

<p>How can I get a list of files? I tried:</p>

<pre><code>hg status --change REV
</code></pre>

<p>But i'm not sure if that is what I want, since I want all files changed in this branch and not a specific revision in the branch.</p>

<p>BTW, how can I view the revision numbers?</p>",5,1,2012-01-19 15:56:08.560000 UTC,2.0,2012-01-19 18:00:46.037000 UTC,9,mercurial|branch|dvcs|kiln,1716,2011-07-11 18:53:54.187000 UTC,2012-03-05 15:19:08.447000 UTC,,8851,250,4,665,"<p>Try with</p>

<pre><code>$ hg status --rev ""branch('your-branch')""
</code></pre>

<p>to get the changes between the first and the last changeset on the branch (<code>hg status</code> will implicitly use <code>min(branch('your-branch'))</code> and <code>max(branch('your-branch'))</code> when you give it a range of revisions like this).</p>

<p>Since you'll be merging, you should really look at</p>

<pre><code>$ hg status --rev default:your-branch
</code></pre>

<p>to see what is changed <em>between</em> the <code>default</code> branch and <code>your-branch</code>. This shows you the modifications done, and filters out any modifications done on the branch due to merges with <code>default</code>.</p>

<p>This is necessary in case your history looks like this:</p>

<pre><code>your-branch:      x --- o --- o --- o --- o --- o --- y
                 /           /           /
default:  o --- a --- o --- b --- o --- c --- o --- o --- d
</code></pre>

<p>where you've already merged <code>default</code> into your branch a couple of times. Merging <code>default</code> into your branch is normal since you want to regularly integrate the latest stuff from that branch to avoid the branches drifting too far away from each other.</p>

<p>But if a new file was introduced on <code>default</code> and later merged up into <code>B</code>, then you don't really want to see that in the <code>hg status</code> output. You will see it if you do</p>

<pre><code>$ hg status --rev a:y
</code></pre>

<p>since the file was not present in <code>a</code>, but is present in <code>y</code>. If you do</p>

<pre><code>$ hg status --rev d:y
</code></pre>

<p>then you wont see the file in the output, assuming that it's present in both heads.</p>

<hr>

<p>You write in a comment that you're working <em>Kiln</em> repository. They mean ""clone"" when they say ""branch"", but the above can still be adapted for your case. All changesets will be on the <code>default</code> <a href=""http://mercurial.selenic.com/wiki/NamedBranches"" rel=""noreferrer"">named branch</a>, but that's okay.</p>

<p>Run the following command in your local clone of the ""branch"" repository:</p>

<pre><code>$ hg bookmark -r tip mybranch
</code></pre>

<p>This marks the current tip as the head of <code>mybranch</code>. Then pull all the changesets from the main repository:</p>

<pre><code>$ hg pull https://you.kilnhg.com/Repo/public/Group/Main
</code></pre>

<p>You then mark the <em>new</em> tip as the tip of the main repository:</p>

<pre><code>$ hg bookmark -r tip main
</code></pre>

<p>You can now run</p>

<pre><code>$ hg status --rev main:mybranch
</code></pre>

<p>to see the changes between <code>main</code> and <code>my-branch</code>. If you want to see what you did on the branch itself, the use</p>

<pre><code>$ hg status --rev ""::mybranch - ::main""
</code></pre>

<p>The <code>::mybranch</code> part will select changesets that are ancestors of <code>mybranch</code> — this is all your new work, plus old history from before you branched. We remove the old history with <code>- ::main</code>. In older versions of Mercurial, you would use <code>hg log -r -r mybranch:0 -P main</code>.</p>",12.0,2012-01-19 16:04:41.320000 UTC,2012-01-20 10:07:07.593000 UTC,8.0,[]
How to see what will be pushed to a Mercurial repo before doing an actual hg push?,"<p>I want to push my code to a repo, but before doing so I would like to see what changes the push will send.</p>",1,1,2012-01-03 04:54:27.887000 UTC,,2012-06-29 07:59:26.903000 UTC,5,version-control|mercurial|dvcs,274,2010-12-02 04:50:00.383000 UTC,2022-03-05 21:59:33.463000 UTC,"Boulder, CO",8335,1335,13,1038,"<p>This should display the changesets to be pushed.</p>

<pre><code>hg outgoing
</code></pre>

<p><a href=""http://www.selenic.com/mercurial/hg.1.html#outgoing"" rel=""nofollow noreferrer"">http://www.selenic.com/mercurial/hg.1.html#outgoing</a></p>

<p>To customize the output, check this answer:
<a href=""https://stackoverflow.com/a/3041751/62054"">https://stackoverflow.com/a/3041751/62054</a></p>",0.0,2012-01-03 05:00:44.340000 UTC,2017-05-23 12:27:39.523000 UTC,9.0,[]
How do I specify a merge-base to use in a 'hg merge',"<p>I'm trying to do a complicated merge in a complicated hg repository. I'm not happy with the ""newest shared ancestor"" that Mercurial chooses to use as the ""base"" to perform the merge.</p>

<p>I'd like to specify a specific commit of my own choice to use as base.</p>

<p>Is this possible, and if so, how?</p>",3,0,2012-02-19 14:24:18.127000 UTC,4.0,2012-03-12 11:46:05.653000 UTC,6,version-control|mercurial|merge|dvcs|three-way-merge,1450,2008-12-19 22:13:49.520000 UTC,2017-08-03 07:54:04.013000 UTC,"Amsterdam, The Netherlands",4254,47,4,284,"<p><strong>Mercurial 3.0:</strong> You can now select the ancestor to use as a merge base. You do that by setting <code>merge.preferancestor</code>. Mercurial will tell you about it when this makes sense. With the example below, you would see:</p>

<pre><code>$ hg merge
note: using eb49ad46fd72 as ancestor of 333411d2f751 and 7d1f71140c74
      alternatively, use --config merge.preferancestor=fdf4b78f5292
merging x
0 files updated, 1 files merged, 0 files removed, 0 files unresolved
(branch merge, don't forget to commit)
</code></pre>

<hr>

<p><strong>Mercurial before version 3.0:</strong> Lazy Badger is correct that you cannot pick the ancestor picked by Mercurial when using it from the command line. However, you can do it internally and it's not too difficult to write an extension for this:</p>

<pre class=""lang-py prettyprint-override""><code>from mercurial import extensions, commands, scmutil
from mercurial import merge as mergemod

saved_ancestor = None

def update(orig, repo, node, branchmerge, force, partial, ancestor=None):
    if saved_ancestor:
        ancestor = scmutil.revsingle(repo, saved_ancestor).node()
    return orig(repo, node, branchmerge, force, partial, ancestor)

def merge(orig, ui, repo, node=None, **opts):
    global saved_ancestor
    saved_ancestor = opts.get('ancestor')
    return orig(ui, repo, node, **opts)

def extsetup(ui):
    extensions.wrapfunction(mergemod, 'update', update)
    entry = extensions.wrapcommand(commands.table, 'merge', merge)
    entry[1].append(('', 'ancestor', '', 'override ancestor', 'REV'))
</code></pre>

<p>Put this in a file and load the extension. You can now use</p>

<pre><code>hg merge --ancestor X
</code></pre>

<p>to override the normal ancestor. As you've found out, this <strong>does</strong> make a difference if there are several possible ancestors. That situation arises if you have criss-cross merges. You can create such a case with these commands:</p>

<pre><code>hg init; echo a &gt; x; hg commit -A -m a x
hg update 0; echo b &gt;&gt; x; hg commit -m b
hg update 0; echo c &gt;&gt; x; hg commit -m c
hg update 1; hg merge --tool internal:local 2; echo c &gt;&gt; x; hg commit -m bc
hg update 2; hg merge --tool internal:local 1; echo b &gt;&gt; x; hg commit -m cb
</code></pre>

<p>The graph looks like this:</p>

<pre><code>@    changeset: 4:333411d2f751
|\
+---o  changeset: 3:7d1f71140c74
| |/
| o  changeset: 2:fdf4b78f5292
| |
o |  changeset: 1:eb49ad46fd72
|/
o  changeset: 0:e72ddea4d238
</code></pre>

<p>If you merge normally you get changeset <code>eb49ad46fd72</code> as the ancestor and the file <code>x</code> contains:</p>

<pre><code>a
c
b
c
</code></pre>

<p>If you instead use <code>hg merge --ancestor 2</code> you get a different result:</p>

<pre><code>a
b
c
b
</code></pre>

<p>In both cases, my KDiff3 were able to handle the merge automatically without reporting any conflicts. If I use the ""recursive"" merge strategy and pick <code>e72ddea4d238</code> as the ancestor, then I'm presented with a sensible conflict. Git uses the recursive merge strategy by default.</p>",1.0,2012-02-24 12:29:57.637000 UTC,2014-04-21 13:10:28.750000 UTC,15.0,[]
Convert XML data to pandas dataframe via pyspark.sql.dataframe,"<p>My background: long-time SAS and R user, trying to figure out how to do some elementary things in Azure Databricks using Python and Spark. Sorry for the lack of a reproducible example below; I'm not sure how to create one like this.</p>

<p>I'm trying to read data from a complicated XML file. I've reached this point, where I have a pyspark.sql.dataframe (call it xml1) with this arrangement:  </p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>RESPONSE:array
  element:array
    element:struct
      VALUE:string
      VARNAME:string</code></pre>
</div>
</div>
</p>

<p>The xml1 dataframe looks like this:  </p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>[Row(RESPONSE=[[Row(VALUE='No', VARNAME='PROV_U'), Row(VALUE='Included', VARNAME='ADJSAMP'), Row(VALUE='65', VARNAME='AGE'), ...</code></pre>
</div>
</div>
</p>

<p>When I use xml2=xml1.toPandas(), I get this:  </p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>                      RESPONSE
0   [[(No, PROV_U), (Included, ADJSAMP), (65, AGE)...
1   [[(Included, ADJSAMP), (71, AGE), ...
...</code></pre>
</div>
</div>
</p>

<p>At a minimum, I would like to convert this to a Pandas dataframe with two columns VARNAME and VALUE. A better solution would be a dataframe with columns named with VARNAME values (such as PROV_U, ADJSAMP, AGE), with one row per RESPONSE. Helpful hints with names of correct Python terms in intermediate steps are appreciated! </p>",1,0,2019-03-15 15:21:57.630000 UTC,,,-1,python|pandas|dataframe|pyspark|azure-databricks,140,2013-12-10 16:14:50.367000 UTC,2021-09-24 12:36:50.640000 UTC,"St. Louis, MO",125,4,0,6,"<p>To deal with array of structs explode is your answer. Here is link on how to use explode <a href=""https://hadoopist.wordpress.com/2016/05/16/how-to-handle-nested-dataarray-of-structures-or-multiple-explodes-in-sparkscala-and-pyspark/"" rel=""nofollow noreferrer"">https://hadoopist.wordpress.com/2016/05/16/how-to-handle-nested-dataarray-of-structures-or-multiple-explodes-in-sparkscala-and-pyspark/</a></p>",1.0,2019-03-28 02:05:23.390000 UTC,,-1.0,[]
How do I merge SSIS package files?,"<p>I am wondering if anyone has any advice on merging SSIS's dtsx files. Here's the problems I see that make merging difficult:</p>

<ul>
<li>They are xml which can already be a pain for merging.</li>
<li>They can have embedded C# scripts in which case they will have both the C# source code and the base64 encoded string of the dll file. </li>
<li>They describe the flow of data in the package as well as the layout of the elements in the IDE.</li>
</ul>

<p>If anyone from Microsoft is listening, a lot of those problems are solved by making the packages several files rather than one file. One dtsx could be an xml describing the flow, an xml describing the layout, some .cs source files, and some dlls. But that's not how it is. <strike>Makes me wonder why anyone uses dtsx.</strike></p>

<p><strong>A non-solution</strong></p>

<p>The only solution I've seen online is to ensure that the dtsx file is locked when editing so only one user will have changes. This works fine when you're only talking about one branch but if you're working with multiple copies of the dtsx in various branches (or god forbid, <a href=""http://mercurial.selenic.com/"">DVCS</a>), then there's no feasible way to lock them all anytime you make a change. Besides that wouldn't really solve the problem unless you could also make sure no one else changed it before you could merge it everywhere.</p>",5,0,2011-03-29 05:10:36.293000 UTC,1.0,2011-08-06 03:01:31.267000 UTC,10,version-control|merge|dvcs|ssis,11825,2009-11-13 13:54:47.917000 UTC,2022-03-04 15:54:37.973000 UTC,"Muncie, IN",935,171,7,123,"<p>Using the free Visual Studio add-in <a href=""http://bidshelper.codeplex.com/"">BIDS Helper</a> may help with your dilemma in two possible ways.</p>

<ol>
<li><p><a href=""http://bidshelper.codeplex.com/wikipage?title=Biml%20Package%20Generator&amp;referringTitle=Documentation"">BIML</a>:  BIML is Business Intelligence Markup Language (<a href=""http://www.varigence.com/documentation/biml/"">BIML Reference</a>).  You can use .biml files to generate your SSIS packages.  BIML files should work better with merge operations because of their more rigid structure.  Although I have no experience with merging them yet, I've been using BIML files to create my SSIS packages faster than the SSIS UI allows.  It has been very helpful with copy-pasting similar data flows and changing just the unique attributes.</p></li>
<li><p><a href=""http://bidshelper.codeplex.com/wikipage?title=Smart%20Diff&amp;referringTitle=Documentation"">Smart Diff</a>: BIDS Helper also has a Smart Diff feature built in to help compare differences in your SSIS packages.  It will not help auto-merge, but it will strip out layout information and order the XML before showing the differences.  This will show you actual functional differences between two SSIS packages.  Then you can use that information to manually merge changes.  For your example from your comment on revelator's answer, you would use Smart Diff to compare version 1.0 of your SSIS to your fixed version in the 1.0 branch, then you would see just the changes necessary to apply that fix manually to your 2.0 branch.</p></li>
</ol>",1.0,2011-08-30 16:29:00.047000 UTC,,8.0,[]
Removing non-ascii and special character in pyspark dataframe column,"<p>I am reading data from csv files which has about 50 columns, few of the columns(4 to 5) contain text data with non-ASCII characters and special characters.</p>

<pre><code>df = spark.read.csv(path, header=True, schema=availSchema)
</code></pre>

<p>I am trying to remove all the non-Ascii and special characters and keep only English characters, and I tried to do it as below</p>

<pre><code>df = df['textcolumn'].str.encode('ascii', 'ignore').str.decode('ascii')
</code></pre>

<p>There are no spaces in my column name. I receive an error </p>

<pre><code>TypeError: 'Column' object is not callable
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;command-1486957561378215&gt; in &lt;module&gt;
----&gt; 1 InvFilteredDF = InvFilteredDF['SearchResultDescription'].str.encode('ascii', 'ignore').str.decode('ascii')

TypeError: 'Column' object is not callable
</code></pre>

<p>Is there an alternative to accomplish this, appreciate any help with this.</p>",2,3,2020-01-28 21:17:43.103000 UTC,2.0,2020-01-28 21:37:30.867000 UTC,4,python|pyspark|apache-spark-sql|pyspark-sql|azure-databricks,8809,2013-12-24 13:56:38.527000 UTC,2021-08-04 13:21:12.127000 UTC,,318,19,1,70,"<p>This should work.</p>

<p>First creating a temporary example dataframe:</p>

<pre><code>df = spark.createDataFrame([
    (0, ""This is Spark""),
    (1, ""I wish Java could use case classes""),
    (2, ""Data science is  cool""),
    (3, ""This is ï»¿aSA"")
], [""id"", ""words""])

df.show()
</code></pre>

<p><strong>Output</strong></p>

<pre><code>+---+--------------------+
| id|               words|
+---+--------------------+
|  0|       This is Spark|
|  1|I wish Java could...|
|  2|Data science is  ...|
|  3|      This is ï»¿aSA|
+---+--------------------+
</code></pre>

<p>Now to write a UDF because those functions that you use cannot be directly performed on a column type and you will get the <code>Column object not callable error</code></p>

<p><strong>Solution</strong></p>

<pre><code>from pyspark.sql.functions import udf

def ascii_ignore(x):
    return x.encode('ascii', 'ignore').decode('ascii')

ascii_udf = udf(ascii_ignore)

df.withColumn(""foo"", ascii_udf('words')).show()
</code></pre>

<p><strong>Output</strong></p>

<pre><code>+---+--------------------+--------------------+
| id|               words|                 foo|
+---+--------------------+--------------------+
|  0|       This is Spark|       This is Spark|
|  1|I wish Java could...|I wish Java could...|
|  2|Data science is  ...|Data science is  ...|
|  3|      This is ï»¿aSA|         This is aSA|
+---+--------------------+--------------------+
</code></pre>",0.0,2020-01-28 22:08:13.890000 UTC,,8.0,[]
How to resolve merging conflicts in Mercurial (v1.0.2)?,"<p>I have a merging conflict, using Mercurial 1.0.2:</p>

<pre><code>merging test.h
warning: conflicts during merge.
merging test.h failed!
6 files updated, 0 files merged, 0 files removed, 1 files unresolved
There are unresolved merges, you can redo the full merge using:
  hg update -C 19
  hg merge 18
</code></pre>

<p>I can't figure out how to resolve this. Google search results instruct to use:</p>

<pre><code>hg resolve
</code></pre>

<p>but for some reason my Mercurial (v1.0.2) doesn't have a resolve command:</p>

<pre><code>hg: unknown command 'resolve'
</code></pre>

<p>How can I resolve this conflict?</p>",3,1,2009-04-28 06:42:22.090000 UTC,9.0,2012-02-24 19:55:30.233000 UTC,36,mercurial|merge|dvcs|conflict,52135,2008-08-30 13:51:36.970000 UTC,2020-09-11 19:46:56.070000 UTC,United States,25033,93,12,520,"<p><strong>Valid for hg &lt; v1.1 only</strong></p>

<p>There is no need to call any <code>hg</code> commands. Unlike <code>svn</code>, Mercurial does not track conflicted files. If you call <code>hg status</code>, you'll see that the file is simply marked as modified.</p>

<p>Just fix the file by hand and commit.</p>",6.0,2009-04-28 06:44:17.263000 UTC,2017-11-03 20:05:20.620000 UTC,14.0,[]
Bitbucket: Update a fork to merge changes of master repo?,"<p>I am working with a bitbucket git repo I have read-only access to, so I created a fork to work on my features. </p>

<p>Question: How do I update my fork to include changes made to the original repo made by the owner? </p>

<p>On github, it seems one has to do the following, so I suspect it's a similar to this: </p>

<pre><code> $ git remote add upstream git://github.com/octocat/Spoon-Knife.git
 $ git fetch upstream
 $ git merge upstream/master
</code></pre>

<p>I couldn't find any information on this in the <a href=""http://confluence.atlassian.com/display/BITBUCKET/Forking+a+bitbucket+Repository"" rel=""noreferrer"">Bitbucket Documentation for forking</a></p>",5,0,2012-03-30 14:26:36.450000 UTC,20.0,2012-03-30 16:18:14.193000 UTC,49,git|fork|dvcs|bitbucket,23878,2011-06-08 11:26:39.147000 UTC,2016-05-08 21:54:55.450000 UTC,,1767,184,2,143,"<p>Just like GitHub, you have to pull the commits down to your own machine, merge, and then push them back to your fork on Bitbucket.</p>

<p>If you go to your fork on Bitbucket you can click ""compare fork"" to get to a page where you see incoming and outgoing commits. If you go to the ""incoming"" tab, you will see instructions like</p>

<pre><code>$ git remote add &lt;remote_name&gt; git@bitbucket.org:&lt;upstream&gt;/&lt;repo&gt;.git
$ git fetch &lt;remote_name&gt;
$ git checkout master
$ git merge &lt;remote_name&gt;/master
</code></pre>

<p>which correspond closely to the GitHub instructions.</p>",4.0,2012-03-30 16:18:03.757000 UTC,2018-02-28 06:49:22.613000 UTC,60.0,[]
"Local branches show-up on GitHub's ""Network"" view","<p>We are using Git and our workflow consists of a 'dev' and 'master' branch which lives on GitHub and each developer's local repository. No work is performed directly on 'master' or 'dev', but rather in local branches and only merges happen on 'dev' and later with 'master'. We do not push local branches to GitHub.</p>

<p>For some reason developers' local branches show up in the ""Network"" view on GitHub and this clutters up the network diagram (I should point out that the branch itself doesn't exist under the list of branches on GitHub).</p>

<p>My question is whether this is normal behavior and happens automatically as a means of showing where the changes to 'dev' and 'master' come from or is it because someone pushed a local branch by mistake and deleted it later? If it's the latter, is there a way to clean-up the clutter?</p>",2,0,2010-06-02 15:31:14.803000 UTC,2.0,2010-06-04 15:47:53.860000 UTC,2,git|version-control|workflow|github|dvcs,2580,2010-04-05 14:09:13.950000 UTC,2022-03-03 21:55:30.157000 UTC,"Washington D.C., DC, USA",1017,224,0,59,"<p>The artifacts you are seeing in the “network” view are probably traces of your merge-based workflow.</p>

<p>When a merge operation results in a merge commit<sup>*</sup> (i.e. it is not a “fast-forward”), the <a href=""http://en.wikipedia.org/wiki/Directed_acyclic_graph"" rel=""noreferrer"">DAG</a> model of the repository's history will include portions that represent both branches. When the non-local branch is pushed, its ancestry will include the commits that were made originally on the local branch.<br>
<sub><sup>*</sup>Either by using <code>git merge --no-ff</code> or because both branches had moved beyond their merge base.</sub></p>

<p>Consider a hypothetical series of events and the resulting history DAG+refs in the central repository:</p>

<pre><code>A$ git fetch &amp;&amp; git checkout -b foo central/dev
# A works and commits to her local branch
B$ git fetch &amp;&amp; git checkout -b bar central/dev
# A and B work and commit to their local branches
A$ git checkout dev &amp;&amp; git pull &amp;&amp;
   git merge --no-ff foo &amp;&amp; git push central dev
# B works and commits to his local branch
C$ git fetch &amp;&amp; git checkout -b quux central/dev
# B and C work and commit to their local branches
B$ git checkout dev &amp;&amp; git pull &amp;&amp;
   git merge --no-ff bar &amp;&amp; git push central dev
C$ git checkout dev &amp;&amp; git pull &amp;&amp;
   git merge --no-ff quux &amp;&amp; git push central dev
D$ git fetch &amp;&amp; 
   git checkout master &amp;&amp; git pull &amp;&amp;
   git merge --no-ff dev &amp;&amp; git push central master

---o---o-------------------------------D  master
        \                             /
         \             o---o---o     /      (was quux in C's local repository)
          \   o---o   /         \   /       (was foo in A's local repository)
           \ /     \ /           \ /
            o-------A---------B---C       dev
             \               /
              o---o----o----o               (was bar in B's local repository)
</code></pre>

<p>At no point were the local (<em>foo</em>, <em>bar</em>, <em>quux</em>) branches ever directly pushed to the central repository. However, “their” commits are referenced by the merge commits that are pushed to the <em>dev</em> branch (and later to the <em>master</em> branch) in the central repository.</p>

<p>I suspect that the GitHub network view is showing you these indirectly pushed branches.</p>

<p>If you want to eliminate such topological evidence of branches, you will need to move to workflow that is based on rebase operations instead of merge operations (this implies that the original “fork point” of the local branch will be discarded, which may or may not be important to your overall workflow).</p>

<p>Do not get bogged down trying to make the DAGs look “pretty”. The tools do not care if the DAGs are “ugly”, neither should you. You should concentrate on picking and properly using a branching workflow that produces a DAG that lets the tools do useful work for you.</p>",0.0,2010-06-03 07:56:49.437000 UTC,,9.0,[]
Describe your workflow of using version control (VCS or DVCS),"<p>I'd like to learn other people workflow when using either vcs or dvcs.  </p>

<p>Please describe your strategy to handle the following tasks:</p>

<ul>
<li>Implement a feature</li>
<li>Fixing bugs (during development and deployed app)</li>
<li>Code Review</li>
<li>Refactoring code (post code-review)</li>
<li>Incorporate patches</li>
<li>Releasing the newer version of your app (desktop, web, mobile, would you treat them differently?)</li>
</ul>

<p>Feel free to organize your answer not grouped by the tasks but grouped by whatever you think is relevant but please organize it by VCS/DVCS (please don't mix them). </p>

<p>Thank you.</p>",2,2,2010-04-24 15:31:34.457000 UTC,38.0,2010-05-26 11:10:29.507000 UTC,51,svn|git|mercurial|dvcs,6909,2009-10-16 18:29:17.830000 UTC,2019-06-19 00:16:57.903000 UTC,"Vancouver, Canada",992,16,0,142,"<p>The main feature all VCS use for the various task you are mentioning is <strong><a href=""https://stackoverflow.com/questions/2100829#2107672"">branching</a></strong>: the ability to isolate a development effort in a collaborative way. Since it is a Central VCS, several developers can collaborate on a same branch, with pessimistic or optimistic locks on files, in order to develop a parallel history.</p>

<p>But being a VCS has two major impact on branching:</p>

<ol>
<li>It tends to discourage commits, because once a file is committed, it will immediately influence the workspace of other views with the same configuration (i.e. ""working on the same branch"").<br>
~ The ""publication"" process is an active one, with immediate consequences,<br>
~ while the ""consuming"" part (updating your workspace) is a passive one (you are forced to deal with changes published by other immediately upon update of your workspace)   </li>
<li>It works well for <a href=""https://stackoverflow.com/questions/216212#216228""><em>linear</em> <strong>merge workflow</strong></a> (i.e. ""only merge from branch A to branch B, not mixing merges in both directions"" -- A to B to A to B...). The merges are trivial, all modifications from A are simply carried over to B</li>
</ol>

<p>Now:</p>

<h2>Implementing a feature</h2>

<p>Any VCS will do that by making a branch, but what greatly surprised me is that a ""feature"" branch is not easy:<br>
* the feature may grow too complicated<br>
* it may be ready in time for the next release<br>
* only some part of it may need to be merged back into the main development branch<br>
* it may depend on other features which are not fully done yet</p>

<p>So you need to be careful in the way you manage your feature branch, and your commits: if they are tightly related to the same feature, it will go well (you merge the whole thing back to your main development branch when you need it). Otherwise, partial merges are not easy with those tools.</p>

<h2>Fixing bugs</h2>

<p>The difference between bug fix during development and after release is that, in the former case you can often do it linearly in the same branch, as in the latter case you will have to establish a bug-fix branch, and decide what bugs you will need to back-port to your current development branch.</p>

<h2>Code Review</h2>

<p>It is best used with external tools (<a href=""http://www.atlassian.com/software/crucible/"" rel=""nofollow noreferrer"">like Crucible</a> for instance), and uses VCS functions like blame or annotations extensively, in order to better assign code fixes after a review.</p>

<h2>Refactoring code (post code-review)</h2>

<p>If the refactoring is minor, it can go on in the same branch. But if it is big, a special branch needs to be setup, with unit-testing done before beginning said refactoring.</p>

<h2>Incorporate patches</h2>

<p>Same comment as last point. If the patch is a big one, a branch needs to be created.</p>

<h2>Releasing the newer version of your app</h2>

<p>A VCS will only get you so far when it comes to releasing your app, because it is not a release management tool.<br>
You will need to formerly identify a version to be released (label), but after that comes the deployment process which involves:  </p>

<ul>
<li>stopping what is currently running</li>
<li>copying the new files</li>
<li>deploying them (updating sql database, webapp, ...)</li>
<li>instantiating all config files (with the right values, addresses, port number, paths, ...)</li>
<li>restarting (and if your system is composed of several components, restarting them in the right order!)</li>
</ul>

<p>The key things with VCS and release management are:</p>

<ul>
<li>they are not very well adapted to store binaries to be released, meaning you need them to build your app, not to store the resulting executable</li>
<li>they are not always welcome in the production environment (where security constraints limit writing access, as well as the number of tools running on those platforms, essentially monitoring and reporting tools)</li>
</ul>

<p>The release mechanism also has an influence on binary dependencies:</p>

<ul>
<li>for external binary dependencies, you will probably use mechanisms like maven to get fixed revisions of external libraries</li>
<li>but for internal dependencies, when you are not developing just one app but several which depends one upon an other, you need to know how to reference the binaries produced by the other apps (internal binary dependencies), and they usually won't be stored in your VCS (especially in the development phase, where you can produce <em>a lot</em> of different releases for your other apps to be able to use) </li>
</ul>

<p>You can also choose to be in source dependencies (and get all the sources of the other internal projects you need for your own), and a VCS is well adapted for that, but it is not always possible/practical to recompile everything.</p>",1.0,2010-04-24 16:23:41.103000 UTC,2017-05-23 12:25:02.757000 UTC,43.0,[]
How to set up Git bare HTTP-available repository on IIS,"<p>My server already runs IIS on TCP ports 80 and 443. I want to make a centralized ""push/pull"" Git repository available to all my team members over the Internet.</p>

<p>So I should use HTTP or HTTPS.</p>

<p>But I cannot use Apache because of IIS already hooking up listening sockets on ports 80 and 443! Is there any way to publish a Git repository over <em>IIS</em>? Does Git use WebDAV?</p>

<p><strong>Update.</strong> It seems that Git HTTP installation is read-only. That's sad. I intended to keep the stable branch on a build server and redeploy using a hook on push. Does anyone see a workaround besides using SVN for that branch?</p>",6,4,2008-09-09 11:11:45.300000 UTC,24.0,2019-07-25 13:05:05.380000 UTC,33,git|version-control|iis-7|dvcs,27109,2008-08-21 15:27:23.130000 UTC,2022-03-02 16:25:44.127000 UTC,"London, United Kingdom",20967,191,56,971,"<p><strong>Bonobo Git Server</strong></p>

<p><a href=""https://bonobogitserver.com/"" rel=""noreferrer"">https://bonobogitserver.com/</a></p>

<hr>

<p><strong>GitAspx</strong> - By Jeremy Skinner</p>

<p><a href=""https://github.com/JeremySkinner/git-dot-aspx/"" rel=""noreferrer"">https://github.com/JeremySkinner/git-dot-aspx/</a></p>

<p><a href=""https://github.com/JeremySkinner/git-dot-aspx/downloads"" rel=""noreferrer"">https://github.com/JeremySkinner/git-dot-aspx/downloads</a></p>

<p><em>Install Instructions</em></p>

<p><a href=""https://www.jeremyskinner.co.uk/2010/10/19/gitaspx-0-3-available/"" rel=""noreferrer"">https://www.jeremyskinner.co.uk/2010/10/19/gitaspx-0-3-available/</a></p>

<hr>

<p><strong>Git Web</strong></p>

<p><a href=""https://gitweb.codeplex.com/"" rel=""noreferrer"">https://gitweb.codeplex.com/</a></p>

<hr>

<p><strong>WebGitNET</strong></p>

<p><a href=""https://github.com/otac0n/WebGitNet"" rel=""noreferrer"">https://github.com/otac0n/WebGitNet</a></p>

<hr>

<p><strong><em>Alternatively ...</em></strong> (non-IIS, but highly recommend, free and open-source)</p>

<p><strong>Gitea</strong> (fork of Gogs): <a href=""https://gitea.io"" rel=""noreferrer"">https://gitea.io</a></p>

<p><strong>Gogs</strong>: <a href=""https://gogs.io"" rel=""noreferrer"">https://gogs.io</a></p>

<p><strong>SCM Manager</strong> allows you to easily set up revision control endpoints for <strong>Git</strong>, <strong>Hg</strong>, and <strong>SVN</strong> under the same hosting process. HTTP/HTTPS is supported along with built-in user authentication.</p>

<p><a href=""https://www.scm-manager.org"" rel=""noreferrer"">https://www.scm-manager.org</a><br>
<a href=""https://bitbucket.org/sdorra/scm-manager/"" rel=""noreferrer"">https://bitbucket.org/sdorra/scm-manager/</a></p>",0.0,2010-10-25 14:16:06.423000 UTC,2019-07-25 13:09:49.123000 UTC,31.0,[]
Mercurial Subrepositories: Prevent accidental recursive commits and pushes,"<p>I work on a team where we have a code in a mercurial repository with several subrepositories:</p>

<pre><code>main/
main/subrepo1/
main/subrepo1/subrepo2/
</code></pre>

<p>The default behavior of Mercurial is that when a <code>hg commit</code> is performed in ""main"", any outstanding changes in the subrepositories ""subrepo1"" and ""subrepo2"" will also be committed. Similarly, when ""main"" is pushed, any outgoing commits in ""subrepo1"" and ""subrepo2"" will also be pushed.</p>

<p>We find that people frequently inadvertently commit and push changes in their subrepositories (because they forgot they had made changes, and <code>hg status</code> by default does not show recursive changes). We also find that such global commits / pushes are almost always accidental in our team.</p>

<p>Mercurial 1.7 recently improved the situation with <code>hg status -S</code> and <code>hg outgoing -S</code>, which show changes in subrepositories; but still, this requires people to be paying attention.</p>

<p><strong>Is there a way in Mercurial to make <code>hg commit</code> and <code>hg push</code> abort if there are changes/commits in subrepostories that would otherwise be committed/pushed?</strong></p>",5,1,2011-01-31 22:31:37.580000 UTC,3.0,,17,version-control|mercurial|dvcs|mercurial-subrepos,4609,2011-01-11 06:39:09.570000 UTC,2022-03-02 22:43:17.920000 UTC,,5588,507,14,341,"<p>Mercurial 2.0 automatically prevents you from committing subrepositories unless you manually specify the <code>--subrepos</code> (or, alternatively, <code>-S</code>) argument to <code>commit</code>.</p>

<p>For example, you try to perform a commit while there are pending changes in a subrepository, you get the following message:</p>

<pre><code># hg commit -m 'change main repo'
abort: uncommitted changes in subrepo hello
(use --subrepos for recursive commit)
</code></pre>

<p>You can successfully perform the commit, however, by adding <code>--subrepos</code> to the command:</p>

<pre><code># hg commit --subrepos -m 'commit subrepos'
committing subrepository hello
</code></pre>

<p>Some things to still be careful about: If you have changed the <em>revision</em> a subrepository is currently at, but not the <em>contents</em> of the subrepository, Mercurial will happily commit the version change without the <code>--subrepos</code> flag. Further, recursive pushes are still performed without warning.</p>",0.0,2011-11-23 00:01:56.387000 UTC,,9.0,[]
"creating a new branch in mercurial: ""abort: push creates new remote head""","<p>I am trying to do something very simple: create a new branch. But I messed up. Where did I make the mistake, and how do I fix it?</p>

<p>I am the only user of Mercurial. I had revision 54 committed and pushed to remote repository. I wanted to create a branch based on revision 53, so I updated my local copy to revision 53, made changes, and committed (ignoring the warning about ""it's not the head""). Then when I am trying to push to remote repository, it says</p>

<pre><code>abort: push creates new remote head
</code></pre>

<p>Maybe I needed to tell Mercurial that I want to create a new branch? If so, how and at what point?</p>

<p>Thanks!</p>",3,1,2012-01-31 19:29:46.177000 UTC,6.0,2012-01-31 20:03:46.923000 UTC,28,mercurial|branch|dvcs,25804,2010-05-09 06:34:12.890000 UTC,2022-03-04 03:12:57.547000 UTC,"Mountain View, CA, USA",44119,1012,27,1930,"<p>You tell Mercurial that it can go ahead with</p>

<pre><code>$ hg push --force
</code></pre>

<p>You need to force it since multiple (unnamed) heads are normally discouraged. The problem with them is that people that clone the repository wont know which one to use. But since you're the only user you can just go ahead and push.</p>

<p>The alternative is to use a <a href=""http://mercurial.aragost.com/kick-start/en/tasks/"" rel=""nofollow noreferrer"">named branch</a> (with <code>hg branch</code>) and then you'll use</p>

<pre><code>$ hg push --new-branch
</code></pre>

<p>to allow the creation of a new branch on the remote. <a href=""https://www.mercurial-scm.org/wiki/NamedBranches"" rel=""nofollow noreferrer"">Named branches</a> have the advantage that they make it easy to distinguish the two branches. They have the disadvantage that they are permanent. Permanent means that you cannot remove the branch name from the changesets on the branch — the name is literally baked directly into the changeset.</p>

<p><a href=""https://www.mercurial-scm.org/wiki/Bookmarks"" rel=""nofollow noreferrer"">Bookmarks</a> provide a way to have non-permanent branch names, see <code>hg help bookmarks</code>.</p>",6.0,2012-01-31 19:37:43.693000 UTC,2018-02-22 10:23:57.040000 UTC,36.0,[]
".hgignore syntax for ignoring only files, not directories?","<p>I have a problem which I can't seem to understand. I'm using TortoiseHg (version 0.7.5) on Windows but on Linux I have the same problem. Here it is:</p>

<p>My <code>.hgignore</code> file:</p>

<pre><code>syntax: regexp
^[^\\/]+$
</code></pre>

<p>What I'm trying to achieve is to add to the ignore list the files which are in the root of the hg repository.</p>

<p>For example if I have like this:</p>

<pre><code>.hg
+mydir1
+mydir2
-myfile1
-myfile2
-anotherfile1
-anotherfile2 
.hgignore
</code></pre>

<p>I want myfile1(2) and anotherfile1(2) to be ignored (names are only for the purpose of this example - they don't have a simple rule that can be put in the hgignore file easily)</p>

<p>Is there something I'm missing because I'm pretty sure that regexp is good (I even tested it)? Ideas?</p>

<p>Is there a simpler way to achieve this? [to add to the ignore list files that are in the root of the mercurial repository]</p>",2,0,2009-06-26 11:30:40.550000 UTC,5.0,2012-02-24 19:55:13.870000 UTC,11,regex|mercurial|dvcs|hgignore,4927,2008-09-16 17:17:38.060000 UTC,2022-03-04 22:06:03.597000 UTC,"Bucharest, Romania",10045,495,19,928,"<p>I relayed this question in <code>#mercurial</code> on irc.freenode.net and the response was that you cannot distinguish between files and directories — the directory is matched without the slash that you're searching for in your regexp.</p>

<p>However, if you can assume that your directories will never contain a full-stop <code>.</code>, but your files will, then something like this seems to work:</p>

<pre>
^[^/]*\..*$
</pre>

<p>I tested it in a repository like this:</p>

<pre>
% hg status -ui
? a.txt
? bbb
? foo/x.txt
? foo/yyy
</pre>

<p>Adding the <code>.hgignore</code> file gives:</p>

<pre>
% hg status -ui
? bbb
? foo/x.txt
? foo/yyy
I .hgignore
I a.txt
</pre>

<p>which indicates that the <code>a.txt</code> file is correctly ignored in your root directory, but <code>x.txt</code> in the <code>foo</code> subdirectory is not. You can also see that a file named just <code>bbb</code> in the root directory is <em>not</em> ignored. But maybe you can add such files yourself to the <code>.hgignore</code> file.</p>

<p>If you happen to have a directory like <code>bar.baz</code> in your root directory, then this directory and all files within will be ignored. I hope this helps a bit.</p>",2.0,2009-06-30 11:47:50.100000 UTC,2011-12-15 10:24:21.637000 UTC,11.0,[]
Are DVCS like Git inappropriate for teams using continuous integration?,"<p>My team's development processes are based on <a href=""http://martinfowler.com/articles/continuousIntegration.html"" rel=""noreferrer"">continuous integration</a>. The only branches we create are maintenance branches when we release, but otherwise developers are expected to commit regularly (daily if not more often) to trunk, so that everyone's work is always integrated, continually tested, and all that good stuff.</p>

<p>My understanding of <a href=""http://en.wikipedia.org/wiki/Distributed_revision_control"" rel=""noreferrer"">DVCS</a> is that it is great for branching. I worked some years ago in a team where this would have been very useful, as every bit of development was done on a branch, and only merged when complete and tested. But this was a different philosophy from continuous integration.</p>

<p>But it seems to me that for a team that uses continous integration, the groovy features of DVCS tools like <a href=""http://git-scm.com/"" rel=""noreferrer"">Git</a> would not be particularly relevant, and might even hinder the continuous integration process if merging changes requires extra steps that may be forgotten.</p>

<p>I'm sure there are other benefits of a DVCS (e.g. committing is very fast because it is local, presumably merging with the main branch could happen in the background while the developer carries on working).</p>

<p>But for this question, I'm interested in how teams which use DVCS and continous integration reconcile the two seemingly conflicting philosophies. I'm mainly interested in hearing from people who are actually doing this.</p>",5,1,2009-08-04 16:10:30.013000 UTC,11.0,2009-08-04 17:10:23.747000 UTC,32,git|version-control|continuous-integration|dvcs,3800,2008-09-09 14:46:59.793000 UTC,2022-01-01 15:18:17.087000 UTC,"London, United Kingdom",4174,51,1,179,"<p>Actually DVCS made continuous integration much easier.</p>

<p>With central VCS, every developer has the rights to commit directly in trunk and therefore he can commit buggy code. CI will detect it after the fact. So it's possible to have broken trunk even with CI.</p>

<p>On the other hand, the basic operations in DVCS world are branching and merging. Because merging is explicit and a separate process vs. commit to the trunk, one can always check the result of a merge <strong>before</strong> it lands on the trunk. I don't have experience with Git, but developers of Bazaar VCS have used this technique successfully for at least 3.5 years with the help of PQM tool. </p>

<p>Basically PQM workflow looks as following: developer publishes his branch so it can be merged, then he sends a special e-mail to the PQM bot with merge instructions. When PQM receives a merge request, it makes a separate integration branch (copy of trunk), then merges the developer's branch and runs tests on the resulting code. If all tests are passed then the integration branch is pushed to trunk, otherwise the developer will receive an e-mail with the log of failing tests.</p>

<p>Running all tests for Bazaar project takes time, but tests are executed on demand on a separate server. Developers won't be blocked by merges and can continue to work on other tasks.</p>

<p>As result of PQM-based merge workflow the bzr trunk is never broken (at least as long as there are enough acceptance and regression tests).</p>",2.0,2009-08-05 10:23:49.970000 UTC,2012-07-30 16:29:33.017000 UTC,32.0,[]
Distributed Version  Control Systems and the Enterprise - a Good mix?,"<p>I can see why distributed source control systems (DVCS - like Mercurial) make sense for open source projects.</p>

<p>But do they make sense for an enterprise? (over a centralized Source Control System such as TFS)</p>

<p>What features of a DVCS make it better or worse suited for an enterprise with many developers? (over a centralized system)</p>",9,1,2011-04-15 23:18:16.463000 UTC,40.0,2012-11-27 22:30:50.447000 UTC,52,git|version-control|mercurial|tfs|dvcs,7689,2008-12-10 00:45:55.743000 UTC,2022-03-05 01:02:25.170000 UTC,"Denver, CO",8475,589,3,826,"<p>I have just introduced a DVCS (Git in this case) in a large banking company, where Perforce, SVN or ClearCase was the centralized VCS of choices:<br />
I already knew of the challenges (see my previous answer &quot;<a href=""https://stackoverflow.com/questions/3597747/can-we-finally-move-to-dvcs-in-corporate-software-is-svn-still-a-must-have-for/3597851#3597851"">Can we finally move to DVCS in Corporate Software? Is SVN still a 'must have' for development?</a>&quot;)</p>
<p>I have been challenged on three fronts:</p>
<ul>
<li><p><strong>centralization</strong>: while the decentralized model has its merits (and allows for private commits or working without the network while having access to the <em>full</em> history), there still needs to be a clear set of <em>centralized</em> repos, acting as the main reference for all developers.</p>
</li>
<li><p><strong>authentication</strong>: a DVCS allows you to &quot;sign-off&quot; (commit) your code as... pretty much anyone (author &quot;<code>foo</code>&quot;, email &quot;<code>foo@bar.com</code>&quot;).<br />
You can do a <code>git config user.name foo</code>, or <code>git config user.name whateverNameIFeelToHave</code>, and have all your commits with bogus names in it.<br />
That doesn't mix well with the unique centralized &quot;Active Directory&quot; user referential used by big enterprises.</p>
</li>
<li><p><strong>authorization</strong>: by default, you can clone, push from or pull to <em>any</em> repository, and modify <em>any</em> branch, or any directory.<br />
For sensitive projects, that can be a blocking issue (the banking world is usually very protective of some pricing or quants algorithms, which require strict read/write access for a very limited number of people)</p>
</li>
</ul>
<p>The answer (for a Git setup) was:</p>
<ul>
<li><strong>centralization</strong>: a unique server has been set up for any repository having to be accessible by <em>all</em> users.<br />
Backup has been taking care of (incremental every day, full every week).<br />
DRP (Disaster Recovery Plan) has been implemented, with a second server on another site, and with real-time data replication through <a href=""http://en.wikipedia.org/wiki/SRDF"" rel=""nofollow noreferrer"">SRDF</a>.<br />
This setup in itself is independent of the type of referential or tool you need (DVCS, or Nexus repo, or main Hudson scheduler, or...): any tool which can be critical for a release into production needs to be installed on servers with backup and DR.</li>
</ul>
<p>.</p>
<ul>
<li><strong>authentication</strong>: only two protocols allow users to access the main repos:
<ul>
<li>ssh based, with public/private key:
<ul>
<li>useful for users external to the organization (like off-shore development),</li>
<li>and useful for <em>generic</em> accounts that Active Directory manager don't want to create (because it would be an &quot;anonymous&quot; account): a real person has to be responsible for that generic account, and that would be the one owning the private key</li>
</ul>
</li>
<li>https-based, with an Apache authenticating the users through a LDAP setting: that way, an actual login must be provided for any git operation on those repos.<br />
Git offers it with its <a href=""http://progit.org/2010/03/04/smart-http.html"" rel=""nofollow noreferrer"">smart http protocol</a>, allowing not just <code>pull</code> (read) through http, but also <code>push</code> (write) through http.</li>
</ul>
</li>
</ul>
<p>The authentication part is also reinforced at the Git level by a <strong><code>post-receive</code></strong> hook which makes sure that <strong>at least one</strong> of the commits you are pushing to a repo has a &quot;committer name&quot; equals to the user name detected through shh or http protocol.<br />
In other words, you need to set up your <code>git config user.name</code> correctly, or any push you want to make to a central repo will be rejected.</p>
<p>.</p>
<ul>
<li><strong>authorization</strong>: both previous settings (ssh or https) are wired to call the same set of perl script, named <strong><a href=""https://github.com/sitaramc/gitolite"" rel=""nofollow noreferrer"">gitolite</a></strong>, with as parameters:
<ul>
<li>the actual username detected by those two protocols</li>
<li>the git command (clone, push or pull) that user wants to do</li>
</ul>
</li>
</ul>
<p>The <a href=""https://github.com/sitaramc/gitolite/blob/pu/doc/3-faq-tips-etc.mkd#_security_access_control_and_auditing"" rel=""nofollow noreferrer"">gitolite perl script will parse a simple text file</a> where the authorizations (read/write access for a all repository, or for branches within a given repository, or even for directories within a repository) have been set.<br />
If the access level required by the git command doesn't match the ACL defined in that file, the command is rejected.</p>
<hr />
<p>The above describes what I needed to implement for a Git setting, but more importantly, it lists the main issues that need to be addressed for a DVCS setting to make sense in a big corporation with a unique user base.</p>
<p>Then, and only then, a DVCS (Git, Mercurial, ...) can add values because of:</p>
<ul>
<li><p><strong>data exchange between multiple sites</strong>: while those users are all authenticated through the same Active Directory, they can be located across the world (the companies I have worked for have developments usually between teams across two or three countries). A DVCS is naturally made for exchanging efficiently data between those distributed teams.</p>
</li>
<li><p><strong>replication across environments</strong>: a setting taking care of authentication/authorization allows for cloning those repositories on other dedicated servers (for integration testing, UAT testing, pre-production, and pre-deployment purposes)</p>
</li>
<li><p><strong>process automation</strong>: the ease with which you can clone a repo can also be used locally on one user's workstation, for unit-testing purposes with the &quot;guarded commits&quot; techniques and other clever uses: see &quot;<a href=""https://stackoverflow.com/questions/3209208/what-is-the-cleverest-use-of-source-repository-that-you-have-ever-seen"">What is the cleverest use of source repository that you have ever seen?</a>&quot;.<br />
In short, you can push to a second local repo in charge of:</p>
<ul>
<li>various tasks (unit test or static analysis of the code)</li>
<li>pushing back to the main repo if those tasks are successful</li>
<li><em>while</em> you are still working in the first repo without having to wait for the result of those tasks.</li>
</ul>
</li>
</ul>
<p>.</p>
<ul>
<li><strong><a href=""https://stackoverflow.com/questions/3900015/distributed-version-control-killer-applications"">killer features</a></strong>: Any DVCS comes with those, the main one being <strong>merging</strong> (ever tried to do a complex merge workflow with SVN? Or <em>sloooowly</em> merge 6000 files with ClearCase?).<br />
That alone (merging) means you can really take advantage of <a href=""https://stackoverflow.com/questions/2100829/when-should-you-branch/2107672#2107672"">branching</a>, while being able at all time to merge back your code to another &quot;main&quot; line of development because you would do so:
<ul>
<li>first locally within your own repo, without disturbing anybody</li>
<li>then on the remote server, pushing the result of that merge on the central repo.</li>
</ul>
</li>
</ul>",1.0,2011-04-16 10:01:32.780000 UTC,2021-08-26 17:52:44.473000 UTC,95.0,[]
Introduction to Mercurial,"<p>I have just begun working on a project which uses Mercurial as a version control system, and I need some basic tips on how to use this. Please use this question to give some introductory tips on this technology.</p>

<ul>
<li><a href=""https://www.mercurial-scm.org"" rel=""nofollow noreferrer"">The official Mercurial site</a></li>
</ul>

<p>Especially, I am looking for tips on the best programs to use and the best techniques to use (branches, in and out-checking etc. I need to learn the best-practices!)</p>",7,0,2009-05-08 01:31:32.533000 UTC,9.0,2017-06-19 09:20:14.863000 UTC,13,version-control|mercurial|dvcs,2399,2008-08-16 14:03:34.777000 UTC,2019-05-27 10:39:55.153000 UTC,"Oslo, Norway",6027,426,6,431,"<p>I know you already have the Mercurial site but the resource most useful to me was the <a href=""https://www.mercurial-scm.org/wiki/MercurialBook"" rel=""nofollow noreferrer"">Mercurial book</a>. It's an excellent overview of the program and how to use it. </p>

<p>I found the best way to learn Mercurial was just to use it on a project. I imported into Mercurial a project I had exported from subversion and did some regular development with it. I made sure to clone the repository for different changesets so that I could get used to the merging and updating. I haven't learned all of the advanced uses but I'm now on a pretty firm footing with it and haven't switched back to Subversion yet. </p>

<p>A lot of projects have different techniques for commit workflow. Some have changes pushed from the developers, like centralized systems, and some will pull the changes from contributors (Linux, for example). It's hard to generalize too much without knowing the process for your project. </p>

<p>This is how I do my development:</p>

<ul>
<li>Centralized tree on a file share or http, called <code>project-trunk</code> or <code>project</code> that is the definitive project version</li>
<li>A clean tree on my system that I clone from the remote repository and use to push back to the repository. I then clone from this tree for my changes. I call this tree <code>project-local</code></li>
<li>Clone the <code>project-local</code> tree for each of my changes: eg. <code>project-addusers</code>, <code>project-141</code>, etc.</li>
<li>After I am finished with the commits to a tree, I then push the changes to the <code>project-local</code> repository </li>
<li>Finally, push the changes in the <code>project-local</code> to <code>project-trunk</code></li>
</ul>

<p>I have the clean <code>project-local</code> tree because then I can push all the changesets back to the trunk at one time, which is helpful if there is a group of related changes that need to push back together.</p>

<p>As for tools, it depends on your platform. I just use the vanilla command line tool. Coming from TortoiseSVN, it was a bit of a change to go to the command line. But I'm fine with it now. I tried using TortoiseHg but it didn't function well on my Windows 7 x64 virtual machine. I hear it's much better on the supported 32-bit platforms. </p>",2.0,2009-05-08 02:36:41.050000 UTC,2017-06-19 09:20:46.747000 UTC,14.0,[]
TFS with Mercurial source control,"<p>I must say, that I know almost nothing about TFS. But have worked with its source control system. For now our team uses Mercurial as source control system, but our managers want the all-in-one system to manage the projects. They want to use Team Foundation Server, but for us distributed model of working with code is better then TFS's source control.</p>

<p>Can we use mercurial as source control system and not loose other TFS benefits for project's management (like bug tracker/ project server)?</p>",1,0,2011-04-15 12:32:58.627000 UTC,4.0,2019-07-08 12:19:14.497000 UTC,11,version-control|tfs|mercurial|project-management|dvcs,1418,2008-12-19 07:15:40.167000 UTC,2022-03-04 06:40:20.473000 UTC,"Tbilisi, Georgia",8211,3242,4,924,"<p>You can absolutely use TFS without using the source control.  I tend to agree with you, TFS's source control would be a huge step backwards if you're coming from Mercurial.</p>

<p>A couple things you'll be giving up.</p>

<ul>
<li>Linking work items to revisions, and the relationship between builds, revisions, and work items.</li>
<li>Any reports that rely on source control metrics (code churn etc)</li>
<li>Some templates allow you to automatically mark work items as done when you associate a check-in with them, using HG you'll have to do this manually.  (not a bad thing, I strongly dislike this behavior)</li>
</ul>

<p>On the other hand, </p>

<ul>
<li>you can still link work items to changesets by adding an http link to the work item.   </li>
<li>You can still use Team build if you want, you'll just have to script the build to pull your code from Hg.</li>
</ul>

<p>I might be missing something, but those are the items off the top of my head.</p>

<p>If they absolutely force you to check into TFS, there are a few people that use Hg ""on top"" of TFS <a href=""https://stackoverflow.com/questions/2331636/real-word-use-of-mercurial-with-a-team-foundation-server"">here</a>, and <a href=""http://lostechies.com/erichexter/2010/06/23/using-mercurial-as-a-local-repository-for-team-foundation-server-start-front-n/"" rel=""nofollow noreferrer"">here</a></p>",1.0,2011-04-15 12:46:04.657000 UTC,2017-05-23 12:06:37.087000 UTC,12.0,[]
Why can't I push to a checked out branch of a non-bare repository?,"<p>I am confused regarding a scenario that I created. I created a repository on Github (Lets call it A) and pushed code to it. After that I cloned that repository to my local (Lets call it B) such that origin of my local was remote repo A.</p>

<p>Now I cloned from my local B to create another local instance C. Now I had remote origin of C as repo B and upstream of C was A.</p>

<pre><code>A → B → C
</code></pre>

<p>This is similar to forking but here I created clone on client side instead of server side. </p>

<p>Now if I tried to use push from C to its origin B:</p>

<pre><code>git push origin 
</code></pre>

<p>then I received an error stating that I cannot push to non-bare repositories. I understand that pushing to non-bare repositories can result in loss of commits in remote not present in local. </p>

<p>However is this case not similar to the one where i push my code from B to A ?</p>

<p><strong>I am confused if B to A is possible then why not C to B.</strong></p>

<p>For merging to A we can push to upstream as:</p>

<pre><code>git push upstream
</code></pre>",2,5,2016-09-21 12:30:57.670000 UTC,1.0,2019-02-13 17:00:57.870000 UTC,4,git|github|dvcs|git-bare|git-non-bare-repository,3781,2014-05-23 19:32:49.087000 UTC,2022-02-23 15:36:40.167000 UTC,,393,19,0,43,"<h2>Some basic stuff</h2>

<ul>
<li><p>Whenever you <code>git clone</code>, from a developer point of view, you'll want to work on the code that you just cloned. So GIT gives you a ""working tree"" to work upon.  It's called a tree because it resembles one when you consider all the commits and branches you made and put on a graph.</p></li>
<li><p>The cloned repository is called <strong>non-bare repository</strong>. To create a non-bare repository, you do a simple <code>git init</code> - which is what the original programmer did to start tracking the cloned code with GIT. You could also clone it into a <em>bare repository</em> but the details and usefulness of it should be an answer of its own in a proper question regarding it.</p></li>
<li><p>A <strong>bare repository</strong> does not contain a working tree. It is only meant to store your code - a server for code managed by GIT, if you prefer. To create a bare repository, you do a simple <code>git init --bare name_of_repository.git</code>. It will create a directory named name_of_repository.git containing all the needed files by GIT. The <em>git</em> extension is just a convention used; it isn't required and could be anything or nothing at all.</p></li>
<li><p>There is something like a <em>pointer</em> in GIT that is called <strong>HEAD</strong>. It points to the latest commit that is active in the branch you work on, be it a bare or a non-bare repository.</p></li>
<li><p><strong>Branches</strong> are like 'different copies' of the code you just pulled from a remote repository (that might have different modifications or not). It has whatever names the developer thought to be proper. They are useful because you can work on different functions or fix different problems without worrying about the current code being developed by you or others. Later, you can always merge everything into the main branch - usually <em>master</em> - and then delete those merged branches that are not necessary anymore.</p></li>
<li><p><strong><em>GIT tries</strong> its best <strong>to avoid problems</strong> between versions of files</em> of different locations or branches. So he won't allow you to <code>git push</code> in some cases it determines to be <em>chaotic</em> to say the least. GIT is never wrong because it asks you to check, change or force what you are doing. So any error won't be GIT's fault but yours only.</p></li>
</ul>

<h2>Understanding the situation</h2>

<p>Let's consider the following:</p>

<ul>
<li>The A repository is a bare repository. Both B and C repositories are non-bare repositories. This means A has no working directory and is used for storage only. B and C are used for work you need to do.</li>
<li>Generally speaking, you (usually) have branches. Normally a beginner will not create branches because he is learning and might not even know about branches yet - even though they are useful for many reasons. So he will almost always be on a 'master' branch - the default branch.</li>
</ul>

<p>That being said, let's say you modify some files in B. You do <code>git commit</code> as many times you want and even a <code>git push</code> at the end. Or you don't do anything at all. But you are on master branch.</p>

<p>Later on, you modify files in C. And you do commit and tries to push to A. Remember: you are on master branch of C. The <code>git push</code> works!</p>

<p>Then, you try pushing C to B as well. <em>It doesn't work.</em></p>

<p><strong>Result</strong>: GIT will <em>(not)</em> literally scream, warning about the fact you are trying to <em>defile</em> (update) the master branch of the non-bare repository B which its HEAD points to another commit! If he lets you do the push, you will mess the history tracked by GIT on repository B. It won't know what happened to B anymore! You might even overwrite modification on that branch with the same name residing on B! So no, you can't push to B from C if both are non-bare repositories!</p>

<p><em>What now?! Will my world end like this?! What could the great <strong>I</strong> have done?! How possibly could GIT ignore his master's wishes?! This is pure heresy!</em></p>

<h2>Solution</h2>

<p><strong>1</strong> - Have two branches on B - the master and a temporary one. And make the head points to the temporary branch. Example:</p>

<pre><code>cd B                  # change to B's working directory
git branch temp       # create 'temp' branch
git checkout temp     # change from master branch to branch temp
</code></pre>

<p><strong>2</strong> - Now, move to C working directory (<em>wd</em> for short) and pull with the contents of B. Note that I'm considering that B is a remote of C (as you have mentioned in your case):</p>

<pre><code>cd ../C               # change to C's working directory
git pull B master     # pulls B's modifications to C
</code></pre>

<p><strong>3</strong> - Modify your files in C. Note that you are on C's master branch. Then, after committing modifications of C, push it to B's master:</p>

<pre><code>git push B master     # pushes C's mods to B's master branch
</code></pre>

<p><strong>4</strong> - Now go back to B <em>wd</em> and make the HEAD point back to the master branch:</p>

<pre><code>cd ../B               # change to B's working directory
git checkout master   # change from temp branch to branch master
</code></pre>

<p><strong>5</strong> - You can delete the temporary branch if you won't be using it anymore:</p>

<pre><code>git branch -d temp    # delete branch temp
</code></pre>

<p><strong>6</strong> - If you are doing new modifications in C, you don't need to do both steps 4 and 5. If you do, any time you wish to do modifications in C, you will need to do both steps 1 and 2 beforehand.</p>

<p><strong>And this solves your issue!</strong> <em>Probably</em>...</p>

<h2>Clarifications &amp; Reinforcements</h2>

<ul>
<li><code>git branch name_of_the_branch</code> creates a new branch;</li>
<li><code>git checkout name_of_the_branch</code> makes the HEAD points to this new branch;</li>
<li><code>git checkout -b name_of_the_branch</code> creates a branch and makes the HEAD point to it in one single command. I used the longer method because you should know the longer method as well;</li>
<li>as said previously, don't delete the branch if you are gonna use it later on. But I do recommend doing so to avoid problems with <em>temporary branches</em> in both repositories while pulling/pushing or even merging. Create the temporary branch as needed basis - quite easy with terminal history - and delete it afterwards;</li>
</ul>",0.0,2016-11-19 08:21:41.613000 UTC,2019-08-23 10:55:54.127000 UTC,8.0,[]
"In Mercurial, is there any way (aside from ""Cherry picking"") to push a changeset without also pushing changesets associated with a different head?","<p>In the answer to <a href=""https://stackoverflow.com/questions/2768112/distributed-source-control-pushing-individual-changesets"">this question</a>, <a href=""https://stackoverflow.com/users/8992/ry4an"">Ry4an</a> states that ""you cannot push Changeset2 without pushing Changeset1"".</p>

<p>This certainly makes sense if the repository looks like this:</p>

<pre><code>+ Changeset2
|
+ Changeset1
|
+ Original
</code></pre>

<p>However it doesn't seem to make as much sense in the following scenario, which is what I currently have:</p>

<pre><code>+ Changeset2
|
|   + Changeset1
|  /
| /
+ Original
</code></pre>

<p>Ideally, I want to be able to push just Changeset2 back to the repository I initially cloned from. Mercurial doesn't seem willing to let me do that. It's insisting I push Changeset 1 also... which is not allowed as it would create a new head in the original repository. Obviously I could ""Cherry pick"", or create a patch to apply on the original repository but that seems clunky. Am I missing something?</p>

<p><strong>Update:</strong> I should probably have mentioned in my initial question that I was trying to perform the operation from the <a href=""http://tortoisehg.bitbucket.io/"" rel=""nofollow noreferrer"">TortoiseHg</a> GUI. As <a href=""https://stackoverflow.com/users/19465/niall-c"">Niall C</a>. correctly identified in his answer, the Mercurial command line allowed me to accomplish what I needed, however I would still be interested in learning if there is any way to accomplish the same operation from the GUI.</p>",2,2,2010-10-20 14:47:14.813000 UTC,2.0,2018-02-27 14:46:18.757000 UTC,11,version-control|mercurial|dvcs,578,2009-06-08 15:22:26.563000 UTC,2022-03-04 17:03:08.157000 UTC,"Palmetto, Florida, United States",4058,352,2,293,"<p>If you're using <code>hg push</code> without any command-line option, it will try to push every changeset in your local repository that doesn't exist in the remote repository.  If you use the <code>-r</code> / <code>--rev</code> option, it will just push that revision and its ancestors.  In your case, you would need to do:</p>

<pre><code>hg push --rev Changeset2
</code></pre>

<p>See <code>hg help push</code> for full details.</p>",2.0,2010-10-20 14:56:16.667000 UTC,,11.0,[]
Good Mercurial repository viewer for Mac,"<p>Is there a good, native Mac tool to view Mercurial repositories, similar to gitnub for Git?</p>",6,0,2008-08-27 19:00:34.610000 UTC,12.0,2012-01-31 20:08:50.433000 UTC,34,macos|version-control|mercurial|dvcs,11338,2008-08-26 14:30:11.040000 UTC,2021-09-29 12:12:55.953000 UTC,"Cambridge, MA",1342,63,1,116,"<p>I know it's pretty old question, however just for sake of completeness, I think it is still worth to mention here the newest kid on the block called <a href=""http://bitbucket.org/snej/murky/wiki/Home"" rel=""nofollow noreferrer"">Murky</a>. </p>",5.0,2009-05-26 10:15:53.910000 UTC,,16.0,[]
Multiple simultaneous version control systems?,"<p>I'm relatively new to version control, and so far only have experience working with Subversion using TortoiseSVN/VisualSVN. I've been reading about other types of VCS (git, mercurial, etc), and am considering trying them out - however, many of the arguments for or against a particular VCS seem like they largely come down to subjective preference, so I'll probably wind up giving each one a look.</p>

<p>In thinking about doing so, I was wondering if it was even theoretically possible to use multiple VCS on a single codebase. Which (if any) combinations of VCS might this be a possibility for? And if possible, how much of a logistical nightmare would it be trying to juggle the respective exclusion lists?</p>

<p>One possible argument for doing so could be backup redundancy. Several VCS providers (Beanstalk, Github, Bitbucket) offer one free repository, so you could have the same repo backed up for free in several different places.</p>",6,0,2009-08-21 21:13:53.723000 UTC,1.0,,7,svn|git|version-control|mercurial|dvcs,1919,2008-12-01 23:58:47.357000 UTC,2022-03-04 18:10:58.860000 UTC,,145,1081,0,25,"<p>Certainly, this is possible, even commonplace in some cases. For example, <code>git</code> and <code>svn</code> are a natural pair for this. The standard use case is an organization which must have a legacy central Subversion repository for various reasons, but in which the developers want to use Git for the productivity boost it provides.</p>

<p>Enter <a href=""http://git-scm.com/docs/git-svn"" rel=""noreferrer""><code>git-svn</code></a>. Now the developers push and pull from their own local Git repos to the central Subversion repository, but still get all the cool power of Git. This process is completely and totally transparent to the Subversion repository, which simply sees the changes that get pushed to it.</p>",0.0,2009-08-21 21:18:48.020000 UTC,,11.0,[]
Go back to old revision in Bazaar,"<p>I want to go back in my bazaar history (change working tree) to find the commit that introduced a certain bug.</p>

<p>I do not want to delete any commits, just change my working tree until I found the bug, and then I want to go back to the latest revision to work on.</p>

<p>What are the two commands for that (going back to an <strong>earlier commit</strong> and afterwards checking out the <strong>latest revision</strong> again)?</p>

<p>Thanks in advance.</p>",5,1,2012-03-13 13:11:23.697000 UTC,1.0,2018-01-27 12:32:38.853000 UTC,32,version-control|dvcs|bazaar|vcs-checkout,11070,2011-02-26 13:14:40.853000 UTC,2022-03-05 21:13:39.650000 UTC,"Berlin, Germany",3582,2173,9,171,"<p>To revert the working tree back to a specific revision N:</p>

<pre><code>bzr revert -rN
</code></pre>

<p>To revert the working tree to the latest revision in the branch:</p>

<pre><code>bzr revert
</code></pre>",1.0,2012-03-13 20:33:02.903000 UTC,,23.0,[]
Retrieve old version of a file without changing working copy parent,"<p>How do you get a copy of an earlier revision of a file in Mercurial without making that the new default working copy of the file in your workspace?</p>

<p>I've found the <code>hg revert</code> command and I think it does what I want but I'm not sure.</p>

<p>I need to get a copy of an earlier revision of my code to work with for a few minutes. But I don't want to disturb the current version which is working fine.</p>

<p>So I was going to do this:</p>

<pre><code>hg revert -r 10 myfile.pls
</code></pre>

<p>Is there a way to output it to a different directory so my current working version of the file is not disturbed? Something like:</p>

<pre><code>hg revert -r 10 myfile.pls &gt; c:\temp\dump\myfile_revision10.pls
</code></pre>",2,0,2010-08-30 00:44:06.367000 UTC,5.0,2012-03-30 12:23:52.923000 UTC,34,version-control|mercurial|dvcs|revert,11560,2010-01-17 00:48:39.840000 UTC,2010-08-30 14:44:39.230000 UTC,,401,6,0,15,"<p>The <a href=""http://www.selenic.com/mercurial/hg.1.html#cat"" rel=""noreferrer"">cat command</a> can be used to retrieve any revision of a file:</p>

<pre><code>$ hg cat -r 10 myfile.pls
</code></pre>

<p>You can redirect the output to another file with</p>

<pre><code>$ hg cat -r 10 myfile.pls &gt; old.pls
</code></pre>

<p>or by using the <code>--output</code> flag. If you need to do this for several files, then take a look at the <a href=""http://www.selenic.com/mercurial/hg.1.html#archive"" rel=""noreferrer"">archive command</a>, which can do this for an entire project, e.g.,</p>

<pre><code>$ hg archive -r 10 ../revision-10
</code></pre>

<p>This creates the folder <code>revision-10</code> which contains a snapshot of your repository as it looked in revision 10.</p>

<p>However, most of the time you should just use the <a href=""http://www.selenic.com/mercurial/hg.1.html#update"" rel=""noreferrer"">update command</a> to checkout an earlier revision. Update is the command you use to bring the working copy up to date after pulling in new changes, but the command can also be used to make your working copy <em>outdated</em> if needed. So</p>

<pre><code>$ hg update -r 10   # go back
(look at your files, test, etc...)
$ hg update         # go back to the tip
</code></pre>",1.0,2010-08-30 06:03:48.240000 UTC,2010-08-31 21:48:44.157000 UTC,52.0,[]
How would you use a DVCS (mercurial in my case) to develop for different versions of the .NET framework?,"<p>I'm writing some sort of new adminstration dashboard for our cms (which runs on asp.net webforms). Some of our older servers can only handle .NET 2.0 for various reasons so I'd have to rewrite my code which uses lambda expressions etc. for that.</p>

<p>I wonder how you would use a dvcs like mercurial to develop those two versions concurrently.</p>

<p>My current codebase and mercurial repository is targeted at .NET 3.5. I'm relatively new to mercurial and I guess I'd have to branch the codebase?</p>

<p>Any best practices or tutorials?</p>",1,0,2009-05-28 07:52:19.460000 UTC,3.0,,8,.net|asp.net|version-control|mercurial|dvcs,348,2008-09-16 19:14:21.870000 UTC,2018-03-27 12:28:00.087000 UTC,Switzerland,11218,119,4,577,"<p>Yes, you can use Mercurial for this. Here is how it would work.</p>

<p>Let's say that your current clone is called <code>new-dot-net</code> since it
support the new .Net version. You make a clone of it and call it
<code>old-dot-net</code> or something like that. The two clones are now identical
and both target .Net 3.5.</p>

<p>Now carefully make small changes in <code>old-dot-net</code> in order to make it
.Net 2.0 compatible. When you make the changes the two clones will
start to diverge:</p>

<pre>
new-dot-net: ... [a] --- [b]

old-dot-net: ... [a] --- [b] --- [c] --- [d]
</pre>

<p>Here you made <code>[c]</code> and <code>[d]</code> changesets to add the .Net 2.0
compatibility. Notice how the <code>old-dot-net</code> clone contains more
changesets than <code>new-dot-net</code> since it has the backwards compatibility
changes that you <em>dont</em> want to see in <code>new-dot-net</code>. As you continue
working, it is important to think of this: <code>net-dot-net</code> will contain
a subset of the changesets in <code>old-dot-net</code>. The changes flow from
<code>new-dot-net</code> to <code>old-dot-net</code>, but <em>never</em> in the opposite direction.</p>

<p>Let's say you make a new change in <code>new-dot-net</code>. You make the change
in <code>new-dot-net</code> and the situation now looks like this:</p>

<pre>
new-dot-net: ... [a] --- [b] --- [x]

old-dot-net: ... [a] --- [b] --- [c] --- [d]
</pre>

<p>You now want to back-port the change to <code>old-dot-net</code> as well, you
change to <code>old-dot-net</code> and pull from <code>net-dot-net</code>:</p>

<pre><code>% cd old-dot-net
% hg pull ../new-dot-net
</code></pre>

<p>This will create a <a href=""https://www.mercurial-scm.org/wiki/Head"" rel=""nofollow noreferrer"">new head</a> in <code>old-dot-net</code>:</p>

<pre>
                             [x]
                            /
old-dot-net: ... [a] --- [b] --- [c] --- [d]
</pre>

<p>since the <code>[x]</code> changeset has <code>[b]</code> as it's parent changeset. You now
have <a href=""https://www.mercurial-scm.org/wiki/MultipleHeads"" rel=""nofollow noreferrer"">multiple heads</a> and have to merge to reduce the number of
heads. By merging you create a new changeset which is your way of
saying ""this is how <code>[x]</code> and <code>[d]</code> should be combined"". If the <code>[x]</code>
changeset only touches code which is not also touched in <code>[c]</code> and
<code>[d]</code>, then the merge should just work. Otherwise you'll be presented
with a merge tool and have to resolve the conflict. You commit the
merge as chageset <code>[e]</code>:</p>

<pre>
                             [x] --------------.
                            /                   \
old-dot-net: ... [a] --- [b] --- [c] --- [d] --- [e]
</pre>

<p>And you're done -- you have now incorporated the <code>[x]</code> change into
your .Net 2.0 compatible code.</p>

<p>You repeat this every time there has been a change in <code>new-dot-net</code>.
Let's say that more features are added:</p>

<pre>
new-dot-net: ... [a] --- [b] --- [x] --- [y] --- [z]
</pre>

<p>After pulling them into <code>old-dot-net</code> you get</p>

<pre>
                             [x] --------------.---- [y] --- [z]
                            /                   \
old-dot-net: ... [a] --- [b] --- [c] --- [d] --- [e]
</pre>

<p>And you now merge <code>[e]</code> and <code>[z]</code>:</p>

<pre>
                             [x] --------------.---- [y] --- [z]
                            /                   \               \
old-dot-net: ... [a] --- [b] --- [c] --- [d] --- [e] ----------- [f]
</pre>

<p>The important parts to remember are these:</p>

<ul>
<li>make any <strong>new features</strong> in <code>new-dot-net</code>.</li>
<li><strong>pull</strong> changes into <code>old-dot-net</code></li>
<li><strong>never push</strong> from <code>old-dot-net</code> to <code>new-dot-net</code>.</li>
</ul>

<p>Should you at some point find that a change in <code>new-dot-net</code> is not
needed in <code>old-dot-net</code>, then you still need to pull it in and merge
it. But you will then do a <em>dummy merge</em>. If the heads are <code>[w]</code> and
<code>[g]</code>, and you want keep <code>[g]</code>, then do</p>

<pre><code>% HGMERGE=true hg merge -y
% hg revert --all --rev g
% hg commit -m 'Dummy merge with y.'
</code></pre>

<p>The <a href=""https://www.mercurial-scm.org/wiki/TipsAndTricks#Keep_.22My.22_or_.22Their.22_files_when_doing_a_merge"" rel=""nofollow noreferrer"">trick</a> is to do the merge without caring about the results,
then revert all changes, and commit the unchanged working copy as the
merge. That way you tell the world that ""the combination of <code>[w]</code> and
<code>[g]</code> is <code>[g]</code>"", i.e., you throw away the changes in <code>[w]</code>. New
changes made in <code>new-dot-net</code> after <code>[w]</code> can then be merged like
normal.</p>",3.0,2009-05-28 08:52:14.723000 UTC,2018-02-22 12:32:58.480000 UTC,13.0,[]
Is there implementation of Git in pure Python?,<p>Is there implementation of Git in pure Python?</p>,3,4,2011-03-18 10:13:15.780000 UTC,3.0,,15,python|git|dvcs,4552,2009-04-24 20:59:35.603000 UTC,2022-03-05 21:38:09.447000 UTC,Poland,39441,5040,374,9472,"<p>Found <a href=""http://samba.org/~jelmer/dulwich/"" rel=""noreferrer"">Dulwich</a>:</p>

<blockquote>
  <p>Dulwich is a pure-Python
  implementation of the Git file formats
  and protocols.</p>
  
  <p>The project is named after the village
  in which Mr. and Mrs. Git live in the
  Monty Python sketch.</p>
</blockquote>

<p>Looks like a low-level library, the API did not appear friendly to my eyes, but there's a tutorial on the <a href=""https://github.com/jelmer/dulwich/blob/master/docs/tutorial/introduction.txt"" rel=""noreferrer"">Github page</a></p>",1.0,2011-03-18 10:25:25.517000 UTC,,16.0,[]
How do I determine the parent repository of a git repository?,<p>When I do a <CODE>git pull</CODE> my repository automatically pulls from the original git repo that I cloned from.  How can I get the URL of that git repository from the child repository?</p>,2,0,2009-10-25 22:51:31.743000 UTC,0.0,2018-04-30 15:36:32.290000 UTC,7,git|dvcs,5243,2008-09-22 20:05:38.207000 UTC,2022-03-06 02:01:22.173000 UTC,"Yakima, WA",22254,4035,49,2261,"<p>Also, <code>git remote -v</code> will show the urls of all your remotes.</p>",2.0,2009-10-26 14:25:54.487000 UTC,,10.0,[]
How can I see incoming commits in git?,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://stackoverflow.com/questions/231211/using-git-how-do-i-find-modified-files-between-local-and-remote"">Using Git how do I find modified files between local and remote</a>  </p>
</blockquote>



<p>How can I see incoming commits in git?  Or even better, see what I just <code>git fetch</code>/<code>git pull</code>ed?</p>

<p><strong>Edit:</strong>  To clarify the question: someone tells me that, to get some fixes, I should pull from their repository.  My goal is to see what their changes are <em>before</em> I accept them.  <code>git pull</code> automatically merges, which is not what I want.  <code>git fetch</code> will grab them without merging, but I'm unsure how to view what exactly I just pulled in.  The reason for the original phrasing is that I normally use Mercurial, where the command would be <code>hg incoming &lt;repo name here&gt;</code>&mdash;a command for which git seems to lack an analog.</p>",5,0,2009-08-25 22:31:30.843000 UTC,75.0,2017-05-23 12:18:29.303000 UTC,175,git|dvcs,74385,2008-08-21 19:55:59.300000 UTC,2021-07-13 04:48:51.717000 UTC,"Raleigh, NC, United States",26282,760,29,1865,"<p><code>incoming</code> isn't quite a direct mapping in git because you can (and I often do) have multiple repos you're pulling from, and each repo has multiple branches.</p>

<p>If there were an equivalent of hg's incoming command, it'd probably be this:</p>

<pre><code>git fetch &amp;&amp; git log ..origin/master
</code></pre>

<p>That is, ""go grab all of the stuff from the upstream, and then compare my current branch against the upstream master branch.""</p>

<p>Similarly, outgoing would be this:</p>

<pre><code>git fetch &amp;&amp; git log origin/master..
</code></pre>

<p>In practice, I just type those manually (even though I created an alias for one of them) because it's easy to have lots of local branches tracking and being tracked by lots of remote branches and have no trouble keeping it together.</p>",8.0,2009-08-26 00:30:09.483000 UTC,,205.0,[]
How do I manage large art assets appropriately in DVCS?,"<p>Is there any <em>good</em> way to handle large assets (i.e. 1000's of images, flash movies etc.) with a DVCS tool such as <a href=""https://www.mercurial-scm.org/wiki/"" rel=""noreferrer"">hg</a> and <a href=""http://git-scm.com/"" rel=""noreferrer"">git</a>. As I see it, to clone repositories that are filled with 4 GB assets seems like an unnecessary overhead as you will be checking out the files. It seems rather cumbersome if you have source code mixed together with asset files.</p>

<p>Does anyone have any thoughts or experience in doing this in a web development context?</p>",5,1,2009-08-16 16:20:53.537000 UTC,33.0,2018-02-22 10:11:02.163000 UTC,50,version-control|dvcs|asset-management,22916,2008-08-30 09:06:27.857000 UTC,2022-01-10 10:24:26.840000 UTC,"Lund, Sweden",115720,2486,36,4308,"<p>These are some thoughts I've had on this matter of subject. In the end you may need to keep assets and code as separate as possible. I can think of several possible strategies:</p>

<h2>Distributed, Two Repositories</h2>

<p>Assets in one repo and code in the other.</p>

<h3>Advantages</h3>

<ul>
<li>In web development context you won't need to clone the giant assets repository if you're not working directly with the graphic files. This is possible if you have a web server that handles assets separate from dynamic content (PHP, ASP.NET, RoR, etc.) and is syncing with the asset repo.</li>
</ul>

<h3>Disadvantages</h3>

<ul>
<li><p>DVCS tools don't keep track of other repositories than their own so there isn't any direct BOM (Bill of Materials) support, i.e. there is no clear cut way to tell when both repositories are in sync. (I guess this is what <a href=""http://git-scm.com/docs/git-submodule"" rel=""noreferrer"">git-submodule</a> or <a href=""http://source.android.com/download/using-repo"" rel=""noreferrer"">repo</a> is for).</p>

<p><em>Example:</em> artist adds a new picture in one repository and programmer adds function to use the picture, however when someone has to backtrack versions they are forced to somehow keep track of these changes on their own.</p></li>
<li><p>Asset repository overhead even though it only affects those who do use it.</p></li>
</ul>

<h2>Distributed, One Repository</h2>

<p>Assets and code reside in the same repository but they are in two separate directories.</p>

<h3>Advantages</h3>

<ul>
<li>Versioning of code and assets are interwoven so BOM is practical. Backtracking is possible without much trouble.</li>
</ul>

<h3>Disadvantages</h3>

<ul>
<li>Since distributed version control tools keep track of the whole project structure there is usually no way to just check out one directory. </li>
<li>You still have the problem with repository overhead. Even more so, you need to check out the assets as well as the code.</li>
</ul>

<p>Both strategies listed above still have the disadvantage of having a large overhead since you need to clone the large asset repository. One solution to this problem is a variant of the first strategy above, two repositories; keep the code in the distributed VCS repo and the assets in a centralized VCS repo (such as SVN, Alienbrain, etc). </p>

<p>Considering how most graphic designers work with binary files there is usually no need to branch unless it is really necessary (new features requiring lots of assets that isn't needed until much later). The disadvantage is that you will need to find a way to back up the central repository. Hence a third strategy:</p>

<h2>Off-Repository Assets (or Assets in CMS instead)</h2>

<p>Code in repository as usual and assets are not in repository. Assets should be put in some kind of content/media/asset management system instead or at least is on a folder that is regularly backed up. This assumes that there is very little need to back-track versions with graphics. If there is a need for back-tracking then graphic changes are negligible.</p>

<h3>Advantages</h3>

<ul>
<li>Does not bloat the code repository (helpful for e.g. git as it frequently does file checking)</li>
<li>Enables flexible handling of assets such as deployment of assets to servers dedicated for just assets</li>
<li>If on CMS with a API, assets should be relatively easy to handle in code</li>
</ul>

<h3>Disadvantages</h3>

<ul>
<li>No BOM support</li>
<li>No easy extensive version back-tracking support, this depends on the backup strategy for your assets</li>
</ul>",1.0,2009-08-16 18:28:29.563000 UTC,2015-07-03 10:26:35.680000 UTC,39.0,[]
Gremlin Coalesce To Add Multiple Vertices and Edges,"<p>Right now I am able to generate a query to create as many vertices and edges as I want.</p>
<p>e.g.</p>
<pre><code>g.V().
addV('vert1').as('a').
addV('vert2').as('b').
addE('has').from('a').to('b')
</code></pre>
<p>^^^^^^^^^^^^^ This works. Easy enough right? Now lets create a gremlin query that only creates these vertices if their label is unique. Then create an edge between the two.</p>
<pre><code>g.V().has(label,'vert1').fold().
    coalesce(
        unfold(),
        addV('vert1')
    ).as('a').
    V().has(label,'vert2').fold().
    coalesce(
        unfold(),
        addV('vert2')
    ).as('b').
    addE('has').from('a').to('b')
</code></pre>
<p>^^^^^^^^^^^^^This does not work</p>
<p>hopefully you can understand what I am trying to do though. Can anyone help me?</p>
<p>Thanks</p>",1,8,2018-08-07 16:02:32.170000 UTC,2.0,2021-07-09 09:48:56.313000 UTC,5,gremlin|tinkerpop3|amazon-neptune,4335,2015-04-16 19:03:51.700000 UTC,2022-02-08 18:17:58.507000 UTC,Cincinnati,85,2,0,20,"<p>You have a <code>fold()</code> which is a <code>ReducingBarrierStep</code> that follows after your step label at <code>as('a')</code> and the path history to ""a"" is lost after that step. You can read more about this aspect of Gremlin <a href=""http://kelvinlawrence.net/book/Gremlin-Graph-Guide.html#rbarriers"" rel=""noreferrer"">here</a>.</p>

<p>You just need to re-write your query to account for that - one way might be to just <code>aggregate()</code> the value of ""a"" rather than simply naming the step ""a"":</p>

<pre><code>gremlin&gt; g = TinkerGraph.open().traversal()
==&gt;graphtraversalsource[tinkergraph[vertices:0 edges:0], standard]
gremlin&gt; g.V().
......1&gt;   has(label,'vert1').fold().
......2&gt;   coalesce(unfold(),
......3&gt;            addV('vert1')).aggregate('a').
......4&gt;   V().has(label,'vert2').fold().
......5&gt;   coalesce(unfold(),
......6&gt;            addV('vert2')).as('b').
......7&gt;   select('a').unfold().
......8&gt;   addE('has').to('b')
==&gt;e[2][0-has-&gt;1]
</code></pre>

<p>If you need to return all the elements, just <code>project()</code> the returned edge and transform the results as necessary:</p>

<pre><code>gremlin&gt; g.V().
......1&gt;   has(label,'vert1').fold().
......2&gt;   coalesce(unfold(),
......3&gt;            addV('vert1')).aggregate('a').
......4&gt;   V().has(label,'vert2').fold().
......5&gt;   coalesce(unfold(),
......6&gt;            addV('vert2')).as('b').
......7&gt;   select('a').unfold().
......8&gt;   addE('has').to('b').
......9&gt;   project('e','in','out').
.....10&gt;     by().
.....11&gt;     by(inV()).
.....12&gt;     by(outV())
==&gt;[e:e[2][0-has-&gt;1],in:v[1],out:v[0]]
</code></pre>

<p>Of course, using a <code>select()</code> at the end might not be so bad either:</p>

<pre><code>gremlin&gt; g = TinkerGraph.open().traversal()
==&gt;graphtraversalsource[tinkergraph[vertices:0 edges:0], standard]
gremlin&gt; g.V().
......1&gt;   has(label,'vert1').fold().
......2&gt;   coalesce(unfold(),
......3&gt;            addV('vert1')).aggregate('a').
......4&gt;   V().has(label,'vert2').fold().
......5&gt;   coalesce(unfold(),
......6&gt;            addV('vert2')).as('b').
......7&gt;   select('a').unfold().
......8&gt;   addE('has').to('b').as('x').
......9&gt;   select('a','b','x')
==&gt;[a:[v[0]],b:v[1],x:e[2][0-has-&gt;1]]
</code></pre>",5.0,2018-08-07 16:21:29.287000 UTC,2018-08-08 13:55:47.747000 UTC,10.0,[]
Moving from SVN to ...?,"<p>I'm currently working in a team where we're ""using"" a subversion repository. I say ""using"", because in reality, everyone's just editing files directly on a server through samba shares, while every once in a while our architect does a commit from that server with our changes, which are then pushed out to servers.</p>

<p>So basically we're missing out on being able to have meaningful commit messages from different users, and being able to commit as often as we like.</p>

<p>I've been trying to spark some interest about distributed systems, and how the workflow that we have seems like it could be set up very nicely with something like git (us committing on our local machines and then pushing changesets to him for review) but i don't feel that i have enough experience with git. Most of my DVCS experience has been with mercurial.</p>

<p>Everyone is pretty much working in a windows environment using tortoisesvn, and that's the way they're used to interacting with the system, but they occasionally use PuTTY to work on one of the linux servers, and know how to do a commandline commit.</p>

<p>What is the way to go with this, i've seen some of the work being done to create gateways between SVN and some DVCS'es, does anyone have any experience setting up and working in such an environment?</p>

<p>How about full-scale migrations from SVN to a DVCS?</p>",9,2,2009-05-01 23:10:59.753000 UTC,,2009-09-03 16:11:28.613000 UTC,4,svn|git|version-control|mercurial|dvcs,772,2009-05-01 21:01:07.117000 UTC,2020-08-18 12:31:17.497000 UTC,"London, UK",8347,152,6,792,"<p>If your team can't understand how to use subversion well, I don't know how you will be able to get them to understand git.  Especially since they are in the mindset of ""lets all work in the same working copy,"" they are going to have a hard time grasping a distributed version control system.</p>

<p>In my experience, to use svn-git, you have to know how to use git AND you have to know how to use svn.  I'd recommend teaching them to use svn correctly.</p>",0.0,2009-05-02 01:43:50.463000 UTC,2009-05-02 18:09:13.013000 UTC,20.0,[]
How can I always know about all tags in Mercurial?,"<p>I don't use Mercurial, but i'd like to start, so i'm reading about it. The only SCM system i've used extensively is CVS. Most of what i've read about Mercurial makes sense, and sounds good. But i'm alternately shocked and perplexed by that way it does tags.</p>

<p>A tag is just a nickname for a changeset (and by 'changeset' we really mean the state resulting from the changeset). Cool. The mapping from tags to changeset IDs is stored in the <code>.hgtags</code> file. Also cool. The <code>.hgtags</code> file is versioned.</p>

<p><em>What?</em></p>

<p>This has lots of counterintuitive consequences. For instance, if i commit a changeset which i then want to tag (say, the code which will form release 1.0), i have to commit again after tagging, to put the updated tag file into the repository. And if i then update to that tagged changeset at some later date, the working copy won't contain any knowledge of that tag. If i do some work, thus founding a new branch (say, for bugfixes, heading towards 1.1), that branch won't have any knowledge of the tag from which it grew. Unless i copy it over manually, that is.</p>

<p>And as development continues both on the original trunk and my new branch, with tags being created to mark important changesets (the 2.0 release on the trunk, the 1.1 and 1.2 bugfix releases on the branch), both branches will proceed in ignorance of the other branch's tags. So, if i finish working on one branch, and want to switch to some particular changeset on another (say, i finish the 1.2 bugfix release, but now have to start on the 2.1 bugfix, based on 2.0), i am now stuffed. My current changeset doesn't know about 2.0!</p>

<p>What can i do?</p>

<ul>
<li>I can ask someone who's working on the 2.x branch to read out the actual changeset ID for 2.0, and use that explicitly, but this is appalling.</li>
<li>I could name my branches, as well as using tags, so that i could hop across to the head of the 2.x branch, thus learning about the new tags, and then skip back to the 2.0 tag. Assuming that branches, unlike tags, are universally visible - is that the case? Even if it is, this seems clunky.</li>
<li>I could maintain a single global <code>hgtags</code> file outside the repository, and use a couple of hooks to pull in a copy on an update, overwriting the local copy, and to copy back any changes on a commit. I'm not sure how this would work in a multi-user environment, where developers are pushing changes to a shared repository; i might need a separate repository just for the <code>hgtags</code> file.</li>
<li>I could use local tags, which stand outside the versioning mechanism, and so avoid the whole problem. As with shared global tags, i would have to put in place a mechanism to synchronise the <code>localtags</code> file between developers.</li>
</ul>

<p>None of these solutions seem totally great. What should i do?</p>

<p>An assumption here is that i'm managing branches using named branches in a single repository, rather than repository-per-branch. Would the situation be any better if i did the latter?</p>",2,1,2009-06-06 22:27:13.100000 UTC,7.0,2012-01-31 22:15:15.020000 UTC,16,mercurial|tags|dvcs,3848,2009-06-03 14:37:48.810000 UTC,2022-03-03 19:10:42.270000 UTC,"London, United Kingdom",44401,1596,108,5862,"<p>Versioning the <code>.hgtags</code> file allows you to</p>

<ul>
<li>edit tags and see who edited them (and why if they left a proper commit message)</li>
<li>transfer tags between repositories using the normal mechanisms</li>
</ul>

<p>However, there are some confusion going on here.</p>

<ul>
<li><p>You write that</p>

<blockquote>
  <p>[...] And if i then update to that tagged changeset at some later date, the working copy won't contain any knowledge of that tag. [...]</p>
</blockquote>

<p>That is wrong, tags are collected from the <code>.hgtags</code> files found in all <strong>heads</strong>. That means that you can update to an old tag (<code>hg update 0.1</code>) and still see all your tags (<code>hg tags</code>).</p></li>
<li><p>You ask if branches are universally visible. Yes they are -- the names of named branches can be used in any context where you need to specify a changeset, as can tags.</p></li>
</ul>

<p>Make sure you understand what named branches are before you begin using them. They are actually <em>not</em> necessary for making a bugfix branch. You can instead choose to simply go back (<code>hg update 1.0</code>) and fix the bug and then commit. That will create a new head, which your line of development towards 1.1 (this gives you <a href=""http://www.selenic.com/mercurial/wiki/MultipleHeads"" rel=""noreferrer"">multiple heads</a>). So you don't have to create a named branch to add a new branch of development to your repository.</p>

<p>Having multiple heads is completely equivalent to having multiple clones. You can even convert back and forth: you can use</p>

<pre><code>% hg clone -r X-head repo repo-X
</code></pre>

<p>to untangle the <code>X-head</code> changeset and its ancestors from the other changesets in <code>repo</code>. And you can combine multiple clones by simply pulling all changesets into one big clone.</p>

<p><a href=""http://www.selenic.com/mercurial/wiki/NamedBranches"" rel=""noreferrer"">Named branches</a> are similar, yet different. They allow you to embed a named in each changeset. So you can have several changesets in your history with the name ""foo"". When you do <code>hg update foo</code> you will end up at the tip-most of these changesets. In this way, named branches function as a kind of floating tag.</p>

<p>If you are uncomfortable with the idea of a permanent label for your changesets, you can instead try the <a href=""http://www.selenic.com/mercurial/wiki/BookmarksExtension"" rel=""noreferrer"">bookmarks extension</a>. That will also give you ""floating tags"" which you can use for updating, but they wont be permanently part of the history since they aren't versioned.</p>

<p>I hope this helps a bit.</p>",5.0,2009-06-07 15:11:01.863000 UTC,,20.0,[]
Build sequencing when using distributed version control,"<p>Right now, we are using Perforce for version control. It has the handy feature of a strictly increasing change number that we can use to refer to builds, eg ""you'll get the bugfix if your build is at least 44902"".</p>

<p>I'd like to switch over to using a distributed system (probably git) to make it easier to branch and to work from home. (Both of which are perfectly possible with Perforce, but the git workflow has some advantages.) So although ""tributary development"" would be distributed and not refer to a common revision sequencing, we'd still maintain a master git repo that all changes would need to feed into before a build was created.</p>

<p>What's the best way to preserve strictly increasing build ids? The most straightforward way I can think of is to have some sort of post-commit hook that fires off whenever the master repo gets updated, and it registers (the hash of) the new tree object (or commit object? I'm new to git) with a centralized database that hands out ids. (I say ""database"", but I'd probably do it with git tags, and just look for the next available tag number or something. So the ""database"" would really be .git/refs/tags/build-id/.)</p>

<p>This is workable, but I'm wondering if there is an easier, or already-implemented, or standard/""best practice"" way of accomplishing this.</p>",8,0,2008-09-23 17:23:32.830000 UTC,23.0,,29,git|build-process|dvcs,6914,2008-09-17 00:43:52.997000 UTC,2022-03-04 22:29:46.633000 UTC,"San Carlos, CA",1686,305,0,168,"<p>I second the suggestion of using <code>git describe</code>. Provided that you have a sane versioning policy, and you don't do any crazy stuff with your repository, <code>git describe</code> will always be monotonic (at least as monotonic as you can be, when your revision history is a DAG instead of a tree) and unique.</p>

<p>A little demonstration:</p>

<pre><code>git init
git commit --allow-empty -m'Commit One.'
git tag -a -m'Tag One.' 1.2.3
git describe    # =&gt; 1.2.3
git commit --allow-empty -m'Commit Two.'
git describe    # =&gt; 1.2.3-1-gaac161d
git commit --allow-empty -m'Commit Three.'
git describe    # =&gt; 1.2.3-2-g462715d
git tag -a -m'Tag Two.' 2.0.0
git describe    # =&gt; 2.0.0
</code></pre>

<p>The output of <code>git describe</code> consists of the following components:</p>

<ol>
<li>The newest tag reachable from the commit you are asking to describe</li>
<li>The number of commits between the commit and the tag (if non-zero)</li>
<li>The (abbreviated) id of the commit (if #2 is non-zero)</li>
</ol>

<p>#2 is what makes the output monotonic, #3 is what makes it unique. #2 and #3 are omitted, when the commit <em>is</em> the tag, making <code>git describe</code> also suitable for production releases.</p>",1.0,2008-09-24 00:35:04.167000 UTC,,28.0,[]
A pretty and feature rich Git GUI for Linux,"<p>I was checking out for a <em>rich GUI</em> for Git on Linux. As of late, I have started using it a lot and find that even though command-line is very useful, I would still want a GUI at my disposal since I am not still very much aware with the advanced features.</p>
<p>I came across <a href=""https://stackoverflow.com/questions/1516720/git-gui-client-for-linux"">this question on Stack Overflow before</a>, but I still have to ask this question again, as that question doesn't have my answer</p>
<ul>
<li><p>I have used <em>git-cola</em> and <a href=""https://git-scm.com/docs/gitk"" rel=""nofollow noreferrer"">gitk</a>, but they look half-baked and gitk looks like I am working on some other desktop environment. All the features I require in gitGUI is not in either of them.</p>
</li>
<li><p>I tried <em>giggle</em>, but it is more useful for watching diffs</p>
</li>
<li><p>I tried QGit, but it is not fully complete.</p>
</li>
</ul>
<p>Now let me tell how I want a foo-Git-GUI to look like:</p>
<ul>
<li>You can have a look at <a href=""http://doc.bazaar.canonical.com/explorer/en/"" rel=""nofollow noreferrer""><code>Bzr Explorer</code></a>. It is highly usable and has many more features than many of the above mentioned GUIs</li>
</ul>
<p>Is there any Git GUI which looks like the above example I posted above?</p>",7,1,2010-01-26 18:23:10.233000 UTC,22.0,2021-08-18 14:58:25.153000 UTC,70,git|user-interface|dvcs|gitk,56734,2009-05-28 08:35:15.910000 UTC,2022-03-05 03:47:38.297000 UTC,"Boston, MA",2062,316,7,428,"<p>I find myself using mainly <code>gitg</code> in combination with the command line for more complicated tasks.</p>
<p>Source repository: <a href=""http://git.gnome.org/browse/gitg"" rel=""nofollow noreferrer"">http://git.gnome.org/browse/gitg</a>.</p>
<p>Recently, a new version 0.3.2 has been released, which is a rewrite using <a href=""https://en.wikipedia.org/wiki/Vala_(programming_language)"" rel=""nofollow noreferrer"">Vala</a>. It's not yet feature-complete compared to the old 0.2 series, but the refactor is expected to speed up future development.</p>
<p>Also, there are normally <a href=""https://en.wikipedia.org/wiki/Ubuntu#Package_Archives"" rel=""nofollow noreferrer"">PPAS</a> on <a href=""https://wiki.ubuntu.com/Launchpad"" rel=""nofollow noreferrer"">Launchpad</a> which contain a more up-to-date version than the standard repository.</p>
<p><img src=""https://i.stack.imgur.com/tNG7G.png"" alt=""Enter image description here"" /></p>
<p><img src=""https://i.stack.imgur.com/XTPfa.png"" alt=""Enter image description here"" /></p>",11.0,2011-03-04 10:45:36.730000 UTC,2021-08-18 15:03:57.200000 UTC,63.0,[]
How to abort a merge in mercurial?,"<p>I goofed up a merge.  I'd like to revert then try again.<br>
Is there a way to revert a merge before it is committed?</p>

<p><code>hg revert</code> doesn't do what I'd like, it only reverts the text of the files.  Mercurial aborts my second attempt at merging and complains original merge is still uncommitted.</p>

<p>Is there a way to undo a merge after an <code>hg merge</code> command but before it's committed?</p>",3,1,2010-10-20 18:24:00.770000 UTC,11.0,2017-07-13 16:44:14.087000 UTC,86,version-control|mercurial|merge|dvcs|undo,30862,2008-10-17 04:26:41.420000 UTC,2022-02-10 02:14:02.977000 UTC,"Woodinville, WA",53865,3531,277,4724,<p><code>hg update -C &lt;one of the two merge changesets&gt;</code></p>,7.0,2010-10-20 18:28:03.953000 UTC,,108.0,[]
Azure Databrics - Running a Spark Jar from Gen2 DataLake Storage,"<p>I am trying to run a spark-submit from Azure Databrics. Currently I can create a job, with the jar uploaded within the Databrics workspace, and run it.</p>

<p>My queries are:</p>

<ol>
<li><p>Is there a way to access a jar residing on a GEN2 DataLake storage and do a spark-submit from Databrics workspace, or even from Azure ADF ? (Because the communication between the workspace and GEN2 storage is protected ""fs.azure.account.key"")</p></li>
<li><p>Is there a way to do a spark-submit from a databrics notebook?</p></li>
</ol>",2,2,2019-09-11 08:22:54.203000 UTC,,,0,scala|azure|apache-spark|azure-databricks,637,2015-11-28 04:35:48.297000 UTC,2022-03-04 10:34:50.097000 UTC,"Stuttgart, Germany",324,4,1,45,"<p>Finally I figured out how to run this:</p>

<ol>
<li><p>You can do a run a Databricks jar from an ADF, and attach it to an existing cluster, which will have the adls key configured in the cluster.</p></li>
<li><p>It is not possible to do a spark-submit from a notebook. But you can create a spark job in jobs, or you can use the <a href=""https://docs.databricks.com/dev-tools/api/latest/jobs.html#runs-submit"" rel=""nofollow noreferrer"">Databricks Run Sumbit</a> api, to do a spark-submit.</p></li>
</ol>",0.0,2020-02-18 20:13:36.617000 UTC,,-1.0,[]
Is there a children command to complement the parents command?,<p>Mercurial provides the command parents to examine a given revision's parent(s).  This can easily be used to Traverse the DAG backward.  I need to traverse the DAG forward.  Is there a <code>hg children</code> command?</p>,2,0,2010-10-19 19:27:24.430000 UTC,1.0,,6,mercurial|dvcs,1863,2008-10-17 04:26:41.420000 UTC,2022-02-10 02:14:02.977000 UTC,"Woodinville, WA",53865,3531,277,4724,"<p>If you're using Mercurial 1.6 or later, there is a built-in functional language for specifying sets of revisions; see <code>hg help revsets</code> for full details.</p>

<p>In your case, you would use</p>

<pre><code>hg log -r ""children(XXX)""
</code></pre>

<p>to show immediate children of revision <code>XXX</code>, or</p>

<pre><code>hg log -r ""descendants(XXX)""
</code></pre>

<p>to show all changesets with <code>XXX</code> as an ancestor.</p>",0.0,2010-10-19 19:37:14.443000 UTC,2019-04-30 09:16:36.927000 UTC,9.0,[]
How to best configure a central repository/multiple central repositories for Mercurial?,"<p>I am new to Mercurial and trying to figure out if it could replace SVN.  Everyone I work with has used SVN, CVS and VSS (shiver), so this could be quite a large change.  I have been very interested after reading about its merge and branch capability, but have a few reservations.</p>

<p>We are currently on SVN, and have one central repository.  From my reading, it seems as though there is no ONE central repository for all projects when using Mercurial.  NOTE: We consider each project a separate logical set of code, or a Visual Studio Solution.  It runs on its own.</p>

<p>We have around 60 separate projects in our one central SVN repository.  After reading about Mercurial it seems to me that I have to create 60 separate central repositories for each one of these projects on the server.  QUESTION #1: Should I create a single repository for each project?</p>

<p>If yes, then I am worried about configuring and hosting 60 separate central Mercurial servers.  I started thinking I could configure one file, but it seems as if each repository must be individually configured using the “C:...\MyRepository.hg\hgrc” file (Windows install).  It also seems as I have to run 60 servers (>hg serve), I would assume on different ports.  QUESTION #2: If the answer to question 1 is yes, there should be a single central repository for each project, then how have people managed many multiple repositories?</p>

<p>Finally, I haven’t looked into moving all history and changes from one SVN repository to a bunch of separate Mercurial repositories, but would appreciate any comments from someone who has done this (or if it is even possible). </p>",4,0,2010-05-01 01:41:13.303000 UTC,5.0,,13,mercurial|dvcs|tortoisehg|version-control,3856,2008-09-15 15:59:29.363000 UTC,2022-03-01 23:57:13.687000 UTC,,3219,445,106,331,"<p>Mercurial supports ""central repositories"" just fine, but it does them by convention rather than fiat.</p>

<p>You need run only a single server to run all sixty repositories, and you wouldn't use <code>hg serve</code> for it.  Instead you pick one of the production-quality methods from <a href=""https://www.mercurial-scm.org/wiki/PublishingRepositories"" rel=""nofollow noreferrer"">among the publishing options</a>.  Likely you'll run <code>hgwebdir</code> behind Apache, Nginx, or IIS depending on your preferences.</p>

<p>Once that's running you can create new repsitories with <code>hg init</code> or <code>hg clone</code> within your repositories directory.  At a previous employer we had five products, with a ""central"" repository for each, and ten to twenty clones of each for various teams and features, all running in a single server.  The central repo determines ""what's official"" but you'll find impromptu repos for unofficial features springing up all the time. </p>

<p><code>hg serve</code> is for two devs to hand off some changesets in a hurry, but published repos require a little more scaffolding.</p>",0.0,2010-05-01 02:36:29.703000 UTC,2018-03-05 11:45:49.243000 UTC,11.0,[]
How are DVCS used in large teams?,"<p>I've recently started getting into Git on a personal project, and I can see how a DVCS might benefit us at work (which is a large enterprise software company, currently running Perforce).  Feature work in my team for example mostly consists of developers creating their own branches; sometimes these are shared between small teams of developers.  I think it would be more efficient in this instance to use a DVCS.  </p>

<p>In the more general case, though, I'd be interested to hear from people that use a DVCS at work, in medium to large teams.  </p>

<ol>
<li>How do you deal with N-way merges?  Is this even a common scenario?  Mercurial only supports N-way merges by doing (N-1) 2-way merges (and <a href=""http://code.google.com/p/support/wiki/DVCSAnalysis"" rel=""noreferrer"">read</a> that this is the preferred solution in other DVCS), which sounds like a very laborious process for even relatively small N.  </li>
<li>Do you use a single central authoritative repository, or is it truly P2P?  </li>
<li>Do developers often push and pull code to and from each other, or does everything go via the central repository?</li>
</ol>",6,5,2009-04-24 21:28:04.583000 UTC,12.0,2009-04-25 22:51:36.217000 UTC,30,git|version-control|mercurial|dvcs,2752,2008-09-08 22:13:27.417000 UTC,2022-03-03 16:54:49.060000 UTC,"Cambridge, UK",6499,680,8,661,"<p>My team at my previous employer used Git, and it worked well for us.  We weren't all that large (maybe 16 or so, with maybe 8 really active committers?), but I have answers to your questions:</p>

<ol>
<li>N-Way merges aren't terribly common.  We came up with some conventions about branch naming that allowed us to write scripts that eased the ""release engineering"" process (I use scare quotes because we didn't have a release engineer), and people would create private feature branches, but we rarely had an issue with merging more than two branches (see the next one).</li>
<li>(and #3).  We had a central repository on a development server for three reasons: (a) The development machine had a RAID5 (more fault tolerant) and nightly backups (dev workstations were not nightly), (b) production releases were built on the development server, and (c) having a central repository simplified scripting.  As a result, N-way merges simply never happened.  The closest thing we had to N-way was when someone merged laterally and then merged vertically.</li>
</ol>

<p>Git was a really great thing for us because of its high degree of flexibility; however, we did have to establish some conventions (branch and tag names, repo locations, scripts, etc, process) or it might have been a little chaotic.  Once we got the conventions set up, the flexibility we had was just fantastic.</p>

<p>Update: our conventions basically were thus:</p>

<ul>
<li>a directory on our NFS server that housed all central repositories</li>
<li>we had several projects that shared components, so we broke them out into libraries, essentially, with their own repositories, and the deliverable projects just included them as git submodules.</li>
<li>there were version strings and release names imposed on us from above, so we just used a variants of those as branch names</li>
<li>similarly, for tags, they followed the process-dictated release names</li>
<li>the deliverable projects contained a properties file which I read into the shell scripts, and that allowed me to write a single script to manage the release process for all the projects, even though each one had slight variations on the process - the variations were accounted for in those property files</li>
<li>I wrote scripts that would rebuild a deliverable package from any tag</li>
<li>using git allowed us to control access using PAM and/or normal user permissions (ssh, etc)</li>
<li>There were other conventions that are harder to put in a bulleted list, like when merges should happen.  Really, me and another guy were sort of the in-house ""git gurus"", and we helped everyone figure out how to use branches and when to merge.</li>
<li>getting people to commit in small chunks and not drop diff-bombs in the master branch was a challenge.  One guy dropped about two solid weeks of work into one commit, and we eventually had to unravel it all.  A <em>huge</em> waste of time, and frustrating to all.</li>
<li>informative and detailed comments to go with commits</li>
</ul>

<p>There were other things that you learn as your team gets experienced and learns to work with each other, but this was enough to get us started.</p>

<p><strong>Update</strong>: anyone who follows such things by now already knows about it, but Vincent Dreissen has written a solid and pretty comprehensive (but not exaustive) <a href=""http://nvie.com/posts/a-successful-git-branching-model/"" rel=""nofollow noreferrer"">take on branching and release engineering using Git</a>.  I would highly encourage using his process as a starting point because for two reasons:</p>

<ul>
<li>lots of teams do it this way or are using some close variant (including Linux, Git, and many other OSS project teams), which means this method has been tested and tweaked to be successful in most circumstances.  You are very unlikely to face an issue that hasn't been faced and solved within the constraints of this model.</li>
<li>because of the foregoing, almost any engineer with Git experience will understand what's going on.  You won't have to write detailed documentation about the fundamental nature of your release process; you'll only have to document things specific only to your project or team.</li>
</ul>",7.0,2009-04-25 04:23:23.307000 UTC,2013-09-06 19:01:40.227000 UTC,13.0,[]
Combination of incoming + outgoing + status?,"<p>Is there an hg command that will combine <code>hg incoming</code> + <code>hg outgoing</code> + <code>hg status</code>?  </p>

<p>This would tell you if there's anything remote that needs to come in, anything committed locally that needs to go out, or any local changes that need to be committed.</p>",5,0,2011-02-01 18:02:11.177000 UTC,1.0,2012-01-07 10:59:49.760000 UTC,10,mercurial|dvcs,914,2008-12-18 03:16:49.993000 UTC,2022-03-03 19:59:46.803000 UTC,"New York, NY",52509,2554,17,3816,"<p>Though you won't get the actual changesets or files, to get the current status summary, use the summary command:</p>

<pre><code>hg summary --remote
</code></pre>

<p>Example output:</p>

<pre><code>C:\Temp\repo&gt; hg summary --remote
parent: 5:18ee64a17016 tip
 Added lots of unit-tests for DatabaseConnection.
branch: default
commit: 1 modified                          &lt;-- status
update: 3 new changesets (update)           &lt;-- local status, not at tip
remote: 1 or more incoming, 1 outgoing      &lt;-- incoming/outgoing
</code></pre>

<p>Note that you only get counts, not the actual changesets, for that you need to execute the actual incoming or outgoing or status commands.</p>",0.0,2011-02-01 18:07:11.277000 UTC,,17.0,[]
MQ vs. branches in Mercurial,"<p>I've been working with Mercurial now for some time. When making (private) changes to some third party software, in the past I always created a separate named branch for these changes. When the upstream code updates, I simply merge it into my named branch. </p>

<p>Today I read about MQ (Mercurial Queues - chapters <a href=""http://hgbook.red-bean.com/read/managing-change-with-mercurial-queues.html"">12</a> and <a href=""http://hgbook.red-bean.com/read/advanced-uses-of-mercurial-queues.html"">13</a>). I think I understood the concept behind MQ, so my question is:</p>

<p>Is there any advantage of MQ over (named) branches in Mercurial (for my scenario)?</p>",2,0,2012-03-12 09:53:24.013000 UTC,3.0,2012-03-13 09:16:02.507000 UTC,9,mercurial|branch|dvcs|mercurial-queue,1466,2011-02-12 11:42:20.397000 UTC,2022-03-04 09:09:16.700000 UTC,"Nuremberg, Germany",7514,450,9,846,"<p>The main advantage of MQ over named branches are:</p>

<ul>
<li><p>You can revise your patches. This lets you edit history and so you can maintain a clean and logical series of patches on top of the upstream code: if you notice a mistake in a patch you refresh the patch instead of making a new commit.</p></li>
<li><p>The changes in your patches will be cleanly separated from the changes made upstream. When you merge two branches, you mixing the two streams of development. This makes it difficult to see the changes you've made without also seeing the changes coming in from the upstream branch.</p></li>
<li><p>The patch names are transient. When you <code>hg qfinish</code> an applied patch, there's no trace of the patch name left in the commit. So you can use MQ without coordinating first with the upstream repository since they'll never notice MQ.</p></li>
<li><p>You avoid merges. Instead of merging with the latest code from upstream, you <a href=""https://www.mercurial-scm.org/wiki/RebaseExtension"" rel=""nofollow noreferrer"">rebase</a> your applied patches. This gives you a simpler history. The history is obviously fake since you pretend that you made all your patches <em>after</em> seeing the code from upstream — when infact you made it in parallel with upstream and later <em>moved</em> your patches to the tip of upstream.</p></li>
<li><p>You have no permanent branch name in the changesets. People sometimes <a href=""https://www.mercurial-scm.org/wiki/StandardBranching#What_not_to_do"" rel=""nofollow noreferrer"">treat named branches as disposable</a> and become upset when they realize that a named branch is fixed in history. (You can actually set the branch name with <code>hg branch</code> before pushing patches so this point is not so bad.)</p></li>
</ul>

<p>Disadvantages of MQ are:</p>

<ul>
<li><p>It's an extra tool to learn. It's powerful, but it also gives you more opportunity to shoot yourself in the foot. Running <code>hg qdelete</code> will really <em>delete</em> the patch and so you can throw away data. (I think this is fine, but we've had a Git user coming to our mailinglist complaining about this.)</p></li>
<li><p>You make it much harder to collaborate with others. You <em>can</em> turn <code>.hg/patches</code> into a repository and push/pull patches around between repositories but it's difficult to do that if you're more than a single developer. The problem is that you end up merging patches if more than one persons refreshes the same patch.</p></li>
<li><p>You have no permanent branch name in the changesets. If you're using named branches right and use stable, long-term branch names, then you will miss that when using MQ.</p></li>
</ul>",7.0,2012-03-12 11:36:56.570000 UTC,2018-02-22 12:32:25.423000 UTC,13.0,[]
Mercurial: check whether last pull/update introduced changes,"<p>I am the process of writing an update script, which pulls the latest version of a number of repositories, and rebuilds the projects. I wanted to make the build conditional, so I tried </p>

<pre><code>hg pull -u &amp;&amp; ant clean build
</code></pre>

<p>and the variation</p>

<pre><code>hg pull; hg update &amp;&amp; ant clean build
</code></pre>

<p>However, the ant build is always invoked, even when nothing has changed. I know that I can use <code>hg incoming</code> to check for changes before doing the pull, but this feels wasteful to me.</p>

<p>How can I check for new changes, without having to contact the server twice (once for <code>hg incoming</code>, once for <code>hg pull</code>)?</p>

<p><strong>UPDATE:</strong> This is my build script now:</p>

<pre><code>update() {
  TIP=$(hg tip --template ""{node""})
  hg pull -u
  if test ""$TIP"" != $(hg tip --template ""{node}""); then
    ant clean build
  fi
}

(cd repo1; update )
(cd repo2; update )
</code></pre>

<p>And for people wondering why I do a clean build every time, there are two reasons for that:</p>

<ol>
<li>The repositories depend on each other, and when the API in one of them changes, I need to do a full rebuild to find places where these API changes break code</li>
<li>The Java compiler inlines constants, also from other class files. Now, when I change a constant in a class back to a field that can change, all other class files using that constant remain untouched by a build, and this can lead to subtle bugs that I want to avoid.</li>
</ol>",2,5,2012-01-19 08:05:44.153000 UTC,3.0,2012-01-19 14:30:18.807000 UTC,10,bash|mercurial|build|dvcs,2987,2009-04-01 11:24:31.047000 UTC,2022-03-04 17:47:04.803000 UTC,"Potsdam, Germany",12872,2249,6,672,"<p>You should not just run <code>hg incoming</code> twice since it will actually <strong>download all the changesets twice</strong> then. This because you cannot just take a sneak peek at the remote repository without running a full <code>hg pull</code>.</p>

<p>So save the incoming changesets in a bundle and pull from that instead:</p>

<pre><code>hg incoming --bundle incoming.hg &amp;&amp; hg pull --update incoming.hg &amp;&amp; echo ""Go!""
</code></pre>

<p>The <code>hg incoming</code> command acts as a guard for the following commands: the <code>&amp;&amp;</code> is short-circuiting so the first command that return a non-zero exit code will make the whole construct fail with that exit code. This means that <code>hg pull</code> and any following commands aren't executed at all when <code>hg incoming</code> signals that there is nothing to pull.</p>",4.0,2012-01-19 11:18:23.843000 UTC,2012-01-19 13:00:11.473000 UTC,10.0,[]
How can I stop .gitignore from appearing in the list of untracked files?,"<p>I just did a <code>git init</code> on the root of my new project.</p>

<p>Then I created a <code>.gitignore</code> file.</p>

<p>Now, when I type <code>git status</code>, <strong>.gitignore</strong> file appears in the list of untracked files. Why is that?</p>",19,9,2009-04-20 06:42:27.050000 UTC,198.0,2018-10-16 09:13:05.773000 UTC,1004,git|version-control|dvcs|gitignore,376583,2008-10-13 06:22:10.643000 UTC,2021-06-21 17:48:49.513000 UTC,"Vancouver, BC, Canada",43215,522,3,1123,"<p>The <code>.gitignore</code> file should be in your repository, so it should indeed be added and committed in, as <code>git status</code> suggests. It has to be a part of the repository tree, so that changes to it can be merged and so on.</p>

<p>So, add it to your repository, it should not be gitignored.</p>

<p>If you really want you can add <code>.gitignore</code> to the <code>.gitignore</code> file if you don't want it to be committed. However, in that case it's probably better to add the ignores to <code>.git/info/exclude</code>, a special checkout-local file that works just like .gitignore but does not show up in ""git status"" since it's in the <code>.git</code> folder.</p>

<p>See also <a href=""https://help.github.com/articles/ignoring-files"" rel=""noreferrer"">https://help.github.com/articles/ignoring-files</a></p>",17.0,2009-04-20 07:11:45.857000 UTC,2015-01-02 10:24:30.973000 UTC,1004.0,[]
hg local ignore,"<p>I could have sworn there was a way to keep a local ignore file in an hg repo, i.e. a file similar in function to .hgignore, but not checked into the repo.  This could be used to ignore changes to an IDE project file if different IDEs are being used, for example.  I'm having trouble finding how it's done.  Does anyone recall the details?</p>",3,0,2009-06-15 14:54:37.387000 UTC,11.0,2009-07-12 11:27:21.340000 UTC,40,version-control|mercurial|dvcs|hgignore,8069,2008-10-10 13:35:04.687000 UTC,2022-02-22 17:18:34.010000 UTC,,2792,481,17,220,"<p><a href=""https://www.mercurial-scm.org/wiki/TipsAndTricks#Ignore_files_in_local_working_copy_only"" rel=""noreferrer"">This</a> is what I was looking for.</p>

<blockquote>
  <p>Add the following to the repo's .hg/hgrc:</p>

<pre><code>[ui]
ignore = /path/to/repo/.hg/hgignore
</code></pre>
  
  <p>and create a new file .hg/hgignore beside it. This new file will be
  untracked, but work the same as the versioned .hgignore file for this
  specific working copy. (The /path/to/repo bit is unfortunate but
  necessary to make it work when invoking 'hg' from within a subdir of
  the repo.)</p>
</blockquote>",3.0,2009-06-15 15:21:55.353000 UTC,2018-02-22 12:26:07.553000 UTC,52.0,[]
"In Mercurial, what's the difference between a 'head' and a 'branch'?","<p>I'm new to Mercurial, and DVCS in general. What's the difference between a <em>head</em> and a <em>branch</em>?</p>",4,0,2011-05-31 13:52:36.473000 UTC,1.0,,7,mercurial|dvcs,2590,2008-10-25 20:57:21.130000 UTC,2022-03-04 22:05:01.983000 UTC,"Sao Paulo, Brazil",3891,601,14,1044,"<p>If you view it as a tree, then a head is a leaf, at the very end of a branch.</p>

<p>It's explained really well at Mercurial's own wiki:<br>
<a href=""http://www.mercurial-scm.org/wiki/Branch"" rel=""nofollow noreferrer"">Branches</a><br>
<a href=""http://www.mercurial-scm.org/wiki/Head"" rel=""nofollow noreferrer"">Heads</a></p>",0.0,2011-05-31 13:57:03.397000 UTC,2016-09-07 00:39:55.977000 UTC,8.0,[]
Migrate Bugzilla Issues to github Issue Tracker,"<p>I would like to migrate all my <a href=""http://www.bugzilla.org/"" rel=""noreferrer"">Bugzilla</a> issues to an Issue Tracker at github.com (<a href=""https://github.com/joyent/node/issues"" rel=""noreferrer"">Example</a>) (you get an issue tracker per repository).</p>

<p>1) <strong>Are there tools out there?</strong></p>

<p>2) <strong>How did you do it?</strong></p>

<p>Thanks for any advices, I will write a summary at the end how I finally did it (hopefully not manually!)</p>",4,4,2011-09-02 09:04:48.220000 UTC,6.0,,13,git|github|dvcs|bugzilla|issue-tracking,3465,2010-06-02 19:27:24.517000 UTC,2022-02-24 08:17:09.127000 UTC,,4005,784,26,741,"<p>You need to export your issues from Bugzilla, and then use the GitHub API to upload the issues into GitHub:</p>

<p><a href=""http://developer.github.com/v3/issues/#create-an-issue"" rel=""nofollow noreferrer"">http://developer.github.com/v3/issues/#create-an-issue</a></p>

<p><em>(note that the old issue-import through GitHub support channels is discontinued)</em></p>

<p>This does mean your issue numbers will change, so you might want to append a 'Bugzilla-Id' footer to your issue description, so you can use GitHub's free-text search on issues to find them from the old Bugzilla ids.</p>

<p>As far as exporting your data from Bugzilla goes, I think your options are:</p>

<ul>
<li>Bugzilla's HTML export</li>
<li>Bugzilla <a href=""http://www.bugzilla.org/docs/4.0/en/html/api/Bugzilla/WebService/Server/JSONRPC.html"" rel=""nofollow noreferrer"">JSON-RPC</a></li>
<li>Connecting directly to the <a href=""http://www.ravenbrook.com/project/p4dti/tool/cgi/bugzilla-schema/index.cgi?action=single&amp;version=3.4.2&amp;view=View%20schema"" rel=""nofollow noreferrer"">bugzilla database</a></li>
</ul>

<p>Good luck! If you do write an export script, I'd encourage you to open-source it on GitHub!</p>",0.0,2012-01-11 11:11:10.623000 UTC,2018-02-13 18:48:09.273000 UTC,11.0,[]
What is the difference between clone and mkdir->cd->init->remote-add->pull?,"<p>After setting up a repo on Github, there seems to be two ways to pull that repo into a local repo.</p>

<p>Firstly, I could create a directory, initialize a blank repo, add a remote and then pull changes from the remote.</p>

<pre><code>&gt; mkdir ""exampleProject""
&gt; cd ""exampleProject""
&gt; git init
&gt; git remote add origin git@github.com:exampleUser/exampleProject.git
&gt; git pull origin master
</code></pre>

<p>Secondly, I could clone the remote.</p>

<pre><code>&gt; git clone git@github.com:exampleUser/exampleProject.git
</code></pre>

<p>Is cloning just a shortcut for the 5 step version above or does it do anything else as well? Will I run into difficulty if I use one method over the other?</p>",1,1,2010-11-05 18:11:17.503000 UTC,10.0,,26,git|dvcs|git-clone|git-pull,8846,2009-12-22 05:03:17.873000 UTC,2022-03-04 19:47:16.310000 UTC,United Kingdom,12450,407,21,429,"<p>A lot of commands, whether git commands or common programs, do things in one line you could otherwise do in ten. It's always good to save work!</p>

<p>That said, your steps are close to, but not entirely the same as, what <code>git clone</code> does. I can think of a few differences, all to do with branches:</p>

<ul>
<li><p>If, for some reason, the remote's HEAD is <em>not</em> master, the clone will do the right thing - give you a branch named the same as the remote's, instead of master. This is rare, but a good detail to be aware of.</p></li>
<li><p>Your <code>git pull</code> won't create any remote branches. If the remote has several branches, the clone creates remote branches <code>remotes/origin/foo</code>, <code>remotes/origin/bar</code>, ... in your repository. A <code>git fetch origin</code> would take care of that in your listed steps.</p></li>
<li><p>You also haven't set up your master branch to track origin's, which the clone does. You could add this to your listed steps as <code>git config branch.master.remote origin; git config branch.master.merge refs/heads/master</code>. This is very significant - with your steps, if you have master checked out and you type git pull, it won't know what to do.</p></li>
</ul>

<p>It's possible I've missed a thing or two. As for difficulties one way or the other, even assuming you iron out all the differences between a default clone and a ""manual clone"", my advice would be not to reinvent <code>git clone</code>:</p>

<ul>
<li><p>It's short. Why do more work?</p></li>
<li><p>It's got handy options to change its behavior. Things like <code>--shared</code> would be really difficult to add to your listed commands.</p></li>
<li><p>It's guaranteed to do the right thing now and in the future. What if you missed a detail, like the ones above? What if git added a global config parameter that affected clones? You'd have to change your commands to take it into account, but git clone would already know.</p></li>
</ul>",3.0,2010-11-05 18:30:17.940000 UTC,2010-11-05 18:58:02.430000 UTC,27.0,[]
How can I back out a merge in Mercurial and later remerge with that branch?,"<p>I have two branches, default and branch1. By mistake one person in our team merged branch1 with default. The content in branch1 is not yet ready to merge with default (it contains a major rework of the build &amp; deploy environment).</p>

<p>We did an experiment with 'hg backout', backing out the merge (not sure this is the right way to do it). Then the changes from branch1 gets deleted on default, which is fine - but we can not remerge with branch1. </p>

<p>How should we solve this issue?</p>",6,4,2010-11-11 08:18:00.277000 UTC,16.0,2011-01-05 02:13:25.827000 UTC,54,version-control|mercurial|dvcs,21585,2009-12-13 19:57:52.810000 UTC,2017-01-29 07:37:09.057000 UTC,Sweden,3098,13,2,123,"<p>There are many scenarios here where you might want to do this, I'll make each scenario a headline, so that you can find the scenario that fits your case. Note that I'm still learning Mercurial, and I'd like pointers if something I say is wrong, using the wrong terminology, could be done better, etc.</p>

<h1>No further changes, merge not shared (no pushes/pulls)</h1>

<p><em>The programmer has merged, but not done anything else, nor has (s)he shared the changes with anyone, in any way</em></p>

<p>In this case, simply discard the local clone, and get a fresh clone from a safe repository.</p>

<h1>Local changes on top of merge, not shared</h1>

<p><em>The programmer has merged, and continued working based on that merge. The changesets that followed the merge should be kept, but the merge itself should be removed. The changes (merge + following changesets) have not been shared with anyone</em> </p>

<p>In this case I would do one of four:</p>

<ol>
<li>Try to use the REBASE extension, this will move the changesets from one location to another. If the changesets are based on code-changes that were introduced with the merge, some manual work must be done to reconcile the differences.</li>
<li>Try to use the MQ extension to pull the changesets that are to be kept into a patch-queue, then push them back in a different location. This will, however, have the same problem as the REBASE extension in terms of changes based on the merge</li>
<li>Try to use the TRANSPLANT extension to ""copy"" the changes from one location to another. Still, same problem exists as with the first two.</li>
<li>Do the work again, probably with the help of a diffing tool to take changes done in the changesets I want to discard, and re-do them in the correct location.</li>
</ol>

<p>To get rid of the merge changeset + all the following changesets, there's a couple of options:</p>

<ol>
<li><p>Use the strip command in the MQ extension</p>

<pre><code>hg strip &lt;hash of merge changeset&gt;
</code></pre></li>
<li><p>Clone and pull, and specify the hash of the changesets leading up to, but not including the merge. In essence, create a new clone by pulling from the damaged clone into a new one, and avoid pulling in the merge you don't want.</p>

<pre><code>hg clone damaged -r &lt;hash of first parent&gt; .
hg pull damaged -r &lt;hash of second parent&gt;
</code></pre></li>
</ol>

<h1>Merge pushed to others, control over clones</h1>

<p><em>The programmer has pushed to master repository, or to someone else, or someone pulled from the programmers repository. However, you (as in the group of developers) have control over all the repositories, as in, you can contact and talk to everyone before more work is done</em></p>

<p>In this case, I would see if step 1 or 2 could be done, but it might have to be done in a lot of places, so this might involve a lot of work.</p>

<p>If nobody has done work based on the merge changeset, I would use step 1 or 2 to clean up, then push to the master repository, and ask everyone to get a fresh clone from the master repository.</p>

<h1>Merge pushed, you don't have control over clones</h1>

<p><em>The programmer pushed the mergeset, and you don't know who will have the merge changeset. In other words, if you succeed in eradicating it from <strong>your</strong> repositories, a stray push from someone who still has it will bring it back.</em></p>

<p>Ignore the merge changeset and work in the two branches as though it never happened. This will leave a dangling head. You can then later, when you've merged the two branches, do a null-merge for this head to get rid of it.</p>

<pre><code>  M         &lt;-- this is the one you want to disregard
 / \
*   *
|   |
*   *
|   |
</code></pre>

<p>Simply continue working in the two branches:</p>

<pre><code>|   |
*   *
| M |       &lt;-- this is the one you want to disregard
|/ \|
*   *
|   |
*   *
|   |
</code></pre>

<p>Then later you merge the two, the real merge you want:</p>

<pre><code>  m
 / \
*   *
|   |
*   *
| M |       &lt;-- this is the one you want to disregard
|/ \|
*   *
|   |
*   *
|   |
</code></pre>

<p>You can then do a null-merge to get rid of the dangling head. Unfortunately I don't know how to do that except through TortoiseHg. It has a checkbox where I can discard the changes from one of the branches.</p>

<p>With TortoiseHg, I would update to the merge I want to keep (the topmost, lowercase m), then select and right-click on the dangling merge head below, and then check the ""Discard all changes from merge target (other) revision"":
<img src=""https://i.stack.imgur.com/RkPoG.png"" alt=""discard changes from target""></p>",5.0,2010-11-11 08:40:49.410000 UTC,2010-11-11 09:46:54.793000 UTC,91.0,[]
What happens when Git pull or push to different branch being in some other branch,"<p>I am familiar with GIT and using the same for versioning of my project.</p>

<p>I am having few queries which I would like to know. I googled but couldn't get the good answer.</p>

<p>So the thing is I am having <strong>master, module1, feature1</strong> branches.</p>

<pre><code>master
 ---------- 
         | module1
         ----------
                  | feature1
                  ------------
</code></pre>

<p>module1 is branched from master and feature1 is branched from module1.</p>

<p><strong>Query 1</strong> : Being in feature1 branch what if I work on few changes and commit and push the same to module1.</p>

<pre><code>git add .
git commit -m ""Changes of feature1""
git push origin module1 //Being in feature1 branch
</code></pre>

<p>What happens here to the code of feature1 to module1 branch and how module1 branch takes it.</p>

<p><strong>My Understaing</strong> : As per my understanding the feature1 changes will be pushed to module1 branch. And later I realised that I should have pushed to feature1 branch then I will push the same to feature1 branch and will checkout to module1 branch and revert the code which I had pushed recently.</p>

<p><strong>Query 2</strong> : What if being in the feature1 branch and I pull the code of module1 in this branch by the following</p>

<pre><code>git pull origin module1 //Being in feature1 branch
</code></pre>

<p><strong>My Understanding</strong> : The changes of the module1 code will get merged to my feature1 branch and is same as the following in commands</p>

<pre><code>git checkout moduel1
git pull origin module1
git checkout feature1
git merge module1
</code></pre>

<p>If there are any conflicts will be shown. Which I need to resolve.</p>

<p>Can anyone help me whether my understanding is correct or not. If not please help me to understand this concept properly. Thanks in advance.</p>",3,0,2017-12-21 09:45:28.857000 UTC,3.0,2017-12-21 09:51:18.377000 UTC,5,git|version-control|dvcs,9894,2012-09-25 08:21:27.430000 UTC,2022-03-05 07:39:11.370000 UTC,"Bangalore, Karnataka, India",2532,494,4,273,"<p>You have, I think, a few broad misconceptions about branch names and the use of <code>git pull</code>.  Let me split this into several parts, and give you this executive summary overview to start with:</p>

<ul>
<li><code>push</code>'s counterpart is not <code>pull</code>, it's <code>fetch</code>;</li>
<li><code>git pull</code> just runs <code>git fetch</code> followed by a second Git command, usually <code>git merge</code>, and I believe it's best for new Git users to avoid <code>git pull</code>, using instead the two separate commands;</li>
<li>while <code>push</code> and <code>fetch</code> use names for transferring hash IDs, it's the <em>commits</em>, identified by hash IDs, that matter; and</li>
<li>for <code>git merge</code> or <code>git rebase</code>, your current branch <em>does</em> matter.  During push or fetch, the current branch does not matter, but if you use <code>git pull</code>, that runs <code>git merge</code> or <code>git rebase</code>, and <em>now</em> the current branch matters.</li>
</ul>

<p>Now, let's dive into the details.</p>

<h3>A branch name is only a pointer to a (single) commit</h3>

<p>Git is all about <em>commits</em>.  Git would, in a sense, ""like it"" if us mere humans did not need branch names, and just talked about commits by hash ID all the time.  I might ask you if you are using commit <code>95ec6b1b3393eb6e26da40c565520a8db9796e9f</code>, and you would say ""yes"" or ""no, but I have that one"" or ""no, and I have not heard of that one yet"".</p>

<p>You mention that:</p>

<blockquote>
  <p>module1 is branched from master and feature1 is branched from module1.</p>
</blockquote>

<p>but in Git's eyes, branches don't branch <em>from</em> another <em>branch</em>.  Instead, each commit links to a previous, or <em>parent</em>, commit.  You drew this:</p>

<blockquote>
<pre><code>master
 ---------- 
         | module1
         ----------
                  | feature1
                  ------------
</code></pre>
</blockquote>

<p>which suggests to me that you think of commits as belonging to (or being ""on"") <em>only one branch</em>.  That's not how Git sees them, though.  Instead, most commits are on <em>many branches at the same time</em>.  For instance, consider a graph that we might draw like this:</p>

<pre><code>          o--o--o   &lt;-- br1
         /
...--o--o--o   &lt;-- master
         \
          o--o   &lt;-- br2
</code></pre>

<p>where each round <code>o</code> represents a commit.  All the commits down the middle row are on <code>master</code>, but <em>most</em> of those commits are <em>also</em> on <code>br1</code> and <code>br2</code>.  The <em>last</em> (newest and right-most) commit on <code>master</code> is <em>only</em> on <code>master</code>; the rest are on the other branches as well.</p>

<p>This is all because, in Git, a branch <em>name</em> like <code>master</code> points only to <em>one</em> commit.  The commit to which the name points is the one furthest to the right, if we draw the commit graph this way, left (earlier) to right (later).  Git calls this the <em>tip</em> commit, or sometimes the <em>head</em> (note lowercase here) of the branch.  To find the rest of the commits that you—or Git—can reach from this tip commit, Git will look up the commit by its big ugly hash ID, such as <code>95ec6b1...</code>.  Again, it's the <em>hash ID</em> that lets Git find the commit.  The <em>name</em> just lets Git find this hash ID!</p>

<p>The commit itself stores a parent commit hash ID, so Git will look up the parent commit by <em>its</em> hash ID, finding the commit one step back.  That commit has another parent ID, and so on.  Walking backwards through this sequence of parent hash IDs, one commit at a time, from later to earlier, is how we see <code>git log</code>, for instance.</p>

<p>If you run:</p>

<pre><code>git checkout br1
</code></pre>

<p>and then do some work and then run:</p>

<pre><code>git add -a &amp;&amp; git commit
</code></pre>

<p>and make a <em>new</em> commit—let's draw this as <code>*</code> instead of <code>o</code> so that we can see it—here's what happens to the branch name <code>br1</code>:</p>

<pre><code>          o--o--o--*   &lt;-- br1 (HEAD)
         /
...--o--o--o   &lt;-- master
         \
          o--o   &lt;-- br2
</code></pre>

<p>We draw this with the <code>(HEAD)</code> (note: all-uppercase) added to remember which name we gave to <code>git checkout</code>.  The <em>new</em> commit <code>*</code> goes in the graph, and points backwards to whichever commit <em>was</em> the tip of <code>br1</code>.  Meanwhile Git <em>changes</em> the name <code>br1</code> so that it points to the new commit we just made.  This is how branches grow, in Git: we add new commits to the graph, and Git updates the <em>name</em> that <code>HEAD</code> is attached-to.</p>

<p>Of course, it's no accident that if we start at the tips of both <code>br1</code> and <code>master</code> and work backwards, we'll eventually come back to a single meeting-point commit.  But this confluence of commits doesn't arise from the <em>names</em>.  It doesn't matter <em>how we choose to start from</em> the tip commit of <code>br1</code> and the tip commit of <code>master</code>; what matters is <em>these two particular commits, and then each commit we find along the way</em>.</p>

<p>The branch names, in other words, get us started in the commit graph.  <strong><em>It's the commit graph that matters most.</em></strong>  The names just serve as starting points!</p>

<h3>The opposite of <code>push</code> is <code>fetch</code></h3>

<p>Everything above is about working within a single Git repository.  But when we do work with Git, we work with more than one repository.  In particular, we have <em>our</em> repository, where we do our own work, but then we often need to talk with another Git repository stored on some other machine, like those provided by GitHub, or by an employer or a friend or colleague.</p>

<p>This means that we want to <em>share</em> commits.  As we saw above, Git is all about the commits and the commit graph—but we mere humans need <em>names</em> to get us started.  This is where <code>git fetch</code> and its counterpart, <code>git push</code>, come in.  Running either command connects our Git to some other Git.</p>

<p>Our Git has all the commits we have, some of which may be commits that we made.  Some of our Git's commits might be commits we got from elsewhere.  Similarly, their Git has all the commits they have, some of which are the same as commits we have.  Some may be different commits.  But in any case, <em>all</em> of these commits are identified by their unique hash IDs.  These IDs are the same in <em>every</em> Git, all around the world, if they have the same commits.  If they don't have our commits (because we just made them), our new commits have IDs that are different from every commit ID they have!  (This seems like magic, but it's just cryptographic mathematics—some of which is similar to what's behind Bitcoin, for instance, though Git uses a weaker set of hashes.)</p>

<p>In the end, this means that each of these two Gits can tell which commits one of us has that the other doesn't, just by looking at these hash IDs.  That's how our Git can give their commits we have that they don't—<code>git push</code>—or their Git can give our Git commits they have that we don't: <code>git fetch</code>.</p>

<p>Once they have sent commit objects (and other related Git objects needed to make those commits complete), the two Gits then need to set up <em>names</em> for any new <em>tip</em> commits.  This is the second place that branch names start to matter.</p>

<p>The fetch direction is simpler.  Your Git calls up some other Git.  Usually we use the name <code>origin</code> to stand for some URL, where the other Git is listening for calls, so we run <code>git fetch origin</code>.  Your Git calls up that Git, and asks it: <em>What are your branch tip names?  What hash IDs are those?</em>  Their Git tells your Git about its branch tips and hash IDs, and your Git either says: <em>Ah, I have that hash ID</em> or <em>Hmm, I don't have that hash ID, please send me that commit, and by the way what's its parent hash ID because maybe I need that one too</em>, and so on.</p>

<p>Eventually, your Git has all the commit and other hashed objects that they suggested.  Now your Git takes their branch names, like their <code>master</code>, and <em>saves those names</em>.  But you have your own branches.  Your Git cannot save those names as <em>your</em> branches.  It <em>renames</em> all those names.  Their branches, like <code>master</code>, become your Git's <em>remote-tracking names</em>, like <code>origin/master</code>.  Note that your Git is simply using your short-hand name, <code>origin</code>, plus a slash, in front of their branch names.</p>

<p>Once <code>git fetch</code> finishes, your Git now remembers where <em>their</em> Git's branch tips were, using <em>your</em> <code>origin/*</code> remote-tracking names.  You have their commits, plus any required associated stuff so that you can check out those commits and get the files that go with them, but any <em>new</em> commits are only going to be <em>found</em> via these remote-tracking names.  If they have, as branch tip commits, some of your <em>older</em> commits, you might already have other ways to find them.</p>

<p>The counterpart to <code>git fetch</code> is <code>git push</code>, but it's not completely symmetric.  To do a <code>git push</code> to <code>origin</code>, for instance, you have your Git call up their Git as before, but this time, you want to <em>send</em> them things.  The first part of this sending operation is to hand over any commits that you have, that they don't, that they will need.  You identify these commits by having your <code>git push</code> take some extra arguments:</p>

<pre><code>git push origin module1:module1
</code></pre>

<p>Note that I've put in the same name twice here.  The name on the left, the <code>module1:</code>, is to find the <em>specific commit hash</em> that you want to send to them.  That's the <em>tip commit</em> of your <code>module1</code>.  The name on the right, the <code>:module1</code> part, is the <em>name you want them to use</em>.  (These names don't have to be the same!  But it gets tricky if you use different names on each side, so avoid that if possible.)</p>

<p>When you run <code>git fetch origin</code>, you generally want to pick up <em>everything</em> they have that you don't.  That's quite safe, because whatever tip commit they have as their <code>master</code>, your Git will call your <code>origin/master</code>.  Whatever tip commit they have as their <code>module1</code>, your Git will call that your <code>origin/module1</code>.  Your remote-tracking names are your own private entries, all reserved for that one remote named <code>origin</code>, and it's harmless, or even a good thing, to just get them all up to date right away.<sup>1</sup></p>

<p>But <code>git push</code> doesn't work this way.  You send them a commit hash ID, and then ask them to set their <em>branch</em>, <code>master</code> or <code>module1</code> or <code>feature1</code>, to that hash ID.  They have your Git send the commit object (and as many parent commits and other objects they need, all identified by hash IDs) if they don't already have that commit-ID, and then they evaluate for themselves <em>whether they will let you set their branch name</em>.  Sometimes they will, and your push succeeds; sometimes they find a rule that says ""don't allow this"", and your push fails.</p>

<p>Note that their rules are up to them!  You send a request (""please set your <code>module1</code> to <code>a9fc312...</code>""); they can look at their current <code>module1</code> branch hash ID, and the commit you've sent if it's new, and <em>choose</em> whether or not to take the request.  You can use <code>--force</code> to send it as a command, but even if you do that, they can still choose whether to obey the command.  All you can do is make a request (or forceful command) and see whether they accept it.  But there's a standard rule that most Gits use most of the time: a request is allowed if the only thing it does is <em>add new commits</em>.</p>

<p>Look what happened when you added a commit to <code>br1</code> above.  The existing commits, which were all <em>reachable from</em> the original tip commit, were still reachable from the new tip commit.  The new commit ""grew the branch"".  It did not change any of the existing—no new commit <em>can</em> ever change any existing commit—but the new commit's parent was the old tip commit.  If we start at the new tip and work backwards, as Git does, we arrive at the old tip.</p>

<p>The same rules work well with <code>git push</code>: if the request you send to the other Git will keep all its existing commits reachable from the new branch tip, the other Git will probably allow the request.</p>

<p>Note that we've been using <code>git push origin module1:module1</code>, but you suggested that we run:</p>

<blockquote>
<pre><code>git push origin module1 //Being in feature1 branch
</code></pre>
</blockquote>

<p>It doesn't matter which branch we're <em>in</em> (or <code>on</code>, as <code>git status</code> would say <code>on branch feature1</code>).  What matters here is the two <code>module1:module1</code> parts to the <code>git push</code> command.  That tells our Git: use <em>our</em> name <code>module1</code> to find the commit to push, and ask their Git to set <em>their</em> <code>module1</code> name.</p>

<p>We didn't give two parts.  We only gave one part.  For <code>git push</code>, if you only give one part, Git just assumes you mean ""use that same name twice"".  So <code>git push origin module1</code> means <code>git push origin module1:module1</code>, after all.</p>

<h3><code>git pull</code> is <code>git fetch</code> followed by a second Git command</h3>

<p>You then asked about this:</p>

<blockquote>
<pre><code>git pull origin module1 //Being in feature1 branch
</code></pre>
</blockquote>

<p>What <code>git pull</code> does is simple to describe:</p>

<ol>
<li><p>It runs <code>git fetch</code> with most of the arguments you pass it:</p>

<pre><code>git fetch origin module1
</code></pre></li>
<li><p>If that works, it runs a second Git command, which you pick in advance.  The default command to run is <code>git merge</code>.  The exact arguments to this second command depend a little bit on what came in during the <code>git fetch</code>, but you still have to pick <code>git merge</code> vs <code>git rebase</code> <em>before</em> you get to see what came in.</p></li>
</ol>

<p>We mentioned before that we normally just run <code>git fetch origin</code>, which brings over all the names <code>origin</code> has, and your Git then renames.  If you run:</p>

<pre><code>git fetch origin module1
</code></pre>

<p>this simply <em>limits</em> the fetch: it finds out what they have as their <code>module1</code>, brings over the commit(s) by ID if necessary, and then sets your <code>origin/module1</code>.<sup>2</sup>  It ignores all their other names.  The <code>module1</code> here is a lot like in <code>git push</code>, except that if you don't use two names—if you don't write <code>module1:module1</code>—your Git will just update your remote-tracking name.  (And you should rarely if ever use two names separated by a colon here.  It does work and can be used for some purposes, but you will need to learn more details.)</p>

<p>During the <code>git fetch</code>, it doesn't matter which branch you have checked out.  But the <em>second</em> Git command is either <code>git merge</code> or <code>git rebase</code>, and for these two commands, it <em>does</em> matter which branch you have checked out.</p>

<p>The second command that <code>git pull</code> runs, runs <em>without changing the current branch</em>.  Assuming the second branch is <code>git merge</code>, the arguments are:</p>

<ul>
<li>the branch tip hash ID<sup>3</sup> obtained from the remote Git, and</li>
<li>the message ""merge branch '<em>name</em>' of <em>url</em>""</li>
</ul>

<p>This is a lot like running <code>git merge origin/<em>name</em></code>, though the message is a bit different.</p>

<p>What <code>git merge</code> does with this is itself a bit complicated, since <code>git merge</code> sometimes does nothing, sometimes does a <em>fast-forward</em> instead of merging, sometimes does a real merge, and sometimes has merge conflicts that make the merge stop in the middle and get help from you.  I'll leave all of this to other answers.</p>

<p>Let's summarize here:</p>

<ul>
<li><p>Your <code>git push</code> and <code>git fetch</code> operations always transfer <em>whole commits</em>.  They never work on individual files: they copy <em>commits</em> from one Git repository to another.  The process of copying commits normally involves setting some name(s) in the receiving Git, so as to remember any adjusted tip commits.  For the most part, these don't care about your current branch (though <code>git push</code> can be told to push the current branch by default: see <code>push.default</code>).</p></li>
<li><p>Your <code>git pull</code> simply runs two separate Git commands.  If you're not already very familiar with both of those commands, I suggest you run each command separately instead.</p></li>
<li><p>Your <code>git merge</code> command is the most complicated one, and should be a separate question.  If you don't think you ran a <code>git merge</code> command, see <code>git pull</code> above.</p></li>
</ul>

<hr>

<p><sup>1</sup>The one drawback to getting <em>everything</em> is that it could take a long time, if there is a lot of ""everything"" and your net connection is slow.  On the other hand, if you get everything today, you will have almost nothing to get tomorrow, so the <em>next</em> <code>git fetch</code> will go fast.  If you selectively get just a little today, maybe tomorrow's <code>git fetch</code> will be slow.</p>

<p><sup>2</sup>This assumes your Git version is at least 1.8.4.  In Git before 1.8.4, <code>git fetch origin module1</code> brings over their hash ID, but then throws away the name, failing to update your own <code>origin/module1</code>.</p>

<p><sup>3</sup>If you run <code>git pull origin name1 name2</code>, Git passes multiple hash IDs to <code>git merge</code>, which produces what Git calls an <em>octopus merge</em>.  This is almost never what you want, so avoid that.  If you avoid <code>git pull</code> you won't make this particular mistake!</p>",2.0,2017-12-21 21:44:19.430000 UTC,,14.0,[]
What parts of a Xamarin solution should be kept out of the repository?,"<p>I'm working with a designer on a Xamarin.Android application and we are having trouble keeping in sync. We use Mercurial, but my question should be as relevant for Git or any other DVCS. My .hgignore file looks like this:</p>

<pre><code>^bin/
^obj/
^Components/
\.keystore$
</code></pre>

<p>I am unsure about these files:</p>

<pre><code>^Resources/Resource.designer.cs
\.sln$
\.userprefs$
\.csproj$
</code></pre>

<p>Thanks!</p>

<p><strong>Update</strong><br>
Found this great community wiki that confirmed excluding *.pidb and
*.userprefs and by implication confirms including *.sln and *.csproj<br>
See: <a href=""https://stackoverflow.com/questions/2143956/gitignore-for-visual-studio-projects-and-solutions"">.gitignore for Visual Studio Projects and Solutions</a></p>

<p>I'm not sold on including the Resource.designer.cs file. It gets automatically regenerated after changes to resources and it includes this notice:</p>

<pre><code>// ------------------------------------------------------------------------------
//  &lt;autogenerated&gt;
//      This code was generated by a tool.
//      Mono Runtime Version: 4.0.30319.17020
// 
//      Changes to this file may cause incorrect behavior and will be lost if 
//      the code is regenerated.
//  &lt;/autogenerated&gt;
// ------------------------------------------------------------------------------
</code></pre>

<p>With a merge you might reject an id name change in a layout file, but accept this file containing the new name.</p>",2,2,2013-08-08 06:31:28.490000 UTC,1.0,2017-05-23 11:54:06.110000 UTC,18,version-control|mercurial|xamarin.android|xamarin|dvcs,7124,2009-04-28 07:36:27.060000 UTC,2022-03-04 15:44:58.863000 UTC,"Cape Town, South Africa",24358,3094,9,1289,"<p>To answer your question, you're wondering about this:</p>

<pre><code>^Resources/Resource.designer.cs
\.sln$
\.userprefs$
\.csproj$
</code></pre>

<p>The <code>Resource.designer.cs</code> file should definitely not be ignored. Sure, it'll get regenerated in some cases, especially if you alter the resx file it is generated from, but just committing it to version control removes that hassle from everyone else, they don't <em>need</em> to regenerate it.</p>

<p>The <code>.sln</code> and <code>.csproj</code> files should <strong>definitely not</strong> be ignored. The solution file contains which projects are part of the solution, and the csproj file contains the project information, both of which you definitely need to commit.</p>

<p><code>.userprefs</code> however is a local file, it contains the user preferences for the user using the machine, you do not want that in version control.</p>

<p>Here is my own Xamarin ignore file, with comments below:</p>

<pre><code>syntax: glob
_ReSharper.*/
[Bb]in/
[Oo]bj/
*.user
*.userprefs
*.suo
.DS_Store
</code></pre>

<p>The first line tells Mercurial to use glob library to handle file masks.</p>

<p>The next line ignore ReSharper temporary files, which are added to their own subdirectory according to project or solution name. Note that you can ask ReSharper to use the temp folder on the machine for this, instead of storing it in the solution directory, but this may be a per-user setting so to ensure those folder doesn't accidentally gets committed, just ignore that folder.</p>

<p>Then I ignore bin and obj folders, handling both upper and lowercase first letter, ie. Bin is ignored, as well as bin.</p>

<p>Then I ignore everything with the extension user, userprefs, and suo, since all of those store local user preferences.</p>

<p>The last entry ignore the Mac resource folder.</p>

<p>If you have NCrunch, TeamCity, or FinalBuilder, there are additional entries you would probably want to add:</p>

<pre><code>_NCrunch_*/
_TeamCity.*/
*.fb7lck
*.fbl7
*.fbpInf
</code></pre>",4.0,2013-08-08 13:06:26.067000 UTC,,18.0,[]
How is dvcs (git/mercurial) branching and merging support better than svn's?,<p>Lots of articles on dvcs systems claim superior branching and merging support as one reason to move from svn to dvcs systems. How exactly do these systems do branching and merging differently that makes it better?  </p>,6,5,2011-02-17 22:30:47.200000 UTC,,,13,svn|git|version-control|mercurial|dvcs,1120,2009-06-07 01:50:25.733000 UTC,2022-03-03 15:25:50.660000 UTC,"Washington, DC",69962,4430,441,3370,"<p>Historically, the difference between merge-tracking in git and svn was this: git has merge-tracking, and until version 1.5, svn didn't. At all. If you wanted to make a merge you had to always specify exactly what changes were to be merged, and if you merged one branch into another more than once, you would have to manually keep track of which revisions had and hadn't been merged, and manually select only the changes that hadn't been merged yet, to avoid conflicts. Good luck with that if you ever cherry-picked any changes.</p>

<p>Beginning with version 1.5 (released in 2008), if your client, server, and repository are all up-to-date, then svn is capable of acting a lot more intelligently; it uses properties to keep track of where a branch came from and what changes have already been merged into it. The upshot is that in many cases you can just <code>svn merge BRANCHNAME</code> and have the right thing happen. But due to its ""bolted on"" nature it's still not very fast and <a href=""http://subversion.tigris.org/issues/show_bug.cgi?id=2897"" rel=""noreferrer"">not entirely robust</a>. Git, on the other hand, <em>has</em> to handle merge scenarios well because of its DVCS nature, and it was designed from the beginning with data structures (like the particular kind of DAG it uses) and algorithms (such as recursive-merge and octopus-merge) that are suited to the task.</p>",0.0,2011-02-17 23:23:57.920000 UTC,2011-02-18 04:51:20.553000 UTC,11.0,[]
Is there a way to remove the history for a single file in Mercurial?,"<p>I think I already know the answer to this but thought I would ask anyway:</p>

<p>We have a file that got added to a Mercurial repository with sensitive information in it. Is there any way to remove that file along with its change history without removing the whole repo?</p>",5,1,2009-06-22 21:24:51.607000 UTC,8.0,2012-01-26 09:12:16.720000 UTC,50,version-control|mercurial|dvcs,12509,2009-01-24 03:47:28.377000 UTC,2022-03-03 07:12:43.270000 UTC,"Dallas, TX",7064,1163,18,505,"<p>No, you can't. Read the <em><a href=""http://hgbook.red-bean.com/read/finding-and-fixing-mistakes.html#sec:undo:aaaiiieee"" rel=""noreferrer"">changes that should have never been</a></em> section of the mercurial red book about it; and particularly the <em><a href=""http://hgbook.red-bean.com/read/finding-and-fixing-mistakes.html#id394667"" rel=""noreferrer"">what about sensitive changes that escape</a></em> subsection, which contains this paragraph:</p>

<blockquote>
  <p>Mercurial also does not provide a way
  to make a file or changeset completely
  disappear from history, because there
  is no way to enforce its
  disappearance; someone could easily
  modify their copy of Mercurial to
  ignore such directives. In addition,
  even if Mercurial provided such a
  capability, someone who simply hadn't
  pulled a “make this file disappear”
  changeset wouldn't be affected by it,
  nor would web crawlers visiting at the
  wrong time, disk backups, or other
  mechanisms. Indeed, no distributed
  revision control system can make data
  reliably vanish. Providing the
  illusion of such control could easily
  give a false sense of security, and be
  worse than not providing it at all.</p>
</blockquote>

<p>The usual way to revert committed changes is supported by mercurial through the <code>backout</code> command (again, mercurial book: <a href=""http://hgbook.red-bean.com/read/finding-and-fixing-mistakes.html#id392218"" rel=""noreferrer""><em>dealing with committed changes</em></a>) but the information does not disappear from the repository: since you never know who exactly cloned your repository, that would give a false sense of security, as explained above.</p>",2.0,2009-06-23 03:07:47.857000 UTC,,24.0,[]
How can multiple people contribute to resolving conflicts during a large DVCS rebase operation?,<p>A rebase of a very long-lived topic branch can be quite painful and take hours or days of work for one person. How can a second (or third) person contribute to the rebasing effort? Is there some tool that can find independent conflicts that two people could work on independently?</p>,1,0,2011-03-23 19:49:52.413000 UTC,6.0,2011-03-23 22:05:06.787000 UTC,9,git|mercurial|dvcs|rebase,830,2009-06-14 19:29:14.573000 UTC,2019-07-02 18:11:10.480000 UTC,"Portland, OR",307,48,2,25,"<p>turn on rerere</p>

<pre><code>git config --global rerere.enabled 1
git config --global rerere.autoupdate true
</code></pre>

<p>whenever you resolve a conflict, the resolution will be recorded. Now you have to share it:</p>

<p>Set up a symlink to the <code>.git/rr-cache</code> directory. Initialize that as a repository. Have the other devs do that.</p>

<p>Now you can share the resolutions across the team via this resolution repository.</p>

<p>here is more on rerere: <a href=""http://progit.org/2010/03/08/rerere.html"" rel=""noreferrer"">http://progit.org/2010/03/08/rerere.html</a></p>

<p>You may also rethink your strategy of rebasing. Merging is a better option for certain situations.</p>

<p>Hope this helps.</p>",2.0,2011-03-23 20:00:35.747000 UTC,2011-03-23 20:07:40.867000 UTC,9.0,[]
git vs mercurial performance,"<p>Do any performance benchmarks exist? </p>

<p>I'm looking to create a repo and commit/ push for legacy code that runs several gigs deep. </p>

<p>Is either faster / footprint etc? </p>

<p>I apologize if this is too vague...</p>",5,0,2011-03-14 06:44:15.357000 UTC,4.0,2012-02-24 19:57:14.483000 UTC,18,performance|git|mercurial|benchmarking|dvcs,7690,2010-12-19 17:31:56.077000 UTC,2015-07-21 22:12:06.007000 UTC,,5229,47,1,262,"<p>Original Answer (March 2011, GitHub had less than 3 years)</p>

<p>The correct performance to measure about a DVCS (which performs all operations locally anyway) is the one about your daily tasks:</p>

<ul>
<li><strong>merge</strong> (how quickly do you decide between the <a href=""https://stackoverflow.com/questions/3766657/using-mercurial-to-separate-three-versions-official-development-testing/3766692#3766692"">various branching models</a>, especially in Mercurial?)</li>
<li><strong><a href=""https://stackoverflow.com/questions/2563836/sell-me-distributed-revision-control/2563917#2563917"">publication workflow</a></strong> (how quickly do you setup one <a href=""http://progit.org/book/ch5-1.html"" rel=""nofollow noreferrer"">push/pull worlflow</a>?)</li>
<li><strong>integration</strong> (how quickly do you integrate Git with IDE, with webapp like Hudson or Jira or Redmine or Track, or ...?)</li>
<li><strong>setup</strong> (how quickly do you setup a centralized repository, with what kind of authentication mechanism: that matters if you use a <a href=""https://stackoverflow.com/questions/3597747/can-we-finally-move-to-dvcs-in-corporate-software-is-svn-still-a-must-have-for/3597851#3597851"">DVCS in an enterprise environment</a>)</li>
</ul>

<p>The raw performance of basic operations isn't that relevant, provided you understand the <a href=""https://stackoverflow.com/questions/3676038/fetch-pull-part-of-very-large-repository/3676163#3676163"">limits of a DVCS</a>: you cannot have one single repo into which you would put <em>everything</em> (all projects, or all kind of files like binaries).<br>
Some kind of modules reorganization must take place to define the right amount of repo per ""modules"" (coherent file sets).</p>

<hr>

<p>Update 2018, seven years later: The Windows support for Git is now a reality, and aim at improving perfomance/scalability of Git.</p>

<p>To illustrate that, Microsoft has its <em>entire</em> Windows codebase into <em>one</em> (giant) Git repository: See ""<a href=""https://blogs.msdn.microsoft.com/bharry/2017/05/24/the-largest-git-repo-on-the-planet/"" rel=""nofollow noreferrer"">The largest Git repo on the planet</a>"": 3.5M files, 300GB, 4,000 engineers producing 1,760 daily “lab builds” across 440 branches in addition to thousands of pull request validation builds.<br>
<em>But</em> this is with the addition of <a href=""https://en.wikipedia.org/wiki/Git_Virtual_File_System"" rel=""nofollow noreferrer"">GVFS (Git Virtual FileSystem)</a>, which  allows to dynamically download only the portions you need based on what you use.<br>
This is <em>not</em> yet in Git native, although <a href=""https://stackoverflow.com/a/48012038/6309"">its integration has begun last Dec. 2017, with the implementation of a narrow/partial cloning</a>.</p>",2.0,2011-03-14 07:06:28.540000 UTC,2018-06-03 22:39:34.523000 UTC,10.0,[]
Comparing the pros and cons of Bitbucket to Github,"<p><sup><em>Disclaimer: This is a subjective question. Please follow <a href=""https://blog.stackoverflow.com/2010/09/good-subjective-bad-subjective/"">relevant guidelines.</a></em></sup></p>
<p>I am considering the migration of source code from a traditional <code>VCS</code> to a <code>DVCS</code>. Since having a decent <strike>GUI</strike> web-based frontend and workflow tools are a must, the two obvious candidates are <a href=""https://bitbucket.org"" rel=""nofollow noreferrer"">Bitbucket</a> and <a href=""https://github.com/"" rel=""nofollow noreferrer"">Github</a>.</p>
<p>Unfortunately I have no in-depth experience with either of them, so I'd really like to have a few second opinions before taking the dive. Especially welcome would be observations <em>from a team/business perspective</em> regarding day-to-day usage and features that have an impact on productivity.</p>
<p><strong>What is Your experience with either of them? Any particular highlights or annoyances?</strong></p>",4,9,2011-08-11 10:43:33.717000 UTC,5.0,2021-01-18 12:38:11.483000 UTC,41,git|github|bitbucket|dvcs,30336,2010-08-20 13:19:14.650000 UTC,2022-03-03 09:35:41.660000 UTC,,17556,7180,132,971,"<p>You really need to answer a basic question first. Do you want to use Mercurial or Git. I had to make this decision earlier this year. Mercurial was by far easier to setup and start using. I ultimately chose Git for the following reasons:</p>

<ul>
<li>Most OpenSource Projects are moving from SVN to Git</li>
<li>Git allowed the most flexibility in whatever I wanted to do. (This is the main reason)</li>
<li>Third-party integration</li>
</ul>

<p>If your business might need complicated version control processes go with Git. The learning curve is steep but it will be easier to do what you really want at the end of the day. I will qualify what I mean by 'steep'. The difference between teaching someone Mercurial vs. Git, is trying to get a Windows user to learn Linux command lines. </p>

<p>If you want a quick easy to use DVCS in which you think simple branching and versioning is all you will ever need, don't kill yourself with Git, use Mercurial. But keep in mind most people you talk to in Mailing lists, IRC, etc. will have experience with Git not Mercurial.</p>

<p>If you plan on paying for GitHub or BitBucket, I suggest you also look at <a href=""http://www.fogcreek.com/kiln/"" rel=""noreferrer"">Kiln</a>. FogCreek has made mercurial even easier.</p>",4.0,2011-08-11 11:04:32.270000 UTC,,14.0,[]
How to specify cluster init script for spark Job,"<p>My job needs some init scripts to be executed on cluster, presently i am using ""Existing Interactive Cluster"" option in job creation and have specified init script for the cluster. But this is getting charged as higher ""Data analytics workload"".</p>

<p>is there an option that i can specify ""New Automated Cluster"" option in job creation page and still get the init scripts executed for new cluster.  I am not sure if it recommended to use <a href=""https://docs.databricks.com/user-guide/clusters/init-scripts.html#global-init-scripts"" rel=""nofollow noreferrer"">Global Init script</a>, since not all jobs needs those init script, only specific category of jobs need init script.</p>",1,0,2019-09-17 21:12:25.470000 UTC,1.0,,0,spark-structured-streaming|azure-databricks,892,2018-02-01 02:06:45.167000 UTC,2022-03-03 18:36:45.980000 UTC,,163,0,0,28,"<p>To fine tune Spark jobs, you can provide custom Spark configuration properties in a cluster configuration.</p>

<p>To set Spark properties for all clusters, create a global init script:</p>

<pre><code>%scala
dbutils.fs.put(""dbfs:/databricks/init/set_spark_params.sh"",""""""
  |#!/bin/bash
  |
  |cat &lt;&lt; 'EOF' &gt; /databricks/driver/conf/00-custom-spark-driver-defaults.conf
  |[driver] {
  |  ""spark.sql.sources.partitionOverwriteMode"" = ""DYNAMIC""
  |}
  |EOF
  """""".stripMargin, true)
</code></pre>

<p>Reference: ""<a href=""https://docs.databricks.com/user-guide/clusters/spark-config.html"" rel=""nofollow noreferrer"">Spark Configuration</a>"".</p>

<p>Hope this helps.</p>

<hr>

<p>If this answers your query, do click “Mark as Answer” and ""Up-Vote"" for the same. And, if you have any further query do let us know.</p>",1.0,2019-09-18 10:02:15.633000 UTC,,-1.0,[]
"Using Mercurial locally, only with Subversion server","<p>We are using a Subversion server at my job for source control.  I was thinking that rather than keeping up with my own branch, I would run Mercurial on my workstation, commit locally, and then commit to the Subversion trunk whenever I’m done with whatever feature I’m working on.</p>

<p>From my understanding of DVCS this is theoretically possible.  Can anyone offer reference to any tutorials on this specific type of integration, or point to any tools that will make such a process as seamless as possible?</p>",2,0,2009-04-28 21:19:51.030000 UTC,6.0,2010-03-23 16:26:20.780000 UTC,21,svn|version-control|mercurial|dvcs,4295,2008-09-15 20:31:33.703000 UTC,2020-03-12 00:10:46.360000 UTC,"Seattle, WA, USA",25126,264,20,905,"<p>Have you looked at <a href=""https://www.mercurial-scm.org/wiki/WorkingWithSubversion"" rel=""nofollow noreferrer"">this page in Mercurial wiki</a> ?</p>",2.0,2009-04-28 21:22:37.857000 UTC,2017-06-20 09:05:38.463000 UTC,8.0,[]
Is it possible to pull from/push to SourceGear Vault repository using Mercurial?,<p>I noticed that this kind of functionality exists for subversion and it works very nicely. I was wondering if there is such thing for SourceGear Vault.</p>,1,0,2010-07-09 04:52:14.217000 UTC,4.0,2012-06-11 10:38:20.627000 UTC,7,mercurial|dvcs|sourcegear-vault,629,2008-09-08 06:22:18.500000 UTC,2019-09-02 12:55:39.727000 UTC,"Vilnius, Lithuania",5679,395,23,1138,"<p>Nope, I'm afraid we only have two-way bridges for <a href=""https://www.mercurial-scm.org/wiki/HgSubversion"" rel=""nofollow noreferrer"">Subversion</a> and <a href=""https://www.mercurial-scm.org/wiki/HgGit"" rel=""nofollow noreferrer"">Git</a>. I have not heard of anyone writing a bridge for SourceGear Vault.</p>

<p>However, you can still use Mercurial <em>on top of</em> the other system. This is a general technique that works for all version control systems (VCSs). What you do is the following:</p>

<p>Checkout the latest version of the code from your foreign version control system. Initialize a Mercurial repository, add all files, and make a commit:</p>

<pre><code># checkout foreign VCS
$ hg init
$ hg addremove
$ hg commit
</code></pre>

<p>The working copy is now both a Mercurial working copy <em>as well</em> as a working copy for the foreign system. You will be doing development in Mercurial and periodically import it into the foreign system, and you will periodically import changes from the foreign VCS into Mercurial.</p>

<p>We will use the branch called <code>default</code> to track the history of the foreign system, and a named branch called <code>hg</code> to track the development we do in Mercurial.</p>

<p><strong>Note:</strong> Anton comments below that Vault will show too many files if you use named branches to separate the two lines of development — use two clones instead if this is a problem for you.</p>

<p>Let us make the <code>hg</code> branch:</p>

<pre><code>$ hg branch hg
$ hg commit -m ""Started hg branch""
</code></pre>

<p>You can now develop something:</p>

<pre><code># work, work, work...
$ hg commit -m 'Fixed bug 42'
# work, hack, work...
$ hg commit -m 'Customers will love this feature!'
</code></pre>

<p>As you work along like this, the <code>default</code> branch will begin to diverge from the <code>hg</code> branch -- the difference is exactly the changes that have yet to be exported to the foreign system. You can see the differences with</p>

<pre><code>$ hg diff default:hg
</code></pre>

<p>To actually export the changes, you update to the <code>default</code> branch, merge <code>hg</code> into it and commit the change to your foreign system:</p>

<pre><code>$ hg update default
$ hg merge hg
$ hg commit -m 'Merge with hg'
# get list of renamed files:
$ hg status --added --copies --change . | grep -A 1 '^ '
# commit to foreign VCS
</code></pre>

<p>You can then update back to the <code>hg</code> branch and continue working with Mercurial</p>

<pre><code>$ hg update hg
# work, work, wok...
</code></pre>

<p>When changes are made by others in the foreign VCS, you have to merge them back into your <code>hg</code> branch. You first update to the <code>default</code> branch. This ensures that the working copy looks how the foreign VCS expects it to look like. You can then update the working copy -- this makes Mercurial see changes, which you commit to Mercurial:</p>

<pre><code>$ hg update default
# update working copy using foreign VCS
$ hg addremove --similarity 90
$ hg commit -m 'Imported changes from foreign VCS'
</code></pre>

<p>The <code>hg addremove</code> step makes sure that Mercurial picks up any renames that has taken place in the foreign VCS. You will need to experiment with the similarity parameter to find a setting that suits you. Use <code>hg status -C</code> to see the scheduled renames.</p>

<p>You now need to merge these changes back into the <code>hg</code> branch so that you can incorporate them in your further Mercurial-based work:</p>

<pre><code>$ hg update hg
$ hg merge default
$ hg commit -m 'Merge with default'
</code></pre>

<p>You continue working like this -- always making new local development on the <code>hg</code> branch, and always updating to the <code>default</code> branch before you use the foreign VCS commands (update, commit, etc).</p>

<p>I hope this guide can help you or someone else! :-)</p>",6.0,2010-07-09 13:51:17.697000 UTC,2018-02-27 14:27:10.140000 UTC,11.0,[]
Spark micro-batches from delta lake are very small,"<p>I'm reading appends to a delta table in Azure storage, and something strange is happening. The cluster is not under any real load, but the offset checkpoint advances very slowly. Looking at the individual offsets being written, the offset progress per batch is miniscule. For example, a streaming checkpoint that is 200 versions behind the end of the commit log writes an offset that catches it up only 1-3 versions, not all 200 (or 200 minus however many were written during the interval) (yes, the job is running very far behind - that's why I noticed).</p>
<p>For reference, the job that appends to the delta table produces a new version about every three minutes. The job that reads from the table runs on a one hour interval. Yet, the offsets produced by that job are between 1 and 3 versions, not the ~20 versions that should be consumed by a one-hour interval.</p>
<p>What's going on here? Is there a way I can see what the decision-making is behind the micro-batch size?</p>",1,0,2021-06-03 00:00:49.050000 UTC,,,0,apache-spark|spark-structured-streaming|azure-databricks|delta-lake,101,2019-07-23 08:17:44.903000 UTC,2022-03-04 20:59:17.463000 UTC,"Vancouver, BC, Canada",165,14,0,15,,,,,,[]
mgcv on Databricks: the value of k-index not identical to that from RStudio,"<p>I've developed a GAM model for some Beta-distributed variable in RStudio with the mgcv package and am now transfering the code (one-to-one) in an R-Notebook on Azure Databricks. </p>

<p>One of the many things one shuould check while working with such models is whether the dimension of the basis of smooth functions is appropriate. This is done in the mgcv-function k.check which computes the so-called k-index. </p>

<p>My problem: In both environments, I use the same dataset and the same model parameters while fittign my model. BUT the k-index which gets computed in the Databricks is different from the one computed in the RStudio!   </p>

<p>The call</p>

<pre><code> k.check(modell_mo, subsample = length(data_mo_train[,1])+ 1)
</code></pre>

<p>produces in RStudio the output </p>

<pre><code>                       k'      edf   k-index p-value
  s(Intervall_Nummer) 20 19.76659 0.9149797       0
</code></pre>

<p>and in Databricks </p>

<pre><code>                      k'      edf   k-index p-value
  s(Intervall_Nummer) 20 19.76659 0.9636656       0
</code></pre>

<p>Note that I set the subsample parameter to the size of the dataset in order to avoid the random subsetting which is otherwise used while computing the k-index. In particular, if I repeat the computations I always get the same k-index in each of the environments.</p>

<p>For the moment, I'm fine with not that brilliant values of edf and p-value.  </p>

<p>I tried to copy the code from the Databricks back to RStudio, but with no effect. The discrepancy is still there.</p>",0,7,2019-08-07 12:38:51.660000 UTC,,,1,r|rstudio|azure-databricks|gam|mgcv,92,2019-08-07 11:58:40.037000 UTC,2021-04-27 15:41:39.940000 UTC,,11,0,0,1,,,,,,[]
"Spark read delta table, getting NoSuchObjectException(message:There is no database named delta) error","<p>Reading delta format data using spark</p>
<pre><code> spark.sql(&quot;select * from delta.`/mnt/data/test`&quot;).createOrReplaceTempView(&quot;test&quot;)
</code></pre>
<p><code>test</code> view creates in spark program and I can use this view in joining. Program works fine. I can get the count of view</p>
<pre><code>spark.sql(&quot;select count(*) from test&quot;).show(false)
+--------+
|count(1)|
+--------+
|551     |
+--------+
</code></pre>
<p>But I am also getting below error logs</p>
<pre><code>21/08/14 13:55:52 ERROR RetryingHMSHandler: NoSuchObjectException(message:There is no database named delta)
at org.apache.hadoop.hive.metastore.ObjectStore.getMDatabase(ObjectStore.java:487)
at org.apache.hadoop.hive.metastore.ObjectStore.getDatabase(ObjectStore.java:498)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)
at com.sun.proxy.$Proxy47.getDatabase(Unknown Source)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:796)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
at com.sun.proxy.$Proxy48.get_database(Unknown Source)
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:949)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
at com.sun.proxy.$Proxy49.getDatabase(Unknown Source)
at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1165)
at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1154)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$databaseExists$1.apply$mcZ$sp(HiveClientImpl.scala:412)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$databaseExists$1.apply(HiveClientImpl.scala:412)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$databaseExists$1.apply(HiveClientImpl.scala:412)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:331)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$retryLocked$1.apply(HiveClientImpl.scala:239)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$retryLocked$1.apply(HiveClientImpl.scala:231)
at org.apache.spark.sql.hive.client.HiveClientImpl.synchronizeOnObject(HiveClientImpl.scala:280)
at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:231)
at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:314)
at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:411)
at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:279)
at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:279)
at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:279)
at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1$$anonfun$apply$1.apply(HiveExternalCatalog.scala:144)
at org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$maybeSynchronized(HiveExternalCatalog.scala:111)
at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1.apply(HiveExternalCatalog.scala:142)
at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:372)
at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:358)
at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:140)
at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:278)
at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.databaseExists(ExternalCatalogWithListener.scala:78)
at org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:265)
at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.isRunningDirectlyOnFiles(Analyzer.scala:767)
at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:692)
at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:730)
</code></pre>
<p>I don't know why this logs are getting? How to get rid of it?
platform :  Azure Databricks
Databricks Runtime Version: 6.4 Extended support
Thanks</p>",0,3,2021-08-14 14:26:53.100000 UTC,,2021-08-15 08:16:02.883000 UTC,2,apache-spark|spark-streaming|azure-databricks|delta-lake,199,2016-12-24 14:53:07.033000 UTC,2022-02-27 13:00:10.517000 UTC,"Mumbai, Maharashtra, India",3202,59,4,242,,,,,,[]
Folder level from GIT checkout on teamcity agent,"<p>We have been tasked to move our source code from perforce to bitbucket recently. our perforce repository is quite huge and has multiple modules which are segregated in different folders . So far we have been using the VCS and checkout rules in TC to checkout only the relevant codes an build it.
However i am not sure if there are any such possibilities with GIT given that folder level checkout is currently not supported by GIT Teamcity integration.</p>

<p>Can someone please let me know how best we can achieve it from teamcity ? Breaking down our repository into multiple ones is a huge task given the age and complexity of the code base</p>",0,1,2017-02-08 14:16:27.560000 UTC,2.0,,3,git|bitbucket|perforce|dvcs|teamcity-9.1,190,2013-12-03 15:29:55.637000 UTC,2017-05-18 14:42:49.970000 UTC,,39,0,0,9,,,,,,[]
What DVCS support Unicode filenames?,"<p>I'm interested in trying out distributed version control systems. git sounds promising, but I saw a note somewhere for the Windows port of git that says ""don't use non-ASCII filenames"". I can't find that now, but there is <a href=""http://code.google.com/p/msysgit/issues/detail?id=230"" rel=""noreferrer"">this link</a>. It's put me off git for now, but I don't know if the other options are any better.</p>

<p>Support for non-ASCII filenames is essential for my Japanese company. I'm looking for one that internally stores filenames as Unicode, not a platform-dependent encoding which would cause endless grief. So:</p>

<ol>
<li>What DVCS support Unicode filenames?</li>
<li>In both Windows and Linux?</li>
<li>Ideally, with the possibility to transfer repositories between Windows and Linux machines with minimal issues?</li>
</ol>",7,1,2009-05-06 13:52:41.567000 UTC,9.0,2009-05-06 14:00:25.650000 UTC,23,git|unicode|mercurial|dvcs|bazaar,4441,2009-01-23 00:24:46.680000 UTC,2022-03-05 11:00:04.100000 UTC,"Melbourne, Australia",39096,6191,151,3741,,,,,,[]
Issue in creating a table over a csv having multiline data,"<p>Hi I am trying to create a table over a location having a csv in Azure Databricks.</p>
<pre><code>CREATE TABLE `sch`.`tab` ( columnspec ) USING CSV 
LOCATION 'dbfs:/mnt/dev/test'
</code></pre>
<p>In /mnt/dev/test storage container, the csv file is uploaded.</p>
<p>Data is like</p>
<pre><code>SECT.dfs,NEWYORK,df,34342,null,null,&quot;23dsfdsdf
sadd: 
FROM:
Latitude: ... Longitude : ...
To:
abd&quot;,err,4353543,4,342,N/A,39,null,4,234,dsf.4545,AZQA,TEST1,61.8,90,0.07,0,N/A - 700,N/A - 700,3,0.18,0.2,0,54,N/A - 700,N/A - 700,33,0,455,xffdg,meme,5,0,243.234,-343.234,38,1,1,null,105029121,null,null,D4_700_BPD_9M,n/a - Macro &gt;500m,null,null,null,null,null,0,null,null,null,null,null,null,null,null,null,null,0,null,0.18,1,0,0,0,5765,0,E,0,null,5,6,AZQA,1.15,DIRECTIONAL,60,1.5,1.999,12.35,49,40,0,0,701304010HC001776,null,null,null,null,null,null,null,null,null,mid-west,ERP-ANT.0005245,null,null,0,D4_700_BPC_9M,null,null,null,null,null,null,null,0,410270,0,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,44.22,435.15,1,0,null,null,null,null,null,3,2,1,1,1,1,null,null,null,null,null,46,null,46,0,null,90,null,780,null,N/A - 700,1,0,null,0,14.64,null,null,null,null,null,null,null,20,0,CITY OF NEWYORK 911 &amp; POLICE,null,null,CITY OF NEWYORK,5675,bpc AB,211,780,8048,null,null,null,3451113553,null,null,null,null,null,null,null,null,null,null,null,2021-06-01T08:06:26.000+0000,2021-06-01T08:06:26.000+0000
</code></pre>
<p>Data is split into multiple lines. Need some help on this.</p>",0,0,2021-06-08 17:26:00.047000 UTC,,,0,azure-databricks,12,2013-03-12 04:29:36.407000 UTC,2022-03-06 01:52:55.413000 UTC,,1339,366,25,444,,,,,,[]
How to improve spark job performance in databricks,"<p>I am executing the below query on Databricks cluster. And storing the result into DBFS in parquet format.</p>
<p>Here  <em><strong>set1_interim</strong></em> and <em><strong>set2_interim</strong></em> are also parquet files having 450 million records each.</p>
<pre><code>
spark.sql(
          s&quot;&quot;&quot;
select *
from set1_interim
union all
select *
from set2_interim
&quot;&quot;&quot;).write.mode(&quot;overwrite&quot;).option(&quot;header&quot;, &quot;true&quot;).option(&quot;delimiter&quot;,&quot;\t&quot;).parquet(s&quot;${Interimpath}/unioned_interim&quot;)
</code></pre>
<p>But its taking so much of time to complete the job. If you see below picture out of 230 tasks 229 got completed soon. For last task it takes hours to complete.</p>
<p><a href=""https://i.stack.imgur.com/0p1RH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0p1RH.png"" alt=""enter image description here"" /></a></p>
<p>DAG for this job.</p>
<p><a href=""https://i.stack.imgur.com/o8dPH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o8dPH.png"" alt=""enter image description here"" /></a></p>
<p>In task page I can see this is running.
<a href=""https://i.stack.imgur.com/YcLW2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YcLW2.png"" alt=""enter image description here"" /></a></p>
<p>In executors all executors are alive. In running application I don't  know what is Databricks Shell in Name.
<a href=""https://i.stack.imgur.com/eWEkH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eWEkH.png"" alt=""enter image description here"" /></a></p>
<p>How can in make this job run faster. My cluster configuration are 1TB, 256 core.</p>",0,2,2022-03-03 15:42:21.217000 UTC,,2022-03-03 17:44:52.963000 UTC,0,apache-spark|query-optimization|azure-databricks,16,2019-02-07 08:01:16.953000 UTC,2022-03-05 12:09:35.997000 UTC,"Vijayawada, Andhra Pradesh, India",51,1,0,7,,,,,,[]
Unable to write dataframe to azure cosmo db,"<p>I am unable to write data to ****cosmos db**** using <em>databricks spark cluster</em>. However, I tried all the links and solutions in Stackoverflow and Github and tried all the possible jars with about every version.</p>

<p>The error stack is:</p>

<blockquote>
  <p></p>
</blockquote>

<p>java.lang.NoSuchMethodError:</p>

<blockquote>
  <p>com.microsoft.azure.documentdb.Offer.getContent()Lorg/json/JSONObject;
  at
  com.microsoft.azure.cosmosdb.spark.CosmosDBConnection.getCollectionThroughput(CosmosDBConnection.scala:163)
  at
  com.microsoft.azure.cosmosdb.spark.CosmosDBSpark$.save(CosmosDBSpark.scala:173)
  at
  com.microsoft.azure.cosmosdb.spark.CosmosDBSpark$.save(CosmosDBSpark.scala:501)
  at
  com.microsoft.azure.cosmosdb.spark.DefaultSource.createRelation(DefaultSource.scala:74)
  at
  org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)
  at
  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:72)
  at
  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:70)
  at
  org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:88)
  at
  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:143)
  at
  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
  at
  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:183)
  at
  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at
  org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:180)
  at
  org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:131)
  at
  org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:114)
  at
  org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:114)
  at
  org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:690)
  at
  org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:690)
  at
  org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:99)
  at
  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:228)
  at
  org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:85)k</p>
</blockquote>

<p>My code is :</p>

<pre><code>import org.joda.time._
import org.joda.time.format._

import com.microsoft.azure.cosmosdb.spark.schema._
import com.microsoft.azure.cosmosdb.spark.CosmosDBSpark
import com.microsoft.azure.cosmosdb.spark.config.Config

import org.apache.spark.sql.functions._

val configMap = Map(
  ""Endpoint"" -&gt; ""MY-CONNECTION-ENDPOINT"",
  ""Masterkey"" -&gt; ""MY-KEY"",
  ""Database"" -&gt; ""Families"",
  ""Collection"" -&gt; ""Families"",""Upsert""-&gt;""true"")

val config = Config(configMap)
val df = spark.range(5).select(col(""id"").cast(""string"").as(""value""))
df.write.mode(SaveMode.Overwrite).cosmosDB(config)
</code></pre>",0,0,2019-04-11 12:25:51.130000 UTC,,2019-04-11 13:05:54.533000 UTC,3,azure|apache-spark|azure-cosmosdb|azure-databricks,386,2019-04-11 12:17:56.750000 UTC,2019-06-28 10:44:06.020000 UTC,,31,0,0,1,,,,,,[]
unable to connect to neptune database instance from tinkerpop,"<p>I am following this guide <a href=""https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-console.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-console.html</a> to connect to my Neptune database but as mentioned in step 7 to configure Neptune endpoint <code>hosts: [your-neptune-endpoint]</code>, I was unable to connect using the cluster or reader endpoint
<a href=""https://i.stack.imgur.com/YZQjy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YZQjy.png"" alt=""enter image description here"" /></a></p>
<p>and unsure on what exactly is the Neptune instance endpoint. Can someone help me out?</p>",0,3,2020-10-22 08:39:45.050000 UTC,,,0,amazon-web-services|amazon-neptune,96,2018-08-30 12:48:19.410000 UTC,2022-03-03 23:49:13.653000 UTC,"Chennai, Tamil Nadu, India",2111,297,31,594,,,,,,[]
Rename files (append timestamp) and move it to different folder on ADLS Gen2 (Databricks),"<p>I want to append the timestamp into each file's name and move it to another folder on ADLS Gen2 from databricks notebook.
I could list the file name as below.</p>
<pre class=""lang-py prettyprint-override""><code>fileList = dbutils.fs.ls(file_input)

for i in fileList:
  try:
    file_path = i.path
    #Rename the file appending timestamp
    parser = datetime.datetime.now()
    time_stamp = parser.strftime(&quot;%Y%m%d-%H%M%S&quot;)
    # Rename the files on ADLS Gen2 appennding time_stamp 

    #Move the file
    dbutils.fs.mv(file_path, file_path_archive)
  except Exception as e:
    raise Exception(str(e))
</code></pre>
<p>How to rename a file appending time stamp for a file on ADLSGen2. Thanks.</p>",0,2,2021-03-16 06:12:34.187000 UTC,,2021-03-17 11:31:23.017000 UTC,0,azure-databricks|azure-data-lake|dbutils,139,2019-08-15 06:38:39.327000 UTC,2022-03-03 11:02:35.683000 UTC,,769,82,0,142,,,,,,[]
How to update a local table built from dataframe by joining to another local table?,"<p>I have a two local tables and I want to update the first one based on the value from the second one by join them using sql like this and store it in another local table</p>

<pre><code>val a=spark.sql("""""" UPDATE PC
SET PC.ComponentCode = 'UN'
,PC.LegacyCategoryCode = 'UN'
FROM tbllabortemp PC
JOIN (
            SELECT CL.ContractScheduleId
                ,CL.WarehouseId
                ,CL.ComponentCode
            FROM tblContractLineitemUnicorn CL
            INNER JOIN tbllabortemp CP ON CP.ContractSchedule_ID = CL.ContractScheduleId
                AND CP.warehouse_id = CL.WarehouseId
                AND CP.Component_Code = CL.ComponentCode
            GROUP BY CL.ContractScheduleId
                ,CL.WarehouseId
                ,CL.ComponentCode
            HAVING SUM(CL.LineItemTotalPurchasedUnits) &lt; 1
            ) LI ON PC.ContractScheduleID = LI.ContractScheduleId
            AND PC.warehouse_id = LI.WarehouseId
            AND PC.Component_Code = LI.ComponentCode """""")

a.createOrReplaceTempView(""MergeTable"")
</code></pre>

<p>but it is giving me a ""mismatched input 'FROM' expecting "", Please help on this.thanks</p>",1,0,2019-11-13 23:43:32.407000 UTC,,,1,sql|scala|apache-spark|azure-databricks,1058,2019-02-18 20:39:58.007000 UTC,2021-11-10 05:52:10.410000 UTC,,125,5,0,58,,,,,,[]
CSV Data format for Bulk Load in Amazon Neptune,"<p>I have two CSV files that I have to Bulk load in Amazon Neptune.</p>
<ol>
<li><p>Vertex Data - <a href=""https://i.stack.imgur.com/OFf69.png"" rel=""nofollow noreferrer"">-Node Data</a></p>
</li>
<li><p>Edge Data <a href=""https://i.stack.imgur.com/nAvLz.png"" rel=""nofollow noreferrer"">-Edge Data</a></p>
</li>
</ol>
<p>I am trying to load this database in Amazon Neptune DB to create a Knowledge Graph on Edge Data,
But after completing all the required steps when I load the data, the LOAD_FAILED response comes up.</p>
<p>Is there any specific format in which I have to upload the CSV sheet?</p>",1,0,2021-03-04 08:49:45.633000 UTC,,2021-03-04 12:20:45.550000 UTC,0,python|amazon-web-services|amazon-s3|amazon-neptune,494,2017-08-24 14:54:06.047000 UTC,2021-08-04 17:36:15.047000 UTC,"Mumbai, Maharashtra, India",3,0,0,4,,,,,,[]
Azure eventhub access in Azure Databricks,"<p>I am trying to run following code in Azure databricks.</p>
<p>The library <code>azure_eventhubs_spark_2_12_2_3_17.jar</code> is registered on data bricks.
Databricks runtime version is <code>7.2 (includes Apache Spark 3.0.0, Scala 2.12)</code></p>
<p>The code is part of Microsoft learn structured streaming module.
The code block that gives issue is below followed by the error message it gives. The code is part of Microsoft lean itself so there is no custom code that I have written.</p>
<pre><code>%python

ehWriteConf = {
  'eventhubs.connectionString' : connection_string
}

checkpointPath = userhome + &quot;/event-hub/write-checkpoint&quot;
dbutils.fs.rm(checkpointPath,True)

(activityStreamDF
  .writeStream
  .format(&quot;eventhubs&quot;)
  .options(**ehWriteConf)
  .option(&quot;checkpointLocation&quot;, checkpointPath)
  .start())
</code></pre>
<p>This gives  error:</p>
<pre><code>Java.lang.NoClassDefFoundError: Could not initialize class 
org.apache.spark.eventhubs.package$
Py4JJavaError  Traceback (most recent call last)
&lt;command-3909542231316002&gt; in &lt;module&gt;
      9   .writeStream
     10   .format(&quot;eventhubs&quot;)
---&gt; 11   .options(**ehWriteConf)
     12   .start())

/databricks/spark/python/pyspark/sql/streaming.py in start(self, 
path, format, outputMode, partitionBy, queryName, **options)
   1223             self.queryName(queryName)
   1224         if path is None:
-&gt; 1225             return self._sq(self._jwrite.start())
   1226         else:
   1227             return self._sq(self._jwrite.start(path))

/databricks/spark/python/lib/py4j-0.10.9- 
src.zip/py4j/java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-&gt; 1305             answer, self.gateway_client, self.target_id, 
self.name)
   1306 
   1307         for temp_arg in temp_args:

/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
    125     def deco(*a, **kw):
    126         try:
--&gt; 127             return f(*a, **kw)
    128         except py4j.protocol.Py4JJavaError as e:
    129             converted = convert_exception(e.java_exception)

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py in 
get_return_value(answer, gateway_client, target_id, name)
    326                 raise Py4JJavaError(
    327                     &quot;An error occurred while calling {0}{1} 
{2}.\n&quot;.
--&gt; 328                     format(target_id, &quot;.&quot;, name), value)
    329             else:
    330                 raise Py4JError(

Py4JJavaError: An error occurred while calling o785.start.
: java.lang.NoClassDefFoundError: Could not initialize class 
org.apache.spark.eventhubs.package$
    at org.apache.spark.sql.eventhubs.EventHubsSourceProvider.createSink(EventHubsSourceProvider.scala:101)
    at org.apache.spark.sql.execution.datasources.DataSource.createSink(DataSource.scala:325)
    at org.apache.spark.sql.streaming.DataStreamWriter.createV1Sink(DataStreamWriter.scala:408)
    at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:386)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
    at py4j.Gateway.invoke(Gateway.java:295)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:251)
    at java.lang.Thread.run(Thread.java:748)
</code></pre>",2,0,2020-09-25 07:01:53.180000 UTC,,2020-09-25 10:22:57.033000 UTC,1,azure-databricks,1049,2020-09-25 06:23:07.127000 UTC,2020-09-25 14:53:47.350000 UTC,,21,0,0,2,,,,,,[]
Java File Object for a File in ADLS Gen2,"<p>I have a tool that works for on-premise data upload. Basically, it reads the file from local system i.e.(on-premise: Linux or Windows) and send it over to a location.
It makes use of Java File class. eg: new File(&quot;/dir/file.txt&quot;)</p>
<p>I want to make use of the same code for input files on ADLS Gen2. I would be running the code on Azure Databricks and stuck with getting the File object for the files in ADLS Gen2. I am using wasbs protocol for making the File object, but it is coming as null as Java is not recognizing the directory structure.</p>",1,1,2020-10-06 13:30:51.773000 UTC,,,1,java|azure|azure-data-lake|azure-databricks|azure-data-lake-gen2,83,2015-10-02 07:41:28.307000 UTC,2022-01-19 06:29:48.733000 UTC,,57,10,0,22,,,,,,[]
"Why am I getting ""AttributeError: 'NoneType' object has no attribute 'processor'"" using client instance of gramlin_python?","<p>I am using Jupyter Notebook to execute gremlin query over Neptune Database and retrieving information from MySQL database and generating gremlin query string, which is being execute by Client module of &quot;gremlin_python library&quot; as follows.</p>
<pre><code>def getConnectionWithNeptuneGraphDB():
    global clientt
    if clientt is not None:
        print('clientt instance is available')
    else:
        print('clientt is not available')
        clientt = client.Client('neptune_db_endpoint', 'g')
    return clientt

#function to Create Person Node
def createPersonNode():
    personsInfo = getPersonInformation(companyIdTouple)
    print(personsInfo)
    personGremQry = queryToCreatePersonNode(personsInfo)
    clientt = getConnectionWithNeptuneGraphDB()
    clientt.submit(personGremQry)
    #print(personsInfo)
    print('Person Node created successfully')

def main():
    #Process to Create Company Node
    createCompanyNode()
    #Process to Create Person Node
    createPersonNode()      
    connection.close()
    clientt.close()
</code></pre>
<p>main() function is entry point here, in execution, createCompanyNode() is working fine and getting company in database but while executing createPersonNode(), this is showing following error, even I have tried only executing createPersonNode(), still the same error.</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-558-263240bbee7e&gt; in &lt;module&gt;
----&gt; 1 main()

&lt;ipython-input-557-e622c1b6eefc&gt; in main()
      1 def main():
      2     #createCompanyNode()
----&gt; 3     createPersonNode()
      4     connection.close()
      5     clientt.close()

&lt;ipython-input-550-60763852d6da&gt; in createPersonNode()
     16     personGremQry = queryToCreatePersonNode(personsInfo)
     17     clientt = getConnectionWithNeptuneGraphDB()
---&gt; 18     clientt.submit(personGremQry)
     19     #print(personsInfo)
     20     print('Person Node created successfully')

~/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/gremlin_python/driver/client.py in submit(self, message, bindings, request_options)
    128 
    129     def submit(self, message, bindings=None, request_options=None):
--&gt; 130         return self.submitAsync(message, bindings=bindings, request_options=request_options).result()
    131 
    132     def submitAsync(self, message, bindings=None, request_options=None):

~/anaconda3/envs/JupyterSystemEnv/lib/python3.6/concurrent/futures/_base.py in result(self, timeout)
    423                 raise CancelledError()
    424             elif self._state == FINISHED:
--&gt; 425                 return self.__get_result()
    426 
    427             self._condition.wait(timeout)

~/anaconda3/envs/JupyterSystemEnv/lib/python3.6/concurrent/futures/_base.py in __get_result(self)
    382     def __get_result(self):
    383         if self._exception:
--&gt; 384             raise self._exception
    385         else:
    386             return self._result

~/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/gremlin_python/driver/connection.py in cb(f)
     64         def cb(f):
     65             try:
---&gt; 66                 f.result()
     67             except Exception as e:
     68                 future.set_exception(e)

~/anaconda3/envs/JupyterSystemEnv/lib/python3.6/concurrent/futures/_base.py in result(self, timeout)
    423                 raise CancelledError()
    424             elif self._state == FINISHED:
--&gt; 425                 return self.__get_result()
    426 
    427             self._condition.wait(timeout)

~/anaconda3/envs/JupyterSystemEnv/lib/python3.6/concurrent/futures/_base.py in __get_result(self)
    382     def __get_result(self):
    383         if self._exception:
--&gt; 384             raise self._exception
    385         else:
    386             return self._result

~/anaconda3/envs/JupyterSystemEnv/lib/python3.6/concurrent/futures/thread.py in run(self)
     54 
     55         try:
---&gt; 56             result = self.fn(*self.args, **self.kwargs)
     57         except BaseException as exc:
     58             self.future.set_exception(exc)

~/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/gremlin_python/driver/protocol.py in write(self, request_id, request_message)
     72     def write(self, request_id, request_message):
     73         message = self._message_serializer.serialize_message(
---&gt; 74             request_id, request_message)
     75         self._transport.write(message)
     76 

~/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/gremlin_python/driver/serializer.py in serialize_message(self, request_id, request_message)
    134 
    135     def serialize_message(self, request_id, request_message):
--&gt; 136         processor = request_message.processor
    137         op = request_message.op
    138         args = request_message.args

AttributeError: 'NoneType' object has no attribute 'processor'
</code></pre>",0,7,2021-10-23 04:16:35.723000 UTC,,2021-10-23 06:18:34.503000 UTC,2,python|gremlin|amazon-neptune|gremlinpython,101,2017-01-03 11:53:12.290000 UTC,2022-03-04 14:49:07.200000 UTC,"Gurugram, Haryana, India",410,57,1,69,,,,,,[]
Run delta table queries from .NET,"<p>I'm trying to run a delta table queries from my .NET app. Now from what I've researched, I see that jdbc/odbc is the only way to connect from my .NET app to Databricks. However, jdbc/odbc requires me to download some software in order to be able to connect to my Databricks. This troubles me because I don't know how will my app behave when I deploy it? Would I need to deploy this odbc software (Simba ODBC I believe)?</p>
<p>I also can't seem to find any code examples on this topic. If you know some, I'd be more than happy to have them as a correct flow. So if you know or have code samples on this specific problem, please do post them :)</p>",0,0,2021-11-23 11:11:24.920000 UTC,,,0,c#|.net|azure-databricks|databricks-connect,31,2016-10-26 12:03:43.167000 UTC,2022-03-02 12:42:37.167000 UTC,,376,43,5,44,,,,,,[]
Moving Messages received from Azure Service Bus to Azure DataLake with Databricks,"<p>I have located a couple of links showing how to Send and Receive messages with Databricks on Apache Spark, included in the following SO question posted sometime ago <a href=""https://stackoverflow.com/questions/56078432/structured-streaming-with-azure-service-bus-topics"">Structured Streaming with Azure Service Bus Topics</a></p>
<p>However, I'm struggling to find information on how to create a dataframe from the received messages in order to move the messages to say Azure Data Lake or SQL DB.</p>
<p>Has anyone come across any useful documentation?</p>
<p>Please Note: I'm not referring to Azure Event Hub</p>",0,1,2022-02-04 11:45:38.710000 UTC,,2022-02-04 12:29:46.987000 UTC,0,apache-spark|azureservicebus|azure-databricks,31,2021-03-31 08:36:43.863000 UTC,2022-03-05 23:03:22.193000 UTC,"London, UK",651,58,0,93,,,,,,[]
How To Delete Mercurial Commits From The Beginning,"<p>We have a huge repository which contains some very old commits that contain huge binary files. I would like to make the repo much more tiny, so I thought I could get rid of the first 200 commits. I saw <code>strip</code> removes all descendants, so it's not for this case.</p>

<p>What I'd like to achieve is like to ged rid of commits A and B, as if the repo's first commit would be C:</p>

<p><code>
[A]-&gt;[B]-&gt;[C]-&gt;[D]-----------&gt;[G]-[H]
                \              ^
                 \-&gt;[E]-[F]----|
</code></p>

<p>Is it common to do that (I never did :) ) and how would you achieve that?</p>",1,3,2015-08-04 20:40:39.827000 UTC,,,1,version-control|mercurial|dvcs,115,2011-10-27 14:58:41.127000 UTC,2022-03-05 09:47:00.500000 UTC,"Berlin, Germany",6401,5580,9,532,,,,,,[]
How to connect to Amazon Neptune using Java?,"<p>This is code that uses Java to connect to Amazon Neptune.</p>
<p>When I run <code>mvn compile exec:exec</code> to compile the program there is an error:
<code>gremlinjava/src/main/java/com/amazonaws/App.java:[39,15] error: illegal start of expression</code> .</p>
<p>Actually I don't know where I am going wrong. So how can I make it work?</p>
<p>Thanks in advance</p>
<pre><code>package com.amazonaws;
import org.apache.tinkerpop.gremlin.driver.Cluster;
import org.apache.tinkerpop.gremlin.driver.Client;
import org.apache.tinkerpop.gremlin.process.traversal.dsl.graph.GraphTraversalSource;
import org.apache.tinkerpop.gremlin.process.traversal.dsl.graph.GraphTraversal;
import static org.apache.tinkerpop.gremlin.process.traversal.AnonymousTraversalSource.traversal;
import org.apache.tinkerpop.gremlin.driver.remote.DriverRemoteConnection;
import org.apache.tinkerpop.gremlin.structure.T;

public class App
{
    public static void main( String[] args )
    {
        Cluster.Builder builder = Cluster.build();
        builder.addContactPoint(&quot;xxxxxxxxxxx.us-east-1.neptune.amazonaws.com&quot;);
        builder.port(8182);
        builder.enableSsl(true);
        builder.keyCertChainFile(&quot;SFSRootCAG2.pem&quot;);

        Cluster cluster = builder.create();

        GraphTraversalSource g = traversal().withRemote(DriverRemoteConnection.using(cluster));

        // Add a vertex.
        // Note that a Gremlin terminal step, e.g. iterate(), is required to make a request to the remote server.
        // The full list of Gremlin terminal steps is at https://tinkerpop.apache.org/docs/current/reference/#terminal-steps
        g.addV(&quot;Person&quot;).property(&quot;Name&quot;, &quot;Justin&quot;).iterate();

        // Add a vertex with a user-supplied ID.
        g.addV(&quot;Custom Label&quot;).property(T.id, &quot;CustomId1&quot;).property(&quot;name&quot;, &quot;Custom id vertex 1&quot;).iterate();
        g.addV(&quot;Custom Label&quot;).property(T.id, &quot;CustomId2&quot;).property(&quot;name&quot;, &quot;Custom id vertex 2&quot;).iterate();

        g.addE(&quot;Edge Label&quot;).from(g.V(&quot;CustomId1&quot;)).to(g.V(&quot;CustomId2&quot;)).iterate();

        // This gets the vertices, only.
        GraphTraversal t = g.V().limit(3).elementMap();

        t.forEachRemaining(
            e -&gt;  System.out.println(t.toList()));

        cluster.close();
    }
}
</code></pre>",1,1,2021-10-14 10:19:53.800000 UTC,,2021-10-15 16:26:56.673000 UTC,0,java|gremlin|graph-databases|tinkerpop3|amazon-neptune,126,2021-06-14 08:14:36.453000 UTC,2022-02-28 01:39:22.357000 UTC,,27,0,0,5,,,,,,[]
Broadcast variable in spark causing performance issue,"<p>I have a cluster where we have 8 nodes. Each has 116 GB ram and 16 core and I am trying to read table X which is 250 GB in size. Table X I am joining with table Y 10 times to derive 10 columns. The size of table Y is 100 MB.</p>

<p>Now my question is when I am broadcasting table Y and explicitly caching the table y, script takes around 20 hours but when I am not caching and only broadcasting the table y the complete process took only 1 hour.</p>

<p>Unable to understand what is actually causing more time if we explicitly cache 100 MB after broadcasting the table.</p>

<p>Tried searching on official documentation but could not much information.</p>

<p>Please help.</p>",0,2,2020-06-05 15:18:55.103000 UTC,1.0,,0,performance|apache-spark|bigdata|azure-databricks,72,2019-03-28 08:35:43.173000 UTC,2020-08-03 18:17:50.680000 UTC,,11,0,0,17,,,,,,[]
I am unable to get the data from AWS Neptune,"<p>I have created an AWS AppSync Graphql API which on being called will run an AWS Lambda function and that function will query data from AWS Neptune using query language Gremlin but I am unable to retrieve the data from AWS Neptune.</p>
<p>This is my gremlin query lambda function:</p>
<pre class=""lang-ts prettyprint-override""><code>const gremlin = require('gremlin')

const DriverRemoteConnection = gremlin.driver.DriverRemoteConnection
const Graph = gremlin.structure.Graph
const uri = process.env.READER

const listPosts = async () =&gt; {
    let dc = new DriverRemoteConnection(`wss://${uri}/gremlin`,  {
      MimeType: 'application/vnd.gremlin-v2.0+json',      
  })
    const graph = new Graph()
    const g = graph.traversal().withRemote(dc)
    try {
      let data = await g.V().hasLabel('posts').elementMap().toList()      
      
      dc.close()
      return data
    } catch (err) {
        console.log('ERROR', err)
        return null
    }
}

export default listPosts
</code></pre>
<p>and this is my gremlin mutation lambda code in this lambda function I used AWS EventBridge:</p>
<pre class=""lang-ts prettyprint-override""><code>import * as gremlin from 'gremlin';
import {EventBridgeEvent, Context} from 'aws-lambda';
const DriverRemoteConnection = gremlin.driver.DriverRemoteConnection
const Graph = gremlin.structure.Graph
const uri = process.env.WRITER


exports.handler = async (event: EventBridgeEvent&lt;string, any&gt;, context: Context) =&gt; {

    let dc = new DriverRemoteConnection(`wss://${uri}/gremlin`, {})
    const graph = new Graph()
    const g = graph.traversal().withRemote(dc)
    
     try {

        if(event[&quot;detail-type&quot;] === &quot;createPost&quot;) {
            const data = await g.addV(&quot;posts&quot;).
            property(&quot;id&quot;, event.detail.id).
            property(&quot;title&quot;, event.detail.title).
            property(&quot;content&quot;, event.detail.content).next()            
        }

    } catch(error) {
        return error
    }
}
</code></pre>
<p>where as this is my schema:</p>
<pre><code>type Event {
  result: String!
}

type Post {
  post_id: String!
  title: String!
  content: String!
}

input PostInput {
  post_id: String!
  title: String!
  content: String!
}

type Query {
  listPosts: [Post!]!
}

type Mutation {
  createPost(post_id: String!, title: String!, content: String!): Event
}
</code></pre>",0,1,2022-01-02 05:10:17.907000 UTC,,2022-01-04 18:14:28.253000 UTC,0,aws-lambda|gremlin|aws-cdk|aws-appsync|amazon-neptune,45,2021-09-27 21:53:13.177000 UTC,2022-03-05 22:31:57.767000 UTC,,31,0,0,1,,,,,,[]
Copied data from S3 to Neptune instance | Need to view my data with SPARQL,"<p>I have copied data from S3 to AWS Neptune instance (4 CSV files).</p>

<p>As per the AWS document, created RDF4 console to connect to Neptune instance.</p>

<p>In this console as per document created Neptune repository also.</p>

<p>Now I want  see may data, how we can write the query now.</p>

<p><a href=""https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-sparql-rdf4j-console.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-sparql-rdf4j-console.html</a></p>

<p>I have opened Neptune repository in my RDF4J console now.</p>

<p>How  to execute the SPARQL queries to check my data?</p>",1,5,2018-07-25 10:20:10.220000 UTC,,2018-07-27 08:09:19.197000 UTC,0,sparql|amazon-neptune,329,2017-07-06 04:54:35.257000 UTC,2018-08-23 09:11:46.100000 UTC,"Hyderabad, Telangana, India",29,0,0,30,,,,,,[]
How can I make complex Gremlin queries in AWS Neptune without variables?,"<p>I'm using Amazon Neptune, which <a href=""https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-differences.html#w3aac15c18c10c15c37"" rel=""nofollow noreferrer"">does not support</a> variables. For complex queries, however, I need to use a variable in multiple places. How can I do this without querying twice for the same data?</p>
<p>Here's the problem I'm trying to tackle:</p>
<blockquote>
<p>Given a start <code>Person</code>, find <code>Persons</code> that the start <code>Person</code> is connected to by at most 3 steps via the <code>knows</code> relationship. Return each <code>Person</code>'s <code>name</code> and <code>email</code>, as well as the <code>distance</code> (1-3).</p>
</blockquote>
<p>How would I write this query in Gremlin without variables, since variables are unsupported in Neptune?</p>",1,0,2021-02-04 13:20:25.040000 UTC,,,0,gremlin|tinkerpop|tinkerpop3|amazon-neptune|gremlinpython,162,2012-11-09 15:22:30.130000 UTC,2022-03-03 06:53:16.050000 UTC,,41,3,0,6,,,,,,[]
How to query to get comma separated values if the subject is the same?,"<p>I have more records of the same subject and predicate, but different object, like:</p>
<pre><code>Alex hasFriend A
Alex hasFriend B
Alex hasFriend C
Alex hasFriend D
</code></pre>
<p>How could I query the data to get the result with comma separated values, like:</p>
<pre><code>Alex hasFriend A, B, C, D
</code></pre>
<p>The SPARQL query is like this:</p>
<pre><code>select distinct ?person ?friend where {

?person &lt;http://www.example.com/hasFriend&gt; ?friend.
  
}
</code></pre>",1,2,2021-10-29 13:31:55.253000 UTC,1.0,,1,sparql|rdf|amazon-neptune|triplestore|blazegraph,42,2021-09-21 07:09:01.583000 UTC,2022-03-04 15:58:53.683000 UTC,,127,10,0,23,,,,,,[]
Load multiple terabyte files in a parallel execution into AZURE cloud SQL database,"<p>I have multiple Terabyte files that needs to be loaded into a database which sits on top of a high performance AZURE SQL server in cloud.</p>

<p>For now i'm trying to load these files via an SSIS package and its taking more than 12 hours to complete for 5 files. </p>

<p>I believe HDInsight/ Data Bricks are in Azure to do big data ETL process and analyze data using Ambari and other UI. But is it possible to use the same(HDInsight or DataBricks) to load the huge data files into a SQL table/database ? (Like using clusters to do load mutiple files in a parallel execution mode)</p>

<p>Any suggestion/help is much appreciated </p>",1,4,2019-11-01 21:08:02.007000 UTC,,,0,azure|azure-sql-database|azure-hdinsight|ambari|azure-databricks,119,2015-12-03 06:42:54.140000 UTC,2022-03-03 15:30:50.130000 UTC,"Fairfax, VA",850,201,9,152,,,,,,[]
AWS Neptune/Gremlin: merge edge exists query and insert edge query into one query,"<p>I have the following query to check is an edge exists:</p>
<pre><code>g.V(&quot;N001&quot;).hasLabel(&quot;my-type&quot;).out(&quot;parent&quot;).hasId(&quot;N002&quot;).hasLabel(&quot;my-type&quot;).limit(1).hasNext()
</code></pre>
<p>I also have the afterward query to insert the edge is not exists:</p>
<pre><code>g.V(&quot;N001&quot;).hasLabel(&quot;my-type&quot;).as(&quot;a&quot;).V(&quot;N002&quot;).hasLabel(&quot;my-type&quot;).as(&quot;b&quot;).addE(&quot;parent&quot;).from(&quot;a&quot;).to(&quot;b&quot;)
</code></pre>
<p>My question is how can I merge this two query into one query?</p>",1,0,2022-01-20 20:51:45.987000 UTC,,,0,amazon-web-services|gremlin|amazon-neptune,26,2012-02-03 16:20:42.710000 UTC,2022-03-04 16:51:03.243000 UTC,,5628,119,6,508,,,,,,[]
"403 error when connecting to S3 using Scala in Azure databricks, Python with boto3 works fine","<p>I have been getting intermittent issues with when trying to read from an S3 bucket from Databricks in Azure.  It can sometimes go months with out working, suddenly work temporarily, and stop again.</p>
<p>The Scala code is as follows:</p>
<pre><code>val access_key = &quot;XXXXXXXXX&quot;
val secret_key = &quot;XXXXXXXXX&quot;
val encoded_secret_key = secret_key.replace(&quot;/&quot;, &quot;%2F&quot;)
val aws_bucket_name = &quot;bucket-name&quot;
val file_path = &quot;filePath&quot;

spark.conf.set(&quot;fs.s3n.awsAccessKeyId&quot;, access_key)
spark.conf.set(&quot;fs.s3n.awsSecretAccessKey&quot;, encoded_secret_key)

var df = dbutils.fs.ls(s&quot;&quot;&quot;s3a://$aws_bucket_name/$file_path&quot;&quot;&quot;)

display(df)
</code></pre>
<p>Sometimes it will work, other times it won't, all without making any configuration changes. At least not on the code or cluster configuration side. When it does fail, the error is as follows</p>
<blockquote>
<p>java.nio.file.AccessDeniedException: s3a:///: getFileStatus on s3a:///: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden; request: HEAD https://..amazonaws.com  {} Hadoop 2.7.4, aws-sdk-java/1.11.655 Linux/5.4.0-1063-azure OpenJDK_64-Bit_Server_VM/25.282-b08 java/1.8.0_282 scala/2.12.10 vendor/Azul_Systems,_Inc. com.amazonaws.services.s3.model.GetObjectMetadataRequest; Request ID: , Extended Request ID: &lt;long/id&gt;, Cloud Provider: Azure, Instance ID:  (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: ; S3 Extended Request ID: ), S3 Extended Request ID: :403 Forbidden</p>
</blockquote>
<p>I'm not even sure how to troubleshoot. Connection works fine with python (boto3) in the same notebook, but the Scala doesn't work.</p>
<p>We are using Spark 3.0.1, Scala 2.12</p>",0,4,2022-01-13 19:27:27.057000 UTC,,2022-01-13 20:46:39.440000 UTC,0,amazon-web-services|scala|amazon-s3|azure-databricks,50,2017-10-21 05:04:37.550000 UTC,2022-03-03 18:58:54.747000 UTC,,95,10,0,15,,,,,,[]
How can I download GeoMesa on Azure Databricks?,"<p>I am interested in performing Big Data Geospatial analysis on Apache Spark. My data is stored in Azure data lake, and I am restricted to use Azure Databricks. Is there anyway to download Geomesa on Databrick? Moreover, I would like to use the python api; what should I do?</p>

<p>Any help is much appreciated!!</p>",4,1,2019-10-28 18:52:36.860000 UTC,,,2,azure-databricks|geomesa,1034,2017-04-18 22:55:38.667000 UTC,2022-03-02 20:54:57.767000 UTC,"Ottawa, ON, Canada",2058,224,3,351,,,,,,[]
How to pass pipeline data to azure ml pipeline databricks step?,"<ol>
<li>I have created an Azure ml pipeline consisting of 4 steps. First, two steps are python script steps and the 3rd one is databricks step and 4th one is also python script step. I am creating a pipeline data and passing it to all subsequent steps.</li>
</ol>
<p><code> prepped_data_parameter = PipelineData('prepped_parameter',  datastore=data_store)</code></p>
<p>2nd python step can read the value from pipeline data but it is not working in databricks step.
2. I have also tested passing data from one databricks step to another databricks step, thinking that dbfs path might be causing the problem. Here also it is not working.</p>
<p>python script step produces path like this when i make pipeline data :</p>
<pre><code>``` /mnt/batch/tasks/shared/LS_root/jobs/******/azureml/bb743894-f7d6-4125-***-bccaa854fb65/mounts/workspaceblobstore/azureml/******-742d-4144-a090-a8ac323fa498/prepped_parameter/ ```
</code></pre>
<p>Databricks step produces like this for the same :</p>
<p><code>wasbs://azureml-blobstore-*******-983b-47b6-9c30-d544eb05f2c6@*************l001.blob.core.windows.net/azureml/*****-742d-4144-a090-a8ac323fa498/prepped_parameter/  </code></p>
<p>I want to know how I can efficiently pass pipeline data from python to databricks step or vice versa without manually storing the data into datastore and deleting it for intermediate pipeline data.</p>",0,0,2021-09-17 06:49:12.533000 UTC,,2021-09-17 07:00:05.493000 UTC,0,azure-databricks|azureml-python-sdk,71,2021-07-15 17:22:37.743000 UTC,2021-09-25 07:00:18.030000 UTC,,1,0,0,2,,,,,,[]
Wanted to get output of databricks activity in the ADF pipeline so that I can use those output parameter in further ADF Activity,<p>I am having a data bricks activity which I am using in ADF and I wanted to get the run output in further activity's like there is one file which I am using in data bricks to get all the days from the column and now I wanted to get all these days as output in data factory parameter so that I can use these day's as parameters in pre-copy script to delete the specific day of data.</p>,2,0,2019-11-22 12:42:09.177000 UTC,2.0,,2,azure-data-factory|azure-data-factory-2|azure-databricks,2716,2019-03-05 12:23:59.657000 UTC,2022-03-04 21:19:09.743000 UTC,,39,2,0,45,,,,,,[]
What's the best three-way merge tool?,"<p>Subversion, Git, Mercurial and others support three-way merges (combining mine, theirs, and the ""base"" revision) and support graphical tools to resolve conflicts.</p>

<p>What tool do you use? Windows, Mac&nbsp;OS&nbsp;X, Linux, free or commercial, you name it.</p>

<p>Here's a few that I've used or heard of, just to get the conversation started: </p>

<ul>
<li><a href=""http://kdiff3.sourceforge.net/"" rel=""noreferrer"">KDiff3</a></li>
<li><a href=""http://www.diffmerge.net/"" rel=""noreferrer"">DiffMerge</a></li>
<li><a href=""https://www.perforce.com/products/helix-core-apps/merge-diff-tool-p4merge"" rel=""noreferrer"">P4Merge</a></li>
<li><a href=""http://en.wikipedia.org/wiki/Meld_%28software%29"" rel=""noreferrer"">Meld</a></li>
<li>Beyond Compare Pro.</li>
</ul>

<p>(I recognize that this is sort of like the <a href=""https://stackoverflow.com/questions/12625/best-diff-tool"" title=""Best Diff Tool"">Best Diff Tool</a>, but it's different in that I explicitly focus on three-way merge tools; <a href=""http://en.wikipedia.org/wiki/WinMerge"" rel=""noreferrer"">WinMerge</a> is off the list, for example.)</p>",13,21,2009-02-21 04:03:42.053000 UTC,170.0,2018-03-04 08:23:47.587000 UTC,299,version-control|merge|dvcs,209159,2009-01-14 00:14:09.153000 UTC,2022-03-04 21:27:55.840000 UTC,,33647,372,94,1670,,,,,,[]
"Simple push/pull between two ""client"" repositories to share changes","<p>I'm in the process of more correctly implementing Source Control via Mercurial at work and I've run into a situation.  My environment is two programmers with a Server and approx 4 dev computers. There are our 2 Office desktops where the majority of the code writting happens. And then there are 2 laptops used in the Labs for testing and debugging.<br>
Previously, we had just been operating over the network; the code projects lived on the server and both my office and the lab laptop opened the files <em>over the network</em>. Yeah, I know it wasn't the best of ideas, but we made it work. Moving to a more correct model of DVCS with local repos presents with me with a problem: How do I get my code updates from my Office where I was typing to the Lab so I can program an actual chip? I feel like this level of changes (10, 20, 50, maybe even 100 little changes over the course of a day of development) doesn't need to go through the Server. Personal opinion is that commits to the Server should be reserved for when I'm actually ready to share what I have with others... not necessarily finshed with the project, just ready to share where I'm at.  </p>

<p>Do I have to <code>push</code> to the Server and then <code>pull</code> to the Laptop everytime?<br>
Can I just <code>push/pull</code> back and forth between my Office and the Lab laptop repos? How would I set that connection up?</p>",1,2,2014-01-16 20:31:23.220000 UTC,,,1,version-control|mercurial|dvcs,83,2013-01-11 20:00:24.220000 UTC,2018-01-23 20:13:55.380000 UTC,Tennessee,133,7,0,11,,,,,,[]
Gremlin Traversal Query Language - Mapping query results to java entities,"<p>I am new to Gremlin query language. Can anyone help with a good tutorial on Gremlin java implementation? I just want to do CRUD operations from java.<br>
 1. Create two Person objects, relation b/w them &amp; its properties and persist in AWS Neptune<br>
 2. Read two person objects from database and output<br>
 3. Update the person object properties<br>
 4. Delete a person object </p>

<p>able to connect to Neptune successfully and execute queries. Looking for java entity mapping query execution and viewing results. and also any visual representation of neptune graph for Gremlin</p>",1,0,2018-03-22 03:40:47.930000 UTC,,,0,java|gremlin|tinkerpop3|amazon-neptune,389,2016-09-29 07:04:12.013000 UTC,2021-02-05 10:57:10.100000 UTC,"Bengaluru, Karnataka, India",314,7,0,32,,,,,,[]
Attaching a library to Azure databricks cluster,"<p>I want to make use of ts-flint on Azure Datatbricks. I believe the process is documented here: <a href=""https://docs.azuredatabricks.net/user-guide/libraries.html"" rel=""nofollow noreferrer"">https://docs.azuredatabricks.net/user-guide/libraries.html</a></p>

<p>I tried to create a library from the Azure portal and attach it to my testCluster, but using the instructions provided but I can't seem to see it (Calling ts-flint in the Notbook tells me its not found).</p>

<p>Am I doing something wrong?</p>

<p>Also this is the Python file I tried to load into the library: <a href=""https://pypi.org/project/ts-flint/#files"" rel=""nofollow noreferrer"">https://pypi.org/project/ts-flint/#files</a></p>

<p>Is this .gz file not a valid PyPy file or something?</p>",1,5,2018-12-27 19:23:48.427000 UTC,0.0,2018-12-27 19:31:18.917000 UTC,1,azure-databricks,332,2012-10-20 16:18:44.083000 UTC,2022-03-03 10:41:10.747000 UTC,,5331,89,4,402,,,,,,[]
Access Azure Key Vault in Pandas read/write Azure Data Lake Storage Gen2 data in serverless Apache Spark pool in Synapse Analytics,"<p>Recently, Microsoft released a way for Pandas to read/write Azure Data Lake Storage Gen2 data in serverless Apache Spark pool in Synapse Analytics as per the below link:
<a href=""https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/tutorial-use-pandas-spark-pool"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/tutorial-use-pandas-spark-pool</a></p>
<p>If I have to use the same strategy for pyspark in Azure DataBricks, how can I use the datalake secret (from Azure Key Vault) containing the account key so that pandas can access the data lake smoothly? In this way, I don't have to expose the secret value in DataBricks notebook</p>",1,0,2022-02-15 03:14:37.493000 UTC,,,1,pandas|apache-spark|azure-databricks,27,2016-07-12 00:17:14.213000 UTC,2022-03-04 13:44:27.090000 UTC,,11,0,0,0,,,,,,[]
ML Spark job in Azure databricks,"<p>Currently we are using ML pyspark jobs in Azure HDInsight cluster(version: HDI 3.6). Is it possible to use the same pyspark jobs in Azure Databricks without making much changes?</p>

<p>Thanks in advance!</p>",1,0,2019-02-22 06:52:51.233000 UTC,,,0,azure|azure-hdinsight|azure-databricks,44,2019-02-21 14:38:16.020000 UTC,2019-12-10 14:12:41.840000 UTC,,33,0,0,3,,,,,,[]
Gremlin slow joinTime on Neptune,"<p>I have an issue with a request performance in Neptune. I have a graph like this :</p>
<blockquote>
<p>hasId('A_id') (count: 1) -&gt; out('has_group') (count: 12) -&gt; out('has_class').hasLabel('C') (count: 9751) -&gt; out('has_type').hasLabel('D') (count: 9749) -&gt;
out('has_element') (count: 472370) -&gt;  hasLabel(Within(11 elements label)) (count: 107233)</p>
</blockquote>
<p>I replace all the label to simplify, but the graph is exactly like this. The within decrease a lot the performance of the query. For more details :</p>
<pre><code>g.V('0f7a21df-9413-4c71-99f3-242ae25356a5').out('has_group').out('has_class').hasLabel('C').out('has_type').hasLabel('D').out('has_element').count()
</code></pre>
<p>This request take less than 1 secondes and return 472370.</p>
<p>If I add the last hasLabel(whithin()) like this :</p>
<pre><code>g.V('0f7a21df-9413-4c71-99f3-242ae25356a5').out('has_group').out('has_class').hasLabel('C').out('has_type').hasLabel('D').out('has_element').hasLabel(P.within('element_1','element_2','element_3','element_4','element_5','element_6','element_7','element_8','element_9','element_10','element_11')).count()
</code></pre>
<p>The time decrease to 18 seconds. And when I profile the query we can see a joinTime which take at least 90% of the query execution time on the within :</p>
<pre><code>Optimized Traversal
===================
Neptune steps: [
    NeptuneCountGlobalStep {
        JoinGroupNode {
            PatternNode[(?1=&lt;0f7a21df-9413-4c71-99f3-242ae25356a5&gt;, ?5=&lt;has_group&gt;, ?3, ?6) . project ?1,?3 . IsEdgeIdFilter(?6) .
            ],
            {estimatedCardinality=12, expectedTotalOutput=12, indexTime=0, joinTime=1, numSearches=1, actualTotalOutput=12
            }
            PatternNode[(?3, ?9=&lt;has_class&gt;, ?7, ?10) . project ?3,?7 . IsEdgeIdFilter(?10) .
            ],
            {estimatedCardinality=208482, expectedTotalOutput=2000, indexTime=0, joinTime=9, numSearches=1, actualTotalOutput=10945
            }
            PatternNode[(?7, &lt;~label&gt;, ?8=&lt;C&gt;, &lt;~&gt;) . project ask .
            ],
            {estimatedCardinality=424296, expectedTotalOutput=2000, indexTime=5, joinTime=111, numSearches=10945, actualTotalOutput=9751
            }
            PatternNode[(?7, ?13=&lt;has_type&gt;, ?11, ?14) . project ?7,?11 . IsEdgeIdFilter(?14) .
            ],
            {estimatedCardinality=9675934, expectedTotalOutput=11695, indexTime=15, joinTime=95, numSearches=10, actualTotalOutput=42226
            }
            PatternNode[(?11, &lt;~label&gt;, ?12=&lt;D&gt;, &lt;~&gt;) . project ask .
            ],
            {estimatedCardinality=2333386, expectedTotalOutput=11695, indexTime=23, joinTime=402, numSearches=42226, actualTotalOutput=9749
            }
            PatternNode[(?11, ?17=&lt;has_element&gt;, ?15, ?18) . project ?11,?15 . IsEdgeIdFilter(?18) .
            ],
            {estimatedCardinality=8562896, expectedTotalOutput=556904, indexTime=18, joinTime=442, numSearches=10, actualTotalOutput=472370
            }
            PatternNode[(?15, &lt;~label&gt;, ?16, &lt;~&gt;) . project ask . ContainsFilter(?16 in (&lt;element_1&gt;, &lt;element_2&gt;, &lt;element_3&gt;, &lt;element_4&gt;, &lt;element_5&gt;, &lt;element_6&gt;, &lt;element_7&gt;, &lt;element_8&gt;, &lt;element_9&gt;, &lt;element_10&gt;, &lt;element_11&gt;)) .
            ],
            {estimatedCardinality=1158922, indexTime=598, joinTime=18991, numSearches=472370
            }
        }, annotations={path=[Vertex(?1):GraphStep, Vertex(?3):VertexStep, Vertex(?7):VertexStep, Vertex(?11):VertexStep, Vertex(?15):VertexStep
            ], joinStats=true, optimizationTime=1, maxVarId=19, executionTime=20799
        }
    }
]
</code></pre>
<p>The request seems quite simple and the volume not that much. The within is not the good approach here ? Do you have some clue to improve the query ?</p>
<p><strong>EDIT 1:</strong></p>
<p>I tried with the request provided by @saikiranboga and the number of index operation is large better (divided by 10) but the join time is still high. I'm quite confuse.</p>
<p>The index operation number before :</p>
<pre><code>Index Operations
================
Query execution:
    # of statement index ops: 525563
    # of unique statement index ops: 525563
    Duplication ratio: 1.0
    # of terms materialized: 0
</code></pre>
<p>and after</p>
<pre><code>Index Operations
================
Query execution:
    # of statement index ops: 53666
    # of unique statement index ops: 53666
    Duplication ratio: 1.0
    # of terms materialized: 0
</code></pre>
<pre><code>Optimized Traversal
===================
Neptune steps: [
    NeptuneCountGlobalStep {
        JoinGroupNode {
            PatternNode[(?1=&lt;0f7a21df-9413-4c71-99f3-242ae25356a5&gt;, ?5=&lt;has_group&gt;, ?3, ?6) . project ?1,?3 . IsEdgeIdFilter(?6) .
            ],
            {estimatedCardinality=12, expectedTotalOutput=12, indexTime=0, joinTime=0, numSearches=1, actualTotalOutput=12
            }
            PatternNode[(?3, ?9=&lt;has_class&gt;, ?7, ?10) . project ?3,?7 . IsEdgeIdFilter(?10) .
            ],
            {estimatedCardinality=208000, expectedTotalOutput=2000, indexTime=0, joinTime=10, numSearches=1, actualTotalOutput=10945
            }
            PatternNode[(?7, &lt;~label&gt;, ?8=&lt;C&gt;, &lt;~&gt;) . project ask .
            ],
            {estimatedCardinality=424296, expectedTotalOutput=2000, indexTime=4, joinTime=102, numSearches=10945, actualTotalOutput=9751
            }
            PatternNode[(?7, ?13=&lt;has_type&gt;, ?11, ?14) . project ?7,?11 . IsEdgeIdFilter(?14) .
            ],
            {estimatedCardinality=9456689, expectedTotalOutput=11695, indexTime=13, joinTime=94, numSearches=10, actualTotalOutput=42226
            }
            PatternNode[(?11, &lt;~label&gt;, ?12=&lt;D&gt;, &lt;~&gt;) . project ask .
            ],
            {estimatedCardinality=2333386, expectedTotalOutput=11695, indexTime=17, joinTime=341, numSearches=42226, actualTotalOutput=9749
            }
            PatternNode[(?11, ?17=&lt;has_element&gt;, ?15, ?18) . project ?11,?15 . IsEdgeIdFilter(?18) .
            ],
            {estimatedCardinality=7919022, expectedTotalOutput=556904, indexTime=17, joinTime=411, numSearches=10, actualTotalOutput=472370
            }
            PatternNode[(?15, &lt;~label&gt;, ?16, &lt;~&gt;) . project ?16 . ContainsFilter(?16 in (&lt;element_1&gt;, &lt;element_2&gt;, &lt;element_3&gt;, &lt;element_4&gt;, &lt;element_5&gt;, &lt;element_6&gt;, &lt;element_7&gt;, &lt;element_8&gt;, &lt;element_9&gt;, &lt;element_10&gt;, &lt;element_11&gt;)) .
            ],
            {estimatedCardinality=1145096, indexTime=848, joinTime=15268, numSearches=473
            }
        }, finishers=[dedup(?15)
        ], annotations={path=[Vertex(?1):GraphStep, Vertex(?3):VertexStep, Vertex(?7):VertexStep, Vertex(?11):VertexStep, Vertex(?15):VertexStep@[element
                ], VertexLabel(?16):LabelStep
            ], joinStats=true, optimizationTime=1, maxVarId=19, executionTime=17283
        }
    }
]
</code></pre>",1,0,2022-02-23 13:24:49.173000 UTC,,2022-02-25 14:29:16.887000 UTC,0,amazon-web-services|gremlin|tinkerpop|amazon-neptune|aws-neptune,59,2015-08-05 10:57:40.773000 UTC,2022-03-04 11:30:43.810000 UTC,,402,18,0,38,,,,,,[]
WebserviceException: Unable to deploy a model with aks and azure machine learning,"<p>I tried to deploy a new model in azure databricks notebook.
This morning it was working and now I have the following error:</p>
<p>After</p>
<pre><code>service.wait_for_deployment(show_output=True)
print(service.state)
print(service.get_logs())
</code></pre>
<p>I have:</p>
<pre><code>&quot;message&quot;: &quot;Timed out waiting for AKS deployment to complete. pollTimeout : 00:20:00 serviceName: simdev serviceId: ...&quot;,
  &quot;details&quot;: [
    {
      &quot;code&quot;: &quot;DeploymentTimedOut&quot;,
      &quot;message&quot;: &quot;Your container endpoint is not available. Please follow the steps to debug:
    1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https://aka.ms/debugimage#dockerlog for more information.
    2. You can also interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.
    3. View the diagnostic events to check status of container, it may help you to debug the issue.
{&quot;InvolvedObject&quot;:&quot;simdev-757df4f999-rbcws&quot;,&quot;InvolvedKind&quot;:&quot;Pod&quot;,&quot;Type&quot;:&quot;Warning&quot;,&quot;Reason&quot;:&quot;FailedScheduling&quot;,&quot;Message&quot;:&quot;0/2 nodes are available: 2 Insufficient nvidia.com/gpu.&quot;,&quot;LastTimestamp&quot;:null}
{&quot;InvolvedObject&quot;:&quot;simdev-757df4f999-rbcws&quot;,&quot;InvolvedKind&quot;:&quot;Pod&quot;,&quot;Type&quot;:&quot;Warning&quot;,&quot;Reason&quot;:&quot;FailedScheduling&quot;,&quot;Message&quot;:&quot;0/2 nodes are available: 2 Insufficient nvidia.com/gpu.&quot;,&quot;LastTimestamp&quot;:null}
{&quot;InvolvedObject&quot;:&quot;simdev-757df4f999-rbcws&quot;,&quot;InvolvedKind&quot;:&quot;Pod&quot;,&quot;Type&quot;:&quot;Normal&quot;,&quot;Reason&quot;:&quot;Scheduled&quot;,&quot;Message&quot;:&quot;Successfully assigned azureml-train-aml-001-dev/simdev-757df4f999-rbcws to aks-agentpool-34690879-vmss000000&quot;,&quot;LastTimestamp&quot;:null}
</code></pre>
<p>Yesterday it didn't work. This morning yes, and now no.</p>
<p>Here is aks config:</p>
<pre><code>aks_config = AksWebservice.deploy_configuration(cpu_cores=0.7,
                                                memory_gb=0.7,
                                                gpu_cores=1,
                                                period_seconds=1800,
                                                failure_threshold=10,
                                                timeout_seconds=60,
                                                max_request_wait_time=300000,
                                                scoring_timeout_ms=300000,)
</code></pre>",0,1,2021-10-05 12:33:32.317000 UTC,,2021-10-05 12:39:53.747000 UTC,0,azure-aks|azure-databricks|azure-machine-learning-service,83,2020-11-29 09:28:19.553000 UTC,2022-02-11 15:40:49.127000 UTC,,323,41,0,48,,,,,,[]
How to create/start cluster from data bricks web activity by invoking databricks rest api,"<p>I have 2 requirements:</p>

<p>1:I have a clusterID. I need to start the cluster from a ""Wb Activity"" in ADF. The activity parameters look like this:</p>

<pre><code>url:https://XXXX..azuredatabricks.net/api/2.0/clusters/start
body: {""cluster_id"":""0311-004310-cars577""}
Authentication: Azure Key Vault Client Certificate
</code></pre>

<p>Upon running this activity I am encountering with below error:</p>

<pre><code>""errorCode"": ""2108"",

""message"": ""Error calling the endpoint 
'https://xxxxx.azuredatabricks.net/api/2.0/clusters/start'. Response status code: ''. More 
 details:Exception message: 'Cannot find the requested object.\r\n'.\r\nNo response from the 
 endpoint. Possible causes: network connectivity, DNS failure, server certificate validation or 
timeout."",

""failureType"": ""UserError"",
""target"": ""GetADBToken"",
""GetADBToken"" is my activity name.
</code></pre>

<p>The above security mechanism is working for other Databricks related activity such a running jar which is already installed on my databricks cluster.</p>

<p>2: I want to create a new cluster with the below settings:</p>

<pre><code>url:https://XXXX..azuredatabricks.net/api/2.0/clusters/create

    body:{

      ""cluster_name"": ""my-cluster"",
      ""spark_version"": ""5.3.x-scala2.11"",
      ""node_type_id"": ""i3.xlarge"",
      ""spark_conf"": {
      ""spark.speculation"": true
  },
    ""num_workers"": 2
}
</code></pre>

<p>Upon calling this api, if a cluster creation is successful I would like to capture the cluster id in the next activity.</p>

<p>So what would be the output of the above activity and how can I access them in an immediate ADF activity?</p>",1,0,2020-03-22 13:08:54.833000 UTC,,2020-03-22 13:19:36.317000 UTC,0,azure-data-factory|azure-databricks,366,2011-04-04 13:47:11.723000 UTC,2022-02-01 10:53:30.877000 UTC,"Bangalore, Karnataka, India",39,2,0,22,,,,,,[]
List of Azure Data Bricks clusters with useful info about cluster size,<p>I Would like to list Azure Data bricks cluster inventory with useful data pertaining to it using PowerShell. Any leads? </p>,1,0,2020-02-10 13:31:33.087000 UTC,,,0,azure|powershell|azure-powershell|azure-databricks,63,2017-06-07 14:28:55.077000 UTC,2020-04-02 06:59:18.433000 UTC,,1,0,0,1,,,,,,[]
Neptune | Gremlin Python | Parallel queries using websockets,"<p>What is the best way to execute parallel queries from gremlin python jupyter notebook to a Neptune cluster? I am trying to solve this using the Multiprocess package in Python. However my three db.r5.4xlarge readers max out very soon at 100% CPU as shown in the graph below. Graph 1 is CPU utilisation and graph 2 is gremlin errors. Below is my code. Is there a way this can be tackled better using websockets? If yes can you please help me with that since I am very new to gremlin or neptune.</p>

<pre><code>params = [tuple(x) for x in new_registrations_list[['id','createddate']].values]
pool = Pool(42)
df=pool.starmap(process_vertex,params)
pool.close()


def process_vertex(vertex_id, reg_date):    
    g=neptune.graphTraversal(neptune_endpoint='neptune-endpoint', neptune_port=xxx1x)
    vertices=g.V(str(vertex_id)).repeat(__.both().dedup()).emit().project('id').by(T.id).toList()
</code></pre>

<p><a href=""https://i.stack.imgur.com/tEjav.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tEjav.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/rujXH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rujXH.png"" alt=""enter image description here""></a></p>",0,2,2020-04-29 19:44:38.053000 UTC,,2020-05-01 19:17:08.440000 UTC,1,websocket|python-multiprocessing|amazon-neptune|gremlinpython,295,2020-03-22 15:31:06.787000 UTC,2022-02-25 04:42:58.837000 UTC,,89,5,0,11,,,,,,[]
Apache Spark Data Generator Function on Databricks to Send Data To Azure Event Hubs,"<p>I created a question on how to use Databricks dummy data generator here <a href=""https://stackoverflow.com/questions/70626328/apache-spark-data-generator-function-on-databricks-not-working/70717215?noredirect=1#comment125048132_70717215"">Apache Spark Data Generator Function on Databricks Not working</a></p>
<p>Everything is working fine. However I would like to take it to next level and send the dummy data to Azure Event Hubs:</p>
<p>I attempted to do this myself with the following code:</p>
<pre><code>import dbldatagen as dg
from pyspark.sql.types import IntegerType, StringType, FloatType
import json
from pyspark.sql.types import StructType, StructField, IntegerType, DecimalType, StringType, TimestampType
from pyspark.sql.functions import *

delay_reasons = [&quot;Air Carrier&quot;, &quot;Extreme Weather&quot;, &quot;National Aviation System&quot;, &quot;Security&quot;, &quot;Late Aircraft&quot;]

# will have implied column `id` for ordinal of row
flightdata_defn = (dg.DataGenerator(spark, name=&quot;flight_delay_data&quot;, rows=num_rows, partitions=num_partitions)
                 .withColumn(&quot;flightNumber&quot;, &quot;int&quot;, minValue=1000, uniqueValues=10000, random=True)
                 .withColumn(&quot;airline&quot;, &quot;string&quot;, minValue=1, maxValue=500,  prefix=&quot;airline&quot;, random=True, distribution=&quot;normal&quot;)
                 .withColumn(&quot;original_departure&quot;, &quot;timestamp&quot;, begin=&quot;2020-01-01 01:00:00&quot;, end=&quot;2020-12-31 23:59:00&quot;, interval=&quot;1 minute&quot;, random=True)
                 .withColumn(&quot;delay_minutes&quot;, &quot;int&quot;, minValue=20, maxValue=600, distribution=dg.distributions.Gamma(1.0, 2.0))
                 .withColumn(&quot;delayed_departure&quot;,  &quot;timestamp&quot;, expr=&quot;cast(original_departure as bigint) +  (delay_minutes * 60) &quot;, baseColumn=[&quot;original_departure&quot;, &quot;delay_minutes&quot;])
                 .withColumn(&quot;reason&quot;, &quot;string&quot;, values=delay_reasons, random=True)
                )

df_flight_data = flightdata_defn.build(withStreaming=True, options={'rowsPerSecond': 100})


streamingDelays = (
  df_flight_data
    .groupBy(
      df_flight_data.flightNumber,
      df_flight_data.airline,
      df_flight_data.original_departure,
      df_flight_data.delay_minutes,
      df_flight_data.delayed_departure,
      df_flight_data.reason,
      window(df_flight_data.original_departure, &quot;1 hour&quot;)
    )
    .count()
)

writeConnectionString = event_hub_connection_string
checkpointLocation = &quot;///checkpoint.txt&quot;

ehWriteConf = {
  'eventhubs.connectionString' : writeConnectionString
}


# Write body data from a DataFrame to EventHubs. 
ds = streamingDelays \
  .writeStream.format(&quot;eventhubs&quot;) \
  .options(**ehWriteConf) \
  .outputMode(&quot;complete&quot;) \
  .option(&quot;checkpointLocation&quot;, checkpointLocation).start()
</code></pre>
<p>However, the Stream starts but abruptly stops and provides the following error:</p>
<pre><code>org.apache.spark.sql.AnalysisException: Required attribute 'body' not found
</code></pre>
<p>Any thoughts on what could be causing this error?</p>",0,6,2022-01-23 13:41:20.030000 UTC,,,1,apache-spark|azure-databricks|azure-eventhub,52,2021-03-31 08:36:43.863000 UTC,2022-03-05 23:03:22.193000 UTC,"London, UK",651,58,0,93,,,,,,[]
Reading .parquet file from spark throwing exceptions when empty,"<p>While reading .parquet file as below:</p>

<pre><code>src = spark.read.parquet(filePath)
</code></pre>

<p>When file is empty is throwing below error:</p>

<pre><code>filename.parquet is not a Parquet file (too small length: 0)
</code></pre>

<p>How to check if the file is empty before reading?</p>",0,2,2020-06-03 02:56:08.893000 UTC,,,0,pyspark|apache-spark-sql|azure-databricks,173,2009-04-02 00:53:40.477000 UTC,2022-03-01 22:30:30.100000 UTC,"Brisbane QLD, Australia",27501,66,1,1101,,,,,,[]
How to execute a stored procedure in Azure Databricks PySpark?,"<p>I am able to execute a simple SQL statement using PySpark in Azure Databricks but I want to execute a stored procedure instead. Below is the PySpark code I tried.</p>

<pre><code>#initialize pyspark
import findspark
findspark.init('C:\Spark\spark-2.4.5-bin-hadoop2.7')
#import required modules
from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession
from pyspark.sql import *
import pandas as pd

#Create spark configuration object
conf = SparkConf()
conf.setMaster(""local"").setAppName(""My app"")
#Create spark context and sparksession
sc = SparkContext.getOrCreate(conf=conf)
spark = SparkSession(sc)

table = ""dbo.test""
#read table data into a spark dataframe
jdbcDF = spark.read.format(""jdbc"") \
    .option(""url"", f""jdbc:sqlserver://localhost:1433;databaseName=Demo;integratedSecurity=true;"") \
    .option(""dbtable"", table) \
    .option(""driver"", ""com.microsoft.sqlserver.jdbc.SQLServerDriver"") \
    .load()

#show the data loaded into dataframe
#jdbcDF.show()
sqlQueries=""execute testJoin""
resultDF=spark.sql(sqlQueries)
resultDF.show(resultDF.count(),False)
</code></pre>

<p>This doesn't work — how do I do it?</p>",2,1,2020-02-22 16:43:37.847000 UTC,2.0,2020-02-22 17:17:52.020000 UTC,6,python|pyspark-sql|azure-databricks|pyspark-dataframes,11566,2019-02-16 05:42:45.137000 UTC,2021-06-01 16:14:28.417000 UTC,,155,6,0,30,,,,,,[]
Can I use Jupyter lab to interact with databricks spark cluster using Scala?,"<p>Can I use Jupyter lab to connect to a databricks spark cluster that is hosted remotely?</p>
<p>There are KB articles about databricks connect, which allows a scala or java client-process to control a spark cluster. Here is an example:<br />
<a href=""https://docs.databricks.com/dev-tools/databricks-connect.html"" rel=""nofollow noreferrer"">https://docs.databricks.com/dev-tools/databricks-connect.html</a></p>
<p>While that KB article covers a lot of scenarios, it doesn't explain how to use Jupyter notebooks to interact with a databricks cluster using the <strong>Scala</strong> programming language.  I'm familiar with scala programming, but not Python.</p>",3,0,2020-09-04 00:41:07.027000 UTC,1.0,2020-09-05 20:01:36.693000 UTC,2,scala|apache-spark|jupyter|jupyter-lab|databricks-connect,1067,2015-01-14 22:59:14.770000 UTC,2022-03-01 22:59:09.143000 UTC,,926,47,3,65,,,,,,[]
"In git, how can I pull changes from another branch into the working tree?","<p>I did some interesting work in a topic branch. Now I want to clean those changes before committing into the main branch. So I'd like to pull all those changes into the working tree: have the working tree contain the topic's branch version, with <code>git diff</code> showing all the changes from the master branch.</p>

<p>Is that possible? How do I do that?</p>

<p>EDIT: <code>git merge --no-ff topic &amp;&amp; git reset --mixed HEAD^</code> seems to do the trick. If someone has a more elegant answer, I'm all ears!</p>",3,0,2012-02-24 13:34:33.740000 UTC,,2012-02-24 13:46:10.413000 UTC,1,git|version-control|dvcs,175,2009-08-27 12:21:50.197000 UTC,2022-03-04 16:32:49.647000 UTC,,49595,2869,185,1996,,,,,,[]
AWS Neptune: More performant to drop Edges before Vertices?,"<p>Using - Neptune Engine: 1.0.5.1, Apache Tinkerpop: 3.5.2</p>
<p>My question is in regard to the performance of Vertex removal - it is not about the loading of the Vertices.</p>
<p>We have a cron job that clears out a limited number (1000) of &quot;expired&quot; Vertices.</p>
<p>We get hold of and store the vertices to be removed in a Set.</p>
<p>We then remove these via a g.V([vertices]).sideEffect(drop()).next().</p>
<p>This works fine.</p>
<p>All of the Vertices to be removed will have 1 inE and 1 outE.</p>
<p>These Edges obviously get automatically removed when the linked Vertex is removed.</p>
<p>I am wondering though if Neptune (under the hood) would be more performant if we got hold of and removed the Edges first, and then removed the Vertices.</p>
<p>Just wondered if anyone out their (mainly using Neptune, but it is probably a &quot;thing&quot; with other graph databases too) has looked into this and has any hard evidence either way.</p>
<p>Many thanks</p>",1,0,2022-02-25 13:55:48.887000 UTC,,2022-02-28 11:45:44.713000 UTC,0,gremlin|graph-databases|amazon-neptune|aws-neptune,16,2012-08-24 10:31:30.627000 UTC,2022-03-03 13:47:31.363000 UTC,"Nottingham, United Kingdom",121,8,0,22,,,,,,[]
PGP encryption in azure databrick,"<p>I need your help badly :D
I wrote a code in python with PGP , I have a trusted public key and I could perfectly encrypt my massage with this code, but when I run it on data brick I faced problem :
gnupghome should be a directory and it isnt
I would like to know how can I access to a directory in databrick.</p>
<pre class=""lang-py prettyprint-override""><code>import gnupg
from pprint import pprint
import os

gpg = gnupg.GPG(gnupghome='/root/.pnugp')
key_data = open(&quot;/dbfs/mnt/xxxx/SCO/oracle/xxx/Files/publickey.asc&quot;).read()
    
import_result = gpg.import_keys(key_data)
pprint(import_result.results)
with open(&quot;/dbfs/mnt/xxxxx-storage/SCO/oracle/xxx/Files/FileToEncrypt.txt&quot;,'rb') as f:
  status = gpg.encrypt_file(
    f, recipients=['securxxxxfertuca@xx.ca'],
    output='my-encrypted.txt.gpg')
  print( 'ok: ', status.ok)
  print ('status: ', status.status)
  print ('stderr: ', status.stderr)
</code></pre>",1,1,2021-11-19 18:29:09.547000 UTC,1.0,2021-11-19 18:52:54.180000 UTC,1,python|azure-databricks,217,2021-11-19 18:13:39.017000 UTC,2022-03-04 22:03:12.230000 UTC,Canada,11,0,0,0,,,,,,[]
Here my Issue is I am getting multiple Records with same CustomerId. I required only one record. If it is possible please tell me your suugeestions,"<p><a href=""https://i.stack.imgur.com/BvTH3.png"" rel=""nofollow noreferrer"">Two Records</a></p>
<p><a href=""https://i.stack.imgur.com/gmoRt.png"" rel=""nofollow noreferrer"">Three Records</a></p>
<p>Here my Issue is I am getting <strong>multiple Records</strong> with same CustId. I required only <strong>one record</strong>. If it is possible please tell me <em>your suugeestions</em>.</p>
<p>With me <strong>4 Tables</strong> are there from each Table only <strong>one field is selecting</strong>. Here Primary key is a <strong>CustomerId</strong>. To get CustomerId we have to do Union, Before doing union we have to check the number of fields are (equal or not) from each table. We geeting one field from one Table and remaing three fields are <strong>HardCoding as NULL</strong>. After doing like this displaying finalDataFrame, if you see the <em>screenshots</em> those are Kept in above.</p>
<p>I want like this please see the <strong>below ScreenShot</strong>
<a href=""https://i.stack.imgur.com/dNFpj.png"" rel=""nofollow noreferrer"">Required one Row</a></p>",0,0,2022-02-22 06:56:17.387000 UTC,,,-1,scala|pyspark|apache-spark-sql|azure-databricks|databricks-community-edition,24,2021-07-13 18:28:12.000000 UTC,2022-03-02 05:49:35.120000 UTC,,1,0,0,8,,,,,,[]
What is the best practice for gremlin client cluster in java spring project,"<p>I am using Neptune (AWS) graph data base, and my client api is in java spring. My application read and write into my database. Actually we have 2 clusters for reading and writing as a bean. W e are generating several traversal and after submitting each we decided to close it by using <a href=""https://docs.oracle.com/javase/tutorial/essential/exceptions/tryResourceClose.html"" rel=""nofollow noreferrer"">try with ressource</a>.
Is it a best practice to close traversal and recreate it <code>traversal().withRemote(..)</code> ?
In huge project with several connection in one thread  what is the best practice?</p>",1,0,2020-09-08 07:36:57.903000 UTC,,,3,java|gremlin|tinkerpop3|amazon-neptune,359,2014-06-21 12:59:11.490000 UTC,2022-03-01 09:37:29.883000 UTC,,31,3,0,1,,,,,,[]
Relink all the edges of a vertex Neo4j/Neptune,"<p>How can I refresh all the edges from a vertex?</p>
<p>Use case:</p>
<p>I have to link different entities provided by some logic and the linking needs to be refreshed periodically. One way is to delete the previous edges and then create a new ones inside a transaction, but this solution wouldn't work for a large graph since there would be large transaction running which will impact other things.</p>
<p>Can someone suggest other way around for graphdb like Neo4j and Neptune?</p>",0,2,2021-01-14 08:32:31.700000 UTC,,2021-01-14 18:09:23.667000 UTC,0,neo4j|graph-databases|amazon-neptune,40,2021-01-14 07:59:21.470000 UTC,2022-01-25 12:04:14.630000 UTC,,1,0,0,4,,,,,,[]
How to pull the next k changesets in mercurial,"<p>Suppose I want to pull the next k changesets from a remote repository, where k is greater than or equal to 1. Is there some syntax that will allow be to do so? A command that falls back to plain <code>pull</code> if the changesets available are less than or equal to k would be nice.</p>

<p>Of course, in this case. the term ""next"" references the local revision numbers of the repository, which the remote repository does not expose publicly.</p>

<p>I realise I can look up a suitable hash if the repository is available to browse via a web interface, but is there a way to do this without any specific information about the remote repository? </p>",2,0,2014-09-06 09:00:00.347000 UTC,1.0,2014-09-06 09:07:49.570000 UTC,3,version-control|mercurial|dvcs|pull|revision,356,2010-05-26 09:07:44.733000 UTC,2022-03-04 12:26:45.923000 UTC,"Mumbai, Maharashtra, India",5756,819,1,958,,,,,,[]
Transform JSON from REST API to Azure Synapse Analytics,"<p>I need to digest time series data from API to Synapse Analytics database. API returns JSON, which I would like to transform into table holding date, value and name of the time serie. I´m able to return following JSON with Python:</p>
<pre><code>import requests
from pandas.io.json import json_normalize

url = 'https://api.citivelocity.com/markets/analytics/chartingbe/rest/external/authed/data?client_id='+CitiClientId
payload = {'startDate': 20200705, 'endDate': 20200712, 'tags': ['FX.SPOT.EUR.CHF.CITI'],'frequency':'DAILY'}
headers = {'authorization': 'Bearer'+access_token}
response = requests.post(url, json=payload, headers=headers)

print(response.text)
</code></pre>
<p>-&gt;</p>
<p>{&quot;frequency&quot;:&quot;DAILY&quot;,&quot;body&quot;:{&quot;FX.SPOT.EUR.CHF.CITI&quot;:{&quot;x&quot;:[20200706,20200707,20200708,20200709,20200710],&quot;c&quot;:[1.06365,1.06255,1.06265,1.06155,1.06325],&quot;type&quot;:&quot;SERIES&quot;}},&quot;status&quot;:&quot;OK&quot;}</p>
<p>Unfortunately with Postman or Azure Data Factory I get no data returned. Calling API is successfull with ADF and Postman, but I guess something is not correct in the the request.</p>
<p>ADF would be my preferred choice, but using databricks is ok if it is possible to write data directly to Synapse.</p>
<p>-&gt; How to transform JSON to a dataframe (date, value and the name of time series) in python? From dataframe I think I´ll manage to write to Synapse. I can limit the data volumes so using json_normalize should be fine?</p>
<p>One tempting option would be to save json first to database, and do transformations with sql, but I suppose doing it with python would be more elegant/robust solution.</p>
<pre><code>    CREATE TABLE #SERIES WITH (DISTRIBUTION=ROUND_ROBIN) AS SELECT 'FX.SPOT.EUR.CHF.CITI' AS SERIES_NAME,  '20200706, 20200707, 20200708, 20200709, 20200710' as dt, '1.06365, 1.06255, 1.06265, 1.06155, 1.06325' as cl

DECLARE @DT NVARCHAR(400)
DECLARE @CL NVARCHAR(400)

SELECT @DT=DT FROM #SERIES
SELECT @CL=CL FROM #SERIES

SELECT
DT.SERIES_NAME, DT.VALUE AS DT, CL.VALUE AS CL
FROM
(
SELECT row_number() OVER (ORDER BY (SELECT NULL)) AS RN, SERIES_NAME, LTRIM(dt.value) AS VALUE
FROM  #SERIES
CROSS APPLY STRING_SPLIT(@DT, ',') dt
) DT
JOIN
(SELECT row_number() OVER (ORDER BY (SELECT NULL)) AS RN, SERIES_NAME, LTRIM(CL.value) AS VALUE
FROM  #SERIES
CROSS APPLY STRING_SPLIT(@CL, ',') CL
) CL ON DT.SERIES_NAME = CL.SERIES_NAME AND DT.RN = CL.RN
</code></pre>
<p>-&gt;</p>
<pre><code>    SERIES_NAME DT  CL
FX.SPOT.EUR.CHF.CITI    20200706    1.06365
FX.SPOT.EUR.CHF.CITI    20200707    1.06255
FX.SPOT.EUR.CHF.CITI    20200708    1.06265
FX.SPOT.EUR.CHF.CITI    20200709    1.06155
FX.SPOT.EUR.CHF.CITI    20200710    1.06325
</code></pre>
<p>Thanks.</p>",0,0,2020-07-14 05:45:16.253000 UTC,,2020-07-16 11:57:12.987000 UTC,2,python|azure|azure-data-factory|azure-databricks|azure-synapse,432,2015-05-20 13:29:08.143000 UTC,2020-10-08 09:26:48.130000 UTC,,31,0,0,9,,,,,,[]
Recommendations for migrating away from Veracity?,"<p>Any recommendations on improvements to my approach for migration away from Veracity?</p>

<h1>Background:</h1>

<p>I finally hit my limit with <a href=""http://veracity-scm.com/"" rel=""nofollow noreferrer"">Veracity</a>:</p>

<ol>
<li>all attempted commits (after 6 months of use) suddenly give ""<code>Error 101 (sqlite): sg_wc_db:tbl_gid can't find alias 7033.</code>"" (I searched - couldn't find help on this anywhere).</li>
<li>Veracity 2.5 (the latest version) is over 4.5 years old at the time of this latest edit.</li>
<li>""Questions"" link at <a href=""http://veracity-scm.com/qa"" rel=""nofollow noreferrer"">http://veracity-scm.com/qa</a> (which was formerly useful) now <strike>gives a 404</strike> simply redirects back to the home page.</li>
<li>The online community surrounding Veracity seems too small, and <a href=""http://sourcegear.com/"" rel=""nofollow noreferrer"">http://sourcegear.com/</a> seems focused on its non-open source version control systems, instead.</li>
</ol>

<p>In summary, I've lost confidence in Veracity to manage my significant bits.</p>

<h1>Extraction Approach I Used (admittedly low-tech):</h1>

<ol>
<li>used ""<code>vv fast-export</code>"" to get a fast-import stream for my new DVCS.  This preserved the source history.</li>
<li>Manually copied out my Veracity wiki pages to another existing wiki I use.</li>
<li>(Most tediously) Pored over my Veracity Work Items to make sure I didn't lose information critical to my project.</li>
</ol>

<h1>Conclusion:</h1>

<p>I was originally seduced by the integrated wiki &amp; bug tracking features of Veracity.  I now regret that choice, and have moved back to a more mainstream DVCS option.</p>",1,1,2014-11-04 20:10:09.533000 UTC,1.0,2017-11-10 23:55:16.587000 UTC,4,version-control|dvcs|version-control-migration|veracity,1066,2011-08-02 14:23:51.523000 UTC,2022-03-05 00:28:35.617000 UTC,"Madison, AL, USA",573,876,2,268,,,,,,[]
AWS Neptune partial gremlin execution,"<p>I'm using AWS Neptune with nodejs gremlin driver.
I'm sending gremlin queries that include a bunch of operations like addV, addE, etc in the same query.<br />
Once in a while I see situations where only part of the query was executed.<br />
For example, sending two addV but only one was created.<br />
I was wondering if Neptune provides a &quot;transaction&quot; behavior for a single gremlin query sent to the server, or it is normal that only part of it is executed?</p>",0,3,2021-12-27 18:02:17.433000 UTC,,,0,amazon-neptune|aws-neptune,24,2011-09-04 11:21:23.543000 UTC,2022-03-06 00:01:34.273000 UTC,,6121,565,1,373,,,,,,[]
AWS Neptune bulk load endpoint results in 403 every time,"<p>I'm trying to bulk load data by hitting the neptune bulk load endpoint:</p>
<pre class=""lang-java prettyprint-override""><code>  private void upload(String source) throws URISyntaxException, IOException {
    String jsonReq = createJson(&quot;s3://&quot; + source);
    InputStream inputStream = new ByteArrayInputStream(jsonReq.getBytes(StandardCharsets.UTF_8));

    Request&lt;Void&gt; req = new DefaultRequest&lt;&gt;(&quot;neptune-db&quot;);
    req.setHttpMethod(HttpMethodName.POST);
    req.setEndpoint(new URI(&quot;https&quot;, null, HOST, 8182, &quot;/loader&quot;, null, null));
    req.setContent(inputStream);
    req.setHeaders(Map.of(&quot;host&quot;,
        HOST,
        &quot;Content-Type&quot;,
        &quot;application/json&quot;,
        &quot;x-amz-content-sha256&quot;,
        &quot;required&quot;));

    AWS4Signer signer = new AWS4Signer();
    signer.setRegionName(region);
    signer.setServiceName(req.getServiceName());
    signer.setOverrideDate(new Date());
    signer.sign(req, getCreds());

    httpClient.requestExecutionBuilder().request(req).errorResponseHandler(new ErrorResponseHandler()).execute();
  }
</code></pre>
<p>I keep getting 403. The same request works in AWScurl and postman. What could be missing in this case? I can see the tokens getting pulled for the signing.</p>",1,0,2021-07-21 23:10:30.020000 UTC,,,0,java|amazon-web-services|amazon-neptune,46,2020-08-20 21:22:57.010000 UTC,2022-03-05 23:21:22.593000 UTC,"Seattle, WA, USA",655,57,5,91,,,,,,[]
Azure Databricks only gets Event Hub Data sent while its runnng,"<p>I'm trying to read Azure Event Hub data using databricks.</p>
<p>I have a producer running in nodejs, as well as a consumer for testing (on a different consumer group) and all seems to be running fine.</p>
<p>I am using the following pyspark code in databricks to get the data:</p>
<pre><code># Initialize event hub config dictionary with connectionString
connectionString = &quot;Endpoint=sb://XXX.servicebus.windows.net/;SharedAccessKeyName=test;SharedAccessKey=XXX;EntityPath=XXX&quot;
ehConf = {}
ehConf['eventhubs.connectionString'] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString)
 
# Add consumer group to the ehConf dictionary
ehConf['eventhubs.consumerGroup'] = &quot;databricks&quot;
 
# Read events from the Event Hub
messages = spark.readStream.format(&quot;eventhubs&quot;).options(**ehConf).load()
 
# Visualize the Dataframe in realtime
display(messages)
</code></pre>
<p>The issue is that it only reads data from the stream if it is sent while the notebook is running. If i produce data and then run the notebook, it does not appear.</p>
<p>What am I missing? I want to use this to collect data from the stream every hour or so and save it.</p>
<p>Config:
Databricks Runtime: 7.3LTS (Spark 3.0.1, Scala 2.12)
Azure eventhub library: <code>com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.21</code></p>",1,0,2022-01-04 15:02:19.293000 UTC,,,1,pyspark|azure-databricks|azure-eventhub,65,2010-01-13 14:19:27.437000 UTC,2022-03-03 11:24:58.757000 UTC,"Holon, ישראל",1099,349,9,83,,,,,,[]
cvs2hg configuration in windows,"<p>We are planning to migrate complex cvs repository with history to Mercurial.</p>

<p>I have configured cvs2hg in my local machine and cvs(pserver)has installed in remote server. 
I am facing below errors while I migrating code from remote cvs repository to local mercurial repository using cvs2hg(local).</p>

<pre><code>D:\cvs2svn\cvs2svn-19b322d42b1f&gt;python cvs2hg --hgrepos=C:\Users\smandadapu2\De
ktop\mercuryy C:\Users\smandadapu2\Desktop\CVS_Checkout\CVSROOT
----- pass 1 (CollectRevsPass) -----
Examining all CVS ',v' files...
ERROR: No RCS files found under 'C:\\Users\\smandadapu2\\Desktop\\CVS_Checkout\
CVSROOT'!
Are you absolutely certain you are pointing cvs2svn
at a CVS repository?

Pass 1 complete.
===========================================================================
Error summary:
ERROR: No RCS files found under 'C:\\Users\\smandadapu2\\Desktop\\CVS_Checkout\
CVSROOT'!
Are you absolutely certain you are pointing cvs2svn
at a CVS repository?

Exited due to fatal error(s).**strong text**
</code></pre>

<p>Please help me on configuring cvs2hg on windows with detailed</p>

<ol>
<li>How to specify the remote cvs repository path while using cvs2hg ?</li>
<li>How to specify the cvs repository modules while using cvs2hg ?</li>
<li>what are the things should be taken care for complete history migration?</li>
</ol>",1,0,2015-01-21 07:42:55.940000 UTC,,2015-01-21 07:46:30.397000 UTC,0,cvs|dvcs|cvs2svn,183,2013-09-20 08:22:57.447000 UTC,2015-03-05 09:47:57.923000 UTC,"Hyderabad, India",11,0,0,3,,,,,,[]
My table schema changes in databricks are not in my view,"<p>I have created a table in my Azure Databricks.</p>
<pre><code>%sql
Create Table db_xmp_ed_dimensions_map.Test1
(
  ID int,
  Name varchar(100)
);

Insert into db_xmp_ed_dimensions_map.Test1 values(1,'test');

%sql
Create view db_xmp_ed_dimensions_map.v_Test1
AS 
SELECT * FROM db_xmp_ed_dimensions_map.Test1
</code></pre>
<p>When i run two below queries i have these results.</p>
<p><a href=""https://i.stack.imgur.com/jWtfO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jWtfO.png"" alt=""enter image description here"" /></a></p>
<p>If i insert a new row into my table i have that row in my view</p>
<pre><code>%sql
Insert into db_xmp_ed_dimensions_map.Test1 values(2,'test2');
</code></pre>
<p><a href=""https://i.stack.imgur.com/YqH2s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YqH2s.png"" alt=""enter image description here"" /></a></p>
<p>I have a problem here. If I change my table schema by adding a new column, I do not have it in my view.</p>
<pre><code>%sql
ALTER TABLE db_xmp_ed_dimensions_map.Test1
  ADD COLUMN Family VARCHAR(10)
</code></pre>
<p><a href=""https://i.stack.imgur.com/QO8dr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QO8dr.png"" alt=""enter image description here"" /></a></p>
<p>but if i run a query on my view i do not have my new column</p>
<p><a href=""https://i.stack.imgur.com/Bd9SA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Bd9SA.png"" alt=""enter image description here"" /></a></p>
<p>I verified my view creation command by running this command</p>
<pre><code>%sql
SHOW CREATE TABLE  db_xmp_ed_dimensions_map.v_Test1
</code></pre>
<p>and i have this command :</p>
<pre><code>CREATE VIEW `db_xmp_ed_dimensions_map`.`v_Test1` **( `ID`, `Name`)** TBLPROPERTIES ( 
'transient_lastDdlTime' = '1626980188') AS SELECT * FROM db_xmp_ed_dimensions_map.Test1
</code></pre>
<p>As you can see in my view I have the column name hardcoded. I think because of that I can not see my new column in my view.</p>
<p>Instead of recreating my view is there any other way to fix this issue?</p>",0,0,2021-07-22 19:39:40.993000 UTC,,2021-07-22 20:55:37.690000 UTC,0,view|apache-spark-sql|azure-databricks,61,2013-02-12 05:03:00.843000 UTC,2022-03-03 20:20:02.957000 UTC,"Montreal, Canada",10809,3254,76,2088,,,,,,[]
"Azure Service Bus Error: Ensure RequiresSession is set to true when creating a Queue, with Databricks","<p>When attempting to retrieve Queue messages from Azure Service via Databricks I get the following error:</p>
<p>Ensure RequiresSession is set to true when creating a Queue or Subscription to enable sessionful behavior</p>
<p>I have set the Message Queue using Azure's Service Bus Explorer, see below</p>
<p><a href=""https://i.stack.imgur.com/exklb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/exklb.png"" alt=""enter image description here"" /></a></p>
<p>As you can see I have set the sessionId to carlsession6</p>
<p>Therefore, I'm not sure why I'm getting the error message</p>
<p>The code in databricks is as follows:</p>
<pre><code>session_id = &quot;carlsession6&quot;

connstr = &quot;Endpoint=sb://carlsbus.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=myaccesskey&quot;
queue_name = &quot;carlsqueue&quot;

prevTable = None
table = None
rows = []
msgs = []
run = True

batchMsgs = 0

totalMsgs = 0

with ServiceBusClient.from_connection_string(connstr) as client:
    # max_wait_time specifies how long the receiver should wait with no incoming messages before stopping receipt.
    # Default is None; to receive forever.
    
    while run:
      
      with client.get_queue_receiver(queue_name, session_id=session_id, max_wait_time=5) as receiver:
           
          logDebug(D_LEVEL[1], 'GOT QUEUE RECEIVER')
          rows = []
          msgs = []
    
          i = 0
          for msg in receiver: 
          
            receiver.session.renew_lock()
          
            logDebug(D_LEVEL[1], 'READ MESSAGE')
            msgs.append(msg)
          
            msgJSON = json.loads(str(msg))
          
            if 'table' in msgJSON['control']:
              table = msgJSON['control']['table']
          
            if prevTable is None:
              prevTable = table
                     
            if prevTable != table or batchMsgs &gt;= MAX_BATCH_SIZE:
              writeData(prevTable, schema, rows, msgs)
              rows = []
              msgs = []
              batchMsgs = 0
            
            prevTable = table
          
            i += 1
            batchMsgs += 1
          
            if (i % 100 == 0):
              logInfo('Processed ' + str(i) + ' messages')
          
            if 'operation' in msgJSON['control']:
              if msgJSON['control']['operation'] == &quot;stop_message_processor&quot;:
                logInfo('RECEIVED stop_message_processor...')
                run = False
                break
        
            schema, rows = processMsg(msgJSON, rows)    
                            
            logDebug(D_LEVEL[1], 'COMPLETED MESSAGE')
          
            if STOP_AFTER is not None:
              if i &gt;= STOP_AFTER:
                run = False
                break
                
          if len(rows) &gt; 0:  
            writeData(table, schema, rows, msgs)
          elif len(msgs) == 1:
            receiver.complete_message(msg)
     
      logInfo('PROCESSED ' + str(i) + ' MESSAGES FROM QUEUE')
      totalMsgs += i  
      
    logInfo('TOTAL MESSAGES PROCESSED ' + str(totalMsgs))
</code></pre>
<p>Any thoughts on how to resolve the error?</p>",1,0,2022-02-10 14:29:56.070000 UTC,,,0,azureservicebus|azure-databricks,15,2021-03-31 08:36:43.863000 UTC,2022-03-05 23:03:22.193000 UTC,"London, UK",651,58,0,93,,,,,,[]
Streaming Failing From Azure Event Hub on Apache Spark Databricks java.lang.IllegalArgumentException: failed to parse 1,"<p>When I attempt to stream data from Azure Event Hub with the following query I get the error:</p>
<pre><code>Stream stopped...
java.lang.IllegalArgumentException: failed to parse 1
</code></pre>
<p>My code is as follows:</p>
<pre><code>streamingQuery = (
  df
  .writeStream
  .format(&quot;delta&quot;)
  .outputMode(&quot;append&quot;)
  .option(&quot;checkpointLocation&quot;, f&quot;{location}/_checkpoints&quot;)
  .start(location)
</code></pre>
<p>The full error is as follows:</p>
<pre><code>1643393895301Expected e.g. {&quot;ehName&quot;:{&quot;0&quot;:23,&quot;1&quot;:-1},&quot;ehNameB&quot;:{&quot;0&quot;:-2}}
    at org.apache.spark.sql.eventhubs.JsonUtils$.partitionSeqNos(JsonUtils.scala:98)
    at org.apache.spark.sql.eventhubs.EventHubsSourceOffset$.apply(EventHubsSourceOffset.scala:61)
    at org.apache.spark.sql.eventhubs.EventHubsSource$$anon$1.deserialize(EventHubsSource.scala:139)
    at org.apache.spark.sql.eventhubs.EventHubsSource$$anon$1.deserialize(EventHubsSource.scala:115)
    at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.readBatchFile(HDFSMetadataLog.scala:242)
    at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.get(HDFSMetadataLog.scala:232)
    at org.apache.spark.sql.eventhubs.EventHubsSource.initialPartitionSeqNos$lzycompute(EventHubsSource.scala:170)
    at org.apache.spark.sql.eventhubs.EventHubsSource.initialPartitionSeqNos(EventHubsSource.scala:113)
    at org.apache.spark.sql.eventhubs.EventHubsSource.getBatch(EventHubsSource.scala:316)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$3(MicroBatchExecution.scala:492)
    at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
</code></pre>
<p>Does anyone have any thoughts on where I might be going wrong?</p>",0,4,2022-01-29 09:18:46.163000 UTC,,,0,apache-spark|pyspark|azure-databricks|azure-eventhub,34,2021-03-31 08:36:43.863000 UTC,2022-03-05 23:03:22.193000 UTC,"London, UK",651,58,0,93,,,,,,[]
Implementing protobuf message in azure databricks notebook,"<p>I want to read a parquet file contents and send it as a protobuf message to event hub. I am using azure databricks notebook with pyspark to read the parquet file which is stored in storage account.
How can i import a .proto file in databricks notebook and refer that structure to write the message in protobuf format.</p>",0,0,2021-03-10 17:38:20.570000 UTC,,,0,protocol-buffers|azure-databricks,83,2019-12-31 17:02:54.403000 UTC,2021-09-08 19:55:55.357000 UTC,,39,0,0,39,,,,,,[]
Is there is way to display output of scala program in databricks?,"<p>I was trying to run the below scala code in the azure data bricks notebook.it was running fine but not printing anything.</p>
<p>it just shows <code>defined object mainobj</code> after running.
How can I display output?</p>
<pre><code>object mainobj{
  def main(args:Array[String])={
      print(&quot;Hello&quot;)
        }  
    }
</code></pre>",1,0,2021-12-31 11:13:30.667000 UTC,,2021-12-31 12:44:01.207000 UTC,-2,python|azure|scala|azure-databricks,30,2019-02-07 08:01:16.953000 UTC,2022-03-05 12:09:35.997000 UTC,"Vijayawada, Andhra Pradesh, India",51,1,0,7,,,,,,[]
"Using Mercurial (hg), is there a way to diff rev 4873 and 4821 but only for my changes?","<p>I want to know what changes I made, without looking at the 30 other files that other team members modified.</p>

<p>So when I <code>hg out</code>, it said the first changeset to push was 4821, so since then I have pulled, merged, and pushed a couple times.  Now I want to make sure all the debugging code is removed.  Is there a way to diff the current revision (4873) against revision 4821 but only for my changes?</p>",4,0,2010-07-31 07:40:40.630000 UTC,,2010-08-12 12:12:42.823000 UTC,0,mercurial|dvcs,102,2009-05-09 15:50:29.477000 UTC,2022-03-04 09:41:10.460000 UTC,,137341,1445,39,12817,,,,,,[]
Cannot find sourceVersion Error while reading from EventHub and writing to delta lake,"<p>I am trying to read from a EventHub, and writing to a 2 delta lake table, pseudo code below</p>

<pre><code>// read from event hub
inputDF = spark.readstream().format(“eventhubs”).option(“consumerGroup”,”myapp”)

//write to 1 delta lake
inputDF.writestream().format(“delta”).option(“checkpointLocation”,”loc1”).start(“table_1”)

//write to 2 delta lake
inputDF.writestream().format(“delta”).option(“checkpointLocation”,”loc2”).start(“table_2”)

</code></pre>

<p>when i start my job it fails with message ""Cannot find sourceVersion"" below message</p>

<pre><code>ERROR: Query termination received for [id=5735eea9-a2c0-42bf-b368-0918985bff3e, runId=88c17d32-d5d9-46b6-bb9c-19f5ab8598c5], with exception: java.lang.IllegalStateException: Cannot find 'sourceVersion' in {""My_EventHub_Event_Name"":{""2"":25,""5"":33,""4"":35,""7"":33,""1"":26,""3"":28,""6"":30,""0"":32}}
    at com.databricks.sql.transaction.tahoe.sources.DeltaSourceOffset$.validateSourceVersion(DeltaSourceOffset.scala:91)
    at com.databricks.sql.transaction.tahoe.sources.DeltaSourceOffset$.apply(DeltaSourceOffset.scala:74)
    at com.databricks.sql.transaction.tahoe.sources.DeltaSource.getBatch(DeltaSource.scala:269)

</code></pre>

<p>Any idea how to fix it?</p>",0,1,2019-09-15 02:54:00.157000 UTC,1.0,,2,azure-databricks|delta-lake,380,2018-02-01 02:06:45.167000 UTC,2022-03-03 18:36:45.980000 UTC,,163,0,0,28,,,,,,[]
Azure Databricks OOM error that causes the connection to the Python REPL to be closed,"<p>In the following sample code, in <code>one cell</code> of our <a href=""https://docs.microsoft.com/en-us/azure/databricks/scenarios/what-is-azure-databricks#:%7E:text=Azure%20Databricks%20is%20a%20data,Microsoft%20Azure%20cloud%20services%20platform.&amp;text=For%20a%20big%20data%20pipeline,Event%20Hub%2C%20or%20IoT%20Hub."" rel=""nofollow noreferrer"">Azure Databricks</a> notebook, the code loads about 20 million records into a <a href=""https://pandas.pydata.org/"" rel=""nofollow noreferrer"">Python pandas</a> <code>dataframe</code> from an <code>Azure SQL db</code>, does some dataframe column tranformation by applying some functions (as shown in the code snippet below). But after running the code for about half an hour, the Databricks throws the following error:</p>
<p><strong>Error</strong>:</p>
<pre><code>ConnectException: Connection refused (Connection refused)
Error while obtaining a new communication channel
ConnectException error: This is often caused by an OOM error that causes the connection to the Python REPL to be closed. Check your query's memory usage.
</code></pre>
<p><strong>Remarks</strong>: Table has about 150 columns. The <code>Spark setting</code> on the Databricks is as follows:
<strong>Cluster</strong>: <code>128 GB , 16 Cores, DBR 8.3, Spark 8.3, Scala 2.12</code></p>
<p><strong>Question</strong>: What could be a cause of the error, and how can we fix it?</p>
<pre><code>import sqlalchemy as sq
import pandas as pd

def fn_myFunction(lastname):
    testvar = lastname.lower()
    testvar = testvar.strip()
    
    return testvar

pw = dbutils.secrets.get(scope='SomeScope',key='sql')
engine = sq.create_engine('mssql+pymssql://SERVICE.Databricks.NONPUBLICETL:'+pw+'MyAzureSQL.database.windows.net:1433/TEST', isolation_level=&quot;AUTOCOMMIT&quot;)

app_df = pd.read_sql('select * from MyTable', con=engine)

#create new column
app_df['NewColumn'] = app_df['TestColumn'].apply(lambda x: fn_myFunction(x))
.............
.............
</code></pre>",2,0,2021-12-28 23:30:35.887000 UTC,,,3,python|pandas|apache-spark|azure-sql-database|azure-databricks,1219,2012-02-25 04:28:19.340000 UTC,2022-03-06 03:21:32.830000 UTC,,18057,2500,22,2093,,,,,,[]
gremlin query works in console but not in javascript,"<p>I have a query that runs fine when I run it in gremlin console but gives me an error when I put it in my javascript lambda.</p>
<pre><code>const people = await g.V(&quot;1&quot;).out(&quot;memberOf&quot;).in(&quot;memberOf&quot;).next()
</code></pre>
<pre><code>&quot;TypeError: g.V(...).out(...).in is not a function&quot;,
</code></pre>
<p>It looks like the .in functionality is breaking.</p>",1,0,2021-09-03 19:45:53.487000 UTC,,2021-09-04 07:50:31.493000 UTC,0,gremlin|graph-databases|amazon-neptune,24,2014-05-14 23:02:12.137000 UTC,2022-03-01 17:13:00.717000 UTC,,974,110,1,49,,,,,,[]
Encrypt existing AWS Neptune DB instance,"<p>I have created AWS Neptune DB.
However, now I want to encrypt it. As per AWS documentation, I should take snapshot and while restoring, encrypt new DB instance.</p>
<p>However, enabling Encryption check-box is disabled in my account.</p>
<p>Am I missing anything? Does it has anything to do with IAM roles/permissions?</p>
<p>I am trying all these steps from AWS Management Console.</p>",1,0,2021-07-02 15:31:56.353000 UTC,,,0,amazon-web-services|encryption|amazon-neptune,35,2017-03-06 12:56:26.710000 UTC,2021-11-12 11:43:39.490000 UTC,"Mumbai, Maharashtra, India",7,0,0,4,,,,,,[]
Connectivity to Sybase from Databricks,"<p>What is the best way to connect to Sybase from Databricks?</p>

<pre><code>&gt;     %sql
&gt;     
&gt;     CREATE TABLE sybase_table
&gt;     USING org.apache.spark.sql.jdbc
&gt;     OPTIONS (
&gt;       dbtable 'table_name',
&gt;       driver 'sybase.jdbc.driver.SybaseDriver',
&gt;       user 'username',
&gt;       password 'pasword',
&gt;       url 'jdbc:sybase:thin://@&lt;hostname&gt;:1521/&lt;db&gt;')
</code></pre>",2,0,2019-12-21 07:37:35.903000 UTC,,2019-12-23 09:33:40.700000 UTC,0,azure|sybase|azure-databricks|sap-iq,729,2019-12-21 07:31:57.657000 UTC,2022-02-08 18:22:05.910000 UTC,,11,0,0,13,,,,,,[]
Connect to Amazon Neptune db from local Gremlin server from java app,"<p>Right now we are using Janusgraph. Nodes and Vertices in our graph have entitlement tagged.  We have customized the Gremlin server to check the entitlement after each step execution.</p>
<p>This Gremlin server is started using a Java app.</p>
<p>I am wondering if we can do the same thing for Neptune i.e. start a local Gremlin server and connect to Neptune DB and customize it for entitlement checks.
Also, is it possible to configure a custom serializer for gremlin query response?</p>
<p>Adding the sample config:</p>
<pre><code>host: MyNeptuneHost
port: 8182
evaluationTimeout: 30000
channelizer: org.apache.tinkerpop.gremlin.server.channel.WebSocketChannelizer
graphs: {
  graph: conf/tinkergraph-empty.properties}
scriptEngines: {
  gremlin-groovy: {
    plugins: { org.apache.tinkerpop.gremlin.server.jsr223.GremlinServerGremlinPlugin: {},
               org.apache.tinkerpop.gremlin.tinkergraph.jsr223.TinkerGraphGremlinPlugin: {},
               org.apache.tinkerpop.gremlin.jsr223.ImportGremlinPlugin: {classImports: [java.lang.Math], methodImports: [java.lang.Math#*]},
               org.apache.tinkerpop.gremlin.jsr223.ScriptFileGremlinPlugin: {files: [scripts/empty-sample.groovy]}}}}
serializers:
  - { className: org.myApp.tinkerpop.gremlin.driver.ser.MyAppGraphSONMessageSerializerV3d0, config: { ioRegistries: [org.myApp.tinkerpop.gremlin.tinkergraph.structure.MyAppIoRegistryV3d0] }}
</code></pre>",1,0,2022-01-07 11:18:47.660000 UTC,,2022-01-10 05:27:58.667000 UTC,0,amazon-neptune|gremlin-server|aws-neptune,61,2012-09-04 20:46:30.430000 UTC,2022-02-25 05:37:32.187000 UTC,,11,0,0,2,,,,,,[]
How to Connect/Query to AWS Neptune instance using HTTP POST/GET request,"<p>I am trying to connect to Amazon Neptune instance by sending a HTTP Post request using Neptune END Point via Fiddler.
But ending up in timeout error.
Can Neptune instance be connected to via HTTP request using fiddler/Postman?</p>",3,1,2018-03-03 19:16:18.890000 UTC,1.0,,1,amazon-neptune,2128,2018-03-03 19:01:48.897000 UTC,2018-03-20 10:21:58.757000 UTC,"Bengaluru, Karnataka, India",11,0,0,5,,,,,,[]
Flatten array in yield statement in Scala,"<p>I have the following piece of code</p>

<pre><code>var splitDf = fullCertificateSourceDf.map(row =&gt; {
  val ID = row.getAs[String](""ID"")
  val CertificateID = row.getAs[String](""CertificateID"")
  val CertificateTag = row.getAs[String](""CertificateTag"")
  val CertificateDescription = row.getAs[String](""CertificateDescription"")
  val WorkBreakdownUp1Summary = row.getAs[String](""WorkBreakdownUp1Summary"")
  val ProcessBreakdownSummaryList = row.getAs[String](""ProcessBreakdownSummaryList"")
  val ProcessBreakdownUp1SummaryList = row.getAs[String](""ProcessBreakdownUp1SummaryList"")
  val ProcessBreakdownUp2Summary = row.getAs[String](""ProcessBreakdownUp2Summary"")
  val ProcessBreakdownUp3Summary = row.getAs[String](""ProcessBreakdownUp3Summary"")
  val ActualStartDate = row.getAs[java.sql.Date](""ActualStartDate"")
  val ActualEndDate = row.getAs[java.sql.Date](""ActualEndDate"")
  val ApprovedDate = row.getAs[java.sql.Date](""ApprovedDate"")
  val CurrentState = row.getAs[String](""CurrentState"")
  val DataType = row.getAs[String](""DataType"")
  val PullDate = row.getAs[String](""PullDate"")
  val PullTime = row.getAs[String](""PullTime"")

  val split_ProcessBreakdownSummaryList = ProcessBreakdownSummaryList.split("","")
  val split_ProcessBreakdownUp1SummaryList = ProcessBreakdownUp1SummaryList.split("","")

  val Pattern = ""^.*?(?= - *[a-zA-Z])"".r

  for{
        subSystem : String &lt;- split_ProcessBreakdownSummaryList
      } yield(ID,
              CertificateID,
              CertificateTag,
              CertificateDescription,
              WorkBreakdownUp1Summary,
              subSystem,
              for{ system: String &lt;- split_ProcessBreakdownUp1SummaryList if(system contains subSystem.trim().substring(0,11))}yield(system),
              ProcessBreakdownUp2Summary,
              ProcessBreakdownUp3Summary,
              ActualStartDate,
              ActualEndDate,
              ApprovedDate,
              CurrentState,
              DataType,
              PullDate,
              PullTime
             )
}).flatMap(identity(_))

display(splitDf)
</code></pre>

<p>How can I get the first matching element from the following portion of the above statement:</p>

<pre><code>for{ system: String &lt;- split_ProcessBreakdownUp1SummaryList if(system contains subSystem.trim().substring(0,11))}yield(system)
</code></pre>

<p>At the moment it returns an array with one element in it. I dont want the array I just want the element.</p>

<p>Thank you in advance.</p>",0,2,2020-01-16 12:38:03.047000 UTC,,,1,scala|azure-databricks,38,2013-03-30 16:25:18.897000 UTC,2021-07-21 01:43:58.397000 UTC,,73,0,0,14,,,,,,[]
Unexplained ConstraintViolationException with AWS Neptune,"<p>I get this constraintViolationException with AWS Neptune when I tried to create a bunch of edges.
<strong>The problem is that it does not tell which edge already exists.</strong>
I add roughly 50 edges at once using script via java gremlin-driver.</p>
<p>Has anyone came across this kind of scenario ?</p>
<pre><code>org.apache.tinkerpop.gremlin.driver.exception.ResponseException: {&quot;requestId&quot;:&quot;c1e55266-6fa9-44f6-91b3-74f08d227ffd&quot;,&quot;code&quot;:&quot;ConstraintViolationException&quot;,&quot;detailedMessage&quot;:&quot;Edge with id already exists: &quot;}
</code></pre>",1,0,2022-01-31 04:26:49.063000 UTC,,2022-02-01 00:04:57.463000 UTC,0,amazon-web-services|gremlin|amazon-neptune,26,2012-06-07 11:12:28.637000 UTC,2022-03-06 05:17:21.457000 UTC,"Bangalore, Karnataka, India",776,85,1,76,,,,,,[]
Mercurial merge losing existing lines,"<p>I'm trying to merge a branch into the trunk. Trunk has file with code added but when I merge the branch into it, the code disappears. In the branch, the file is present but code is not.</p>

<p>I'm using TortoiseHg for the merge. 'Discard all changes from the other revision' is unchecked and there are no merge issues in the log. Why would the code in trunk disappear during the merge?</p>",0,2,2018-01-30 18:01:56.850000 UTC,,2018-01-30 20:32:00.427000 UTC,1,merge|version-control|mercurial|dvcs,37,2008-12-14 01:56:05.507000 UTC,2021-08-05 19:23:01.437000 UTC,United States,4745,542,3,726,,,,,,[]
mount.err while trying to access databricks file system on ADLS Gen2,"<p>I am trying to mount ADLS Gen2 to databricks but I am not able to get list of files after a certain directory using bash/shell script on Databricks.
DBUTILS works fine.</p>
<pre><code>import os
os.listdir('/dbfs/mnt/temp')
mount.err

%sh
ls /dbfs/mnt/temp
mount.err

#using dbtuits work
for files in dbutils.fs.ls(external_path):
  print(files.path)

dbfs:/mnt/temp/test1/
</code></pre>
<p>I tried to unmount and remount but the issue still persists.
I am not able to list anything inside temp but able to see upto
/mnt.
Please let me know, went through other answers but none of them faced this on Gen2.</p>",0,0,2021-02-24 05:49:11.653000 UTC,,,1,azure-databricks|azure-data-lake-gen2,143,2013-02-27 15:14:04.467000 UTC,2022-02-16 11:14:12.063000 UTC,"Bangalore, Karnataka, India",222,13,1,32,,,,,,[]
Pyspark: transform categorial column into a binary array,"<p>I have a dataframe where there is a column where each value is a string, and i wish to transform it such that each appearance of a string for the specific id (represented in a different column) would serve as a one in the array in the appropriate location. As an example if i have this dataset:</p>

<pre><code>category   id
a          1
b          1
c          1
a          2
d          2
</code></pre>

<p>I wish to transform it into:</p>

<pre><code>id   result
1    [1,1,1,0]
2    [1,0,0,1]
</code></pre>

<p>Any idea on how to do this?</p>

<p>EDIT: i tried using the string indexer but all it did was count the amount of appearences up when i really wanted to get the array itself.</p>",1,0,2020-06-01 10:08:23.780000 UTC,,,-1,python|pyspark|azure-databricks,60,2018-11-06 08:45:26.337000 UTC,2022-02-27 16:44:55.723000 UTC,,39,0,0,12,,,,,,[]
Azure databricks : pandas.read_parquet error,"<p>I have some error to read parquet into pandas in databricks like in the following :</p>
<p><a href=""https://i.stack.imgur.com/5coWY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5coWY.png"" alt=""enter image description here"" /></a></p>
<p>anyone has an idea ?following is my databricks runtime.</p>
<p><a href=""https://i.stack.imgur.com/AolLI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AolLI.png"" alt=""enter image description here"" /></a></p>
<p>my pandas version</p>
<p><a href=""https://i.stack.imgur.com/nhRMa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nhRMa.png"" alt=""enter image description here"" /></a></p>",0,4,2021-10-21 15:14:02.607000 UTC,,2021-10-21 15:18:42.070000 UTC,2,pandas|pyspark|azure-databricks|fsspec,59,2019-05-03 07:43:53.913000 UTC,2022-02-19 16:28:58.100000 UTC,,367,0,0,93,,,,,,[]
How to fix read timeout exception in spark Cassandra connector,"<p>I am using spark 2.4 and scala 2.11 in azure databricks platform, DSE 6.0.7 and spark cassandra connector version 2.4.0</p>

<p>I am getting below error while getting the count for one of my table which has around ~100Million records. Exact row count is required for one of the applications. Below is my code - </p>

<pre><code>val count = spark.read
  .format(""org.apache.spark.sql.cassandra"")
  .option(""table"", tableName)
  .option(""keyspace"", keyspace)
  .load()
  .count()
</code></pre>

<p>Below is exception - </p>

<pre><code>java.io.IOException: Exception during execution of SELECT count(*) FROM ""mykeyspace"".""mytable"" WHERE token(""id"") &gt; ? AND token(""id"") &lt;= ?   ALLOW FILTERING: [/host:9042] Timed out waiting for server response
  at com.datastax.spark.connector.rdd.CassandraTableScanRDD.com$datastax$spark$connector$rdd$CassandraTableScanRDD$$fetchTokenRange(CassandraTableScanRDD.scala:350)
  at com.datastax.spark.connector.rdd.CassandraTableScanRDD$$anonfun$17.apply(CassandraTableScanRDD.scala:367)
  at com.datastax.spark.connector.rdd.CassandraTableScanRDD$$anonfun$17.apply(CassandraTableScanRDD.scala:367)
  at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
  at com.datastax.spark.connector.util.CountingIterator.hasNext(CountingIterator.scala:12)
  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:634)
  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
  at org.apache.spark.scheduler.Task.doRunTask(Task.scala:139)
  at org.apache.spark.scheduler.Task.run(Task.scala:112)
  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:497)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1432)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:503)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)
Caused by: com.datastax.driver.core.exceptions.OperationTimedOutException: [/host:9042] Timed out waiting for server response 
</code></pre>",0,3,2019-05-30 12:08:00.530000 UTC,1.0,,0,apache-spark|cassandra|azure-databricks,1016,2015-10-11 15:43:58.547000 UTC,2020-05-29 10:18:11.063000 UTC,,189,3,0,86,,,,,,[]
How to list sub-directories in a data lake file system on Azure Databricks using R,"<p>I am working in an R-notebook in databricks on Azure. Using AzureStor package, I can list the names of objects in the data lake or the file system therein, resp., the following way:</p>

<pre><code> endPoint &lt;- AzureStor::adls_endpoint(endpoint = ""https://&lt;myStorageName&gt;.dfs.core.windows.net"" ,key = &lt;myStorageKey&gt;)
 storage_containers &lt;- AzureStor::list_storage_containers(endPoint)

 paste0(""https://"", myStorageName,"".dfs.core.windows.net/"", names(storage_containers)[1]) -&gt; path2fs
 myFileSys &lt;- AzureStor::adls_filesystem(path2fs, key)
 AzureStor::list_adls_files(myFileSys, ""/"")
</code></pre>

<p>That gives my an R data.frame that comprises information about the ""name"" of the content and also a column ""isDirectory"". </p>

<p>If ""isDirectory"" is true, I would like to see the content of this directory. How does that work? Trying to set a new endpoint as</p>

<pre><code> endPoint &lt;- AzureStor::adls_endpoint(endpoint = ""https://&lt;myStorageName&gt;.dfs.core.windows.net/&lt;myDirectoryName&gt;"" ,key = &lt;myStorageKey&gt;)
</code></pre>

<p>fails.</p>

<p>So, how can I further let my code explore the directory and its content when the structure is like
DataLake -> FileSystem -> Directory -> Directory&amp;Files -> Directory&amp;Files -> ... etc.?</p>",1,0,2019-09-02 17:05:46.860000 UTC,,,0,r|sparkr|azure-databricks,636,2017-10-09 15:56:03.437000 UTC,2022-02-17 09:42:19.327000 UTC,,61,8,0,16,,,,,,[]
What is the best Python Design pattern for consuming Paginated Messages from a TWILIO's HTTP GET REST API response?,"<p>Trying to implement source_to_raw to consume Twilio API responses via a python script. Below is a sample code I have tried. I hope there should be a better way than this.</p>
<p>I'm exploring options to accomplish via Python helper libraries without any schema options as its only to raw_zone. I ran into infinite loops of never ending 'next_page_uri''s. Twilio offers pageSize limits but couldn't figure out an end of 'page'(s) for designing an exit condition for loops and conditional statements in my code. Any help regarding Twilio Pagination on Python-AzureDatabricks stack would be of great help.</p>
<p>Following is the sample code &amp; a couple of sample responses.</p>
<pre><code>page_data = page_response(url,date,creds)
data.update(page_data)
while(page_data['next_page_uri']!=None):
    page_data = page_response(url,date,creds)
    data.update(page_data)
    next_page_url=data['next_page_uri']
    src_url='https://api.twilio.com'
    url=src_url+next_page_url
    print(url)```

Sample Responses:   
#response_0:
{
   &quot;first_page_uri&quot;:&quot;&quot;,
   &quot;end&quot;:11111,
   &quot;previous_page_uri&quot;:&quot;/2010-04-01/..../&quot;,
   &quot;messages&quot;:[{raw...data}]
   &quot;next_page_uri&quot;:&quot;&quot;/2010-04-01/Accounts/ACXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX/Messages.json?start=2020-12-02PageSize=50&amp;Page=1&quot;
   &quot;page&quot;:0
}  
#response_1:
{
   &quot;first_page_uri&quot;:&quot;&quot;,
   &quot;end&quot;:49,
   &quot;previous_page_uri&quot;:&quot;&quot;,
   &quot;messages&quot;:[{raw...data}]
   &quot;next_page_uri&quot;:&quot;&quot;/2010-04-01/Accounts/ACXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX/Messages.json?start=2020-12-02PageSize=50&amp;Page=2&quot;
   &quot;page&quot;:1
}
</code></pre>",1,0,2020-12-02 21:58:12.267000 UTC,,,0,python|pagination|python-requests|twilio|azure-databricks,180,2014-04-26 12:12:05.093000 UTC,2022-02-02 16:29:18.807000 UTC,"Atlanta, GA, USA",216,7,0,51,,,,,,[]
"Git ""stucked"" branch","<p>I'm having problem pushing my branch to the remote repository. Git returns me an error:
<code>error: dst refspec refs/heads/XXX matches more than one.</code> When I run <code>git ls-remote</code> it only shows me one <code>XXX</code> branch though! (I've tried to delete the branch using <code>git push origin :refs/heads/XXX</code> as well with the same result)</p>

<p>We've faced this issue more than once in more than one Git repositories. What we did previously was to clone the repositories and the ""stucked"" branch was gone from the cloned repositories.</p>

<p>I'm wondering if there is any alternative solutions to solve this issue without have to resort to re-clone the repository. Git version is 1.7.XX.</p>

<p>Thanks!</p>",1,0,2014-04-10 12:33:36.823000 UTC,4.0,,5,git|version-control|dvcs,1001,2013-06-21 06:52:33.090000 UTC,2015-04-09 11:19:41.727000 UTC,,564,5,1,28,,,,,,[]
How to resolve GC overhead in pyspark Databricks,"<p>I am trying to load a 100 gb json file in a spark dataframe and creating an temporary view over it. Then I am querying the data in this view with query </p>

<pre><code>select * from &lt;table_view&gt; limit 1;
</code></pre>

<p>But The query doesn't get complete rather it gives Error <code>Caused by : java.lang.OutOfMemoryError: GC Overhead limit exceeded.</code> I am running my code in databricks cluster. Here are my cluster details:</p>

<pre><code>Cluster Mode : standard
Databricks runtime Version : 6.1(Apache Spark 2.4.4)
Worker Type : 56 GB Memory, 16 cores 3 DBU (min worker= 2, max worker = 8) 
Driver Type : 56 GB Memory, 16 cores 3 DBU
</code></pre>

<p>I tried setting the below config parameters but didn't get any success.</p>

<pre><code>spark.conf.set(""spark.executor.memory"", '50g')
spark.conf.set('spark.executor.cores', '5')
spark.conf.set('spark.cores.max', '16')
spark.conf.set(""spark.driver.memory"",'30g')
spark.conf.set(""spark.yarn.executor.memoryOverhead"",4096)
</code></pre>

<p>I am very new to apache spark. Please let me know if some other details are required. </p>",1,0,2020-03-31 09:07:52.193000 UTC,,,1,java|apache-spark|pyspark|garbage-collection|azure-databricks,799,2018-02-12 11:55:37.653000 UTC,2022-02-25 08:07:16.973000 UTC,"Hyderabad, Telangana, India",495,24,0,92,,,,,,[]
Why is Github upstream while the main repo is the local repo?,"<p>I just started with Git and Github and I am confused with the terms 'upstream' and 'downstream' when it comes to pushing. </p>

<p>I read this question here: <a href=""https://stackoverflow.com/questions/2739376/definition-of-downstream-and-upstream"">Definition of &quot;downstream&quot; and &quot;upstream&quot;</a> but the answer from brian d foy (most voted at time of writing this question) refers to cloning from a repo. </p>

<p>I am not cloning. I started with work in a directory and made it a repository with <code>git init</code>. I intend to work on it on my laptop and save my work on Github in case something happens to my laptop. My local repo will be the main repo. </p>

<p>Why does the command to push to Github have  <code>-u</code> option in <code>git push -u origin master</code> which means upstream. Won't I be pushing downstream if the main repo is on my computer? </p>",2,2,2017-03-24 19:38:03.837000 UTC,1.0,2017-05-23 12:25:27.293000 UTC,0,git|github|version-control|dvcs,92,2016-10-31 01:00:10.630000 UTC,2022-03-05 19:30:17.460000 UTC,,1238,1794,1,190,,,,,,[]
Sharepoint OnPremise Integration with Azure,"<p>Is there any way to integrate SharePoint on premise with Azure ADF,though there is a SharePoint connector in azure ADF but its only work with SharePoint online.</p>
<p>Is there any other way around to download from SharePoint on premise using python or scala</p>
<p>Thanks</p>",1,0,2021-06-07 16:35:32.197000 UTC,,,0,python|azure|sharepoint|azure-data-factory-2|azure-databricks,115,2017-10-07 18:36:26.833000 UTC,2021-12-16 03:38:02.400000 UTC,India,11,0,0,10,,,,,,[]
Slow insertion using Neptune and Gremlin,"<p><br />
I'm having problems with the insertion using gremlin to Neptune.
I am trying to insert many nodes and edges, potentially hundred thousands of nodes and edges, <strong>with checking for existence</strong>.</p>
<p>Currently, we are using <strong>inject</strong> to insert the nodes, and the problem is that it is <strong>slow</strong>.</p>
<p>After running the explain command, we figured out that the problem was the <strong>coalesce</strong> and the <strong>where</strong> steps - it takes more than 99.9% of the run duration.</p>
<p>I want to <strong>insert each node and edge only if it doesn’t exist,</strong> and that’s why I am using the coalesce and where steps.</p>
<p>For example, the query we use to insert nodes with inject:</p>
<pre><code>properties_list = [{‘uid’:’1642’},{‘uid’:’1322’}…]
g.inject(properties_list).unfold().as_('node')
  .sideEffect(__.V().where(P.eq('node')).by(‘uid).fold()
  .coalesce(__.unfold(), __.addV(label).property(Cardinality.single,'uid','1')))
</code></pre>
<p>With 1000 nodes in the graph and <strong>properties_list</strong> with 100 elements, running the query above takes around 30 seconds, and it gets slower as the number of nodes in the graph increases.<br />
Running a naive injection with the same environment as the query above, <strong>without</strong> coalesce and where, takes less than 1 second.
I’d like to hear your suggestions and to know what are the best practices for inserting many nodes and edges (with checking for existence).</p>
<p>Thank you very much.</p>",1,0,2022-03-02 13:54:03.230000 UTC,1.0,,0,amazon-web-services|gremlin|tinkerpop|amazon-neptune|tinkerpop3,41,2015-05-19 18:54:33.720000 UTC,2022-03-04 20:07:17.613000 UTC,,56,2,0,6,,,,,,[]
Failure to start a Neptune notebook,"<p>I can't seem to make a neptune notebook, everytime I try I get the following error:</p>

<pre><code>Notebook Instance Lifecycle Config 'arn:aws:sagemaker:us-west-2:XXXXXXXX:notebook-instance-lifecycle-config/aws-neptune-tutorial-lc' 
for Notebook Instance 'arn:aws:sagemaker:us-west-2:XXXXXXXXX:notebook-instance/aws-neptune-tutorial' 
took longer than 5 minutes. 
Please check your CloudWatch logs for more details if your Notebook Instance has Internet access.
</code></pre>

<p>Note that the cloudwatch logs that it suggests to look at don't exist.</p>

<p>The neptune database was created using this cloudformation template: <a href=""https://github.com/awslabs/aws-cloudformation-templates/blob/master/aws/services/NeptuneDB/Neptune.yaml"" rel=""nofollow noreferrer"">https://github.com/awslabs/aws-cloudformation-templates/blob/master/aws/services/NeptuneDB/Neptune.yaml</a></p>

<p>Which created the neptune cluster in the default VPC. </p>

<p>The notebook instance was created using this cloudformation template: <a href=""https://s3.amazonaws.com/aws-neptune-customer-samples/neptune-sagemaker/cloudformation-templates/neptune-sagemaker/neptune-sagemaker-nested-stack.json"" rel=""nofollow noreferrer"">https://s3.amazonaws.com/aws-neptune-customer-samples/neptune-sagemaker/cloudformation-templates/neptune-sagemaker/neptune-sagemaker-nested-stack.json</a></p>

<p>passing in the relevant values from in for the created neptune stack.</p>

<p>Has anyone seen this type of error and knows how to get over it?</p>",2,0,2020-03-18 19:55:39.587000 UTC,,2021-11-23 23:19:22.237000 UTC,2,amazon-neptune|graph-notebook,692,2013-02-07 02:21:18.593000 UTC,2022-03-02 17:59:43.957000 UTC,,1494,595,3,98,,,,,,[]
Azure Databricks Concurrent Job -Avoid Consuming the same eventhub messages in all Jobs,"<p>Please help us to implement the partition/grouping when receiving eventhub messages in a Azure Databricks concurrent Job and the right approach to consume eventhub messages in a concurrent job. </p>

<p>Created 3 concurrent jobs in Azure Databricks uploading consumer code written in scala as a jar files. In this case receiving the same messages in all 3 concurrent jobs.  To overcome from this issue tried to consume the events by partitioning but receiving the same messages in all 3 partitions.</p>

<p>And also tried by sending messages based on partition key and also tried creating a  consumer groups in eventhubs even though receiving same messages in all the groups. We are not sure to handle the eventhub messages in the concurrent job</p>

<p>EventHub Configuration:</p>

<p>No.of partitions is 3 and  Message Retention is 1
EventHub Producer: Sending messages to Eventhub using .NET (C#) is working fine.
EventHub Consumer: Able to receive messages through Scala Program without any issues.</p>

<p>Problem : Created 3 concurrent jobs in Azure Databricks uploading consumer code written in Scala as a jar files.In this case receiving the same messages in all 3 concurrent jobs.  To overcome from this issue tried to consume the events by partitioning but receiving the same messages in all 3 partitions.And
also tried by sending messages based on partition key and also tried creating a  consumer groups in eventhubs even though receiving same messages in all the groups. We are not sure to handle the eventhub messages in the concurrent job. </p>

<p>Producer C# Code:</p>

<pre><code>string eventHubName = ConfigurationManager.AppSettings[""eventHubname""];
string connectionString = ConfigurationManager.AppSettings[""eventHubconnectionstring""];
eventHubClient = EventHubClient.CreateFromConnectionString(connectionString, eventHubName);

for (var i = 0; i &lt; 100; i++)
    {       
        var sender = ""event hub message 1""  + i;
        var data = new EventData(Encoding.UTF8.GetBytes(sender));
        Console.WriteLine($""Sending message: {sender}"");
        eventHubClient.SendAsync(data);
    }
eventHubClient.CloseAsync();
Console.WriteLine(""Press ENTER to exit."");
Console.ReadLine();
</code></pre>

<p>Consumer Scala Code:</p>

<pre><code>object ReadEvents {
  val spark = SparkSession.builder()
    .appName(""eventhub"")
    .getOrCreate()
  val sc = spark.sparkContext
  val ssc = new StreamingContext(sc, Seconds(5))
  def main(args : Array[String]) : Unit = {

    val connectionString = ConnectionStringBuilder(""ConnectionString"").setEventHubName(""eventhub1"").build
    val positions = Map(new NameAndPartition(""eventhub1"", 0) -&gt; EventPosition.fromStartOfStream)
    val position2 = Map(new NameAndPartition(""eventhub1"", 1) -&gt; EventPosition.fromEnqueuedTime(Instant.now()))
    val position3 = Map(new NameAndPartition(""eventhub1"", 2) -&gt; EventPosition.fromEnqueuedTime(Instant.now()))

    val ehConf = EventHubsConf(connectionString).setStartingPositions(positions)
    val ehConf2 = EventHubsConf(connectionString).setStartingPositions(position2)
    val ehConf3 = EventHubsConf(connectionString).setStartingPositions(position3)
    val stream = org.apache.spark.eventhubs.EventHubsUtils.createDirectStream(ssc, ehConf)
    println(""Before the loop"")
    stream.foreachRDD(rdd =&gt; {
          rdd.collect().foreach(rec =&gt; {
        println(String.format(""Message is first stream ===&gt;: %s"", new String(rec.getBytes(), Charset.defaultCharset())))
         })
    })
    val stream2 = org.apache.spark.eventhubs.EventHubsUtils.createDirectStream(ssc, ehConf2)
        stream2.foreachRDD(rdd2 =&gt; {
          rdd2.collect().foreach(rec2 =&gt; {
    println(String.format(""Message  second stream is ===&gt;: %s"", new String(rec2.getBytes(), Charset.defaultCharset())))

      })
    })
    val stream3 = org.apache.spark.eventhubs.EventHubsUtils.createDirectStream(ssc, ehConf)
     stream3.foreachRDD(rdd3 =&gt; {
      println(""Inside 3rd stream foreach loop"")
      rdd3.collect().foreach(rec3 =&gt; {
        println(String.format(""Message is third stream ===&gt;: %s"", new String(rec3.getBytes(), Charset.defaultCharset())))

      })
    })
    ssc.start()
    ssc.awaitTermination()
  }

}
</code></pre>

<p>Expecting to partition the eventhub messages properly when receiving it on   concurrent jobs running using scala program.</p>",1,0,2019-07-25 18:39:38.747000 UTC,,2019-07-26 09:04:01.017000 UTC,1,scala|azure-eventhub|azure-databricks,163,2016-07-06 05:47:52.607000 UTC,2019-07-25 19:35:21.400000 UTC,,11,0,0,0,,,,,,[]
Suggestions for neptune DB graph traversal management,"<p>We have implemented AWS Neptune DB as the graph implementation for our solution, I am having an issue with the connection being timed out multiple times.</p>

<p>This is our local development environment (this is a springboot application) where we access the neptune DB via an ALB and we have setup the traversal like below:</p>

<pre><code>@Bean
public GraphTraversalSource createReadGraphTraversalSource(){

LOGGER.debug(""Connecting to neptune endpoint : {}"", neptuneDBEndpoint);
Cluster cluster = Cluster.build()
    .addContactPoint(neptuneDBEndpoint)
      .port(80)
        .enableSsl(false)
          .serializer(Serializers.GRAPHBINARY_V1D0)
            .create();

LOGGER.trace(""Read Traversal source setup successfully"");
return traversal().withRemote(DriverRemoteConnection.using(cluster));
</code></pre>

<p>}</p>

<p>We then create TraversalSources from the GraphTraversalSource object and use this to query as below:</p>

<pre><code>   public List&lt;ObjectTO&gt; findLeadsByProject(String projectSid) throws DatabaseException, RecordNotFoundException {
    List objectVs = null;
    try (GraphTraversal&lt;Vertex, Vertex&gt; graphTraversal = readG.V(parentVSid)) {
        leadVs = graphTraversal.out(GraphEdgeLabels.pHasO.name()).hasLabel(GraphVertexLabels..name()).valueMap()
                .toList();

        objectVs.forEach(vertexMap-&gt;{
            final ObjectTO objectTO = objectTOSupplier.get();
            objectTO.setSystemAttributes((Map&lt;EnumAttributes, String&gt;) vertexMap);
            objectVs.add(objectTO);
        });

    } catch (Exception e) {
        throw new DatabaseException(e);
    }

    if (objectVs.isEmpty()) {
        throw new RecordNotFoundException();
    }
    return objectVs;
}
</code></pre>

<p>My call to this DAO API is pretty conventional, RestController -> Service -> DAO API.</p>

<p>The issue that i am facing is that when i make a query i can see that the DAO API returns the results to the service API and the service API does the same to the Rest API but some error happens in there and my API call ends with a 500 internal server error. The Rest API error is not an issue as this is something that i need to take care of, but what is troubling me is that subsequent calls to this API ends up with the following error.</p>

<p>java.util.concurrent.TimeoutException: Timed-out waiting for connection on Host{address=neptune-aws-alb.amazonaws.com/X.XXX.XX.XX:80, hostUri=ws://X.XXX.XX.XX:80/gremlin} - possibly unavailable</p>

<p>I am not sure why this error is happening.</p>",0,2,2020-02-19 13:42:02.630000 UTC,,,0,spring-boot|amazon-neptune,113,2013-03-14 19:18:22.823000 UTC,2020-12-08 06:52:45.573000 UTC,,33,0,0,14,,,,,,[]
Send string gremlin query to Amazon Neptune database using TinkerPop's gremlinpython,"<p>We can do the following to create a connection, and then attached the connection to the graph <code>g</code> object, and then use <code>g</code> to mirror gremlin query inline.</p>
<pre class=""lang-py prettyprint-override""><code>    from gremlin_python import statics
    from gremlin_python.structure.graph import Graph
    from gremlin_python.process.graph_traversal import __
    from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection
    Create a GraphTraversalSource which is the basis for all Gremlin traversals:
    
    graph = Graph()
    connection = DriverRemoteConnection('ws://localhost:8182/gremlin', 'g')
    g = graph.traversal().withRemote(connection)
    g.V().limit(2).toList()
</code></pre>
<p>However, I want to submit string grelmin query like below,</p>
<pre class=""lang-py prettyprint-override""><code>    connection = DriverRemoteConnection('ws://localhost:8182/gremlin', 'g')
    
    query = &quot;g.V().limit(2).toList()&quot;
    connection.submit(query)

</code></pre>
<p>Then I'm getting the following error. Looks like I did NOT call the <code>submit()</code> function correctly, and I can't find any docs or examples on this function. Please help.</p>
<pre><code>[ERROR] AttributeError: 'str' object has no attribute 'source_instructions'
Traceback (most recent call last):
  File &quot;/var/task/sentry_sdk/integrations/aws_lambda.py&quot;, line 152, in sentry_handler
    return handler(aws_event, aws_context, *args, **kwargs)
    response = remoteConn.submit(query)
  File &quot;/var/task/gremlin_python/driver/driver_remote_connection.py&quot;, line 56, in submit
    result_set = self._client.submit(bytecode, request_options=self._extract_request_options(bytecode))
  File &quot;/var/task/gremlin_python/driver/driver_remote_connection.py&quot;, line 81, in _extract_request_options
    options_strategy = next((x for x in bytecode.source_instructions
</code></pre>",1,0,2021-12-18 01:09:55.137000 UTC,,2021-12-19 00:20:33.770000 UTC,0,amazon-web-services|gremlin|tinkerpop|amazon-neptune|gremlinpython,52,2012-02-03 16:20:42.710000 UTC,2022-03-04 16:51:03.243000 UTC,,5628,119,6,508,,,,,,[]
Databricks SQL Server connection using integrated authentication,"<p>I'm trying to connect my Databricks cluster to an existing SQL Server database using python. I will like to leverage the integrated authentication method. Getting error <code>com.microsoft.sqlserver.jdbc.SQLServerException: This driver is not configured for integrated authentication.</code></p>

<pre><code>jdbcHostname = ""sampledb-dev.database.windows.net""
jdbcPort= 1433
jdbcDatabase = ""sampledb-dev""
jdbcUrl = ""jdbc:sqlserver://{0}:{1}; database={2}"".format(jdbcHostname, jdbcPort, jdbcDatabase)

connectionProperties={
  ""integratedSecurity"" : ""true"",
  ""driver"" : ""com.microsoft.sqlserver.jdbc.SQLServerDriver""
}

print(jdbcUrl)
query =""(SELECT * FROM TABLE1.Domain)""

domains = spark.read.jdbc(url = jdbcUrl, table = query, properties = connectionProperties)
display(domains) 
</code></pre>",1,1,2020-04-27 21:21:39.593000 UTC,,,0,python|sql-server|azure-active-directory|azure-databricks|databricks-connect,1909,2019-11-13 18:46:26.490000 UTC,2022-01-14 18:14:41.117000 UTC,,333,128,1,66,,,,,,[]
How to use neptune-export,"<p>Can someone please list out a detailed stepwise process to export data from Neptune to S3(or local storage) in form of CSV.
I followed the doc(which seems to be the only resource available online), but it is not very clear.
TIA</p>",1,1,2021-09-09 10:36:00.933000 UTC,,,0,amazon-web-services|amazon-s3|amazon-neptune,135,2021-09-09 10:32:05.717000 UTC,2021-10-08 10:56:46.677000 UTC,,1,0,0,10,,,,,,[]
How to reuse the Azure Data Factory pipeline for multiple users,"<p>I have a Azure data factory pipeline that is calling a Databricks notebook.
I have parameterized the pipeline and via this pipeline I am passing the product name to the databricks notebook.</p>
<p>Based on the parameter the Databricks will push the processed data into the specific ADLS directory.
Now the problem is- How do I make my pipeline aware that which parameter need to pass to the Databricks.</p>
<p>Example: If I pass the <strong>Nike</strong> via the adf to the databricks then my data would get pushed into Nike directory or If I pass <strong>Adidas</strong> then data would get pushed into Adidas directory.</p>
<p>Please note that I am triggering the ADF from the automation account.</p>",1,1,2020-07-09 05:49:30.393000 UTC,,,0,azure|azure-pipelines|azure-data-factory|azure-databricks|azure-automation,328,2020-01-16 14:10:24.510000 UTC,2020-07-09 19:27:14.627000 UTC,,57,0,0,22,,,,,,[]
How to stop concurrent writing in Delta Lake External Table?,"<p>General EXTERNAL table like Oracle doesn't allow Insert/Update operation. But Databricks EXTERNAL Delta Table enables Update/Insert operation. This way I can see a flaw, or is there anyway to stop that? Example -</p>

<pre><code>CREATE TABLE employee
USING DELTA
LOCATION '/mnt/ADLS/employee'
PARTITIONED BY (date)
</code></pre>

<p>This will create an External table of <code>employee</code> loading data from <code>/mnt/ADLS/employee</code>. Now if I again create another External table say <code>employee_new</code> and do an <code>INSERT</code> it will get reflected to actual <code>employee</code> table. Is there anyway to stop this?</p>",0,4,2019-06-16 14:49:11.280000 UTC,,,0,azure-databricks|delta-lake,193,2013-05-04 15:49:41.620000 UTC,2022-02-23 22:26:41.467000 UTC,"Kolkata, India",5360,541,106,983,,,,,,[]
"using ""project"" with ""select"" in gremlin-javascript is throwing error","<p>I have a simple query which gives me expected result when i run it on console, but fails when i run it in aws-neptune DB using gremlin node.js driver/ gremlin-javascript.</p>

<p>query running successfully in console </p>

<pre><code>g.V().hasLabel('item').project('id').by(id).select(values)
==&gt;[item1]
==&gt;[item2]
==&gt;[item3]
</code></pre>

<p>I tried to ran same query in gremlin-javascript using import  ""gremlin.process.t""</p>

<pre><code>g.V().hasLabel('item').project('id').by(gremlin.process.t.id).select(gremlin.process.t.values)

</code></pre>

<p>But i get following error ""detailedMessage"":""null:select([null])""}</p>

<pre><code>error Error: Server error: {""requestId"":""0521e945-04fb-4173-b4fe-0426809500fc"",""code"":""InternalFailureException"",""detailedMessage"":""null:select([null])""} (599)

</code></pre>

<p>What is the correct way to use project with select in gremlin-javascript ??</p>",1,0,2019-05-19 20:13:26.083000 UTC,1.0,,1,gremlin|amazon-neptune,394,2018-11-22 14:54:38.193000 UTC,2019-08-01 18:21:20.080000 UTC,,11,0,0,3,,,,,,[]
"Converting RDFConnection.load(String graphName, Model model) into SPARQL Update notation while specifying named graph","<p>I am working on a code base that uses Apache Jena (3.14.0) to save triples into either Anzo or Fuseki (for local testing).</p>
<p>I am trying to adapt the code to support AWS Neptune - see related <a href=""https://stackoverflow.com/questions/67077733/load-data-into-aws-neptune-via-apache-jena-missing-query-or-update-param?noredirect=1#comment118572622_67077733"">question</a>.</p>
<p>A fellow SO user brought my attention to the fact that Neptune does not support GSP.</p>
<p>The code I'm looking at persists triples with the <a href=""http://localhost:63342/digital-twin-parent/jena-rdfconnection-3.14.0-javadoc.jar/org/apache/jena/rdfconnection/RDFConnection.html#load-java.lang.String-org.apache.jena.rdf.model.Model-"" rel=""nofollow noreferrer"">RDFConnection.load(String graphName, Model model)</a> notation.</p>
<p>My idea was to convert it to <a href=""http://localhost:63342/digital-twin-parent/jena-rdfconnection-3.14.0-javadoc.jar/org/apache/jena/rdfconnection/RDFConnection.html#update-org.apache.jena.update.Update-"" rel=""nofollow noreferrer"">RDFConnection.update(Update update)</a>.</p>
<p>In other words:</p>
<pre><code>myRdfConnectionInstance.load( myGraphNameString, myJenaModel )
</code></pre>
<p>... would become something in the lines of:</p>
<pre><code>myRdfConnectionInstance.update(
    new UpdateBuilder()
        .addInsert( myGraphNameString, myJenaModel )
        .build()
);
</code></pre>
<p>(<code>myGraphNameString</code> represents a URN)</p>
<p>My take was that this notation would employ the SPARQL update protocol as opposed to GSP, hence enabling persisting the triples in Neptune.</p>
<p>I was comforted in that regard by the fact that, if I omitted the named graph parameter and just invoked <code>.addInsert( myJenaModel )</code>, the request would be valid with all triple stores I tried it with.</p>
<p>Unfortunately the same parametrized with a named graph fails not only with Neptune, but also with my local Fuseki store.</p>
<p>The <a href=""http://localhost:63342/digital-twin-parent/jena-querybuilder-3.14.0-javadoc.jar/org/apache/jena/arq/querybuilder/UpdateBuilder.html#addInsert-java.lang.Object-org.apache.jena.rdf.model.Model-"" rel=""nofollow noreferrer"">javadoc</a> states:</p>
<blockquote>
<p>Add all the statements in the model a specified graph to the insert statement.[...]</p>
</blockquote>
<p>... which was confusing in terms of English, but seemed to lean towards what I wanted.</p>
<p>I suspect the second part of the description:</p>
<blockquote>
<p>The graph object is converted by a call to makeNode().</p>
</blockquote>
<p>... is where I'm messing up.</p>
<p>Unfortunately I happen to be neither too familiar with triple stores in depth, nor with Jena, so I don't know where to chase next.</p>
<p><strong>Questions</strong></p>
<ul>
<li>Is <code>RDFConnection#update</code> the right direction to convert the write notation to SPARQL update, hence preparing for compatibility with Neptune?</li>
<li>If so, what am I missing about the parametrization of the graph name?</li>
<li>If there any additional documentation that would be relevant, aside from the APIs quoted here?</li>
</ul>
<p><strong>Some error messages</strong></p>
<p>The response I get from Neptune looks like (formatting added for clarity):</p>
<pre><code>Http exception response 
{
&quot;detailedMessage&quot;:&quot;Malformed query: Illegal subject value: 
    \&quot;urn:[my URN]\&quot;^^&lt;http://www.w3.org/2001/XMLSchema#string&gt; [line 2]&quot;,
&quot;code&quot;:&quot;MalformedQueryException&quot;,&quot;requestId&quot;:&quot;[some UUID]&quot;
}
</code></pre>
<p>No explicit error message from Fuseki, just the HTTP 400.</p>
<p>The stack trace looks like:</p>
<pre><code>org.apache.jena.atlas.web.HttpException: 400 - Bad Request
    at org.apache.jena.riot.web.HttpOp.exec(HttpOp.java:1091)
    at org.apache.jena.riot.web.HttpOp.execHttpPost(HttpOp.java:721)
    at org.apache.jena.riot.web.HttpOp.execHttpPost(HttpOp.java:517)
    at org.apache.jena.riot.web.HttpOp.execHttpPost(HttpOp.java:473)
    at org.apache.jena.rdfconnection.RDFConnectionRemote.lambda$updateExec$6(RDFConnectionRemote.java:324)
    at org.apache.jena.rdfconnection.RDFConnectionRemote.exec(RDFConnectionRemote.java:668)
    at org.apache.jena.rdfconnection.RDFConnectionRemote.updateExec(RDFConnectionRemote.java:324)
    at org.apache.jena.rdfconnection.RDFConnectionRemote.update(RDFConnectionRemote.java:311)
    at org.apache.jena.rdfconnection.RDFConnection.update(RDFConnection.java:250)
    at [my code]
</code></pre>",0,13,2021-04-15 12:03:41.303000 UTC,,2021-04-15 16:51:18.983000 UTC,1,sparql|jena|amazon-neptune,97,2011-04-14 09:37:56.253000 UTC,2022-03-05 09:33:38.053000 UTC,"Berlin, Germany",46537,673,6480,5274,,,,,,[]
Hive casting function,"<p>In a hive table how can I add the <strong>'-'</strong> sign in a field, but for random records? If I use the syntax below it changes <strong>all</strong> the records in the field to negative, but I want to change <strong>random</strong> records to negative.</p>

<p>This is the syntax I used which changed all the records to negative:</p>

<pre class=""lang-sql prettyprint-override""><code>CAST(CAST(-1 AS DECIMAL(1,0)) AS DECIMAL(19,2)) 
*CAST(regexp_replace(regexp_replace(TRIM(column name),'\\-',''),'-','') as decimal(19,2)),
</code></pre>",1,0,2019-10-07 19:02:48.740000 UTC,,2019-10-07 21:01:58.977000 UTC,0,sql|hive|azure-databricks|azure-notebooks,104,2019-10-07 18:55:13.523000 UTC,2019-10-14 14:50:57.787000 UTC,,1,0,0,0,,,,,,[]
Is there a distributed version control system that handles big binaries well?,"<p>A few months ago we switched from Subversion to Git (mainly because Git is faster, more convenient for branching and generaly less pain). Only after some time we realized that Git is very ill-suited to projects with big binary files. As an example, Git hosting sites like GitHub or Bitbucket don’t like to support repo size much over 1 GB, because such repos swallow <em>a lot</em> of system resources on the server. Even on our desktop machines some repos with big binary artworks (like 100MB Photoshop files) take a lot of memory and CPU power. It seems like the consensus is that these big binary files don’t belong to the code repository, at least as far as Git goes.</p>

<p>Are there distributed version control systems that would handle big binaries just fine, with no extra memory or CPU requirements? I’m talking about file sizes in hundreds of MBs and repo sizes anywhere from 1 GB to 10 GBs.</p>",1,1,2012-10-18 15:00:11.600000 UTC,2.0,,2,version-control|dvcs|binaryfiles,179,2008-09-18 05:53:12.510000 UTC,2022-03-03 17:00:45.757000 UTC,"Boskovice, Czech Republic",99042,3497,338,6136,,,,,,[]
How to sharing code between two projects on Azure Databricks,"<p>I have two ML projects on Azure Databricks that work almost the same except that they are for different clients. Essentially I want to use some management system so I can share and reuse the same code across different projects. (i.e. python files that store helpful functions for feature engineering, Databricks notebooks that perform similar initial data preprocessing, some configuration files, etc.) At same time, if an update is made in the shared code, it needed to be sync with all the projects that use the code.</p>
<p>I know for Git we can use submodule to do this where we have common code stored in Repo C, and add it as a submodule to Repo A and Repo B. But the problem is that Azure Databricks doesn't support submodule. Also, it only supports working branch up to 200 MB, so I cannot do Monorepo (i.e. have all the code in one repository) either. I was thinking creating a package for shared Python files, but I also have a few core version of notebooks that I want to share which I don't think is possible to built as a package?</p>
<p>Is there any other ways that I can do this on Databricks so I can reuse the code and don't just copy and paste?</p>",0,3,2022-02-15 16:11:40.410000 UTC,,2022-02-15 17:05:06.347000 UTC,1,git|git-submodules|dependency-management|azure-databricks|databricks-repos,30,2022-02-15 15:45:04.243000 UTC,2022-03-04 20:36:14.200000 UTC,,11,0,0,0,,,,,,[]
Databricks Widget Panel Default Settings,"<p>I have a number of notebooks which have widgets and currently the default setting for the 'On Widget Change' is 'Run Accessed Commands'. Is there any way of globally setting this to 'Do Nothing'. </p>

<p>I can do this on an individual notebook, but if I close the notebook and then re-open, it has reset itself to 'Run Accessed Commands'</p>",0,0,2020-03-11 08:41:16.990000 UTC,1.0,,4,azure-databricks,305,2019-08-28 12:05:37.513000 UTC,2022-03-04 12:12:43.257000 UTC,"Bournemouth, UK",69,0,0,20,,,,,,[]
"How to do efficient bulk upserts (insert new vertex, or update property(ies)) in Gremlin?","<p><strong>Context</strong>:  </p>

<p>I do have a graph with about 2000 vertices, and 6000 edges, this over time might grow to 10000 vertices and 100000 edges. Currently I am upserting the new vertices using the following traversal query:</p>

<p><em>Upserting Vertices &amp; Edges</em></p>

<pre><code>queryVertex = ""g.V().has(label, name, foo).fold().coalesce(
               unfold(), addV(label).property(name, foo).property(model, 2)
               ).property(model, 2)""
</code></pre>

<p>The intent here is to look for vertex, named foo, and if found update its <code>model</code> property, otherwise create a new vertex and set the <code>model</code> property. this is issued twice: once for the source vertex and then for the target vertex.<br>
Once the two related vertices are created, another query is issued to create the edge between them: </p>

<pre><code>queryEdge = ""g.V('id_of_source_vertex').coalesce(
             outE(edge_label).filter(inV().hasId('id_of_target_vertex')), 
             addE(edge_label).to(V('id_of_target_vertex'))
             ).property(model, 2)""
</code></pre>

<p>here, if there is an edge between the two vertices, the <code>model</code> property on edge is updated, otherwise it creates the edge between them.  </p>

<p>And the pseudocode that does this, is something as follows: </p>

<pre><code>for each edge in the list of new edges:
   //upsert source and target vertices:  
   execute queryVertex for edge.source
   execute queryVertex for edge.target
   // upsert edge: 
   execute queryEdge
</code></pre>

<p>This works, but it is highly inefficient; for example for the mentioned graph size it takes several minutes to finish, and with some in-app concurrency, it reduces the time only by couple of minutes. Surely, there must be a more efficient way of doing this for such a small graph size.</p>

<p><strong>Question</strong><br>
* How can I make these upserts faster? </p>",1,0,2020-06-09 05:19:51.727000 UTC,1.0,2020-06-11 23:07:13.207000 UTC,5,graph|gremlin|tinkerpop|tinkerpop3|amazon-neptune,1400,2012-11-09 04:56:39.227000 UTC,2020-10-23 18:34:43.597000 UTC,,277,55,0,88,,,,,,[]
How to authenticate users on TortoiseHg web server?,"<p>TortoiseHg web server configuration has an ""Allow Push"" parameter where I put the users allowed to push changes. But when I try to push I get an ""authorization failed"" error. How are the users authenticated? Where do the passwords come from?</p>",3,0,2009-12-10 03:27:59.137000 UTC,2.0,,7,authentication|mercurial|dvcs|tortoisehg,7260,2009-06-28 02:18:43.113000 UTC,2022-03-04 22:27:22.153000 UTC,,4934,276,11,1141,,,,,,[]
Unexpected Count & Filter Behaviour in AWS Neptune,"<p>I'm getting an unexpected <code>StopIteration</code> error with some gremlin queries that contain a <code>count</code> step within nested <code>filter</code> steps.</p>
<p>This error can be recreated with the following code (using <code>Gremlin-Python</code>, <code>3.5.0</code> in my case):</p>
<pre><code>filter_header = g.addV().id().next()
count_headers = [g.addV().id().next() for _ in range(10)]

for i, c in enumerate(count_headers):
    # Add 10 nodes
    sub_nodes = [g.addV().id().next() for _ in range(10)]
    # Connect them all to the header
    for s in sub_nodes:
        g.V(c).addE('edge').to(__.V(s)).iterate()
    # Connect i of them to the filter header
    for s in sub_nodes[:i]:
        g.V(filter_header).addE('edge').to(__.V(s)).iterate()

# This raises StopIterationError
g.V(count_headers).filter(
    __.out('edge').filter(
        __.in_('edge').hasId(filter_header)
    ).count().is_(P.gt(1))
).count().next()
</code></pre>
<p>(Equivalently if using <code>toList</code> instead of <code>next</code> I get an empty list)</p>
<p>However this error doesn't happen if you <code>unfold</code> after the <code>count</code>:</p>
<pre><code># No StopIterationError
g.V(count_headers).filter(
    __.out('edge').filter(
        __.in_('edge').hasId(filter_header)
    ).count().unfold().is_(P.gt(1))
).count().next()
</code></pre>
<p>Neither does it happen if you use <code>map</code> instead of <code>filter</code>:</p>
<pre><code># No StopIterationError
g.V(count_headers).as_('c').map(
    __.out('edge').filter(
        __.in_('edge').hasId(filter_header)
    ).count().is_(P.gt(1))
).select('c').count().next()
</code></pre>
<p>I've tested and this error doesn't happen when using TinkerGraph, so I suspect this is specific to AWS Neptune.</p>
<p>I'd really appreciate any guidance as to why this happens, if I'm doing anything wrong, or what the differences are that means this just happens in Neptune. Alternatively - if the consensus is that this is a bug - I'd appreciate it if anyone could let me know where to raise it.</p>",1,0,2021-11-25 10:37:10.460000 UTC,,,0,gremlin|amazon-neptune,107,2021-10-05 15:02:07.300000 UTC,2022-03-04 16:07:33.413000 UTC,,1,0,0,2,,,,,,[]
How do we write gremlin update/modify query in node.js?,"<p>How do we write gremlin update/modify query in node.js  ?</p>
<p>I want to update particular field and save the same and do not modify other fields which are not edited.</p>
<p>I have a table called &quot;org&quot; and it has some properties like name, id, label.<br />
How can we modify particular property or all the properties depends on the put body request ? using gremlin query</p>",1,1,2021-03-03 19:24:30.923000 UTC,,2021-03-04 08:06:14.327000 UTC,0,node.js|gremlin|amazon-neptune,155,2020-08-11 14:43:34.270000 UTC,2021-10-01 18:41:00.777000 UTC,,87,3,0,6,,,,,,[]
Databricks notebook is not deployed to the Databricks workspace from Azure DevOps pieline?,"<p>I was trying to deploy Databricks notebook from azure DevOps pipeline using YAML script.</p>
<pre><code>trigger:
 - master
pool:
  vmImage: 'windows-2019'


stages:
- stage: Build
  displayName: Build stage

  jobs:
  - job: Build
    displayName: Build
    steps:
    - task: CopyFiles@2
      displayName: 'Copy Files to:  $(build.artifactstagingdirectory)'
      inputs:
        SourceFolder: '$(System.DefaultWorkingDirectory)'
        TargetFolder: ' $(build.artifactstagingdirectory)'
        
    - task: PublishBuildArtifacts@1
      inputs:
        PathtoPublish: '$(Build.ArtifactStagingDirectory)'
        ArtifactName: 'publish build'
        publishLocation: 'Container'

- stage: 'Stg_Deploy'
  displayName: 'Deploy to Stg Environment'
  dependsOn: Build

  jobs:
  - deployment: 'Stg_Deploy'
    displayName: 'Deploy to Stg '
    environment: 'Stage-1'
    strategy:
      runOnce:
        deploy:
          steps:
          - task: databricksDeployScripts@0
            inputs:
              authMethod: 'bearer'
              bearerToken: 'dapi9ke861645hh96fv483bbac61b78ce9b1-2'
              region: 'Central US'
              localPath: '$(System.DefaultWorkingDirectory)'
              databricksPath: '/Shared'

</code></pre>
<p>In Stg_Deploy stage I have created a Environment and I was using Databricks Notebook Deploy Task to deploy code in databricks workspace.<br />
But I can see there is no notebook deployed in Databricks workspace directory.</p>",0,0,2022-02-15 06:46:29.550000 UTC,,,0,azure|azure-devops|azure-databricks|cicd|azure-devops-extensions,11,2022-01-21 03:11:50.470000 UTC,2022-03-04 06:13:20.257000 UTC,"Hyderabad, Telangana, India",33,0,0,2,,,,,,[]
"Use Platform independent ""Microsoft.AnalysisServices.Tabular.dll""","<p>I need to execute script in databrick using notebook. My code is working as desktop version when I refer DLL file from the location &quot;C:\Windows\Microsoft.NET\assembly\GAC_MSILJ&quot;. However, when i need to execute from azure cloud local destination i can't use. What might be the alternative solution to resolve the issue. Attached my code screenshot.</p>
<p>I have just started coding and its my first project . So if anyone can help me that would be appreciated.</p>
<p>Thanks in advance<img src=""https://i.stack.imgur.com/w3Gez.png"" alt=""enter image description here"" /></p>",0,0,2022-03-05 02:28:50.543000 UTC,,,0,.net|python-3.x|azure-databricks|ssas-tabular,4,2019-11-07 21:55:27.527000 UTC,2022-03-05 17:06:39.193000 UTC,,15,2,0,7,,,,,,[]
I want to Install SIMBA ODBC drivers in AZURE PAAS,"<p>I have implemented a .Net console application which pulls the data from databricks Applications .
I have installed SIMBA ODBC drivers in my local and was able to connect to databricks cluster from my local .net application.</p>
<p>Now I want to host my .net application in  AZURE PAAS .So how do I install SIMBA ODBC drivers in AZURE PAAS ?</p>
<p>Please let me know . Thanks in advance !</p>",1,0,2020-09-26 03:46:11.993000 UTC,,2020-09-28 14:16:47.793000 UTC,0,azure|odbc|azure-databricks|paas|simba,231,2019-09-13 04:09:54.680000 UTC,2022-03-04 21:14:15.923000 UTC,,75,10,0,43,,,,,,[]
Finding list of jobs from Azure Databricks notebook using code,"<p>I found the reference in Databricks that by using REST API I can find number of jobs running in my notebook: <a href=""https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/1.2/"" rel=""nofollow noreferrer"">Sources</a></p>
<p>Do you have any simple example implementing this?</p>",1,2,2021-04-28 05:49:55.630000 UTC,,2021-04-29 13:47:36.227000 UTC,0,azure|jobs|azure-databricks,264,2018-05-28 09:46:44.583000 UTC,2022-03-02 08:05:29.517000 UTC,Hyderabad,103,20,0,6,,,,,,[]
Databricks error while reading GTiff file using RasterIO,"<p>While reading raster file in Databricks getting below error:</p>

<pre><code>ConnectException: Connection refused (Connection refused)
Error while obtaining a new communication channel
</code></pre>

<p>Code: </p>

<pre><code>import rasterio
import rasterio.plot
import pyproj
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import rasterio.features
import rasterio.warp

raster = rasterio.open('/dbfs/mnt/Firescar/cvmsre_201909_afka2.tif')
raster.read(1)
</code></pre>",0,0,2020-02-06 05:22:34.177000 UTC,,,1,azure-databricks|geotiff|rasterio,140,2009-04-02 00:53:40.477000 UTC,2022-03-01 22:30:30.100000 UTC,"Brisbane QLD, Australia",27501,66,1,1101,,,,,,[]
Azure Databricks Light Runtime,"<p>I am trying to create a Azure Databricks <strong>Light</strong> Runtime for some non-advanced performance. However, when creating a cluster, I was not able to find the specific runtime - (Databricks Light 2.4).</p>
<p>Can anyone share how to create a cluster with the expected runtime?</p>
<p>Many thanks!</p>",1,1,2020-09-07 08:50:39.933000 UTC,,,0,azure-databricks,54,2020-09-07 08:43:34.957000 UTC,2021-11-02 06:51:24.740000 UTC,,1,0,0,5,,,,,,[]
Connect to Databricks from python via managed identity,"<p>I managed to connect to Databricks from python using the following code snippet:</p>
<pre class=""lang-py prettyprint-override""><code>from databricks import sql

connection = sql.connect(
  server_hostname='&lt;server-hostname&gt;',
  http_path='&lt;http-path&gt;',
  access_token='&lt;personal-access-token&gt;')

cursor = connection.cursor()

cursor.execute('SELECT * FROM &lt;database-name&gt;.&lt;table-name&gt; LIMIT 2')

result = cursor.fetchall()

for row in result:
  print(row)

cursor.close()
</code></pre>
<p>This snippet is from the official <a href=""https://docs.databricks.com/dev-tools/python-sql-connector.html#language-Cluster"" rel=""nofollow noreferrer"">documentation</a> and as you can see, it requires <code>server_hostname</code>, <code>http_path</code> and <code>access_token</code>. My question here is, can I authenticate myself without the <code>access_token</code>? Maybe use a managed identity, since both technologies are from Microsoft?</p>",1,0,2021-11-25 08:30:35.830000 UTC,,2021-11-25 09:54:27.497000 UTC,1,python|azure-databricks|azure-managed-identity|databricks-sql,129,2016-10-26 12:03:43.167000 UTC,2022-03-02 12:42:37.167000 UTC,,376,43,5,44,,,,,,[]
Stopping a Running Spark Application (Databricks Interactive Cluster),"<p>I'm using <strong>databricks</strong> with an interactive cluster.  If I review their management user-interface, there is only one &quot;application&quot; listed.  And when I try to kill it, I always get this message</p>
<blockquote>
<p>HTTP ERROR 405
Problem accessing /app/kill/. Reason:</p>
<pre><code>   Method Not Allowed
</code></pre>
</blockquote>
<p>The end result is that I'm forced to restart the entire cluster.  I use their &quot;cluster pool&quot; feature which makes the wait time a bit less. but it still involves waiting for about a minute before I'm able to get back to work.</p>
<p>The reason I need to restart the application is to swap fresh jar's into the spark environment.  Otherwise when I repeatedly use addJar(), I run into some annoying jar-hell issues (class not found errors and such).</p>
<p>Why does Databricks only list one application at a time in their &quot;interactive&quot; cluster?
Why doesn't databricks have a way to stop one application and start another in its place (without restarting the whole cluster)?</p>
<p>This affects development productivity when we are forced to sit around waiting an extra minute for no good reason.  It is already pretty hard to be productive with spark.</p>",0,0,2020-09-22 13:49:19.700000 UTC,,,2,apache-spark|azure-databricks,98,2015-01-14 22:59:14.770000 UTC,2022-03-01 22:59:09.143000 UTC,,926,47,3,65,,,,,,[]
RMariaDB on Databricks,"<p>I'm trying to get R (either via a notebook or RStudio) to connect to MariaDB on Databricks Azure 10.1. However, whether I add RMariaDB in the Libraries tab of the cluster or via install.packages(&quot;RMariaDB&quot;) in RStudio I get a failure because:</p>
<pre><code>-----------------------------[ ANTICONF ]-----------------------------
Configure could not find suitable mysql/mariadb client library. Try installing:
* deb: libmariadb-dev (Debian, Ubuntu)
* rpm: mariadb-connector-c-devel | mariadb-devel | mysql-devel (Fedora, CentOS, RHEL)
* csw: mysql56_dev (Solaris)
* brew: mariadb-connector-c (OSX)
If you already have a mysql client library installed, verify that either
mariadb_config or mysql_config is on your PATH. If these are unavailable
you can also set INCLUDE_DIR and LIB_DIR manually via:
R CMD INSTALL --configure-vars='INCLUDE_DIR=... LIB_DIR=...'
--------------------------[ ERROR MESSAGE ]----------------------------
&lt;stdin&gt;:1:10: fatal error: mysql.h: No such file or directory
compilation terminated.
-----------------------------------------------------------------------
</code></pre>
<p>python, R, and java jar files I have installed on databricks, but not C libraries. I found the ubuntu library to download to my laptop, but the 'upload library' function in databricks seems to just want jars.</p>
<p>Anyone have any idea how to get R to speak to MariaDB in Databricks? Alternatively, is it possible to do the query in a python cell of a notebook (I have this working) and access the data in an R cell?</p>
<p>thanks</p>",1,3,2021-12-17 14:42:43.253000 UTC,,2021-12-17 14:47:11.797000 UTC,1,r|apache-spark|azure-databricks|mariadb-connector-c,39,2015-09-28 18:20:20.997000 UTC,2022-02-25 11:21:35.473000 UTC,,11,0,0,1,,,,,,[]
%run magic using get_ipython().run_line_magic() in Databricks,"<p>I am trying to import other modules inside an Azure Databricks notebook. For instance, I want to import the module called 'mynbk.py' that is at the same level as my current Databricks notebook called 'myfile'</p>
<p>To do so, inside 'myfile', in a cell, I use the magic command:</p>
<pre><code>%run ./mynbk
</code></pre>
<p>And that works fine.</p>
<p>Now, I would like to achieve the same result, but with using <strong>get_ipython().run_line_magic()</strong></p>
<p>I thought, this is what I needed to type:</p>
<pre><code>get_ipython().run_line_magic('run', './mynbk')
</code></pre>
<p>Unfortunately, that does not work. The error I get is:</p>
<pre><code>Exception: File `'./mynbk.py'` not found.
</code></pre>
<p>Any help is appreciated.</p>",1,0,2021-03-30 20:55:19.380000 UTC,,,1,python-3.x|python-import|azure-databricks,290,2017-03-07 02:04:59.347000 UTC,2022-02-27 17:54:01.257000 UTC,,793,45,0,63,,,,,,[]
Git Fetch/Pull confusion,"<p>I am a newbie in git and currently trying out various combinations to understand git.</p>

<p>I have a repository with branch named 'dev'. Now initially i brought my local in sync with the remote. Then i changed the remote directly from Github.</p>

<p>Now if i use </p>

<pre><code>git fetch origin dev:dev
</code></pre>

<p>then i receive an error</p>

<pre><code>fatal: Refusing to fetch into current branch refs/heads/dev of non-bare repository
</code></pre>

<p>However, the following pull command works fine</p>

<pre><code>git pull origin dev
</code></pre>

<p>In one of the Stackoverflow answer it was mentioned that fetch followed by merge was equivalent to git pull.</p>

<p>If it is so then why this discrepency ?</p>

<p>Question Link : <a href=""https://stackoverflow.com/questions/3419658/understanding-git-fetch-then-merge"">understanding git fetch then merge</a></p>

<p>PS : The answer suggested from link below in comments is correct one. However i was wondering that</p>

<pre><code>git fetch origin 
</code></pre>

<p>fetches from all branches including the current one. Then why perform the check only in case of </p>

<pre><code>git fetch origin master:master
</code></pre>

<p><strong>I am not able to understand the reason behind this validation.
Any help is appreciated.</strong></p>

<p>Thanks</p>",1,2,2016-09-20 22:30:51.330000 UTC,,2017-05-23 12:08:19.640000 UTC,0,git|github|dvcs,488,2014-05-23 19:32:49.087000 UTC,2022-02-23 15:36:40.167000 UTC,,393,19,0,43,,,,,,[]
Problems for Hail0.2 working on Azure DataBrick,"<p>Hello? Anyone who can help for Hail 0.2 on Azure DataBrick?</p>

<p>After pip install  lots of problems came out....</p>

<p>can't find Java Package , import hail.plot , hl.init()</p>

<p>According to document</p>

<p><a href=""https://docs.azuredatabricks.net/applications/genomics/tertiary/hail.html#create-a-hail-cluster"" rel=""nofollow noreferrer"">https://docs.azuredatabricks.net/applications/genomics/tertiary/hail.html#create-a-hail-cluster</a></p>

<p>I've pip install hail</p>

<p>set  ENABLE_HAIL=true       in Cluster Environment Setting</p>

<p>However</p>

<pre><code>import hail as hl
hl.init(sc, idempotent=True)

AttributeError: module 'hail' has no attribute 'init'
</code></pre>

<p>Also another document</p>

<p><a href=""https://docs.azuredatabricks.net/applications/genomics/tertiary/hail.html"" rel=""nofollow noreferrer"">https://docs.azuredatabricks.net/applications/genomics/tertiary/hail.html</a></p>

<pre><code>import hail as hl
import hail.expr.aggregators as agg
hl.init(sc, idempotent=True)

ModuleNotFoundError: No module named 'hail.expr'
</code></pre>

<p>Anyone can give a solution?
Thanks a lot !!!</p>",2,0,2019-10-15 03:35:16.537000 UTC,,2019-12-10 00:22:27.470000 UTC,0,python-3.7|azure-databricks|genome|hail,71,2018-03-08 13:37:14.270000 UTC,2021-10-06 10:13:27.897000 UTC,Taiwan,43,6,0,14,,,,,,[]
How to integrate Eclipse IDE with Databricks Cluster,"<p>I am trying to integrate my Scala Eclipse IDE with my Azure Databricks Cluster so that I can directly run my Spark program through Eclipse IDE on my Databricks Cluster.</p>

<p>I followed the official documentation of Databricks Connect(<a href=""https://docs.databricks.com/dev-tools/databricks-connect.html"" rel=""nofollow noreferrer"">https://docs.databricks.com/dev-tools/databricks-connect.html</a>)
.
I have:
Installed Anaconda.
Installed Python Lib 3.7 and Databricks Connect library 6.0.1.
Did the Databricks Connect Configuration part(CLI part).
Also, added the client libraries in the Eclipse IDE.
Set the SPARK_HOME env. variable to the path which I get from running command in Anaconda, i.e. 'databricks-connect get-jar-dir'</p>

<p>I have not set any other environment variables apart from the one mentioned above.</p>

<p>Need help on finding what else is to be done to accomplish this integration, like how the ENV. variable related to connection works if running through IDE.</p>

<p>If someone has already done this successfully, guide me please.</p>",0,2,2020-01-28 07:26:04.470000 UTC,,,0,eclipse|integration|azure-databricks|scala-ide|databricks-connect,273,2017-11-10 09:32:53.157000 UTC,2021-11-30 07:26:41.390000 UTC,,21,0,0,6,,,,,,[]
Remove or replace id with entityId in gremlin response,"<p>Using Strategy I'm able to transform g.V(id)  to g.V().has(&quot;entityId&quot;, id)
But is there a way I can remove id from gremlin output or replace it with entityId?
Without making changes at the serializer level?</p>",0,10,2022-02-14 16:14:23.040000 UTC,,,0,azure-cosmosdb|gremlin|tinkerpop|janusgraph|amazon-neptune,42,2021-04-26 14:22:15.553000 UTC,2022-03-02 14:20:43.727000 UTC,"Bengaluru, Karnataka, India",73,3,0,14,,,,,,[]
Gremlin - sorting group by results,"<p>I am grouping vertices by any number of group parameters and I expect the resut to be sorted by those parameters.</p>
<p>Simple test data</p>
<pre class=""lang-sh prettyprint-override""><code>g.addV(&quot;machine&quot;).property(&quot;type&quot;,&quot;PC&quot;).property(&quot;age&quot;,2)
g.addV(&quot;machine&quot;).property(&quot;type&quot;,&quot;PC&quot;).property(&quot;age&quot;,11)
g.addV(&quot;machine&quot;).property(&quot;type&quot;,&quot;Mac&quot;).property(&quot;age&quot;,2)
g.addV(&quot;machine&quot;).property(&quot;type&quot;,&quot;Mac&quot;).property(&quot;age&quot;,2)
g.addV(&quot;machine&quot;).property(&quot;type&quot;,&quot;Mac&quot;).property(&quot;age&quot;,11)
</code></pre>
<p>My prefered output format should look like:</p>
<pre class=""lang-sh prettyprint-override""><code>==&gt;[{age=2, type=Mac}]=[{type=[Mac], age=[2]}, {type=[Mac], age=[2]}]
==&gt;[{age=2, type=PC}]=[{type=[PC], age=[2]}]
==&gt;[{age=11, type=Mac}]=[{type=[Mac], age=[11]}]
==&gt;[{age=11, type=PC}]=[{type=[PC], age=[11]}]
</code></pre>
<p>or</p>
<pre class=""lang-sh prettyprint-override""><code>==&gt;[2, Mac]=[{type=[Mac], age=[2]}, {type=[Mac], age=[2]}]
==&gt;[2, PC]=[{type=[PC], age=[2]}]
==&gt;[11, Mac]=[{type=[Mac], age=[11]}]
==&gt;[11, PC]=[{type=[PC], age=[11]}]
</code></pre>
<p>Simply saying: passed grouping parameters and result vertices should be separated.</p>",2,0,2021-12-06 16:33:43.833000 UTC,,2021-12-06 16:40:38.650000 UTC,0,gremlin|janusgraph|amazon-neptune|graph-traversal,36,2014-06-19 18:49:04.387000 UTC,2022-03-03 20:44:24.400000 UTC,,33,1,0,3,,,,,,[]
gremlin query using select() in coalesce() step,"<p>I'm trying to access the previously saved traversal using <em>.as()</em> in the second traversal inside <em>coalesce()</em> as below:</p>
<p>query to upsert edge (update edge if present / create)</p>
<p>Java code:</p>
<pre><code>g.V('x').as('start')
 .V('y').as('stop')
.inE('label').where(outV().as('edge'))
.select('start','stop','edge').fold()
.coalesce(unfold(),
        addE('label').from(select('start')).to(select('stop')))
.property('key','value')
.promise(Traversal::Next);
</code></pre>
<p>Throws error as below: (precised for brevity)</p>
<blockquote>
<p>gremlin.driver.exception.ResponseException: The provided traverser does not map to a value [stop]</p>
</blockquote>
<p>when i replace the last step as below its working fine <em>(instead of alias querying the vertices again)</em></p>
<p>Replaced <code>addE('label').from(select('start')).to(select('stop'))</code></p>
<p>with <code>addE('label').from(V('x')).to(V('y'))</code></p>
<p>Is there anyway to refer the alias in the second traversal in coalesce?</p>
<p>Note: I'm collecting all data related to finding edges before coalesce <strong>in order to make the gremlin throw error when any of the vertex / vertices are missing</strong> while creating edge</p>
<p>Expected behaviour: True on successful transaction and error when any vertex missing while creating edge.</p>
<p>This works as expected without using as() alias. But, i'm trying with as(). which i couldn't make it.</p>
<p>Hope this is clear. Please comment if in need of more info. Thanks.</p>",1,0,2021-05-04 15:34:54.607000 UTC,,2021-05-04 20:00:05.390000 UTC,0,gremlin|coalesce|tinkerpop|amazon-neptune|graph-traversal,414,2015-02-04 05:25:51.150000 UTC,2022-01-12 09:48:00.607000 UTC,"Chennai, Tamil Nadu, India",185,44,0,104,,,,,,[]
What Egit actions do I take to get copy of src file TESTGIT1 from MainServerRepository into LocalServerRepo to work on?,"<p>As a new Egit user, aiming to set up version control to update the way a team manages source code without changing locations of our build and dev servers and to identify correct Egit commands to use for check in and check out code from both servers.</p>

<p>In a single Eclipse workspace, initially had one project without use of Egit just a basic automatic and adhoc file copy back up of changed development directory into folders with dates of backup. Promotion to the build server was done by manually copying source files and binaries, doing a diff between source on the MainServer and local server to manage any merges. Now need to implement a proper version control system, using Egit, so as a first time Egit user. I have set up two test Eclipse projects in my workspace: project A shared with MainServerRepository and project B shared with LocalServerRepo. The two git repositories will be in different directories: MainServerRepository on the network and LocalServerRepo on my area on the network where I backup my PC. </p>

<p>What Egit actions do I take to promote latest TESTGIT1.src from MainServerRepository into LocalServerRepo to work on in my local server? (Team > Fetch from upstream is grayed out). Then, once tested, how do I check in src for TESTGIT1 into the main build server project, source and objects are in MainServerRepository? </p>

<p>I've used Team > Commit successfully to check in, but am using cut and paste to move code between two projects (prior to commit) and feel there must be a better way to do this or to set up the projects differently within Eclipse. </p>

<p>Or should I be using Team > push or Team > merge?</p>

<p>Do I need local repository or should I just check out into workspace? </p>

<p>Any comments/ question/assistance would be welcome as haven't figured it out from reading the <a href=""http://wiki.eclipse.org/EGit/User_Guide"" rel=""nofollow noreferrer"">EGit/User_Guide</a>.</p>",1,2,2018-10-17 13:07:18.553000 UTC,1.0,2019-04-11 12:42:58.760000 UTC,0,eclipse|egit|dvcs,81,2013-04-10 08:18:29.150000 UTC,2021-01-14 13:20:15.173000 UTC,UK,1221,1216,33,558,,,,,,[]
Access a function from another script in Shared folder in Azure Databricks,"<p>I am new to Azure Databricks, and have run into a situation. I have a dev_tools python script in workspace/Shared/dev_tools location. The dev_tools script contains the following code (This is an example and not the actual code).</p>
<pre><code>def add (first_num, second_num):
    return first_num + second_num

def multiply (first_num, second_num):
    return first_num * second_num

if __name__ == &quot;__main__&quot;:
    print(add(3,6))
</code></pre>
<p>Now I have another script let's say myScript in workspace/Users/ashish that wants to import the function multiply() from dev_tools. How do I access that in the second script? I have used %run but that just runs the dev_tools script (Including the code in if <strong>name</strong> == &quot;<strong>main</strong>&quot;) Which should not happen. Is there any other way to accomplish the task.</p>
<p>The final goal is to make the code re-usable, The functions and classes in dev_tools will be used by many scripts in the Users dir.</p>",0,2,2021-10-12 12:16:50.810000 UTC,,,0,azure|azure-data-factory|azure-databricks,57,2020-08-10 07:40:56.303000 UTC,2021-12-07 05:14:43.600000 UTC,"Pune, Maharashtra, India",51,11,0,10,,,,,,[]
Apache Spark Databricks ModuleNotFoundError: No module named 'azure',"<p>I am trying to install the following library on Databricks:</p>
<pre><code>from azure.storage.filedatalake import DataLakeFileClient
</code></pre>
<p>However, I'm getting the following error:</p>
<pre><code>ModuleNotFoundError: No module named 'azure'
</code></pre>
<p>Can someone let me know the actual module to install?</p>",1,0,2022-01-31 20:43:23.307000 UTC,,,0,azure-databricks,74,2021-03-31 08:36:43.863000 UTC,2022-03-05 23:03:22.193000 UTC,"London, UK",651,58,0,93,,,,,,[]
Install rgdal and rgeos on Azure Databricks,"<p>I cannot install rgdal and rgeos on Databricks, any suggestions?</p>

<pre><code>configure: error: gdal-config not found or not executable.
ERROR: configuration failed for package ‘rgdal’
* removing ‘/databricks/spark/R/lib/rgdal’

configure: error: geos-config not found or not executable.
ERROR: configuration failed for package ‘rgeos’
* removing ‘/databricks/spark/R/lib/rgeos’
</code></pre>",1,0,2019-02-24 20:35:05.917000 UTC,,2020-08-25 16:06:36.003000 UTC,3,r|azure|gdal|azure-databricks|geos,510,2014-06-04 11:35:19.353000 UTC,2021-04-12 13:54:58.870000 UTC,,107,5,0,20,,,,,,[]
Traverse janusGraph with 100M vertices and 350M edges,"<p>I'm using Janusgraph with Cassandra as backend storage and Elasticsearch for indexing.
My graph consists of around 150M vertices and 350M edges. Now I'm trying to find out connected component using the following query but getting timed out -</p>
<pre><code>g.V().has(&quot;name&quot;, &quot;driver1&quot;). repeat(__.where(without(&quot;a&quot;)).store(&quot;a&quot;).both(). simplePath()).emit().hasLabel(&quot;driver&quot;). dedup(). count().fold()
</code></pre>
<p>This query is working fine for data less than 100M but getting timed out for large data.
Is there any optimization I can do in this query or on the Janusgraph side?</p>",0,1,2021-07-19 13:46:36.543000 UTC,,2021-07-21 23:35:34.373000 UTC,0,gremlin|janusgraph|amazon-neptune|gremlin-server,48,2021-04-26 14:22:15.553000 UTC,2022-03-02 14:20:43.727000 UTC,"Bengaluru, Karnataka, India",73,3,0,14,,,,,,[]
Visualizing Gremlin query results from AWS Neptune - is there a way?,"<p>im a bit new on AWS Neptune. Currently using gremlin to query data from it. My question is, is there a way to view the query results visually? I tried creating a Notebook (Jupyter) that connects to my Neptune cluster but it doesn't show the option to view the query results in a graph.</p>
<p>Can anyone help me on this?</p>",0,2,2021-11-05 02:56:41.230000 UTC,,2021-11-23 23:03:54.810000 UTC,0,gremlin|graph-databases|tinkerpop|amazon-neptune|graph-notebook,55,2021-02-06 01:27:48.573000 UTC,2022-02-22 09:13:32.560000 UTC,,45,0,0,12,,,,,,[]
"git clone failing, can't repack on remote","<p>While researching this I have found a <strong>lot</strong> of messages about this concerning the exhaustion of memory, but I am not actually getting any malloc errors.</p>

<p>When I try and clone I get this mid-compression</p>

<pre><code>error: git-upload-pack: git-pack-objects died with error.
fatal: git-upload-pack: aborting due to possible repository corruption on the remote side.
remote: aborting due to possible repository corruption on the remote side.
fatal: early EOF
fatal: index-pack failed
</code></pre>

<p>When I go to the remote and run <code>git gc</code> it gives me this.. (same amount through compression)</p>

<pre><code>error: failed to run repack
</code></pre>

<p>When I run <code>git fsck</code> I get no output at all...</p>

<p>ideas?</p>",1,4,2012-02-15 22:18:02.377000 UTC,1.0,,4,git|dvcs|corruption,1734,2010-12-27 21:22:31.900000 UTC,2020-05-28 18:44:50.433000 UTC,"Springfield, MO, United States",60561,601,173,4086,,,,,,[]
Python script doesn't change the OS variable sent to it from Azure DevOps pipeline,"<p>The problem I am facing has two main dimensions:</p>
<ol>
<li>Azure DevOps</li>
<li>Python</li>
</ol>
<p>To reproduce the error follow the steps below:</p>
<ol>
<li>Make an Azure DevOps project and copy all these files into the repo.</li>
<li>You need to have some code that can generate a wheel. Either use your own code, or use the sample wheel here.</li>
<li>Create three Azure Subscriptions. In each subscription create one resource group and in each RG create a new databricks workspace. If you do not have access to three subscriptions, one is also possible with only one databricks.</li>
<li>If you have three databricks workspaces, create one cluster in each. If you have only one subscription and one databricks, create three databricks clusters.</li>
<li>In Azure Active Directory make an app registration to be able to connect Azure DevOps to Azure Databricks.</li>
<li>Define an environment in Azure DevOps called qa_env_dbrx and make it an approver with your email address.</li>
<li>Make a variable group in Azure DevOps called databricks-sp-vg. This variable group must contain the following values:
<a href=""https://i.stack.imgur.com/JBw5p.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JBw5p.jpg"" alt=""Variable group list"" /></a></li>
</ol>
<p>I am using Azure DevOps pipelines to deploy python wheels into Azure databricks clusters. I am using a yaml file with different stages. My goal is to build a wheel at build stage, publish it to Azure DevOps artifact and deploy it to Dev, QA and PROD environments of 3 different databricks workspaces. To interact with databricks API, I am using python codes, which I call from my yaml pipeline. Each python code requires a number of parameters to be set in order to work. These parameters are sent from the yaml pipeline as variables to the python script and at python code they are being captured as OS variables by os.environ[&quot;var_name&quot;].</p>
<p>My problem is related to send variables from DevOps yaml pipeline to python code, which I have been able to successfully do so. <strong>But</strong> in one of my python files to manage databricks cluster status, all OS variables are set properly from the pipeline, <strong>except for cluster Id</strong>. What is confusing me is that I am using exactly the same method and process to set the parameters in python code, but the cluster Id only works for DEV stage and in QA and PROD stage it is still reading the value that has been set at DEV stage, despite the fact that in pipeline the value is correct, but when passed to python code, it is still the old value from DEV stage call!</p>
<p>In the following I share the code, and how the pipeline stages look like and how the variables are set:</p>
<p>The pipeline has 5 stages to build the wheel, and deploy it to DEV, QA and PROD clusters in three different subscriptions:</p>
<p><a href=""https://i.stack.imgur.com/UddUy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UddUy.png"" alt=""Pipeline stages consists of build, DEV, QA and PROD"" /></a></p>
<p>The yaml file I am using is as follows:</p>
<pre><code>pool: 
  vmImage: ubuntu-20.04

trigger:
  branches:
    include:
      - &quot;MYBRANCH1732&quot;      
    exclude:
      - &quot;release&quot;
      - &quot;test*&quot;
      - &quot;*&quot;
      - &quot;feature/*&quot;
      - &quot;fix/*&quot;
      - &quot;main&quot;
  paths:
    include:
      - &quot;mypackage_core_py/*&quot;
      - &quot;setup.py&quot;


variables: 
- group: databricks-sp-vg

name: $(BuildFrom).$(major).$(minor)$(Rev:.r)

stages:
  - stage: Build
    displayName: 'Build Stage'
    jobs:

    - job: build_publish_package
      variables:
        mydate: $[format('{0:yyyyMMdd}', pipeline.startTime)]
        pythonVersion: 3.8
        package: py_artifacts_1732 
        envVariable: &quot;dev&quot;
        srcDirectory: mypackage_core_py/$(package)
        square_varray_lower: $[  lower( variables['Build.SourceBranchName'] ) ]
        curly_varray_lower:  ${{ lower( variables['Build.SourceBranchName'] ) }} 
        BuildDate: $(mydate)
        ${{ if eq(variables['Build.SourceBranch'], 'refs/heads/main') }}:
          BuildFrom: main
          brpatch: $[counter(variables['build.sourcebranchname'], 0)]
        ${{ else }}:
          bar: $[replace(variables['square_varray_lower'], '_', '-')]
          BuildFrom: $[replace(lower( variables['Build.SourceBranchName'] ) , '_', '-')]
          brpatch: $[counter(variables['build.sourcebranchname'], 0)]  
        major: 1
        minor: 1  
        RevisionNumber: $(major).$(minor).$(brpatch)
        packageBuildName: dip-py-whl-build-$(BuildFrom)-$(mydate)-$(major).$(minor).$(brpatch)


      steps:
        - task: UsePythonVersion@0
          displayName: Use Python $(pythonVersion)
          inputs:
            versionSpec: $(pythonVersion)
        - checkout: self
        - script: |
            pip install pkginfo wheel
            python setup.py bdist_wheel
          displayName: Build package
        - powershell: |
            Write-Host $(mydate)
        - bash: |
            VERSION=$(python -c 'import glob; from pkginfo import Wheel; w = Wheel(glob.glob(&quot;*.whl&quot;)[0]);print(w.version)')
            WHEELNAME=$(python -c 'import glob; from pkginfo import Wheel; w = Wheel(glob.glob(&quot;*.whl&quot;)[0]);print(w)')
            echo &quot;bar val is: &quot; $(bar)
            echo &quot;Build Definition version:&quot; $(Build.DefinitionVersion)
            echo &quot;Get package version&quot;
            echo &quot;Source Branch is:&quot;
            echo $(Build.SourceBranch)
            echo &quot;Source Branch Name is:&quot;
            echo $(Build.SourceBranchName)  
            echo &quot;Date is:&quot;
            echo $(BuildDate)
            echo square_varray_lower:   $(square_varray_lower)
            echo curly_varray_lower:  $(curly_varray_lower)
            echo &quot;Revision number is:&quot;
            echo $(RevisionNumber)
            echo &quot;Variable BuildFrom is:&quot;
            echo $(BuildFrom)
            echo &quot;srcDirectory is:&quot;
            echo $(srcDirectory)
            echo &quot;Get patch number&quot;
            echo     &quot;##vso[build.updatebuildnumber]$(major).$(minor)-$(Build.SourceBranchName).$(brpatch)&quot;
            echo $VERSION
            echo &quot;Wheel name is:&quot;
            echo $WHEELNAME
            echo &quot;package build name&quot;
            echo $(packageBuildName)
          displayName: Get package version
          workingDirectory: dist/

        - task: CopyFiles@2
          displayName: Copy package for later use in the pipeline
          inputs:
            contents: dist/**
            targetFolder: $(Build.ArtifactStagingDirectory)

        - task: PublishBuildArtifacts@1
          inputs:
            pathToPublish: '$(Build.ArtifactStagingDirectory)'
            artifactName: dfordbx

        - task: UniversalPackages@0
          displayName: Publish
          inputs:
            command: publish
            publishDirectory: $(Build.ArtifactStagingDirectory)/dist/
            vstsFeedPublish: 'MyProjectName/py_artifacts_1732'
            vstsFeedPackagePublish: $(packageBuildName)


        - bash: |
            echo &quot;Structure of work folder of this pipeline:&quot;
            tree $(Agent.WorkFolder)\1 /f
            echo &quot;Build.ArtifactStagingDirectory:&quot; 
            echo &quot;$(Build.ArtifactStagingDirectory)&quot;
            echo &quot;Build.BinariesDirectory:&quot; 
            echo &quot;$(Build.BinariesDirectory)&quot;
            echo &quot;Build.SourcesDirectory:&quot;
            echo &quot;$(Build.SourcesDirectory)&quot;
          displayName: GetFolderStructure 
        
### Deploy built wheel on databricks Dev workspace
  - stage: DeployDev
    displayName: 'Deploy Stage in Dev env'
    dependsOn: 
    - Build
    - Release

    jobs:


    - job: download_wheel_from_build_pipeline
      steps:

        - task: DownloadPipelineArtifact@2
          inputs:
            buildType: 'current'
            project: 'MyProjectName'
            buildVersionToDownload: 'latest'
            artifactName: dfordbx      
            targetPath: '$(System.ArtifactsDirectory)'


        - pwsh: |
            $whlFile = Get-ChildItem -Filter *.whl -Path &quot;$(System.ArtifactsDirectory)/dist&quot; | ForEach-Object { $_.fullname } | Select-Object -First 1
            Write-Host &quot;##vso[task.setvariable variable=whlFile]$whlFile&quot;
          name: SetVars

        - pwsh: |
            Write-Host &quot;Wheel name is $env:whlFile&quot;

        - bash: |
            echo &quot;Wheel name is:&quot; 
            echo &quot;$(whlFile)&quot;
          displayName: GetWheelName


    
    - job: authenticate_databricks
      dependsOn: download_wheel_from_build_pipeline
      variables: 
      - group: databricks-sp-vg

      steps:
        - task: PythonScript@0
          displayName: &quot;Get authentication tokens&quot;
          name: &quot;auth_tokens&quot;
          inputs: 
            scriptSource: 'filePath'
            scriptPath: authenticate.py
          env:
            SVCDirectoryID: '$(SVCDirectoryID)'
            SVCApplicationID: '$(SVCApplicationID)'
            SVCSecretKey: '$(SVCSecretKey)'

    - job: upload_wheel_to_DBFS
      dependsOn:
        - authenticate_databricks
      variables:
        DBRKS_BEARER_TOKEN:     $[dependencies.authenticate_databricks.outputs['auth_tokens.DBRKS_BEARER_TOKEN']]

      steps:

        - task: DownloadPipelineArtifact@2
          inputs:
            buildType: 'current'
            project: 'MyProjectName'
            buildVersionToDownload: 'latest'
            artifactName: dfordbx      
            targetPath: '$(System.ArtifactsDirectory)'
    
        - pwsh: |
            $whlFile = Get-ChildItem -Filter *.whl -Path &quot;$(System.ArtifactsDirectory)/dist&quot; | ForEach-Object { $_.fullname } | Select-Object -First 1
            Write-Host &quot;##vso[task.setvariable variable=whlFile]$whlFile&quot;
          name: SetVars

        - bash: |
            echo &quot;Wheel name is:&quot; 
            echo &quot;$(whlFile)&quot;
          displayName: GetWheelName

        - bash: |
            echo &quot;SubscriptionID:&quot;
            echo $(SubscriptionID)
            echo &quot;ResourceGroup:&quot;
            echo $(ResourceGroup)
            echo &quot;WorkspaceName:&quot;
            echo $(WorkspaceName)
            echo &quot;DBRKS_CLUSTER:&quot;
            echo $(DBRKS_CLUSTER_ID)
            echo &quot;DBXInstance:&quot;
            echo $(DBXInstance)
            echo &quot;System.ArtifactsDirectory&quot;
            echo $(System.ArtifactsDirectory)
          displayName: GerEnvVariablesBeforeUploadToDBFS

        - task: PythonScript@0
          displayName: &quot;upload wheel to DBFS&quot;
          inputs:
            scriptSource: 'filePath' 
            scriptPath: upload_wheel_to_dbfs.py
          env:
            DBRKS_BEARER_TOKEN: '$(DBRKS_BEARER_TOKEN)'
            DBRKS_MANAGEMENT_TOKEN: '$(DBRKS_MANAGEMENT_TOKEN)'
            DBRKS_SUBSCRIPTION_ID: '$(SubscriptionID)'
            DBRKS_INSTANCE: '$(DBXInstance)'
            DBRKS_RESOURCE_GROUP: '$(ResourceGroup)'
            DBRKS_WORKSPACE_NAME: '$(WorkspaceName)'
            SYSTEM_ARTIFACTSDIRECTORY: '$(System.ArtifactsDirectory)'
            DBRKS_DBFS_WHL_LOC: '$(DBRKS_DBFS_WHL_LOC_DEV)'
            WHL_NAME: '$(whlFile)'

    - job: install_wheel_on_cluster
      dependsOn: 
        - authenticate_databricks
        - upload_wheel_to_DBFS

      variables:
        DBRKS_BEARER_TOKEN: $[dependencies.authenticate_databricks.outputs['auth_tokens.DBRKS_BEARER_TOKEN']]
        DBRKS_MANAGEMENT_TOKEN: $[dependencies.authenticate_databricks.outputs['auth_tokens.DBRKS_MANAGEMENT_TOKEN']]
  
      steps:

        - task: DownloadPipelineArtifact@2
          inputs:
            buildType: 'current'
            project: 'MyProjectName'
            buildVersionToDownload: 'latest'
            artifactName: dfordbx      
            targetPath: '$(System.ArtifactsDirectory)'

        - pwsh: |
            $whlFile = Get-ChildItem -Filter *.whl -Path &quot;$(System.ArtifactsDirectory)/dist&quot; | ForEach-Object { $_.fullname } | Select-Object -First 1
            Write-Host &quot;##vso[task.setvariable variable=whlFile]$whlFile&quot;
          name: SetVars

        - bash: |
            echo &quot;SubscriptionID:&quot;
            echo $(SubscriptionID)
            echo &quot;ResourceGroup:&quot;
            echo $(ResourceGroup)
            echo &quot;WorkspaceName:&quot;
            echo $(WorkspaceName)
            echo &quot;DBRKS_CLUSTER:&quot;
            echo $(DBRKS_CLUSTER_ID)
            echo &quot;DBXInstance:&quot;
            echo $(DBXInstance)
            echo &quot;System.ArtifactsDirectory&quot;
            echo $(System.ArtifactsDirectory)
            echo &quot;Wheel name:&quot;
            echo $(whlFile)
          displayName: GerEnvVariablesBeforeStartingCluster

        - task: PythonScript@0
          displayName: &quot;Start cluster before installing wheel&quot;
          inputs:
            scriptSource: 'filepath'
            scriptPath: start_cluster.py
          env:
            DBRKS_BEARER_TOKEN: '$(DBRKS_BEARER_TOKEN)'
            DBRKS_MANAGEMENT_TOKEN: '$(DBRKS_MANAGEMENT_TOKEN)'
            DBRKS_SUBSCRIPTION_ID: '$(SubscriptionID)'
            DBRKS_RESOURCE_GROUP: '$(ResourceGroup)'
            DBRKS_WORKSPACE_NAME: '$(WorkspaceName)'
            DBRKS_CLUSTER_ID: '$(DBRKS_CLUSTER_ID)'
            DBRKS_INSTANCE: '$(DBXInstance)'


        - task: PythonScript@0
          displayName: &quot;install wheel&quot;
          inputs:
            scriptSource: 'filepath'
            scriptPath: install_wheel.py
          env:
            DBRKS_BEARER_TOKEN: '$(DBRKS_BEARER_TOKEN)'
            DBRKS_MANAGEMENT_TOKEN: '$(DBRKS_MANAGEMENT_TOKEN)'
            DBRKS_SUBSCRIPTION_ID: '$(SubscriptionID)'
            DBRKS_RESOURCE_GROUP: '$(ResourceGroup)'
            DBRKS_WORKSPACE_NAME: '$(WorkspaceName)'
            DBRKS_CLUSTER_ID: '$(DBRKS_CLUSTER_ID)'
            DBRKS_INSTANCE: '$(DBXInstance)'
            WHL_NAME: $(whlFile)
            DBRKS_DBFS_WHL_LOC: '$(DBRKS_DBFS_WHL_LOC_DEV)'

        - task: PythonScript@0
          displayName: &quot;Restart cluster to apply changes&quot;
          inputs:
            scriptSource: 'filepath'
            scriptPath: start_cluster.py
          env:
            DBRKS_BEARER_TOKEN: '$(DBRKS_BEARER_TOKEN)'
            DBRKS_MANAGEMENT_TOKEN: '$(DBRKS_MANAGEMENT_TOKEN)'
            DBRKS_SUBSCRIPTION_ID: '$(SubscriptionID)'
            DBRKS_RESOURCE_GROUP: '$(ResourceGroup)'
            DBRKS_WORKSPACE_NAME: '$(WorkspaceName)'
            DBRKS_CLUSTER_ID: '$(DBRKS_CLUSTER_ID)'
            DBRKS_INSTANCE: '$(DBXInstance)'
### End of deploy built wheel on databricks Dev workspace            

### Deploy to QA env 
  - stage: DeployQA
    displayName: 'Deploy Stage in QA env'
    dependsOn: 
    - Build
### Copy Paste jobs from previous stage (StageDev) here just pass the values for QA from variable group.
     
</code></pre>
<p>This pipeline has 5 stages, for simplicity I haven't put PROD stage above. Only StageDev and StageQA are important here as these are the stages I am facing an issue. I have made DeployQA stage dependent on Build stage to run it faster for test purposes but in fact it dependes on DeployDEV, which wouldn't change the core of the question. Basically in Build stage, I make a wheel. In DeployDEV and DeployQA stage I download the wheel, upload it to DBFS (databricks file system) and install it on a cluster in each workspace. Later on you'll see I have used those echos to see the variable values at pipeline and their corresponding values at python scripts and you will see DBRKS_CLUSTER_ID is the only value that doesn't change and cause error.</p>
<p>I put the python scripts to be able to regenerate the error I am getting; as shown in the pipeline, there are four python scripts:</p>
<ol>
<li>authenticate.py</li>
<li>upload_wheel_to_dbfs.py</li>
<li>start_cluster.py</li>
<li>install_wheel</li>
</ol>
<p>start_cluster.py and install_wheel.py are the two parts that are causing failure. As mentioned earlier and in the code, variables defined in variable group in pipeline are sent correctly to the python scripts. I can see that by the echo I have put in the pipeline yaml file in each job:</p>
<pre><code>    - bash: |
        echo &quot;SubscriptionID:&quot;
        echo $(SubscriptionID)
        echo &quot;ResourceGroup:&quot;
        echo $(ResourceGroup)
        echo &quot;WorkspaceName:&quot;
        echo $(WorkspaceName)
        echo &quot;DBRKS_CLUSTER:&quot;
        echo $(DBRKS_CLUSTER_ID)
        echo &quot;DBXInstance:&quot;
        echo $(DBXInstance)
        echo &quot;System.ArtifactsDirectory&quot;
        echo $(System.ArtifactsDirectory)
        echo &quot;Wheel name:&quot;
        echo $(whlFile)
      displayName: GerEnvVariablesBeforeStartingCluster
</code></pre>
<p>This task for StageDEV shows the following values:</p>
<pre><code>SubscriptionID:
&lt;sub_id&gt;-3fcf7e
ResourceGroup:
&lt;rg_name&gt;-processing-dev
WorkspaceName:
&lt;databricks_workspace_name_dev&gt;-proc-job-dev
DBRKS_CLUSTER:
&lt;cluster_id_in_dev&gt;-oonfx6kc
DBXInstance:
adb-&lt;some_value&gt;.12
System.ArtifactsDirectory
/home/vsts/work/1/a
Wheel name:
/home/vsts/work/1/a/dist/&lt;wheel_name&gt;-0.6.5-py3-none-any.whl
</code></pre>
<p>And for StageQA shows the following values:</p>
<pre><code>SubscriptionID_QA:
&lt;sub_id&gt;-bcf88fb
ResourceGroup_QA:
&lt;rg_name&gt;-processing-qa
WorkspaceName_QA:
&lt;databricks_workspace_name_qa&gt;-proc-job-qa
DBRKS_CLUSTER_QA:
&lt;cluster_id_in_qa&gt;-sy5u5oas
DBXInstance_QA:
adb-&lt;some_value&gt;.14
System.ArtifactsDirectory
/home/vsts/work/1/a
Wheel name:
/home/vsts/work/1/a/dist/&lt;wheel_name&gt;-0.6.5-py3-none-any.whl
</code></pre>
<p>Same goes for PROD environment. In other words, pipeline is passing exactly correct values as expected. However, at start_cluster.py and install_wheel.py DBRKS_CLUSTER_ID value is not being updated by the value set in StageQA or StagePROD! All the other values are correct, except for this one! How is that possible? What am I doing wrong?</p>
<p>StageDev is working correctly but StageQA and StagePROD fail, because the cluster Id doesn't exist in their subscriptions. And of course it doesn't exist because it is in dev subscription. I thought if I train the stages one after another as shown below, the problem would be resolved, but it didn't make any difference. The reason I add gc.collect() at the end of my scripts in start_cluster.py was to make sure nothing is being cached, but that shouldn't be the case either as each job in the stage must run in a different agent. So nothing is really being cached, or shouldn't really being cached. Am I wrong here? Even if it is being cached, how come subscription Id, RG, worksapce name, databricks instance are passed correctly?</p>
<p>Changing dependency graph also didn't help as mentioned earlier:</p>
<p><a href=""https://i.stack.imgur.com/sWZso.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sWZso.png"" alt=""Making StageQA dependent on StageDev doesn't resolve the issue either."" /></a></p>
<p>The screen shot of print values in start_cluster.py in StageDev is as follows:</p>
<p><a href=""https://i.stack.imgur.com/Jhx8m.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jhx8m.png"" alt=""Cluser_Id value in start_cluster.py in StageDev"" /></a></p>
<p>DBRKS_CLUSTER_ID value in start_cluster.py in StageDev is absolutely correct. Its value is coming from variable group and is equal to &lt;some_value&gt;-oonfx6kc. However, looking at the printed values in start_cluster.py for StageQA would reveal the same DBRKS_CLUSTER_ID value, although as shown above in bash task in pipeline, the echo shows it is correctly set to &lt;some_value&gt;-sy5u5oas. Interestingly the subscription, RG, workspace name, everything else is passed properly, except for this value as shown below:</p>
<p><a href=""https://i.stack.imgur.com/4c556.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4c556.png"" alt=""Cluser_Id value in start_cluster.py in StageQA is still same as StageDev"" /></a></p>
<p>authenticate.py file:</p>
<pre><code>import requests
import json
import os


TOKEN_BASE_URL = 'https://login.microsoftonline.com/' + os.environ['SVCDirectoryID'] + '/oauth2/token'
TOKEN_REQ_HEADERS = {'Content-Type': 'application/x-www-form-urlencoded'}
TOKEN_REQ_BODY = {
       'grant_type': 'client_credentials',
       'client_id': os.environ['SVCApplicationID'],
       'client_secret': os.environ['SVCSecretKey']}

def dbrks_management_token():
        TOKEN_REQ_BODY['resource'] = 'https://management.core.windows.net/'
        response = requests.get(TOKEN_BASE_URL, headers=TOKEN_REQ_HEADERS, data=TOKEN_REQ_BODY)
        if response.status_code != 200:
            raise Exception(response.text)
    return response.json()['access_token']

def dbrks_bearer_token():
        TOKEN_REQ_BODY['resource'] = '2ff814a6-3304-4ab8-85cb-cd0e6f879c1d'
        response = requests.get(TOKEN_BASE_URL, headers=TOKEN_REQ_HEADERS, data=TOKEN_REQ_BODY)
        if response.status_code != 200:
            raise Exception(response.text)
        return response.json()['access_token']

DBRKS_BEARER_TOKEN = dbrks_bearer_token()
DBRKS_MANAGEMENT_TOKEN = dbrks_management_token()

os.environ['DBRKS_BEARER_TOKEN'] = DBRKS_BEARER_TOKEN 
os.environ['DBRKS_MANAGEMENT_TOKEN'] = DBRKS_MANAGEMENT_TOKEN 

print(&quot;##vso[task.setvariable variable=DBRKS_BEARER_TOKEN;isOutput=true;]{b}&quot;.format(b=DBRKS_BEARER_TOKEN))
print(&quot;##vso[task.setvariable variable=DBRKS_MANAGEMENT_TOKEN;isOutput=true;] {b}&quot;.format(b=DBRKS_MANAGEMENT_TOKEN))
</code></pre>
<p>upload_wheel_to_dbfs.py:</p>
<pre><code>import requests
import os
import os.path

DBRKS_REQ_HEADERS = {
    'Authorization': 'Bearer ' + os.environ['DBRKS_BEARER_TOKEN'],
    'X-Databricks-Azure-Workspace-Resource-Id': '/subscriptions/'+ os.environ['DBRKS_SUBSCRIPTION_ID'] +'/resourceGroups/'+ os.environ['DBRKS_RESOURCE_GROUP'] +'/providers/Microsoft.Databricks/workspaces/' + os.environ['DBRKS_WORKSPACE_NAME'],
    'X-Databricks-Azure-SP-Management-Token': os.environ['DBRKS_MANAGEMENT_TOKEN']}

dbrks_rest_url = &quot;https://&quot;+os.environ['DBRKS_INSTANCE']+&quot;.azuredatabricks.net/api/2.0/dbfs/put&quot;
wheel_location_in_pipeline = os.environ['SYSTEM_ARTIFACTSDIRECTORY'] + '/dist/' + os.path.basename(os.environ['WHL_NAME'])

DBRKS_DBFS_WHL_LOCation = os.environ['DBRKS_DBFS_WHL_LOC']
print('New path: {}'.format('/' + os.environ['DBRKS_DBFS_WHL_LOC'] + '/'+ os.path.basename(os.environ['WHL_NAME'])))


f = open(wheel_location_in_pipeline, 'rb')
files = {&quot;content&quot;: (wheel_location_in_pipeline, f)}
response = requests.post(dbrks_rest_url, files=files, headers=DBRKS_REQ_HEADERS, data={'path': '/' + DBRKS_DBFS_WHL_LOCation + '/'+ os.path.basename(os.environ['WHL_NAME']), 'overwrite': 'true'})
if response.status_code != 200:
    raise Exception(response.text)
</code></pre>
<p>starter_cluster.py:</p>
<pre><code>import requests
import time
import os
import json
import gc

DBRKS_REQ_HEADERS = {
    'Authorization': 'Bearer ' + os.environ['DBRKS_BEARER_TOKEN'],
    'X-Databricks-Azure-Workspace-Resource-Id': '/subscriptions/'+ os.environ['DBRKS_SUBSCRIPTION_ID'] +'/resourceGroups/'+ os.environ['DBRKS_RESOURCE_GROUP'] +'/providers/Microsoft.Databricks/workspaces/' + os.environ['DBRKS_WORKSPACE_NAME'],
    'X-Databricks-Azure-SP-Management-Token': os.environ['DBRKS_MANAGEMENT_TOKEN']}
# This value is not being set correctly in QA stage.
DBRKS_CLUSTER_ID = {'cluster_id': os.environ[&quot;DBRKS_CLUSTER_ID&quot;]} 

def get_dbrks_cluster_info():
    DBRKS_INFO_ENDPOINT = 'api/2.0/clusters/get'
    response = requests.get(&quot;https://&quot;+os.environ['DBRKS_INSTANCE']+&quot;.azuredatabricks.net/&quot; + DBRKS_INFO_ENDPOINT, headers=DBRKS_REQ_HEADERS, params=DBRKS_CLUSTER_ID)
    if response.status_code == 200:
        return json.loads(response.content)
    else:
        raise Exception(json.loads(response.content))

def start_dbrks_cluster():
    DBRKS_START_ENDPOINT = 'api/2.0/clusters/start'
    response = requests.post(&quot;https://&quot;+os.environ['DBRKS_INSTANCE']+&quot;.azuredatabricks.net/&quot; + DBRKS_START_ENDPOINT, headers=DBRKS_REQ_HEADERS, json=DBRKS_CLUSTER_ID)
    if response.status_code != 200:
        raise Exception(json.loads(response.content))

def restart_dbrks_cluster():
    DBRKS_RESTART_ENDPOINT = 'api/2.0/clusters/restart'
    response = requests.post(
    &quot;https://&quot;+os.environ['DBRKS_INSTANCE']+&quot;.azuredatabricks.net/&quot; + DBRKS_RESTART_ENDPOINT,
    headers=DBRKS_REQ_HEADERS,
    json=DBRKS_CLUSTER_ID)
if response.status_code != 200:
    raise Exception(json.loads(response.content))
    

def manage_dbrks_cluster_state():
    await_cluster = True
    started_terminated_cluster = False
    cluster_restarted = False
    start_time = time.time()
    loop_time = 1200  # 20 Minutes
    while await_cluster:
        current_time = time.time()
        elapsed_time = current_time - start_time
        if elapsed_time &gt; loop_time:
            raise Exception('Error: Loop took over {} seconds to run.'.format(loop_time))
        if get_dbrks_cluster_info()['state'] == 'TERMINATED':
            print('Starting Terminated Cluster')
            started_terminated_cluster = True
            start_dbrks_cluster()
            time.sleep(60)
        elif get_dbrks_cluster_info()['state'] == 'RESTARTING':
            print('Cluster is Restarting')
            time.sleep(60)
        elif get_dbrks_cluster_info()['state'] == 'PENDING':
            print('Cluster is Pending Start')
            time.sleep(60)
        elif get_dbrks_cluster_info()['state'] == 'RUNNING' and not cluster_restarted and not started_terminated_cluster:
            print('Restarting Cluster')
            cluster_restarted = True
            restart_dbrks_cluster()
        else:
            print('Cluster is Running')
            await_cluster = False

manage_dbrks_cluster_state()

del DBRKS_CLUSTER_ID
del os.environ[&quot;DBRKS_CLUSTER_ID&quot;]
del os.environ[&quot;DBRKS_SUBSCRIPTION_ID&quot;]
del os.environ[&quot;DBRKS_RESOURCE_GROUP&quot;]
del os.environ[&quot;DBRKS_WORKSPACE_NAME&quot;]
del os.environ[&quot;DBRKS_INSTANCE&quot;]
del os.environ[&quot;DBRKS_BEARER_TOKEN&quot;]
del os.environ[&quot;DBRKS_MANAGEMENT_TOKEN&quot;]
gc.collect()
</code></pre>
<p>install_wheel.py:</p>
<pre><code>import requests
import os

DBRKS_REQ_HEADERS = {
    'Authorization': 'Bearer ' + os.environ['DBRKS_BEARER_TOKEN'],
    'X-Databricks-Azure-Workspace-Resource-Id': '/subscriptions/'+ os.environ['DBRKS_SUBSCRIPTION_ID'] +'/resourceGroups/'+ os.environ['DBRKS_RESOURCE_GROUP'] +'/providers/Microsoft.Databricks/workspaces/' + os.environ['DBRKS_WORKSPACE_NAME'],
    'X-Databricks-Azure-SP-Management-Token': os.environ['DBRKS_MANAGEMENT_TOKEN']}


DBRKS_REQ_BODY = {'cluster_id': os.environ[&quot;DBRKS_CLUSTER_ID&quot;], 'libraries': [{'whl': 'dbfs:/' + os.environ[&quot;DBRKS_DBFS_WHL_LOC&quot;] + '/'+os.path.basename(os.environ['WHL_NAME'])}]}

DBRKS_INSTALL_ENDPOINT = 'api/2.0/libraries/install'

response = requests.post(
    &quot;https://&quot;+os.environ['DBRKS_INSTANCE']+&quot;.azuredatabricks.net/&quot; + DBRKS_INSTALL_ENDPOINT,
    headers=DBRKS_REQ_HEADERS,
    json=DBRKS_REQ_BODY)

if response.status_code != 200:
    raise Exception(response.content)
else:
    print(response.status_code)
</code></pre>
<p>Any help is being highly appreciated. Thank you.</p>
<p>PS: the <strong><a href=""https://github.com/ErfanEbrahimiBazaz/azure_databricks_devops_python"" rel=""nofollow noreferrer"">github repo</a></strong> is available if you are interested.</p>",0,9,2022-02-23 23:02:38.460000 UTC,,2022-02-24 00:48:10.243000 UTC,-1,python|azure|azure-devops|azure-pipelines|azure-databricks,43,2014-02-14 13:23:01.677000 UTC,2022-03-04 14:05:34.207000 UTC,"Düsseldorf, Germany",769,203,1,77,,,,,,[]
unable to load data in azure sql using azure databricks,"<p>I am new to azure databricks . I have written a sample spark program in scala to load in azure sql via below query . I am getting an error . can someone please help me in this </p>

<p>Error Message ----<br>
com.microsoft.sqlserver.jdbc.SQLServerException: The TCP/IP connection to the host b63da5ce2d2d.tr27.northeurope1-a.worker.database.windows.net, port 65535 has failed. Error: ""connect timed out. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.""</p>

<p>Scala code - </p>

<p>import com.microsoft.azure.sqldb.spark.config.Configimportcom.microsoft.azure.sqldb.spark.connect._// Aquire a DataFrame collection (val collection)valconfig=Config(Map(""url""->""mysqlserver.database.windows.net"",""databaseName""->""MyDatabase"",""dbTable""->""dbo.Clients""""user""->""username"",""password""->""xxxxxxxx""))importorg.apache.spark.sql.SaveModecollection.write.mode(SaveMode.Append).sqlDB(config)</p>",2,0,2019-03-02 05:37:20.020000 UTC,,2019-03-04 12:08:48.730000 UTC,1,azure-sql-database|azure-data-factory|azure-data-factory-2|azure-databricks,1717,2019-02-13 05:50:19.390000 UTC,2022-02-21 06:31:27.660000 UTC,"Bangalore, Karnataka, India",69,1,0,17,,,,,,[]
How to iterate through multiple tables in azure databricks data and delete the matching record in postgres database?,"<p>I am extracting data from azure databricks table and loading only the first row (rank = 1) into similar table(same table structure) in postgres. However before loading, I'm checking if the column5 value in postgres table is same as the databricks table, if yes, then that row has to be deleted from postgres table and the remaining values have to be loaded.</p>
<p>Here I want to iterate over the rows of my extracted databricks table and run delete command for each row in postgres. Please suggest a way to achieve this in SQL without using cursors ?</p>",1,0,2021-03-04 10:36:28.730000 UTC,,,0,sql|postgresql|azure-databricks,220,2020-02-07 08:18:13.437000 UTC,2021-03-24 04:04:30.467000 UTC,,85,7,0,31,,,,,,[]
Databricks Scala : How to stream result from sql select,"<p>I need to send data from a databricks delta table into azure event hubs.
The data will be selected with a sql select</p>
<pre><code>spark.sql(&quot;SELECT [columns] FROM table WHERE [where clause]&quot;)
</code></pre>
<p>This select will return many many rows and after it, I will apply some transformation (mainly to be in accordance to the event hub event data message).
At the end I will send it to event hub.</p>
<p>As far as I can tell, at the moment of writing, I need to use &quot;writeStream&quot; but is this enough? How can I control how many messages are sent per batch? Do I even need to care about it or does the lib handle it?</p>
<p>Another question I have is, from the moment I use &quot;writeStream&quot; the command hangs in a running/streaming state for eternity. Is this correct or am I not being patient enough? If I'm correct, then how can I stop it (in a non-manual way) after sending all data?</p>
<p>Notes:</p>
<ul>
<li>This will be running in a job that is to be triggered manually</li>
<li>The lib i use for the event hub connection is com.microsoft.azure:azure-eventhubs-spark_2.11:2.3.14.1</li>
</ul>",1,0,2020-07-16 15:08:06.137000 UTC,,,0,scala|spark-streaming|azure-eventhub|azure-databricks,97,2012-07-31 15:35:28.560000 UTC,2022-01-28 11:51:46.850000 UTC,Portugal,699,29,0,119,,,,,,[]
AWS Neptune Gremlin Composite Index,"<p>I'm using gremlin-java. And I want to create a unique index and a composite-index. But Graph object doesn't provide <code>createIndex</code> function.
I will work on aws-neptune.</p>
<p>Is there a way to create composite-index and unique-index on aws-neptune?</p>
<p>My expected indexes are:</p>
<pre><code>CREATE INDEX ON :`VertexLabel1`(`country`,`value`)
CREATE UNIQUE INDEX ON :`VertexLabel2`(`x_id`)
</code></pre>
<p>Graph configuration:</p>
<pre><code>@Bean
public Cluster gremlinCluster()
{
    return Cluster.build()
            .addContactPoint(GREMLIN_ENDPOINT)
            .port(GREMLIN_PORT)
            .enableSsl(GREMLIN_SSL_ENABLED)
            .create();
} 
...
@Bean
public GraphTraversalSource gremlinGraph(Cluster gremlinCluster)
{
    return traversal().withRemote(DriverRemoteConnection.using(gremlinCluster, &quot;g&quot;));
}
</code></pre>",1,0,2021-02-22 10:00:21.287000 UTC,,,1,gremlin|tinkerpop3|amazon-neptune,463,2019-01-14 13:53:40.937000 UTC,2022-03-04 18:10:27.017000 UTC,"Ankara, Türkiye",609,25,0,144,,,,,,[]
Spark. Recover checkpointed RDD after driver shutdown,"<p>I checkpointed a <code>rdd</code> which takes very long to compute. Then I executed many jobs on such a <code>rdd</code>. Eventually, one of this job failed and the driver shutdown during night. Now I need to recover the checpointed data but I can't. There are many questions like this in SO but none of them answer the question. f.e.:</p>

<p><a href=""https://stackoverflow.com/questions/44022313/how-to-read-checkpointed-rdd"">How to read checkpointed RDD</a> <code>&lt;=</code> The only answer replicates the documentation. Which Is useless</p>

<p><a href=""https://stackoverflow.com/questions/34428056/how-to-recover-from-checkpoint-when-using-python-spark-direct-approach"">How to recover from checkpoint when using python spark direct approach?</a> <code>&lt;=</code> Is about streaming context. </p>

<p>My enviroment is azure databricks notebooks <code>spark 2.4.3</code> and <code>python 3</code></p>",0,2,2019-10-28 09:38:35.537000 UTC,,,1,azure|pyspark|azure-databricks,38,2018-01-26 07:51:24.790000 UTC,2022-03-04 12:06:48.040000 UTC,"Sevilla, España",3552,249,35,227,,,,,,[]
How to list all delta tables in Databricks Azure?,"<p>I have saved one dataframe in my delta lake, below is the command:</p>

<pre><code>df2.write.format(""delta"").mode(""overwrite"").partitionBy(""updated_date"").save(""/delta/userdata/"")
</code></pre>

<p>Also I can load and see the delta lake /userdata:</p>

<pre><code>dfres=spark.read.format(""delta"").load(""/delta/userdata"")
</code></pre>

<p>but here , I have one doubt like when I am moving several parquet files from blob to delta lake creating dataframe, then how some one else would know which file I have moved and how he can work on those delta, is there any command to list all the dataframes in delta lake in databricks?</p>",1,1,2019-12-13 06:45:33.757000 UTC,,2019-12-29 20:38:40.923000 UTC,1,apache-spark|azure-databricks|delta-lake,3193,2016-02-04 06:04:24.133000 UTC,2020-07-08 13:20:56.697000 UTC,,175,24,0,59,,,,,,[]
Unable to delete Azure Resource Group,"<p>I'm trying to delete Ra esource Group or the resources in that group.</p>
<ul>
<li>Databricks got deleted half-way is now in a failed state and can't be removed</li>
<li>A storage account can't be deleted because of an other resource requiring exclusive access</li>
<li>The Private Endpoint on the Storage Account can't be deleted, because there is no changes possible on the Storage Account - I'm owner on Subscription level</li>
<li>The AKS namespace returns an 'internal server error'</li>
<li>The Log Analytics workspace I can't delete because a SQL solution can be deleted 'ajaxExtended call failed'</li>
</ul>
<p>Tried using CLI, PowerShell, checked for Locks, re-registered providers, tried moving resources ....</p>",0,4,2021-02-13 18:28:39.100000 UTC,,,0,azure-resource-manager|azure-aks|azure-databricks|azure-log-analytics|azure-storage-account,171,2016-02-01 18:14:49.507000 UTC,2022-03-04 14:50:05.950000 UTC,"Zürich, Switzerland",223,12,0,67,,,,,,[]
Python code to list files in each sub directory in Azure Databricks,"<p>I am trying to list the files, their column count, column names from each sub directory present inside a directory,</p>
<pre><code>Directory : dbfs:/mnt/adls/ib/har/
Sub Directory    2021-01-01
File                A.csv
File                B.csv
Sub Directory    2021-01-02
File                A1.csv
File                B1.csv
</code></pre>
<p>With the below code I am getting the error 'PosixPath' object is not iterable in the second for loop. Could someone help me out please?</p>
<pre><code>files = dbutils.fs.ls(f&quot;dbfs:/mnt/adls/ib/har/&quot;)
for fi in files: 
  il=fi.path
  print(il)
  ill=Path(il)
  for fii in ill:
    if(&quot;.csv&quot; in fii.path):
      df2 = spark.read.option(&quot;header&quot;,&quot;true&quot;).option(&quot;sep&quot;, &quot;;&quot;).option(&quot;escape&quot;, &quot;\&quot;&quot;).csv(f&quot;{fii.path}&quot;)
      m = df2.columns
      l = len(df2.columns)
      print(f&quot;{fii.path} has, {l} columns, {m}&quot;)
      cols[fii.path] = l

maxkey = max(cols, key=cols.get)
maxvalue = cols.get(maxkey)
</code></pre>",1,0,2021-09-27 15:04:04.787000 UTC,,2021-09-27 16:24:02.833000 UTC,0,pyspark|azure-functions|azure-databricks,628,2021-09-27 14:52:57.320000 UTC,2022-03-01 17:59:06.020000 UTC,,1,0,0,1,,,,,,[]
Register Azure ML Model from DatabricksStep,"<p>I'm calculating a model while executing a DatabricksStep in an Azure ML Pipeline, save it on my Blob Storage as .pkl file and upload it to the current Azure ML Run using Run.upload_file (). All this works without any problems.</p>
<p>But as soon as I try to register the model to the Azure ML Workspace using Run.register_model (), the script throws the following error:</p>
<p>UserErrorException: UserErrorException:
Message:
Operation returned an invalid status code 'Forbidden'. The possible reason could be:</p>
<ol>
<li>You are not authorized to access this resource, or directory listing denied.</li>
<li>you may not login your azure service, or use other subscription, you can check your
default account by running azure cli commend:
'az account list -o table'.</li>
<li>You have multiple objects/login session opened, please close all session and try again.</li>
</ol>
<p>InnerException None
ErrorResponse
{
&quot;error&quot;: {
&quot;code&quot;: &quot;UserError&quot;,
&quot;message&quot;: &quot;\nOperation returned an invalid status code 'Forbidden'. The possible reason could be:\n1. You are not authorized to access this resource, or directory listing denied.\n2. you may not login your azure service, or use other subscription, you can check your\ndefault account by running azure cli commend:\n'az account list -o table'.\n3. You have multiple objects/login session opened, please close all session and try again.\n                &quot;
}
}</p>
<p>with the following call stack</p>
<p>/databricks/python/lib/python3.7/site-packages/azureml/_restclient/models_client.py in register_model(self, name, tags, properties, description, url, mime_type, framework, framework_version, unpack, experiment_name, run_id, datasets, sample_input_data, sample_output_data, resource_requirements)
70         return self.<br />
71             _execute_with_workspace_arguments(self._client.ml_models.register, model,
---&gt; 72                                               custom_headers=ModelsClient.get_modelmanagement_custom_headers())
73
74     @error_with_model_id_handling</p>
<p>/databricks/python/lib/python3.7/site-packages/azureml/_restclient/workspace_client.py in _execute_with_workspace_arguments(self, func, *args, **kwargs)
65
66     def _execute_with_workspace_arguments(self, func, *args, **kwargs):
---&gt; 67         return self._execute_with_arguments(func, copy.deepcopy(self._workspace_arguments), *args, **kwargs)
68
69     def get_or_create_experiment(self, experiment_name, is_async=False):</p>
<p>/databricks/python/lib/python3.7/site-packages/azureml/_restclient/clientbase.py in _execute_with_arguments(self, func, args_list, *args, **kwargs)
536                 return self._call_paginated_api(func, *args_list, **kwargs)
537             else:
--&gt; 538                 return self._call_api(func, *args_list, **kwargs)
539         except ErrorResponseException as e:
540             raise ServiceException(e)</p>
<p>/databricks/python/lib/python3.7/site-packages/azureml/_restclient/clientbase.py in _call_api(self, func, *args, **kwargs)
234                 return AsyncTask(future, _ident=ident, _parent_logger=self._logger)
235             else:
--&gt; 236                 return self._execute_with_base_arguments(func, *args, **kwargs)
237
238     def _call_paginated_api(self, func, *args, **kwargs):</p>
<p>/databricks/python/lib/python3.7/site-packages/azureml/_restclient/clientbase.py in _execute_with_base_arguments(self, func, *args, **kwargs)
323         total_retry = 0 if self.retries &lt; 0 else self.retries
324         return ClientBase._execute_func_internal(
--&gt; 325             back_off, total_retry, self._logger, func, _noop_reset, *args, **kwargs)
326
327     @classmethod</p>
<p>/databricks/python/lib/python3.7/site-packages/azureml/_restclient/clientbase.py in _execute_func_internal(cls, back_off, total_retry, logger, func, reset_func, *args, **kwargs)
343                 return func(*args, **kwargs)
344             except Exception as error:
--&gt; 345                 left_retry = cls._handle_retry(back_off, left_retry, total_retry, error, logger, func)
346
347             reset_func(*args, **kwargs)  # reset_func is expected to undo any side effects from a failed func call.</p>
<p>/databricks/python/lib/python3.7/site-packages/azureml/_restclient/clientbase.py in _handle_retry(cls, back_off, left_retry, total_retry, error, logger, func)
384 3. You have multiple objects/login session opened, please close all session and try again.
385                 &quot;&quot;&quot;
--&gt; 386                 raise_from(UserErrorException(error_msg), error)
387
388             elif error.response.status_code == 429:</p>
<p>/databricks/python/lib/python3.7/site-packages/six.py in raise_from(value, from_value)</p>
<p>Did anybody experience the same error and knows what is its cause and how to solve it?</p>
<p>Best,
Jonas</p>
<p>UPDATE:</p>
<pre><code> model = sklearn.linear_model.LinearRegression ( )
 model_path = &quot;&lt;path to 'model.pkl' in my blob storage&gt;&quot;
 joblib.dump(model, model_path)
 aml_run = azureml.core.get_context ( )
 aml_run.upload_file (name = &quot;model.pkl&quot;, path_or_stream = model_path)
 # Until this point, everything works fine
    
 aml_run.register_model (model_name = &quot;model.pkl&quot;)
 # This throws the posted &quot;Forbidden&quot;-Error
</code></pre>",2,1,2020-11-13 10:58:44.633000 UTC,,2020-11-16 07:57:08.203000 UTC,2,azure-databricks|azure-machine-learning-studio,485,2020-07-16 05:39:33.727000 UTC,2022-01-24 09:55:29.407000 UTC,Germany,51,0,0,2,,,,,,[]
How do you test locally with Neptune?,"<p>I am trying to test our neptune db using gremlin. I have come up short with way to test locally. What are my options?</p>

<p>Currently just testing in real time. This is obviously not very effecient.</p>",1,1,2019-05-31 15:16:53.800000 UTC,1.0,,1,javascript|testing|local|amazon-neptune,782,2019-05-31 15:11:19.277000 UTC,2021-04-13 00:22:44.427000 UTC,"Portland, OR, USA",41,0,0,7,,,,,,[]
Sql Apache Spark connector from data bricks to sqlserver for Bulk insertion : performance issue,"<p>This is my problem statment and  requirement, looking for some help</p>
<p>“ We are using the spark connector to import the data from the delta files into azure sqlserver, our entities are close to 60 million records this connector has helped us to import the data from my notebook successful. As our entities were growing the need to import the data into sqlserver has also increased the load, we have observed some performance issues arising with this increase in load. I have observed that the Bulk operation is creating multiple connections to the  sqlsever and I can see there are range of 70 connections created for the same  operation ( insert bulk in my case ) but only one is running as I have the tablock and other are in waiting state but my CPU consumption spikes to 100% and other jobs get failed because of this.</p>
<p>My questions : Can we control the connections or concurrency being raised /operated from the spark connector, can you help me with this state of problem on how we can control the connections &amp; consumption of CPU</p>",0,2,2021-01-31 21:54:13.373000 UTC,,,0,sql-server|scala|apache-spark|bulkinsert|azure-databricks,51,2013-03-22 19:12:27.517000 UTC,2022-03-03 19:58:16.283000 UTC,"Hyderabad, Telangana, India",51,2,0,24,,,,,,[]
Querying Turtle data through SPARQL,"<p>I have uploaded my turtle files  data from AWS S3 to Neptune service.</p>

<p>Now I am querying below sparql from RDF4J console to view my data in Neptune  but getting ""no gremlin script supplied"" error.</p>

<pre><code>neptune&gt; select ?p ?o { &lt;http://www.1234.com/XXX/App_Inst/1007259&gt; ?p ?o }
select ?p ?o { &lt;http://www.1234.com/XXX/App_Inst/1007259&gt; ?p ?o }
Evaluating SPARQL query...
Query evaluation error: {""requestId"":""62b298f5-032e-0d8e-82ac-ed88a1469298"",""code"":""MissingParameterException"",""detailedMessage"":""no gremlin script supplied""}
</code></pre>

<p>Can you please help me to resolve this issue.</p>

<p>I have configured RDF4J console as per the <a href=""https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-sparql-rdf4j-console.html/"" rel=""nofollow noreferrer"">document</a> provided by AWS.</p>

<p>If I executing the same query from Unix EC2 instance , getting below results.</p>

<pre><code>[ec2-user@ip-10-10-10-69 ~]$ curl -X POST  --data-binary 'query=select ?p ?o { &lt;http://www.1234.com/XXX/App_Inst/1007259&gt; ?p ?o }'  http://neputne.amazonaws.com:882/sparql
</code></pre>

<p><strong>EDIT :</strong></p>

<p>This is the result coming from EC2 instance when ran sparql.</p>

<pre><code>{ ""head"" : { ""vars"" : [ ""p"", ""o"" ] }, ""results"" : { ""bindings"" : [ ] }
</code></pre>",2,4,2018-08-13 06:33:17.030000 UTC,,2018-08-14 05:55:12.220000 UTC,0,sparql|amazon-neptune,272,2017-07-06 04:54:35.257000 UTC,2018-08-23 09:11:46.100000 UTC,"Hyderabad, Telangana, India",29,0,0,30,,,,,,[]
PySpark XML processing - Ignoring bad records,"<p>I am processing a large XML file using the Spark XML Library (HyukjinKwon:spark-xml:0.1.1-s_2.11). The XML processing fails with an analysis exception for a couple of records. I would like to keep processing the file ignoring these records. </p>

<p>I have the below code for processing the xml and I tried the option of 'DROPMALFORMED' but didn't help.</p>

<pre><code>df = (spark.read.format(""xml"")
      .option(""rootTag"",""Articles"")
      .option(""rowTag"", ""Article"")
      .option(""inferSchema"", ""true"")
      .option(""mode"", ""DROPMALFORMED"")
      .load(""/mnt/RawAdl2/problemfile.xml""))

AnalysisException: ""cannot resolve '['Affiliation']' due to data type mismatch: argument 2 requires integral type, however, ''Affiliation'' is of string type.;
</code></pre>

<p>I would like to drop the malformed records and continue with the processing of the file. Is there any other option I could try? Appreciate the inputs!</p>

<p>EDIT: Looking at the source code <a href=""https://github.com/databricks/spark-xml/pull/22/files/3cab3933319bdbee9bd9c314d7a2c70071ffc439#diff-d85018a30b75475a6008ee92c967036b"" rel=""nofollow noreferrer"">link</a> the Malformed option is supported by the library. As I am not well versed with Scala, I am not really sure whether I am using the correct syntax for this option. Please advise. </p>

<p>After going through the source code, I tried this below code but no luck</p>

<pre><code>.option(""mode"", ""DROP_MALFORMED_MODE"")
</code></pre>",1,0,2019-04-04 17:57:19.067000 UTC,,2019-04-04 19:39:47.177000 UTC,0,pyspark|azure-databricks,272,2017-03-08 17:25:54.630000 UTC,2022-03-03 21:48:31.427000 UTC,"Madison, WI, USA",379,68,0,55,,,,,,[]
How to hock commits (on JIRA 7.0.5 with DVCS connected to Bitbucket Cloud),"<p>I have JIRA 7.0.5 + DVCS (Latest) connected to Bitbucket Cloud.</p>

<p>In JIRA: repo enabled, smart commits are on, branches creating successful.</p>

<p>But I see no activity about my new commits. What I need to do?</p>",1,1,2016-01-15 13:29:30.967000 UTC,,,0,git|jira|bitbucket|commit|dvcs,51,2016-01-15 12:43:11.593000 UTC,2016-01-15 14:45:03.460000 UTC,,1,0,0,4,,,,,,[]
Data load from blob storage to sql data warehouse using azure databricks scala,"<p>I am trying load the data into SQL datawarehouse  from blob storage using azure databricks scala.</p>

<pre><code>spark.conf.set(""spark.sql.parquet.writeLegacyFormat"",""true"")    
df.write.format(""com.databricks.spark.sqldw"")
.option(""url"",sqlDwUrlSmall)
.option(""dbtable"", ""Person"")        
.option(""forward_spark_azure_storage_credentials"",""True"")
.option(""tempdir"",tempDir).mode(""overwrite"").save()
</code></pre>

<p>I am getting this error</p>

<blockquote>
  <p>Underlying SQLException(s):
    - com.microsoft.sqlserver.jdbc.SQLServerException: External file access failed due to internal error: 'Error occurred while accessing
  HDFS: Java exception raised on call to HdfsBridge_IsDirExist. Java
  exception message: HdfsBridge::isDirExist - Unexpected error
  encountered checking whether directory exists or not:
  StorageException: This request is not authorized to perform this
  operation.' [ErrorCode = 105019] [SQLState = S0001]</p>
</blockquote>",1,1,2019-11-20 03:47:15.460000 UTC,1.0,2019-11-20 05:41:11.180000 UTC,1,scala|azure|azure-databricks,418,2019-11-20 03:32:03.047000 UTC,2021-09-28 21:19:44.823000 UTC,"Evansville, IN, USA",67,0,0,6,,,,,,[]
pyspark write to external hive cluster from databricks running on azure cloud,"<p>I have pyspark notebooks running in databricks.
I connect to an external hive cluster using 'hive.Connection' from pyhive.
I have my data in spark dataframes.
My question is how do I write this data from dataframes in a new table in Hive which resides in a different cluster other than databricks?</p>
<p>Thanks</p>",1,0,2020-09-25 18:20:15.347000 UTC,,,0,pyspark|hive|azure-databricks|pyhive,110,2012-11-28 16:01:55.660000 UTC,2021-05-28 23:12:09.437000 UTC,,1218,6,1,159,,,,,,[]
feeding filenames in azure databrick activity one by one in azure datafactory pipeline,<p>I am trying to create a data factory pipeline where one activity injects names of files (from a container or some other folder) in databricks activity one by one to be processed in the order of incoming. How do I achieve it?</p>,2,0,2019-06-17 11:27:50.870000 UTC,0.0,,1,azure-pipelines|azure-data-factory-2|azure-databricks,663,2019-04-16 07:45:05.030000 UTC,2020-03-21 06:53:38.723000 UTC,"Gurgaon, Haryana, India",11,0,0,2,,,,,,[]
Is Data encrypted during In-Transit in Azure Data Factory and Databricks runtime?,"<p>Is Data encrypted during In-Transit in Azure Data Factory while data movement
and Databricks runtime when data transformation.</p>

<p>Please share artifcats if any to understand the flow.</p>

<p>Thanks,
Mahammad khan</p>",0,3,2020-03-13 14:27:23.093000 UTC,,,0,azure-data-factory|azure-databricks,118,2016-03-03 14:22:40.160000 UTC,2021-08-12 10:07:30.020000 UTC,,1,0,0,4,,,,,,[]
FATAL ERROR: Committing semi space failed. Allocation failed - process out of memory,"<p>I have the following snippet, in which I am trying to execute 10000 execute statement to understand that how many writes that Gremlin can withstand.</p>

<pre><code>var gremlin = require('gremlin');
var async = require('async');
var client = gremlin.createClient(8182, ""development.cluster-coeuolcg4r.us-east-1-beta.rds.amazonaws.com"", {
    accept: ""application/vnd.gremlin-v2.0+json""
});

console.time('load');
async.times(10000,  function (t, tCB) {
    client.execute('g.addV(""loadtest#1-50000#2"").property(""idx"", '+new Date().getTime()+')', function (err) {
        if(err) {
            console.log(err);
        }
        tCB(null, 1);
    });
}, function () {
    console.timeEnd('load')
});
</code></pre>

<p>It is a simple snippet while running this I am getting an error and program execution gets stop.</p>

<p><a href=""https://i.stack.imgur.com/ifkgj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ifkgj.png"" alt=""enter image description here""></a></p>

<p>It is working as expected if I ran this snippet for 5000.</p>

<p>Side note: --max-old-space-size didn't worked.</p>",1,1,2018-04-05 11:46:26.557000 UTC,,2018-04-06 12:27:53.387000 UTC,1,node.js|amazon-web-services|gremlin|async.js|amazon-neptune,1812,2014-01-27 05:57:33.367000 UTC,2021-09-29 18:22:53.603000 UTC,,1206,55,18,105,,,,,,[]
Which head to position on before doing a merge?,"<p>Is there a head to prefer when doing a merge?</p>

<p>What I mean is this: I've got, say, old rev 1000.  Meanwhile I did 234 commits and I'm at rev 1234.  Now I need to go back to rev 1000 to implement a bugfix for a customer.  I commit the bugfix, give the release to the customer, and have the commit 1235.</p>

<p>It's only a tiny change: affecting only one file.</p>

<p>At this point I have two heads: 1235 (whose parent is 1000) and 1234.  Their common (grand-grand-...-parent) is 1000.</p>

<p>If I issue an <code>hg merge</code> followed by an <code>hg status</code>, I receive a gigantic list of changes.</p>

<p>However if I do first <code>hg update -C 1234</code>, followed by an <code>hg merge</code> and an <code>hg status</code>, then I only see my unique change (unless I'm mistaken as to what just happened).</p>

<p>Basically, doing this:</p>

<pre><code>hg update -C 1234
hg merge  # (merging 1234 and 1235, my two only heads)
hg status
</code></pre>

<p>gives a different status than this:</p>

<pre><code>hg update -C 1235
hg merge  # (merging 1234 and 1235, my two only heads)
hg status
</code></pre>

<p>So basically, I'm asking the status (<code>hg status</code>) after merging the two same heads, but the output of <code>hg status</code> seems to depends on the head I'm currently at.</p>

<p>Is this the normal behavior and, if yes, is there one head to ""prefer"" over the other?</p>

<p>Do both operation result in the same repository / source code state at the end?</p>",1,0,2012-01-17 20:15:35.560000 UTC,,2012-01-18 06:37:30.270000 UTC,5,mercurial|merge|dvcs|head,91,2010-05-02 17:42:37.973000 UTC,2012-01-29 18:51:10.783000 UTC,,4183,362,77,241,,,,,,[]
org.postgresql.util.PSQLException: SSL error: Received fatal alert: handshake_failure while writing from Azure Databricks to Azure Postgres Citus,"<p>I am trying to write pyspark dataframe to Azure Postgres Citus (Hyperscale).
I am using latest Postgres JDBC Driver and I tried writing on Databricks Runtime 7,6,5.</p>
<p><code>df.write.format(&quot;jdbc&quot;).option(&quot;url&quot;,&quot;jdbc:postgresql://&lt;HOST&gt;:5432/citus?user=citus&amp;password=&lt;PWD&gt;&amp;sslmode=require&quot; ).option(&quot;dbTable&quot;, table_name).mode(method).save()</code></p>
<p>This is what I get after running the above command
<code>org.postgresql.util.PSQLException: SSL error: Received fatal alert: handshake_failure</code></p>
<p>I have already tried different parameters in the URL and unders the option as well, but no luck so far.
However, I am able to connect to this instance using my local machine and on databricks driver/notebook using psycopg2
Both the Azure Postgres Citus and Databricks are in the same region and Azure Postgres Citus is public.</p>",1,0,2020-09-13 08:36:07.657000 UTC,,,2,azure|apache-spark|pyspark|azure-databricks|azure-postgresql,2230,2016-02-05 13:45:06.600000 UTC,2022-03-04 09:44:32.483000 UTC,JAIPUR,548,19,10,70,,,,,,[]
Moving data from azure sql to Databricks,"<p>I am new to databricks, I am here facing issue in moving data from Azure Sql database to Azure Databricks, can anyone please help me in doing this?</p>",1,0,2019-04-02 09:15:12.180000 UTC,1.0,,0,azure-databricks,69,2019-03-12 10:50:53.323000 UTC,2020-11-06 10:18:34.930000 UTC,,11,0,0,20,,,,,,[]
"With Mercurial (Distributed Version Control), how do we know usernames are not false?","<p>Supposing there were 3 or 4 developers using Mercurial, and all making updates to a project. Right now, usernames are self-configured.</p>

<p>If I pull changes from a colleague, (which may include other changes he pulled from a different colleague), how can I be sure the usernames on each commit actually were authored by that user, and not a different user who might have entered a fake username on that commit?</p>

<p>I assume Mercurial has some solution for this problem built in, perhaps using cryptography to compare the username to the hash and private salt or key or something.</p>

<p>Is there a way to validate authors for each commit? How does this work, and is it possible to do this whilst maintaining the Distributed nature of our Version Control System, or will we need an authenticating server?</p>",1,0,2017-01-30 18:51:29.660000 UTC,,,1,authentication|mercurial|commit|dvcs|pull,28,2010-09-30 21:11:12.500000 UTC,2022-03-05 21:05:07.657000 UTC,"Chattanooga, Tennessee, USA",80429,943,80,4269,,,,,,[]
"What is the difference between ""Git"" in general and Github?","<p>I know that Github is the most common site programmers use to save and make changes to code, but is this the only software out there? What are other relevant sites that utilize ""Git"" or version control that I could use?</p>",2,1,2016-05-06 05:23:20.987000 UTC,,2016-12-05 17:09:05.990000 UTC,1,git|github|version-control|dvcs,52,2015-11-09 08:34:10.477000 UTC,2018-06-18 18:49:37.957000 UTC,,37,32,0,2,,,,,,[]
Question with sql execution in Azure Databricks,"<p>I'm new in this of Spark and Azure Databricks, I hope you can understand my question.</p>
<p>We have a query that is created in another company department, and we just need to execute it and store the data. It is a very simple query that should return something like this:</p>
<p><a href=""https://i.stack.imgur.com/bBxvA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bBxvA.png"" alt=""enter image description here"" /></a></p>
<pre><code>%sql    
Select 
From Table_1 as A inner join Table_2 as B
on A.PK = B.FK
Where A.columnA = 'h' or B.columnaB = 'g'
</code></pre>
<p>However, we are facing a behaviour that we can not understand. With the current where sentences, the query is not showing any result. If we execute any of the two conditions apart (Where A.columnA = 'h' or B.columnaB = 'g') we will get some rows. But the only way in which we can see any output from the original <strong>where</strong> is comparing one of the column (or both) against a capital letter:</p>
<pre><code>Where A.columnA = 'H' or B.columnaB = 'G'
Where A.columnA = 'h' or B.columnaB = 'G'
Where A.columnA = 'H' or B.columnaB = 'g'
</code></pre>
<p>Just to add more doubts, during the test we saw that by changing the <strong>or</strong> operator for an <strong>and</strong> condition the query provides a result.</p>
<p>What could be a reason for this or how can we check what is happening?</p>
<p>This is just to simplify the question, we can not modify the query. We need to inform what the issue is and the other team will fix it.</p>",0,2,2022-02-16 20:29:40.003000 UTC,,,0,sql|apache-spark-sql|azure-databricks,26,2013-10-09 10:41:34.473000 UTC,2022-03-05 02:55:01.717000 UTC,,658,82,2,188,,,,,,[]
Read file from Azure Databricks DBFS REST 2.0 API,"<p>I am working on an application to read and write files using Azure Databricks DBFS API 2.0. Reference Documentation for this API:
<a href=""https://docs.azuredatabricks.net/api/latest/dbfs.html#read"" rel=""nofollow noreferrer"">https://docs.azuredatabricks.net/api/latest/dbfs.html#read</a></p>

<p>I am able to upload file (lets say 1.4MB file) by converting it into base64 then divided into 1MB chunks of data.</p>

<p>As the read length is limited to 1MB, I am iterating a loop to read data from offset <code>0</code> to <code>1000000</code> and <code>1000001</code> to <code>end of the file</code>. Now the first iteration of data, <code>0</code> - <code>1000000</code>, is valid and I can confirm from the original file which i used for upload.</p>

<p>But the second and later data iterations, the base64 data is completely different and not present in the original base64 file.</p>

<p>Following is my test code:
<code>Second iteration</code> - 0.4MB</p>

<pre><code>const axios = require('axios')
const fs = require('fs')

axios({
  method: 'get',
  url: 'https://********.azuredatabricks.net/api/2.0/dbfs/read',
  data: {
    path: '/Test/backlit-beach-clouds-1684881.jpg',
    offset: 0,
    length: 1000000
  },
  headers: {
    'Authorization': `Bearer ****`
  }
}).then(({data}) =&gt; {
  if (data) {
    console.log('Success', data.bytes_read)
    fs.writeFile('./one.txt', data.data, function (err) {
      console.log('done', err)
    })
  } else {
    console.log('failed')
  }
})
</code></pre>

<p><code>First iteration</code> - 1MB</p>

<pre><code>const axios = require('axios')
const fs = require('fs')

axios({
  method: 'get',
  url: 'https://********.azuredatabricks.net/api/2.0/dbfs/read',
  data: {
    path: '/Test/backlit-beach-clouds-1684881.jpg',
    offset: 1000001,
    length: 1000000
  },
  headers: {
    'Authorization': `Bearer ****`
  }
}).then(({data}) =&gt; {
  if (data) {
    console.log('Success', data.bytes_read)
    fs.writeFile('./two.txt', data.data, function (err) {
      console.log('done', err)
    })
  } else {
    console.log('failed')
  }
})
</code></pre>

<p>Here, *** are replaced with relevant domain and tokens.</p>

<p>As you can see, the above test code samples will generate <code>one.txt</code> and <code>two.txt</code>. By using <code>cat one.txt two.txt &gt; final.txt</code> I can get <code>final.txt</code> which I will use to decode original file.</p>

<p>As this is just a testing code, I haven't used any loops or better coding format. This is just to understand what went wrong.</p>

<p>I stuck with this for over 1 week now. I am referring other code samples written for python, but no help.</p>

<p>I am not trying to waste anyone's time. But please, someone help me to figure out what went wrong or any other standard procedure that I can follow?</p>",1,0,2019-01-22 17:54:55.027000 UTC,,,2,azure-databricks,548,2017-03-27 05:41:08.753000 UTC,2019-05-15 15:15:16.873000 UTC,,45,0,0,27,,,,,,[]
How to create a new column with 2 or more condition validation in Koalas,"<p>I have made the column &quot;Turno&quot; on the df3 using 3 validation to classify into &quot;Turno_PM&quot;, &quot;Turno_AM&quot; or &quot;N/A&quot;, but I want to know if exist an &quot;easies way&quot; to reach the same result, like a &quot;cycle for&quot; with if/elif/else or something like that.</p>
<p>Here the code that I have used.</p>
<pre><code>from databricks import koalas as ks
from databricks.koalas.config import set_option, reset_option
set_option(&quot;compute.ops_on_diff_frames&quot;, True)

#Turno PM
kdf.loc[(kdf['dot_agencia_origen'] == 'AGENCIA RM') &amp; (kdf['dot_agencia_destino']!='AGENCIA RM') | (kdf['dot_agencia_origen'] == 'AGENCIA VALPARAISO') &amp; (kdf['dot_agencia_destino']!='AGENCIA RM') &amp; (kdf['dot_agencia_destino']!='AGENCIA VALPARAISO') | (kdf['dot_agencia_origen'] == 'AGENCIA RANCAGUA') &amp; (kdf['dot_agencia_destino']!='AGENCIA RM') &amp; (kdf['dot_agencia_destino']!='AGENCIA RANCAGUA'),'Turno']= 'Turno_PM'


#Turno AM
kdf.loc[(kdf['dot_agencia_origen'] == 'AGENCIA RM') &amp; (kdf['dot_agencia_destino']=='AGENCIA RM') | (kdf['dot_agencia_origen'] == 'AGENCIA VALPARAISO') &amp; (kdf['dot_agencia_destino']=='AGENCIA RM')|(kdf['dot_agencia_origen'] == 'AGENCIA RANCAGUA') &amp; (kdf['dot_agencia_destino']=='AGENCIA RM'),'Turno']='Turno_AM'

#Regiones
kdf.loc[(df3['Turno'].isnull()),'Turno']='Regiones'
</code></pre>",1,1,2021-02-10 03:41:51.873000 UTC,,2021-04-10 01:31:16.593000 UTC,0,pandas|dataframe|azure-databricks|spark-koalas,137,2020-04-26 04:34:22.070000 UTC,2021-10-27 04:28:00.323000 UTC,"Santiago, Chile",13,0,0,3,,,,,,[]
Does branching in Mercurial work as nice as Git?,"<p>I have read nice things about ""branch"" in Git recently.  It seems like Mercurial's way may be to clone a local repo?  But Mercurial has branch too.  Does it work as nice as Git?</p>

<p>What about saving disk space in Mac / Linux / Windows?  Do they all do links on Mac and Linux but make a copy on Windows?</p>",3,1,2010-09-23 07:17:08.490000 UTC,,,3,git|mercurial|dvcs,241,2009-05-09 15:50:29.477000 UTC,2022-03-04 09:41:10.460000 UTC,,137341,1445,39,12817,,,,,,[]
How to move files from one folder to another on databricks,"<p>I am trying to move the file from one folder to another folder using databricks python notebook.
My source is azure data lake gen 1.</p>
<p>Suppose, my file is present adl://testdatalakegen12021.azuredatalakestore.net/source/test.csv
and I am trying to move the file from adl://testdatalakegen12021.azuredatalakestore.net/demo/test.csv to adl://testdatalakegen12021.azuredatalakestore.net/destination/movedtest.csv</p>
<p>I tried various logic but not none of my code is working fine.</p>
<pre><code># Move a file by renaming it's path
import os
import shutil
os.rename('adl://testdatalakegen12021.azuredatalakestore.net/demo/test.csv', 'adl://testdatalakegen12021.azuredatalakestore.net/demo/renamedtest.csv')

# Move a file from the directory d1 to d2
shutil.move('adl://testdatalakegen12021.azuredatalakestore.net/demo/test.csv', 'adl://testdatalakegen12021.azuredatalakestore.net/destination/renamedtest.csv')
</code></pre>
<p>Please, let me know If I am using correct logic as I am executing this on databricks, not in my local.</p>",3,0,2021-01-04 09:35:35.050000 UTC,,2021-01-04 10:03:07.490000 UTC,1,python|apache-spark|pyspark|azure-data-lake|azure-databricks,5291,2020-07-17 18:47:19.493000 UTC,2022-03-04 07:03:48.263000 UTC,,37,0,0,35,,,,,,[]
How to traverse two recursive patterns using one SPARQL query?,"<p>I want to have recursive patterns in one query. Can somebody help me with that?</p>
<p>For traversing the &quot;knows&quot; pattern I have written this query and I want to add a &quot;name&quot; as well -</p>
<pre><code>construct { 
   ?y :knows ?z . 
}
WHERE
{
    VALUES ?x {:A} 
    ?x :knows* ?y .
    ?y :knows ?z .
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/u27NW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u27NW.png"" alt=""Here is a sample of what I want to achieve"" /></a></p>",0,1,2021-11-18 11:53:45.003000 UTC,,2021-11-18 15:27:16.867000 UTC,0,sparql|rdf|amazon-neptune,36,2021-09-09 15:06:28.590000 UTC,2022-03-04 09:53:27.580000 UTC,,27,1,0,6,,,,,,[]
Latency in Spark streaming job Databricks which sources from an Azure Iot Hub,"<p>I have been using a Spark <strong>streaming</strong> job using <strong>Python</strong> on <strong>Databricks</strong> to load sources from an Azure <strong>IotHub</strong>. However I noticed, when we have a large number of received frames, the job comes long, so we have <strong>latency</strong> knowing that when we look at the metrics the CPU and memory are <strong>not</strong> used at 100% of their capacity.</p>",1,0,2021-12-28 18:32:18.417000 UTC,1.0,2022-02-13 18:11:15.767000 UTC,1,python|apache-spark|azure-databricks|spark-structured-streaming|azure-iot-hub,57,2021-12-28 16:04:47.720000 UTC,2022-01-25 08:45:48.813000 UTC,,19,0,0,4,,,,,,[]
hg mq (nested|sub|child) queues,"<p>I know this is not an hg feature, but maybe someone knows a way to get something similar. Hopefully my description makes sense:</p>

<p>I find it usefully to keep my mainline (i.e. default branch) commits in a patch queue for a few weeks to let them ""settle"". However, I'd also like to be able to create topic branches via new queues. These two ideas are mutually exclusive, as you cannot create new queues starting at an applied patch. Sounds like the only way to do this is to finalize my mainline patches, and start branch queues off the qparent commit, and handle tweaks by importing finalized patches back to mq. Any other ideas? Is git better at this sort of workflow?</p>",3,0,2013-06-20 19:20:28.877000 UTC,,,1,version-control|mercurial|dvcs,94,2008-10-10 13:35:04.687000 UTC,2022-02-22 17:18:34.010000 UTC,,2792,481,17,220,,,,,,[]
Is any ready made connecter available which reads message from Kafka topic and insert into AWS Neptune?,"<p>I need to read kafka topic and store them into Neptune (graph db) using kafka connector.</p>
<p>Or Is there any other way to accomplish the task without writing any language specific code which connect to Neptune db and insert the data?</p>",1,2,2021-04-09 11:29:57.877000 UTC,,2021-04-09 14:00:33.963000 UTC,0,amazon-web-services|apache-kafka|apache-kafka-connect|amazon-neptune,139,2020-02-25 09:06:22.840000 UTC,2022-02-08 11:57:07.773000 UTC,"Bangalore, Karnataka, India",21,0,0,7,,,,,,[]
"Trying to update properties if it exists using gremlin query I am getting error as ""Cannot find name 'has'""","<p>Trying to update properties if it exists using gremlin query I am getting error as <code>Cannot find name 'has'</code> and the same query runs fine in Neptune notebook. But for the same thing in node.js I am getting error at &quot;has&quot;</p>
<p>Below is the query. What am I missing here?
<code>Graph.V(Id).hasLabel('test').optional(has('nameProperty').property(single,'nameProperty', 'value'))</code></p>",1,0,2021-03-05 16:45:20.993000 UTC,,2021-03-07 00:45:00.767000 UTC,0,node.js|amazon-neptune|gremlin-server,203,2020-08-11 14:43:34.270000 UTC,2021-10-01 18:41:00.777000 UTC,,87,3,0,6,,,,,,[]
DataType issue from Synapse to Delta table in Databricks?,"<ol>
<li>Copying the data from Synapse to Managed Delta table.</li>
<li>We enabled the staging and copied data from synapse to managed delta table.</li>
<li>We have some of the date columns in the synapse and same schema defined in the delta table.</li>
<li>we have designed the simple pipeline using data flows as the source is synapse and sink is delta file, but while converting all the date columns is changing to timestamp at the run time.</li>
</ol>",1,1,2021-10-30 13:44:59.807000 UTC,,2021-10-30 17:55:48.723000 UTC,0,pyspark|azure-data-factory|azure-data-factory-2|azure-databricks|delta-lake,200,2021-10-15 03:25:09.527000 UTC,2022-03-03 13:38:59.430000 UTC,,39,4,0,7,,,,,,[]
How to convert scala spark.sql.dataFrame to Pandas data frame,"<p>I wanted to Convert scala dataframe into pandas data frame</p>

<pre><code>    val collection = spark.read.sqlDB(config)
    collection.show()

    #Should be like df=collection
</code></pre>",2,1,2019-08-05 06:19:04.253000 UTC,,,0,scala|dataframe|apache-spark|azure-databricks,1153,2017-05-30 03:43:03.657000 UTC,2020-10-21 16:24:10.000000 UTC,"Gurgaon, Haryana, India",273,0,0,93,,,,,,[]
Using graph-notebook to connect to Blazegraph Database,"<p>I tried to use <strong><a href=""https://github.com/aws/graph-notebook"" rel=""nofollow noreferrer"">https://github.com/aws/graph-notebook</a></strong> to connect to Blazegraph Database. I have verified Blazegraph is running from the following.</p>
<pre><code>serviceURL: http://192.168.1.240:9999

Welcome to the Blazegraph(tm) Database.

Go to http://192.168.1.240:9999/bigdata/ to get started.
</code></pre>
<p>I did the following the jupyter notebook</p>
<pre><code>%%graph_notebook_config 
{
  &quot;host&quot;: &quot;localhost&quot;,
  &quot;port&quot;: 9999,
  &quot;auth_mode&quot;: &quot;DEFAULT&quot;,
  &quot;iam_credentials_provider_type&quot;: &quot;ENV&quot;,
  &quot;load_from_s3_arn&quot;: &quot;&quot;,
  &quot;aws_region&quot;: &quot;us-west-2&quot;,
  &quot;ssl&quot;: false,
  &quot;sparql&quot;: {
    &quot;path&quot;: &quot;blazegraph/namespace/foo/sparql&quot;
  }
}
</code></pre>
<p>then do the following</p>
<pre><code>%status
</code></pre>
<p>gives error <em><strong>{'error': JSONDecodeError('Expecting value: line 1 column 1 (char 0)',)}</strong></em></p>
<p>I tried to replace host with <em><strong>192.168.1.240</strong></em> and is still having the same problem.</p>",1,0,2021-06-30 16:33:24.393000 UTC,,2021-10-26 16:05:52.750000 UTC,2,amazon-web-services|jupyter|amazon-neptune|blazegraph|graph-notebook,70,2012-02-03 16:20:42.710000 UTC,2022-03-04 16:51:03.243000 UTC,,5628,119,6,508,,,,,,[]
Is there anyway to take backup/archive of a named graph data only from aws neptune using python?,"<p>Neptune has a feature of snapshot but instead of taking backup for a single named graph, it takes backup of the complete cluster. Is there anyway it can specifically get the backup of give graph name only?(sparql)</p>",1,6,2021-04-06 11:51:21.117000 UTC,,2021-04-07 18:26:46.670000 UTC,0,python|amazon-web-services|sparql|amazon-neptune|named-graphs,86,2020-01-29 11:38:51.017000 UTC,2022-01-06 05:09:56.753000 UTC,,13,0,0,1,,,,,,[]
Can we use a create procedure in pre copy script azure - azure data factory,"<p>I am trying to use a create or alter Procedure in pre copy script of azure copy activity. My intention is not to copy data but to create for alter my stored procedure.</p>
<p><a href=""https://i.stack.imgur.com/OGqGI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OGqGI.png"" alt=""enter image description here"" /></a></p>",1,0,2021-06-25 08:12:07.143000 UTC,,2021-07-10 19:40:21.803000 UTC,0,sql|azure|azure-data-factory|azure-data-factory-2|azure-databricks,131,2020-02-03 14:06:11.353000 UTC,2021-07-16 06:58:41.060000 UTC,,49,0,0,31,,,,,,[]
Collecting properties and certain type of out nodes during a gremlin graph traversal,"<p>I have a simple graph of the type:</p>

<pre><code>(3) -&gt; (2) -&gt; (1)
 |     | |     |
(D)    |(B)   (A)
      (C)
</code></pre>

<p>In this case, I want to traverse, from (3) to (1). Think of this as a tree where node (1) is the root and node (3) is a leaf and the edges between (3) to (2) and (2) to (1) represent parent relationship (2 is parent of 3, etc.).</p>

<p>At the same time, each vertex that represents tree node (1, 2, 3) has one or more edges that represent permissions relationships, so in this case, (A) represents a user and the relationship between (1) to (A) represents a permission (access) the user (A) has over (1). In the case of (2), there are two users that have access to this vertex (B and C).</p>

<p>You can imagine that nodes with numbers are ""folders"" and have certain attributes (i.e. name) and nodes with letters are users or permissions and have certain attributes too (i.e name, type of access, etc).</p>

<p>I can successfully traverse the graph from 3 to 1, printing properties of each of the nodes in the path (see example below).</p>

<p>So for example if I do:</p>

<pre><code>gremlin&gt; g.V('3').repeat(out()).until(has(id, '1')).path().by('name')
==&gt;[folder1.1, folder1, root]
</code></pre>

<p>or</p>

<pre><code>gremlin&gt; g.V('3').repeat(out()).until(has(id, '1')).path().by('name')
==&gt;[folder1.1, folder1, root]

gremlin&gt; g.V('3').repeat(out('parent').simplePath().store('x')).until(has(id, '1')).cap('x')
==&gt;{v[2]=1, v[1]=1}

// Although at this point I missed listing v[3]
</code></pre>

<p>The problem is that I need to traverse the tree and collect the permissions in the way, for example (and format may vary) I'm looking for a way to get something in the lines of:</p>

<pre><code>gremlin&gt; ...
==&gt;[ { { 3, name=""foo"" } , [ { D, permission=""x"" } ] }, { 2, [ {A, permission=""y"" }, {B, permission=""z""} ] }, { { 3, name=""foo"" } , [ { D, permission=""x"" } ] } ]
</code></pre>

<p>Basically I want to traverse the path from (3) to (1) (there may be more than just three nodes), collecting attributes of the vertex in the path, and collecting vertex related to certain out edges (just one level, I don't want to expand beyond a single edge in depth for permissions) together with their attributes.</p>

<p>Note that I'm very new to gremlin, and I've been trying and learning in the process for a couple of days without even knowing if it is possible...</p>

<p>Any idea, suggestion would be appreciated, thanks!</p>",1,0,2019-05-11 02:05:42.910000 UTC,,,1,graph-databases|gremlin|tinkerpop|tinkerpop3|amazon-neptune,381,2019-05-11 01:34:15.640000 UTC,2019-05-15 01:39:41.750000 UTC,,11,0,0,1,,,,,,[]
Azure log analytics data collector rest API connection timeout error from Databricks,"<p>I'm trying to send some custom logs to log analytics from Databricks notebook using <a href=""https://docs.microsoft.com/en-us/azure/azure-monitor/platform/data-collector-api"" rel=""nofollow noreferrer"">Microsoft tutorial</a> , however I'm facing rest API connection timeout error.</p>
<pre><code>ConnectionError: HTTPSConnectionPool(host='XXXXXXX-XXXX-XXXX-XXXX-XXXXXXXX.ods.opinsights.azure.com', port=443): Max retries exceeded with url: /api/logs?api-version=2016-04-01 (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7fbed9108310&gt;: Failed to establish a new connection: [Errno 110] Connection timed out'))
</code></pre>
<p>Any suggestions please? How can I allow Azure Databricks to access log analytics workspace?</p>",1,0,2020-12-09 00:38:19.117000 UTC,,,0,azure|azure-databricks|azure-log-analytics,428,2013-09-12 10:12:49.767000 UTC,2022-03-04 00:11:05.370000 UTC,"Hyderabad, India",393,27,0,137,,,,,,[]
"How to setup a Mercurial ""server"" that allows for authenticated push without apache?","<p>I'm trying to propose switching from CVS and SVN to Mercurial at work.  Before I do, I'd like to have any foreseeable questions answered.</p>

<p>How can I set up a repository to allow push and authenticate users?</p>

<p>I'm more familiar with administering SVN, and in SVN it was just a few lines like:</p>

<pre><code>[users]
userA = passwordA
userB = passwordB
</code></pre>

<p>And then for permissions it was like:</p>

<pre><code>[general]
userA = write
userB = read
</code></pre>

<p>I would really like something like svnserve that allowed me to circumvent using a full-blown apache, since all I need is a central location for pushing change sets.  I know that Mercurial doesn't necessarily require a central location, but I think it would be convenient in my workplace.</p>

<p>Thanks!</p>",4,2,2010-09-08 17:02:48.763000 UTC,1.0,,4,mercurial|dvcs|push,2191,2010-07-16 21:41:14.470000 UTC,2015-03-27 20:30:30.367000 UTC,,676,7,1,27,,,,,,[]
User Access Control on Neptune,"<p>I want to know if we can restrict data-level access in the Neptune database?
I am aware that we can manage the access via IAM roles, but I want a mechanism for data-level security, not role-based.</p>",0,0,2021-12-16 15:00:23.803000 UTC,,,0,amazon-neptune,29,2021-09-09 15:06:28.590000 UTC,2022-03-04 09:53:27.580000 UTC,,27,1,0,6,,,,,,[]
"Gremlin Python - ""Server disconnected - please try to reconnect"" error","<p>I have a Flask web app in which I want to keep a persistent connection to an AWS Neptune graph database. This connection is established as follows:</p>
<pre><code>from gremlin_python.process.anonymous_traversal import traversal
from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection

neptune_endpt = 'db-instance-x.xxxxxxxxxx.xx-xxxxx-x.neptune.amazonaws.com'
remoteConn = DriverRemoteConnection(f'wss://{neptune_endpt}:8182/gremlin','g')
self.g = traversal().withRemote(remoteConn)
</code></pre>
<p>The issue I'm facing is that the connection automatically drops off if left idle, and I cannot find a way to detect if the connection has dropped off (so that I can reconnect by using the code snippet above).</p>
<p>I have seen this similar issue: <a href=""https://stackoverflow.com/questions/46386299/gremlin-server-withremote-connection-closed-how-to-reconnect-automatically"">Gremlin server withRemote connection closed - how to reconnect automatically?</a> however that question has no solution as well. This <a href=""https://stackoverflow.com/questions/59191457/python-gremlin-connection-timeout-issue"">similar question</a> has no answer either.</p>
<p>I've tried the following two solutions (both of which did not work):</p>
<ol>
<li>I setup my webapp behind four Gunicorn workers with a timeout of a 100 seconds, hoping that worker restarts would take care of Gremlin timeouts.</li>
<li>I tried catching exceptions to detect if the connection has dropped off. Every time I use <code>self.g</code> to do some traversal on my graph, I try to &quot;refresh&quot; the connection, by which I mean this:</li>
</ol>
<pre><code>def _refresh_neptune(self):
    try:
        self.g = traversal().withRemote(self.conn)
    except:
        self.conn = DriverRemoteConnection(f'wss://{neptune_endpt}:8182/gremlin','g')
        self.g = traversal().withRemote(self.conn)
</code></pre>
<p>Here <code>self.conn</code> was initialized as:</p>
<pre><code>self.conn = DriverRemoteConnection(f'wss://{neptune_endpt}:8182/gremlin','g')
</code></pre>
<p>Is there any way to get around this connection error?</p>
<p>Thanks</p>
<p><strong>Update</strong>: Added the error message below:</p>
<pre><code>  File &quot;/home/ubuntu/.virtualenvs/rundev/lib/python3.6/site-packages/gremlin_python/process/traversal.py
&quot;, line 58, in toList
    return list(iter(self))
  File &quot;/home/ubuntu/.virtualenvs/rundev/lib/python3.6/site-packages/gremlin_python/process/traversal.py
&quot;, line 48, in __next__
    self.traversal_strategies.apply_strategies(self)
  File &quot;/home/ubuntu/.virtualenvs/rundev/lib/python3.6/site-packages/gremlin_python/process/traversal.py
&quot;, line 573, in apply_strategies
    traversal_strategy.apply(traversal)
  File &quot;/home/ubuntu/.virtualenvs/rundev/lib/python3.6/site-packages/gremlin_python/driver/remote_connec
tion.py&quot;, line 149, in apply
    remote_traversal = self.remote_connection.submit(traversal.bytecode)
  File &quot;/home/ubuntu/.virtualenvs/rundev/lib/python3.6/site-packages/gremlin_python/driver/driver_remote
_connection.py&quot;, line 56, in submit
    results = result_set.all().result()
  File &quot;/usr/lib/python3.6/concurrent/futures/_base.py&quot;, line 425, in result
    return self.__get_result()
  File &quot;/usr/lib/python3.6/concurrent/futures/_base.py&quot;, line 384, in __get_result
    raise self._exception
  File &quot;/home/ubuntu/.virtualenvs/rundev/lib/python3.6/site-packages/gremlin_python/driver/resultset.py&quot;
, line 90, in cb
    f.result()
  File &quot;/usr/lib/python3.6/concurrent/futures/_base.py&quot;, line 425, in result
    return self.__get_result()
  File &quot;/usr/lib/python3.6/concurrent/futures/_base.py&quot;, line 384, in __get_result
    raise self._exception
  File &quot;/usr/lib/python3.6/concurrent/futures/thread.py&quot;, line 56, in run
    result = self.fn(*self.args, **self.kwargs)
  File &quot;/home/ubuntu/.virtualenvs/rundev/lib/python3.6/site-packages/gremlin_python/driver/connection.py
&quot;, line 83, in _receive
    status_code = self._protocol.data_received(data, self._results)
  File &quot;/home/ubuntu/.virtualenvs/rundev/lib/python3.6/site-packages/gremlin_python/driver/protocol.py&quot;,
 line 81, in data_received
    'message': 'Server disconnected - please try to reconnect', 'attributes': {}})
gremlin_python.driver.protocol.GremlinServerError: 500: Server disconnected - please try to reconnect

</code></pre>",1,3,2020-08-03 17:52:53.677000 UTC,1.0,2020-08-18 21:29:40.340000 UTC,2,python|gremlin|gremlin-server|amazon-neptune|gremlinpython,1211,2017-05-27 14:56:23.987000 UTC,2022-02-12 02:00:28.813000 UTC,,61,185,0,6,,,,,,[]
Failed to establish a new connection: [Errno 110] Connection timed out,"<p>I am trying to query the <strong>Neptune</strong> data-processing awscurl API from the EC2 instance mentioned in this <a href=""https://docs.aws.amazon.com/neptune/latest/userguide/machine-learning-on-graphs-processing.html"" rel=""nofollow noreferrer"">link</a>. While querying this I am getting the following:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/ec2-user/.local/bin/awscurl&quot;, line 8, in &lt;module&gt;
    sys.exit(main())
  File &quot;/home/ec2-user/.local/lib/python3.7/site-packages/awscurl/awscurl.py&quot;, line 521, in main
    inner_main(sys.argv[1:])
  File &quot;/home/ec2-user/.local/lib/python3.7/site-packages/awscurl/awscurl.py&quot;, line 509, in inner_main
    allow_redirects=args.location)
  File &quot;/home/ec2-user/.local/lib/python3.7/site-packages/awscurl/awscurl.py&quot;, line 142, in make_request
    return __send_request(uri, data.encode('utf-8'), headers, method, verify, allow_redirects)
  File &quot;/home/ec2-user/.local/lib/python3.7/site-packages/awscurl/awscurl.py&quot;, line 348, in __send_request
    response = requests.request(method, uri, headers=headers, data=data, verify=verify, allow_redirects=allow_redirects)
  File &quot;/home/ec2-user/.local/lib/python3.7/site-packages/requests/api.py&quot;, line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File &quot;/home/ec2-user/.local/lib/python3.7/site-packages/requests/sessions.py&quot;, line 542, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;/home/ec2-user/.local/lib/python3.7/site-packages/requests/sessions.py&quot;, line 655, in send
    r = adapter.send(request, **kwargs)
  File &quot;/home/ec2-user/.local/lib/python3.7/site-packages/requests/adapters.py&quot;, line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='neptunedbcluster-abc.cluster-xyz.us-east-2.neptune.amazonaws.com', port=443): Max retries exceeded with url: /ml/dataprocessing (Caused by NewConnectionError('&lt;urllib3.connection.HTTPSConnection object at 0x7f4aa7de2710&gt;: Failed to establish a new connection: [Errno 110] Connection timed out'))
</code></pre>",0,5,2021-12-13 17:26:52.647000 UTC,,,0,amazon-web-services|amazon-ec2|amazon-neptune,365,2018-04-01 13:46:25.433000 UTC,2022-03-04 07:35:35.963000 UTC,India,46,10,0,9,,,,,,[]
Why is a query to a tiny azure databricks database table taking 5 seconds to return?,"<p>I’m new to databricks and spark and I have no intuition yet of what to expect.</p>
<p>I have a small database table of 200 rows with 5 columns in azure databricks’ database.</p>
<p>I have saved it from a data frame in a spark job.</p>
<p>I’m querying it directly via power bi and I can see the query appearing in the spark SQL log and indeed I can see it taking many seconds and 4 jobs are being spun up to return the result.</p>
<p>I want to understand what to think of this.</p>
<p>Is this just a fixed overhead of spark or should I expect milliseconds?</p>
<p>If it isn’t normal, what should I be looking at?</p>
<p>I’m just trying to understand whether I should be looking at pushing this data to another data store and query that or I could pursue my original idea to query databricks directly.</p>",0,0,2021-03-27 10:32:33.787000 UTC,,,0,apache-spark|azure-databricks,23,2009-03-21 18:36:34.060000 UTC,2022-03-04 19:43:50.010000 UTC,,1160,12,1,104,,,,,,[]
Using a commit midway through an interactive rebase when rerere is not on,"<p>So if I am using <code>git rebase -i -p &lt;commit&gt;</code> on a topic branch and there is a merge commit in between somewhere which I am expected to redo by git. But the last time I did that merge commit it was pretty hairy and made quite a few changes to support a big fix in the other branch. I did not have rerere on.</p>

<p>I used a <code>git cherry-pick -n &lt;merge commit&gt; -p &lt;parent number&gt;</code> in the middle of a rebase and it worked. I just had to say <code>git checkout --theirs</code> for everything to resolve the conflicts.</p>

<p>But I was wondering whether we can run rerere midway through an interactive rebase? Also is there a good alternative to use instead of rerere?</p>",0,0,2014-03-19 18:20:53.807000 UTC,0.0,2014-07-26 15:41:54.630000 UTC,1,git|dvcs|git-rerere,46,2010-01-10 03:08:49.883000 UTC,2022-03-04 19:34:51.647000 UTC,,357,20,0,102,,,,,,[]
Multi-processing in Azure Databricks,"<p>I have been tasked lately,  to ingest JSON responses onto Databricks Delta-lake. I have to hit the REST API endpoint URL 6500 times with different parameters and pull the responses.</p>
<p>I have tried two modules,  ThreadPool and Pool from the multiprocessing library, to make each execution a little quicker.</p>
<p><strong>ThreadPool:</strong></p>
<ol>
<li>How to choose the number of threads for ThreadPool, when the Azure Databricks cluster is set to autoscale from 2 to 13 worker nodes?</li>
</ol>
<p>Right now, I've set <strong>n_pool = multiprocessing.cpu_count()</strong>, will it make any difference, if the cluster auto-scales?</p>
<p><strong>Pool</strong></p>
<ol>
<li>When I use Pool to use processors instead of threads. I see the following errors randomly on each execution. Well, I understand from the error that Spark Session/Conf is missing and I need to set it from each process. But I am on Databricks with default spark session enabled, then why do I see these errors.</li>
</ol>
<pre><code>Py4JError: SparkConf does not exist in the JVM 
**OR** 
py4j.protocol.Py4JError: org.apache.spark.api.python.PythonUtils.getEncryptionEnabled does not exist in the JVM
</code></pre>
<ol start=""2"">
<li>Lastly, planning to replace multiprocessing with 'concurrent.futures.ProcessPoolExecutor'. Does it make any difference?</li>
</ol>",2,0,2022-02-12 18:45:34.963000 UTC,,,1,python|azure|azure-databricks,89,2016-10-27 11:09:27.537000 UTC,2022-03-05 17:58:55.937000 UTC,"London, United Kingdom",457,1,0,72,,,,,,[]
Azure Databricks scheduled jobs taking longer to respond,"<p>In Azure DataBricks i have scheduled one job with notebook attached to simple python file.</p>

<pre><code>[![dbutils.widgets.text(""input"", """","""")
dbutils.widgets.get(""input"")
y = getArgument(""input"")
print (""Param -\'input':"")
print (y)][1]][1]
</code></pre>

<p>Cluster: D8s_v3 ( 1 Worker)</p>

<p>even though its quite simple code its take about 9 to 10 second to execute by DataBricks Jobs. If i run python file directly it execute under 1 second.</p>

<p>Please guide me to optimize it for DataBricks Jobs</p>

<p><a href=""https://i.stack.imgur.com/ys4Fm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ys4Fm.png"" alt=""enter image description here""></a>tg.png</p>",0,3,2019-04-29 11:46:58.573000 UTC,,,0,job-scheduling|azure-databricks,237,2011-11-24 10:41:30.180000 UTC,2021-08-12 09:54:45.657000 UTC,"Ahmadabad, India",760,38,6,114,,,,,,[]
Teradata fastload fails when pyspark dataframe has more than one partition,"<p>I am trying to write a spark dataframe into teradata using <strong>FASTLOAD</strong>. The write operation works if I force the dataframe to have only one partition by using <code>df_final = df_final.repartition(1)</code>. But, fails if there are more than one partition. Since the data size is huge if repartitioned(1) is applied on the dataframe it will be overhead on the master node. I even tried to match partitions with # of sessions it didn't work.</p>
<pre><code>

    df_final.write.option(""truncate"",truncate)\
    .mode(mode).option(""batchsize"",100000)\
    .jdbc(url=""jdbc:teradata://host/DBS_PORT=port,LOGMECH=TD2,TMODE=ANSI,CHARSET=UTF16,ENCRYPTDATA=ON,TYPE=FASTLOAD,SESSIONS=2,ERROR_TABLE_DATABASE=errortble"",
    table=""tempdb.temptable"",
    properties=connectionProperties)

</pre></code>
<p>Teradata Version:16.20.53.04 <br>
JDBC Version: 17.00.00.03</p>
<p>Stack Trace:</p>
<pre><code>

2022-01-13 15:58:04.701899: Loading data into tempdb.temptable with write mode as overwrite and truncate as true
An error occurred while calling o1002.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 4 times, most recent failure: Lost task 0.3 in stage 15.0 (TID 31, X.X.X.X, executor 0): java.sql.BatchUpdateException: [Teradata JDBC Driver] [TeraJDBC 17.00.00.03] [Error 1154] [SQLState HY000] A failure occurred while inserting the batch of rows destined for database table ""TempDB"".""temptable"". Details of the failure can be found in the exception chain that is accessible with getNextException.
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeBatchUpdateException(ErrorFactory.java:149)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeBatchUpdateException(ErrorFactory.java:133)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.executeBatch(FastLoadManagerPreparedStatement.java:2389)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:691)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:858)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:856)
    at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1001)
    at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1001)
    at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2379)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)
    at org.apache.spark.scheduler.Task.run(Task.scala:117)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:655)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:658)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 17.00.00.03] [Error 1147] [SQLState HY000] The next failure(s) in the exception chain occurred while beginning FastLoad of database table ""TempDB"".""temptable""
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:95)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:70)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.beginFastLoad(FastLoadManagerPreparedStatement.java:966)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.executeBatch(FastLoadManagerPreparedStatement.java:2210)
    ... 15 more

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)
    at scala.Option.foreach(Option.scala:407)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2339)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2360)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2379)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2404)
    at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1001)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
    at org.apache.spark.rdd.RDD.withScope(RDD.scala:395)
    at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:999)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:856)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:58)
    at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:91)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:200)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$3(SparkPlan.scala:252)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:248)
    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:192)
    at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:158)
    at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:157)
    at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:999)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:116)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:249)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:101)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)
    at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:199)
    at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:999)
    at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:437)
    at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:421)
    at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:827)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
    at py4j.Gateway.invoke(Gateway.java:295)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:251)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.BatchUpdateException: [Teradata JDBC Driver] [TeraJDBC 17.00.00.03] [Error 1154] [SQLState HY000] A failure occurred while inserting the batch of rows destined for database table ""TempDB"".""temptable"". Details of the failure can be found in the exception chain that is accessible with getNextException.
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeBatchUpdateException(ErrorFactory.java:149)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeBatchUpdateException(ErrorFactory.java:133)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.executeBatch(FastLoadManagerPreparedStatement.java:2389)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:691)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:858)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:856)
    at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1001)
    at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1001)
    at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2379)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)
    at org.apache.spark.scheduler.Task.run(Task.scala:117)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:655)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:658)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    ... 1 more
Caused by: java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 17.00.00.03] [Error 1147] [SQLState HY000] The next failure(s) in the exception chain occurred while beginning FastLoad of database table ""TempDB"".""temptable""
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:95)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:70)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.beginFastLoad(FastLoadManagerPreparedStatement.java:966)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.executeBatch(FastLoadManagerPreparedStatement.java:2210)
    ... 15 more
</pre></code>",1,2,2022-01-13 19:28:59.427000 UTC,,2022-01-13 20:29:26.773000 UTC,0,jdbc|pyspark|teradata|azure-databricks,74,2020-01-06 14:20:52.863000 UTC,2022-01-26 01:36:11.710000 UTC,,9,0,0,3,,,,,,[]
pyspark data in wrong column,"<p>I have a dataframe that I need to write to a datalake. In my notebook the dataframe stucture en content looks ok, but when I look in the datalake the dat is in the wrong columns</p>
<p>dataframe:
<a href=""https://i.stack.imgur.com/guvk0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/guvk0.png"" alt=""dataframe"" /></a></p>
<p>datalake (via azure synapse):
<a href=""https://i.stack.imgur.com/YQ7H5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YQ7H5.png"" alt=""datalake"" /></a></p>
<p>The data is added with a function, but when I execute with the code that is used by the function it's the same story</p>
<pre><code>path_integration = &quot;/mnt/integratie/CL1/xxx&quot;

# Overwrite delta and schema using the dataframe
df.write \
    .mode(&quot;overwrite&quot;) \
    .format(&quot;delta&quot;) \
    .option(&quot;overwriteSchema&quot;, &quot;true&quot;) \
    .save(path_integration)
</code></pre>
<p>replacing the option overwriteSchema with inferSchema does not help.</p>
<p>In other notebooks everything just works fine</p>",0,0,2021-10-28 11:00:11.183000 UTC,,,0,apache-spark-sql|schema|azure-databricks,24,2016-07-29 12:16:09.277000 UTC,2022-02-01 09:53:13.433000 UTC,,243,0,0,27,,,,,,[]
How to use Azure DevOps artifacts repository as source for DatabricksStep of AzureML?,"<p>If we have PyPi Packages added as Artifacts to an Azure DevOps Project Feed, how can we use these packages as a source for installing packages in <code>DatabricksStep</code> of Azure Machine Learning Service?</p>
<p>While using <code>pip</code> in any environment, we use our Azure DevOps Project Artifacts feed in the following way:</p>
<pre><code>pip install example-package --index-url=https://&lt;Personal-Access-Token&gt;@pkgs.dev.azure.com/&lt;Organization-Name&gt;/_packaging/&lt;Artifacts-Feed-Name&gt;/pypi/simple/
</code></pre>
<p>The DatabricksStep class of the Azure Machine Learning Service accepts the following parameters:</p>
<pre><code>python_script_name = &quot;&lt;Some-Script&gt;.py&quot;
source_directory = &quot;&lt;Path-To-Script&gt;&quot;

&lt;Some-Placeholder-Name-for-the-step&gt; = DatabricksStep(
    name=&lt;Some-Placeholder-Name-for-the-step&gt;,
    num_workers=1,
    python_script_name=python_script_name,
    source_directory=source_directory,
    run_name= &lt;Name-of-the-run&gt;,
    compute_target=databricks_compute,
    pypi_libraries = [
                      PyPiLibrary(package = 'scikit-learn'), 
                      PyPiLibrary(package = 'scipy'), 
                      PyPiLibrary(package = 'azureml-sdk'), 
                      PyPiLibrary(package = 'joblib'), 
                      PyPiLibrary(package = 'azureml-dataprep[pandas]'),
                      PyPiLibrary(package = 'example-package', repo='https://&lt;Personal-Access-Token&gt;@pkgs.dev.azure.com/&lt;Organization-Name&gt;/_packaging/&lt;Artifacts-Feed-Name&gt;/pypi/simple/')
                    ], 

    allow_reuse=True
)
</code></pre>
<p>However, <code>PyPiLibrary(package = 'example-package', repo='https://&lt;Personal-Access-Token&gt;@pkgs.dev.azure.com/&lt;Organization-Name&gt;/_packaging/&lt;Artifacts-Feed-Name&gt;/pypi/simple/')</code> will give an error. How exactly should we consume the Artifacts Feed as an input to the <code>PyPiLibrary</code> property of the <code>DatabricksStep</code> Class in Azure Machine Learning Service?</p>",0,0,2021-01-21 13:13:54.813000 UTC,,,4,azure|azure-devops|azure-databricks|azure-machine-learning-service|azure-artifacts,272,2020-10-03 12:46:02.437000 UTC,2022-02-28 17:58:49.850000 UTC,"Hyderabad, Telangana, India",704,173,32,103,,,,,,[]
Can we pass Databricks output to function in an ADF Job?,"<p>Can anyone help me with Databricks and Azure function?
I'm trying to pass data bricks JSON output to azure function body in ADF job, is it possible? 
If yes, How? 
If No, what other alternatives to do the same?</p>",2,0,2019-04-17 11:54:34.587000 UTC,2.0,2019-04-17 13:25:49.437000 UTC,4,python|json|azure-functions|azure-data-factory-2|azure-databricks,4556,2018-08-01 14:58:33.147000 UTC,2019-09-20 09:43:02.913000 UTC,,111,0,0,11,,,,,,[]
azure devops for ADF and databricks,"<p>I am trying to create an azure ci/cd pipeline for my azure data factory in which I have used databricks notebook. Pipeline got created successfully with the ARM template for ADF but I am not able to see any override parameter for databricks workspace URL, that's why i got the same databricks URL in my dev and prod environment.</p>
<p>Can anyone help me to set databricks workspace URL for Dev and prod dynamically?</p>",2,7,2021-03-28 06:49:51.127000 UTC,,2021-03-28 09:03:20.703000 UTC,2,azure-devops|azure-data-factory|azure-databricks,185,2017-07-11 05:28:19.920000 UTC,2022-03-02 13:02:53.157000 UTC,"Ahmedabad, Gujarat, India",63,4,0,20,,,,,,[]
Write data using JDBC connection to Azure SQL DB with Scala code Databricks notebook,"<p>I am trying to insert data from hive table to Azure SQL DB table. The SQL DB table already exists and I just want to overwrite data into it with following Scala JDBC write code. This code is writing data to SQL DB table, however it is changing its DDL (datatypes/column names). How can I avoid it. I want simple insert on table.</p>

<p><a href=""https://i.stack.imgur.com/LOP0K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LOP0K.png"" alt=""enter image description here""></a></p>",1,1,2019-04-29 11:01:26.820000 UTC,,,0,scala|apache-spark|hive|azure-sql-database|azure-databricks,1147,2013-09-26 12:12:36.613000 UTC,2022-02-28 14:44:50.623000 UTC,"Pune, Maharashtra, India",2237,77,8,475,,,,,,[]
Spark Streaming Set of Data,"<p>I am relatively new to Databricks and am  trying to and read incoming sensor data and fire a ruleset based in each set of data. Looking for some help and guidance on how to proceed further</p>
<pre><code>val connectionString = ConnectionStringBuilder(&quot;ConnectionString&quot;)   
    .setEventHubName(&quot;EventHubname&quot;)
    .build

  val eventHubsConf = EventHubsConf(connectionString)
    .setStartingPosition(EventPosition.fromEndOfStream)
    .setConsumerGroup(&quot;azuredatabricks&quot;)

  val eventhubs = spark.readStream
    .format(&quot;eventhubs&quot;)
    .options(eventHubsConf.toMap)
    .load() 

 /*
  val df = eventhubs.select(($&quot;enqueuedTime&quot;).as(&quot;Enqueued_Time&quot;),($&quot;systemProperties.iothub- 
           connection-device-id&quot;) .as(&quot;Device_ID&quot;),($&quot;body&quot;.cast(&quot;string&quot;)).as(&quot;telemetry_json&quot;)) 

  df.createOrReplaceTempView(&quot;tel_table&quot;)
*/
</code></pre>
<p>The code above reads incoming streaming data from EventHub and loads the data in dataframe as well ( commented section of code). Now I need  to loop through each row of data and pass it to ruleset. Since its a streaming data,spark sql throws error. Could somebody guide on how these streaming data can be read sequentially.</p>
<p>The next part on which I was looking for help is on how to infer schema from body of messages. Since these are sensor data so the schema won't be static. One message can have DeviceId,Temperature,pH and other would be Device,Voltage,Current. For cases like these is there a way to infer the schema dynamically in Apache Spark.</p>
<p>Thanks a lot for help in advance.</p>",0,3,2020-09-19 22:10:11.063000 UTC,,,1,apache-spark|spark-streaming|azure-databricks,43,2014-01-28 20:06:13.987000 UTC,2021-12-25 19:30:47.583000 UTC,India,120,0,0,40,,,,,,[]
Data Explorer: ImportError No module named Kqlmagic,"<p>I'm following this tutorial:
<a href=""https://docs.microsoft.com/en-us/azure/data-explorer/kqlmagic"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/data-explorer/kqlmagic</a></p>

<p>I have a Databricks cluster so I decided to use the notebook that is available on there.</p>

<p>When I get to step 2 and run:</p>

<pre><code>reload_ext Kqlmagic
</code></pre>

<p>I get the error message:</p>

<pre><code>ImportError: No module named Kqlmagic
</code></pre>",2,0,2018-12-24 15:10:41.777000 UTC,0.0,,1,azure-databricks|azure-data-explorer|azure-notebooks,237,2012-10-20 16:18:44.083000 UTC,2022-03-03 10:41:10.747000 UTC,,5331,89,4,402,,,,,,[]
Getting Error in Databricks :Could not initialize class reactor.netty.http.client.HttpClientConfiguration,"<pre><code>dependencies {
  compile('org.apache.hadoop:hadoop-azure:3.2.1'){
        exclude group: &quot;com.google.guava&quot;, module : &quot;guava&quot;
    }
    
    // Azure storage dependencies
    compile group: 'com.azure', name: 'azure-storage-blob', version: '12.7.0'
    
    // HBase
    compile group: 'org.apache.hbase', name: 'hbase-client', version: '1.6.0'
    
    compile group: 'io.projectreactor', name: 'reactor-core', version: '3.3.5.RELEASE' , force: true
    compile group: 'io.projectreactor.netty', name: 'reactor-netty', version: '0.9.7.RELEASE', force: true
    
    
    compile group: 'io.netty', name: 'netty-transport', version: '4.1.49.Final', force: true
       compile (group: 'io.netty', name: 'netty-codec-http', version: '4.1.49.Final', force: true){
            exclude group: 'io.netty', module: 'netty-codec'
        }
        compile group: 'io.netty', name: 'netty-common', version: '4.1.49.Final', force: true
        compile group: 'io.netty', name: 'netty-handler', version: '4.1.49.Final', force: true
        compile group: 'io.netty', name: 'netty-transport-native-epoll', version: '4.1.49.Final', force: true
        compile group: 'io.netty', name: 'netty-resolver', version: '4.1.49.Final', force: true
        compile group: 'io.netty', name: 'netty-buffer', version: '4.1.49.Final', force: true
        compile group: 'io.netty', name: 'netty-transport-native-unix-common', version: '4.1.49.Final', force: true
        compile group: 'io.netty', name: 'netty-codec', version: '4.1.49.Final', force: true
       compile group: 'io.netty', name: 'netty-all', version: '4.1.49.Final', force: true
        
    
}
</code></pre>
<p>This is spark cluster dependency . I have removed netty versions. But still in databricks it fails. I checked the jar also, it contains handler.</p>
<pre><code>dependencies {
    // Spark dependency.
    compile( group: 'org.apache.spark', name: 'spark-core_2.11', version: '2.4.5')
    {
        exclude group: &quot;io.netty&quot;, module : &quot;netty&quot;
        exclude group: &quot;io.netty&quot;, module : &quot;netty-all&quot;
    }
   

    // Spark for SQL and parquet file.
    compile group: 'org.apache.spark', name: 'spark-sql_2.11', version: '2.4.5'

    compile group: 'com.esotericsoftware', name: 'kryo', version: '4.0.2'

    compile 'org.apache.commons:commons-math3:3.6.1'

    compile group: 'org.apache.commons', name: 'commons-text', version: '1.8'

    compile group: 'org.codehaus.janino', name: 'janino', version: '3.1.2'

    // Gson
    compile group: 'com.google.code.gson', name: 'gson', version: '2.8.6'

    // Java tuple for Pair.
    compile group: 'org.javatuples', name: 'javatuples', version: '1.2'

    // Lombok dependency
    compileOnly 'org.projectlombok:lombok:1.18.12'

    // Use JUnit test framework
    testImplementation 'junit:junit:4.12'

}
</code></pre>
<p>Getting issue of could not initialise. Please let me know what I am missing . I can see in dependency tree that new version of netty handler is getting used</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>20/09/13 18:57:43 ERROR Schedulers: Scheduler worker in group main failed with an uncaught exception
java.lang.NoSuchMethodError: io.netty.handler.ssl.SslProvider.isAlpnSupported(Lio/netty/handler/ssl/SslProvider;)Z
    at reactor.netty.http.client.HttpClientSecure.&lt;clinit&gt;(HttpClientSecure.java:79)
    at reactor.netty.http.client.HttpClientConnect$MonoHttpConnect.lambda$subscribe$0(HttpClientConnect.java:301)
    at reactor.core.publisher.MonoCreate.subscribe(MonoCreate.java:57)
    at reactor.core.publisher.FluxRetryPredicate$RetryPredicateSubscriber.resubscribe(FluxRetryPredicate.java:124)
    at reactor.core.publisher.MonoRetryPredicate.subscribeOrReturn(MonoRetryPredicate.java:51)
    at reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:57)
    at reactor.netty.http.client.HttpClientConnect$MonoHttpConnect.subscribe(HttpClientConnect.java:326)
    at reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:64)
    at reactor.core.publisher.MonoDefer.subscribe(MonoDefer.java:52)
    at reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:64)
    at reactor.core.publisher.MonoDelaySubscription.accept(MonoDelaySubscription.java:52)
    at reactor.core.publisher.MonoDelaySubscription.accept(MonoDelaySubscription.java:33)
    at reactor.core.publisher.FluxDelaySubscription$DelaySubscriptionOtherSubscriber.onNext(FluxDelaySubscription.java:123)
    at reactor.core.publisher.MonoDelay$MonoDelayRunnable.run(MonoDelay.java:117)
    at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:68)
    at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:28)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)</code></pre>
</div>
</div>
</p>",0,6,2020-09-09 19:39:05.760000 UTC,,2020-09-13 19:04:22.873000 UTC,1,azure|apache-spark|azure-databricks,390,2015-10-19 07:22:36.693000 UTC,2022-03-04 18:50:31.090000 UTC,,89,9,0,42,,,,,,[]
Attaching IAM role to Neptune cluster using cloudformation,"<p>I want to attach the role I am creating in CFN script to the neptune DB cluster. Following is my code where I am creating a cluster and a NeptuneRole.</p>
<p>I want to attach the created role to the above cluster. How can I do that?
Also, how do I create a cluster snapshot via CFN?</p>
<pre><code>Resources:
  # Creating Neptune DB Cluster
  NeptuneDBCluster:
    Type: AWS::Neptune::DBCluster
    Properties:
      DBClusterIdentifier:   
        !Join
          - ''
          - - !Ref enviroment
            - '-neptune-graphcluster-'
            - !FindInMap [RegionShortname, !Ref &quot;AWS::Region&quot;, RN]
            
      DBClusterParameterGroupName: !Ref NeptuneDBClusterParameterGroup
      DBSubnetGroupName: !Ref NeptuneDBSubnetGroup
      BackupRetentionPeriod: 1
      VpcSecurityGroupIds: !Ref SecurityGroupIds
      IamAuthEnabled: false
      Preferre dBackupWindow: &quot;07:14-07:44&quot;
      

 
  # Creating Netpune IAM Role to get NeptuneFullAccess
  NeptuneRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName:  
        !Join
            - ''
            - - !Ref enviroment
              - '-neptune-role-'
              - !FindInMap [RegionShortname, !Ref &quot;AWS::Region&quot;, RN]
              
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - ec2.amazonaws.com
            - rds.amazonaws.com
          Action: 'sts:AssumeRole'
      Path: /
      ManagedPolicyArns: arn:aws:iam::aws:policy/NeptuneFullAccess

    
</code></pre>",0,2,2019-10-07 08:16:04.933000 UTC,,2020-10-08 16:42:11.353000 UTC,2,amazon-web-services|cluster-computing|amazon-cloudformation|amazon-neptune,269,2017-12-02 13:27:23.057000 UTC,2022-03-03 10:56:35.243000 UTC,,65,2,0,16,,,,,,[]
Logging the errors to azure blob,"<p>I am trying to log errors to azure blob but, its not creating any table in the blob. I have gone through many docs and also searched for ans in stackoverflow as well. Please help me with this.
Thanks </p>

<p>below is the code</p>

<p>def log():</p>

<pre><code>import logging
import sys
from azure_storage_logging.handlers import BlobStorageRotatingFileHandler

mystorageaccountname='***'
mystorageaccountkey='***'

_LOGFILE_TMPDIR = mkdtemp()

logger = logging.getLogger('service_logger')
logger.setLevel(logging.DEBUG)
log_formater = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(process)d - %(message)s')
azure_blob_handler = TableStorageHandler(account_name=mystorageaccountname,
                                         account_key=mystorageaccountkey,
                                         protocol='https',
                                         table='logtable',
                                         batchsize=100,
                                         extra_properties=None, 
                                         partition_key_formatter=None, 
                                         row_key_formatter=None, 
                                         is_emulated=False)
logger.addHandler(azure_blob_handler)

logger.warning('warning message')
</code></pre>",1,2,2020-05-05 07:02:02.887000 UTC,,,1,azure-storage|azure-blob-storage|azure-databricks|python-logging,744,2020-05-05 06:54:09.247000 UTC,2021-03-17 12:30:10.670000 UTC,,11,0,0,6,,,,,,[]
AWS neptune throwing timeout error when th e query timeout value is higher,"<p>I am running a gremlin query on the AWS neptune which use to take 2.5 minutes for the results,
I have kept my <code> neptune_query_timeout = 500000</code> , the neptune engine version is 1.0.4.1</p>
<p>Recently I am seeing this error on my same query which just use to work fine earlier-</p>
<pre><code>{'error': TimeoutError('Operation timed out after 30 seconds',)}
</code></pre>
<p>It doesn't look to me that actual query on neptune is timing out, i have kept a long timeout of 500000 in the config file already and it use to work fine before.</p>
<p>Recently i am seeing the above error and have no idea how to overcome this.</p>
<hr />
<p>EDITED</p>
<p>The graph looks like</p>
<p>Users (node) ---- played(edge)-----&gt; games(node)</p>
<p>So the actual query i am trying to run is this-</p>
<pre><code>g.V().hasLabel('users').where(outE('played').count().is(gt(10)))
</code></pre>
<p>which runs fine and gives me all the users who played more than 10 times or in other words had 10 or more &quot;played&quot; edges from the users node.</p>
<p>but when i just want the count of users and modified the query as below I am hitting the timeout error.</p>
<pre><code>g.V().hasLabel('transient_id').where(outE('visited').count().is(gt(10))).count().next()
</code></pre>
<p>Any help is appreciated, Thanks</p>",1,3,2020-12-18 19:10:28.870000 UTC,,2020-12-18 19:37:45.813000 UTC,0,amazon-web-services|gremlin|amazon-neptune|gremlinpython,526,2016-06-22 22:32:02.920000 UTC,2022-02-08 23:53:57.950000 UTC,,45,3,0,34,,,,,,[]
How to setup a starting point for the batchId of foreachBatch?,"<p>The problem that I am facing is that my process relies on the batchId of the foreachBatch as some sort of control of what is ready to the second stage of the pipeline. So it wil only go to the second stage if the first stage (batch) is completed.</p>

<p>I want to guarantee that in case of something goes wrong, the stream can continue from where it stopped.</p>

<p>We tried to do some control by adding all completed batchs to a delta table, however, I couldn't find a way to set the initial batchId. </p>",2,0,2019-11-20 13:39:28.607000 UTC,,2019-11-28 08:30:45.213000 UTC,0,apache-spark|pyspark|spark-structured-streaming|azure-databricks,463,2015-09-15 08:29:24.193000 UTC,2021-08-26 20:22:38.807000 UTC,"São Paulo, State of São Paulo, Brazil",328,306,3,79,,,,,,[]
Function history with Mercurial,"<p>I'd like to be able to get the complete history of a function or a particular text block inside my code. </p>

<p>I know i can have the diffs of all my commits on a particular file, but I only want to follow the life of a particular small block of text inside my files (a C++ function for instance).</p>

<p>I want to see it change though past revisions, no matter if is moved inside the file or to another file or even renamed (the rest of the function remaining more or less the same when renaming)</p>

<p>I heard Mercurial could do this easily thanks to its proper recording of history, but I don't remember where I heard that (in my dreams ?) and I can't find any tool or way to do that except the traditional history and diff tools. Maybe I don't search with the right keywords... Anyone can help ?</p>

<p>Thanks</p>

<p>PS: I still use SVN for other projects, and if someone knows a way to accomplish the same thing with SVN, I take it too :-)</p>",2,0,2011-07-18 22:39:04.433000 UTC,,,8,svn|function|mercurial|dvcs,244,2010-05-11 21:44:18.147000 UTC,2022-01-02 17:32:06.667000 UTC,,101,8,0,2,,,,,,[]
Azure Databricks processing files differently based on the confuguration,"<p>We've an application which processes huge file(excel) and calculate data from that file based on different conditions(written/coded in scala notebook).</p>
<p>The issue which we're facing is the inconsistency of results produced by the same file for different time and/or different configuration for Azure Databricks Compute.</p>
<p>We've already double check our scala notebook code and which doesn't has any bug, it might be something from configuration end(not sure).</p>
<p>Below is the current configuration of my dev compute
<a href=""https://i.stack.imgur.com/K7gzO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K7gzO.png"" alt=""enter image description here"" /></a></p>",0,2,2022-01-05 05:42:10.640000 UTC,,,0,azure-databricks,18,2019-01-29 09:12:34.020000 UTC,2022-02-25 09:41:27.740000 UTC,,177,17,0,48,,,,,,[]
Spark structured streaming window when no stream,"<p>I want to log number of records read to database from incoming stream of spark structured streaming. I'm using foreachbatch to transform incoming stream batch and write to desired location. I want to log 0 records read if there are no records in a particular hour. But foreach batch does not execute when there is no stream. Can anyone help me with it? My code is as below:</p>
<p><code>val incomingStream = spark.readStream.format(&quot;eventhubs&quot;).options(customEventhubParameters.toMap).load()</code></p>
<pre><code>val query=incomingStream.writeStream.foreachBatch{
  (batchDF: DataFrame, batchId: Long)=&gt; writeStreamToDataLake(batchDF,batchId,partitionColumn,fileLocation,errorFilePath,eventHubName,configMeta)
}
              .option(&quot;checkpointLocation&quot;,fileLocation+checkpointFolder+&quot;/&quot;+eventHubName)
              .trigger(Trigger.ProcessingTime(triggerTime.toLong))
              .start().awaitTermination()
</code></pre>",1,8,2020-06-22 11:53:34.030000 UTC,0.0,,1,apache-spark|spark-streaming|spark-structured-streaming|azure-databricks,174,2016-10-05 18:41:49.827000 UTC,2021-08-16 12:45:48.950000 UTC,"Pune, Maharashtra, India",45,5,0,24,,,,,,[]
How to push only a subset of committed files?,"<p>We recently switched from svn to mercurial. We're using Aptana as our IDE with the MercurialEclipse plugin, along with BitBucket for our repositories and SourceTree as our (additional) source control GUI.</p>

<p>I created 2 new files in Aptana, and committed each of them. Now in the Synchronize view, where the 2 files are listed as ""outgoing"", I'd like to push <strong>only one of them</strong>. I avoided using the ""push all"" icon at the top which would push all outgoing changes - instead I right-clicked a specific file in the outgoing list and chose ""push"" from the context menu. However, this caused <strong>both</strong> outgoing changes to be pushed. I can't seem to find any option to push only a specific file or subset of files of the committed changes. Is there any way to accomplish this in Aptana?</p>",1,0,2014-02-26 10:10:12.687000 UTC,2.0,2014-02-26 21:16:34.357000 UTC,2,eclipse|mercurial|push|dvcs,343,2010-01-26 20:14:02.257000 UTC,2022-01-21 15:40:36.623000 UTC,,74946,1267,10,2096,,,,,,[]
"AWS Neptune performance issue, how query was resolved?","<p>I have a performance issue in gremlin when I try to start multiple query in parallel (in a java application). I did a simple test with a <code>g.V().count()</code>.</p>
<p>My cluster has one instance (db.r5.xlarge) with 8 vCPUs. The query alone take 11 seconds the be executed the first time, and the second time 5 seconds (I think the difference is due to the caching mechanism). After that, each query take 5 seconds one by one. But when I execute them in parallel the execution increase until 8 query in parallel (19 seconds by query). After 8 query, they are set in the Neptune queue and they will still took 19 seconds to be executed.</p>
<p>My question is: why the execution time increase ?</p>
<p>When only one query is executed Neptune will use more than one vCPU to execute it ? All metrics seems good, buffer hit ratio in never under 99.9%, the freeable memory increase but not so much.</p>",0,1,2021-12-09 08:59:04.580000 UTC,,2021-12-09 10:27:16.960000 UTC,0,performance|gremlin|graph-databases|tinkerpop|amazon-neptune,82,2015-08-05 10:57:40.773000 UTC,2022-03-04 11:30:43.810000 UTC,,402,18,0,38,,,,,,[]
Spark fails to kill tasks that read/write to Azure SQL-DB,"<p>I am running an ETL-Pipeline on Azure-Databricks that writes cleaned data into an Azure-SQL-DB. 
During runtime, some tasks get killed because they get <em>'preempted by scheduler'</em> (see executor-log below). My understanding is that this happens because the fair-scheduling mode of spark tries to free up resources for other tasks that should take precedence. Whenever the task to be killed happens to be a read or write from an Azure-SQL-DB, Spark fails to kill the task, ultimately killing the whole executor.</p>

<p>This pattern repeats on all of the workers/executors, with every dead executor having pretty much the same log-entries as the one below.</p>

<p>Other tasks are still killed successfully throughout the pipeline-run. It only seems to happen to tasks that build a connection to the SQL-DB.</p>

<p><strong>Databricks Cluster-Details</strong>:</p>

<ul>
<li>28 GB Memory, 8 Cores x 2 (Worker)</li>
<li>14 GB Memory, 4 Cores x 1 (Driver)</li>
<li>Runtime 5.5 LTS (Spark 2.4.3)</li>
</ul>

<p><strong>Additional Spark-Config</strong> (through Databricks' spark-config window):</p>

<ul>
<li>yarn.nodemanager.resource.cpu-vcores 7</li>
<li>spark.executor.cores 3</li>
<li>spark.executor.memory 10g</li>
<li>spark.executor.instances 3</li>
<li>yarn.nodemanager.resource.memory-mb 26329</li>
<li>spark.serializer org.apache.spark.serializer.KryoSerializer</li>
</ul>

<p>resulting in 4 executors with 3 Cores, 5.5 GB Storage Memory + driver (<em>spark.executor.instances</em> seems to get ignored)</p>

<p><strong>Code-Snippet</strong>:</p>

<p>The problem arises for tasks executing the following code-snippet.</p>

<pre><code>current_csv.repartition(5).write.jdbc(mode = 'overwrite', url=jdbc_url, table= sql_tabl, properties=connection_properties)
</code></pre>

<p>However this problem can also be reproduced on bigger/smaller clusters, when reading data and also forced to happen when the task or execution of the code is cancelled manually (either through the SparkUI or the cancel-button of the Databricks Notebook cell). This applies to pretty much all queries that are not done/executed more or less instantly. </p>

<p><strong>Executor-Log</strong>:</p>

<pre><code>19/09/10 03:03:50 INFO Executor: Executor is trying to kill task 3.0 in stage 484.0 (TID 16969), reason: preempted by scheduler
19/09/10 03:04:00 WARN Executor: Killed task 16969 is still running after 10002 ms
19/09/10 03:04:00 WARN Executor: Thread dump from task 16969:
...
19/09/10 03:04:10 WARN Executor: Killed task 16969 is still running after 20011 ms
19/09/10 03:04:10 WARN Executor: Thread dump from task 16969:
...
19/09/10 03:04:20 WARN Executor: Killed task 16969 is still running after 30013 ms
19/09/10 03:04:20 WARN Executor: Thread dump from task 16969:
...
19/09/10 03:04:30 WARN Executor: Killed task 16969 is still running after 40014 ms
19/09/10 03:04:30 WARN Executor: Thread dump from task 16969:
...
19/09/10 03:04:40 WARN Executor: Killed task 16969 is still running after 50016 ms
19/09/10 03:04:40 WARN Executor: Thread dump from task 16969:
...
19/09/10 03:04:50 WARN Executor: Killed task 16969 is still running after 60017 ms
19/09/10 03:04:50 WARN Executor: Thread dump from task 16969:
...
19/09/10 03:04:50 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Task reaper-0,5,main]
java.lang.Error: org.apache.spark.SparkException: Killing executor JVM because killed task 16969 could not be stopped within 60000 ms.
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
</code></pre>

<p><strong>Last Thread-Dump</strong>:</p>

<pre><code>java.net.SocketInputStream.socketRead0(Native Method)
java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
java.net.SocketInputStream.read(SocketInputStream.java:171)
java.net.SocketInputStream.read(SocketInputStream.java:141)
com.microsoft.sqlserver.jdbc.TDSChannel$ProxyInputStream.readInternal(IOBuffer.java:1003)
com.microsoft.sqlserver.jdbc.TDSChannel$ProxyInputStream.read(IOBuffer.java:991)
sun.security.ssl.InputRecord.readFully(InputRecord.java:465)
sun.security.ssl.InputRecord.read(InputRecord.java:503)
sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975)
sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933)
sun.security.ssl.AppInputStream.read(AppInputStream.java:105)
com.microsoft.sqlserver.jdbc.TDSChannel.read(IOBuffer.java:1981)
com.microsoft.sqlserver.jdbc.TDSReader.readPacket(IOBuffer.java:6310)
com.microsoft.sqlserver.jdbc.TDSReader.nextPacket(IOBuffer.java:6269)
com.microsoft.sqlserver.jdbc.TDSReader.ensurePayload(IOBuffer.java:6247)
com.microsoft.sqlserver.jdbc.TDSReader.readBytes(IOBuffer.java:6531)
com.microsoft.sqlserver.jdbc.TDSReader.readWrappedBytes(IOBuffer.java:6552)
com.microsoft.sqlserver.jdbc.TDSReader.readInt(IOBuffer.java:6499)
com.microsoft.sqlserver.jdbc.StreamRetStatus.setFromTDS(StreamRetStatus.java:30)
com.microsoft.sqlserver.jdbc.SQLServerStatement$1NextResult.onRetStatus(SQLServerStatement.java:1458)
com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:65)
com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1531)
com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatementBatch(SQLServerPreparedStatement.java:2482)
com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtBatchExecCmd.doExecute(SQLServerPreparedStatement.java:2383)
com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7151)
com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:2478)
com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:219)
com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:199)
com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2294)
org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:672)
org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:839)
org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:839)
org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:951)
org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:951)
org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2284)
org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2284)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
org.apache.spark.scheduler.Task.doRunTask(Task.scala:139)
org.apache.spark.scheduler.Task.run(Task.scala:112)
org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:497)
org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1526)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:503)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
java.lang.Thread.run(Thread.java:748)
</code></pre>

<p>I can circumvent this whole issue by simply disabling preemtion or increase the cluster to the point were preemtion and therefore killing tasks is no longer necessary, but that seems to be a rather heavyhanded solution.</p>

<p><strong>Since I could not find anything regarding this isse:</strong></p>

<ul>
<li>Is there an obvious reason Spark seems to be unable to kill tasks building a connection to a SQL-DB?</li>
<li>Since I am not familiar with Java/Scala and ThreadDumps, is there any additional info in these Dumps that could give me a hint as to what is going on?</li>
</ul>",0,0,2019-09-11 13:01:32.047000 UTC,1.0,,2,sql-server|apache-spark|pyspark|azure-sql-database|azure-databricks,364,2019-09-11 11:15:02.727000 UTC,2020-02-13 11:19:17.823000 UTC,,21,0,0,4,,,,,,[]
How to convert string to time datatype in pyspark or scala?,"<p>Please note that I am not asking for <code>unix_timestamp</code> or <code>timestamp</code> or <code>datetime</code> data type I am asking for <code>time</code> data type, is it possible in pyspark or scala?</p>

<p>Lets get in details,
I have a dataframe like this with column <code>Time</code> string type</p>

<pre><code>+--------+
|    Time|
+--------+
|10:41:35|
|12:41:35|
|01:41:35|
|13:00:35|
+--------+
</code></pre>

<p>I want to convert it in <code>time</code> data type because in my SQL database this column is <code>time</code> data type, so I am trying to insert my data with spark connector applying <code>Bulk Copy</code>
So for bulk copy my both data-frame and DB table schema must be same, that's why I need to convert my <code>Time</code>column into <code>time</code> data type.</p>

<p>Appreciate Any suggestion or help. Thanks in advance.</p>",2,0,2020-01-07 04:16:53.960000 UTC,,2020-01-07 05:34:37.027000 UTC,0,scala|pyspark|type-conversion|azure-databricks|pyspark-dataframes,1773,2019-05-17 08:07:04.323000 UTC,2022-03-01 08:11:37.760000 UTC,"Tokyo, Japan",235,44,0,86,,,,,,[]
Can I connect AWS Neptune to Tableau Desktop?,"<p>Just a simple question, can I connect AWS Neptune to Tableau Desktop?</p>

<p>If yes, then how?</p>

<p>And also, can I write query in SPARQL using new Custom SQL?</p>

<p>Thanks.</p>",1,0,2019-10-28 15:00:58.863000 UTC,,2020-12-07 14:34:47.843000 UTC,0,amazon-web-services|tableau-api|amazon-neptune,197,2019-09-17 15:44:29.730000 UTC,2021-08-25 17:03:19.987000 UTC,,57,11,0,16,,,,,,[]
terraform maven coordinates azure artifacts,"<p>I am trying to get Azure Artifacts (maven library install on a Databricks cluster). I am trying to follow the terraform <a href=""https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/cluster"" rel=""nofollow noreferrer"">documentation</a> but I struggle to get right coordinates. Do you know what is the correct URL for the Azure Artifact ?</p>
<pre><code>library {
  maven {
    coordinates = &quot;com.amazon.deequ:deequ:1.0.4&quot;
  }
}
</code></pre>",1,0,2021-02-11 15:52:55.217000 UTC,,2021-12-19 12:01:31.000000 UTC,0,azure|maven|coordinates|artifacts|terraform-provider-databricks,73,2015-01-22 19:39:54.813000 UTC,2022-03-04 16:31:11.630000 UTC,,89,0,0,24,,,,,,[]
How to call a method in other class/object in Scala using azure Data bricks Notebooks,"<p>I have below similar scenario in local Scala IDE. I was trying to replicate same in Azure Databricks Notebook. I was not able to do it.</p>
<p>Scenario Explained: I have one main class. I am calling methods in other object in to main class
using Threading to execute parallelly.</p>
<p>I have tried by putting each object in different notebook and tried in data bricks. But able to do.</p>
<p>Main Class</p>
<pre><code>object cli_main {
    
   def main(args: Array[String]): Unit = {
    
    var ModuleOneStart = &quot;Y&quot;
    var ModuleTwoStart = &quot;Y&quot;
    
    var ResultOne=&quot;0&quot;
    var ResultTwo=&quot;0&quot;
    
    var a=100
    var b=200
        
    val ThreadOne = new Thread {
        override def run(): Unit = {
            ResultOne = cli_cpm.cpm_process(a, b)
          }
        }
    val ThreadTwo = new Thread {
         override def run(): Unit = {
             ResultTwo = cli_res.res_process(a, b)
            }
        }
                
    if (ModuleOneStart == &quot;Y&quot;) { 
          ThreadOne.start()
        }
    if (ModuleTwoStart == &quot;Y&quot;) {
            ThreadTwo.start()
        }
    
    }
    
}
</code></pre>
<p>Method-1</p>
<pre><code>object cli_cpm extends Thread {
    def cpm_process(a: Int, b: Int): String = {
    
    val mul=a*b
    
    if(mul&gt;100){
    &quot;1&quot;  //return
    }
    &quot;0&quot;//return
    }
}
</code></pre>
<p>Method-2</p>
<pre><code>object cli_res extends Thread {
    def res_process(a: Int, b: Int): String = {
    
    val sum=a+b
    
    if(sum&gt;10){
    &quot;1&quot;  //return
    }
    &quot;0&quot;//return
    }
}
</code></pre>
<p>I know <code> %run</code>  and <code>dbutils.notebook.run</code> are used to call a notebook.</p>
<p>Any different way to implement above scenario in Azure Databricks.</p>",1,0,2022-02-07 07:00:30.360000 UTC,,,0,multithreading|azure|scala|parallel-processing|azure-databricks,27,2019-02-07 08:01:16.953000 UTC,2022-03-05 12:09:35.997000 UTC,"Vijayawada, Andhra Pradesh, India",51,1,0,7,,,,,,[]
'View TensorBoard' goes to 502 Bad Gateway,"<p>​Hello, </p>

<p>I am trying to launch Tensor board on Tensorflow/Keras on Azure Databricks, 
but it will go to '502 Bad Gateway' after clicking 'View TensorBoard'. </p>

<p>Please advise how to fix this. </p>

<p>Thanks.</p>

<blockquote>
<pre><code>Lo​oking for active tensorboard process...  
Active tensorboard processkilled... 
Starting tensorboard process...  
Tensorboard process started.
TensorBoard log directory set to: /dbfs/ml/lenet/train/1570698969.0092866. View TensorBoard
</code></pre>
</blockquote>",0,2,2019-10-10 09:30:22.277000 UTC,,2019-10-10 09:37:01.630000 UTC,4,tensorflow|tensorboard|azure-databricks,271,2017-11-17 23:13:29.297000 UTC,2021-12-22 23:30:03.973000 UTC,Bangkok Thailand,176,2,0,31,,,,,,[]
git log difference between 2 branches AND starting from a specific commit id,"<p>I'm doing <code>git log HEAD..remotes/upstream/master</code> to get the difference in commits between the two branches. I want to do the same thing but listing commits after a specific commit hash. </p>

<p>For example, my current output is something like:</p>

<pre><code>1e191 commit 5
e0fa4 commit 4
fc27c commit 3
99df2 commit 2
9cfef commit 1
</code></pre>

<p>I would like to pass in <code>9cfef</code> and get:</p>

<pre><code>1e191 commit 5
e0fa4 commit 4
fc27c commit 3
99df2 commit 2
</code></pre>

<p>Is there a simple way to do this?</p>",0,3,2014-06-14 18:56:46.883000 UTC,,,0,git|version-control|dvcs|git-log,84,2009-06-04 19:01:29.137000 UTC,2022-02-13 06:05:24.840000 UTC,,5687,453,38,194,,,,,,[]
Create a Fat jar to package multiple jars,"<p>I am using Databricks cluster to execute spark application..<br>
My application having some dependency on few libraries but now these libraries are not available via the Databricks install new library option.<br>
I came to know the through the Fat jar or Uber jar I can add multiple libraries and pass this to the cluster.<br>
I also came to know that to create a fat jar you have to provide a main class so I have written a simple program in my local system and added the dependencies to the build.sbt file.<br>
I am using the 'sbt assembly' command to create fat jar.<br>
Please note that I am not using the library in my sample program.   </p>

<p>My aim is to create a fat jar that inherits all the required jar in it so that my other Spark based application can access the libraries via this fat jar..</p>

<p>I did the following steps.</p>

<h2>'Sample Program'</h2>

<pre><code>  def main(args: Array[String]): Unit = {
    print(""Hello World"")
  }

}
</code></pre>

<h2>'Build.sbt'</h2>

<pre><code>name := ""CrealyticsFatJar""

version := ""0.1""

scalaVersion := ""2.11.12""

// https://mvnrepository.com/artifact/com.crealytics/spark-excel
libraryDependencies += ""com.crealytics"" %% ""spark-excel"" % ""0.12.0""

assemblyMergeStrategy in assembly := {
  case PathList(""META-INF"", xs @ _*) =&gt; MergeStrategy.discard
  case x =&gt; MergeStrategy.first
}
</code></pre>

<h2>'assembly.sbt'</h2>

<pre><code>addSbtPlugin(""com.eed3si9n"" % ""sbt-assembly"" % ""0.14.6"")
</code></pre>

<p>But I am not sure whatever I am doing is correct and will help to execute my spark programs in Databricks cluster.    </p>

<p><strong>Q1)</strong> There might be some possibility that one library having dependency on the other library so If I have mention the library name in SBT then would it load other dependent libraries?<br>
<strong>Q2)</strong> If I am not using the libraries to the existing program would it be available to the other program of the cluster.<br>
<strong>Q3)</strong> After the installation of Fat jar in cluster- how do I access the libraries.. I mean by which name I would access the libraries..the import command...</p>

<p>Apologies if my questions are so silly.
Thanks</p>",0,8,2020-03-18 19:11:53.003000 UTC,,2020-03-19 08:48:24.820000 UTC,0,scala|sbt|azure-databricks|uberjar|fatjar,395,2020-01-16 14:10:24.510000 UTC,2020-07-09 19:27:14.627000 UTC,,57,0,0,22,,,,,,[]
Connect to Databricks through ODBC without downloading driver,"<p>I need to connect to Databricks in order to run queries from my .NET app. I'd like to avoid the <code>Rest API</code> approach and use ODBC but I saw that, in order for the ODBC approach to work, I'd need to download an ODBC driver (Simba Spark). So, can I connect with Databricks through ODBC without any ODBC driver?</p>
<p>I need this because when it comes time for a deploy on my web app, I saw that these kind of drivers cannot be deployed on Azure so basically I can make this work only locally. But that's not my goal, the goal is to deploy it and put it in a production, providing service for different conditions and different queries.</p>",0,2,2021-11-23 13:15:01.250000 UTC,,,0,c#|.net|odbc|azure-databricks|databricks-connect,39,2016-10-26 12:03:43.167000 UTC,2022-03-02 12:42:37.167000 UTC,,376,43,5,44,,,,,,[]
Sending Dynamic file path from Azure Databricks Code to Azure Logic apps for email attachments,"<p>Hi I am new to logic apps. I have a python code for sending mail with attachments using Azure Logic Apps. When I provide a static file path in Get Blob Content the mail is working fine with the attachment. But when I am trying to the send the file path dynamically from azure databricks it is not receiving at the get blob content in logic app. I have also tried few examples using initialise variables but it is not working.</p>
<pre><code>import requests

def send_email(_to, _subject, _body, file):
  
  email_body = &quot;{0} &lt;br&gt;&lt;br&gt;&lt;br&gt; &lt;h6&gt; Hi Hello World &lt;/h6&gt;&quot;.format(_body)
  
  task = { &quot;body_data_1&quot;: email_body , &quot;subject_name_1&quot;: _subject , &quot;email_to_list_1&quot;: _to, &quot;attach&quot; : file }
  logic_app_post_url = 'https://prod-27.centralindia.logic.azure.com:443/workflows/*****'
  resp = requests.post(logic_app_post_url, json=task)
  print(resp.status_code)
  if resp.status_code == 202:
    print('Email Sent!')

email_to = &quot;abc@outlook.com&quot;
email_subject = &quot;Testing Logic App Send Mail with Dynamic File&quot;
email_body = &quot;Hi This is a test mail Dynamic&quot;
file_path = &quot;/super-store/output/orders/_SUCCESS&quot;
send_email(email_to, email_subject, email_body, file_path)

</code></pre>
<p><strong>Logic App Design</strong></p>
<p><a href=""https://i.stack.imgur.com/N5M1C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/N5M1C.png"" alt=""enter image description here"" /></a></p>
<p><strong>Logic App Run Error Details</strong></p>
<p><a href=""https://i.stack.imgur.com/3sGyP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3sGyP.png"" alt=""enter image description here"" /></a></p>
<p><strong>HTTP Input</strong>
<a href=""https://i.stack.imgur.com/Ntfft.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ntfft.png"" alt=""enter image description here"" /></a></p>
<p><strong>HTTP Output</strong>
<a href=""https://i.stack.imgur.com/IAQI6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IAQI6.png"" alt=""enter image description here"" /></a></p>
<p><strong>Get Blob Content Input</strong>
<a href=""https://i.stack.imgur.com/pfEWG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pfEWG.png"" alt=""enter image description here"" /></a></p>
<p><strong>Get Blob Content Output</strong>
<a href=""https://i.stack.imgur.com/jKDUp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jKDUp.png"" alt=""enter image description here"" /></a></p>",1,0,2021-07-18 18:14:14.723000 UTC,,2021-07-18 18:19:24.047000 UTC,0,python|azure-logic-apps|email-attachments|azure-databricks|dynamicparameters,263,2017-11-21 08:24:42.863000 UTC,2022-03-05 09:04:12.903000 UTC,,95,30,0,22,,,,,,[]
Where does databricks delta stores it's metadata?,<p>Hive stores it's metadata I'm external database like SQL server. Similar to that where does the databricks delta stores it's metadata Information?</p>,1,0,2019-04-23 08:41:54.687000 UTC,,2019-05-07 08:25:49.053000 UTC,2,pyspark|metadata|azure-databricks|delta|delta-lake,760,2018-09-22 08:12:38.960000 UTC,2022-01-27 13:35:58.563000 UTC,Singapore,158,151,0,37,,,,,,[]
Where can I find existing Service Principals for the business units?,"<p>changed teams, looking for names of service principals. only found example for Ent.Capabilities
<a href=""https://azure-help-ui.prd.cdc2.cfapps.8451.com/content/automation-tools/databricks-jobs.htm?Highlight=service%20principal#principal"" rel=""nofollow noreferrer"">https://azure-help-ui.prd.cdc2.cfapps.8451.com/content/automation-tools/databricks-jobs.htm?Highlight=service%20principal#principal</a></p>",0,2,2021-10-18 14:28:32.667000 UTC,,,0,azure|azure-databricks|azure-service-principal,50,2020-08-24 15:38:41.713000 UTC,2022-03-04 18:26:10.370000 UTC,,1,0,0,0,,,,,,[]
Advantages of using HDInsights SPARK over Azure Databricks cluster,"<p>I have gone through multiple documents, but not able to get the list of advantages of using HDInsigths spark cluster compared to Azure Databricks cluster. Is there any key differentiators between these two. I need basically the list of features supported by HDInsights and not supported by Azure Databricks.</p>",1,1,2019-01-24 12:50:56.353000 UTC,,,2,azure-hdinsight|azure-databricks,3434,2016-04-28 14:47:00.813000 UTC,2021-06-08 12:35:47.530000 UTC,,1173,3,0,147,,,,,,[]
dbutils.fs.ls returning FileNotFoundException exception inside filter,"<p>I have a list of blob store paths like this <code>wasbs://requestlogs@logs.blob.core.windows.net/logs/1/2021/04/12/*.avro.</code>
when I am trying to read it's throwing <code>FileNotFoundException</code> exception for all paths.</p>
<pre><code>def checkPathExists(path:String): Boolean = 
{ 
  try
  {
    dbutils.fs.ls(path.replace(&quot;*.avro&quot;,&quot;&quot;))
    return true
  }
  catch
  {
    case ioe:java.io.FileNotFoundException =&gt; return false
  }
}

val pathsRdd = sc.parallelize(paths)
val fileteredPaths2 = pathsRdd.filter(p =&gt; checkPathExists(p)).collect.toList
println(fileteredPaths2.length)
</code></pre>
<p>But when I am running the below command its is working
<code>dbutils.fs.ls(&quot;wasbs://requestlogs@logs.blob.core.windows.net/logs/1/2021/04/12/&quot;)</code></p>
<p>I am not sure why this is not working inside filter</p>",0,1,2021-04-12 06:46:32.063000 UTC,,2021-04-12 07:14:48.577000 UTC,0,java|scala|apache-spark|azure-blob-storage|azure-databricks,107,2019-02-04 11:51:03.887000 UTC,2022-02-25 13:19:56.217000 UTC,"Bangalore, Karnataka, India",59,7,0,17,,,,,,[]
Azure Devops integration error in Azure DataBricks,"<p>I am getting following error while syncing a azure databricks notebook with an Azure DevOps repo I've linked it to: </p>

<p>Error while syncing Git history: Numeric value (3204746134) out of range of int? at [Source: {""id"":</p>

<p>The repository branch specified in azure databricks is successfully created over on Azure DevOps.</p>

<p>I tried removing the revision history of the notebook that is no longer required to reduce the number of changes to be checked-in. This did not help to fix the error.</p>",1,1,2019-06-25 22:36:56.813000 UTC,,,0,azure|azure-devops|azure-databricks,329,2019-05-30 22:41:11.443000 UTC,2022-02-24 02:35:50.323000 UTC,"Redmond, WA, USA",1,0,0,5,,,,,,[]
unable to connect to azure gen2 storage account in South Central US region from azure databrick in EAST US,"<p>We are facing an access issue to connect azure storage account. Firewall is enabled in the storage account to provide access to only specific IPs. Storage account is hosted in Azure South Central US.</p>
<p>We are trying to access data from azure databricks which is hosted in EAST US region.</p>
<p>Now issue is if databricks is also hosted in South central then we can easily whitelist databricks vnet and access storage. But in our case we can't do since vnet is only accessible within the region. We also don't have public IPs for databricks cluster.</p>
<p>We can't all traffic from all the networks.</p>
<p>Can someone suggest how can we access storage account from databricks in this use case.</p>",1,0,2021-12-30 11:48:51.173000 UTC,,,0,azure|networking|azure-databricks|azure-storage-account|vnet,59,2021-05-05 16:36:33.407000 UTC,2022-01-19 09:44:46.317000 UTC,,1,0,0,1,,,,,,[]
How to create a checkpoint for DeltaLake in Azure Databricks for past data with Pyspark?,"<p>I don't know how to create a checkpoint for DeltaLake in Azure Databricks for a past version.</p>
<p>I tried to access the &quot;DeltaLog&quot; object without success to execute that:</p>
<pre><code>DeltaLog.forTable(spark,dataPath).checkpoint()
</code></pre>
<p>I would like to create checkpoints for an older version.</p>",0,2,2021-03-17 08:00:16.433000 UTC,,,0,pyspark|azure-databricks|delta-lake,144,2015-02-11 07:34:40.283000 UTC,2022-03-04 09:52:16.787000 UTC,"Lyon, France",377,18,2,112,,,,,,[]
AWS Neptune find all ancestors for a node,"<p>I used Jupyter notebook to insert the following vertices and edges into Neptune database.</p>
<pre><code>%%gremlin

g.addV('my').property(T.id, '1').next()
g.addV('my').property(T.id, '2').next()
g.addV('my').property(T.id, '3').next()
g.addV('my').property(T.id, '4').next()
g.addV('my').property(T.id, '5').next()
g.addV('my').property(T.id, '6').next()
g.addV('my').property(T.id, '7').next()
g.addV('my').property(T.id, '8').next()

g.V('1').addE('parent').to(g.V('2')).next()
g.V('2').addE('parent').to(g.V('3')).next()
g.V('3').addE('parent').to(g.V('4')).next()
g.V('4').addE('parent').to(g.V('5')).next()

g.V('1').addE('parent').to(g.V('6')).next()
g.V('6').addE('parent').to(g.V('7')).next()
g.V('7').addE('parent').to(g.V('8')).next()
</code></pre>
<p>Then I used the follow query to find all ancestors for node &quot;1&quot;. However, It only return node &quot;5&quot; and note &quot;8&quot;, and highest level of ancestors.</p>
<p>How can I modify the query to get all the in-and-between ancestors like &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;8&quot;</p>
<pre><code>%%gremlin -p v,oute,inv
g.V('1').repeat(out('parent')).until(outE('parent').count().is(0)).toList()
</code></pre>",1,1,2021-07-01 17:48:12.517000 UTC,,2021-10-26 16:04:58.027000 UTC,0,amazon-web-services|gremlin|amazon-neptune,45,2012-02-03 16:20:42.710000 UTC,2022-03-04 16:51:03.243000 UTC,,5628,119,6,508,,,,,,[]
"How to use pipeline parameters in DF Data Flow, Dataset source options","<p>I've been at this for a day now, trying every variation possible and searching for others solutions. I have a DF pipeline with a few DataBricks notebooks with the end result being saved to blob storage, but needs to be joined with a SQL table in order to update some values. I am using dynamic folder names to pull the blob file which works fine, but when I try the same thing for my SQL query, it doesn't fail, but it doesn't seem to select any records. So how does one use pipeline parameters in a Data Flow SQL query?</p>
<p>Overview of the pipeline:<a href=""https://i.stack.imgur.com/88xvp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/88xvp.png"" alt=""enter image description here"" /></a></p>
<p>The failing source:<a href=""https://i.stack.imgur.com/yf7d7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yf7d7.png"" alt=""enter image description here"" /></a></p>
<p>My query to use the variable:
<a href=""https://i.stack.imgur.com/Af4yj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Af4yj.png"" alt=""enter image description here"" /></a></p>
<p>Finally the results:
<a href=""https://i.stack.imgur.com/GnNnJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GnNnJ.png"" alt=""enter image description here"" /></a></p>
<p>Any help is greatly appreciated!</p>",1,1,2020-10-07 22:03:46.787000 UTC,,,0,azure-data-factory|azure-databricks,1771,2014-06-06 20:12:16.617000 UTC,2022-02-17 15:12:34.187000 UTC,,103,11,0,9,,,,,,[]
How to use more than two logical operators in iF expression of azure data factory or nested expression of IF condition in azure datafactory?,"<p>I am trying below code in expression block of if condition..but its saying error as :Function 'and' does not accept 5 arguments(s)</p>
<p>Code:
@and(equals(activity('Notebook1').output.runoutput.status,'Success'),equals(activity('Notebook2').output.runoutput.status,'Success'),equals(activity('Notebook3').output.runoutput.status,'Success'),equals(activity('Notebook4').output.runoutput.status,'Success'),equals(activity('Notebook5').output.runoutput.status,'Success'))</p>
<p>All the notebooks from 1 to 5 are being executed in true block of IF condition just connected before the above IF condition</p>
<p>My target is to check all the outputs of 5 notebooks if all the notebooks pass suscess message then it should go for 'True' block in IF condition else if any of the notebooks from 1 to 5 returns any message other than 'Success' the should go for Flase block of IF condition</p>",1,0,2020-12-11 09:55:35.060000 UTC,,,0,azure|azure-data-factory|azure-data-factory-2|azure-databricks,335,2020-12-11 09:41:25.747000 UTC,2021-03-17 04:08:46.197000 UTC,"Hyderabad, Telangana, India",1,0,0,3,,,,,,[]
What is a good way of writing some pre run data validation inside my json config file?,"<p>I have a config file that describes an ETL lineage.</p>
<p>I want my config file to be able to perform some validation before running the ETL, based on the title of the files, the number of files received. As this is a config file driven approach, this check should be as general as possible and be able to adapt to different cases.</p>
<p>Below some examples of the cases :</p>
<ol>
<li>The ETL has two sources : file A_yyyy_MM_dd.csv and file B_yyyy_MM_dd.csv. A join will be performed between them.</li>
</ol>
<p>But i can only perform this join if the dates of files A and file B are the same : eg. A_2021_08_10.csv and B_2021_08_10.csv. If B_2021_08_10.csv arrived, but not A_2021_08_10.csv the ETL should not be triggered.</p>
<ol start=""2"">
<li>We will receive 300 hundred files every morning, and a marker file containing success_300. I want my ETL to be triggered upon the arrival of the marker file. But the ETL should also wait until the 300 files are received.</li>
</ol>
<p>To cope with these different cases, I was thinking about creating views eg (based on the file name or the number of files expected). And then perform some sql checks based on the views.</p>
<pre><code>     &quot;pre_run_validation&quot; : [
          {&quot;create_view&quot; : [&quot;sources_file_name&quot;,&quot;number_of_files&quot;],
          &quot;sql_validation&quot; : &quot;mnt/sql_path/sql&quot;}
]
</code></pre>
<p>To execute this data validation, I am using pyspark running on databricks. I will read the sql statement using &quot;spark.sql('query')&quot;</p>
<p>I feel like it is not that great. I'm not sure what is the best approach.</p>
<p>Please let me know if you have any best practices for this kind of config files/ETL.</p>",0,1,2021-08-11 07:21:00.157000 UTC,,2021-08-11 11:39:14.110000 UTC,0,sql|pyspark|config|azure-databricks,43,2018-06-02 16:53:24.290000 UTC,2022-03-05 21:11:03.483000 UTC,,684,199,1,121,,,,,,[]
How can I improve order().by() performance in Neptune?,"<p>I am trying to solve a performance issue with a traversal and have tracked it down to the <code>order().by()</code> step. It seems that <code>order().by()</code> greatly increases the number of statement index ops required (per the profiler) and dramatically slows down execution.</p>
<p>A non-ordered traversal is very fast:</p>
<pre><code>g.V().hasLabel(&quot;post&quot;).limit(40)
</code></pre>
<p>execution time: 2 ms</p>
<p>index ops: 1</p>
<p>Adding a single ordering step adds thousands of index ops and runs much slower.</p>
<pre><code>g.V().hasLabel(&quot;post&quot;).order().by(&quot;createdDate&quot;, desc).limit(40)
</code></pre>
<p>execution time: 62 ms</p>
<p>index ops: 3909</p>
<p>Adding a single filtering step adds thousands more index ops and runs even slower:</p>
<pre><code>g.V().hasLabel(&quot;post&quot;).has(&quot;isActive&quot;, true).order().by(&quot;createdDate&quot;, desc).limit(40)
</code></pre>
<p>execution time: 113 ms</p>
<p>index ops: 7575</p>
<p>However the same filtered traversal without ordering runs just as fast as the original unfiltered traversal:</p>
<pre><code>g.V().hasLabel(&quot;post&quot;).has(&quot;isActive&quot;, true).limit(40)
</code></pre>
<p>execution time: 1 ms</p>
<p>index ops: 49</p>
<p>By the time we build out the actual traversal we run in production there are around 12 filtering steps and 4 <code>by()</code> step-modulators causing the traversal to take over 6000 ms to complete with over 33000 index ops. Removing the <code>order().by()</code> steps causes the same traversal to run fast (500 ms).</p>
<p>The issue seems to be with <code>order().by()</code> and the number of index ops required to sort. I have seen the performance issue noted <a href=""https://docs.aws.amazon.com/neptune/latest/userguide/gremlin-traversal-tuning.html"" rel=""nofollow noreferrer"">here</a> but adding <code>barrier()</code> did not help. The traversal is also fully optimized requiring no Tinkerpop conversion.</p>
<p>I am running engine version 1.1.0.0 R1. There are about 5000 <code>post</code> vertices.</p>
<p>How can I improve the performance of this traversal?</p>",1,0,2021-12-27 21:58:36.250000 UTC,,2021-12-28 15:02:56.270000 UTC,0,gremlin|amazon-neptune,53,2010-08-23 23:03:20.887000 UTC,2022-03-03 17:19:08.790000 UTC,,4656,718,14,204,,,,,,[]
AWS Neptune Performance / Issues with Gremlin,"<p>Working on loading data to Neptune using gremlin , Having Neptune Infrastructure of DB Instance size (db.r5.4xlarge(16 vCPUs)).
Data is loaded to Neptune via AWS Glue job with <strong>5</strong> worker threads using pyspark.</p>
<p>Loading data by doing an upsertion with deduped  dataset and batching(50 records/batch) them together as single query to Neptune,</p>
<p><strong>Vertices :</strong> Compute all vertices to be loaded in graph after deduping (There are no duplicate nodes)</p>
<p><strong>Query used :</strong></p>
<pre><code>g.V().has(T.id, record.id).fold().coalesce(__.unfold(),__.addV(record.source).property(T.id, record.id)
.V().has(T.id, record.id).fold().coalesce(__.unfold(),__.addV(record.source).property(T.id, record.id)
(Do 48 items).next()
</code></pre>
<p>Time taken to perform for 2.45M unique vertices is <strong>5 mins</strong></p>
<p><strong>Edges:</strong> Compute all edges to be loaded in graph after deduping  (There are no duplicate edges)</p>
<p><strong>Query used :</strong></p>
<pre><code>g.V(edgeData.id1).bothE().where(__.otherV().hasId(edgeData.id2)).fold().coalesce(__.unfold(),__.addE('coincided_with').from_(__.V(edgeData.id1)).to(__.V(edgeData.id2))).property(Cardinality.single, timestamp, edgeData.timestamp).property(Cardinality.single, count, edgeData.count)
.V(edgeData.id1).bothE().where(__.otherV().hasId(edgeData.id2)).fold().coalesce(__.unfold(),__.addE('coincided_with').from_(__.V(edgeData.id1)).to(__.V(edgeData.id2))).property(Cardinality.single, timestamp, edgeData.timestamp).property(Cardinality.single, count, edgeData.count)
(Do 48 items).next()
</code></pre>
<p>Time taken to perform for 1.88M unique edges with properties is <strong>21 mins</strong></p>
<p>If we perform just <strong>edge creation alone without any properties</strong>  to edge ,</p>
<p><strong>Query used :</strong></p>
<pre><code> g.V(edgeData.id1).bothE().where(__.otherV().hasId(edgeData.id2)).fold().coalesce(__.unfold(),__.addE('coincided_with').from_(__.V(edgeData.id1)).to(__.V(edgeData.id2)))
.V(edgeData.id1).bothE().where(__.otherV().hasId(edgeData.id2)).fold().coalesce(__.unfold(),__.addE('coincided_with').from_(__.V(edgeData.id1)).to(__.V(edgeData.id2)))
(Do 48 items).next()
</code></pre>
<p>Time taken to perform for 1.88M unique edges without properties is <strong>4 mins</strong></p>
<p><strong>Performance Issues:</strong></p>
<ol>
<li>Ideally while inserting vertices we shouldn’t be seeing any ConcurrentModification exception, but we do get it frequently even while creating vertices in a fresh instance of Neptune (db.r5.4xlarge),  We have mitigated  this by doing retry logic  on them ,  There are cases while doing edge inserts  from Vertex (A -&gt; B) even after retrying 10 times with interval of 300 millisecond it still fails to insert them.  Overall issue,  we are ended up with more time to insert our data and is there a way to avoid concurrent exception even though we are avoiding the scenarios of concurrency.</li>
<li>While adding edges properties during batch upsertion, we could see the time taken is  much longer than upserting edges without properties
Eg: Adding 2 properties to edges
1.8M edge with properties took close to 21 min to upsert our data
1.8M edges without properties took close to 4 min to upsert our data
Edge Creation with properties is much slower , is there anyway to speed up loading of edges with properties (we have 40M edges so the time to insert is much longer)</li>
<li>Adding more parallel worker threads , we endup being much slower and concurrency errors are more (cpu load is around 50% and its not maxing)</li>
</ol>
<p>Any suggestions to improve performance would be much of help</p>",1,5,2021-07-14 11:08:16.137000 UTC,,2021-08-22 14:21:19.483000 UTC,2,amazon-web-services|gremlin|aws-glue|amazon-neptune,541,2010-07-14 10:43:18.890000 UTC,2021-07-16 08:17:20.510000 UTC,,207,0,0,42,,,,,,[]
java.lang.OutOfMemoryError While receiving HTTP response,"<p>I have below function to call API and that returns the text from response body.</p>
<pre><code>def GetResponse(headers: Map[String, String], url: String): String = {
    
    val obj: URL = new URL(url)
    val con: HttpsURLConnection = obj.openConnection().asInstanceOf[HttpsURLConnection]
    var iStream: InputStream = null
  
    try {
      var output: String = null
    
      con.setRequestMethod(&quot;GET&quot;)
      headers.foreach { case (k, v) =&gt; {
        con.setRequestProperty(k, v)
      }
      }
      val httpResponseCode = con.getResponseCode
      
      if (httpResponseCode == 200 || httpResponseCode == 206) {
        iStream = con.getInputStream
        output = Source.fromInputStream(iStream).getLines().mkString(&quot;\n&quot;)
      }
      output
    }
    catch {
      case exception: Exception =&gt; throw new Exception(&quot;Http Handler exception in GetHttpResponse&quot;, exception)
    }
  finally{
    if(iStream != null){
      iStream.close()
    }
    con.disconnect()
  }
  }
</code></pre>
<p>I have 6GB CSV file on blob storage and I am trying to get the data bytes by bytes i.e. first 0 to 500MB, then 501MB to 1000MB etc.</p>
<p>removing unnecessary code from below block. In short, below loop gets executed 4 times, 500MB X 4 and exactly fails after importing 2GB data. How exactly we can flush the data here? We are simply calling the function and saving data in data frame. We are not appending data to variable, right?</p>
<pre><code>for(i &lt;- 1 to chunkNum) {
      println(i)
              println(&quot;BeforeResponse&quot;)
              val response =  GetResponse(headers, &quot;https://myblobstorage.blob.core.windows.net/test/test6gb.csv&quot;)
              println(&quot;AfterResponse&quot;)
              dfRestAPI = dfRestAPI.union(Seq((response,currentDate)).toDF(&quot;Chunk&quot;,&quot;InsertedDate&quot;))
            }
</code></pre>
<p>Result:</p>
<p><a href=""https://i.stack.imgur.com/j5hRZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j5hRZ.png"" alt=""ErrorMessage"" /></a></p>",0,5,2021-06-30 19:11:37.573000 UTC,,2021-07-01 06:43:58.477000 UTC,0,java|scala|apache-spark|azure-databricks,69,2019-06-26 15:30:58.003000 UTC,2022-03-02 15:25:32.720000 UTC,,159,9,0,33,,,,,,[]
Two Spark foreachPartition(func) statements can be run sequentially and maintain execution order?,"<p>So the idea is I have a dataframe in pysaprk running on azure databricks which has all my transformation done. Now during persisting onto the database have some conditions to mark a record as 'insert' or 'del_insert'. Meaning each of the records will either be inserted alone or DELETED THEN inserted. Thus the sequence matters, delete has to be first.</p>
<p><strong>df</strong></p>
<pre><code>col1| col2| db_action
val | val2| insert
val3| val4| del_insert
val5| val6| insert
</code></pre>
<p>Now at the very end am splitting the df based on the db_action. (Or when am creating col db_action I can make it a separate df as well.)</p>
<p>Now I have two dfs <strong>df_del &amp; df_ins</strong> and I do</p>
<pre><code>df_del.repartition(2)
df_del.foreachParition(deleteFunc)

df_insert.repartition(2)
df_insert.foreachParition(insertFunc)
</code></pre>
<p>my motive is insertFunc should NEVER be invoked until df_del.foreachParition(deleteFunc) has completed for all records in all partitions for df_del.</p>
<p>Is that valid approach valid? Won't line &quot;df_insert.foreachParition(insertFunc)&quot; be executed    even before execution is finished for &quot;df_del.foreachParition(deleteFunc)&quot;?</p>",0,0,2022-02-15 13:54:19.153000 UTC,,,0,python|dataframe|apache-spark|pyspark|azure-databricks,20,2020-04-21 14:25:58.840000 UTC,2022-03-01 17:58:58.563000 UTC,,1,0,0,1,,,,,,[]
Is it possible to profile Tensorflow with Tensorboard in Azure Databricks?,<p>I have a Tensorflow model in an Azure Databricks notebook and I have setup Tensorboad to visualise metrics from it. I would also like to be able to profile training but if I try to capture a profile from localhost:6009 it fails. Is it possible to do this in Azure Databricks?</p>,0,2,2020-03-11 11:09:54.747000 UTC,,,1,azure|tensorflow|azure-databricks,124,2020-03-11 11:04:32.383000 UTC,2020-04-01 15:18:28.117000 UTC,"Leeds, UK",11,0,0,1,,,,,,[]
Update pandas data into existing csv,"<p>I have a csv which I'm creating from pandas data-frame. </p>

<p>But as soon as I append it, it throws: OSError: [Errno 95] Operation not supported</p>

<pre><code>for single_date in [d for d in (start_date + timedelta(n) for n in range(day_count)) if d &lt;= end_date]:
  currentDate = datetime.strftime(single_date,""%Y-%m-%d"")
  #Send request for one day to the API and store it in a daily csv file
  response = requests.get(endpoint+f""?startDate={currentDate}&amp;endDate={currentDate}"",headers=headers)
  rawData = pd.read_csv(io.StringIO(response.content.decode('utf-8')))


  outFileName = 'test1.csv'
  outdir = '/dbfs/mnt/project/test2/'
  if not os.path.exists(outdir):
    os.mkdir(outdir)

  fullname = os.path.join(outdir, outFileName)    


  pdf = pd.DataFrame(rawData)
  if not os.path.isfile(fullname):
    pdf.to_csv(fullname, header=True, index=False)
  else: # else it exists so append without writing the header
    with open(fullname, 'a') as f: #This part gives error... If i write 'w' as mode, its overwriting and working fine.
      pdf.to_csv(f, header=False, index=False, mode='a')
</code></pre>",3,4,2019-12-04 10:59:13.583000 UTC,,2019-12-04 12:56:32.407000 UTC,0,python-3.x|pandas|azure-databricks,469,2016-01-19 23:58:20.783000 UTC,2022-03-02 11:59:52.213000 UTC,,153,18,0,56,,,,,,[]
Gremlin - sort shortest weighted path output by cost,"<p>I try to get the shortest weighted path from Amazon Neptune graph using Gremlin, as demonstrated in TinkerPop recipes - </p>

<pre><code>gremlin&gt; g.V(1).repeat(outE().inV().simplePath()).until(hasId(5)).
           path().as('p').
           map(unfold().coalesce(values('weight'),
                                 constant(0.0)).sum()).as('cost').
           select('cost','p')
</code></pre>

<p>But, I need that the output will be ordered by the calculated cost (the lowest cost as the first output) and not by the number of nodes in the path.</p>

<p>I tried a few combinations of <code>order().by(..)</code> in the query without success</p>",1,0,2019-02-26 14:37:29.530000 UTC,,,1,graph|gremlin|traversal|shortest-path|amazon-neptune,217,2015-10-04 14:41:37.400000 UTC,2019-07-08 08:17:49.790000 UTC,Israel,41,0,0,25,,,,,,[]
Kafka consumer using AWS_MSK_IAM ClassCastException error,"<p>I have MSK running on AWS and I'd like to consume information using AWS_MSK_IAM authentication.</p>
<p>My MSK is properly configured and I can consume the information using Kafka CLI with the following command:</p>
<pre><code>../bin/kafka-console-consumer.sh --bootstrap-server b-1.kafka.*********.***********.amazonaws.com:9098 --consumer.config client_auth.properties --topic TopicTest --from-beginning
</code></pre>
<p>My client_auth.properties has the following information:</p>
<pre><code># Sets up TLS for encryption and SASL for authN.
security.protocol = SASL_SSL

# Identifies the SASL mechanism to use.
sasl.mechanism = AWS_MSK_IAM

# Binds SASL client implementation.
sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required;

# Encapsulates constructing a SigV4 signature based on extracted credentials.
# The SASL client bound by &quot;sasl.jaas.config&quot; invokes this class.
sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler
</code></pre>
<p><strong>When I try to consume from my Databricks cluster using spark, I receive the following error:</strong></p>
<pre><code>Caused by: kafkashaded.org.apache.kafka.common.KafkaException: java.lang.ClassCastException: software.amazon.msk.auth.iam.IAMClientCallbackHandler cannot be cast to kafkashaded.org.apache.kafka.common.security.auth.AuthenticateCallbackHandler
</code></pre>
<p>Here is my cluster config:
<a href=""https://i.stack.imgur.com/KKz0N.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KKz0N.png"" alt=""enter image description here"" /></a></p>
<p>The libraries I'm using in the cluster:</p>
<p><a href=""https://i.stack.imgur.com/zJLFS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zJLFS.png"" alt=""enter image description here"" /></a></p>
<p>And the code I'm running on Databricks:</p>
<pre><code>raw = (
    spark
        .readStream
        .format('kafka')
        .option('kafka.bootstrap.servers', 'b-.kafka.*********.***********.amazonaws.com:9098')
        .option('subscribe', 'TopicTest') 
        .option('startingOffsets', 'earliest')
        .option('kafka.sasl.mechanism', 'AWS_MSK_IAM')
        .option('kafka.security.protocol', 'SASL_SSL')
        .option('kafka.sasl.jaas.config', 'software.amazon.msk.auth.iam.IAMLoginModule required;')
        .option('kafka.sasl.client.callback.handler.class', 'software.amazon.msk.auth.iam.IAMClientCallbackHandler')
        .load()
)
</code></pre>",0,4,2021-10-29 17:17:11.080000 UTC,,,0,apache-kafka|aws-msk|aws-databricks|aws-iam-authenticator,359,2021-08-30 20:16:35.510000 UTC,2022-03-04 12:05:05.910000 UTC,,51,3,0,8,,,,,,[]
Error while storing vertex and edge using gremlin with spring-boot,"<p>I am using spring-boot to create a rest api. So I am sending data over api which will run 24/7.</p>

<p>I configured cluster, client and GraphTraversalSource with remote connection once. using @Inject annotation.</p>

<p>When the method is called from the controller, I am sending two objects. </p>

<p>User and Movie and craeted pipeline like this.</p>

<p>g.addV(""USER"").property(T.id,""userid1"").addV(""MOVIE"").property(T.id,""movie1"").next()</p>

<p>this stores both vertexes.</p>

<p>Now I call the api again with different user and different movie.</p>

<p>g.addV(""USER"").property(T.id,""userid2"").addV(""MOVIE"").property(T.id,""movie2"").next()</p>

<p>even both vertexes ids are different, I still get error for the ""userid1"". I don't understand, why I am getting the error for ""userid""</p>

<p>org.apache.tinkerpop.gremlin.driver.exception.ResponseException: {""requestId"":""965ffcdc-204f-4c2d-989b-108f4f2fd53c"",""detailedMessage"":""Vertex with id already exists: userid1"",""code"":""ConstraintViolationException""}</p>

<pre><code>at org.apache.tinkerpop.gremlin.driver.Handler$GremlinResponseHandler.channelRead0(Handler.java:259) ~[gremlin-driver-3.4.2.jar:3.4.2]

at org.apache.tinkerpop.gremlin.driver.Handler$GremlinResponseHandler.channelRead0(Handler.java:198) ~[gremlin-driver-3.4.2.jar:3.4.2]

at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]

at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]

at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]

at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]

at org.apache.tinkerpop.gremlin.driver.Handler$GremlinSaslAuthenticationHandler.channelRead0(Handler.java:124) ~[gremlin-driver-3.4.2.jar:3.4.2]

at org.apache.tinkerpop.gremlin.driver.Handler$GremlinSaslAuthenticationHandler.channelRead0(Handler.java:68) ~[gremlin-driver-3.4.2.jar:3.4.2]

at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]

at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]

at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]

at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]

at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]

at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]

at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]

at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]

at org.apach
</code></pre>",0,3,2019-06-27 19:55:00.763000 UTC,,,0,gremlin|tinkerpop3|amazon-neptune,193,2013-07-02 22:49:57.270000 UTC,2020-07-07 21:00:44.237000 UTC,,1197,121,1,59,,,,,,[]
Benchmarks to compare two azure data factory pipelines,"<p>I have built two pipelines with different transformations for the same functionality.</p>
<p>Are there any benchmarks to compare these two pipelines in terms of efficiency and/or resource utilization?</p>
<p>To explain in detail:
Pipeline 1 : Uses only 2 Mapping data flows. One with 4 transformations and other with 20 transformations.
Pipeline 2 : Uses 2 Mapping data flows. One with 4 transformations , second DF other with 15 transformations and with Databricks notebook.</p>
<p>I want to compare these two pipelines in terms of
1.Efficieny
2.Resource utilization
3.Costs</p>
<p>Any inputs?</p>
<p>Thank you</p>",1,0,2020-08-31 12:11:16.263000 UTC,,,0,azure|azure-data-factory|azure-databricks,156,2019-02-12 07:08:32.167000 UTC,2022-02-26 10:44:53.403000 UTC,"Chennai, Tamil Nadu, India",470,63,3,120,,,,,,[]
JSON one to one mapping in Azure Databricks with python,"<p>I have 2 JSON structures, I want to create a map for JSON A to JSON B. For eg. A.employee.name will be mapped to B.employee.fullname. When any object of schema A is given , we can expect an output in form of B.
salary should be mapped to reenumeration and name should be mapped to fullname,
Input:</p>
<pre><code>var A = {&quot;employee&quot;: { &quot;name&quot;:&quot;sam&quot;, &quot;salary&quot;:56000,&quot;married&quot;:true}}  
</code></pre>
<p>Output:</p>
<pre><code>var B = { &quot;employee&quot;: {&quot;fullname&quot;:&quot;sam&quot;,&quot;reenumeration&quot;:56000,&quot;unmarried&quot;:true}}  
</code></pre>
<p>I have created RDD of the JSON object, not sure how to map it.</p>
<pre><code>from pyspark.sql.functions import explode,map_keys
from pyspark.sql.functions import get_json_object

testJsonData = spark.read.json(&quot;dbfs:/FileStore/tables/SrcObj.json&quot;,multiLine=True);

def toMapSrcToTarg(rdd):
    key = str(group[0]);
    values = list(group[1]);

    return Row(key,values,e)

testJsonData.rdd.map(toMapSrcToTarg);

for x in rdd.collect():
    print(x)
</code></pre>
<p>PS: I am fairly new to azure databricks and python.</p>",0,3,2021-12-27 05:17:25.540000 UTC,,,0,python|pyspark|apache-spark-sql|azure-databricks,37,2015-12-31 11:35:08.190000 UTC,2022-03-05 08:53:33.627000 UTC,,692,59,13,88,,,,,,[]
Is it possible to follow the history of files in another repository,"<p>Here is the scenario :</p>

<p>We have an ""official"" repository containing some folders. This folder is owned by user A and user A should be the only one to be allowed to push on it :</p>

<p><code></p>

<pre><code>repoA
 |
 -- folderA1
  |
  |- fileA11 .. fileA12
 |
 -- folderA2
  |
  |....
</code></pre>

<p></code></p>

<p>User B needs to maintain his own copy of folderA1 (from repoA) and should be able to merge the commits pushed by userA in its own copy. User B doesn't want folderA2</p>

<p>Of course, user B will commit some modifications to his own copy of folderA1 and the history of folderA1 (viewed from user B perspective) should look like that :</p>

<p><code></p>

<pre><code>HEAD
|
*    Merge user A master into user B master
| \
* |  Last commit made by user A
* |  Previous commit made by user A
| *  Last commit made by user B
| *  Previous commit made by user B
|/
*    Initial commits made by user A
*
*
|
</code></pre>

<p></code></p>

<p>User B should not have folderA2 (from user A) in his own repository.</p>

<p>User B should be able to have folderB1 and folderB2 in his own repository.</p>

<p>Thanks</p>",1,1,2012-04-27 08:17:03.613000 UTC,,,1,git|version-control|dvcs,59,2009-12-04 14:08:19.313000 UTC,2018-06-14 09:36:38.393000 UTC,"Toulouse, France",908,165,3,105,,,,,,[]
AWS Neptune - Betweennes Centrality computation,"<p>I'm struggling to get a betweenness centrality computation done on a graph in AWS Neptune using Gremlin. I am using the Gremlin recipe:</p>
<pre><code>g.V().as(&quot;v&quot;).
           repeat(both().simplePath().as(&quot;v&quot;)).emit(). 
           filter(project(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;).by(select(first, &quot;v&quot;)).
                                       by(select(last, &quot;v&quot;)).
                                       by(select(all, &quot;v&quot;).count(local)).as(&quot;triple&quot;).
                  coalesce(select(&quot;x&quot;,&quot;y&quot;).as(&quot;a&quot;).
                             select(&quot;triples&quot;).unfold().as(&quot;t&quot;).
                             select(&quot;x&quot;,&quot;y&quot;).where(eq(&quot;a&quot;)).
                             select(&quot;t&quot;),
                           store(&quot;triples&quot;)).
                  select(&quot;z&quot;).as(&quot;length&quot;).
                  select(&quot;triple&quot;).select(&quot;z&quot;).where(eq(&quot;length&quot;))).
           select(all, &quot;v&quot;).unfold(). 
           groupCount().next() 
</code></pre>
<p>this works fine on this toy graph, also from the recipe:</p>
<pre><code>g.addV().property(id,'A').as('a').
           addV().property(id,'B').as('b').
           addV().property(id,'C').as('c').
           addV().property(id,'D').as('d').
           addV().property(id,'E').as('e').
           addV().property(id,'F').as('f').
           addE('next').from('a').to('b').
           addE('next').from('b').to('c').
           addE('next').from('b').to('d').
           addE('next').from('c').to('e').
           addE('next').from('d').to('e').
           addE('next').from('e').to('f').iterate()
</code></pre>
<p>but when using the Movielens100k dataset, as pre-loaded in Neptune, it does not compute in reasonable time. Presumably this is because the graph is too large and the betweenness centrality is computationally too expensive.
The graph is loaded correctly in Neptune and I can do standard traversals.</p>
<p>And so my question: for larger graphs can Gremlin be used or do we need to rely on Networkx and other packages or is there a solution in Sparql? So my question is which is the most efficient method to compute centrality metrics in AWS Neptune? Thanks a lot!!</p>",0,0,2021-03-25 09:32:09.690000 UTC,1.0,2021-03-25 16:19:00.727000 UTC,1,amazon|amazon-neptune,148,2020-09-21 05:21:20.227000 UTC,2021-12-11 16:48:09.240000 UTC,"London, UK",11,0,0,2,,,,,,[]
Unable to read json file from dbfs which inturn the data is getting corrupted in python spark,"<p>I am unable to read json files from <code>dbfs</code> which is getting corrupted dataframe.</p>

<p>I have tried </p>

<pre><code>dfx = spark.read.option(""multiline"",""true"").json(""/FileStore/tables/vv.json"")

dfx = spark.read.option(""multiline"", ""true"").json(""/FileStore/tables/vv.json"")
</code></pre>

<p>output </p>

<pre><code>dfx:pyspark.sql.dataframe.DataFrame = [_corrupt_record: string]
</code></pre>",0,5,2019-06-11 10:11:46.500000 UTC,,2019-06-11 10:38:43.347000 UTC,0,python|json|pyspark|apache-spark-sql|azure-databricks,363,2017-11-21 08:24:42.863000 UTC,2022-03-05 09:04:12.903000 UTC,,95,30,0,22,,,,,,[]
Insert overwrite Hive Table via the databricks notebook is throwing error,"<p>I've batch job that inserts the data to hive table on a daily basis and creates multiple smaller ORC's files on the blob location, i will need to combine all the small ORC files to one larger ORC file so that the read performance would be much better.</p>
<p>In this context, i used to schedule the below SQL query to run every day post my batch job completes in Azure HDInsight. When i try to schedule the same query in Azure Databricks notebook, it's throwing the below error. Is there a reason why this works in HDInsight and not working in Azure Databricks notebook.
Is there a better way i can achieve this.</p>
<p>My Azure Databricks runtime version: 6.3 (includes Apache Spark 2.4.4, Scala 2.11)</p>
<pre><code>INSERT OVERWRITE TABLE TABLE_NAME SELECT * FROM TABLE_NAME ORDER BY dlloaddate desc;
</code></pre>
<p>Error:</p>
<pre><code>com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Cannot overwrite a path that is also being read from.;
at org.apache.spark.sql.execution.command.DDLUtils$.verifyNotReadPath(ddl.scala:962)
at org.apache.spark.sql.execution.datasources.DataSourceAnalysis$$anonfun$apply$1.applyOrElse(DataSourceStrategy.scala:194)
at org.apache.spark.sql.execution.datasources.DataSourceAnalysis$$anonfun$apply$1.applyOrElse(DataSourceStrategy.scala:136)
at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)
at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)
at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)
at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:107)
at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:106)
at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsDown(AnalysisHelper.scala:106)
at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperators(AnalysisHelper.scala:73)
at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)
at org.apache.spark.sql.execution.datasources.DataSourceAnalysis.apply(DataSourceStrategy.scala:136)
at org.apache.spark.sql.execution.datasources.DataSourceAnalysis.apply(DataSourceStrategy.scala:54)
at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:112)
at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:109)
at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
at scala.collection.immutable.List.foldLeft(List.scala:84)
at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:109)
at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:101)
at scala.collection.immutable.List.foreach(List.scala:392)
at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:101)
at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:137)
at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:131)
at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:103)
at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)
at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)
at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:79)
at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:115)
at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)
at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)
at org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)
at org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:83)
at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)
at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)
at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)
at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:696)
at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:716)
at com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:88)
at com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:34)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.immutable.List.foreach(List.scala:392)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
at scala.collection.immutable.List.map(List.scala:296)
at com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:34)
at com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:141)
at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:385)
at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:362)
at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:251)
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:246)
at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)
at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:288)
at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)
at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:362)
at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)
at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)
at scala.util.Try$.apply(Try.scala:192)
at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)
at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)
at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)
at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)
at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)
at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)
at java.lang.Thread.run(Thread.java:748)

at com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:126)
at com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:141)
at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:385)
at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:362)
at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:251)
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:246)
at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)
at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:288)
at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)
at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:362)
at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)
at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)
at scala.util.Try$.apply(Try.scala:192)
at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)
at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)
at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)
at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)
at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)
at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)
at java.lang.Thread.run(Thread.java:748)
</code></pre>",1,1,2020-07-07 06:32:56.940000 UTC,,,0,hive|apache-spark-sql|azure-databricks,173,2019-11-07 05:42:55.603000 UTC,2021-07-05 13:51:53.573000 UTC,"Bangalore, Karnataka, India",263,19,0,68,,,,,,[]
Structured Streaming vs Batch Performance differences,"<p>We have a job that aggregates data over time windows. We're new to spark,
and we observe significantly different performance characteristics for running
the logically same query as a streaming vs a batch job. We want to understand
what's going on and find possible ways to improve the speed of the structured
streaming based approach.</p>
<p>For the sake of this post, suppose the schema is</p>
<pre><code>root
 |-- objectId: long (nullable = true)
 |-- eventTime: long (nullable = true)
 |-- date: date (nullable = true)
 |-- hour: integer (nullable = true)
</code></pre>
<p>where</p>
<ul>
<li><code>date</code> and <code>hour</code> are (derived) partition keys, i.e. parquet files are stored in
folders like <code>date=2020-07-26/hour=4</code>.</li>
<li>the underlying format type is a <em>delta lake</em>.</li>
<li>an hour of data has about 200 million events</li>
<li><code>objectId</code> is  widely spread (10 million distinct values observed in an hour,
very uneven distribution)</li>
<li>we're trying to count the number of events per <code>objectId</code>, in 5 minute buckets</li>
<li>the underlying source is streamed to from a kafka queue (and runs every minute)
<ul>
<li>two new files appear on the ADL2 every minute, size is 25MB each (actual file
contains some 10 additional columns that are not shown above)</li>
</ul>
</li>
</ul>
<p>We're running a structured streaming job basically doing:</p>
<pre class=""lang-scala prettyprint-override""><code>df.read.format(&quot;delta&quot;)
  .withWatermark(&quot;7 minutes&quot;) // watermark only applied to streaming query
  .groupBy($&quot;date&quot;, $&quot;hour&quot;, $&quot;objectId&quot;, window($&quot;eventTime&quot;, &quot;5 minutes&quot;))
  .coalesce(1) // debatable; we like limited number of files
  .partitionBy(&quot;date&quot;, &quot;hour&quot;)
  .writeStream
  .format(&quot;delta&quot;)
  .option(&quot;checkpointLocation&quot;, &lt;...&gt;)
  .partitionBy(&quot;date&quot;, &quot;hour&quot;)
  .start(&lt;destination url&gt;)
  .awaitTermination
</code></pre>
<p>The associated batch job basically does the same thing with the exception of
<code>withWatermark</code> and comparable replacements for <code>writeStream</code> etc. It reads from
exactly the same source, so it will read exactly the same files, with the same
size etc.</p>
<p>We are running these on:</p>
<ul>
<li>azure databricks</li>
<li>azure data lake gen 2</li>
</ul>
<p>Observations:</p>
<ul>
<li>the batch job is able to aggregate one hour in about one minute, running
on the smallest possible cluster (3x F4s)</li>
<li>the structured streaming job OOMs, even with (3x DS3_v2), so we had to
configure larger instances (3x L4s, 32GB per node)
<ul>
<li>CPUs are practically idle (97.4% idle)</li>
<li>each micro batch takes 30-60s (almost exclusively spent in <code>addBatch</code>)</li>
<li>low network activity (maybe 2MB / s)</li>
</ul>
</li>
<li>generally, I have the feeling that the streaming job wouldn't be able to
hold up when data intake increases (we're planning for 10x as much traffic)</li>
</ul>
<p>My understanding is that the streaming query, given the watermark (7 minutes)
and the window size (5 minutes) only has to look back for less than 15 minutes,
until it can write out a 5 minute window and discard all associated state.</p>
<p>Questions:</p>
<ul>
<li>why does the structured streaming based solution needs so much more memory?
<ul>
<li>assuming we have to maintain state for some 10 million entries,
I don't see how we could need that much</li>
</ul>
</li>
<li>what could cause the high processing time for the streaming job, given that it
sits idle?</li>
<li>what kind of metrics should I look at (spark newbie here)?</li>
</ul>",1,0,2020-07-26 17:59:30.590000 UTC,1.0,2020-07-27 14:44:32.803000 UTC,3,apache-spark|spark-structured-streaming|azure-databricks,621,2009-12-19 21:30:16.373000 UTC,2022-03-04 13:20:52.483000 UTC,"Jena, Germany",91,63,0,20,,,,,,[]
Databricks connect & PyCharm & remote SSH connection,"<p>Hey StackOverflowers!</p>
<p>I run into a problem.</p>
<p>I have set up PyCharm to be connected with an (azure) VM through SSH connection.</p>
<ol>
<li><p>So first i make the configuration for the ssh connection
<a href=""https://i.stack.imgur.com/MYatT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MYatT.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>I set up the mappings</p>
</li>
<li><p>I create a conda enviroment by spining up a terminal in the vm and then I download and connect to databricks-connect. I test it on the terminal and it works fine.</p>
</li>
<li><p>I set up the console on the pycharm configurations
<a href=""https://i.stack.imgur.com/wJBst.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wJBst.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
<p>But when I try to run the spark session (spark = SparkSession.builder.getOrCreate()), databricks-connect searches for the .databricks-connect file in the wrong folder and gives me the following error:</p>
<p><code>Caused by: java.lang.RuntimeException: Config file /root/.databricks-connect not found. Please run </code>databricks-connect configure<code> to accept the end user license agreement and configure Databricks Connect. A copy of the EULA is provided below: Copyright (2018) Databricks, Inc.</code></p>
<p>and the full error + some warnings.</p>
<pre><code>20/07/10 17:23:05 WARN Utils: Your hostname, george resolves to a loopback address: 127.0.0.1; using 10.0.0.4 instead (on interface eth0)
20/07/10 17:23:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
20/07/10 17:23:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to &quot;WARN&quot;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).

Traceback (most recent call last):
  File &quot;/anaconda/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py&quot;, line 3331, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File &quot;&lt;ipython-input-2-23fe18298795&gt;&quot;, line 1, in &lt;module&gt;
    runfile('/home/azureuser/code/model/check_vm.py')
  File &quot;/home/azureuser/.pycharm_helpers/pydev/_pydev_bundle/pydev_umd.py&quot;, line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File &quot;/home/azureuser/.pycharm_helpers/pydev/_pydev_imps/_pydev_execfile.py&quot;, line 18, in execfile
    exec(compile(contents+&quot;\n&quot;, file, 'exec'), glob, loc)
  File &quot;/home/azureuser/code/model/check_vm.py&quot;, line 13, in &lt;module&gt;
    spark = SparkSession.builder.getOrCreate()
  File &quot;/anaconda/envs/py37/lib/python3.7/site-packages/pyspark/sql/session.py&quot;, line 185, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
  File &quot;/anaconda/envs/py37/lib/python3.7/site-packages/pyspark/context.py&quot;, line 373, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File &quot;/anaconda/envs/py37/lib/python3.7/site-packages/pyspark/context.py&quot;, line 137, in __init__
    conf, jsc, profiler_cls)
  File &quot;/anaconda/envs/py37/lib/python3.7/site-packages/pyspark/context.py&quot;, line 199, in _do_init
    self._jsc = jsc or self._initialize_context(self._conf._jconf)
  File &quot;/anaconda/envs/py37/lib/python3.7/site-packages/pyspark/context.py&quot;, line 312, in _initialize_context
    return self._jvm.JavaSparkContext(jconf)
  File &quot;/anaconda/envs/py37/lib/python3.7/site-packages/py4j/java_gateway.py&quot;, line 1525, in __call__
    answer, self._gateway_client, None, self._fqn)
  File &quot;/anaconda/envs/py37/lib/python3.7/site-packages/py4j/protocol.py&quot;, line 328, in get_return_value
    format(target_id, &quot;.&quot;, name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.lang.ExceptionInInitializerError
    at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:99)
    at org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:61)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
    at py4j.Gateway.invoke(Gateway.java:250)
    at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
    at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
    at py4j.GatewayConnection.run(GatewayConnection.java:251)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: Config file /root/.databricks-connect not found. Please run `databricks-connect configure` to accept the end user license agreement and configure Databricks Connect. A copy of the EULA is provided below: Copyright (2018) Databricks, Inc.
This library (the &quot;Software&quot;) may not be used except in connection with the Licensee's use of the Databricks Platform Services pursuant to an Agreement (defined below) between Licensee (defined below) and Databricks, Inc. (&quot;Databricks&quot;). This Software shall be deemed part of the “Subscription Services” under the Agreement, or if the Agreement does not define Subscription Services, then the term in such Agreement that refers to the applicable Databricks Platform Services (as defined below) shall be substituted herein for “Subscription Services.”  Licensee's use of the Software must comply at all times with any restrictions applicable to the Subscription Services, generally, and must be used in accordance with any applicable documentation. If you have not agreed to an Agreement or otherwise do not agree to these terms, you may not use the Software.  This license terminates automatically upon the termination of the Agreement or Licensee's breach of these terms.
Agreement: the agreement between Databricks and Licensee governing the use of the Databricks Platform Services, which shall be, with respect to Databricks, the Databricks Terms of Service located at www.databricks.com/termsofservice, and with respect to Databricks Community Edition, the Community Edition Terms of Service located at www.databricks.com/ce-termsofuse, in each case unless Licensee has entered into a separate written agreement with Databricks governing the use of the applicable Databricks Platform Services. Databricks Platform Services: the Databricks services or the Databricks Community Edition services, according to where the Software is used.
Licensee: the user of the Software, or, if the Software is being used on behalf of a company, the company.
To accept this agreement and start using Databricks Connect, run `databricks-connect configure` in a shell.
    at com.databricks.spark.util.DatabricksConnectConf$.checkEula(DatabricksConnectConf.scala:41)
    at org.apache.spark.SparkContext$.&lt;init&gt;(SparkContext.scala:2679)
    at org.apache.spark.SparkContext$.&lt;clinit&gt;(SparkContext.scala)
    ... 13 more
</code></pre>
<p>However, I do not have access rights to that folder so I can not drop there the databricks connect file.</p>
<p>What is also strange is that if I run in  : Pycharm -&gt; ssh terminal -&gt; activate conda env -&gt; python the following</p>
<p>Is it a way to either:</p>
<pre><code>1. Point out to java where the databricks-connect file is

2. Configure databricks-connect in another way throughout the script or enviromental variables inside pycharm

3. Other way? 

or do I miss something?
</code></pre>",3,1,2020-07-10 18:03:24.563000 UTC,,,2,python|ssh|pycharm|databricks-connect,1456,2016-05-17 16:17:16.743000 UTC,2022-03-03 14:09:09.147000 UTC,"Rotterdam, Netherlands",1255,310,21,156,,,,,,[]
How to get job run result logs of notebook in databricks using Python or Pyspark or scala,<p>I have to get the job run results logs of notebook in databricks using python or pyspark or scala and write those log details to a file. I'm not able to get these logs.Can someone help me on this?</p>,1,0,2019-04-01 14:49:15.610000 UTC,,,1,python-3.x|pyspark|azure-data-lake|azure-databricks,2019,2018-10-04 16:30:46.120000 UTC,2021-02-25 15:56:54.010000 UTC,,113,3,0,83,,,,,,[]
What GIT is and what it is not.. version control?,"<p>Ok so I am having trouble understanding what git does or what it does not do. </p>

<p>When I hear version control, I think I no longer have to store different files on my computer of basically the same code with minor changes.</p>

<p>For example if I have a C file where I am generating a PWM with timer1
but then I want to test it with timer2. I would normally either comment code out and try it. Or I would copy the C file and modify it and not worry about breaking my working code. Then I would go on and try other things again always having my original file and working code.</p>

<p>Now with git I think I can have my code and do all these commits and if I feel like going back to the original version it will be there nice and safe, hence version control. But I am trying some things out with an online tutorial on codecademy.</p>

<p>I have a local repo which I sent to github. 
I have a read me file and i edit it and send to git hub and it shows up. Then I did more changes and send it again.</p>

<p>Then I did a reset using the commit sha..and i tried pushing that and it said that the head of my local doesnt match the remote. and then I check the read me file and I was expecting it to revert to my version 1 and it did not... so I do not have these multiple versions of my file?  </p>

<p>So I am not understanding the version control part... can I have multiple versions of a file and git keeps track of all the versions and the ability to revert my files back to version 1 for example...or am I missing something here?</p>

<p>Because what i thought this did was:</p>

<ol>
<li><p>I have a file</p></li>
<li><p>I modify my file</p></li>
<li>I use git to have version control</li>
<li>I edit my file and git will somehow index this as version 2 and it will have a back up version of my version 1.</li>
<li>I keep editing and committing and git keeps track of all the changes to my file and I can always go back to an old version of my file. </li>
</ol>

<p>I thought it was amazing how it did this with out just copying my file which is what I did but it seems to me we are not there yet and this is not what git does.</p>",2,2,2018-12-22 01:57:12.887000 UTC,,2018-12-22 11:12:01.257000 UTC,2,git|version-control|version|dvcs,467,2015-03-11 20:58:34.270000 UTC,2022-02-26 23:51:36.267000 UTC,,143,2,0,15,,,,,,[]
Create a function in Databricks notebook for spark.sql,"<p>I am trying to create a function in scala that would write a log message to delta table.
example</p>
<pre><code>def logMessage(message: String): Unit = {
spark.sql(&quot;INSERT INTO tablename values ('${message}')&quot;)
}
</code></pre>
<p>The problem is that spark.sql will return DataFrame, but in my notebook I want to execute this method in various catch blocks to log error. Obviously the return type does not match. How to write this function that would return unit and I could use it in multiple catch blocks ?</p>",0,1,2021-10-28 10:45:42.860000 UTC,,,1,scala|function|apache-spark|azure-databricks,20,2015-01-22 19:39:54.813000 UTC,2022-03-04 16:31:11.630000 UTC,,89,0,0,24,,,,,,[]
Neptune: feature similar to Alias switching in ElasticSearch,"<p>I have a Elasticsearch and Neptune database, only graph relations are store in Neptune.</p>
<p>I'm doing &quot;Alias Switching&quot; in ES, is there a feature like this in Neptune?</p>",0,3,2022-02-15 18:15:19.287000 UTC,,,0,amazon-web-services|amazon-neptune,12,2012-02-03 16:20:42.710000 UTC,2022-03-04 16:51:03.243000 UTC,,5628,119,6,508,,,,,,[]
How to edit specific hunk of MQ patch?,"<p>My use case:<br>
I noticed that I made unnecessary edit in earlier revision and I want to discard one hunk from patch, and preserve all other changes for this file. 
I've tried to edit the patch inside <code>.hg/patches/</code> and then <code>hg qrefresh</code>, but after that the hunk reappear. </p>

<p>Possible solutions:</p>

<ul>
<li>Export that patch, cut all except unneeded hunk, and apply this patch in reverse.</li>
<li>Somehow make workdir ""dirty"" with patch that contains hunk, and then edit file nicely with GUI of my IDE, which polls VCS and highlights changes, and offer option to rever these changes.</li>
</ul>

<p>Questions:</p>

<ul>
<li>Is editing <code>.hg/patches</code> a valid practice?</li>
<li>How to make workdir dirty by neede patch and then reapply it with edits? Like <code>git reset --soft</code>?</li>
<li>Is there any other convenient methods to do subj?</li>
</ul>",1,0,2014-01-31 12:20:22.633000 UTC,,2014-01-31 14:38:20.753000 UTC,1,version-control|mercurial|dvcs|mercurial-queue,66,2012-11-12 16:07:20.193000 UTC,2022-03-03 12:33:42.817000 UTC,,10532,754,9,2794,,,,,,[]
Enforce tagging to azure databricks workspace,"<p>My aim is to enforce the developers to provide tags while they create clusters. I have added a policy in my ARM template which creates an azure workspace. It successfully completes the validation but fails to deploy.Sorry if my question is vague. Please help</p>

<pre><code>{  
   ""$schema"":""https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#"",
   ""contentVersion"":""1.0.0.0"",
""parameters"":{  
  ""workspaceName"":{  
   ""defaultValue"":""xyxy"",


   ""type"":""String"",
     ""metadata"":{  
        ""description"":""The name of the Azure Databricks workspace to create.""
     }
  },
  ""subscriptionName"":{  
     ""allowedValues"":[  
        ""yy"",
        ""xx""
     ],
     ""type"":""String"",
     ""metadata"":{  
        ""description"":""Specifies the subscription in which to create the workspace.""
     }
  },
  ""resourceGroup"":{  
     ""defaultValue"":""abc"",
     ""allowedValues"":[  
        ""dd"",
        ""bb"",
        ""abc""
     ],
     ""type"":""String"",
     ""metadata"":{  
        ""description"":""Resource group in which to create the workspace.""
     }
  },
  ""pricingTier"":{  
     ""defaultValue"":""premium"",
     ""allowedValues"":[  
        ""standard"",
        ""premium""
     ],
     ""type"":""String"",
     ""metadata"":{  
        ""description"":""The pricing tier of workspace.""
     }
  },
  ""location"":{  
     ""defaultValue"":""east us"",
     ""type"":""String"",
     ""metadata"":{  
        ""description"":""Location for all resources.""
     }
  }
   },
""variables"":{  
  ""managedResourceGroupName"":""[concat('databricks-rg-', parameters('workspaceName'), '-', uniqueString(parameters('workspaceName'), resourceGroup().id))]""
   },
   ""resources"":[  
  {  
     ""type"":""Microsoft.Databricks/workspaces"",
     ""apiVersion"":""2018-04-01"",
     ""name"":""[parameters('workspaceName')]"",
     ""location"":""[parameters('location')]"",
     ""sku"":{  
        ""name"":""[parameters('pricingTier')]""
     },
     ""properties"":{  
        ""displayName"":""Enforce tag and its value"",
        ""policyType"":""BuiltIn"",
        ""ManagedResourceGroupId"":""[concat(subscription().id, '/resourceGroups/', variables('managedResourceGroupName'))]"",
        ""description"":""Enforces a required tag and its value."",
        ""parameters"":{  
           ""tagName"":{  
              ""type"":""String"",
              ""metadata"":{  
                 ""description"":""Name of the tag, such as costCenter""
              }
           },
           ""tagValue"":{  
              ""type"":""String"",
              ""metadata"":{  
                 ""description"":""Value of the tag, such as headquarter""
              }
           }
        },
        ""policyRule"":{  
           ""if"":{  
              ""not"":{  
                 ""field"":""[concat('tags[', parameters('tagName'), ']')]"",
                 ""equals"":""[parameters('tagValue')]""
              }
           },
           ""then"":{  
              ""effect"":""deny""
           }
        }
     },
     ""outputs"":{  
        ""workspace"":{  
           ""type"":""Object"",
           ""value"":""[reference(resourceId('Microsoft.Databricks/workspaces', parameters('workspaceName')))]""
        }
     }
  }
   ]
}
</code></pre>

<p>Error: 
Unable to process template language expressions for resource '/subscriptions/04jdmgb-5642-8640-9a15-a0504248340f/resourceGroups/abc/providers/Microsoft.Databricks/workspaces/test12' at line '73' and column '9'. 'The template parameter 'tagName' is not found. Please see <a href=""https://aka.ms/arm-template/#parameters"" rel=""nofollow noreferrer"">https://aka.ms/arm-template/#parameters</a> for usage details.' Click here for details
Your deployment faile</p>",1,0,2019-07-31 13:09:13.567000 UTC,,,1,azure|azure-databricks,766,2019-07-31 12:40:00.517000 UTC,2022-02-24 11:14:33.060000 UTC,"Pune, Maharashtra, India",33,0,0,6,,,,,,[]
Azure Data Lake Store as EXTERNAL TABLE in Databricks with multiple PATH?,"<p>I am trying to create external tables shown below
Path for the table is dynamic, can external table accept multiple path?</p>
<pre><code>CREATE TABLE tablename
(BusinessDate string,
StoreNumber string)
 USING csv
 OPTIONS ('DELIMITER' '~', 
PATH &quot;/mnt/raw/2021/08/19/store01.txt,/mnt/raw/2021/08/17/store09.txt&quot;)
</code></pre>",1,1,2021-08-19 16:13:04.840000 UTC,,2021-08-19 17:46:22.973000 UTC,0,azure|azure-databricks,39,2021-08-19 16:09:18.907000 UTC,2021-08-25 12:24:16.740000 UTC,,1,0,0,1,,,,,,[]
Log insertion to log-table after deleting the file in Azure Blob Storaage - Databricks,"<p>I try to monitor my blob storage. I want to inserd log to the log table after each change to the blob storage (e.g. updating files, deleting, creating, moving files etc.). I want to get information about the user, date, etc. who made the change. I read a lot of topics but didn't find anything.</p>",1,0,2021-10-06 10:04:35.697000 UTC,,,-2,python-3.x|azure|logging|azure-blob-storage|azure-databricks,63,2021-10-06 09:01:25.090000 UTC,2022-02-09 12:47:13.773000 UTC,,1,0,0,2,,,,,,[]
DynamoDB table design for social network,"<p>I got a thinking-problem in DynamoDB.
My structure is looking as following: </p>

<ul>
<li><p>primary key = ""id""</p></li>
<li><p>sort key = ""sort""
I have posts, users and ""user A following user B"" relationships.</p></li>
</ul>

<p><strong>Users:</strong></p>

<ul>
<li>id=1234</li>
<li>sort=""USER_USER_1234""</li>
<li>name=""max"" (for example)</li>
</ul>

<p>-</p>

<ul>
<li>id=3245</li>
<li>sort=""USER_USER_3245""</li>
<li>name=""tom""</li>
</ul>

<p><strong>Post:</strong></p>

<ul>
<li><p>id=9874</p></li>
<li><p>sort=""POST_POST_1234 (because its created by user id 1234)</p></li>
<li><p>createdAt=1560371687</p></li>
</ul>

<p><strong>Following:</strong></p>

<ul>
<li><p>id=1234</p></li>
<li><p>sort=""USER_FOLLOW_3245"" </p></li>
</ul>

<p>--> tom follows max (but max not tom)</p>

<p>How could I design a query to get all posts by the people which tom(id=3245) is following? So in my case the post id 9874?
My approach was to put a GSI where sort is the primary key and id is the sort key (that i can query all people which user A is following), than get all the posts from the users (with help of the same GSI) and sort the result after a second index where createdAt is the sort key. The problem is that this needs much much querys (imagine user A would follow 10000 people and they all make posts). Is there a technique or design thinking approach which you could recommend for this situation? My second approach was to index the whole application table to elastic search and do a nested query. Would this make more sense? Or would you recommend using another type of database like AWS neptune?</p>",2,0,2019-06-12 20:52:01.123000 UTC,,2019-06-13 06:40:37.680000 UTC,1,amazon-web-services|amazon-dynamodb|dynamodb-queries|amazon-neptune,1930,2017-01-15 22:34:12.493000 UTC,2021-07-12 20:59:48.317000 UTC,,305,30,0,37,,,,,,[]
Should I add the dll of my third libraries in the version control repository?,"<p>Version control Best practices.
When developing a program, I use third party libraries, NUnit and others.
I want to share the sources of this program hosted on <a href=""http://www.codeplex.com/"" rel=""nofollow"">http://www.codeplex.com/</a> or <a href=""http://code.google.com/hosting/"" rel=""nofollow"">http://code.google.com/hosting/</a>.</p>

<p>What are good practices as regards third libraries?
Should I add the dll of my third libraries in the version control ?</p>

<p>Thank you,</p>",2,1,2011-03-18 11:26:09.827000 UTC,1.0,2011-06-07 05:52:12.743000 UTC,1,version-control|dll|mercurial|dvcs|nuget,211,2009-08-30 17:41:41.300000 UTC,2014-02-25 14:08:25.357000 UTC,,551,3,0,33,,,,,,[]
How to convert Json array list with multiple possible values into columns in a dataframe using pyspark,"<p>I am using the Google Admin Report API via the Python SDK in Databricks (Spark + Python 3.5).</p>

<p>It returns data in the following format (Databricks pyspark code): </p>

<pre><code>dbutils.fs.put(""/tmp/test.json"", '''{
    ""userEmail"": ""rod@test.com"", 
    ""parameters"": [
        {
            ""intValue"": ""0"",
            ""name"": ""classroom:num_courses_created""
        },
        {
            ""boolValue"": true,
            ""name"": ""accounts:is_disabled""
        },
        {
            ""name"": ""classroom:role"",
            ""stringValue"": ""student""
        }
    ]
}''', True)
</code></pre>

<p>There are 188 parameters and for each param it could be an int, bool, date or string. Depending on the field type the Api returns the value in the appropriate value (e.g. intValue for an int field and boolValue for a boolean).</p>

<p>I am writing out this JSON untouched into my datalake and processing it later by loading it into a spark dataframe:</p>

<pre><code>testJsonData = sqlContext.read.json(""/tmp/test.json"", multiLine=True)
</code></pre>

<p>This results in a dataframe with this schema:</p>

<ul>
<li>userEmail:string </li>
<li>parameters:array 

<ul>
<li>element:struct 

<ul>
<li>boolValue:boolean</li>
<li>intValue:string</li>
<li>name:string</li>
<li>stringValue:string</li>
</ul></li>
</ul></li>
</ul>

<p>If I display the dataframe it shows as</p>

<p>{""boolValue"":null,""intValue"":""0"",""name"":""classroom:num_courses_created"",""stringValue"":null}
        {""boolValue"":true,""intValue"":null,""name"":""accounts:is_disabled"",""stringValue"":null}<br>
        {""boolValue"":null,""intValue"":null,""name"":""classroom:role"",""stringValue"":""student""}</p>

<p>As you can see, it has inferred nulls for the typeValues that do not exist.</p>

<p>The end state that I want is columns in a dataframe like:</p>

<p><a href=""https://i.stack.imgur.com/Q9CKC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q9CKC.png"" alt=""required result""></a></p>

<p>and the pivoted columns would be typed correctly (e.g classroom:num_courses_created would be of type int - see yellow columns above)</p>

<p>Here is what I have tried so far:</p>

<pre><code>from pyspark.sql.functions import explode
tempDf = testJsonData.select(""userEmail"", explode(""parameters"").alias(""parameters_exploded""))
explodedColsDf = tempDf.select(""userEmail"", ""parameters_exploded.*"")
</code></pre>

<p>This results in a dataframe with this schema:</p>

<ul>
<li>userEmail:string</li>
<li>boolValue:boolean</li>
<li>intValue:string</li>
<li>name:string</li>
<li>stringValue:string</li>
</ul>

<p>I then pivot the rows into columns based on the Name field (which is """"classroom:num_courses_created"", ""classroom:role"" etc (there are 188 name/value parameter pairs):</p>

<pre><code>#turn intValue into an Int column
explodedColsDf = explodedColsDf.withColumn(""intValue"", explodedColsDf.intValue.cast(IntegerType()))
pivotedDf = explodedColsDf.groupBy(""userEmail"").pivot(""name"").sum(""intValue"")
</code></pre>

<p>Which results in this dataframe:</p>

<ul>
<li>userEmail:string</li>
<li>accounts:is_disabled:long</li>
<li>classroom:num_courses_created:long</li>
<li>classroom:role:long</li>
</ul>

<p>which is not correct as the types for the columns are wrong.</p>

<p>What I need to do is somehow look at all the typeValues for a parameter column (there is no way of knowing the type from the name or inferring it - other than in the original Json where it returns just the typeValue that is relevant) and whichever one is not null is the type of that column. Each param only appears once so the string, bool, int and date values just need to be outputed for the email key, not aggregated.</p>

<p>This is beyond my current knowledge however I was thinking a simpler solution might be to go back all the way to the beginning and pivot the columns <em>before</em> I write out the Json so it would be in the format I want when I load it back into Spark, however I was reluctant to transform the raw data at all.  I also would prefer not to handcode the schema for 188 fields as I want to dynamically pick which fields I want so it needs to be able to handle that.</p>",1,0,2019-04-13 06:50:06.040000 UTC,1.0,2019-04-14 00:47:16.180000 UTC,1,python|json|apache-spark|pyspark|azure-databricks,1142,2009-10-21 01:51:25.500000 UTC,2022-03-05 10:11:02.900000 UTC,"Sydney, Australia",4808,272,7,518,,,,,,[]
Accessing AzureDataLake Gen2 from Databricks,"<p>I would like to read data from Azure DataLake Gen 2 from Databricks. I have an Owner role for both and read/write/execute rights. DataLake allows access from the Vnet where Databricks are added. I try two methods: Auth2 and passthrough (preferable method).</p>
<p>For the passthrough method I tried the following code in Databricks notebook and I set up all the prerequisites (<a href=""https://github.com/devlace/azure-databricks-storage/blob/master/notebooks/directconnect_adpassthrough_py.py"" rel=""nofollow noreferrer"">from this git</a>):</p>
<pre><code># MAGIC 1. Azure Databricks Premium Plan.
# MAGIC 2. Azure Data Lake Storage Gen2: Databricks Runtime 5.3 or above.
# MAGIC 3. High concurrency clusters, which support only Python and SQL. [Enabled AD Passthrough checkbox under Advanced Options](https://docs.azuredatabricks.net/spark/latest/data-sources/azure/adls-passthrough.html#enable-passthrough-for-a-cluster)
# MAGIC 4. User needs to have [Storage Data Blob Owner/Contributor/Reader role](https://docs.microsoft.com/en-us/azure/storage/common/storage-auth-aad-rbac-portal#rbac-roles-for-blobs-and-queues) OR [appropriate ACL permissions (R/W/E) on ADLA Gen2](https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control#access-control-lists-on-files-and-directories) is granted

# COMMAND ----------

# Try to access file
# You'll need data.csv at root of container/filesystem
df = spark.read.csv(&quot;abfss://&lt;STORAGE_CONTAINER&gt;@&lt;STORAGE_ACCOUNT&gt;.dfs.core.windows.net/data.csv&quot;)
display(df)
</code></pre>
<p>And the error appears: <code>&quot;abfss://&lt;STORAGE_CONTAINER&gt;@&lt;STORAGE_ACCOUNT&gt;.dfs.core.windows.net/data.csv&quot; has invalid authority</code></p>
<p>For the Auth method, I use the following code:</p>
<pre><code># Databricks notebook source
# MAGIC %md
# MAGIC ## Azure DataLake Gen2
# MAGIC 
# MAGIC Pre-requisites:
# MAGIC 1. [Create Service Principle](https://docs.microsoft.com/en-        us/azure/active-directory/develop/howto-create-service-principal-portal)
# MAGIC 1. Service Principle has [Storage Data Blob     Owner/Contributor/Reader role](https://docs.microsoft.com/en-    us/azure/storage/common/storage-auth-aad-rbac-portal#rbac-roles-for-blobs-and-    queues) OR [appropriate ACL permissions (R/W/E) on ADLA Gen2]    (https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-    access-control#access-control-lists-on-files-and-directories) is granted
# MAGIC 2. **Databricks Runtime 5.2** or above
# MAGIC 3. ADLS Gen2 storage account in the **same region** as your Azure     Databricks workspace

# COMMAND ----------

# Set spark configuration
spark.conf.set(&quot;fs.azure.account.auth.type&quot;, &quot;OAuth&quot;)
spark.conf.set(&quot;fs.azure.account.oauth.provider.type&quot;,     &quot;org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider&quot;)
spark.conf.set(&quot;fs.azure.account.oauth2.client.id&quot;, &quot;    &lt;SERVICE_PRINCIPLE_CLIENT_ID&gt;&quot;)
spark.conf.set(&quot;fs.azure.account.oauth2.client.secret&quot;, &quot;    &lt;SERVICE_PRINCIPLE_SECRET&gt;&quot;)
spark.conf.set(&quot;fs.azure.account.oauth2.client.endpoint&quot;,     &quot;https://login.microsoftonline.com/&lt;DIRECTORY_TENANT_ID&gt;/oauth2/token&quot;)

# COMMAND ----------

# Try to access file
# You'll need data.csv at root of container/filesystem
df =     spark.read.csv(&quot;abfss://&lt;STORAGE_CONTAINER&gt;@&lt;STORAGE_ACCOUNT&gt;.dfs.core.windows    .net/data.csv&quot;)
display(df)
</code></pre>
<p>An the error is <code>STORAGE_ACCOUNT.dfs.core.windows.net/STORAGE_CONTAINER//?action=getAccessControl&amp;amp;timeout=90</code></p>
<p>What could be wrong here? I have read/write/execute rights.</p>
<p>Do I identify correctly STORAGE_ACCOUNT and STORAGE_CONTAINER?</p>
<p>I have the following file system (example):</p>
<p><img src=""https://i.stack.imgur.com/h0xpE.png"" alt=""enter image description here"" /></p>
<p>For example, I would like to read data.csv in shared-read-and-write.
Would this code correct for that:</p>
<pre><code> df =     spark.read.csv(&quot;abfss://&lt;STORAGE_CONTAINER&gt;@&lt;STORAGE_ACCOUNT&gt;.dfs.core.windows    .net/data.csv&quot;)
</code></pre>
<p>-&gt;</p>
<pre><code> df =     spark.read.csv(&quot;abfss://shared-read-and-write@DATALAKE GEN2_NAME.dfs.core.windows.net/shared-read-and-write/data.csv&quot;)
</code></pre>",1,0,2019-07-16 08:52:45.013000 UTC,2.0,2020-11-08 00:45:14.340000 UTC,0,azure|azure-data-lake|azure-databricks,1857,2017-12-19 12:36:29.963000 UTC,2020-11-19 15:39:41.670000 UTC,,23,7,0,17,,,,,,[]
What is the state of Bazaar version control?,"<p>I am looking to start a project with distributed source control and am evaluating the different options. Looking at Bazaar, I stumbled over several articles (e.g. <a href=""http://stationary-traveller.eu/pages/bzr-a-retrospective.html"">this</a>) that development has slowed down significantly.</p>

<p>What is the status of the project bazaar? Are bugs getting fixed?</p>",1,5,2013-02-17 22:22:00.257000 UTC,4.0,,18,version-control|dvcs|bazaar,4115,2010-05-23 21:25:42.847000 UTC,2021-10-17 10:20:18.053000 UTC,,6768,207,12,359,,,,,,[]
"Git - fetch+merge on feature branch, rebase on master","<p>I have spent the last half hour reading up on <code>merge</code> vs <code>rebase</code> at
Stack Overflow. As I understand things, <code>rebase</code> is good because it maintains
linearity, but can be confusing for collaborators because it alters history.</p>

<p>So, would it be fair to deduce that</p>

<ol>
<li><p>On the feature branch, use fetch+merge to maintain history and avoid
confusion amongst collaborators. The clutter doesn't matter since it'll be
deleted after pushing to master anyway.</p></li>
<li><p>On the master branch, use rebase to avoid clutter. Modifying history doesn't
matter because the feature branch gets pushed out as a single commit.</p></li>
</ol>",2,1,2013-05-12 04:39:58.483000 UTC,,2013-05-12 04:42:57.117000 UTC,0,git|version-control|github|dvcs,182,2012-05-04 18:38:19.703000 UTC,2022-02-27 00:51:19.087000 UTC,,11557,894,5,643,,,,,,[]
Optimizing conversion between PySpark and pandas DataFrames,"<p>I have a pyspark dataframe of 13M rows and I would like to convert it to a pandas dataframe. The dataframe will then be resampled for further analysis at various frequencies such as 1sec, 1min, 10 mins depending on other parameters.</p>
<p>From literature [<a href=""https://docs.databricks.com/spark/latest/spark-sql/spark-pandas.html"" rel=""nofollow noreferrer"">1</a>, <a href=""https://towardsdatascience.com/how-to-efficiently-convert-a-pyspark-dataframe-to-pandas-8bda2c3875c3"" rel=""nofollow noreferrer"">2</a>] I have found that using either of the following lines can speed up conversion between pyspark to pandas dataframe:</p>
<pre><code>spark.conf.set(&quot;spark.sql.execution.arrow.pyspark.enabled&quot;, &quot;true&quot;) 
spark.conf.set(&quot;spark.sql.execution.arrow.enabled&quot;, &quot;true&quot;)
</code></pre>
<p>However I am have been unable to see any improvement in performance during the dataframe conversion. All the columns are strings to ensure they are compatible with the PyArrow.  In the examples below the time taken is always between 1.4 and 1.5 minutes:</p>
<p><a href=""https://i.stack.imgur.com/vAi9N.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vAi9N.png"" alt=""enter image description here"" /></a></p>
<p>I have seen example where the change in processing time reduced from seconds to milliseconds [<a href=""https://blog.clairvoyantsoft.com/optimizing-conversion-between-spark-and-pandas-dataframes-using-apache-pyarrow-9d439cbf2010"" rel=""nofollow noreferrer"">3</a>]. I would like to know what I am doing wrong and how to optimize the code further. Thank you.</p>",2,6,2021-11-19 12:57:32.377000 UTC,,,0,pandas|pyspark|apache-spark-sql|azure-databricks|pyarrow,59,2015-07-27 14:27:36.327000 UTC,2022-03-06 01:17:03.970000 UTC,UK,138,261,1,29,,,,,,[]
Problem on saving Spark timestamp into Azure Synapse datetime2(7),"<p>I have a database in Azure synapse with only one column with datatype datetime2(7).
In Azure Databricks I have a table with the following schema.</p>
<pre><code>df.schema
StructType(List(StructField(dates_tst,TimestampType,true)))
</code></pre>
<p>When I try to save on Synapse, I get an error message</p>
<p><code>Py4JJavaError: An error occurred while calling o535.save.: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 15.0 failed 4 times, most recent failure: Lost task 3.3 in stage 15.0 (TID 46) (10.139.64.5 executor 0): com.microsoft.sqlserver.jdbc.SQLServerException: 110802;An internal DMS error occurred that caused this operation to fail</code>
<code>SqlNativeBufferBufferBulkCopy.WriteTdsDataToServer, error in OdbcDone: SqlState: 42000, NativeError: 4816, 'Error calling: bcp_done(this-&gt;GetHdbc()) | SQL Error Info: SrvrMsgState: 1, SrvrSeverity: 16,  Error &lt;1&gt;: ErrorMsg: [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Invalid column type from bcp client for colid 1. | Error calling: pConn-&gt;Done() | state: FFFF, number: 75205, active connections: 35', Connection String: Driver={pdwodbc17e};app=TypeD00-DmsNativeWriter:DB2\mpdwsvc (56768)-ODBC;autotranslate=no;trusted_connection=yes;server=\\.\pipe\DB.2-e2f5d1c1f0ba-0\sql\query;database=Distribution_24</code></p>
<p>EDIT: <strong>Runtime version 9.1 LTS (includes Apache Spark 3.1.2, Scala 2.12)</strong></p>
<p>EDIT 2:
It could be solved, the errors were:</p>
<ul>
<li>use incorrect format in write options, I was using &quot;com.microsoft.sqlserver.jdbc.spark&quot; and changed it to &quot;com.databricks.spark.sqldw&quot;.</li>
<li>There were also errors in the scope credentials</li>
</ul>",1,0,2022-01-31 19:31:51.683000 UTC,,2022-02-12 16:52:44.147000 UTC,1,azure|apache-spark|azure-databricks|azure-synapse,91,2022-01-13 21:53:16.433000 UTC,2022-02-19 22:00:01.330000 UTC,,13,0,0,1,,,,,,[]
How to extract Azure Application Insights Events using Pyspark?,"<p>I'm trying to capture Azure Application Insights event in structured format using the below code in Pyspark (Azure Databricks) -</p>
<pre><code>import requests
import json

appId = &quot;...&quot;
appKey = &quot;...&quot;

query = &quot;&quot;&quot;traces | where timestamp &gt; ago(1d) | order by timestamp&quot;&quot;&quot;
params = {&quot;query&quot;: query}
headers = {'X-Api-Key': appKey}
url = f'https://api.applicationinsights.io/v1/apps/{appId}/query'
response = requests.get(url, headers=headers, params=params)
logs = json.loads(response.text)

json = json.dumps(logs)

jsonRDD = sc.parallelize([json])
df = spark.read.option('multiline', &quot;true&quot;).json(jsonRDD)

display(df) 
</code></pre>
<p>However, for some reason, this is returning json structure only. How to convert this into the structured or tabular format ? <br></p>
<p>Please help!</p>",1,2,2021-09-24 10:21:04.137000 UTC,,2021-09-27 05:56:23.210000 UTC,0,json|python-3.x|pyspark|azure-application-insights|azure-databricks,70,2021-01-30 15:09:41.543000 UTC,2022-03-06 04:51:59.323000 UTC,,195,6,0,26,,,,,,[]
Breaking from repeat step using count of retreived vertices,"<p>I have a graph structure that looks something like the following:</p>

<pre><code>user1 -&gt; user2 -&gt; user3 -&gt; post
user1 -&gt; user4 -&gt; user5 -&gt; post2
user1 -&gt; user4 -&gt; user6 -&gt; post3
user1 -&gt; user5 -&gt; user7 -&gt; post4
user1 -&gt; user5 -&gt; user6 -&gt; post5
</code></pre>

<p>I would like to create a traversal which can search N edges deep from <code>user1</code> and then get posts on the user vertices at depth N. This would be usually be straight forward. But I have the following conditions: </p>

<ul>
<li>Be able to execute in a depth first search strategy</li>
<li>Break from traversal when a given threshold for desired amount of posts is reached</li>
<li>Ensure that for each <code>.repeat()</code> users vertices retrieved in <code>.out()</code> are random and not used in a previous <code>.repeat()</code> step</li>
</ul>

<p>These conditions were made up by me with the hopes of ensuring that query time is acceptable even with a large number of user connections and post vertices.</p>

<p>Here is where I am testing with 2 degrees deep:</p>

<pre><code>g.withSideEffect('Neptune#repeatMode', 'DFS')\
    .V(user1)\
    .repeat(__.out(""connection"").sample(1).out(""connection"").sample(1).out(""post"").limit(5).store(""posts""))\
    .until(__.select(""post"").count().is_(50))\
    .values(""name"")\
    .toList()
</code></pre>

<p>This query never returns data as it only breaks when the number of posts on a user is <code>50</code>. But here the number of posts can only ever be 5 as the store is being re-written on each repeat?
So I think I need some kind of global state/store where vertices can be stored and then evaluated in <code>.until()</code>. Is this even possible? I saw that using a <code>sack()</code> might be a solution but I couldn't get my head around it.</p>

<p>Please can someone point me in the right direction to solving this problem. I might be trying to solve this using the wrong traversal techniques - but this was the closest I could come to something that seemed to make sense.</p>

<p>Cheers!</p>",1,0,2019-11-21 20:15:34.953000 UTC,,,0,gremlin|tinkerpop|amazon-neptune,62,2015-07-15 17:52:54.750000 UTC,2021-02-23 21:13:12.060000 UTC,,2357,58,1,85,,,,,,[]
Keeping a collection of mercuial subrepos at tip,"<p>I would like to keep a collection of mercurial subprepositories updates at <code>tip</code> and this is what I've come up with. Are there any gotchas I'm not seeing if I do it this way?</p>

<h3>Backstory</h3>

<p>For many years now I have been using monolithic subversion repository <code>main</code> but I am getting ready to convert it over to mercurial. The only thing I don't like about mercurial is that I can't clone just one subdirectory. I was thinking of making each top level directory of <code>main</code> into its own mercurial repository. Then I could create one mercurial repo called <code>maincollections</code> that would reference each new mercurial repo as a subrepo. In order to get changes to any subrepo by just pulling once inside <code>maincollections</code> I set up the following. </p>

<h3>Method</h3>

<p>When a push is received on the server in any one of the subrepos it triggers the <code>changegroup</code> hook <code>commit-sub</code> to force an update and commit of <code>.hgsubstate</code> in the <code>maincollections</code> repo.</p>

commit-subs

<pre><code>#!/bin/bash
SUBNAME=$(basename $(pwd))
cd ../../maincollection/$SUBNAME
hg pull
hg up
cd ..
hg ci -m ""Automated update of .hgsubstate""
</code></pre>

Directory organization

<pre><code>maincollection # repo with subrepos
main/*         # Individual subrepos
</code></pre>",0,5,2018-04-09 22:22:44.397000 UTC,,2019-02-12 22:54:08.477000 UTC,0,version-control|mercurial|dvcs|mercurial-subrepos|subrepos,31,2012-10-10 02:01:15.417000 UTC,2022-03-05 15:44:54.973000 UTC,New Hampshure,1336,60,5,154,,,,,,[]
How can I reuse the dataframe and use alternative for iloc to run an iterative imputer in Azure databricks,"<p>I am running an iterative imputer in Jupyter Notebook to first mark the known incorrect values as &quot;Nan&quot; and then run the iterative imputer to impute the correct values to achieve required sharpness in the data. The sample code is given below:</p>
<pre><code>from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
import numpy as np
import pandas as p

idx =  [761, 762, 763, 764]
cols = ['11','12','13','14']

def fit_imputer():
    for i in range(len(idx)):
        for col in cols:
            dfClean.iloc[idx[i], col] = np.nan
            print('Index = {} Col = {} Defiled Value is: {}'.format(idx[i], col, dfClean.iloc[idx[i], col]))
            # Run Imputer for Individual row        
            tempOut = imp.fit_transform(dfClean)
            print(&quot;Imputed Value  = &quot;,tempOut[idx[i],col] )
            dfClean.iloc[idx[i], col] = tempOut[idx[i],col]
            print(&quot;new dfClean Value = &quot;,dfClean.iloc[idx[i], col])
            origVal.append(dfClean_Orig.iloc[idx[i], col])
</code></pre>
<p>I get an error when I try to run this code on Azure Databricks using pyspark or scala. Because the dataframes in spark are immutable also I cannot use iloc as I have used it in pandas dataframe.</p>
<p>Is there a way or better way of implementing such imputation in databricks?</p>",0,2,2021-03-14 12:40:28.997000 UTC,,,0,scala|pyspark|azure-databricks|pyspark-dataframes,53,2017-08-28 12:37:08.183000 UTC,2022-03-04 08:55:39.077000 UTC,"Mumbai, Maharashtra, India",39,5,0,19,,,,,,[]
Is there any way to connect to perform operation on AWS neptune giving gremlin code in .java file,"<p>I tried Connecting the AWS Neptune with this Java code and got the error , <code>NoHostAvailable Exception</code></p>
<p>approach 1:</p>
<pre><code>public static void main(String[] args) throws Exception {
        Cluster.Builder builder = Cluster.build();
        builder.addContactPoint(&quot;endpoint&quot;);
        builder.port(8182);
        builder.enableSsl(true);
        builder.keyStore(&quot;pem-file&quot;);
        Cluster cluster = builder.create();
        GraphTraversalSource g = traversal().withRemote(DriverRemoteConnection.using(cluster));
        System.out.println(g.V().limit(10).toList());
        cluster.close();
       }}
</code></pre>
<p>approach 2:</p>
<pre><code>    Cluster cluster = Cluster.build(&quot;endpoint&quot;).
                enableSsl(true).keyStore(&quot;pem&quot;).
                    handshakeInterceptor( r -&gt; {
                    NeptuneNettyHttpSigV4Signer sigV4Signer = null;
                    try {
                     sigV4Signer = new NeptuneNettyHttpSigV4Signer(&quot;us-east-2&quot;, new 
       DefaultAWSCredentialsProviderChain());
                    } catch (NeptuneSigV4SignerException e) {
                        e.printStackTrace();
                    }
                    try {
                        sigV4Signer.signRequest(r);
                    } catch (NeptuneSigV4SignerException e) {
                        e.printStackTrace();
                    }
                    return r;
                }).create();
        Client client=Cluster.open(&quot;src\\conf\\remote-objects.yaml&quot;).connect();
        client.submit(&quot;g.V().limit(10).toList()&quot;).all().get();
</code></pre>
<p>what ever I do, I am getting this error:</p>
<pre><code>Sep 02, 2021 3:18:34 PM io.netty.channel.ChannelInitializer exceptionCaught
    WARNING: Failed to initialize a channel. Closing: 
    java.lang.RuntimeException: java.lang.NullPointerException
 
 
 
 org.apache.tinkerpop.gremlin.driver.Channelizer$AbstractChannelizer.initChannel(Channelizer.java:117)
    Caused by: org.apache.tinkerpop.gremlin.driver.exception.NoHostAvailableException: All hosts 
    are considered unavailable due to previous exceptions. Check the error log to find the actual 
    reason.
</code></pre>
<p>I need the code or the document to connect my Gremlin code in .java file to AWS neptune. I am struggling and tried various number of ways,
1.created EC2 instance and did installed maven and apache still got error and code is running in Server(EC2), i want code to present in IntelliJ</p>
<p>it would be more helpful, if I get the Exact Code any way. what should be added in <code>remote-objects.yaml</code>.</p>
<p>if we require Pem-file to access Amazon Neptune, please help with the creation of it.</p>",1,6,2021-09-06 06:26:01.697000 UTC,,2021-09-06 22:35:04.623000 UTC,1,java|amazon-web-services|graph-databases|amazon-neptune|gremlin-server,271,2021-09-01 07:52:55.400000 UTC,2021-10-25 12:04:47.287000 UTC,"Hyderabad, Telangana, India",11,0,0,18,,,,,,[]
How to Save Great Expectations Html validation results to Databricks DBFS or Azure Blob,"<p>Sometime ago I asked the question</p>
<blockquote>
<p>How to Save Great Expectations results to File From Apache Spark -
With Data Docs</p>
</blockquote>
<p><a href=""https://stackoverflow.com/questions/68023413/how-to-save-great-expectations-results-to-file-from-apache-spark-with-data-doc/68364259?noredirect=1#comment120855332_68364259"">How to Save Great Expectations results to File From Apache Spark - With Data Docs</a></p>
<p>The answers centred on viewing the results in Databricks, however I would like to know how to save the Html results to file - either on Databricks DBFS or on Azure ADLS / Blob.</p>
<p>Alex Ott, mentioned the following:</p>
<blockquote>
<p>If you're not using Databricks, then you can render the data into HTML
and store it as files stored somewhere</p>
</blockquote>
<p>However, I'm not sure if he was suggesting that its not possible to store the files if I'm not using Databricks?</p>
<p>In any case, can someone show me how to store / save the file:</p>
<p>I tried the code below but I got the error message:</p>
<pre><code>ResourceNotFoundError: The specified filesystem does not exist.
</code></pre>
<p>and the error pointed to:</p>
<pre><code>file.create_file()
</code></pre>
<p>The code I uses was as follows:</p>
<pre><code>with open('/dbfs/mnt/lake/RAW/pitstops_suite.html', 'r') as file:
    data = file.read()

file = DataLakeFileClient.from_connection_string(&quot;DefaultEndpointsProtocol=https;AccountName=adlsbiukadlsdev;AccountKey=sz8sRd66FLYMjjqa7GfxW1V/ZyMWdmc0YQ==;EndpointSuffix=core.windows.net&quot;, file_system_name=&quot;polybase&quot;, file_path=&quot;pitstops_suite.html&quot;)

##htmp test
file.create_file()
file.append_data(html, offset=0, length=len(html))
file.flush_data(len(html))
</code></pre>
<p>Any thoughts?</p>",0,0,2022-02-01 12:10:08.047000 UTC,,,0,azure-databricks|great-expectations,25,2021-03-31 08:36:43.863000 UTC,2022-03-05 23:03:22.193000 UTC,"London, UK",651,58,0,93,,,,,,[]
Creating a custom progress bar indicator in Spark Azure Databricks,"<p>I would like to create a custom progress bar for a job I am running in pyspark Azure Databricks. I tried doing this using matplotlib by creating multiple figures and calling display one by one for each but the output is displayed only when the entire job completes so it's not much of a progress bar.</p>
<p>Is there a simple way to create such a display that also gets updated as progress is being made?</p>
<p>Thanks for your help.</p>
<p><a href=""https://i.stack.imgur.com/k0Kre.png"" rel=""nofollow noreferrer"">the spark databricks default progress bar</a></p>",1,3,2020-11-16 18:35:08.390000 UTC,,2020-11-18 16:06:48.597000 UTC,1,pyspark|visualization|azure-databricks,280,2019-12-03 13:32:03.707000 UTC,2021-05-29 05:31:55.387000 UTC,,11,0,0,2,,,,,,[]
group child vertices by property,"<p>I'm using Amazon Neptune with the following simple graph schema:</p>

<p><code>root --has--&gt; child</code></p>

<p>where the root vertex can have many different properties.</p>

<p>I want a query that creates a dictionary that includes all the root vertex properties as keys and an array of all the child's vertices with the same value.</p>

<p>for example with the following data:</p>

<pre><code>{
  root: {
   a: 1
   b: 2
   c: 3
  },
  child1: {
   a: 4
   b: 2
   c: 3
  },
  child2: {
   a: 1
   b: 4
   c: 3
  }
}
</code></pre>

<p>I'll get the following result:</p>

<pre><code>{
  a: [child2],
  b: [child1],
  c: [child1, child2]
}
</code></pre>

<p>When I know the properties I can do something like this: </p>

<pre><code>g.V().hasLabel('Root').as('root')
  .project('a', 'b', 'c')
  .by(out('has').where(eq('root')).by('a').fold())
  .by(out('has').where(eq('root')).by('b').fold())
  .by(out('has').where(eq('root')).by('c').fold())
</code></pre>

<p>is there a way to create this without knowing the root properties?</p>",1,0,2020-05-17 15:11:04.780000 UTC,,,1,gremlin|amazon-neptune,67,2020-05-17 14:26:41.717000 UTC,2020-07-28 10:23:09.997000 UTC,"London, UK",11,37,0,8,,,,,,[]
Implement PartitionStrategy in Gremlin JavaScript,"<p>I'm developing a multi-tenant service that utilizes a graph database. I want to store multiple tenants in a single cluster to reduce costs. However, I want to ensure that their resources are isolated. I know in Gremlin that one can use the PartitionStrategy to cause traversals to only explore certain subgraphs. I'm using NodeJS and can't seem to find a PartitionStrategy object. Is there a way to implement it or create a custom class that carries out the same functionality? Also, would labeling every node with a <code>:Tenant</code> property suffice?</p>

<p>Thanks!</p>",1,0,2020-06-09 20:30:54.900000 UTC,,,1,javascript|gremlin|graph-databases|amazon-neptune,249,2020-05-31 18:14:32.097000 UTC,2020-06-10 19:13:47.860000 UTC,,73,1,0,1,,,,,,[]
Add aggregation from different dataframe as column,"<p>With this dataset: </p>

<pre><code>start,end,rms,state,maxTemp,minTemp
2019-02-20T16:16:31.752Z,2019-02-20T17:33:34.750Z,4.588481,charge,35.0,32.0
2019-02-20T17:33:34.935Z,2019-02-20T18:34:49.737Z,5.770562,discharge,35.0,33.0
</code></pre>

<p>And this: </p>

<pre><code>[{""EventDate"":""2019-02-02T16:17:00.579Z"",""Value"":""23""},
{""EventDate"":""2019-02-02T16:18:01.579Z"",""Value"":""23""},
{""EventDate"":""2019-02-02T16:19:02.581Z"",""Value"":""23""},
{""EventDate"":""2019-02-02T16:20:03.679Z"",""Value"":""23""},
{""EventDate"":""2019-02-02T16:21:04.684Z"",""Value"":""23""},
{""EventDate"":""2019-02-02T17:40:05.693Z"",""Value"":""23""},
{""EventDate"":""2019-02-02T17:40:06.694Z"",""Value"":""23""},
{""EventDate"":""2019-02-02T17:40:07.698Z"",""Value"":""23""},
{""EventDate"":""2019-02-02T17:40:08.835Z"",""Value"":""23""}]

schema = StructType([
    StructField('EventDate', TimestampType(), True),
    StructField('Value', FloatType(), True)
])
</code></pre>

<p>I want to add max and min values of the json dataset as columns into the csv dataset. </p>

<p>I have tried: </p>

<pre><code>cyclesWithValues = csvDf\
.withColumn(""max"", jsondata.filter((col(""EventDate"") &gt;= csvDf.start) &amp; (col(""EventDate"") &lt;= csvDf.end)).agg({""value"": ""max""}).head()[""max(Value)""])\
.withColumn(""min"", jsondata.filter((col(""EventDate"") &gt;= csvDf.start) &amp; (col(""EventDate"") &lt;= csvDf.end)).agg({""value"": ""min""}).head()[""min(Value)""])
</code></pre>

<p>But I get this error:</p>

<pre><code>AnalysisException: 'Resolved attribute(s) start#38271,end#38272 missing from EventDate#38283,Value#38286 in operator !Filter ((EventDate#38283 &gt;= start#38271) &amp;&amp; (EventDate#38283 &lt;= end#38272)).;;\n!Filter ((EventDate#38283 &gt;= start#38271) &amp;&amp; (EventDate#38283 &lt;= end#38272))\n+- Project [EventDate#38283, cast(Value#38280 as float) AS Value#38286]\n   +- Project [to_timestamp(EventDate#38279, None) AS EventDate#38283, Value#38280]\n      +- Relation[EventDate#38279,Value#38280] json\n'
</code></pre>

<p>I have a solution based on arrays, but it seems very slow, so I was hoping something like this would speed things up a bit. </p>

<p>Right now I am using this solution: </p>

<pre><code>dfTemperature = spark.read.option(""multiline"", ""true"").json(""path"")
dfTemperatureCast = dfTemperature.withColumn(""EventDate"", to_timestamp(dfTemperature.EventDate)).withColumn(""Value"", dfTemperature.Value.cast('float'))

def AddVAluesToDf(row):
  temperatures = dfTemperatureCast.filter((col(""EventDate"") &gt;= row[""start""]) &amp; (col(""EventDate"") &lt;= row[""end""]))
  maxTemp = temperatures.agg({""value"": ""max""}).head()[""max(value)""]
  minTemp = temperatures.agg({""value"": ""min""}).head()[""min(value)""]
  return (row.start, row.end, row.rms, row.state, maxTemp, minTemp)

pool = ThreadPool(10)
withValues = pool.map(AddVAluesToDf, rmsDf)

schema = StructType([
    StructField('start', TimestampType(), True),
    StructField('end', TimestampType(), True),
    StructField('maxTemp', FloatType(), True),
    StructField('minTemp', FloatType(), True)
])

cyclesDF = spark.createDataFrame(withValues, schema)
</code></pre>",0,6,2019-08-28 18:11:04.337000 UTC,1.0,2019-09-02 19:44:08.297000 UTC,1,python|azure|pyspark|azure-databricks|pyspark-dataframes,91,2012-01-25 11:19:06.340000 UTC,2021-11-23 14:00:25.417000 UTC,"Bergen, Norge",1705,181,4,141,,,,,,[]
"Migrate Azure DevOps Git repos from one organization to another organization with all the histories, tags and pull requests","<p>How do I migrate Azure DevOps Git repos from one organization to another organization with all the histories, tags and pull requests?</p>

<p>I tried multiple options to import all the data like</p>

<ol>
<li><p>Azure DevOps import repository </p></li>
<li><pre><code>git clone --mirror old repos
git push to new repos
</code></pre></li>
<li><pre><code>git clone --bare old repos
git push new repos
</code></pre></li>
</ol>",1,0,2020-03-24 13:36:52.673000 UTC,,2020-03-24 16:36:24.597000 UTC,0,azure-devops|pull-request|dvcs|gitversion,611,2020-03-24 13:19:22.310000 UTC,2022-02-02 19:01:53.797000 UTC,,11,0,0,37,,,,,,[]
"Not able to write Eventhub data into ADLS in Databricks, getting runtime error","<p>when I'm running the writeStream code in databricks which looks like below:</p>
<pre><code>df_out = cdf.writeStream\
  .format(&quot;json&quot;)\
  .outputMode(&quot;append&quot;)\
  .option(&quot;checkpointLocation&quot;, &quot;/path/to/checkpoint/directory&quot;)\
  .start(&quot;/mnt/container-name/folder-name&quot;)
</code></pre>
<p>I'm getting runtime error in databricks:</p>
<p>Now after running the writeStream code I am getting the below error:
How can i resolve this error</p>
<pre><code>org.apache.spark.SparkException: Job aborted.
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)
    at org.apache.spark.sql.execution.streaming.FileStreamSink.addBatch(FileStreamSink.scala:198)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:606)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:126)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:267)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)
    at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:217)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:604)
    at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:293)
    at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:291)
    at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:73)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:604)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$4(MicroBatchExecution.scala:243)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withSchemaEvolution(MicroBatchExecution.scala:647)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:240)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:293)
    at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:291)
    at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:73)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:209)
    at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:203)
    at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:366)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)
    at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:341)
    at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:268)
Caused by: java.io.FileNotFoundException: Unable to find batch /mnt/outputcontainer/Sample/_spark_metadata/709.compact
    at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.applyFnToBatchByStream(HDFSMetadataLog.scala:279)
    at org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.applyFnInBatch(CompactibleFileStreamLog.scala:215)
    at org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.foreachInBatch(CompactibleFileStreamLog.scala:201)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)
    ... 28 more
</code></pre>",1,0,2021-10-21 13:29:05.180000 UTC,,,0,pyspark|spark-streaming|azure-databricks|azure-eventhub,28,2019-05-09 13:26:02.973000 UTC,2022-03-03 14:03:30.113000 UTC,,97,14,0,2,,,,,,[]
Gremlin (Neptune) somewhat complex query doesn't show edges properties,"<p>I have a set of data in my DB (AWS Neptune), and I query it using Gremlin (NodeJS).</p>
<p><a href=""https://i.stack.imgur.com/aScnr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aScnr.png"" alt=""enter image description here"" /></a></p>
<p>The data looks like this and I have a query that looks like this:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>g.V().hasLabel('group').inE('belongs_to')                       
     .outV().has('user_id', user_id).outE()                           
       .otherV().inE().hasLabel('shared_with')                     
           .outV().has('type', type).aggregate('object_id').fold() 
     .V().hasLabel('user').has('user_id', user_id).inE().hasLabel('shared_with')
         .outV().has('type', type).aggregate('object_id').cap('object_id')
     .unfold().dedup().valueMap().toList();</code></pre>
</div>
</div>
</p>
<p>What I end up with (which is correct) is a list of objects and whom they are shared with, the &quot;whom&quot; can be a user or a group.
So that was a good start (took me a while to get it, and I grabbed the main idea from StackOverflow), but I don't get the properties of the edges.</p>
<p>Initially, I had two separate queries, one for objects shared with groups, and one for objects shared with users. That worked fine, but obviously took more time and I had to re-conciliate the two lists I was getting.</p>
<p>I have tried to modify my query putting as('x') behind the edges and the Vertices that I need, but then the select was returning nothing at all. Without the select(), the as() is ignored, and I would get the same data as before.</p>
<p>I initially tried with a .local(__.union()) but that didn't work either.
I started Gremlin/Neptune last week, and while I made decent progress, I can only use somewhat simple queries :(</p>
<p>Update:</p>
<p>Here's how I create groups:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>let group = await g.addV('group')
                .property('group_id', group_id)
                .property('name', name)
                .property('type', groupType)
                .property('owner', owner)
                .next();

//group_id and owner are UUID.</code></pre>
</div>
</div>
</p>
<p>How I add a user to a group:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>t = await g.addV('user')
          .property('user_id', user.id)
          .property('name', user.name)
          .next();

t = await g.V()
          .has('user', 'user_id', user.id)     // Find the user
          .addE('belongs_to')                  // Add a new edge
          .to(g.V().has('group', 'group_id', group_id))
          
// user.id is a UUID</code></pre>
</div>
</div>
</p>
<p>Object creation:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>let obj = await g.addV('object')
                .property('object_id', object_id)
                .property('ref', ref)
                .property('type', type)
                .property('owner', owner)                
                .property('creation_time', d.getTime().toString())
                .next();</code></pre>
</div>
</div>
</p>
<p>Sharing with user or group:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>t = await g.V().has('object','object_id', object_id).has('owner', owner)
         .addE('shared_with')                // Add a new edge
         .property('read', canRead)
         .property('edit', canEdit)
         .property('share', canShare)
         .to(g.V().has('user', 'user_id', id))  // Find the user to link to
         .next();

// For group: .to(g.V().has('group', 'group_id', id))</code></pre>
</div>
</div>
</p>",1,0,2020-10-20 19:01:48.660000 UTC,,2020-10-21 22:18:49.380000 UTC,0,javascript|gremlin|amazon-neptune,111,2013-07-16 20:58:07.570000 UTC,2022-03-04 01:20:13.760000 UTC,,1005,18,2,53,,,,,,[]
Synchronizing DVCS repositories across multiple machines,"<p>What's the best way to keep a DVCS repository synchronized across multiple machines?  I'm a solo developer interested in being able to easily move between my work desktop, home desktop and a (Mac) laptop.  At the moment I use Dropbox to keep files in sync, but I'd like to be able to use some source control specific features.  Is there an easy way to do this peer to peer without relying on a centralized repository?  Is there any particular DVCS that's better suited to this kind of workflow?</p>

<p><strong>Dupe notes</strong>: Inspired by <a href=""https://stackoverflow.com/questions/2096476/how-can-a-mobile-student-effectively-use-dropbox-with-a-source-control-system"">How can a mobile student effectively use Dropbox with a source control system?</a>, but I'm interested in a DVCS, not Subversion.  Similar to <a href=""https://stackoverflow.com/questions/1734405/synchronizing-git-repos-across-machines-without-push"">Synchronizing Git repos across machines without push</a>, but I'm not necessarily looking for a Git specific answer.</p>",2,0,2010-01-19 22:13:32.967000 UTC,,2017-05-23 12:11:48.560000 UTC,-1,version-control|dvcs,169,2008-08-23 13:40:14.243000 UTC,2018-11-28 15:27:40.023000 UTC,"Columbia, SC",14853,729,47,1737,,,,,,[]
Does the .hg folder need to reside on the working directory?,"<p>I am new to Mercurial. I was wondering if there is a way to move the .hg folder to another location than the working directory and still monitor changes?</p>

<p>For example,</p>

<pre><code>I have the .hg in /foo/.hg/*
Can I move it to  /bar/.hg/* and still monitor the contents of foo?
</code></pre>

<p>Edit:
I am paranoid about working directory. I have the Dropbox backup my Mercurial repository. But the .hg folder is getting too big for my Dropbox</p>",3,1,2010-11-09 01:49:35.633000 UTC,1.0,2010-11-09 05:19:37.897000 UTC,1,version-control|mercurial|dvcs,645,2009-03-25 04:27:50.323000 UTC,2020-09-26 16:43:48.403000 UTC,"Boston, MA",49589,384,98,3212,,,,,,[]
How to ingest data from Eventhub to ADLS using Databricks cluster(Scala),"<p>I'm want to ingest streaming data from Eventhub to ADLS gen2 with specified format.</p>
<p>I did for batch data ingestion, from DB to ADLS and Container to Container but now I want to try with streaming data ingestion.</p>
<p>Can you please guide me from where to start to proceed further step. I did create Eventhub, Databrick Instance and Storage Account in Azure.</p>",1,0,2021-10-21 05:41:30.000000 UTC,1.0,2021-10-21 13:21:05.543000 UTC,0,apache-spark|azure-databricks|spark-structured-streaming|azure-eventhub|azure-data-lake-gen2,71,2019-05-09 13:26:02.973000 UTC,2022-03-03 14:03:30.113000 UTC,,97,14,0,2,,,,,,[]
apache spark sql table overwrite issue,"<p>I am using the below code to create a table from a dataframe in databricks and run into error.</p>
<pre><code>df.write.saveAsTable(&quot;newtable&quot;)
</code></pre>
<p>This works fine the very first time but for re-usability if I were to rewrite like below</p>
<pre><code>df.write.mode(SaveMode.Overwrite).saveAsTable(&quot;newtable&quot;)
</code></pre>
<p>I get the following error.</p>
<p><em><strong>Error Message:</strong></em></p>
<pre><code>org.apache.spark.sql.AnalysisException: Can not create the managed table newtable. The associated location dbfs:/user/hive/warehouse/newtable already exists
</code></pre>",2,0,2020-09-10 20:30:36.150000 UTC,1.0,2020-09-10 21:49:54.133000 UTC,5,apache-spark-sql|azure-databricks,2693,2016-10-11 08:37:40.020000 UTC,2022-03-02 16:29:37.847000 UTC,,604,41,11,39,,,,,,[]
Azure Data Factory: can I pause a Pipeline run while another Pipeline is running and then resume the first Pipeline?,"<p>So I have this Pipeline that runs for a long time (weeks), which loads some tables with Data Factory and processes them with Databricks.</p>

<p>Also, I have another Pipeline that is run each day for a couple of hours. However, the Databricks cluster seems not to be powerful enough to run both pipelines simultaneously, as it throws an error when both Pipelines are active (seems to be a memory error, ""Spark driver has stopped unexpectedly"").</p>

<p>The daily Pipeline is highest priority, though, so ideally I would like to <strong>pause</strong> for around 3 hours the long term Pipeline, then <strong>execute the daily</strong> trigger, and then <strong>resume</strong> the long term Pipeline execution.</p>

<p>Is it possible to do that?</p>

<p>Thanks in advance!</p>",2,2,2020-04-16 10:12:49.997000 UTC,,,0,azure|azure-pipelines|azure-data-factory|azure-data-factory-2|azure-databricks,547,2018-07-27 09:56:35.410000 UTC,2022-03-02 07:16:46.517000 UTC,,9,0,0,8,,,,,,[]
Tinkerpop Neptune Config - Running In Docker,"<p>Does anyone have a <code>gremlin-config.yaml</code>  file that would more accurately reflect how Gremlin acts in Neptune?</p>
<p>I am trying to run as much as I can using a local docker container, and I've substituted properties like <code>gremlin.tinkergraph.vertexIdManager=ANY</code> so that the Vertex IDs can be strings.
But I'm still missing details like multiple labels, which I think is only available via the Neo4Js config, unsure what else this will change.</p>
<p>But yeah, generally looking for a config that represents how Neptune functions as closely as possible</p>
<p>Current:</p>
<pre class=""lang-yaml prettyprint-override""><code>host: 172.17.0.2
port: 8182
evaluationTimeout: 30000
channelizer: org.apache.tinkerpop.gremlin.server.channel.WebSocketChannelizer
graphs: {
  graph: conf_local/tinkergraph-custom.properties}
scriptEngines: {
  gremlin-groovy: {
    plugins: { org.apache.tinkerpop.gremlin.server.jsr223.GremlinServerGremlinPlugin: {},
               org.apache.tinkerpop.gremlin.tinkergraph.jsr223.TinkerGraphGremlinPlugin: {},
               org.apache.tinkerpop.gremlin.jsr223.ImportGremlinPlugin: {classImports: [java.lang.Math], methodImports: [java.lang.Math#*]},
               org.apache.tinkerpop.gremlin.jsr223.ScriptFileGremlinPlugin: {files: [scripts/empty-sample.groovy]}}}}
serializers:
  - { className: org.apache.tinkerpop.gremlin.driver.ser.GraphSONMessageSerializerV3d0, config: { ioRegistries: [org.apache.tinkerpop.gremlin.tinkergraph.structure.TinkerIoRegistryV3d0] }}        # application/json
  - { className: org.apache.tinkerpop.gremlin.driver.ser.GraphBinaryMessageSerializerV1 }                                                                                                           # application/vnd.graphbinary-v1.0
  - { className: org.apache.tinkerpop.gremlin.driver.ser.GraphBinaryMessageSerializerV1, config: { serializeResultToString: true }}                                                                 # application/vnd.graphbinary-v1.0-stringd
processors:
  - { className: org.apache.tinkerpop.gremlin.server.op.session.SessionOpProcessor, config: { sessionTimeout: 28800000 }}
  - { className: org.apache.tinkerpop.gremlin.server.op.traversal.TraversalOpProcessor, config: { cacheExpirationTime: 600000, cacheMaxSize: 1000 }}
metrics: {
  consoleReporter: {enabled: true, interval: 180000},
  csvReporter: {enabled: true, interval: 180000, fileName: /tmp/gremlin-server-metrics.csv},
  jmxReporter: {enabled: true},
  slf4jReporter: {enabled: true, interval: 180000}}
strictTransactionManagement: false
idleConnectionTimeout: 0
keepAliveInterval: 0
maxInitialLineLength: 4096
maxHeaderSize: 8192
maxChunkSize: 8192
maxContentLength: 10485760
maxAccumulationBufferComponents: 1024
resultIterationBatchSize: 64
writeBufferLowWaterMark: 32768
writeBufferHighWaterMark: 65536
ssl: {
  enabled: false}
</code></pre>
<pre><code>gremlin.tinkergraph.vertexIdManager=ANY
gremlin.graph=org.apache.tinkerpop.gremlin.tinkergraph.structure.TinkerGraph
</code></pre>
<pre class=""lang-yaml prettyprint-override""><code>version: '3'
services:
  gremlin-server:
    container_name: gwent_onboarding_neptune
    image: tinkerpop/gremlin-server:3.5.0
    user: $USER
    volumes:
      - ./configuration/gremlin-conf:/opt/gremlin-server/conf_local
      # - ./gremlin-console/data:/gremlin-server/scripts
    ports:
      - 8182:8182
    command: ./conf_local/gremlin-serverr.yaml
</code></pre>
<p>Edit:
Started down this rabbit hole when I was trying to get tinkerpop to work with multiple labels</p>",1,0,2022-02-25 19:45:10.140000 UTC,,2022-02-25 20:18:03.203000 UTC,0,gremlin|tinkerpop|amazon-neptune,19,2017-05-18 12:46:15.957000 UTC,2022-02-25 20:39:16.950000 UTC,"Cincinnati, OH, USA",26,0,0,5,,,,,,[]
How to read .shp files in Databricks notebook,"<p>I am working on a problem where I need to plot output on a Map.<br />
In past I was able to do it using <code>geopandas</code>.
However this does not work in <code>databricks-notebook</code>.
I tried to look at alternative but couldn't find any on web.</p>
<p>Pages I looked in to:<br />
<a href=""https://databricks.com/notebooks/geopandas-notebook.html"" rel=""nofollow noreferrer"">GeoPandas Notebook</a><br />
<a href=""https://databricks.com/blog/2019/12/05/processing-geospatial-data-at-scale-with-databricks.html"" rel=""nofollow noreferrer"">Processing Geospatial Data at Scale With Databricks</a></p>
<p>In the second link it mentions that we can read <code>.shp</code> flies through <code>scala</code>
however it does not mention what <code>sc</code> in <code>ShapefileReader.readToGeometryRDD</code> stands for?</p>
<pre><code>%scala
var spatialRDD = new SpatialRDD[Geometry]
spatialRDD = ShapefileReader.readToGeometryRDD(sc, &quot;/ml/blogs/geospatial/shapefiles/nyc&quot;)

var rawSpatialDf = Adapter.toDf(spatialRDD,spark)
rawSpatialDf.createOrReplaceTempView(&quot;rawSpatialDf&quot;) //DataFrame now available to SQL, Python, and R 
</code></pre>",3,0,2021-10-28 08:21:25.063000 UTC,,2021-10-28 08:28:32.953000 UTC,0,python|azure-databricks|geopandas,326,2020-09-08 06:14:32.637000 UTC,2022-03-04 09:23:14.573000 UTC,"Bangalore, Karnataka, India",500,31,1,59,,,,,,[]
Connect Azure Databricks to SQL Database using a Service Principal and R,"<p>In Azure I would like to connect Databricks to an Azure SQL database using using a service principal and R language via Azure Active Directory Autentification. </p>

<p><a href=""https://thedataguy.blog/connect-azure-databricks-to-sql-database-azure-sql-data-warehouse-using-a-service-principal/"" rel=""nofollow noreferrer"">Here</a> I learned how to do that using scala. Is there by any means a possibility to do the same using R? </p>

<p>Applying the <a href=""https://cran.r-project.org/web/packages/AzureAuth/index.html"" rel=""nofollow noreferrer"">AzureAuth</a> R-package I was at least able to create a token like that:</p>

<pre><code> get_azure_token(resource = ""https://database.windows.net/"",
            tenant = ""&lt;teneantID&gt;"",
            app = ""&lt;servicePrincipalID&gt;"", 
            password = ""&lt;servicePrincipalPW&gt;"",
            auth_type = ""client_credentials"") -&gt; myToken
</code></pre>

<p>However, I do not see how to continue. The <a href=""https://cran.r-project.org/web/packages/AzureStor/index.html"" rel=""nofollow noreferrer"">AzureStor</a> R-package seemingly provides merely the connection to data lakes or blob storages.</p>

<p>So, how can I move on and connect to an Azure SQL from databricks using R?</p>",0,5,2019-12-11 11:22:44.153000 UTC,1.0,2019-12-17 13:03:33.463000 UTC,0,r|azure|azure-active-directory|azure-databricks,430,2017-10-09 15:56:03.437000 UTC,2022-02-17 09:42:19.327000 UTC,,61,8,0,16,,,,,,[]
SQL query to remove everything after a comma in varchar string values in a column,"<p>I created a database for addresses, however, I made a mistake of concatenating the unit number with the road address which happened to include the unit number as well. So now, the unit number is repeated in some rows.</p>
<pre><code>+----------------------------+---------------+
| physical_address           | city          |
+----------------------------+---------------+
| 1 AGATE COURT              | New York City |
| 1 ANGELAS PLACE, 1A, 1A    | New York City |
| 1 ARLINGTON COURT          | New York City |
| 1 AVENUE J                 | New York City |
| 1 BAY CLUB DRIVE, 10M, 10M | New York City |
| 1 BAY CLUB DRIVE, 11B, 11B | New York City |
| 1 BAY CLUB DRIVE, 11V, 11V | New York City |
| 1 BAY CLUB DRIVE, 12H, 12H | New York City |
| 1 BAY CLUB DRIVE, 14S, 14S | New York City |
| 1 BAY CLUB DRIVE, 15B, 15B | New York City |
+----------------------------+---------------+
</code></pre>
<p>So if you look in the table above in the last 6 records, in the <code>physical_address</code> column, the unit numbers like <code>10M</code>, <code>11B</code>, <code>11V</code> etc are repeated.</p>
<p>Is there any query I can run to remove everything after the last <code>,</code> in each row? The type is <code>varchar</code> for the column if that helps. Also, keeping in mind that some addresses don't have any <code>,</code> in them.</p>
<p><strong>EDIT</strong> what I have tried:</p>
<pre><code>UPDATE sales
SET MyAddress = LEFT(MyAddress, CHARINDEX(',', MyAddress) - 1)
WHERE CHARINDEX(',', MyAddress) &gt; 0
</code></pre>
<p>This unfortunately removes everything after the first comma, not the last.</p>
<p>This is on a database named <a href=""https://docs.dolthub.com/introduction/what-is-dolt"" rel=""nofollow noreferrer"">Dolt</a></p>",2,7,2022-01-09 15:55:24.593000 UTC,,2022-01-09 16:41:20.610000 UTC,-2,sql|database|dolt,61,2011-11-04 14:51:35.597000 UTC,2022-03-04 15:41:08.743000 UTC,United States,301,4,0,111,,,,,,[]
How to clone from a repository and bring all remote branches using git?,"<p>I Have repository 'A', I cloned from it to Repository 'B' using <em><strong>git clone <a href=""http://example/A.git"" rel=""nofollow noreferrer"">http://example/A.git</a></strong>.</em>
So now, I have one local branch ""master"" and all other branches as remote branches in 'B'.</p>

<p>Next Step, I want to clone from 'B' to a new Repository 'C' using <em><strong>git clone --bare</strong></em>, and bring all remote branches from 'B' and make them as local at 'C'.</p>

<p><a href=""https://i.stack.imgur.com/YU1D7.png"" rel=""nofollow noreferrer"">Image of My Workflow</a></p>",2,0,2018-10-11 11:17:53.873000 UTC,,2018-10-11 12:39:07.870000 UTC,0,git|version-control|dvcs|git-clone,197,2016-11-24 20:35:50.533000 UTC,2019-06-11 08:26:35.927000 UTC,,1,0,0,5,,,,,,[]
Issues running PySpark UDF with Databricks Connect,"<p>I'm having problems running my PySpark UDFs in a distributed way, e.g. via Databricks Connect.</p>
<p>For example:</p>
<pre><code>import pyspark.sql.functions as f

class MyClass(object):
    def __init__(self, number_string):
        self.number = int(number_string)

    def simple_fun(self, num1, num2):
        return self.number + num1 + num2

    def nested_udf(self):
        @f.udf('string')
        def nested_fun(num1, num2):
            self.simple_fun(num1, num2)

        return nested_fun
</code></pre>
<p>Then, with the following code:</p>
<pre><code>from pyspark.sql import SparkSession, Row
from pyspark.sql import types as t
from demo.demoModule import MyClass

spark = return SparkSession.builder.getOrCreate()

rdd = spark.sparkContext.parallelize(
    [
        Row(
            num1=1,
            num2=1
        ),
        Row(
            num1=1,
            num2=2
        ),
        Row(
            num1=2,
            num2=2
        ),
        Row(
            num1=2,
            num2=3
        ),
    ]
)
schema = t.StructType(
    [
        t.StructField(&quot;num1&quot;, t.IntegerType(), True),
        t.StructField(&quot;num2&quot;, t.IntegerType(), True)
    ]
)

input_df = spark.createDataFrame(rdd, schema)

my_class = MyClass(&quot;10&quot;)
result = input_df.withColumn(&quot;new_col&quot;, my_class.nested_udf()(&quot;num1&quot;, &quot;num2&quot;))

result.columns
result.show()
</code></pre>
<p>I get no issues if I run it locally using just <code>pyspark</code> but if I try to run it with <code>databricks-connect</code> I get the following error:</p>
<blockquote>
<p>pyspark.serializers.SerializationError: Caused by Traceback (most recent call last):
File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 165, in _read_with_length
return self.loads(obj)
File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 469, in loads
return pickle.loads(obj, encoding=encoding)
ModuleNotFoundError: No module named 'demo'</p>
</blockquote>
<p>I tried <a href=""https://stackoverflow.com/a/39878574/6327771"">this solution</a> by tweaking my code:</p>
<pre><code>def take_df(self, df, colname1, colname2):
    my_udf = f.udf(self.simple_fun, &quot;string&quot;)
    return df.withColumn(&quot;new_col&quot;, my_udf(colname1, colname2))

...

my_class = MyClass(&quot;10&quot;)
result = my_class.take_df(input_df, &quot;num1&quot;, &quot;num2&quot;)

</code></pre>
<p>But I get the same error. Any hints on how to get around it are welcome!</p>",0,0,2022-01-19 12:48:30.343000 UTC,,,0,python|pyspark|user-defined-functions|distributed-computing|databricks-connect,46,2016-05-12 21:42:54.870000 UTC,2022-03-04 16:48:31.223000 UTC,"London, United Kingdom",1496,934,2,691,,,,,,[]
ODBC: ERROR [HY000] [Microsoft][DriverSupport] (1170),"<p>Getting this error when connecting Power BI with Azure Databricks through spark build in connector:-</p>

<blockquote>
  <p>Details: ""ODBC: ERROR [HY000] [Microsoft][DriverSupport] (1170)
  Unexpected response received from server. Please ensure the server
  host and port specified for the connection are correct.""</p>
</blockquote>

<p>I have checked many times host and port of the databrick cluster , and also tried after restarting of cluster .</p>

<p>Guide for the connection:-
<a href=""https://docs.azuredatabricks.net/user-guide/bi/power-bi.html"" rel=""nofollow noreferrer"">https://docs.azuredatabricks.net/user-guide/bi/power-bi.html</a></p>",2,0,2019-04-09 07:06:43.353000 UTC,,2019-04-09 07:51:03.503000 UTC,1,powerbi|azure-databricks,10672,2019-04-09 06:52:04.877000 UTC,2019-06-10 05:06:59.473000 UTC,,11,0,0,1,,,,,,[]
git rebase -i foo/dev gives noop,"<p>When I try to get into the interactive rebase mode as explained <a href=""http://schacon.github.io/gitbook/4_interactive_rebasing.html"" rel=""nofollow"">here</a>, all I'm getting is <strong>noop</strong> which is weird but I cannot figure out why.</p>

<p>I am inside a temp branch in my machine and when I try to view the difference between, I get this:</p>

<pre>
Tugberk@TUGBERKPC /D/Dropbox/AppsPublic/gulp-aspnet-k (temp-a)
$ git log --pretty=oneline  temp-a..ajryan/dev
f0ef4ee40fde4de5360515fd556de9c3ed40431c update readme with new optionsfcba1ae3306ae041a1d82647e4cf65a0c96fe2f7 do not loop for build, restore8bedd238660908f06d698815ef12fce786d716ed fix command concat
e2e3a0d00e3950911c00c0e6e51a671f5f2ac2d3 bump version
d65923ff91eb9d6b9782bc2bf5ab606d19733203 add quiet option
91a1faff0a3aa1ca552df2151190a3ecd87cfc6f allow caller to loop all comma962c55854457314324d81457c42913532875cc85 trailing whitespace, tabs > sp
</pre>

<p>However, when I run the following command, I'm getting noop:</p>

<pre>
Tugberk@TUGBERKPC /D/Dropbox/AppsPublic/gulp-aspnet-k (temp-a)
$ git rebase -i ajryan/dev
</pre>

<p>Any idea why and what am I doing wrong?</p>",1,1,2014-11-23 23:38:59.713000 UTC,1.0,2014-11-23 23:49:07.597000 UTC,2,git|dvcs,4838,2010-10-01 10:58:31.217000 UTC,2021-07-23 16:13:45.477000 UTC,"Cambridge, United Kingdom",55556,1795,64,4460,,,,,,[]
Pyspark is trying to convert column to bool. Why?,"<p>I have some SQL that creates a temp table:</p>
<pre><code>%sql
CREATE OR REPLACE TEMPORARY VIEW MyTempTable AS 
select
  SaleDate,
  Count(salesId) as count_of_sales
from
  myviews.completed_sales
where
  SaleDate between '2018-01-01 00:00:00'
  and '2020-12-31 23:59:59'
group by
  SaleDate
</code></pre>
<p>This gives me the following results: a sample</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>SaleDate</th>
<th>count_of_sales</th>
</tr>
</thead>
<tbody>
<tr>
<td>2020-12-20</td>
<td>20</td>
</tr>
<tr>
<td>2020-11-21</td>
<td>83</td>
</tr>
<tr>
<td>2020-10-20</td>
<td>6</td>
</tr>
<tr>
<td>2020-09-01</td>
<td>1365</td>
</tr>
<tr>
<td>2020-08-20</td>
<td>408</td>
</tr>
</tbody>
</table>
</div>
<p>I then want to send this to a dataframe and begin using fbprophet model:</p>
<pre><code>import pyspark
import pandas as pd
from fbprophet import Prophet
df = spark.table('MyTempTable')
#display(df)


# instantiate the model and set parameters
model = Prophet(
    interval_width=0.95,
    growth='linear',
    daily_seasonality=False,
    weekly_seasonality=True,
    yearly_seasonality=True,
    seasonality_mode='multiplicative'
)

# fit the model to historical data
model.fit(df)
</code></pre>
<p>I get the following error when I try to run it:</p>
<pre><code>ValueError: Cannot convert column into bool: please use '&amp;' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.
</code></pre>
<p>update: Full error</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;command-4274713499654188&gt; in &lt;module&gt;
     17 
     18 # fit the model to historical data
---&gt; 19 model.fit(df)
     20 

/local_disk0/.ephemeral_nfs/envs/pythonEnv-1/lib/python3.8/site-packages/fbprophet/forecaster.py in fit(self, df, **kwargs)
   1099             raise Exception('Prophet object can only be fit once. '
   1100                             'Instantiate a new object.')
-&gt; 1101         if ('ds' not in df) or ('y' not in df):
   1102             raise ValueError(
   1103                 'Dataframe must have columns &quot;ds&quot; and &quot;y&quot; with the dates and '

/databricks/spark/python/pyspark/sql/column.py in __nonzero__(self)
    911 
    912     def __nonzero__(self):
--&gt; 913         raise ValueError(&quot;Cannot convert column into bool: please use '&amp;' for 'and', '|' for 'or', &quot;
    914                          &quot;'~' for 'not' when building DataFrame boolean expressions.&quot;)
    915     __bool__ = __nonzero__
</code></pre>",0,8,2021-04-26 09:30:39.460000 UTC,,2021-04-26 09:35:55.687000 UTC,1,pyspark|apache-spark-sql|azure-databricks|facebook-prophet,384,2013-10-01 20:07:40.060000 UTC,2021-10-01 13:27:31.087000 UTC,United Kingdom,55,3,0,48,,,,,,[]
automatic creation of jira ticket whenever job fails on azure databricks,"<p>I have a job in azure databricks. currently it sends a notification to email on failure. I want to create a jira ticket automatically whenever job fails.
How do i achieve this?</p>",0,1,2021-12-22 08:43:29.347000 UTC,,,0,azure|azure-devops|azure-databricks,30,2021-12-22 08:39:52.257000 UTC,2022-03-04 05:08:28.210000 UTC,,1,0,0,1,,,,,,[]
Databricks Notebook Command Status Handshake with ADF,"<p>I have an ADF pipeline which has around 30 activities that call Databricks Notebooks. The activities are arranged sequentially, that is, one gets executed only after the successful completion of the other.</p>
<p>However, at times, even when there is a run time error with a particular notebook, the activity that calls the notebook is not failing, and the next activity is triggered. Ideally, this should not happen.</p>
<p>So, I want to keep an additional check on the link condition between the activities. I plan to put a condition on the status of the commands running in the notebook (imagine a notebook has 10 python commands, I want to capture the status of 10th command).</p>
<p>Is there a way to configure this? Appreciate ideas. Thank you.</p>",1,1,2021-12-03 00:49:32.397000 UTC,,,0,python-3.x|azure-data-factory|pipeline|azure-databricks,49,2021-06-28 19:54:30.930000 UTC,2022-03-05 22:49:45.047000 UTC,,23,0,0,5,,,,,,[]
How to move and copy files/read files from Azure Blob into databricks and transform the file and send to target bob container,"<p>I am new beginner in to Azure Databricks. I wanted to know how can we copy and read files from Azure Blob source container into databricks, transform as needed and sent back to target container in blob.</p>
<p>Can some provide python code here?</p>",1,0,2021-01-07 20:40:44.080000 UTC,,,-1,azure-databricks,800,2021-01-07 20:28:43.977000 UTC,2021-01-15 21:47:19.910000 UTC,"Dallas, TX, USA",1,0,0,0,,,,,,[]
Referring to the current bookmark in mercurial,"<p>I am using bookmarks in mercurial to emulate a git branches-like workflow. </p>

<p>One thing I'm finding is that whenever I push, I invariably want to push just the current bookmark. Rather than typing</p>

<pre><code>hg push -B &lt;bookmark_name&gt;
</code></pre>

<p>all the time, I'd like to alias <code>hg push</code> to just push the current bookmark. To do that, I need a way of referring to the current bookmark without mentioning its name. Is there a way to do that?</p>",5,0,2013-09-12 17:58:31.570000 UTC,,,5,version-control|mercurial|branch|dvcs,1653,2009-07-21 00:16:41.453000 UTC,2022-03-06 00:44:56.803000 UTC,"Toronto, ON, Canada",46459,3245,69,1439,,,,,,[]
How should framework/library files be treated in git?,"<p>What is the best practice with versioning projects with multiple 3rd party libraries in git or other DVSC? Should the project source code be mixed with libraries in the same repository, or should they be separated somehow, or not versioned at all? Is this a good place for using submodules? I am talking about uncompiled libraries(php frameworks for example)</p>",1,2,2012-01-31 23:46:05.087000 UTC,,2012-01-31 23:54:08.827000 UTC,2,git|version-control|dvcs|php,333,2012-01-22 15:44:27.600000 UTC,2021-11-03 12:55:48.493000 UTC,"Prague, Czechia",137,11,0,17,,,,,,[]
How to get the last modification time of each files present in azure datalake storage using python in databricks workspace?,"<p>I am trying to get the last modification time of each file present in azure data lake.</p>

<p><strong>files = dbutils.fs.ls('/mnt/blob')</strong></p>

<p>for fi in files: 
    print(fi)</p>

<p>Output:-FileInfo(path='dbfs:/mnt/blob/rule_sheet_recon.xlsx', name='rule_sheet_recon.xlsx', size=10843)</p>

<p>Here i am unable to get the last modification time of the files. Is there any way to get that property.</p>

<p>I tries this below shell command to see the properties,but unable to store it in python object.</p>

<p><strong>%sh ls -ls /dbfs/mnt/blob/</strong></p>

<p>output:-
total 0</p>

<p>0 -rw-r--r-- 1 root root 13577 Sep 20 10:50 a.txt</p>

<p>0 -rw-r--r-- 1 root root 10843 Sep 20 10:50 b.txt</p>",3,0,2019-10-04 13:24:07.500000 UTC,3.0,2019-11-01 10:52:52.743000 UTC,6,python|azure|azure-data-lake|azure-databricks,5355,2018-06-28 18:39:34.413000 UTC,2020-03-13 06:22:39.747000 UTC,,77,2,0,5,,,,,,[]
Trying to read excel file from Spark (crealytics) but can't see data,"<p>1) I am trying to read first excel file using </p>

<pre><code>val df1 = spark.read.excel(
   dataAddress = dataAdd,
   header = true,
**maxRowsInMemory = 5)**
  .option(""ignoreLeadingWhiteSpace"", ""true"")
  .option(""ignoreTrailingWhiteSpace"", ""true"")
  .option(""inferSchema"", ""true"")
  .load(dataFilePath)
</code></pre>

<p>with enabled maxRowInMemory, so it doesn't give data but it only shows Schema.</p>

<p>But, If I removed maxRowInMemory I get data.</p>

<p>2)  But with another excel file if I remove maxRowInMemory, I am getting Cannot Convert String to Numeric Cell.....</p>

<p>Can anyone please help me in this.</p>",1,0,2020-05-04 18:17:47.470000 UTC,,,0,excel|scala|apache-spark|azure-databricks,437,2016-09-20 07:21:56.893000 UTC,2021-07-12 06:02:55.593000 UTC,,41,4,0,7,,,,,,[]
How to effectively fetch organization hierarchy from Microsoft Graph API using Pyspark/Python?,"<p>I'm trying to get the organization hierarchy (top to bottom approach) using Microsoft Graph API in Azure Databricks.</p>
<p>Below is the API I'm trying to query - <br>
<code>https://graph.microsoft.com/v1.0/users/{id}/directReports</code></p>
<p>Code Used -</p>
<pre><code>url = &quot;https://graph.microsoft.com/v1.0/users/{id}/directReports&quot;

payload={}

response = requests.request(&quot;GET&quot;, url, headers=headers, data=payload)

df = spark.read.json(sc.parallelize([response.text]))

df = df.select(f.explode_outer('value').alias('data'))

df = df.select(&quot;data.*&quot;)
df.write.mode(&quot;overwrite&quot;).saveAsTable(&quot;default.orgTree&quot;)
</code></pre>
<p>Now, for the 1st instance (topmost level is CEO) it works. Below are the list of columns I was able to populate within <code>default.orgTree</code>- <br></p>
<pre><code>['@odata.type',  'businessPhones',  'displayName',  'givenName',  'id',  'jobTitle',  'mail',  'mobilePhone',  'officeLocation',  'preferredLanguage',  'surname',  'userPrincipalName']
</code></pre>
<p>However, next level(I fetched all the <code>id</code> - extracted under 1st iteration) onwards in some cases there are blanks and in some cases it contains multiple string dictionaries within a list. My requirement is to iterate till the lowest level (till whichever level direct reportee is available ) and append in <code>default.orgTree</code> respectively for parent and child.</p>
<p>Is there an effective way to achieve this complex scenario? I'm bit confused how to handle the subsequent iterations.</p>
<p>Please help.</p>",1,0,2021-10-20 14:52:10.430000 UTC,,2021-10-21 07:11:13.700000 UTC,0,python|azure|pyspark|microsoft-graph-api|azure-databricks,123,2021-08-20 14:18:09.427000 UTC,2022-03-04 17:03:01.683000 UTC,,3,0,0,6,,,,,,[]
Pushing logs to Log Analytics from Databricks,"<p>I have logs collected in Databricks cluster but I need to pushed to Log Analytics in Azure to have a common log collection</p>

<p>Have not tried anything but would like to know what the approach</p>",1,1,2019-04-02 19:55:13.253000 UTC,0.0,,1,azure-log-analytics|azure-databricks,164,2014-11-12 06:40:53.927000 UTC,2020-06-07 22:34:55.520000 UTC,,37,0,0,4,,,,,,[]
Gremlin intersection of common vertices,"<p>I have a query that looks like the following:</p>

<p><code>""g.V('0_mr').out().out()""</code></p>

<pre><code>v[11_i]
v[13_i]
v[22_i]
v[23_i]
v[25_i]
v[28_i]
v[11_i]
v[13_i]
v[19_i]
v[29_i]
</code></pre>

<p>Vertices <code>11_i</code> and <code>13_i</code> are common vertices for this traversal. How can filter to simply return the common intersecting vertices?</p>",1,0,2020-03-25 17:19:56.620000 UTC,,,1,gremlin|amazon-neptune|gremlinpython,161,2015-12-25 04:06:14.027000 UTC,2022-02-07 19:20:17.893000 UTC,,127,7,0,4,,,,,,[]
how to add a calculated column to a dataframe in scala/python?,"<p>First of all, I'm new to programming and just a beginner playing on azure databricks.
Currently, I have a dataframe named 'df' (huge data set with 50+ columns and above million rows) and I would like to add another calculated column to it based on the existing 'timestamp' column in it. </p>

<p>I'm comfortable with SQL so I converted the DF into the table and tried using </p>

<pre><code>ALTER TABLE logdata 
ADD sli VARCHAR(255)
</code></pre>

<p>but I was always hitting an error (code is right, working perfectly on W3schools) as below</p>

<blockquote>
  <p>Error in SQL statement: ParseException: 
  no viable alternative at input 'ALTER LOGDATA'(line 1, pos 6)</p>
</blockquote>

<p>So, I'm trying to use either scala or python. My use case is out of the huge dataframe 'df' I have a column called 'timestamp'(like 2019-04-18 07:31:45). I need to create a custom column that grabs only the 'mins' section of the timestamp and inserts it into the newly created column of the dataframe. I tried using withcolumn and lit from other user's suggestions, but no success. Can anyone help me with this please how to loop for this huge dataset?</p>",1,0,2019-10-09 03:54:42.393000 UTC,,2019-10-09 08:32:05.913000 UTC,0,mysql|python-3.x|scala|azure-databricks,439,2019-10-09 01:58:37.883000 UTC,2019-11-16 04:20:19.683000 UTC,,1,0,0,2,,,,,,[]
Adding key vault scopes for azure databricks having issues,"<p>I am trying to automate adding Keyvault scope for Azure DataBricks service.</p>

<p>Tried to use example from <a href=""https://github.com/DataThirstLtd/azure.databricks.cicd.tools/blob/master/Tests/Add-DatabricksSecretScope.tests.ps1"" rel=""nofollow noreferrer"">here</a></p>

<p><strong>Code</strong></p>

<pre><code>$Region = ""southindia""
$ResID = ""/subscriptions/*******/resourceGroups/******/providers/Microsoft.KeyVault/vaults/testkv123d""
$databricsOrgId = **********


Connect-Databricks -Region $Region -DatabricksOrgId $databricsOrgId `
    -ApplicationId ************ `
    -Secret ************ -TenantId ************  -Verbose

Connect-Databricks -Region $Region -ApplicationId ************** `
            -Secret ****************** `
            -ResourceGroupName ******* `
            -SubscriptionId ****************************** `
            -WorkspaceName *********** `
            -TenantId ********** -Verbose

Add-DatabricksSecretScope -ScopeName ""kvscope"" -Verbose -KeyVaultResourceId $ResID
</code></pre>

<p><strong>Issue Details</strong></p>

<pre><code>VERBOSE: POST https://southindia.azuredatabricks.net/api/2.0/secrets/scopes/create with -1-byte payload
Invoke-RestMethod : {""error_code"":""INTERNAL_ERROR"",""message"":""There was an internal error handling request POST to /api/2.0/secrets/scopes/create. Please try again later.""}
At C:\Program Files\WindowsPowerShell\Modules\azure.databricks.cicd.tools\2.0.55\Public\Add-DatabricksSecretScope.ps1:73 char:9
+         Invoke-RestMethod -Method Post -Body $BodyText -Uri ""$global: ...
+         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : InvalidOperation: (System.Net.HttpWebRequest:HttpWebRequest) [Invoke-RestMethod], WebException
    + FullyQualifiedErrorId : WebCmdletWebResponseException,Microsoft.PowerShell.Commands.InvokeRestMethodCommand
</code></pre>",1,2,2020-01-21 11:35:03.210000 UTC,,2020-01-22 02:56:44.977000 UTC,1,azure|powershell|azure-keyvault|azure-databricks,1010,2018-01-29 03:09:24.503000 UTC,2021-08-07 11:24:48.007000 UTC,"Chennai, Tamil Nadu, India",11,0,0,5,,,,,,[]
GIT GUI client on Windows for Unix GIT installation,"<p>Our company programs and runs data analysis on a Linux server.  The programming is done in Windows clients (SAS, generally).  Each project is only programmed by one person and is reviewed by another. </p>

<p>We would like to put our projects under version control but leave the code on the server (ie not pulling to local Windows repositories).  The advantage is incremental backup and helps with confirming changes from the reviewer.</p>

<p>Does anyone know of a Windows client that can read remote repositories but perform GIT actions using GIT on Linux, rather than on Windows?  Trying a few clients (e.g. SourceTree and SmartGIT) suggests only the latter is possible.</p>

<p>Thanks, Rich</p>",1,5,2013-05-14 08:35:54.463000 UTC,0.0,2013-05-14 09:23:20.767000 UTC,0,windows|git|version-control|sas|dvcs,377,2010-01-02 20:41:40.417000 UTC,2021-03-31 13:09:54.083000 UTC,,696,40,1,54,,,,,,[]
"How to use ""Spark Connection"" with databricks to extract data from SQL database and then convert to R dataframe?","<p>I am trying to extract data from an Azure SQL database into an R notebook on Azure Databricks to run an R script on it. Because of the difficulties I have experienced using jdbc and DBI (detailed here: <a href=""https://stackoverflow.com/questions/56756844/no-suitable-driver-error-when-using-sparklyrspark-read-jdbc-to-query-azure-s"">&quot;No suitable driver&quot; error when using sparklyr::spark_read_jdbc to query Azure SQL database from Azure Databricks</a> and here: <a href=""https://stackoverflow.com/questions/56701533/how-to-get-sql-database-connection-compatible-with-dbidbgetquery-function-when?noredirect=1#comment99967902_56701533"">How to get SQL database connection compatible with DBI::dbGetQuery function when converting between R script and databricks R notebook?</a>) I have decided to use the inbuilt spark connector as such (credentials changed for security reasons):</p>

<pre><code>%scala

//Connect to database:

import com.microsoft.azure.sqldb.spark.config.Config
import com.microsoft.azure.sqldb.spark.connect._

// Aquire a DataFrame collection (val collection)

val dave_connection = Config(Map(
  ""url""          -&gt; ""servername.database.windows.net"",
  ""databaseName"" -&gt; ""databasename"",
  ""dbTable""      -&gt; ""myschema.mytable"",
  ""user""         -&gt; ""username"",
  ""password""     -&gt; ""userpassword""
))

val collection = sqlContext.read.sqlDB(dave_connection)
collection.show()
</code></pre>

<p>This works in the sense that it displays the data, but as someone who doesn't know the first thing about scala or spark I now have no idea how to get it into an R or R-compatible dataframe.</p>

<p>I have tried to see what kind of object ""collection"" is, but:</p>

<pre><code>%scala
getClass(collection)
</code></pre>

<p>returns only: </p>

<pre><code>notebook:1: error: too many arguments for method getClass: ()Class[_ &lt;: $iw]
getClass(collection)
</code></pre>

<p>And trying to access it using sparklyr implies that it doesn't actually exist, e.g.</p>

<pre><code>library(sparklyr)
sc &lt;- spark_connect(method=""databricks"")

sdf_schema(collection)
</code></pre>

<p>returns:</p>

<pre><code>Error in spark_dataframe(x) : object 'collection' not found
</code></pre>

<p>I feel like this is may well be pretty obvious to anyone who understands scala, but I don't (I come from an analyst rather than computer science background), and I just want to get this data into an R dataframe to perform analyses on it. (I know Databricks is all about parallelisation and scaling, but I'm not performing any parallelised functions on this dataset, the only reason I'm using Databricks is because my work PC doesn't have sufficient memory to run my analyses locally!)</p>

<p>So, does anyone have any ideas on how I can convert this spark object ""collection"" into an R dataframe?</p>",0,0,2019-07-12 10:28:37.597000 UTC,,,1,r|scala|azure-sql-database|sparklyr|azure-databricks,293,2017-03-09 14:43:18.847000 UTC,2021-10-14 10:10:11.463000 UTC,"Bristol, UK",620,44,1,101,,,,,,[]
One Gremlin query to get all connected vertices for multiple vertices,"<p>may I get some help from improving our query?</p>
<p>So the idea is that we have many connected sub-graphs, and each vertex has an unique id. Now we know some of ids, and we want to get all of the connected vertices in one query.</p>
<p>For example, we have <strong>a -&gt; b &lt;- c -&gt; d</strong>, and <strong>e -&gt; f &lt;- g</strong>. Now the input is <strong>{b, e}</strong>, and the result we want is <strong>{a, b, c, d, e, f, g}</strong>. Because, {a, c, d} is connected to b and {f, g} is connected to e.</p>
<p>Current I’m using a very dumb query like</p>
<pre><code>g.V(&quot;b&quot;).emit().repeat(both().simplePath()).aggregate(&quot;connected&quot;)
 .V(&quot;e&quot;).emit().repeat(both().simplePath()).aggregate(&quot;connected&quot;)
 .select(&quot;connected&quot;).unfold().dedup()
</code></pre>
<p>which might work sometimes, but when (if) all the vertices are already connected to each other, I will run into MemoryLimitExceededException</p>",1,0,2021-04-13 00:02:47.910000 UTC,,2021-04-13 08:29:26.677000 UTC,0,java|gremlin|graph-databases|amazon-neptune,218,2021-04-12 23:53:10.487000 UTC,2021-08-17 23:25:43.207000 UTC,"Seattle, WA, USA",1,0,0,0,,,,,,[]
Unable to load the data from a MS SQL database from Microsoft Azure Databricks notebook,"<p>I want to retrieve the data from a MS SQL database (not hosted on Azure) to Microsoft Azure Databricks notebook.
Here are the steps of what I have done:</p>
<ol>
<li>go on the portal of azure and create a resource groups</li>
<li>create Azure Databricks Service (but I do NOT use the 'Deploy Azure Databricks workspace in your own Virtual Network (VNet)' option → maybe I should...)</li>
<li>once Azure Databricks Service is ready, I launch it and create a cluster without specific configuration</li>
<li>then I create a notebook (running on the previous cluster) with this script</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>msSqlServer = &quot;jdbc:sqlserver://xxx.xxx.xxx.xxx:1433;ApplicationIntent=readonly;databaseName=&quot; + msSqlDatabase
query = &quot;&quot;&quot;(select * from mytable)foo&quot;&quot;&quot;

df = (
  spark.read.format(&quot;jdbc&quot;)
  .option(&quot;url&quot;, msSqlServer)
  .option(&quot;dbtable&quot;, query)
  .option(&quot;user&quot;, msSqlUser)
  .option(&quot;password&quot;, msSqlPassword)
  .load()
)
</code></pre>
<p>and I get this error:</p>
<pre><code>com.microsoft.sqlserver.jdbc.SQLServerException: The TCP/IP connection to the host xxx.xxx.xxx.xxx, port 1433 has failed. Error: &amp;#34;connect timed out. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.&amp;#34;.
</code></pre>
<p>Before asking on StackoverFlow, I have contacted my company network and DBA teams. The DBA said the connection is okay but then disconnects immediatly&quot;</p>
<p>For your information, I have followed this tutorial <a href=""https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/sql-databases"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/sql-databases</a></p>
<p>Maybe there is something to configure, but I am not in networking at all (I am just a little data scientist who wants to play with notebook on azure databricks and access his company databases). For example how can I <code>Make sure that TCP connections to the port are not blocked by a firewall</code> ?</p>
<p>If you have some idea or you have already met this issue, feel free to share. :)</p>
<p>If you need more information, please tell me.</p>",1,2,2020-09-03 14:33:58.497000 UTC,,2020-09-18 08:44:37.697000 UTC,0,sql-server|apache-spark|azure-databricks,506,2019-01-14 11:16:25.457000 UTC,2022-02-17 17:37:14.647000 UTC,,195,174,0,63,,,,,,[]
Azure DataBricks - webdriver installation best practices,"<p>What are the best practices regarding usage and installation of webdrivers in Azure DataBricks?</p>
<p>Currently I'm checking for the existence of <code>/tmp/chromedriver/chromedriver</code>, and if it does not, running the following:</p>
<pre class=""lang-py prettyprint-override""><code>installCommands = [
  'wget https://chromedriver.storage.googleapis.com/83.0.4103.39/chromedriver_linux64.zip -O /tmp/chromedriver_linux64.zip',
  'unzip /tmp/chromedriver_linux64.zip -d /tmp/chromedriver/',
  'sudo add-apt-repository ppa:canonical-chromium-builds/stage',
  '/usr/bin/yes | sudo apt update',
  '/usr/bin/yes | sudo apt install chromium-browser'
]

if not exists(driverPath):
  print('Downloading and installing ChromeDriver')
  for command in installCommands:
    print(f'&gt; {command}')
    system(command)
  print('Installation complete')
</code></pre>
<p>From what I've observed:</p>
<ul>
<li>the installation is specific to each notebook</li>
<li>installations aren't persistent, depending on cluster liveliness? possibly frequency of runs?</li>
<li>the download appears to cache, potentially same reasoning as above</li>
</ul>
<p>Would storing and retrieving the unpacked driver in something like <code>dbfs:FileStore/WebDrivers/...</code> make more sense, or is there a better method of having persistent installations to be shared by multiple notebooks?</p>
<p>Edit: I've tried importing chromedriver_binary after installing the specific version I need (<code>chromedriver-binary==83.0.4103.39.0</code>) at the cluster level, but does not appear to work</p>",0,2,2020-06-23 23:52:57.507000 UTC,1.0,2020-06-24 05:45:58.853000 UTC,0,python|selenium|pyspark|azure-databricks,373,2015-11-10 05:04:41.180000 UTC,2020-09-07 04:44:50.010000 UTC,,1,0,0,5,,,,,,[]
error: value show is not a member of Unit CaseFileDFTemp.show(),"<p>I ran below code in <code>databricks</code> scala notebook but I am getting error.</p>

<p>LIBRARY ADDED : azure-cosmosdb-spark_2.4.0_2.11-1.3.4-uber 
CODE :</p>

<pre><code>import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

import spark.implicits._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.Column
import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType,LongType,FloatType,DoubleType, TimestampType}
import org.apache.spark.sql.cassandra._

//datastax Spark connector
import com.datastax.spark.connector._
import com.datastax.spark.connector.cql.CassandraConnector
import com.datastax.driver.core.{ConsistencyLevel, DataType}
import com.datastax.spark.connector.writer.WriteConf

//Azure Cosmos DB library for multiple retry
import com.microsoft.azure.cosmosdb.cassandra

import sqlContext.implicits._
spark.conf.set(""x"",""x"")
spark.conf.set(""x"",""x"")
spark.conf.set(""x"",""x"")
spark.conf.set(""x"",""x"")

val CaseFileDFTemp = sqlContext
  .read
  .format(""org.apache.spark.sql.cassandra"")
  .options(Map( ""table"" -&gt; ""case_files"", ""keyspace"" -&gt; ""shared""))
.load().show()

CaseFileDFTemp.show()

</code></pre>

<p><strong>ERROR:</strong></p>

<blockquote>
  <p>error: value show is not a member of Unit CaseFileDFTemp.show()</p>
</blockquote>",2,0,2020-01-10 06:04:47.847000 UTC,,2020-01-10 06:14:57.220000 UTC,0,sql|scala|apache-spark|azure-databricks|azure-cosmosdb-cassandra-api,1941,2017-09-23 03:41:56.653000 UTC,2020-01-23 07:06:03.373000 UTC,,1,0,0,7,,,,,,[]
How to make a generic pipeline for data transformation using Azure Databricks and Data Factory,"<p>I have a requirement to create a GUI to get some user input and also they can import a CSV file from GUI. Once the file is imported, I want to do data transformation on that file using Azure databricks(pyspark) and store the transformed data somewhere so that the user can download the transformed data. I would like to know how to make it a generic pipeline so that anyone in the organization can upload their file(it can have different columns and datatypes) and databricks does the transformation and stores the result. And for all these activities I want to leverage the Azure platform.</p>",1,0,2020-08-28 05:46:11.877000 UTC,,2020-08-31 07:21:34.730000 UTC,0,azure|azure-data-factory|azure-web-app-service|azure-databricks,171,2018-03-25 02:34:22.113000 UTC,2022-03-02 12:55:39.133000 UTC,,359,71,0,157,,,,,,[]
Azure Pipeline - Post-Deployment Gate for Databricks Job,"<p><strong>UPDATE:</strong> I am significantly editing this question as comments below highlighted the need for more clarity and detail</p>
<p>Objective is to have a multi-stage pipeline defined in a yaml file. The first stage &quot;Build&quot; has a task which triggers a Databricks job from Azure Pipeline (using databricks cli). This job may take a while to finish. Build Stage should only succeed after job status has been confirmed with &quot;SUCCESS&quot;.</p>
<p>One approach would be to add a while loop that checks for job status every X minutes, see code example below (only shows the task of triggering the job and checking the status / example taken from <a href=""https://github.com/SaschaDittmann/MLOps-Databricks/blob/master/azure-pipelines.yml"" rel=""nofollow noreferrer"">here</a>):</p>
<pre><code>stages:
- stage: Build
  displayName: 'Train Model'
  jobs:
  - job: Train
    displayName: 'Train Model'
    pool:
      vmImage: 'ubuntu-18.04'
    steps:

    - task: Bash@3
      displayName: 'Run Training Job'
      inputs:
        targetType: 'inline'
        script: |
          run_id1=$(databricks jobs run-now --job-id $(databricks.job.train.id) | jq &quot;.run_id&quot;)
          run_state=$(databricks runs get --run-id $run_id1 | jq -r &quot;.state.life_cycle_state&quot;)
          while [ $run_state == &quot;RUNNING&quot; -o $run_state == &quot;PENDING&quot; ]
          do
            sleep 30
            run_state=$(databricks runs get --run-id $run_id1 | jq -r &quot;.state.life_cycle_state&quot;)
          done
          result_state1=$(databricks runs get --run-id $run_id1 | jq -r &quot;.state.result_state&quot;)
          
          if [ $result_state1 == &quot;SUCCESS&quot; ]
          then
            exit 0
          else
            exit 1
          fi
          
- stage: Staging
  displayName: 'Deploy to Staging'
  dependsOn: Build
  condition: succeeded()
</code></pre>
<p>I cannot use the above approach because it blocks the build agent from the available pool of agents for other pipelines in the queue.</p>
<p>Comment below points towards using <a href=""https://docs.microsoft.com/en-us/azure/devops/pipelines/release/approvals/gates?view=azure-devops"" rel=""nofollow noreferrer"">post-deployment gate</a> as a potential solution for this problem.</p>
<p>How could this be implemented? yaml code snippet to indicate the solution would aready be very helpful</p>",0,3,2021-10-26 16:23:04.227000 UTC,,2021-10-27 16:18:06.807000 UTC,0,azure-pipelines|azure-databricks|azure-deployment,56,2015-07-09 13:14:04.057000 UTC,2022-03-05 15:18:55.723000 UTC,,734,75,2,90,,,,,,[]
Repeat until at least one vertex?,"<p>The following query returns three items in the dataset I'm using:</p>
<p><code>...in().hasLabel(&quot;product&quot;)</code></p>
<p>I would expect this other query to also return three items</p>
<p><code>...repeat(in_().hasLabel(&quot;product&quot;)).until(hasLabel(&quot;product&quot;).count().is(gt(0)))</code></p>
<p>But it returns more than three items. My guess is that count is applied to each path. I would like to apply path to the total of all paths.</p>
<p>How can I achieve this?</p>",0,1,2021-05-24 14:29:41.877000 UTC,,,0,gremlin|amazon-neptune,26,2012-03-21 15:20:44.840000 UTC,2022-03-04 10:41:46.517000 UTC,,15380,642,6,1371,,,,,,[]
How to trigger Slack notifications as a Validation Action in Great_Expectations,"<p>I have the following Great_Expectation in Apache Spark with Databricks and Synapse on Apache Spark</p>
<pre><code>ge_df.expect_column_values_to_be_between('load_id', min_value=1000, max_value=1049). 
</code></pre>
<p>I have tried to add a slack notification to the Great_Expectation as follows:</p>
<pre><code>ge_df.expect_column_values_to_be_between('load_id', min_value=1000, max_value=1049, slack_render = {&quot;text&quot;: (test information,['MY_SLACK_WEBHOOK'])
</code></pre>
<p>But it fails</p>
<p>Can someone let me know where I might be going wrong?</p>
<p>I also tried the following, but it was a little too advanced for me:</p>
<pre><code>if not validation_results[&quot;success&quot;]:
    num_evaluated = validation_results[&quot;statistics&quot;][&quot;evaluated_expectations&quot;]
    num_successful = validation_results[&quot;statistics&quot;][&quot;successful_expectations&quot;]
    validation_results_text = json.dumps(
        [result.to_json_dict() for result in validation_results[&quot;results&quot;]],
        sort_keys=True,
        indent=4,
    )
    slack_renderer = {
        &quot;text&quot;: (
            f&quot;⚠️ Dataset has failed expecations\n&quot;
            f&quot;*Successful Expectations*: `{num_successful}/{num_evaluated}`\n&quot;
            f&quot;*Results*: ```\n{validation_results_text}\n```&quot;
        )
    }

    response = requests.post(
        os.environ['SLACK_WEBHOOK'],
        data=json.dumps(slack_renderer),
        headers={&quot;Content-Type&quot;: &quot;application/json&quot;},
    )
</code></pre>
<p>I get the error when I enter slack_webhook</p>
<pre><code>KeyError: 'https://hooks.slack.com/services/T1L0WSW9F/B028H2KKPU3/56EZfTdU1oIprsrxxx'
Traceback (most recent call last):

  File &quot;/home/trusted-service-user/cluster-env/env/lib/python3.6/os.py&quot;, line 669, in __getitem__
    raise KeyError(key) from None

KeyError: 'https://hooks.slack.com/services/T1L0WSW9F/B028H2KKPU3/56EZfTdU1oIprsrxxx'
</code></pre>
<p>I believe the problem is here:</p>
<pre><code>response = requests.post(
        os.environ['https://hooks.slack.com/services/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'],
        data=json.dumps(slack_renderer),
        headers={&quot;Content-Type&quot;: &quot;application/json&quot;},
    )
</code></pre>
<p>Any help greatly appreciated.</p>",1,1,2021-07-19 17:36:17.730000 UTC,,2021-07-19 19:55:19.003000 UTC,0,apache-spark|pyspark|azure-databricks|azure-synapse,151,2021-03-31 08:36:43.863000 UTC,2022-03-05 23:03:22.193000 UTC,"London, UK",651,58,0,93,,,,,,[]
Adding Multiple Unique Vertices,"<p>I want to write a single gremlin query that will create multiple vertices, but only does so if they are all unique. I know I can use the get or addV method by using coalesce step mentioned in the comments. </p>

<p><code>g.V().has('com.demo.test', '__type', 'Namespace').fold().coalesce(unfold(), addV('com.demo.test').property('__type', 'Namespace'))</code></p>

<p>This will hadd a single vertex only if it does not exist already. What if i want to do this same procedure for multiple edges and vertices all in a single query? My goal is that if one of the vertices/edges is not unique, none of them are created. However I understand that may not be possible so all answers are welcome.</p>

<p>Thanks</p>",1,7,2018-08-03 15:01:05.737000 UTC,0.0,2018-08-07 01:08:27.017000 UTC,0,gremlin|tinkerpop3|amazon-neptune,588,2015-04-16 19:03:51.700000 UTC,2022-02-08 18:17:58.507000 UTC,Cincinnati,85,2,0,20,,,,,,[]
Scalable Power BI data model on delta lake ADLS Gen2,"<p>Our current architecture for reporting and dashboarding is similar to the following:</p>
<p>[Sql Azure] &lt;-&gt; [Azure Analysis Services (AAS)] &lt;-&gt; [Power BI]</p>
<p>We have almost 30 Power BI Pro Licenses (no Premium Tier)</p>
<p>As we migrate our on-premise data feeds to ADLS Gen2 with Data Factory and Databricks (in the long run, we will dismiss SQL Azure DBs), we investigate how to connect Power BI to the delta tables.</p>
<p>Several approaches suggest using SQL Databricks endpoints for this purpose:</p>
<p><a href=""https://www.youtube.com/watch?v=lkI4QZ6FKbI&amp;t=604s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=lkI4QZ6FKbI&amp;t=604s</a></p>
<p>IMHO this is nice as long as you have a few reports. What if you have, say, 20-30? Is there a middle layer between ADLS Gen2 delta tables and Power BI for a scalable and efficient tabular model? How to define measures, calculated tables, manage relationships efficiently without the hassle of doing this from scratch in every single .pbix?</p>
<p>[ADLS Gen2] &lt;-&gt; [?] &lt;-&gt; [Power BI]</p>
<p>As far as I can tell, no AAS Direct Query is allowed in this scenario:
<a href=""https://docs.microsoft.com/en-us/azure/analysis-services/analysis-services-datasource"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/analysis-services/analysis-services-datasource</a></p>
<p>Is there a workaround to avoid the use of Azure Synapse Analytics? We are not using it, and I am afraid we will not include it in the roadmap.</p>
<p>Thanks in advance for your invaluable piece of advice</p>",1,0,2022-02-16 22:43:32.260000 UTC,,,0,powerbi|azure-databricks|azure-synapse|delta-lake|azure-analysis-services,27,2017-07-16 07:55:39.347000 UTC,2022-03-04 16:13:46.327000 UTC,"Milano, Metropolitan City of Milan, Italy",11,0,0,3,,,,,,[]
Batching operations in gremlin on Neptune,"<p>In AWS Neptune documentation (<a href=""https://docs.aws.amazon.com/neptune/latest/userguide/best-practices-gremlin-java-batch-add.html"" rel=""nofollow noreferrer"">best-practices-gremlin-java-batch-add</a>) they recommend batching operations together.<br />
How can I batch a few operations together in case one of them may end the stream.<br />
For example if I want to batch together the following:</p>
<pre><code>g.V(2).drop().addV('test').property(id,1)
</code></pre>
<p>The problem is that the addV won't be called.<br />
Is there a way to batch the drop and the addV together and making sure the addV will be called?<br />
I tried to put fold() in between but because it isn't supported natively in Neptune and will probably create performance issues.<br />
The sideEffect isn't a good option for performance reasons with Neptune as well (see drop documentation in <a href=""https://docs.aws.amazon.com/neptune/latest/userguide/gremlin-step-support.html"" rel=""nofollow noreferrer"">gremlin-step-support</a>).</p>",1,0,2021-09-27 00:25:55.057000 UTC,1.0,2021-09-27 11:01:25.540000 UTC,2,gremlin|amazon-neptune,49,2011-09-04 11:21:23.543000 UTC,2022-03-06 00:01:34.273000 UTC,,6121,565,1,373,,,,,,[]
Pushing specific commits with git (compared to ClearCase),"<p>We are currently using ClearCase for managing our source code.</p>

<p>With ClearCase, i'm used to making several changes, and to be able to commit (check in) these changes to the server in any way that i want.</p>

<p>This also means i can edit (check out and modify) 10 different files, but check in only some of these back to the server, and in any order that i'd like to.</p>

<p>With git, commiting locally forces me to push the changes back to the server in that particular order.</p>

<p>Is there any workflow in Git, that is similar to the one i'm used to with ClearCase? (being able to push only some of my local commits to the server without loads of work).</p>",2,1,2012-07-24 08:37:08.537000 UTC,,,1,git|version-control|clearcase|dvcs,139,2010-11-01 23:01:19.087000 UTC,2022-03-02 08:59:13.857000 UTC,,18570,2260,20,1022,,,,,,[]
how do you use target data-validator in azure databricks?,"<p>I'm trying to run the data validation framework called data-validator created by Target to validate data from a parquet file in Azure databricks.</p>
<p>I have created a spark job that will use the data-validator fat jar file.</p>
<p>If I give a parameter --help, I can get the help regarding how to use data-validator, but when I pass the --config test_config.yaml file, the data-validator can't find the file.
<a href=""https://i.stack.imgur.com/kZ2p1.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<pre><code>OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0
Warning: Ignoring non-Spark config property: libraryDownload.sleepIntervalSeconds
Warning: Ignoring non-Spark config property: libraryDownload.timeoutSeconds
Warning: Ignoring non-Spark config property: eventLog.rolloverIntervalSeconds
21/12/30 06:17:29 INFO Main$: Logging configured!
21/12/30 06:17:29 INFO Main$: Data Validator
21/12/30 06:17:30 INFO ConfigParser$: Parsing `dbfs:/FileStore/shared_uploads/jyoti/test_config.yaml`
21/12/30 06:17:30 INFO ConfigParser$: Attempting to load `dbfs:/FileStore/shared_uploads/jyoti/test_config.yaml` from file system
21/12/30 06:17:30 ERROR Main$: Failed to parse config file 'dbfs:/FileStore/shared_uploads/jyoti/test_config.yaml, {}
DecodingFailure(java.io.FileNotFoundException: dbfs:/FileStore/shared_uploads/jyoti/test_config.yaml (No such file or directory)
    at java.io.FileInputStream.open0(Native Method)
    at java.io.FileInputStream.open(FileInputStream.java:195)
    at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:138)
    at scala.io.Source$.fromFile(Source.scala:94)
    at scala.io.Source$.fromFile(Source.scala:79)
    at scala.io.Source$.fromFile(Source.scala:57)
    at com.target.data_validator.ConfigParser$.com$target$data_validator$ConfigParser$$loadFromFile(ConfigParser.scala:39)
    at com.target.data_validator.ConfigParser$$anonfun$6.apply(ConfigParser.scala:57)
    at com.target.data_validator.ConfigParser$$anonfun$6.apply(ConfigParser.scala:54)
    at scala.util.Try$.apply(Try.scala:213)
    at com.target.data_validator.ConfigParser$.parseFile(ConfigParser.scala:53)
    at com.target.data_validator.Main$.loadConfigRun(Main.scala:23)
    at com.target.data_validator.Main$.main(Main.scala:171)
    at com.target.data_validator.Main.main(Main.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:951)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
, List())
21/12/30 06:17:30 ERROR Main$: data-validator failed!

</code></pre>
<p>I had stored the yaml file in the dbfs.</p>
<p>Please let me know how to pass the YAML config file in data-validator using the spark-job in databricks.</p>",1,0,2021-12-30 06:02:34.487000 UTC,,2021-12-30 06:23:15.743000 UTC,1,apache-spark|validation|azure-databricks,56,2020-09-27 19:00:14.083000 UTC,2022-03-05 16:15:56.760000 UTC,,11,0,0,5,,,,,,[]
how can we run Neptune graph database on docker,"<p>how can we run Neptune graph database on docker</p>

<p>Since Neptune DB has been productized recently it is not available on Localstack can someone guide me how to deploy AWS Neptune DB Service into docker container</p>",4,1,2018-07-11 15:47:51.597000 UTC,,2018-07-11 16:58:14.087000 UTC,4,amazon-web-services|docker|docker-compose|amazon-neptune|localstack,2479,2018-07-11 15:43:22.250000 UTC,2019-09-22 12:05:13.850000 UTC,,41,0,0,4,,,,,,[]
Null pointer exception while writing from Spark to Cassandra,"<p>I am using spark-cassandra-connector-2.4.0-s_2.11 to write data from spark to Cassandra on Databricks cluster.</p>

<p>I am getting java.lang.NullPointerException while writing data from Spark to Cassandra. This is working fine with few records.</p>

<p>But getting issue when I try to load <strong>~150 Million</strong> records.</p>

<p>Can someone help me in finding the root cause?</p>

<p>Here is the code snippet:</p>

<pre><code>val paymentExtractCsvDF = spark
                          .read
                          .format(""csv"")
                          .option(""header"", ""true"")
                          .load(/home/otl/extract/csvout/Payment)

    paymentExtractCsvDF.printSchema()

root
 |-- BAN: string (nullable = true)
 |-- ENT_SEQ_NO: string (nullable = true)
 |-- PYM_METHOD: string (nullable = true)

case class Payment(account_number: String, entity_sequence_number: String, payment_type: String)
val paymentResultDf = paymentExtractCsvDF.map(row =&gt; Payment(row.getAs(""BAN""),
        row.getAs(""ENT_SEQ_NO""),
        row.getAs(""PYM_METHOD""))).toDF()

var paymentResultFilterDf = paymentResultDf
                            .filter($""account_number"".isNotNull || $""account_number"" != """")
                            .filter($""entity_sequence_number"".isNotNull || $""entity_sequence_number"" != """")

paymentResultFilterDf
  .write
  .format(""org.apache.spark.sql.cassandra"")
  .mode(""append"")
  .options(Map( ""table"" -&gt; ""cassandratable"", ""keyspace"" -&gt; ""cassandrakeyspace""))
  .save()
</code></pre>

<p>here is the exception I am getting:</p>

<pre><code>Failed to write statements to cassandrakeyspace.cassandratable. The
latest exception was
  An unexpected error occurred server side on /10.18.15.198:9042: java.lang.NullPointerException

Please check the executor logs for more exceptions and information

    at com.datastax.spark.connector.writer.TableWriter$$anonfun$writeInternal$1$$anonfun$apply$3.apply(TableWriter.scala:243)
    at com.datastax.spark.connector.writer.TableWriter$$anonfun$writeInternal$1$$anonfun$apply$3.apply(TableWriter.scala:241)
    at scala.Option.map(Option.scala:146)
    at com.datastax.spark.connector.writer.TableWriter$$anonfun$writeInternal$1.apply(TableWriter.scala:241)
    at com.datastax.spark.connector.writer.TableWriter$$anonfun$writeInternal$1.apply(TableWriter.scala:210)
    at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withSessionDo$1.apply(CassandraConnector.scala:112)
    at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withSessionDo$1.apply(CassandraConnector.scala:111)
    at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:145)
    at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:111)
    at com.datastax.spark.connector.writer.TableWriter.writeInternal(TableWriter.scala:210)
    at com.datastax.spark.connector.writer.TableWriter.insert(TableWriter.scala:197)
    at com.datastax.spark.connector.writer.TableWriter.write(TableWriter.scala:183)
    at com.datastax.spark.connector.RDDFunctions$$anonfun$saveToCassandra$1.apply(RDDFunctions.scala:36)
    at com.datastax.spark.connector.RDDFunctions$$anonfun$saveToCassandra$1.apply(RDDFunctions.scala:36)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:139)
    at org.apache.spark.scheduler.Task.run(Task.scala:112)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:497)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1526)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:503)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
19/11/22 01:12:17 INFO CoarseGrainedExecutorBackend: Got assigned task 1095
19/11/22 01:12:17 INFO Executor: Running task 39.1 in stage 21.0 (TID 1095)
19/11/22 01:12:17 INFO ShuffleBlockFetcherIterator: Getting 77 non-empty blocks including 10 local blocks and 67 remote blocks
19/11/22 01:12:17 INFO ShuffleBlockFetcherIterator: Started 7 remote fetches in 3 ms
19/11/22 01:12:17 INFO ShuffleBlockFetcherIterator: Getting 64 non-empty blocks including 8 local blocks and 56 remote blocks
19/11/22 01:12:17 INFO ShuffleBlockFetcherIterator: Started 7 remote fetches in 1 ms
19/11/22 01:12:17 ERROR Executor: Exception in task 7.0 in stage 21.0 (TID 1012)
</code></pre>",1,4,2019-11-23 16:29:08.470000 UTC,,,0,apache-spark|datastax-enterprise|cassandra-3.0|spark-cassandra-connector|azure-databricks,823,2015-10-11 15:43:58.547000 UTC,2020-05-29 10:18:11.063000 UTC,,189,3,0,86,,,,,,[]
Getting getaddrinfo ENOTFOUND when trying to connect to my AWS Neptune with node.js and gremlin,"<p>I am trying to connect to my Amazon Neptune instance from a API GW. I am using Node.js and Lambda</p>
<p>My YML looks like this</p>
<pre><code>  NeptuneDBCluster:
    Type: &quot;AWS::Neptune::DBCluster&quot;

Outputs:
  NeptuneEndpointAddress:
    Description: Neptune DB endpoint address
    Value: !GetAtt NeptuneDBCluster.Endpoint
    Export:
      Name: !Sub ${env:STAGE}-neptune-endpoint-address

</code></pre>
<p>My code looks like this</p>
<pre><code>const gremlin = require('gremlin');
const {
    NEPTUNE_ENDPOINT
} = process.env;
const { cardinality: { single } } = gremlin.process;
const DriverRemoteConnection = gremlin.driver.DriverRemoteConnection;
const Graph = gremlin.structure.Graph;

async function createUserNode(event, context, callback) {
    const url = 'wss://&quot; + NEPTUNE_ENDPOINT + &quot;:8182/gremlin';
    const dc = new DriverRemoteConnection(url);

    const parsedBody = JSON.parse(event.body);


    try {
        const graph = new Graph();
        const g = graph.traversal().withRemote(dc);

        const vertex = await g.addV(parsedBody.type)
            .property(single, 'name', parsedBody.name)
            .property(single, 'username', parsedBody.username)
            .property('age', parsedBody.age)
            .property('purpose', parsedBody.purpose)
            .next();

        if (vertex.value) {
            return callback(null, {
                statusCode: 200,
                body: vertex.value
            });
        }

    } catch (error) {
        console.error(error);
    }

};
</code></pre>
<p>I keep getting the folowing error from Cloudwatch (I also tried creating a local js file and it gives the same error)</p>
<pre><code>ERROR   Error: getaddrinfo ENOTFOUND my-url
    at GetAddrInfoReqWrap.onlookup [as oncomplete] (dns.js:66:26) {
  errno: 'ENOTFOUND',
  code: 'ENOTFOUND',
  syscall: 'getaddrinfo',
  hostname: 'my-url'
}
</code></pre>
<p><strong>I also tried to write the endpoint without getting it from process.env and I am still facing the same issue. What am i missing?</strong></p>",1,0,2021-11-19 23:34:21.420000 UTC,,2021-11-20 02:36:07.477000 UTC,2,node.js|aws-lambda|aws-api-gateway|gremlin|amazon-neptune,182,2015-11-03 14:14:32.257000 UTC,2022-03-05 12:58:38.137000 UTC,,109,2,0,11,,,,,,[]
"How to integrate DevOps CI-CD for Azure Databricks (build notebooks,create pipeline)","<p>I am new to Azure Databricks to implement ETL and needs to understand how to integrate CI-CD for databricks.I tried with several sites for reference to understand the process.I created a project and is stuck with pipeline->build for databricks notebook and test plan.Can someone help me out?
I have a dev databricks workspace already provisioned and associated devops project with master and dev branches in devops.
How to set up the build or build server and deploy to other environment workspace</p>

<p>I had referred to below urls prior to posting the question
<a href=""https://thedataguy.blog/ci-cd-with-databricks-and-azure-devops/"" rel=""nofollow noreferrer"">https://thedataguy.blog/ci-cd-with-databricks-and-azure-devops/</a>
<a href=""https://github.com/annedroid/DevOpsforDatabricks"" rel=""nofollow noreferrer"">https://github.com/annedroid/DevOpsforDatabricks</a>
<a href=""https://marketplace.visualstudio.com/items?itemName=DataThirstLtd.databricksDeployScriptsTasks"" rel=""nofollow noreferrer"">https://marketplace.visualstudio.com/items?itemName=DataThirstLtd.databricksDeployScriptsTasks</a></p>",0,0,2019-04-22 17:21:15.620000 UTC,,,2,azure-databricks,275,2019-04-22 16:52:57.357000 UTC,2021-11-25 12:20:05.967000 UTC,,21,0,0,0,,,,,,[]
AWS Neptune/Gremlin query took more than 3 minutes,"<p>I have the following query to check for existing edge, and then insert edge. But it took more than 3 minutes to run it, anyway I can tune up this query?</p>
<pre><code>g.V(&quot;V001&quot;).
  hasLabel(&quot;my-label&quot;).
  out(&quot;parent-my-label&quot;).
  hasId(&quot;V002&quot;).
  hasLabel(&quot;my-label&quot;).
  limit(1).
  coalesce(
    unfold(),
    g.V(&quot;V001&quot;).
      hasLabel(&quot;my-label&quot;).as(&quot;src&quot;).
      V(&quot;V002&quot;).
      hasLabel(&quot;my-label&quot;).as(&quot;dest&quot;).
      addE(&quot;parent-my-label&quot;).from(&quot;src&quot;).to(&quot;dest&quot;))
</code></pre>",0,1,2022-01-21 16:01:21.350000 UTC,,,0,amazon-web-services|gremlin|amazon-neptune,29,2012-02-03 16:20:42.710000 UTC,2022-03-04 16:51:03.243000 UTC,,5628,119,6,508,,,,,,[]
Unable to connect to s3 endpoint from Session Manager,"<p>I am trying to bulk load RDF N-triples data from S3 bucket into Neptune loader. I have created a S3 bucket, IAM Role, Endpoint and Neptune cluster as per the following link <a href=""https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html</a>.</p>
<p>To connect to Neptune instance, I have created a EC2 instance with Session Manager Enabled. I connected with Session Manager and executed following SPARQL ntriples insert command to ingest single data into Neptune instance, which worked fine.</p>
<pre><code>curl -X POST --data-binary 'update=INSERT DATA { triples }' https://your-neptune-endpoint:port/sparql
</code></pre>
<p>But, when I try to execute bulk load data into Neptune DB from S3 bucket using below command, getting the following error,</p>
<p><strong>command executed in Session Manager</strong></p>
<pre><code>curl -X POST \
    -H 'Content-Type: application/json' \
    https://your-neptune-endpoint:port/loader -d '
    {
      &quot;source&quot; : &quot;s3://bucket-name/File-name.nt&quot;,
      &quot;format&quot; : &quot;ntriples&quot;,
      &quot;iamRoleArn&quot; : &quot;arn:aws:iam::account-id:role/role-name&quot;,
      &quot;region&quot; : &quot;us-east-1&quot;,
      &quot;failOnError&quot; : &quot;FALSE&quot;,
      &quot;parallelism&quot; : &quot;MEDIUM&quot;,
      &quot;updateSingleCardinalityProperties&quot; : &quot;FALSE&quot;,
      &quot;queueRequest&quot; : &quot;TRUE&quot;,
    }'
</code></pre>
<p><strong>Error</strong></p>
<p>{&quot;detailedMessage&quot;:&quot;Unable to connect to s3 endpoint. Provided source = s3://bucket-name/File-name.nt and region =us-east-1. Please verify your S3 configuration.&quot;,&quot;code&quot;:&quot;InternalFailureException&quot;,&quot;requestId&quot;:&quot;4xx-xxxxxxxxxxx-xx&quot;}</p>
<p>I also verified S3 configuration and IAM role associated with Neptune DB, it seems to be correct. Am I missing any important configuration part? Can anyone help with this issue?</p>",0,3,2021-08-09 09:45:57.480000 UTC,,,0,amazon-web-services|amazon-s3|amazon-neptune|aws-session-manager,74,2019-12-18 03:36:52.957000 UTC,2021-10-06 13:32:28.730000 UTC,,51,1,0,8,,,,,,[]
Using Azure Data Factory transform multiple Excel data to a main file,"<p>I have two excel files in my Azure Database Container and I would like to transform that data and populate a single database or file in Azure Data Factory.</p>
<p>For Example:</p>
<p>I would like to copy the data from the below excel file:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Product</th>
<th>Units Sold</th>
<th>Sale Price</th>
</tr>
</thead>
<tbody>
<tr>
<td>Carretera</td>
<td>1618.5</td>
<td>$ 20.00</td>
</tr>
<tr>
<td>Carretera</td>
<td>1321</td>
<td>$ 20.00</td>
</tr>
<tr>
<td>Montana</td>
<td>921</td>
<td>$ 15.00</td>
</tr>
<tr>
<td>Montana</td>
<td>2518</td>
<td>$ 12.00</td>
</tr>
<tr>
<td>Paseo</td>
<td>292</td>
<td>$ 20.00</td>
</tr>
<tr>
<td>Paseo</td>
<td>974</td>
<td>$ 15.00</td>
</tr>
</tbody>
</table>
</div>
<p>Into the main file with the headers:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Segment</th>
<th>Country</th>
<th>Product</th>
<th>Discount Band</th>
<th>Units Sold</th>
<th>Manufacturing Price</th>
<th>Sale Price</th>
<th>Gross Sales</th>
<th>Discounts</th>
<th>Sales</th>
<th>COGS</th>
<th>Profit</th>
<th>Date</th>
<th>Month Number</th>
<th>Month Name</th>
<th>Year</th>
</tr>
</thead>
</table>
</div>
<p>Being new to Azure data factory I could not find much relevant data to my query on the internet, therefore it would be great if I got some suggestions or assistance regarding my query.</p>
<p><strong>Update</strong></p>
<p>When performing Mapping in Copy Activity I am not able to see or select the Output columns, the Excel source gets replicated to CSV sink by overwriting the existing data.</p>
<p><a href=""https://i.stack.imgur.com/4qph2.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4qph2.jpg"" alt=""enter image description here"" /></a></p>
<p>Thanks in advance.</p>",1,3,2021-11-07 09:51:58.493000 UTC,,2021-11-15 21:14:00.010000 UTC,1,excel|azure|azure-data-factory|azure-data-factory-2|azure-databricks,150,2021-01-12 16:13:02.437000 UTC,2022-02-09 10:03:22.063000 UTC,,21,1,0,7,,,,,,[]
Neptune/Gremlin RemoteConnectionException,"<p>I see this issue(exception details posted below) quite frequently while making calls to the Neptune server from AWS Lambda.</p>
<pre><code>org.apache.tinkerpop.gremlin.process.remote.RemoteConnectionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.TimeoutException: Timed out while waiting for an available host - check the client configuration and connectivity to the server if this message persists.
</code></pre>
<p>Sometimes, I also see a 'Connection reset by peer' exception.</p>
<p>I have setup the Gremlin Java client as follows, following Neptune's best practices document:</p>
<pre><code>Cluster
      .build()
      .addContactPoint(NeptuneClusterEndpoint)
      .port(8182)
      .maxContentLength(655360)
      .maxInProcessPerConnection(32)
      .maxSimultaneousUsagePerConnection(32)
      .enableSsl(true)
      .create()
</code></pre>
<p>Does anyone know why this happens and how to fix this?</p>",0,2,2020-09-15 22:29:58.903000 UTC,,,1,java|amazon-web-services|aws-lambda|gremlin-server|amazon-neptune,288,2019-10-22 22:17:28.080000 UTC,2020-11-04 21:43:12.517000 UTC,,81,5,0,5,,,,,,[]
How to get the number of active connections from AWS Neptune?,"<p>How to get the number of active connections from AWS Neptune?</p>
<p>I am getting the following error</p>
<pre><code>Cannot write to closing transport: ConnectionResetError
Traceback (most recent call last):
  File &quot;/var/task/chalice/app.py&quot;, line 1569, in __call__
    return self.handler(event_obj)
  File &quot;/var/task/chalice/app.py&quot;, line 1522, in __call__
    return self._original_func(event.to_dict(), event.context)
  File &quot;/var/task/app.py&quot;, line 28, in handler
    return query(event, context)
  File &quot;/var/task/app.py&quot;, line 45, in query
    is_user_available(event)
  File &quot;/var/task/app.py&quot;, line 186, in is_user_available
    user_available = get_g().V(cognito_username).hasNext()
  File &quot;/var/task/gremlin_python/process/traversal.py&quot;, line 80, in hasNext
    self.traversal_strategies.apply_strategies(self)
  File &quot;/var/task/gremlin_python/process/traversal.py&quot;, line 548, in apply_strategies
    traversal_strategy.apply(traversal)
  File &quot;/var/task/gremlin_python/driver/remote_connection.py&quot;, line 63, in apply
    remote_traversal = self.remote_connection.submit(traversal.bytecode)
  File &quot;/var/task/gremlin_python/driver/driver_remote_connection.py&quot;, line 59, in submit
    result_set = self._client.submit(bytecode, request_options=self._extract_request_options(bytecode))
  File &quot;/var/task/gremlin_python/driver/client.py&quot;, line 123, in submit
    return self.submitAsync(message, bindings=bindings, request_options=request_options).result()
  File &quot;/var/lang/lib/python3.6/concurrent/futures/_base.py&quot;, line 432, in result
    return self.__get_result()
  File &quot;/var/lang/lib/python3.6/concurrent/futures/_base.py&quot;, line 384, in __get_result
    raise self._exception
  File &quot;/var/task/gremlin_python/driver/connection.py&quot;, line 66, in cb
    f.result()
  File &quot;/var/lang/lib/python3.6/concurrent/futures/_base.py&quot;, line 425, in result
    return self.__get_result()
  File &quot;/var/lang/lib/python3.6/concurrent/futures/_base.py&quot;, line 384, in __get_result
    raise self._exception
  File &quot;/var/lang/lib/python3.6/concurrent/futures/thread.py&quot;, line 56, in run
    result = self.fn(*self.args, **self.kwargs)
  File &quot;/var/task/gremlin_python/driver/protocol.py&quot;, line 86, in write
    self._transport.write(message)
  File &quot;/var/task/gremlin_python/driver/aiohttp/transport.py&quot;, line 86, in write
    self._loop.run_until_complete(async_write())
  File &quot;/var/lang/lib/python3.6/asyncio/base_events.py&quot;, line 488, in run_until_complete
    return future.result()
  File &quot;/var/task/gremlin_python/driver/aiohttp/transport.py&quot;, line 83, in async_write
    await self._websocket.send_bytes(message)
  File &quot;/var/task/aiohttp/client_ws.py&quot;, line 155, in send_bytes
    await self._writer.send(data, binary=True, compress=compress)
  File &quot;/var/task/aiohttp/http_websocket.py&quot;, line 685, in send
    await self._send_frame(message, WSMsgType.BINARY, compress)
  File &quot;/var/task/aiohttp/http_websocket.py&quot;, line 598, in _send_frame
    raise ConnectionResetError(&quot;Cannot write to closing transport&quot;)
ConnectionResetError: Cannot write to closing transport
</code></pre>",0,1,2021-09-22 07:00:37.480000 UTC,,,0,amazon-web-services|aws-lambda|amazon-neptune|aws-neptune,20,2014-03-07 07:02:31.267000 UTC,2022-03-04 04:37:05.967000 UTC,"Bangalore, India",4909,538,406,442,,,,,,[]
PySpark explode json array issue in Databricks,"<p>I am consuming an api json payload and create a table in Azure Databricks using PySpark explode array and map columns to rows so that the results are tabular with columns &amp; rows.</p>
<p><em><strong>JSON:</strong></em></p>
<pre><code>{
    &quot;Data&quot;: [
        {
            &quot;Id&quot;: &quot;1&quot;,
            &quot;Name&quot;: &quot;New Company&quot;,
            &quot;Flag&quot;: &quot;1&quot;
        },
        {
            &quot;Id&quot;: &quot;2&quot;,
            &quot;Name&quot;: &quot;Old Company&quot;,
            &quot;Flag&quot;: &quot;0&quot;
        }
    ]
}
</code></pre>
<p>PySpark explode works as below.</p>
<pre><code>dataframe.write.format(&quot;delta&quot;).mode(&quot;overwrite&quot;).saveAsTable(&quot;petbl&quot;)
</code></pre>
<p><em><strong>Result:</strong></em></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Id</th>
<th>Name</th>
<th>Flag</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>New Company</td>
<td>1</td>
</tr>
<tr>
<td>One</td>
<td>Old Company</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>When I use the below code the json code remains the same as a string in the table with the data column and 1 row with all the json array.</p>
<pre><code>dataframe.write.format(&quot;delta&quot;).mode(&quot;overwrite&quot;).option(&quot;path&quot;,&quot;abfss://&lt;file_system&gt;@&lt;account_name&gt;.dfs.core.windows.net/&lt;path&gt;/&lt;file_name&gt;&quot;).saveAsTable(&quot;petbl&quot;)
</code></pre>
<p>How to keep the tabular data format intact even after running the above command instead of having a JSON string?</p>",0,0,2021-07-26 17:01:52.470000 UTC,,2021-07-28 00:33:47.650000 UTC,1,pyspark|apache-spark-sql|azure-databricks,127,2016-10-11 08:37:40.020000 UTC,2022-03-02 16:29:37.847000 UTC,,604,41,11,39,,,,,,[]
Some streams terminated before this command could finish! Structured Streaming,"<p>I am trying to read streaming data into Azure Databricks coming from Azure Eventhubs.
This is the code i've been using:</p>
<pre><code>connectionString = &quot;Connection string&quot;
ehConf = {
  'eventhubs.connectionString' : connectionString
}

df = spark \
  .readStream \
  .format(&quot;eventhubs&quot;) \
  .options(**ehConf) \
  .load()
query = df \
    .writeStream \
    .outputMode(&quot;append&quot;) \
    .format(&quot;console&quot;) \
    .start()
</code></pre>
<p>And its giving me an error saying:</p>
<pre><code>ERROR: Some streams terminated before this command could finish!
</code></pre>
<p>I understood that we have to give the Jar file of Azure Eventhub according to the Databricks run time and also according to the spark version.
My spark version is 2.4.5 and Databricks runtime is 6.6, and the jar file i used is azure-eventhubs-spark_2.12-2.3.17.jar for this combination as specified</p>
<p><a href=""https://i.stack.imgur.com/Xcj7o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Xcj7o.png"" alt=""Azure Event Hub jar files recommendations"" /></a></p>
<p>But i'm still facing this issue as &quot;Some streams terminated before this command could finish!&quot;.Can anyone please help me on this.</p>
<p>Thanks</p>",1,1,2020-08-19 02:18:19.890000 UTC,,,0,azure|spark-streaming|azure-eventhub|azure-databricks,1370,2018-09-06 04:18:09.453000 UTC,2022-03-04 07:54:38.533000 UTC,"Hyderabad, Telangana, India",300,12,0,56,,,,,,[]
Connector in Visual Studio Code,"<p>I'm finding hard to do development in Databricks notebook directly. Could anyone share me the best approach to do development in Databricks notebook? Looks like Microsoft Visual Studio Code has connector. Using the connector, would my development and troubleshooting issues would be quick in my Visual Studio Code? Any info. would be really helpful. Thanks in advance.</p>",0,0,2022-01-12 12:20:28.853000 UTC,,,0,azure-databricks,14,2018-01-09 11:01:06.050000 UTC,2022-03-04 17:05:34.420000 UTC,,117,0,0,28,,,,,,[]
"How to cluster based on specific, identical column values","<p>I'm looking at different blockchain transactions and want to cluster <code>INPUT_ADDRESS</code> and <code>OUTPUT_ADDRESS</code> values based on a few rules to help identify if the addresses are being used by the same owner. In short, here are the rules I want to create for the cluster:</p>
<p>First, I want to check to see if there are &gt;=2 of the same <code>TRANS_HASH</code> values. In the event that there is a duplicate value:</p>
<p><strong>(A) if <code>OUTPUT_ADDRESS</code> is identical and/or <code>OUTPUT_AMOUNT</code> is identical, then cluster the <code>INPUT_ADDRESS</code>;
(B) if <code>INPUT_ADDRESS</code> is identical and/or <code>INPUT_AMOUNT</code> is identical, then cluster the <code>OUTPUT_ADDRESS</code></strong></p>
<p>Here's a sample of a table I'm using:</p>
<pre><code>BLOCK_DATE | BLOCK_HEIGHT | TRANS_HASH | INPUT_ADDRESS | OUTPUT_ADDRESS | INPUT_AMOUNT | OUTPUT_AMOUNT
01/11/2020    190            15c7853       xyz             abc              -0.01          0.70
01/11/2020    190            15c7853       def             abc              -0.50          0.70
01/11/2020    191            19vc842       abc             xyz3             -5.03          0.413
01/11/2020    191            19vc842       abcd            xyz3             -0.06          0.201
01/12/2020    191            188fdx8       abc             xyz4             -0.10          0.09
01/12/2020    192            154gf34       xyz1            abc              -0.07          0.18
01/12/2020    192            45f4ti5       ggg             abc              -0.10          0.24
01/12/2020    192            33cv5c5       jjj             abc              -0.08          1.13 
</code></pre>
<p>From this example, we see that <code>TRANS_HASH</code> <strong>15c7853</strong> occurs &gt;=2 times, so now we check to see if the <code>OUTPUT_ADDRESS</code> and/or the OUTPUT_AMOUNT values are identical. Since they both are, we cluster <code>INPUT_ADDRESS</code> <strong>xyz</strong> and <strong>def</strong> with <strong>abc</strong></p>
<p>I want to return something like this:</p>
<pre><code>TRANS_HASH |  OUTPUT_ADDRESS | OUTPUT_AMOUNT | CLUSTERED_INPUT_ADDRESS | CLUSTERED_INPUT_AMOUNT
-----------+-----------------+---------------+-------------------------+-----------------------
15c7853         abc              0.70             xyz                       -0.01
15c7853         abc              0.70             def                       -0.50
</code></pre>
<p>I tried using a HAVING clause to return the duplicates, but I'm not sure if this is the right approach and I'm not sure how to return the clustered addresses/amounts:</p>
<pre><code>SELECT TRANS_HASH, COUNT(*)
FROM blockchain_table
GROUP BY TRANS_HASH
HAVING COUNT(*) &gt;= 2
</code></pre>
<p>Would I need to make a subquery here? Something like:</p>
<pre><code>SELECT TRANS_HASH, OUTPUT_ADDRESS, OUTPUT_AMOUNT, INPUT_ADDRESS AS CLUSTERED_INPUT_ADDRESS, INPUT_AMOUNT AS CLUSTERED_INPUT_AMOUNT
FROM
(
    SELECT TRANS_HASH, COUNT(*)
    FROM blockchain_table
    GROUP BY TRANS_HASH
    HAVING COUNT(*) &gt;= 2
)
</code></pre>
<p>Also, would it be more efficient to create two separate queries to account for (A) and (B)? Or should I write this all in one query?</p>",0,0,2021-03-15 20:40:53.397000 UTC,,,0,sql|azure|azure-databricks,29,2015-10-14 20:03:23.607000 UTC,2021-05-06 13:56:35.667000 UTC,,449,77,0,63,,,,,,[]
Writing Databricks dataframe to BLOB storage,"<p>I am using Azure databricks and have a mounted BLOB store.
I tried saving my dataframe into there using:</p>

<pre><code>df.write.mode(""overwrite"").format(""com.databricks.spark.csv"").option(""header"",""true"").csv(""/mnt/gl"")
</code></pre>

<p>and  I got the following error:</p>

<pre><code> shaded.databricks.org.apache.hadoop.fs.azure.AzureException: java.util.NoSuchElementException: An error occurred while enumerating the result, check the original exception for details.

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
&lt;command-1181559690773266&gt; in &lt;module&gt;()
----&gt; 1 df.write.mode(""overwrite"").format(""com.databricks.spark.csv"").option(""header"",""true"").csv(""/mnt/gl"")

/databricks/spark/python/pyspark/sql/readwriter.py in csv(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue)
    927                        charToEscapeQuoteEscaping=charToEscapeQuoteEscaping,
    928                        encoding=encoding, emptyValue=emptyValue)
--&gt; 929         self._jwrite.csv(path)
    930 
    931     @since(1.5)

/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1255         answer = self.gateway_client.send_command(command)
   1256         return_value = get_return_value(
-&gt; 1257             answer, self.gateway_client, self.target_id, self.name)
   1258 
   1259         for temp_arg in temp_args:

/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
</code></pre>

<p>Update:
Error now says 'Job Aborted' after I recreated the SAS key</p>",1,4,2019-01-14 00:38:34.670000 UTC,,2019-01-14 21:59:30.587000 UTC,2,azure-databricks,2503,2012-10-20 16:18:44.083000 UTC,2022-03-03 10:41:10.747000 UTC,,5331,89,4,402,,,,,,[]
Mercurial repository with bitbucket subrespository - how to prevent push,"<p>I am in the process of setting up some third-party subrepositories under a Mercurial repository. One subrepo is another Mercurial repo hosted on Bitbucket.</p>

<p>Since it is a public repo, and I am not a contributor to it, I don't wish to push back to it. However I would like to still have the repository automatically cloned when I clone the parent repository. For one thing, I'd like to have access to the collective history of the subrepository so I can see what may or may not have changed over time.</p>

<p>So, I made an entry in the parent repo's <code>.hgsub</code> file as follows:</p>

<pre><code>thesubrepo = https://bitbucket.org/user/repo
</code></pre>

<p>and cloned the repo using</p>

<pre><code>$ hg clone https://bitbucket.org/user/repo thesubrepo
</code></pre>

<p>I made a commit to record the subrepo state. I then went to push my parent repo back to it's server (Kiln) only to discover that it was trying to push the subrepo I back to the Bitbucket server. The push to the Bitbucket subrepository appears to not do anything, though.</p>

<p>I did not observe this behaviour when I made a Git subrepo in the same manner (hosted on Git hub) using an entry in <code>.hgsub</code> like this</p>

<pre><code>abc = [git]git://github.com/xyz/abc
</code></pre>

<p>Is it best for me just to do this by not setting up a subrepository, and just let Mercurial store the files as files? Or (preferably) is there some setting somewhere that I can use to tell Mercurial to never actually push the contents of this subrepo back to it's source location?</p>

<p>I'd rather be able to configure it to only push those subrepos manually, so if anyone can shed some light on this, I would appreciate it.</p>

<p>I found a reference to <a href=""https://stackoverflow.com/a/6942351/830899""><code>commitsubrepos = no</code></a> in another stack overflow answer, which as far as i can tell is about commits, and not pushes of sub repositories. I then looked this up <a href=""http://www.selenic.com/mercurial/hgrc.5.html#ui"" rel=""nofollow noreferrer"">on the mercurial website</a>, in the hope there might be some reference to a setting pertaining to pushing subrepos, but... no</p>",1,1,2012-01-20 02:21:38.427000 UTC,,2017-05-23 11:48:08.533000 UTC,4,mercurial|dvcs|bitbucket|subrepos|kiln,362,2010-12-28 02:12:25.580000 UTC,2021-08-16 14:07:32.410000 UTC,New Zealand,4679,27,5,1005,,,,,,[]
pyspark - Run a spark sql query in parallel for multiple ids in a list,"<p>I have a list, let's say</p>

<pre><code>ids = ['K50', 'K51', 'K51', 'P41', 'P41', 'P42']  
</code></pre>

<p>What I need to achieve - </p>

<ol>
<li>Take each id from the list</li>
<li>Get data from a hive table (eg. tableA ) for each site.</li>
<li>Pivot some values for that data. </li>
<li>Write to a parquet file for each id. </li>
</ol>

<p>Note - I need to run this job in parallel for each id in the list. </p>

<p>I have looked at several posts but did not find any concrete solution. How can I solve this problem in pyspark?</p>

<p>Spark Version - 2.4.3</p>",1,1,2019-09-23 11:14:45.787000 UTC,,2019-09-25 12:51:40.247000 UTC,0,apache-spark|pyspark|azure-databricks,691,2017-08-30 09:56:47.050000 UTC,2020-03-06 10:37:44.563000 UTC,"Edinburgh, UK",72,2,0,20,,,,,,[]
StringInderxer and One hot encoding in SparkR,"<p>I am trying to convert string variable in SparkR to numeric by using one hot encoding concept and using stringindexer on below code:
df&lt;-ft_string_indexer(spark_df,input_col=cluster_group,output_col=new)
However, I am getting below error:
no applicable method for 'ft_string_indexer' applied to an object of class &quot;SparkDataFrame&quot;</p>
<p>Any idea on correct code for stringindexer and Onehotencoding in SparkR?</p>",1,0,2020-10-05 10:20:19.250000 UTC,,,0,azure-databricks|sparkr|sparklyr,99,2016-02-24 10:44:15.857000 UTC,2021-02-05 10:58:26.373000 UTC,,11,0,0,5,,,,,,[]
Execute python script from azure data factory,"<p>Can someone help me with executing python function from azure data factory.
I have stored python function in blob and i'm trying to trigger the same.
However i'm not able to do it. Please assist.</p>

<p>Second, Can i parameterize python function call from ADF?</p>",1,0,2019-03-20 14:46:43.467000 UTC,,,1,python|azure|azure-data-factory|azure-databricks|azure-triggers,2740,2018-08-01 14:58:33.147000 UTC,2019-09-20 09:43:02.913000 UTC,,111,0,0,11,,,,,,[]
Trying to read events from event hub in azure databricks in python,"<p>I am trying to read data from event hubs. I am sending twitter data as json through event hub sender.
When I am trying to read the data and append it in a list I am getting the following error.Where am I possibly wrong?
below code:</p>

<pre><code>        for tweet in ts.search_tweets_iterable(tso):
            print(tweet)
            sender.send(EventData(tweet))


try:
    receiver = client.add_receiver(CONSUMER_GROUP, PARTITION, prefetch=5000, offset=OFFSET)
    client.run()
    start_time = time.time()
    for event_data in receiver.receive(timeout=100):
        last_offset = event_data.offset
        last_sn = event_data.sequence_number
        messages.append(next(event_data.body))
        total += 1
</code></pre>

<p>I am getting the below error:</p>

<p><strong>TypeError: 'dict' object is not an iterator</strong></p>",1,1,2019-09-23 18:34:15.953000 UTC,,,0,python|azure-eventhub|azure-databricks,119,2019-06-06 12:01:42.137000 UTC,2021-07-09 14:14:02.727000 UTC,,31,6,0,16,,,,,,[]
Databricks- PGP Encryption,"<p>I need to Decrypt a file arrived in Azure Blob before futher processing, Encryption Key is stored in KeyVault.  Once processed need to send the file to downstream system's SFTP site Encrypted.</p>
<p>Can this be done in Databricks (Decryption  / Encryption), and example or links</p>",0,0,2021-07-26 20:16:19.780000 UTC,,,0,encryption|azure-databricks|pgp,118,2009-04-02 00:53:40.477000 UTC,2022-03-01 22:30:30.100000 UTC,"Brisbane QLD, Australia",27501,66,1,1101,,,,,,[]
Issue connecting to Databricks table from Azure Data Factory using the Spark odbc connector,"<p>​We have managed to get a valid connection from Azure Data Factory towards our Azure Databricks cluster using the Spark (odbc) connector. In the list of tables we do get the expected list, but when querying a specific table we get an exception.</p>

<blockquote>
  <p>ERROR [HY000] [Microsoft][Hardy] (35) Error from server: error code:
  '0' error message:
  'com.databricks.backend.daemon.data.common.InvalidMountException:
  Error while using path xxxx for resolving path xxxx within mount at
  '/mnt/xxxx'.'.. Activity ID:050ac7b5-3e3f-4c8f-bcd1-106b158231f3</p>
</blockquote>

<p>In our case the Databrick tables and mounted parquet files stored in Azure Data Lake 2, this is related to the above exception. Any suggestions how to solve this issue?</p>

<p>Ps. the same error appaers when connectin from Power BI desktop.</p>

<p>Thanks
Bart</p>",1,5,2019-03-01 09:05:40.097000 UTC,,2019-03-01 09:13:31.583000 UTC,1,powerbi|parquet|azure-data-factory|azure-data-lake|azure-databricks,844,2019-03-01 09:01:56.513000 UTC,2019-05-29 15:03:52.813000 UTC,,11,0,0,1,,,,,,[]
ARIMA model : How does the ARIMA model forecast?,"<p>The aim of my request is to know how the forecast function of arima model works.</p>
<p>here I am trying to make predictions on my monthly time serie. I know that it is stationary.
I am performing a simple ARIMA(1,0,0) with the pmdarima package like following:</p>
<pre><code>model_auto = pm.auto_arima(df_train.values, start_p=1, start_q=0,max_p=1, max_q=0,d=0,trace=True)
</code></pre>
<p>Of course I forced it to return an ARIMA(1,0,0) in order to see how it calculates the predictions.
By hand, the calculation would be : X_t = <strong>a</strong>*X_{t-1} + <strong>intercept</strong>, with <strong>a</strong> and <strong>intercept</strong> given by the model.
The length of my test df is 10 so I want to forecast 10 values. The first value I want to forecast is the value at 01/01/2021. So the X_{t-1} is the value at 01/12/2020, which is 27779546.0.</p>
<p>The arima model brings me the parameters : <strong>a = 0.700, intercept = 8.204e+06.</strong></p>
<p>The forecasted value with the forecast function for 01/01/2021 is <strong>27658114.17</strong>.</p>
<p>The calculation by hand is X_t = a<em>X_{t-1} + intercept = X_{01/01/2021} = a</em>X_{01/12/2020} + intercept = 27779546.0 * 0.700 + 8.204e+06 = <strong>27649682.2</strong>.</p>
<p>So my question is : why I get two different values with the two methods? (forecasted : <strong>27658114.17</strong>, calculate by hand : <strong>27649682.2</strong>)</p>
<p>I would appreciate your help  Thank you!</p>",0,2,2022-02-10 08:23:54.143000 UTC,,,0,python|time-series|azure-databricks|arima,26,2020-02-19 08:25:57.707000 UTC,2022-02-18 09:53:56.157000 UTC,"Lille, France",1,0,0,4,,,,,,[]
how to check if an array has colums in a schema?,"<p>I have a schema and I would like to check the array if it has columns inside before exploding it. my schema looks like this</p>

<pre><code> |-- CaseNumber: string (nullable = true)
 |-- Interactions: struct (nullable = true)
 |    |-- EmailInteractions: array (nullable = true)
 |    |    |-- element: struct (containsNull = true)
 |    |    |    |-- CreatedBy: string (nullable = true)
 |    |    |    |-- CreatedOn: string (nullable = true)
 |    |    |    |-- Direction: string (nullable = true)
 |    |-- PhoneInteractions: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |    |-- WebInteractions: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |-- EntityAction: string (nullable = true)
</code></pre>

<p>I would like to check if ""EmailInteractions"" has elements under it before I run the job that will explode it, </p>

<p>I have edited the question for clarity</p>

<pre><code>1. check if email interactions array exist and check if it has columns, if both true, explode the array and finish, if one of the conditions is false, pass to step 2

2.check if phone interactions array exist and check if it has columns, if both true, explode the array and finish, if one of the conditions is false, pass to step 3

3.check if web interactions exist and check if it has columns, if both true, explode the array and finish, if one of the conditions is false, finish
</code></pre>

<p>I am new to coding and data bricks, please help on this.</p>",1,2,2020-02-07 00:55:58.763000 UTC,,2020-03-12 23:19:52.780000 UTC,0,apache-spark|schema|azure-databricks,50,2019-02-18 20:39:58.007000 UTC,2021-11-10 05:52:10.410000 UTC,,125,5,0,58,,,,,,[]
Diffing two versions in Hg,"<p>I am trying to view the differences between two versions of a project. The way I initially thought to do it:</p>

<ol>
<li>Create new repository</li>
<li>Commit first version to repository</li>
<li>Overwrite first version with second version, commit to repository</li>
<li><code>hg diff</code> the two versions</li>
</ol>

<p>This did not work, because when the second version overwrote the first version Mercurial assumed every single file was changed. In reality only a few files are changed, and I want to see what specifically was changed in each file.</p>

<p>WinDiff gives me a list of the files that were changed but that is it, so I would really like to use Hg to get the specifics.</p>

<p><strong>EDIT:</strong> I am using Eclipse. What I specifically am doing is creating a new Java project, using Eclipse's import feature to import the source files I want, and then commit that project to my repository. I then use import again to import version 2 into the same project so that the files from the first are overwritten. I am left with files that are named the same but that are version 2 files. I then commit the new files to the repository.</p>

<p>My Mercurial version is 1.8.3. Doing the steps above gives me just one changeset.</p>

<p>Also, when creating the diff is there a way to specify to only diff Java files, not text or properties, etc.?</p>",3,13,2011-06-22 15:48:24.920000 UTC,,2012-03-12 11:48:50.973000 UTC,1,eclipse|version-control|mercurial|diff|dvcs,235,2011-04-06 06:04:41.987000 UTC,2014-10-07 21:45:43.407000 UTC,,759,62,1,152,,,,,,[]
How to reclassify a user_id based on a similar transaction address,"<p>In short, I'm trying to look at different <code>transaction_address</code> values and any time there are 2 or more identical values, I want to check to see if any of the following conditions are met:</p>
<ol>
<li><p>if OUTPUT_IP is identical and/or OUTPUT_AMOUNT is identical, then create a new column (e.g. <code>reclassified_address</code>) with the OUTPUT_ID listed;</p>
</li>
<li><p>if INPUT_IP is identical and/or INPUT_AMOUNT is identical, then create a new column (e.g. <code>reclassified_address</code>) with the INPUT_ID listed</p>
</li>
</ol>
<p>The goal here is to use a duplicate <code>transaction_address</code> as a way of investigating whether or not a user is controlling multiple IP addresses.</p>
<p>Here's a sample of a table I'm using:</p>
<pre><code>TRANSACTION_ADDRESS | INPUT_IP | OUTPUT_IP | INPUT_AMOUNT | OUTPUT_AMOUNT
--------------------+----------+-----------+--------------+-------------------
 15c7853                xyz         abc          -0.01          0.70
 15c7853                def         abc          -0.50          0.70
 19vc842                abc         xyz3         -5.03          0.413
 19vc842                abcd        xyz3         -0.06          0.201
 188fdx8                abc         xyz4         -0.10          0.09
 154gf34                xyz1        abc          -0.07          0.18
 45f4ti5                ggg         abc          -0.10          0.24
 33cv5c5                jjj         abc          -0.08          1.13 
</code></pre>
<p>Here's an example and quick overview of when condition 1 is met (similar logic would be used for condition 2):</p>
<pre><code>TRANSACTION_ADDRESS | INPUT_IP | OUTPUT_IP | INPUT_AMOUNT | OUTPUT_AMOUNT
--------------------+----------+-----------+--------------+---------------
 15c7853                xyz         abc          -0.01          0.70
 15c7853                def         abc          -0.50          0.70
</code></pre>
<p>From this example, we see that <code>TRANSACTION_ADDRESS</code> <strong>15c7853</strong> occurs &gt;=2 times, so now we check to see if the <code>OUTPUT_IP</code> and/or the <code>OUTPUT_AMOUNT</code> values are identical. In this case, they both are so we reclassify <code>INPUT_IP</code> <strong>xyz</strong> and <strong>def</strong> with <strong>abc</strong>. This would let me know that whomever is using <strong>xyz</strong> is also most likely using <strong>def</strong> and <strong>abc</strong> too. The desired output would look something like this:</p>
<pre><code>TRANSACTION_ADDRESS | INPUT_IP | OUTPUT_IP | INPUT_AMOUNT | OUTPUT_AMOUNT | reclassified_address
--------------------+----------+-----------+--------------+---------------+---------------------
 15c7853                xyz         abc          -0.01          0.70             abc
 15c7853                def         abc          -0.50          0.70             abc
</code></pre>
<p>I tried using a HAVING clause to return the duplicates:</p>
<pre><code>SELECT TRANSACTION_ADDRESS, COUNT(*)
FROM sample_table
GROUP BY TRANSACTION_ADDRESS
HAVING COUNT(*) &gt;= 2
</code></pre>
<p>However, I'm not sure if this is the right approach and I'm not sure how to return the new column when condition 1 or 2 is met.</p>",0,5,2021-03-21 19:36:10.883000 UTC,,,0,sql|azure-databricks,37,2015-10-14 20:03:23.607000 UTC,2021-05-06 13:56:35.667000 UTC,,449,77,0,63,,,,,,[]
How to integrate the visualisation tool Graphexp with Neptune from EC2 instance in the same VPC as Neptune,"<p>I have setup an AWS Neptune database cluster with one primary and two replica nodes in three private subnets, each in three availability zones within the same region. I have also created corresponding public subnets where EC2 instances will have a graph db visualizer app like <a href=""https://github.com/bricaud/graphexp"" rel=""nofollow noreferrer"">https://github.com/bricaud/graphexp</a> or <a href=""https://github.com/erandal/graphexp"" rel=""nofollow noreferrer"">https://github.com/erandal/graphexp</a>. I am using the later (erandal), due to its more appealing UI. It is also deployed in apache httpd web server.</p>

<p>The private subnets have access to the internet through a NAT gateway associated with the public subnets.</p>

<p>For now I just have one EC2 instance to try out the connectivity of these UI tools to Neptune first before increasing the availability of the instances by deployment replicas in different AZs.</p>

<p>I have tried to connect by both options that Graphexp exposes (websockets and http) but without success. I keep getting the error - ERR_ADDRESS_UNREACHABLE in the Chrome Dev tools console. I have tried connecting to both the Neptune clusters' cluster endpoint hostname and its internal IP address etc. What is interesting is that I am able to call Neptune's gremlin endpoint successfully from the EC2 instance's shell using cURL.
Why is this web app not working?</p>",2,0,2019-04-02 16:03:17.913000 UTC,,2019-07-20 20:36:17.407000 UTC,3,amazon-web-services|amazon-vpc|amazon-neptune|graphexp,1165,2014-12-21 10:36:43.400000 UTC,2022-03-04 18:05:35.533000 UTC,"Amsterdam, Netherlands",487,20,2,73,,,,,,[]
Does AWS Neptune need internet access?,"<p>I was starting a neptune database from this base stack
<code>https://s3.amazonaws.com/aws-neptune-customer-samples/v2/cloudformation-templates/neptune-base-stack.json</code></p>
<p>However now i am wondering why a NAT Gateway and also an Internet Gateway are started in this stack? are they required for updates within Neptune? This seems like a huge security risk.</p>
<p>On top of that these gateways are not cheap.</p>
<p>I would be happy for an explanation on this</p>",1,0,2021-06-06 16:55:33.197000 UTC,,,0,amazon-web-services|amazon-cloudformation|amazon-neptune,66,2017-01-23 10:57:10.473000 UTC,2022-03-03 21:01:04.863000 UTC,,133,137,0,74,,,,,,[]
"Is there a feature to batch upload folders with files in git, svn, hg?","<p>There are folders with files with names: &quot;1&quot;, &quot;2&quot;, &quot;3&quot;... and &quot;2020070801&quot;, &quot;2020070802&quot;, &quot;2020070803&quot; ... locally currently.</p>
<p>Is there a feature to batch upload such folders with files in git, svn, hg not step by step and in batch so to get commits with automatic creation of commit messages and history of commits?</p>",1,3,2020-07-08 10:00:04.687000 UTC,,,-1,git|svn|mercurial|cvs|dvcs,34,2012-11-25 14:54:02.390000 UTC,2022-03-04 12:26:31.747000 UTC,FI/UA,2268,5036,3,435,,,,,,[]
"xgb.train(): TypeError: float() argument must be a string or a number, not 'DMatrix'","<p>When I look at the documentation, the argument is supposed to be a 'DMatrix' (xgboost version 1.5.0).</p>
<p><a href=""https://xgboost.readthedocs.io/en/latest/python/python_api.html#:%7E:text=Customized%20objective%20function.-,Learning%20API,num_boost_round%20(int)%20%E2%80%93%20Number%20of%20boosting%20iterations,-"" rel=""nofollow noreferrer"">https://xgboost.readthedocs.io/en/latest/python/python_api.html#:~:text=Customized%20objective%20function.-,Learning%20API,num_boost_round%20(int)%20%E2%80%93%20Number%20of%20boosting%20iterations,-</a>.</p>
<p>Indicates pretty much the same thing for the version I'm using (goto subheading '1.2.2 Python' in document link below):</p>
<p><a href=""https://xgboost.readthedocs.io/_/downloads/en/release_1.3.0/pdf/"" rel=""nofollow noreferrer"">https://xgboost.readthedocs.io/_/downloads/en/release_1.3.0/pdf/</a></p>
<p>I don't understand why it is asking for a float argument when it is supposed to be a DMatrix.</p>
<p>I've looked at all the Stack posts that have the string 'TypeError: float() argument must be a string or a number, not...', but none of them include 'DMatrix' and I have not been able to find a solution that I could adapt this particular issue.</p>
<p>The the following is the bit of code that elicits this error (go to 'clf - xgb.train(...)'):</p>
<pre><code>def grid_search(timeout_seconds, cv_splits, num_boost_round):
#   Read input data
X, y = preprocessing()
y.replace({1:0,2:1,3:2,4:3,5:4,6:5,7:6,8:7,9:8,10:9,11:10,12:11,13:12,14:13,
           15:14,16:15,17:16,18:17,19:18,20:19,21:20,22:21}, inplace = True)

#   Create dataframe to collect the results
tests_columns = [&quot;test_nr&quot;, &quot;cv_mean&quot;, &quot;cv_min&quot;, &quot;cv_max&quot;, &quot;cv_median&quot;, &quot;params&quot;]
test_id = 0
tests = pd.DataFrame(columns=tests_columns)

#   Cross validation number of splits
kf = KFold(n_splits=cv_splits)

#   Execute until timeout occurs
with timeout(timeout_seconds, exception=RuntimeError):

    #   Get the grid
    grid_iter, keys, length = get_grid_iterable()
    try:

        #   For every element of the grid
        for df_grid in grid_iter:
            #   Prepare a list to collect the scores
            score = []
            params = dict(zip(keys, df_grid))

            #   The objective function
            params[&quot;objective&quot;] = &quot;multi:softprob&quot;
            params['num_class'] = 22
            
            print('X.reason_action_converted: ', X.reason_action_converted)  
            #   For each fold, train XGBoost and spit out the results
            for train_index, test_index in kf.split(X.values):

                #   Get X train and X test
                X_train, X_test = X.iloc[train_index], X.iloc[test_index]

                **#   Get y train and y test**
                y_train, y_test = y.iloc[train_index], y.iloc[test_index]
                
                #   Convert into DMatrix
                d_train = xgb.DMatrix(X_train, label=y_train, missing=np.NaN)
                d_valid = xgb.DMatrix(X_test, label=y_test, missing=np.NaN)
                d_test = xgb.DMatrix(X_test, missing=np.NaN)
                watchlist = [(d_train, 'train'), (d_valid, 'valid')]

                #   Create the classifier using the current grid params. Apply early stopping of 50 rounds
                '''clf = xgb.train(params, d_train, boosting_rounds, watchlist, early_stopping_rounds=50, feval=log_loss, maximize=True, verbose_eval=10)'''
                **clf = xgb.train(params, d_train, num_boost_round, watchlist, early_stopping_rounds=50, feval=log_loss, maximize=True, verbose_eval=10)**
                y_hat = clf.predict(d_test)

                #   Append Scores on the fold kept out
                score.append(r2_score(y_test, y_hat))

            #   Store the result into a dataframe
            score_df = pd.DataFrame(columns=tests_columns, data=[
                [test_id, np.mean(score), np.min(score), np.max(score), np.median(score),
                 json.dumps(dict(zip(keys, [str(g) for g in df_grid])))]])
            test_id += 1
            tests = pd.concat([tests, score_df])
    except RuntimeError:
        #   When timeout occurs an exception is raised and the main cycle is broken
        pass

#   Spit out the results
tests.to_csv(&quot;grid-search.csv&quot;, index=False)
print(tests)


**if __name__ == &quot;__main__&quot;:
grid_search(timeout_seconds=3600, cv_splits=4, num_boost_round=500)**
</code></pre>
<p>The error message:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;command-3902447645915365&gt; in &lt;module&gt;
    106 
    107 if __name__ == &quot;__main__&quot;:
--&gt; 108     grid_search(timeout_seconds=3600, 
cv_splits=4, num_boost_round=500)

&lt;command-3902447645915365&gt; in grid_search(timeout_seconds, cv_splits, num_boost_round)
     84                     #   Create the classifier using the current grid params. Apply early stopping of 50 rounds
     85                     '''clf = xgb.train(params, 
d_train, boosting_rounds, watchlist, 
early_stopping_rounds=50, feval=log_loss, 
maximize=True, verbose_eval=10)'''
---&gt; 86                     clf = xgb.train(params, 
d_train, num_boost_round, watchlist, 
early_stopping_rounds=50, feval=log_loss, 
maximize=True, verbose_eval=10)
     87                     y_hat = clf.predict(d_test)
     88 

/databricks/python/lib/python3.8/site- 
packages/xgboost/training.py in train(params, dtrain, 
num_boost_round, evals, obj, feval, maximize, 
early_stopping_rounds, evals_result, verbose_eval, 
xgb_model, callbacks)
    204     Booster : a trained booster model
    205     &quot;&quot;&quot;
--&gt; 206     bst = _train_internal(params, dtrain,
    207                           
num_boost_round=num_boost_round,
    208                           evals=evals,

/databricks/python/lib/python3.8/site-packages/xgboost/training.py in _train_internal(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)
    107         nboost += 1
    108         # check evaluation result.
--&gt; 109         if callbacks.after_iteration(bst, i, 
dtrain, evals):
    110             break
    111         # do checkpoint after evaluation, in 
case evaluation also updates

/databricks/python/lib/python3.8/site- 
packages/xgboost/callback.py in after_iteration(self, 
model, epoch, dtrain, evals)
    421             for _, name in evals:
    422                 assert name.find('-') == -1, 
'Dataset name should not contain `-`'
--&gt; 423             score = model.eval_set(evals, 
epoch, self.metric)
    424             score = score.split()[1:]  # into 
datasets
    425             # split up `test-error:0.1234`

/databricks/python/lib/python3.8/site- 
packages/xgboost/core.py in eval_set(self, evals, 
iteration, feval)
   1350         if feval is not None:
   1351             for dmat, evname in evals:
-&gt; 1352                 feval_ret = 
feval(self.predict(dmat, training=False,
   1353                                                
output_margin=True), dmat)
   1354                 if isinstance(feval_ret, list):

/databricks/python/lib/python3.8/site- 
packages/sklearn/utils/validation.py in inner_f(*args, 
**kwargs)
     70                           FutureWarning)
     71         kwargs.update({k: arg for k, arg in 
zip(sig.parameters, args)})
---&gt; 72         return f(**kwargs)
     73     return inner_f
     74 

/databricks/python/lib/python3.8/site- 
packages/sklearn/metrics/_classification.py in 
log_loss(y_true, y_pred, eps, normalize, sample_weight, 
labels)
   2184     The logarithm used is the natural logarithm 
(base-e).
   2185     &quot;&quot;&quot;
-&gt; 2186     y_pred = check_array(y_pred, 
ensure_2d=False)
   2187     check_consistent_length(y_pred, y_true, 
sample_weight)
   2188 

/databricks/python/lib/python3.8/site- 
packages/sklearn/utils/validation.py in inner_f(*args, 
**kwargs)
     70                           FutureWarning)
     71         kwargs.update({k: arg for k, arg in 
zip(sig.parameters, args)})
---&gt; 72         return f(**kwargs)
     73     return inner_f
     74 

/databricks/python/lib/python3.8/site- 
packages/sklearn/utils/validation.py in 
check_array(array, accept_sparse, accept_large_sparse, 
dtype, order, copy, force_all_finite, ensure_2d,  
allow_nd, ensure_min_samples, ensure_min_features, 
estimator)
    636         # make sure we actually converted to 
numeric:
    637         if dtype_numeric and array.dtype.kind 
== &quot;O&quot;:
--&gt; 638             array = array.astype(np.float64)
    639         if not allow_nd and array.ndim &gt;= 3:
    640             raise ValueError(&quot;Found array with 
dim %d. %s expected &lt;= 2.&quot;

TypeError: float() argument must be a string or a number, not 'DMatrix'
</code></pre>
<p>I'm using Databricks, Python 3.8.8, and xgboost 1.3.1.</p>
<p>I am trying to adapt code from the following tutorial: Effortless Hyperparameters Tuning with Apache Spark.</p>",0,0,2021-09-30 12:12:01.547000 UTC,,2021-09-30 12:18:37.933000 UTC,0,python-3.x|xgboost|aws-databricks,31,2016-02-25 16:12:31.837000 UTC,2022-03-04 10:57:34.420000 UTC,,321,62,0,151,,,,,,[]
aws lambda timeout doing neptune query,"<p>I'm very new to AWS Neptune and gremlin and trying to get my lambda function to run a simple query but they are hanging and the lambda function times out before I get a response.
I set up the connection according to AWS docs but having a hard time getting any queries to execute and return data.  I have an EC2 instance that I can connect to the database through the gremlin console and can run queries fine there, I only have issues running them in my lambda function.</p>
<p>connection.ts</p>
<pre class=""lang-js prettyprint-override""><code>const gremlin = require('gremlin');
const {getUrlAndHeaders} = require('gremlin-aws-sigv4/lib/utils');
const traversal = gremlin.process.AnonymousTraversalSource.traversal;
const DriverRemoteConnection = gremlin.driver.DriverRemoteConnection;

let conn = null;
let g = null;

export function connection(){
    const getConnectionDetails = () =&gt; {
        if (process.env['USE_IAM'] == 'true'){
           return getUrlAndHeaders(
               process.env['NEPTUNE_ENDPOINT'],
               process.env['NEPTUNE_PORT'],
               {},
               '/gremlin',
               'wss'); 
        } else {
            const database_url = 'wss://' +  &quot;my database endpoing&quot; + ':' + &quot;8182&quot; + '/gremlin';
            return { url: database_url, headers: {}};
        }      
      };

      const createRemoteConnection = () =&gt; {
        const { url, headers } = getConnectionDetails();
        
        const c = new DriverRemoteConnection(
            url, 
            { 
                mimeType: 'application/vnd.gremlin-v2.0+json', 
                headers: headers 
            });  
    
         c._client._connection.on('close', (code, message) =&gt; {
                 console.info(`close - ${code} ${message}`);
                 if (code == 1006){
                     console.error('Connection closed prematurely');
                     throw new Error('Connection closed prematurely');
                 }
             });  
        
         return c;       
      };
    
      const createGraphTraversalSource = (conn) =&gt; {
        return traversal().withRemote(conn);
      };
    
      if (conn == null){
        console.info(&quot;Initializing connection&quot;)
        conn = createRemoteConnection();
        g = createGraphTraversalSource(conn);
      }

      return g;
      
}
</code></pre>
<p>index.ts</p>
<pre class=""lang-js prettyprint-override""><code>import {connection} from &quot;./connection&quot;
export async function handler(event, context): Promise&lt;any&gt; {

  const g = connection()
  let result;
  console.log(&quot;before query&quot;) // this gets called
  const user =  await g.V('1').values(&quot;name&quot;).next(); // hangs here
  console.log(&quot;after query&quot;)  //This never gets executed
  return user
}
</code></pre>
<p>Does anyone know what I might be doing wrong?</p>",1,2,2021-08-28 20:28:04.437000 UTC,,2021-08-28 21:12:31.173000 UTC,1,node.js|amazon-web-services|aws-lambda|gremlin|amazon-neptune,200,2014-05-14 23:02:12.137000 UTC,2022-03-01 17:13:00.717000 UTC,,974,110,1,49,,,,,,[]
Need a better/optimised way to list files according to the pattern,"<p>I am trying to list files from Azure data lake storage using a pattern using os.walk. It is too slow and not accepted by the business. Is there any faster way to do this?</p>
<p><strong>code snippet below:</strong></p>
<pre><code># pattern holds something like '201707' (YYYYMM) as files are dated.
pattern=&quot;*{0}*.*&quot;.format(batch_no)
print(pattern)
files_list=[]
  #os Walk to get file paths
for root in root_list:
  for path, subdirs, files in os.walk(root):
    for name in files:
      if fnmatch(name.upper(), pattern.upper()):
        files_list.append(str(batch_no)+path.replace(&quot;dbfs/&quot;,&quot;&quot;)+&quot;/&quot;+name)
</code></pre>",2,5,2021-11-14 02:44:44.040000 UTC,,,0,python|python-3.x|azure-databricks,47,2018-01-03 13:40:54.173000 UTC,2022-03-04 12:18:27.540000 UTC,,19,1,0,8,,,,,,[]
Azure Databricks - Reading Parquet files into DataFrames,"<p>Am newbie with Python ...  trying to read parquet files from Databricks, but when the file is empty is throwing error.  How can i check filesize before reading it into DataFrame.  Code below:</p>

<pre><code>%python

##check if file is empty ???
##if not empty read
##else do something else

try:
   parquetDF =              
   spark.read.parquet(""wasbs://XXXXX@XXXX.blob.core.windows.net/XXXX/2019-10- 11/account.parquet"")
except:
   print('File is Empty !!!')
</code></pre>",1,0,2019-10-15 23:25:00.770000 UTC,,,2,python|dataframe|parquet|azure-databricks,1150,2009-04-02 00:53:40.477000 UTC,2022-03-01 22:30:30.100000 UTC,"Brisbane QLD, Australia",27501,66,1,1101,,,,,,[]
"Create Vertex only if ""from"" and ""to"" vertex exists","<p>I want to create 1000+ Edges in a single query.
Currently, I am using the AWS Neptune database and gremlin.net for creating it.
The issue I am facing is related to the speed. It took huge time because of HTTP requests.
So I am planning to combine all of my queries in a string and executing in a single shot.</p>
<pre><code>_g.AddE(&quot;allow&quot;).From(_g.V().HasLabel('person').Has('name', 'name1')).To(_g.V().HasLabel('phone').Where(__.Out().Has('sensor', 'nfc'))).Next();
</code></pre>
<p>There are chances that the <em><strong>&quot;To&quot;</strong></em> (target) Vertex may not be available in the database. When it is the case this query fails as well. So I had to apply a check if that vertex exists before executing this query using hasNext().</p>
<p>So as of now its working fine, but when I am thinking of combining all 1000+ edge creation at once, is it possible to write a query which doesn't break if <em><strong>&quot;To&quot;</strong></em> (target) Vertex not found?</p>",1,0,2020-09-16 21:08:57.820000 UTC,,,0,gremlin|amazon-neptune|gremlinnet,58,2010-12-05 06:13:15.167000 UTC,2022-03-04 15:48:07.797000 UTC,"Ahmadabad, India",1120,134,7,360,,,,,,[]
Distributed version control for configuration settings,"<p>I am running several Hadoop clusters and one best practice that became very handy was to use distributed version control systems to have a revision control for configuration files.</p>

<p>Example: for Hadoop, I create a GIT repository on in the etc directories. This was very helpful, as I was able to revert to earlier configuration settings. The only drawback is that if more people configure the server and one admin does not commit regularly after a change, the idea is gone.</p>

<p>Is there any better recommendation on how to do some revision control on configuration files?</p>

<p>Stefan</p>",1,2,2014-12-16 13:18:14.140000 UTC,,,0,git|hadoop|dvcs,199,2010-09-21 07:59:26.200000 UTC,2022-03-05 06:55:41.873000 UTC,"Vienna, Austria",2013,595,4,475,,,,,,[]
Design Data Processing Solution in Azure,"<p>We have large amounts of <code>CSV</code> files which arrive to a dedicated drive (e.g. D:) on a daily basis. Then, a set of <code>SSIS</code> packages will pick up those files, performs transformations on them, and then ingest the result into several tables in a database. Logging and error handling do also exist.</p>
<p>As we are experimenting a possible move to the cloud (<code>Azure</code> in particular), we went for a lift and shift scenario at the beginning. In this approach, we simply deployed the same <code>SSIS</code> packages into <code>Azure SQL Server</code>, created <code>Azure Data Factory</code> <code>ADF</code> pipelines, and run those packages from there.</p>
<p>We would like to re-factor our solution to replace <code>SSIS</code> packages, with cloud-native services of <code>Azure</code>.</p>
<p>My questions would be:</p>
<ol>
<li><p>Based on the scenario explained in the first paragraph, is this considered as a Batch Processing scenario ? Does <code>Azure Batch Service</code> fit in as potential service to use ? Or it would be more efficient to use <code>Azure DataBricks</code> with <code>ADF</code> ?</p>
</li>
<li><p>Below is the solution environment and main tasks on-premises. I would like to have a comparison between what we do in <code>SSIS</code>, and the counterpart in <code>Azure</code> world:</p>
</li>
</ol>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">Item</th>
<th style=""text-align: left;"">On-Premise World</th>
<th style=""text-align: left;"">Azure World</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">Storage to receive CSV files</td>
<td style=""text-align: left;"">Normal Disk Drive D:\</td>
<td style=""text-align: left;"">?</td>
</tr>
<tr>
<td style=""text-align: left;"">CSV File Processing</td>
<td style=""text-align: left;"">SSIS -&gt; Data Flow -&gt; Script Component</td>
<td style=""text-align: left;"">?</td>
</tr>
<tr>
<td style=""text-align: left;"">Ingest to Destination Table</td>
<td style=""text-align: left;"">SSIS -&gt; Data Flow -&gt;  OLE DB Destination</td>
<td style=""text-align: left;"">?</td>
</tr>
<tr>
<td style=""text-align: left;"">Custom Scripting</td>
<td style=""text-align: left;"">Script Task &amp; Script Component</td>
<td style=""text-align: left;"">?</td>
</tr>
<tr>
<td style=""text-align: left;"">Database</td>
<td style=""text-align: left;"">SQL Server</td>
<td style=""text-align: left;"">?</td>
</tr>
</tbody>
</table>
</div>
<ol start=""3"">
<li>Recommendations/best practices/approaches used in similar migration projects ?</li>
</ol>",2,2,2021-06-21 12:22:08.770000 UTC,,,1,azure|ssis|azure-data-factory|azure-data-factory-2|azure-databricks,68,2021-01-26 09:23:45.760000 UTC,2022-03-01 14:38:11.180000 UTC,"Hamburg, Germany",123,15,0,21,,,,,,[]
PySpark: difficulty implementing KMeans with mapreduce functions,"<p>I am currently tasekd in a Distributed DataBase class to create an implementation of kmeans with map reduce based approach (yes i know that there is a premade function for it but the task is specifically to do your own approach), and while i have figured out the approach itself, i am struggling with implementing it with the appropriate use of the map and reduce functions.</p>

<pre><code>def Find_dist(x,y):
  sum = 0
  vec1= list(x)
  vec2 = list(y)
  for i in range(len(vec1)):
    sum = sum +(vec1[i]-vec2[i])*(vec1[i]-vec2[i])
  return sum

def mapper(cent, datapoint):
  min = Find_dist(datapoint,cent[0])
  closest = cent[0]
  for i in range(1,len(cent)):
    curr = Find_dist(datapoint,cent[i])
    if curr &lt; min:
      min = curr
      closest = cent[i]
  yield closest,datapoint

def combine(x):
  Key = x[0]
  Values = x[1]
  sum = [0]*len(Key)
  counter = 0
  for datapoint in Values:
    vec = list(datapoint[0])
    counter = counter+1
    sum = sum+vec
  point = Row(vec)
  result = (counter,point)
  yield Key, result

def Reducer(x):
  Key = x[0]
  Values = x[1]
  sum = [0]*len(Key)
  counter = 0
  for datapoint in Values:
    vec = list(datapoint[0])
    counter = counter+1
    sum = sum+vec
  avg = [0]*len(Key)
  for i in range(len(Key)):
    avg[i] = sum[i]/counter
  centroid = Row(avg)
  yield Key, centroid

def kmeans_fit(data,k,max_iter):
  centers = data.rdd.takeSample(False,k,seed=42)
  for i in range(max_iter):
    mapped = data.rdd.map(lambda x: mapper(centers,x))
    combined = mapped.reduceByKeyLocally(lambda x: combiner(x))
    reduced = combined.reduceByKey(lambda x: Reducer(x)).collect()
    flag = True
    for i in range(k):
      if(reduced[i][1] != reduced[i][0] ):
        for j in range(k):
          centers[i] = reduced[i][1]
        flag = False
        break
    if (flag):
      break
  return centers
data = spark.read.parquet(""/mnt/ddscoursedatabricksstg/ddscoursedatabricksdata/random_data.parquet"")
kmeans_fit(data,5,10)
</code></pre>

<p>My main issue for the most part is I encounter difficulty in the usage of dataframes and the map, reducebykeylocally and reducebykey fucntions. 
Currently the run fails at calling reduceByKeyLocally(lambda x: combiner(x)) because ""ValueError: not enough values to unpack (expected 2, got 1)"", and i really need to get this all working properly soon, so please, anyone i would love assistance on this, and thank you in advance, I will be very grateful for any help!</p>",0,0,2020-05-30 12:26:19.187000 UTC,,2020-05-30 13:09:19.060000 UTC,1,python|pyspark|mapreduce|k-means|azure-databricks,128,2018-11-06 08:45:26.337000 UTC,2022-02-27 16:44:55.723000 UTC,,39,0,0,12,,,,,,[]
"Unable to find the strategy to implement constraints, keys, indexes while loading tables","<p>I want to create one Generic pipeline in ADF which covers below scenario.</p>
<p>Pipeline should accept two input parameters :
Source Table name
Sink Table name</p>
<p>Pipeline should execute below way.</p>
<ol>
<li>Connect to source</li>
<li>Identify the source table fields and data types and create the table with the given name in the sink</li>
<li>Data should copied to sink.</li>
<li>Pipeline should handle indexes, keys and constraints.</li>
</ol>
<p>For above requirement i created a lookup which fetches table list names from a csv file. Lookup is followed by Foreach activity which have a copy activity inside it. Now i am able to create tables from list of tables and also copy the data. But unable to handle indexes, keys and constraints(refer point 4). Can someone help how to achieve this.</p>",1,3,2020-12-21 08:20:05.423000 UTC,1.0,,0,sql|azure|stored-procedures|azure-databricks|azure-data-factory-pipeline,145,2020-12-21 07:53:26.647000 UTC,2021-06-17 17:49:34.480000 UTC,"Pune, Maharashtra, India",1,0,0,4,,,,,,[]
What tools and processes are available for effective Trunk-Based Development?,"<p>At my company, we are using git-flow, and having many branches and PRs is often confusing, and slows down development.  It is especially unnecessary during the startup phase of a project.</p>

<p>One problem is that most features take more than 1 or 2 days to complete. If there is a code review process, this can take another day or two. If there are multiple developers working on separate branches for several days in an immature (rapidly changing) codebase, there is a lot of opportunities for the branches to diverge, leading to merge conflicts, duplication of effort, etc.  Sometimes a branch is created off another feature branch that has not yet been merged for some reason.  This is also prone to errors, for example a PR can have the wrong ""base branch"".  Sometimes after a PR is merged I try to merge trunk/master into my feature branch, and this has led to painful merges, where every file modified shows up as a conflict. </p>

<p>However, some kind of code-review practice is necessary, so I want to know what other teams and companies have used. </p>

<p>I could imagine a system where a feature is identified by a tag in the commit message.  For example ""[XYZ-1234] added new component"". The tag identifies the ticket, and the message gives more details. So everyone could be committing to a ""dev"" branch, and synchronizing often, and when a feature is determined to be complete, all the commits associated with that feature could be code reviewed then merged into ""master"" (by searching the commit messages).  This seems simpler and better to me, so I'm not sure why a system like this is not created and used.</p>",2,5,2018-02-16 20:33:59.257000 UTC,,2018-02-23 07:07:00.363000 UTC,2,git|github|version-control|dvcs,547,2010-06-21 20:11:22.080000 UTC,2022-03-05 03:44:18.923000 UTC,,1638,432,1,154,,,,,,[]
Getting HTTP error 403 - invalid access token while trying to access cluster through Azure databricks,"<p>I'm trying to access Azure databricks spark cluster by a python script which takes token as an input generated via databricks user settings and calling a Get method to get the details of the cluster alongwith the cluster-id. </p>

<p>The below is the code snippet. As shown, I have created a cluster in southcentralus zone.</p>

<pre><code>import requests
headers = {""Authorization"":""Bearer dapiad************************""}
data=requests.get(""https://southcentralus.azuredatabricks.net/api/2.0/clusters/get?cluster_id=**************"",headers=headers).text
print data
</code></pre>

<p>Expected result should give the full detail of the cluster eg.
 <code>{""cluster_id"":""0128-******"",""spark_context_id"":3850138716505089853,""cluster_name"":""abcdxyz"",""spark_version"":""5.1.x-scala2.11"",""spark_conf"":{""spark.databricks.delta.preview.enabled"":""true""},""node_type_id"" and so on .....}</code></p>

<p><strong>The above code is working when I execute the code on google colaboratory whereas the same is not working with my local IDE i.e. idle. It gives the error of HTTP 403 stating as below:</strong> </p>

<pre><code>&lt;p&gt;Problem accessing /api/2.0/clusters/get. Reason:
&lt;pre&gt;    Invalid access token.&lt;/pre&gt;&lt;/p&gt;
</code></pre>

<p>Can anyone help me resolve the issue? I'm stuck on this part and not able to access the cluster through APIs.</p>",1,1,2019-02-05 08:17:21.723000 UTC,,,2,python|apache-spark|access-token|http-error|azure-databricks,5802,2016-01-27 15:07:06.643000 UTC,2020-11-09 18:39:49.130000 UTC,,309,8,0,25,,,,,,[]
Azure Blob Storage - Automatically storing files,"<p>I am taking my first major steps into azure and just doing some research for educational purposes. There is a currently a process that exists in the business whereby files are being used and then queries are being run within the PowerBI report to provide the result.</p>
<p>This is causing the report to be slow naturally as its doing the calculating, I want to come up with an automated solution for this.</p>
<p>In my mind it would go as such:</p>
<ul>
<li>User (Finance Department) uploads said file to Blob Storage</li>
<li>Use ADF to pull this data into a Data Lake</li>
<li>Use Databricks + Python Notebook to manipulate this data</li>
<li>Push Data into a SQLDB or DW Solution.</li>
</ul>
<p>Is this correct? How would I get the user (insert generic finance person here) to be able to upload the file into blob storage, at present they email it to the BI person in question, this obviously has major flaws.</p>
<p>Cheers</p>",2,0,2022-01-17 09:36:56.493000 UTC,,,0,azure|azure-data-factory|azure-databricks,59,2013-04-12 15:11:37.637000 UTC,2022-03-04 15:26:56.120000 UTC,,931,63,0,243,,,,,,[]
git log revision range gives incorrect range of commits,"<p>I am trying to use list all commits within a given range on a branch using the  argument of <code>git log</code>. For some reason it doesn't seem to be giving me the right result (or maybe I'm understanding the command wrong?).</p>

<p>Here's the steps for what I'm doing:</p>

<ol>
<li><p>Clone the repo</p>

<p><code>git clone https://github.com/openstack/nova.git</code></p></li>
<li><p>Do <code>git log</code> and these are the last 9 commits:</p>

<pre><code>d5bde44 Merge ""Make metadata password routines use Instance object""
6cbc9ee Merge ""Fix object change detection""
39b7875 Merge ""Fix object leak in nova.tests.objects.test_fields.TestObject""
94d1034 Merge ""maint: correct docstring parameter description""
6407f17 Merge ""Fix live_migration method's docstring""
7406661 Merge ""Fix infinitely reschedule instance due to miss retry info""
9d8a34f Merge ""Remove unused code from test_compute_cells""
429cd4b Fix object change detection
01381b8 Fix object leak in nova.tests.objects.test_fields.TestObject
...
</code></pre></li>
<li><p>Lets say I want to get all the commits starting after <code>01381b8</code>. I issue <code>git log 01381b8..HEAD</code> and the following output is seen:</p>

<pre><code>d5bde44 Merge ""Make metadata password routines use Instance object""
6cbc9ee Merge ""Fix object change detection""
39b7875 Merge ""Fix object leak in nova.tests.objects.test_fields.TestObject""
94d1034 Merge ""maint: correct docstring parameter description""
6407f17 Merge ""Fix live_migration method's docstring""
7406661 Merge ""Fix infinitely reschedule instance due to miss retry info""
9d8a34f Merge ""Remove unused code from test_compute_cells""
429cd4b Fix object change detection
2214bc0 Remove unused code from test_compute_cells
9639b55 Fix infinitely reschedule instance due to miss retry info
a5184d3 Fix live_migration method's docstring
76729a3 maint: correct docstring parameter description
28224a6 Make metadata password routines use Instance object
</code></pre></li>
</ol>

<p>Wow! I actually got <strong>13</strong> commits in that output when I expected <strong>8</strong>. What is going on here? Is the revision range the correct mechanism to get show commits after a given commit? Or is this a bug?</p>",2,1,2014-06-15 01:37:48.457000 UTC,,,3,git|version-control|git-branch|dvcs|git-log,1840,2009-06-04 19:01:29.137000 UTC,2022-02-13 06:05:24.840000 UTC,,5687,453,38,194,,,,,,[]
Calling a column with $ value spark sql,"<p>I have a problem selecting a column name with $ in it.</p>
<p>For  example:</p>
<p>select a$a from x</p>
<p>This is getting read as a parameter in azure databricks</p>
<p>I have tried 'a$a', &quot;a$a&quot; and tilda a$a...Is there a way to select such column value in spark sql</p>",1,0,2020-09-17 12:35:55.903000 UTC,,,0,azure|pyspark|apache-spark-sql|azure-databricks,25,2019-09-26 10:07:19.227000 UTC,2022-02-23 04:40:22.907000 UTC,,125,7,0,17,,,,,,[]
Azure Databricks Python Job from several python files,"<p>Still new to Azure Databricks..</p>
<p>We created a Databricks Job(Type: Python) on our Azure Databricks workspace.
So far, we're able to run the python script file stored in DBFS.</p>
<p>Due to the complexity of the python file, we're thinking to split the code into several python files and then have a 'main.py' to organize what we want to do.</p>
<p>Locally, as long as the other python files stays under the same folder as 'main.py', we can easily import those files and do the job. But on Azure Databricks, it seems to be more complicated. Even if we upload the python files under the same path on DBFS, it's giving the ModuleNotFoundError:</p>
<blockquote>
<p>ModuleNotFoundError: No module named 'part1'
ModuleNotFoundError                       Traceback (most recent call last)
----&gt; 1 import part1</p>
</blockquote>
<p>Appreciate some guide on this. Thanks!</p>",0,0,2021-12-15 03:33:07.120000 UTC,,2021-12-15 04:19:17.963000 UTC,0,python|azure|azure-databricks,61,2019-04-12 00:04:57.613000 UTC,2022-03-03 08:43:03.543000 UTC,,1,0,0,1,,,,,,[]
mercurial_keyring extension for hg - How can I change the keyring password?,"<p>I recently installed mercurial on my ubuntu server (running Ubuntu Server 11.10) and with it the oh so useful mercurial_keyring extension. In setting things up, I mistakenly entered a keyring password that I do not like...</p>

<p>This bothers me a bit, because now every operation with my remote repository asks me for this password...</p>

<p>Is there a way to change the keyring password easily?</p>

<pre><code>mysuperusername@mysuperserver:/var/www/...$ hg push
pushing to https://...@bitbucket.org/.../...
Please input your password for the keyring
Password:
</code></pre>",1,0,2012-02-24 13:49:26.350000 UTC,,2012-02-27 13:30:11.477000 UTC,0,python|mercurial|dvcs|mercurial-extension,732,2012-02-24 13:44:39.143000 UTC,2020-10-16 15:26:47.560000 UTC,"Quebec, Canada",1940,10,1,329,,,,,,[]
How to solve Error of offset mismatch in Azure Databricks Autoloader cloudfiles source?,"<p>This happens when some files are deleted from the data source that Autoloader stream is reading from.</p>
<pre class=""lang-py prettyprint-override""><code>try:
    raw_df = spark.readStream.format(&quot;cloudFiles&quot;) \
            .option(&quot;cloudFiles.format&quot;,&quot;csv&quot;) \
            .option(&quot;cloudFiles.includeExistingFiles&quot;, &quot;true&quot;) \
            .option(&quot;cloudFiles.allowOverwrites&quot;, &quot;true&quot;) \
            .option(&quot;cloudFiles.schemaLocation&quot;, 
                opPath.outputPath +&quot;/checkpoints/&quot; + storageAccountInfo.adlsContainerName) \
            .option(&quot;delimiter&quot;,&quot;\t&quot;)\
            .load(source)

  except Exception as f:
    print(f)
</code></pre>
<p><a href=""https://i.stack.imgur.com/9wfoY.png"" rel=""nofollow noreferrer"">Error Image</a></p>",0,1,2021-10-01 06:40:38.897000 UTC,,2021-10-01 15:27:01.867000 UTC,2,azure-databricks|databricks-autoloader,52,2020-12-28 14:37:08.313000 UTC,2022-02-28 12:27:46.463000 UTC,"Bangalore, Karnataka, India",21,0,0,0,,,,,,[]
Invalid characters in JSON,"<p>I have JSON with an array of struct type in string format. I am getting the following error when I was trying to write the same in a json file. I used this output file for my schema validation and causing fail because of special characters.</p>

<pre><code>from pyspark.sql import Row
from pyspark.sql import DataFrame
from pyspark.sql.types import * 
import pandas as pd

jsondata1 = '{""activity"":[{""activity"":""test"",""activityValue"":""1""},{""activity"":""test"",""activityValue"":""1""}]}'
my_automatic_schema = json_schema.dumps(jsondata1)
my_automatic_schema

atoDF = sqlContext.read.json(sc.parallelize([my_automatic_schema]))
atoDF.write.format(""json"").save(""mnt//Users/name/test3.json"")
</code></pre>

<p>I am  expecting the following output:
<code>'{""activity"": [{""activityValue"": ""str"", ""activity"": ""str""}, ""...""]}'</code></p>

<p>and getting the below output in my JSON
<code>{""activity"":[""{\""activityValue\"":\""str\"",\""activity\"":\""str\""}"",""...""]}</code></p>

<p>Please help me provide a solution about how to get rid of special characters in my output file</p>",1,1,2019-08-02 19:39:20.793000 UTC,,,0,python|pyspark|apache-spark-sql|pypi|azure-databricks,166,2018-05-07 10:19:18.923000 UTC,2021-10-14 08:51:54.637000 UTC,"Richmond, VA, USA",25,0,0,16,,,,,,[]
java.net.SocketException: Connection reset error when calling Azure Maps service from Azure databricks,"<p>We have a resource group which contains Azure Databricks service and Azure Maps Account. 
The goal is to call Azure Maps <a href=""https://docs.microsoft.com/en-us/azure/azure-maps/how-to-search-for-address#request-latitude-and-longitude-for-an-address-geocoding"" rel=""nofollow noreferrer"">endpoints related to Geo coding</a>. For some reason, I keep on receiving </p>

<pre><code>java.net.SocketException: Connection reset
</code></pre>

<p>no matter the very same request executes successfully via Postman. 
<a href=""https://i.stack.imgur.com/EMITv.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EMITv.jpg"" alt=""enter image description here""></a></p>

<p>In order to prepare my HTTP request for Scala (the language that I use in my Databricks notebook) I use <a href=""https://github.com/scalaj/scalaj-http"" rel=""nofollow noreferrer"">this</a> library/package. So, the notebook content looks something like this:</p>

<pre><code>val response: HttpResponse[String] = Http(""https://atlas.microsoft.com/search/address/json?subscription-key=my_key&amp;api-version=1.0&amp;query=my_query_str"")
.option(HttpOptions.connTimeout(10000)).asString
//.timeout(connTimeoutMs = 100000, readTimeoutMs = 500000).asString

response.code
</code></pre>

<p>and then, the detailed error looks like this:</p>

<pre><code>java.net.SocketException: Connection reset
    at java.net.SocketInputStream.read(SocketInputStream.java:210)
    at java.net.SocketInputStream.read(SocketInputStream.java:141)
    at sun.security.ssl.InputRecord.readFully(InputRecord.java:465)
    at sun.security.ssl.InputRecord.read(InputRecord.java:503)
    at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975)
    at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1367)
    at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1395)
    at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1379)
    at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559)
    at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.setNewClient(AbstractDelegateHttpsURLConnection.java:100)
    at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.setNewClient(AbstractDelegateHttpsURLConnection.java:80)
    at sun.net.www.protocol.http.HttpURLConnection.writeRequests(HttpURLConnection.java:706)
    at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1591)
    at sun.net.www.protocol.http.HttpURLConnection.access$200(HttpURLConnection.java:92)
    at sun.net.www.protocol.http.HttpURLConnection$9.run(HttpURLConnection.java:1490)
    at sun.net.www.protocol.http.HttpURLConnection$9.run(HttpURLConnection.java:1488)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.security.AccessController.doPrivilegedWithCombiner(AccessController.java:784)
    at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1487)
    at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480)
    at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:352)
    at scalaj.http.HttpRequest.scalaj$http$HttpRequest$$doConnection(Http.scala:367)
    at scalaj.http.HttpRequest.exec(Http.scala:343)
    at scalaj.http.HttpRequest.asString(Http.scala:492)
    at lined047365c6afd4053bf68147e840c60bb25.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3596544709550112:4)
    at lined047365c6afd4053bf68147e840c60bb25.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3596544709550112:55)
    at lined047365c6afd4053bf68147e840c60bb25.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3596544709550112:57)
    at lined047365c6afd4053bf68147e840c60bb25.$read$$iw$$iw$$iw.&lt;init&gt;(command-3596544709550112:59)
    at lined047365c6afd4053bf68147e840c60bb25.$read$$iw$$iw.&lt;init&gt;(command-3596544709550112:61)
    at lined047365c6afd4053bf68147e840c60bb25.$read$$iw.&lt;init&gt;(command-3596544709550112:63)
    at lined047365c6afd4053bf68147e840c60bb25.$read.&lt;init&gt;(command-3596544709550112:65)
    at lined047365c6afd4053bf68147e840c60bb25.$read$.&lt;init&gt;(command-3596544709550112:69)
    at lined047365c6afd4053bf68147e840c60bb25.$read$.&lt;clinit&gt;(command-3596544709550112)
    at lined047365c6afd4053bf68147e840c60bb25.$eval$.$print$lzycompute(&lt;notebook&gt;:7)
    at lined047365c6afd4053bf68147e840c60bb25.$eval$.$print(&lt;notebook&gt;:6)
    at lined047365c6afd4053bf68147e840c60bb25.$eval.$print(&lt;notebook&gt;)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)
    at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)
    at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)
    at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)
    at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
    at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)
    at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)
    at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)
    at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)
    at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:215)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:202)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:202)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:202)
    at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:699)
    at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:652)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:202)
    at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:385)
    at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:362)
    at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:251)
    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
    at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:246)
    at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)
    at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:288)
    at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)
    at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:362)
    at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)
    at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)
    at scala.util.Try$.apply(Try.scala:192)
    at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)
    at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)
    at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)
    at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)
    at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)
    at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)
    at java.lang.Thread.run(Thread.java:748)
</code></pre>

<p><strong>UPDATE</strong>: No matter if we have a valid shared key in the URL or not, the error is still the same:
    <a href=""https://atlas.microsoft.com/search/address/json?subscription-key="" rel=""nofollow noreferrer"">https://atlas.microsoft.com/search/address/json?subscription-key=</a><strong>my_key</strong>&amp;api-version=1.0&amp;query=my_query_str</p>",0,5,2020-06-16 06:55:10.597000 UTC,,2020-06-17 07:19:40.170000 UTC,1,scala|azure|https|azure-databricks|azure-maps,1019,2013-03-03 11:31:34.343000 UTC,2022-03-05 19:13:03.263000 UTC,,1867,164,4,276,,,,,,[]
How to structure the ETL project in Azure Databricks?,"<p>In the past, the following project structure has been useful in etl development using pyspark (on-premises infrastructure)</p>
<p><a href=""https://i.stack.imgur.com/8MQlN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8MQlN.png"" alt=""enter image description here"" /></a></p>
<p>Since Azure Databricks uses the concept of Notebooks, I'm not certain how this structure needs to be modified to be able to deploy etl pipelines using Azure Databricks.</p>
<p>Can someone please share what'd be an equivalent representation of this structure in an Azure Databricks workspace?a</p>",0,0,2020-10-06 16:32:33.860000 UTC,1.0,2020-10-07 13:39:49.573000 UTC,1,azure|pyspark|etl|azure-databricks,167,2015-03-22 05:21:55.310000 UTC,2021-12-20 16:41:13.473000 UTC,"Toronto, ON, Canada",884,18,3,66,,,,,,[]
How to approach to data access ACL for ADLS Gen 2 + Databricks + Power BI?,"<p>I would like to manage the ACL for users to access the data on the data lake and visualization tool. The data lake is built using Azure ADLS Gen 2 and accessed through Delta Lake + Data bricks. Datasets are created to load into Power BI to serve users.</p>
<p>I need to give access to data at table/column/row level from Data Lake through ADLS Gen 2 or Delta lake and Power BI. For a user assigned to an AD Group, s/he should be able to access to reports or data based on restrictions at table/column/row level.</p>
<p>I was thinking the following approach:</p>
<ul>
<li>Define AD groups for data engineer, business_user_unrestricted_readers, business_user_restricted_readers accordingly to access to Data Lake folders) - no column level security is possible</li>
<li>Use Pass through authentication from delta lake towards to ADLS Gen 2 Data lake to use the AD ACL permissions. I would like to use Dynamic Views on Data Bricks (on delta tables) protecting certain columns but the ACL is not integrated with AD and needs to be implemented separately.</li>
<li>Define Object Level Security for Power BI and assign the tables/columns accordingly to AD user groups</li>
<li>Encrypt the sensitive data (for those who have full access to the data lake, apply additional security to specific columns)</li>
</ul>
<p>On the above, there is no fine grained configuration.</p>
<p>How to achieve a best practice ACL approach for the above scenario? Are there alternative and flexible approaches to manage the access across Azure native data sources driven by policies and sensitive data tags defined on the meta-data catalogue, etc?</p>",0,1,2021-10-05 18:11:54.617000 UTC,,2021-10-05 19:14:22.383000 UTC,0,azure|azure-databricks|azure-security,96,2015-05-23 10:03:21.140000 UTC,2022-03-02 23:25:08.817000 UTC,,181,1,0,48,,,,,,[]
Will git push commits from all branches automatically?,"<p>From some answers I found(<a href=""https://stackoverflow.com/a/19088435/1119381"">example</a>), I understand that if I do <code>git push</code>, only the current branch commits will be pushed. However when I type 'git push`, I get the following output (partial):</p>

<pre><code>. . . 
Writing objects: 100% (12/12), 999 bytes | 0 bytes/s, done.
Total 12 (delta 4), reused 0 (delta 0)
To https://gitlab.bla-bla.git
   773f335..db28181  doedev_userAgentTest -&gt; doedev_userAgentTest
 ! [rejected]        master -&gt; master (fetch first)
error: failed to push some refs to 'https://gitlab.bla-bla.git'
. . .
</code></pre>

<p>My current branch is <code>doedev_userAgentTest</code>:</p>

<pre><code>$ git status
On branch doedev_userAgentTest
</code></pre>

<p>So what I understand from the above output is that the commits to the current branch were made correctly (I confirmed that the file I have committed is actually in the remote repo) but there was an attempt to push master branch and it failed.</p>

<p>So my questions are (disregard the error while pushing to master):</p>

<ol>
<li>Am I right in my assumption that there was a push to master made despite the fact I was pushing while on different branch?</li>
<li>If the answer is yes, how do I push only from current branch?</li>
</ol>

<p><strong>NOTE</strong>: I do have all the above-mentioned branches remotely.</p>",2,0,2016-06-22 08:49:56.360000 UTC,,2017-05-23 12:31:49.080000 UTC,1,git|version-control|dvcs,72,2011-12-28 13:44:32.297000 UTC,2022-03-04 14:26:41.307000 UTC,"Perth, Western Australia, Australia",6332,504,74,1189,,,,,,[]
How to do pagination in NeptuneDB to achieve high performance,"<p>Hi I am now building a website using aws NeptuneDB(Gremlin), NodeJs as Backend and Angular as Frontend. Now I am facing a problem, I want to do pagination on my website because without pagination I may load and display 5000 items one query. I know in MySQL, we can using like</p>

<pre><code>select * from mydb limit 0, 20;
</code></pre>

<p>to do pagination.</p>

<p>Can I achieve similar in NeptuneDB(or GraphDB). I investigated for a while and I found this:
<a href=""https://stackoverflow.com/questions/39826983/how-to-perform-pagination-in-gremlin"">How to perform pagination in Gremlin</a></p>

<p>Refer to the answer of this question, It seems we cannot avoid loading all query results into memory. Does it mean it doesn't make any difference with or without the pagination.</p>

<p>Or can I achieve pagination between Node and Angular(I am just guessing)?</p>

<p>So Any ideas to improve performance?</p>",1,0,2019-12-17 07:45:59.817000 UTC,,2019-12-19 04:21:38.490000 UTC,4,node.js|angular|pagination|gremlin|amazon-neptune,1014,2019-06-14 23:53:24.103000 UTC,2022-03-05 04:00:19.063000 UTC,,385,115,0,92,,,,,,[]
How best to model data in AWS Neptune,"<p>I'm currently evaluating AWS Neptune as a potential graph database (specifically comparing it to Azure Cosmos Graph DB). The scenario is that I have a bunch of test data which I'm adding with the bulk loader and will then run some benchmarking tests on the DB performance.</p>
<p>I'm curious about how best to model the data in AWS Neptune.</p>
<p>In Azure Cosmos Graph DB, edges are unidirectional and are stored on the source vertex. So queries which need to look for inbound edges will be slow unless an edge is also stored on the other vertex.</p>
<p>So far in AWS Neptune, I've not found an answer on how the edges can be best optimised in a similar way.</p>
<p>Reading this description of the Neptune internal data model (<a href=""https://docs.aws.amazon.com/neptune/latest/userguide/feature-overview-data-model.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/neptune/latest/userguide/feature-overview-data-model.html</a>) suggests that there's a common storage pattern used for both vertices, edges and properties which are indexed using the 3 most common access patterns.</p>
<p>So I would assume based on this that:</p>
<ol>
<li>we need to store both incoming and outgoing edges, or</li>
<li>we need to enable &quot;OSGP Index Creation Using Lab Mode&quot; to index from both directions</li>
</ol>
<p>What is the best approach here?</p>",1,1,2020-12-08 02:56:10.400000 UTC,,,0,amazon-web-services|graph|gremlin|amazon-neptune,277,2012-06-28 04:20:35.363000 UTC,2022-03-04 03:14:40.533000 UTC,New South Wales,1422,1444,1,101,,,,,,[]
Truncate and reload,"<p>I'm trying to truncate the Neptune db and reload same data by bulk-load
but the database is not considering the same data to load in to Neptune.
we used g.V().drop() and similar for edges. can any one help me with this problem. my project involves schedule based truncate and reload the data with modifications and will have same Id's</p>

<p>we delete the instance and recreate Neptune Again then the load work's fine  </p>

<pre><code>curl -X POST \
    -H 'Content-Type: application/json' \
    https://*:8182/loader -d '
    {
      ""source"" : ""s3://***"",
      ""format"" : ""csv"",
      ""iamRoleArn"" : """",
      ""region"" : ""*"",
      ""failOnError"" : ""FALSE"",
      ""parallelism"" : ""MEDIUM""
    }'
</code></pre>

<p>number of records provided and the number Neptune bulk-loader identified are not matching and reload fails for same data which got success for the first Time</p>",1,1,2019-06-10 06:58:01.003000 UTC,,2019-06-10 06:59:26.947000 UTC,1,python|amazon-web-services|gremlin|bulk-load|amazon-neptune,307,2019-06-10 06:02:33.297000 UTC,2020-01-30 05:55:25.943000 UTC,,11,0,0,0,,,,,,[]
Using an OGM with AWS Neptune,"<p>I can't seem to find any resources that state whether or not some popular OGMs can be used with neptune instead of writing raw gremlin queries for example.</p>

<p>Would any of you know what OGMs are supported or can be used with Neptune and which would you recommend for python in particular?</p>

<p>Also, would it be better to use an OGM (like goblin for example) or stick to the gremlin python GVL instead?</p>",1,0,2019-03-13 08:34:03.253000 UTC,,2019-05-16 21:05:21.220000 UTC,1,python|amazon-web-services|gremlin|amazon-neptune|goblin,208,2019-03-13 08:28:23.883000 UTC,2022-02-18 18:38:32.070000 UTC,"Taguig, Metro Manila, Philippines",311,11,0,41,,,,,,[]
Can pure python script (not pyspark) run in parallel in a cluster in Azure Databricks?,"<p>I want to migrate my python scripts from local to run on cloud, specifically on a cluster created on <strong>Azure Databricks</strong>.</p>

<ol>
<li>Can pure python script run in parallel (using multiple nodes in a cluster at the same time) without having to be converted into pyspark?</li>
<li>Is it possible to check whether the job is running in parallel?</li>
</ol>",1,0,2018-11-28 10:35:52.183000 UTC,,,1,pyspark|azure-databricks,533,2016-11-26 09:27:03.973000 UTC,2021-05-11 04:41:20.057000 UTC,,21,0,0,9,,,,,,[]
"Doing without partial commits the ""Mercurial way""","<p>Subversion shop considering switching to Mercurial, trying to figure out in advance what all the complaints from developers are going to be. There's one fairly common use case here that I can't see how to handle.</p>

<ol>
<li>I'm working on some largish feature, and I have a significant part of the code -- or possibly several significant parts of the code -- in pieces all over the garage floor, totally unsuitable for checkin, maybe not even compiling.</li>
<li>An urgent bugfix request comes in. The fix is nice and local and doesn't touch any of the code I've been working on.</li>
<li>I make the fix in my working copy.</li>
</ol>

<p>Now what?</p>

<p>I've looked at ""<a href=""https://stackoverflow.com/questions/854930/mercurial-cherry-picking-changes-for-commit"">Mercurial cherry picking changes for commit</a>"" and ""<a href=""https://stackoverflow.com/questions/448567/best-practices-in-mercurial-branch-vs-clone-and-partial-merges"">best practices in mercurial: branch vs. clone, and partial merges?</a>"" and all the suggestions seem to be extensions of varying complexity, from Record and Shelve to Queues.</p>

<p>The fact that there apparently isn't any core functionality for this makes me suspect that in some sense this working style is Doing It Wrong. What would a Mercurial-like solution to this use case look like?</p>

<hr>

<p><strong>Edited to add:</strong> git, by contrast, seems designed for this workflow: <code>git add</code> the bugfix files, don't <code>git add</code> anything else (or <code>git reset HEAD</code> anything you might have already added), <code>git commit</code>.</p>",6,2,2010-06-10 08:53:21.333000 UTC,8.0,2017-05-23 11:55:13.553000 UTC,12,mercurial|dvcs,3252,2008-10-13 09:24:59.200000 UTC,2022-03-05 21:17:15.113000 UTC,"San Francisco, CA",42604,1658,447,2193,,,,,,[]
Azure Databricks stream fails with StorageException: Could not verify copy source,"<p>We have a Databricks job that has suddenly started to consistently fail. Sometimes it runs for an hour, other times it fails after a few minutes.</p>
<p>The inner exception is</p>
<pre><code>ERROR MicroBatchExecution: Query [id = xyz, runId = abc] terminated with error
shaded.databricks.org.apache.hadoop.fs.azure.AzureException: hadoop_azure_shaded.com.microsoft.azure.storage.StorageException: Could not verify copy source.
</code></pre>
<p>The job targets a notebook which consumes from event-hub with PySpark structured streaming, calculates some values based on the data, and streams data back to another event-hub topic.</p>
<p>The cluster is a pool with 2 workers and 1 driver running on standard Databricks 9.1 ML.</p>
<p>We've tried to restart job many times, also with clean input data and checkpoint location.
We struggle to determine what is causing this error.
We cannot see any 403 Forbidden errors in logs, which is sometimes mentioned on forums as a reason
.
Any assistance is greatly appreciated.</p>",1,1,2021-12-16 13:45:42.280000 UTC,1.0,2021-12-17 14:03:35.017000 UTC,1,azure-blob-storage|azure-databricks|spark-structured-streaming,170,2021-12-16 13:17:53.210000 UTC,2022-03-03 15:18:26.930000 UTC,,21,0,0,4,,,,,,[]
Gremlin query to get all interconnected vertex value in javascript,"<p>Can any one help me to figure out how to fetch content of all vertex which are connected each other in javascript , I need all the values of all vertex which are connected</p>
<p>For example in below diagram I have a business vertex and there are two outgoing edges from business  , what I need is all the contents of business vertex and video and image vertex</p>
<p><a href=""https://i.stack.imgur.com/PPjA5.png"" rel=""nofollow noreferrer"">graph look like this</a></p>",1,1,2021-09-26 13:44:58.010000 UTC,,,-1,javascript|gremlin|amazon-neptune,39,2020-12-11 15:51:55.000000 UTC,2022-03-02 13:31:50.383000 UTC,"Mangalore, Karnataka, India",1,0,0,1,,,,,,[]
Writing dataframe to blob storage - file either empty or job aborted,"<p>I'm trying to write a simple dataframe containing some string types as a csv and store it in my blob storage container. I've mounted the blob container in workspace using a SAS that has both read and write permissions and is not expired. I also want it to be a single file so used coalesce(1) but is coming out as PART-1, PART-0.</p>

<p>Here's my code</p>

<pre><code>file_path_blob = ""/mnt/LogData/predictions1""
anomalies_df.coalesce(1).write.csv(file_path_blob)
</code></pre>

<p>When I run the job it writes something but eventually fails ...with an error message like </p>

<pre><code>Caused by: com.microsoft.azure.storage.StorageException: This request is not authorized to perform this operation using this permission.
    at com.microsoft.azure.storage.StorageException.translateException(StorageException.java:89)
    at com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:305)
    at com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:178)
    at com.microsoft.azure.storage.blob.CloudBlob.delete(CloudBlob.java:928)
    at shaded.databricks.org.apache.hadoop.fs.azure.StorageInterfaceImpl$CloudBlobWrapperImpl.delete(StorageInterfaceImpl.java:313)
    at shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.safeDelete(AzureNativeFileSystemStore.java:2533)
    at shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2727)
    ... 28 more
</code></pre>

<p><a href=""https://i.stack.imgur.com/ezyKG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ezyKG.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/MufIn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MufIn.png"" alt=""enter image description here""></a></p>",0,3,2019-03-28 21:59:52.143000 UTC,,,0,azure-databricks,318,2015-09-17 00:57:51.347000 UTC,2020-06-05 19:30:07.297000 UTC,,328,49,1,78,,,,,,[]
Can we change the maximum length of content on an RDF graph node in the Neptune database?,"<p>In Gremlin there is : <code>-l &lt;text-length&gt;</code> to increase the length of content displayed in each node. Does anyone know how to increase length in RDF nodes?
<a href=""https://i.stack.imgur.com/C4OQx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/C4OQx.png"" alt=""The values in my graph are getting truncated"" /></a></p>",1,0,2021-11-11 09:26:48.040000 UTC,,2021-11-20 22:30:46.357000 UTC,0,graph|sparql|amazon-neptune|graph-notebook,52,2021-09-09 15:06:28.590000 UTC,2022-03-04 09:53:27.580000 UTC,,27,1,0,6,,,,,,[]
Is there way to attach revisions to ticket in JIRA without writing ticket ID in commit-message,"<p>Im using Jira with Bitbucket, and I wonder - can I somehow store info about that one or more revisions are related to some ticket, without writing ugly prefixes like <code>ABC-123</code> in commit-messages?</p>",2,0,2013-09-02 14:06:37.397000 UTC,,,0,version-control|jira|dvcs|bitbucket,112,2012-11-12 16:07:20.193000 UTC,2022-03-03 12:33:42.817000 UTC,,10532,754,9,2794,,,,,,[]
Databricks Spark SQL subquery based query throws TreeNodeException,"<p>I am running a pretty simple query in databricks notebook which involves a subquery.</p>
<pre><code>select recorddate, count(*) 
from( select record_date as recorddate, column1 
      from table1 
      where record_date &gt;= date_sub(current_date(), 1) 
    )t
group by recorddate
order by recorddate
</code></pre>
<p>I get the following exception:
Error in SQL statement: package.TreeNodeException: Binding attribute, tree: recorddate</p>
<p>And when  remove the order by clause, the query runs fine. I see some posts talking about similar issues but exactly the same. Is this a known behavior? Any workaround/fix for this?</p>",1,3,2020-07-29 10:37:14.013000 UTC,,2020-07-31 06:59:06.363000 UTC,0,apache-spark|apache-spark-sql|azure-databricks,181,2014-08-06 14:14:46.690000 UTC,2022-03-02 05:19:39.750000 UTC,Pune,641,71,1,95,,,,,,[]
Azure Data Factory user parameter,"<p>In Azure Databricks I want to get the user that trigger manually a Notebook in Data Factory pipeline. I think Data Factory doesn't have a dynamic parameter to pass the user to Databricks, only pipeline features and functions. Do you know any solution for this?</p>",1,0,2019-10-18 18:11:05.390000 UTC,,,1,azure|azure-pipelines|azure-data-factory|azure-data-factory-2|azure-databricks,965,2018-08-02 15:05:23.500000 UTC,2022-02-23 12:57:57.230000 UTC,"Lisboa, Portugal",308,58,2,25,,,,,,[]
How to use Azure Databricks to run ML programs with multiple files and modules?,"<p>I know how to run a Machine learning program in a single notebook. But how do I run if there are multiple files and dependencies. How do I run this <a href=""https://github.com/iliaschalkidis/lmtc-eurlex57k"" rel=""nofollow noreferrer"">code</a>. Any ideas in this case?</p>",1,1,2020-07-25 05:27:34.210000 UTC,,2020-07-27 18:17:16.227000 UTC,0,python|azure|azure-databricks,171,2016-02-05 20:00:35.083000 UTC,2022-03-03 09:02:06.707000 UTC,"Hyderabad, Telangana, India",61,3,0,22,,,,,,[]
Azure DataBrick PYTHONPATH pointing at imported wheel?,"<p>I successfully created a python wheel of my python project following simple steps from here: <a href=""https://python101.pythonlibrary.org/chapter39_wheels.html"" rel=""nofollow noreferrer"">https://python101.pythonlibrary.org/chapter39_wheels.html</a></p>
<p>Then from my DataBrick Notebook, installed my project dependencies (I uploaded separately my project's requirements.txt to my blob storage):</p>
<pre><code>%pip install -r /dbfs/mnt/testdb-blob-container1/requirements.txt
</code></pre>
<p>I then uploaded python wheel of my project via Azure DataBrick interface: <a href=""https://docs.microsoft.com/en-us/azure/databricks/libraries/workspace-libraries"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/databricks/libraries/workspace-libraries</a></p>
<p>From my DataBrick Notebook, i successfully reference:</p>
<pre><code>import myproject
import myproject.src
from myproject.src.core import constants as constants   &lt;-- This is fine.
</code></pre>
<p>But this blew up because my datetimeutil needs &quot;constants&quot;. From local, we have PYTHONPATH. In DataBrick, we dont have this, so below attempt to import datetimeutil below up:</p>
<pre><code>  from myproject.src.helpers import datetimeutil as datetimeutil
</code></pre>
<p>How do we set PYTHONPATH in DataBrick environment?</p>
<p>One thing i tried is... my wheel file is here:</p>
<pre><code>dbfs:/FileStore/jars/23011937_5e16_4be0_b82a_88e83aaecadf/myproject-1.0-py3-none-any.whl
</code></pre>
<p>From my notebook:</p>
<pre><code>import sys
sys.path.append(&quot;dbfs:/FileStore/jars/23011937_5e16_4be0_b82a_88e83aaecadf/&quot;)
</code></pre>
<p>This did <strong>not</strong> work.</p>
<p>Thanks</p>",1,0,2021-03-28 06:34:26.443000 UTC,,2021-03-28 08:53:47.427000 UTC,0,azure|azure-databricks,153,2018-10-10 11:29:39.193000 UTC,2022-03-05 23:25:15.713000 UTC,Hong Kong,543,10,0,64,,,,,,[]
Spark/Databricks data write issue with Cosmos DB with Mongo DB API,"<p>I am trying to write data from Spark (using Databricks) to Mongo DB inside Azure Cosmos DB .I have created one Cosmos DB Account with API as <strong>“Azure Cosmos DB for Mongo DB API”</strong>.</p>

<p>I have created a Database and also a Collection using the Data Explorer of Azure Cosmos DB Account.</p>

<p>While creating the collection I have provided the following details:</p>

<p><a href=""https://i.stack.imgur.com/4aHgn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4aHgn.png"" alt=""Cosmos DB Collection Input""></a></p>

<p>I entered the following record into the collection</p>

<p><a href=""https://i.stack.imgur.com/vaHFB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vaHFB.png"" alt=""Collection Data input""></a></p>

<p>I can check the data as well by executing query</p>

<p><a href=""https://i.stack.imgur.com/mAUAC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mAUAC.png"" alt=""Query output""></a></p>

<p>When I am connecting using Databricks I get the following output with <em>printSchema</em>.</p>

<p><a href=""https://i.stack.imgur.com/Q3Bfk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q3Bfk.png"" alt=""printschema output""></a></p>

<p>When <em>display(df)</em> is invoked following details is shown</p>

<p><a href=""https://i.stack.imgur.com/3fLie.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3fLie.png"" alt=""Display df output ""></a></p>

<p>If I try to to insert one row using Spark Dataframe ,then the inserted data shows null for $t and $v using the following code </p>

<pre><code>import com.microsoft.azure.cosmosdb.spark.schema._
import com.microsoft.azure.cosmosdb.spark._
import com.microsoft.azure.cosmosdb.spark.config.Config
import org.apache.spark.sql.types._
import org.apache.spark.sql.Row

val configMap = Map(  
  ""Endpoint"" -&gt; ""https://XXXXXX.documents.azure.com:443/"",  
  ""Masterkey"" -&gt; ""xxxxxxxxxxxxxxxxx=="",  
  ""Database"" -&gt; ""db1"",  
  ""Collection"" -&gt; ""collection1"")  
val config = Config(configMap)  
val data = Seq(Row(""B"", ""US""))
val schema = List(StructField(""name"", StringType, true),StructField(""country"", StringType, true))
val writeDF = spark.createDataFrame(
spark.sparkContext.parallelize(data),
StructType(schema)
)
CosmosDBSpark.save(writeDF, config)
val df = spark.sqlContext.read.cosmosDB(config)
</code></pre>

<p><a href=""https://i.stack.imgur.com/qXEqY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qXEqY.png"" alt=""Data output""></a></p>

<p>I see new columns are added like the following image for the new record.</p>

<p><a href=""https://i.stack.imgur.com/J8zK9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/J8zK9.png"" alt=""New Columns""></a></p>

<p><strong>The way the data gets inserted from Data Browser and from Spark are very different .</strong></p>

<p>Another thing I noticed that after the data is inserted from spark ,the Data browser can not display the collection,rather it gets some error while trying to show the documents. </p>

<p><a href=""https://i.stack.imgur.com/oLi2O.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oLi2O.png"" alt=""Data Browser Error ""></a></p>

<p><strong>I want to know what needs to be done to  insert data properly into Cosmos DB Mongo DB from Spark / Databricks .Please help.</strong></p>",0,0,2019-01-12 16:51:54.380000 UTC,,2019-01-13 08:01:49.287000 UTC,1,apache-spark|azure-cosmosdb-mongoapi|azure-databricks,378,2014-07-17 03:21:33.427000 UTC,2020-12-14 16:44:41.383000 UTC,"Kolkata, West Bengal, India",379,1,0,20,,,,,,[]
Azure databricks cluster local storage maximum size,"<p>I am having a databrick cluster on Azure,<br />
there is a local storage /mnt /tmp /user..<br />
May I know are there any folder size limitation for each of the folder ?<br />
And how long the data will be retention ?</p>",1,1,2020-12-08 02:58:39.977000 UTC,,,0,azure-databricks,175,2019-05-03 07:43:53.913000 UTC,2022-02-19 16:28:58.100000 UTC,,367,0,0,93,,,,,,[]
How can I execute and schedule Databricks notebook from Azure Devops Pipeline using YAML,"<p>I wanted to do CICD of my azure Databricks notebook using YAML file.
I have followed the below flow</p>
<ol>
<li>Pushed my code from Databricks notebook to Azure Repos.</li>
<li>Created a Build using below YAML script.</li>
</ol>
<pre><code>stages:
- stage: Build
  displayName: Build stage

  jobs:
  - job: Build
    displayName: Build
    steps:
    - task: CopyFiles@2
      displayName: 'Copy Files to:  $(build.artifactstagingdirectory)'
      inputs:
        SourceFolder: '$(System.DefaultWorkingDirectory)'
        TargetFolder: ' $(build.artifactstagingdirectory)'

    - task: PublishBuildArtifacts@1
      displayName: 'Publish Artifact: notebooks'
      inputs:
        ArtifactName: dev_release
    - task: PublishBuildArtifacts@1
      inputs:
        PathtoPublish: '$(Build.ArtifactStagingDirectory)'
        ArtifactName: 'publish build'
        publishLocation: 'Container'

</code></pre>
<p>By doing above I was able to create a Artifact.</p>
<p>Now I have added another task to deploy that artifact to my Databricks workspace. By using below YAML Script.</p>
<pre><code>- stage: Deploy
  displayName: Deploy stage

  jobs:
  - job: Deploy
    displayName: Deploy
    pool:
      vmImage: 'vs2017-win2016'
    steps:
    - task: DownloadBuildArtifacts@0
      inputs:
        buildType: 'current'
        downloadType: 'single'
        artifactName: 'dev_release'
        downloadPath: '$(System.ArtifactsDirectory)'
    - task: databricksDeployScripts@0
      inputs:
        authMethod: 'bearer'
        bearerToken: 'dapj0ee865674cd9tfb583dbad61b78ce9b1-4'
        region: 'Central US'
        localPath: '$(System.DefaultWorkingDirectory)'
        databricksPath: '/Shared'

</code></pre>
<p>Now i want to run the deployed notebook from here only. So I have &quot;Configure Databricks CLI&quot; task and &quot;Execute Databricks&quot; task to execute the note book.</p>
<p><strong>Got below Error</strong>:<br />
##[error]Error: Unable to locate executable file: 'databricks'. Please verify either the file path exists or the file can be found within a directory specified by the PATH environment variable. Also verify the file has a valid extension for an executable file.
##[error]The given notebook does not exist.</p>
<p>How can I execute notebook from Azure DevOps. My notebooks are in Scala Language.<br />
<strong>Is there any other way to use in Production servers.</strong></p>",1,0,2022-02-11 14:38:01.443000 UTC,,,1,azure-devops|azure-pipelines|azure-databricks|azure-devops-rest-api|cicd,74,2022-01-21 03:11:50.470000 UTC,2022-03-04 06:13:20.257000 UTC,"Hyderabad, Telangana, India",33,0,0,2,,,,,,[]
Streaming data from Cosmos DB using Apache Spark 3,"<p>Streaming from CosmosDB using Spark 2 connector can be achieved using Changefeed.</p>
<p><a href=""https://docs.microsoft.com/en-us/azure/cosmos-db/spark-connector#streaming-reads-from-cosmos-db"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/cosmos-db/spark-connector#streaming-reads-from-cosmos-db</a></p>
<p>How do we do the same in Spark 3? I am using Cosmos DB Apache Spark 3 OLTP Connector for SQL API (beta).</p>
<p><a href=""https://docs.microsoft.com/en-us/azure/cosmos-db/create-sql-api-spark#query-cosmos-db"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/cosmos-db/create-sql-api-spark#query-cosmos-db</a></p>",1,0,2021-04-28 04:52:35.553000 UTC,0.0,2021-07-05 20:55:00.447000 UTC,2,apache-spark|azure-cosmosdb|spark-streaming|spark-structured-streaming|azure-databricks,308,2020-07-03 10:22:16.220000 UTC,2022-03-05 05:00:45.310000 UTC,,121,2,0,16,,,,,,[]
How to setup jar configs in databricks for redis connections,"<p>I have installed the following jar in databricks <code>&quot;com.redislabs:spark-redis_2.12:2.5.0&quot;</code>. And trying create a spark session with the respective authentications</p>
<p>Below is the code where I create a spark session with creds</p>
<pre><code>redis= SparkSession.builder.appName(&quot;redis_connection&quot;).config(&quot;spark.redis.host&quot;, &quot;hostname&quot;).config(&quot;spark.redis.port&quot;, &quot;port&quot;).config(&quot;spark.redis.auth&quot;, &quot;pass&quot;).getOrCreate()
</code></pre>
<p>But when I try to save it using the follwing code</p>
<pre><code>df.write.format(&quot;org.apache.spark.sql.redis&quot;).option(&quot;table&quot;, &quot;velocity&quot;).option(&quot;key.column&quot;, &quot;name&quot;).option(&quot;ttl&quot;, 30).save()
</code></pre>
<p>This throws me the following error.</p>
<pre><code>Caused by: redis.clients.jedis.exceptions.JedisConnectionException: Failed connecting to host localhost:6379
</code></pre>
<p>It obviously mean to connect to <strong>localhost</strong> rather the <strong>hostname</strong> I provide. How to pass the jar configuration with hostnames and passphrase in databricks to validate the connection.</p>",2,2,2021-03-01 18:07:05.743000 UTC,,,1,pyspark|azure-databricks|spark-redis,175,2018-09-13 22:00:28.417000 UTC,2022-03-04 16:16:20.187000 UTC,,445,28,2,102,,,,,,[]
Migrate from OrientDB to AWS Neptune,"<p>I need to migrate a database from OrientDB to Neptune. I have an exported JSON file from Orient that contains the schema (classes) and the records - I now need to import this into Neptune. However, it seems that to import data into Neptune there must be a csv file containing all the vertex's and another file containing all the edges.</p>
<p>Are there any existing tools to help with this migration and converting to the required files/format?</p>",1,3,2021-08-17 08:41:58.990000 UTC,,,0,orientdb|amazon-neptune,55,2021-03-17 16:23:53.313000 UTC,2022-02-21 15:01:29.907000 UTC,,27,7,0,11,,,,,,[]
Issue with write from Databricks to Azure cosmos DB,"<p>I am trying to write data from Spark (using Databricks) to Azure Cosmos DB(Mongo DB). There are no errors when executing notebook but i am getting below error when querying the collection.</p>

<p>I have used jar from databricks website azure-cosmosdb-spark_2.4.0_2.11-2.1.2-uber.jar. My versions is 6.5 (includes Apache Spark 2.4.5, Scala 2.11)</p>

<pre><code>import org.joda.time.format._
import com.microsoft.azure.cosmosdb.spark.schema._
import com.microsoft.azure.cosmosdb.spark.CosmosDBSpark
import com.microsoft.azure.cosmosdb.spark.config.Config
import org.apache.spark.sql.functions._
val configMap = Map(
  ""Endpoint"" -&gt; ""https://******.documents.azure.com:***/"",
  ""Masterkey"" -&gt; ""****=="",
  ""Database"" -&gt; ""dbname"",
  ""Collection"" -&gt; ""collectionname""
  )
val config = Config(configMap)
val df = spark.sql(""select id,name,address from emp_db.employee"")
CosmosDBSpark.save(df, config)
</code></pre>

<p>when i query the collection i get below response</p>

<pre><code>Error: error: {
    ""_t"" : ""OKMongoResponse"",
    ""ok"" : 0,
    ""code"" : 1,
    ""errmsg"" : ""Unknown server error occurred when processing this request."",
    ""$err"" : ""Unknown server error occurred when processing this request.""
}
</code></pre>

<p>Any help would be much appreciated. Thank you!!!</p>",1,0,2020-06-11 19:43:51.817000 UTC,,,0,mongodb|scala|apache-spark|azure-cosmosdb|azure-databricks,532,2019-02-11 11:08:51.977000 UTC,2021-04-27 05:32:27.987000 UTC,,13,0,0,4,,,,,,[]
CORS error while accessing azure databricks rest api from front end,"<p>CORS error while accessing azure databricks rest api from front end</p>
<p>/api/2.0/jobs/run/list while accessing this from postman I am getting the response as expected</p>
<p>But while accessing the same api from front end it’s throwing cORS
‘ local host:4200 has been blocked by Cors policy: Response to preflight: it doesn’t have http ok status</p>
<p>How can I solve this issue</p>",1,4,2020-10-28 16:51:04.063000 UTC,,,1,angular|azure|api|azure-databricks,252,2020-10-28 16:43:06.733000 UTC,2020-12-07 07:09:35.197000 UTC,"Texas, USA",11,0,0,3,,,,,,[]
Finding lineage of notebooks in azure databricks,<p>I am working on a project where we would be creating many notebooks in Azure databricks. In many cases there is a possibility to nesting notebook calls. We are looking for an approach to create automated lineage across notebooks. Any help or guidance here is appreciated.</p>,0,0,2019-05-18 19:03:20.480000 UTC,,,1,azure-databricks|data-lineage,28,2013-11-11 05:59:55.433000 UTC,2021-06-25 20:51:24.590000 UTC,,43,3,0,3,,,,,,[]
Error while creating data frame from Rest Api in Pyspark,"<p>I have a below pyspark code. I am reading a json data from Rest API and trying to load using pyspark.
But i couldnt read the data in DataFrame in spark.Can some one help me in this.</p>
<pre><code>import urllib
from pyspark.sql.types import StructType,StructField,StringType

schema = StructType([StructField('dropoff_latitude',StringType(),True),\
                     StructField('dropoff_longitude',StringType(),True),
                     StructField('extra',StringType(),True),\
                     StructField('fare_amount',StringType(),True),\
                     StructField('improvement_surcharge',StringType(),True),\
                     StructField('lpep_dropoff_datetime',StringType(),True),\
                     StructField('mta_tax',StringType(),True),\
                     StructField('passenger_count',StringType(),True),\
                     StructField('payment_type',StringType(),True),\
                     StructField('pickup_latitude',StringType(),True),\
                     StructField('ratecodeid',StringType(),True),\
                     StructField('tip_amount',StringType(),True),\
                     StructField('tolls_amount',StringType(),True),\
                     StructField('total_amount',StringType(),True),\
                     StructField('trip_distance',StringType(),True),\
                     StructField('trip_type',StringType(),True),\
                     StructField('vendorid',StringType(),True)
                    ])
url = 'https://data.cityofnewyork.us/resource/pqfs-mqru.json'
data =  urllib.request.urlopen(url).read().decode('utf-8')

rdd = sc.parallelize(data)
df = spark.createDataFrame(rdd,schema)
df.show()```

**The Error message is TypeError: StructType can not accept object '[' in type &lt;class 'str'&gt;**
** I have been able to do using dataset in scala but i am not able to understand why its not possible using python **
</code></pre>
<p>import spark.implicits._</p>
<p>// Load the data from the New York City Taxi data REST API for 2016 Green Taxi Trip Data
val url=&quot;https://data.cityofnewyork.us/resource/pqfs-mqru.json&quot;
val result = scala.io.Source.fromURL(url).mkString</p>
<p>// Create a dataframe from the JSON data
val taxiDF = spark.read.json(Seq(result).toDS)</p>
<p>// Display the dataframe containing trip data
taxiDF.show()</p>
<pre><code></code></pre>",1,0,2020-07-19 06:38:10.750000 UTC,,2020-07-19 15:14:37.573000 UTC,-1,apache-spark|pyspark|azure-databricks,138,2019-02-13 05:50:19.390000 UTC,2022-02-21 06:31:27.660000 UTC,"Bangalore, Karnataka, India",69,1,0,17,,,,,,[]
GIT or SVN Comment Appending to File After Commit or Vice Versa,"<p>I am curious if it is possible either in GIT (or Mercurial) or SVN to append your comments from the commit comment.  For example, if i have a file ABC.cpp and do a Commit -m ""hello world"", at the end of the ABC.cpp file, it will have the commit comment append to it?  Or vice versa?  EG: Have GIT or SVN look at a tag value in near the end of the ABC.cpp code and append that as a commit comment?</p>

<p>Thanks</p>",3,0,2011-11-01 00:51:04.473000 UTC,,,0,git|svn|version-control|mercurial|dvcs,286,2011-11-01 00:43:42.533000 UTC,2015-12-17 17:30:00.047000 UTC,,29,0,0,14,,,,,,[]
Sharing a graph database between Microservices,"<p>Is there any way to share a neo4j / aws Neptune graph database between microservices while restricting the access to the specific parts of the graph database to only a specific microservice ? By doing so, will there be any performance impact ?</p>",1,2,2018-08-24 15:59:00.257000 UTC,1.0,,0,neo4j|microservices|graph-databases|amazon-neptune,357,2012-12-25 14:17:31.490000 UTC,2018-09-10 09:25:14.100000 UTC,,61,6,0,7,,,,,,[]
Can I use the Gremlin Python client in a Jupyter notebook and avoid event loop errors?,"<p>I am aware of the graph-notebook project that allows Gremlin queries to be submitted using magic commands. However, sometimes I need to code in Python and connect to the server using code, from within a regular Jupyter notebook cell. If, using the Gremlin Python 3.5.2 client I try to do something like this:</p>
<pre class=""lang-py prettyprint-override""><code>server = '&lt;your server endpoint goes here&gt;'
port = 8182

endpoint = f'wss://{server}:{port}/gremlin'

connection = DriverRemoteConnection(endpoint,'g')

g = traversal().withRemote(connection)
</code></pre>
<p>an error is thrown because the Jupyter event loop is already running.</p>
<p>Is there a way around this?</p>",1,0,2022-03-03 20:40:00.903000 UTC,1.0,,0,gremlin|tinkerpop|amazon-neptune|gremlinpython|graph-notebook,24,2015-10-13 17:54:09.803000 UTC,2022-03-06 03:01:34.957000 UTC,,10012,604,7,780,,,,,,[]
Attempting to write a pyspark function to connect to SQL Server with Databricks on Apache Spart,"<p>I am trying to write a function that I can use to connect to SQL Server from Databricks.</p>
<p>My attempt is as follows:</p>
<pre><code>def readFromDb():
         jdbcDF = (spark.read
        .format(&quot;jdbc&quot;)
        .option(&quot;driver&quot;, &quot;com.microsoft.sqlserver.jdbc.SQLServerDriver&quot;)
        .option(&quot;url&quot;, &quot;jdbc:sqlserver://mysqlserver.database.windows.net;database=mydatabase&quot;)
        .option(&quot;user&quot;, 'myusername')
        .option(&quot;query&quot;, 'query')
        .option(&quot;password&quot;, 'myquery')
        .load()    
          )
</code></pre>
<p>But I keep on getting the error:</p>
<pre><code>com.microsoft.sqlserver.jdbc.SQLServerException: Incorrect syntax near &amp;#39;)&amp;#39;.
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
&lt;command-2138842992712231&gt; in &lt;module&gt;
----&gt; 1 readFromDb()

&lt;command-2138842992712230&gt; in readFromDb()
      1 def readFromDb():
----&gt; 2          jdbcDF = (spark.read
</code></pre>
<p>Any thoughts on the error?</p>",1,9,2022-03-03 21:31:22.813000 UTC,,,0,apache-spark|pyspark|azure-databricks,18,2021-03-31 08:36:43.863000 UTC,2022-03-05 23:03:22.193000 UTC,"London, UK",651,58,0,93,,,,,,[]
How to use mercurial for release management?,"<p>This is a cousin question with my <a href=""https://stackoverflow.com/questions/3719019/how-to-manage-concurrent-development-with-mercurial"">earlier question</a> of ""How to manage concurrent development with mercurial"", which covers individual developer workflow. The answer to this question will actually influence the choice for developer workflow.</p>

<p>This is <em>NOT</em> a general ""release management best practice"" or CI question, as it has been <a href=""https://stackoverflow.com/questions/134778/release-management-best-practice"">asked</a> <a href=""https://stackoverflow.com/questions/300703/what-release-management-solutions-are-worth-investigating"">many</a> <a href=""https://stackoverflow.com/questions/1714925/version-control-and-release-management"">times</a> with good answers, and there's a huge body of literature available to kill time.</p>

<p>I'm only asking for <strong>specific ways to use mercurial within the context of release management</strong>.</p>

<p>The most obvious and predominant answer would supposedly be <strong>stable/default</strong>, which is thoroughly covered by the <a href=""http://stevelosh.com/blog/2010/05/mercurial-workflows-stable-default/"" rel=""nofollow noreferrer"">beautiful blog</a> of @Steve Losh, and more concisely in <a href=""https://stackoverflow.com/questions/1405604/managing-release-branches-in-mercurial/1406906#1406906"">an answer</a> from him. It's simple and effective.</p>

<p>A prominent example of this setup is hg itself. hg uses <a href=""https://www.mercurial-scm.org/wiki/DeveloperRepos"" rel=""nofollow noreferrer"">a few more repositories</a> for active development, but for release management purposes everything seems to be contained in the stable/default branches of the <a href=""https://www.mercurial-scm.org/repo/hg"" rel=""nofollow noreferrer"">main repo</a>.</p>

<p>The hg setup actually manifests a variation, or rather an extended version, of stable/default: <strong>branch clone</strong>. I described the process in <a href=""https://stackoverflow.com/questions/890723/mercurial-named-branches-vs-multiple-repositories/3757525#3757525"">an answer</a> to a question on named branch vs. multiple repos (with another <a href=""https://stackoverflow.com/questions/890723/mercurial-named-branches-vs-multiple-repositories/890892#890892"">great answer</a> from @Martin Geisler). What I forgot to mention in my answer is how branch clone works for developer workflow: if you need to fix a bug for a branch, you would <code>hg clone &lt;main repo&gt;#&lt;branch&gt;</code> but <em>not</em> the branch clone, because your changeset will still go back to the main repo and pushed out to branch clone automatically. Of course you can choose not to clone and just <code>hg update &lt;branch&gt;</code> in your main clone, but most arguments for using separate clones (especially the independent build) apply here.</p>

<p>Now back to the question: <strong>Are there any other ways that fit different real-world scenarios?</strong> For example, a traditional major/minor/patch release cycle with long lapse between releases probably require quite a different workflow than a fast-paced, release-as-you-go web application. Please also comment on the stable/default and branch clone approaches if you feel like.</p>

<p>Since this is almost a survey question, I can only try to accept the ""best"" answer subjectively. If I can get a few more answers than my developer workflow question, that is.</p>

<p>Thank you for all your inputs!</p>",1,9,2010-09-30 04:02:00.330000 UTC,22.0,2018-02-22 10:30:37.620000 UTC,27,mercurial|dvcs|release-management,4324,2009-02-04 15:39:00.073000 UTC,2021-10-26 02:06:19.417000 UTC,"Livingston, NJ, USA",6424,592,19,608,,,,,,[]
Remove bad character if at beginning of column,"<p>I have a <code>DF</code> of emails where the first character in the email is occasionally a symbol. I am trying to remove it if it exists.</p>
<pre class=""lang-text prettyprint-override""><code>+--------------------+
|       Email        |
+--------------------+
|bob@gmail.com       |
|*steve@yahoo.com    |
|leeroy@hotmail.com  |
|@grant@gmail.com    |
+--------------------+
</code></pre>
<p>The final <code>df</code> would look like this:</p>
<pre class=""lang-text prettyprint-override""><code>+--------------------+
|       Email        |
+--------------------+
|bob@gmail.com       |
|steve@yahoo.com     |
|leeroy@hotmail.com  |
|grant@gmail.com     |
+--------------------+
</code></pre>
<p>Is there a way to do this efficiently?</p>",1,1,2021-11-15 19:14:03.003000 UTC,,2021-11-15 19:34:49.963000 UTC,0,scala|apache-spark|apache-spark-sql|azure-databricks,34,2012-11-29 21:14:27.167000 UTC,2022-03-04 18:06:15.353000 UTC,,283,21,0,96,,,,,,[]
Copy data from azure blob storage to adls gen 2,"<p>I have around 2 million json files in azure blob storage.Each file contains one record.I need to move all those json files to adls gen 2 whose createdate is greater than 2019-01-01.</p>
<p>Note:createdate is one of the field inside json.</p>
<p>Is it possible to achieve this through azure data factory</p>",1,0,2020-07-30 18:56:40.190000 UTC,,,0,azure|azure-data-factory|azure-data-factory-2|azure-databricks|azure-data-lake-gen2,446,2020-05-05 07:29:45.147000 UTC,2021-04-26 08:21:33.783000 UTC,,21,0,0,7,,,,,,[]
Using Entity Framework Code First Migrations in a DVCS Project,"<p>I have always found version control a bit of an issue when it comes to database schemas. </p>

<p>So - I am currently evaluating Entity Framework Code First Migrations and so far I'm really impressed.</p>

<p>My question is, does anyone have any experience of using Migrations in a team using a DVCS?</p>

<p>If developers working on different branches each create their own Migrations, does the 'Update-Database' tool cope well with that when the branches are merged?</p>

<p>I guess what could happen is that a new Migration would appear in the middle of the list. Would this then get picked up, or does it just look for 'newer' migrations than the last one deployed?</p>

<p>I appreciate the team are going to have to be careful not to create conflicting schema changes - this is something we can manage - but it would be useful to know if 'Update-Database' is smart enough to spot the 'missing' migration?</p>

<p>Thanks,
- Chris</p>",3,1,2012-04-24 12:56:27.787000 UTC,1.0,,13,entity-framework|dvcs|entity-framework-migrations,381,2008-08-06 07:16:20.927000 UTC,2022-03-03 11:18:38.340000 UTC,"Cheltenham, United Kingdom",18224,138,22,1062,,,,,,[]
Performant query to find all reacheable nodes,"<p>I'm studying AWS Neptune with Gremlin to build a permission system.
This system would have basically 3 types of vertices: Users, Permissions and Groups.</p>

<ul>
<li>A group has <em>0..n</em> permissions</li>
<li>A user can have <em>0..n</em> groups</li>
<li>A user can be directly  connected to <em>0..n</em> permissions</li>
<li>A user can be connected to another user, in which case it ""inheritates"" that user permission's</li>
<li>A group can be inside of another group, that is inside of another group.... so on.</li>
</ul>

<p>I'm looking for a performant query to find all permissions for a given user.</p>

<p>This graph may get really huge so to stress it out I have build a 17kk user vertices graph, created 10 random edges for each one of them and then created a few permissions.</p>

<p>Then the query I was using to get all permissions is obviouly running forever... n_n'</p>

<p>What I'm trying is simply:</p>

<pre><code>g.V('u01')
    .repeat(out())
    .until(hasLabel('Permission'))
    .simplePath()
</code></pre>

<p>Is there a better query to achieve it? Or maybe even a better modeling for this scenario?</p>

<p>I was thinking that maybe my 10 random edges have created a lot of cycles and connections that ""make no sense"" and thats why the query is slow. Does it make sense?</p>

<p>Thanks in advance!</p>",1,0,2019-12-17 15:23:13.760000 UTC,,,0,graph-databases|gremlin|amazon-neptune,108,2012-11-08 03:58:38.637000 UTC,2022-03-04 15:54:36.213000 UTC,"Belo Horizonte, MG, Brasil",2860,92,34,448,,,,,,[]
MagContainer field in Microsoft Academic Graph PySpark examples,"<p>I am trying out a PySpark tutorial using Microsoft Academic Graph(MAG) data.
(<a href=""https://github.com/Azure-Samples/microsoft-academic-graph-pyspark-samples/blob/master/src/AIIndex.ipynb"" rel=""nofollow noreferrer"">https://github.com/Azure-Samples/microsoft-academic-graph-pyspark-samples/blob/master/src/AIIndex.ipynb</a>)</p>
<p>I keep getting errors in the &quot;Load MAG data&quot; section.
Even though I run the 4th line that would take the least time (Affiliations.txt - due to the small file size of 5MB), it takes about 11minutes to run, and outputs the following error message.</p>
<blockquote>
<p>shaded.databricks.org.apache.hadoop.fs.azure.AzureException:
shaded.databricks.org.apache.hadoop.fs.azure.AzureException: Unable to
access container $root in account mag-datashare using anonymous
credentials, and no credentials found for them  in the configuration.</p>
</blockquote>
<p>I've searched for the error messages but to no avail. I think I've done something wrong in the &quot;Initialize storage account and container details&quot; section, especially the MagContainer and OutputContainer variables.</p>
<p>In my storage account,</p>
<ol>
<li>I have a container named 'mag-datashare' which is shared from Microsoft, and I am trying to put the path in the MagContainer variable. The description says its in the form of 'mag-yyyy-mm-dd', but trying that fails. I have tried 'mag-datashare/mag/2021-10-11', 'mag-datashare/mag', 'mag-2021-10-11', 'mag-2021-10-11', 'mag-datashare' which all have failed. Some fail right away, but some take 11 minutes. Have searched for other examples, but they seem to have a different format, an additional MagVersion variable is used.</li>
<li>I have made a container named 'mag-output' and put its path in the OutputContainer variable.</li>
</ol>
<p>Can anyone please help me run the code?</p>",0,0,2021-10-28 06:26:56.293000 UTC,,,0,azure|pyspark|azure-databricks|microsoft-academic-graph,17,2015-06-28 07:24:12.840000 UTC,2022-02-22 04:14:55.607000 UTC,,115,11,0,11,,,,,,[]
Databricks version 7.0 not behaving like version 6.3: class java.lang.Long cannot be cast to class java.lang.Integer,"<p>I have a working notebook at azure databricks version 6.3 - Spark 2.4.4</p>
<p>This notebook does ingestions into Azure Synapse Analytics using it's connector</p>
<p>When I upgraded the notebook to version 7.0 - Spark 3.0.0, the process begun to fail with the following error:</p>
<blockquote>
<p>com.microsoft.sqlserver.jdbc.SQLServerException:
HdfsBridge::recordReaderFillBuffer - Unexpected error encountered
filling record reader buffer: ClassCastException: class java.lang.Long
cannot be cast to class java.lang.Integer (java.lang.Long and
java.lang.Integer are in module java.base of loader 'bootstrap')
[ErrorCode = 106000] [SQLState = S0001]</p>
</blockquote>
<p>This is the table schema in the Synapse Analytics:</p>
<pre><code>CREATE TABLE [dbo].[IncrementalDestination]
(
[Id] [int] NOT NULL,
[VarChar] [varchar](1000) NULL,
[Char] [char](1000) NULL,
[Text] [varchar](1000) NULL,
[NVarChar] [nvarchar](1000) NULL,
[NChar] [nchar](1000) NULL,
[NText] [nvarchar](1000) NULL,
[Date] [date] NULL,
[Datetime] [datetime] NULL,
[Datetime2] [datetime2](7) NULL,
[Smalldatetime] [smalldatetime] NULL,
[Bigint] [bigint] NULL,
[Bit] [bit] NULL,
[Decimal] [decimal](18, 0) NULL,
[Int] [int] NULL,
[Money] [money] NULL,
[Numeric] [numeric](18, 0) NULL,
[Smallint] [smallint] NULL,
[Smallmoney] [smallmoney] NULL,
[Tinyint] [tinyint] NULL,
[Float] [float] NULL,
[Real] [real] NULL,
[Column With Space] [varchar](1000) NULL,
[Column_ç_$pecial_char] [varchar](1000) NULL,
[InsertionDateUTC] [datetime] NOT NULL,
[De_LastUpdated] [datetime2](3) NOT NULL
)
WITH
(
DISTRIBUTION = ROUND_ROBIN,
CLUSTERED COLUMNSTORE INDEX
)
GO
</code></pre>
<p>This is the schema generated by Databricks after reading a bunch of parquets in the Azure BlobStorage</p>
<pre><code>root
 |-- Id: long (nullable = true)
 |-- VarChar: string (nullable = true)
 |-- Char: string (nullable = true)
 |-- Text: string (nullable = true)
 |-- NVarChar: string (nullable = true)
 |-- NChar: string (nullable = true)
 |-- NText: string (nullable = true)
 |-- Date: timestamp (nullable = true)
 |-- Datetime: timestamp (nullable = true)
 |-- Datetime2: timestamp (nullable = true)
 |-- Smalldatetime: timestamp (nullable = true)
 |-- Bigint: long (nullable = true)
 |-- Bit: boolean (nullable = true)
 |-- Decimal: long (nullable = true)
 |-- Int: long (nullable = true)
 |-- Money: double (nullable = true)
 |-- Numeric: long (nullable = true)
 |-- Smallint: long (nullable = true)
 |-- Smallmoney: double (nullable = true)
 |-- Tinyint: long (nullable = true)
 |-- Float: double (nullable = true)
 |-- Real: double (nullable = true)
 |-- Column_With_Space: string (nullable = true)
 |-- Column_ç_$pecial_char: string (nullable = true)
 |-- InsertionDateUTC: timestamp (nullable = true)
 |-- De_LastUpdated: timestamp (nullable = false)
</code></pre>
<p>I saw this</p>
<pre><code>Int: long (nullable = true)
</code></pre>
<p>But what can I do?</p>
<p>Shouldn't this conversion be natural and easily done?</p>
<p>I think something broke with these new features =]</p>",1,0,2020-06-20 22:07:13.437000 UTC,,,3,azure-databricks|azure-sql-data-warehouse|azure-synapse,696,2015-09-15 08:29:24.193000 UTC,2021-08-26 20:22:38.807000 UTC,"São Paulo, State of São Paulo, Brazil",328,306,3,79,,,,,,[]
How to perform subtraction 1 with column value,"<p>I was stuck with perform an subtraction operation on withColumn in scala.</p>
<pre><code>val result = finalJoinedDf.withColumn(&quot;CTF&quot;, when(col(&quot;VPC&quot;) === null or col(&quot;FreightExpense&quot;) === null or col(&quot;FreightRevenue&quot;) === null or col(&quot;Cost&quot;) === null,  null)
                                  .otherwise(1 - col(&quot;VPC&quot;).cast(DoubleType)/100))
</code></pre>
<p>My issues is that I'm unable to perform this subtraction it is giving the following error.</p>
<p><a href=""https://i.stack.imgur.com/sCQjt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sCQjt.png"" alt=""enter image description here"" /></a></p>
<p>Even I'm unable to perform addition also.</p>
<p>Please someone help me here</p>",1,1,2021-08-18 13:13:07.893000 UTC,,,0,scala|azure-databricks,29,2019-04-04 07:53:20.343000 UTC,2022-02-21 11:16:47.800000 UTC,"Chennai, Tamil Nadu, India",133,3,0,11,,,,,,[]
Gremlin convert properties to date and calculate diff in days,"<p>In a graph database, imagine I have a vertex with 2 properties: createDate and modifiedDate, both of them of type string with values like: 2015-12-22 04:35:36.0</p>

<p>Is there a way to calculate the difference in days between the modifiedDate and createDate?</p>

<p>This query will be running on AWS Neptune.</p>

<p>Thank you</p>",1,0,2020-02-25 22:50:05.317000 UTC,,,2,gremlin|janusgraph|amazon-neptune,239,2012-09-11 10:59:37.887000 UTC,2022-03-05 21:40:45.320000 UTC,,408,319,6,45,,,,,,[]
Azure Databricks API: import entire directory with notebooks,"<p>I need to import many notebooks (both Python and Scala) to Databricks using Databricks REST API 2.0</p>
<p>My source path (local machine) is <code>./db_code</code> and destination (Databricks workspace) is <code>/Users/dmitriy@kagarlickij.com</code></p>
<p>I'm trying to build <a href=""https://docs.databricks.com/dev-tools/api/latest/workspace.html#import"" rel=""nofollow noreferrer"">2.0/workspace/import</a> call, so my body is: <code>{ &quot;content&quot;: &quot;$SOURCES_PATH&quot;, &quot;path&quot;: &quot;$DESTINATION_PATH&quot;, &quot;format&quot;: &quot;SOURCE&quot;, &quot;language&quot;: &quot;SCALA&quot;, &quot;overwrite&quot;: true }</code></p>
<p>However I'm getting error: <code>Could not parse request object: Illegal character</code> and as per documentation <code>content</code> must be <code>The base64-encoded content</code></p>
<p>Should I encode all notebooks to base64?</p>
<p>Maybe there're some examples of importing directory to Databricks using API?</p>",2,1,2020-06-29 11:02:16.757000 UTC,,,0,azure-databricks,769,2016-01-11 10:20:46.393000 UTC,2022-03-05 18:14:32.720000 UTC,,5936,509,9,269,,,,,,[]
Neptune connectivity failing,"<p>I'm trying to hit my Neptune cluster in a production account. This works in my non-production account.</p>
<pre><code>awscurl -X POST -d '{&quot;gremlin&quot;:&quot;g.V().count().iterate()&quot;}' https://neptune-instance-........neptune.amazonaws.com/gremlin --service neptune-db --header 'host: neptune-instance-.......neptune.amazonaws.com'
</code></pre>
<p>I'm getting <em>timeout</em> exceptions</p>
<ul>
<li>Subnet group is good, it's all 3 of my internal subnets</li>
<li>IAM enabled</li>
<li>Open security group</li>
<li>Role is Neptune/RDS full access</li>
</ul>
<p>What could I be missing in this case?</p>
<p>The cluster is up and running and everything. I've ran over the configuration of the cluster many times and it all looks good, security groups, IAM, etc.</p>",1,0,2021-09-02 16:15:59.413000 UTC,,,0,amazon-web-services|amazon-neptune,33,2020-08-20 21:22:57.010000 UTC,2022-03-05 23:21:22.593000 UTC,"Seattle, WA, USA",655,57,5,91,,,,,,[]
View and edit the create statement of a view in SQL databricks-delta lake,"<p>Ive created a SQL view in databricks
ie.</p>
<pre><code>create view [database].[view name] as select col1, col2 from [table] 
</code></pre>
<p>How do I view and edit that create statement now?</p>
<p>I've tried <code>show create table [database].[view name]</code>, which does return the create statement in unformatted text in the result window, which is a pain to reformat, especially if you have comments in the code and its a particularly large and complex query.</p>
<p>Then the only way i can see to edit that view is to copy that text into a new window, make changes and paste it beneath an alter view alter body statement.</p>
<p>Is there a simpler way to show and edit queries behind SQL view objects in databricks?</p>
<p>For example like in ssms where you can &quot;Script View As&quot;, then change the create to an alter and change some things and run again to edit.</p>",1,2,2021-12-01 01:21:45.377000 UTC,,2021-12-01 01:29:11.500000 UTC,2,sql|view|azure-databricks,102,2021-07-13 23:07:01.007000 UTC,2021-12-09 22:24:04.560000 UTC,,21,0,0,0,,,,,,[]
Order by (Sorting) is very slow in neptune gremlin,"<p>We are using Amazon Neptune AWS graph database. When we apply order by it is taking too much time.</p>
<pre><code>g.V().
  has(&quot;Persons&quot;,&quot;_id&quot;,&quot;xxxxxxxxx-xxx-xxxx&quot;).
  out(&quot;joinGroup&quot;).
  skip(0).
  limit(100).
  project(&quot;lastActivity&quot;).
    by(__.inE(&quot;activity&quot;).
          has(&quot;display&quot;,&quot;visible&quot;).
          order().
            by(&quot;_createdOn&quot;,desc).
          limit(1).valueMap())
</code></pre>
<p>I simple want to get last Chat and have saved a chat node with group node. Person can have groups and can chat</p>
<pre><code>&gt;  Neptune Gremlin Profile
&gt;     
&gt;     ===================
&gt;     Neptune steps:
&gt;     [
&gt;         NeptuneGraphQueryStep(Vertex) {
&gt;             JoinGroupNode {
&gt;                 PatternNode[(?1, &lt;_id&gt;, &quot;xxxxxxxxx-xxx-xxxx&quot;, ?) . project ?1 .], {estimatedCardinality=1, indexTime=0, joinTime=0,
&gt; numSearches=1, actualTotalOutput=1}
&gt;                 PatternNode[(?1, &lt;~label&gt;, ?2=&lt;Persons&gt;, &lt;~&gt;) . project ask .], {estimatedCardinality=1008, indexTime=0, joinTime=0,
&gt; numSearches=1, actualTotalOutput=1}
&gt;                 PatternNode[(?1, ?5=&lt;joinGroup&gt;, ?3, ?6) . project ?1,?3 . IsEdgeIdFilter(?6) .], {estimatedCardinality=9566,
&gt; indexTime=0, joinTime=0, numSearches=1}
&gt;             }, finishers=[limit(100)], annotations={path=[Vertex(?1):GraphStep, Vertex(?3):VertexStep],
&gt; joinStats=true, optimizationTime=1, maxVarId=11, executionTime=901}
&gt;         },
&gt;         NeptuneTraverserConverterStep
&gt;     ]
&gt;     + not converted into Neptune steps: [ProjectStep([lastActivity],[[VertexStep(IN,[activity],edge),
&gt; ProfileStep, NeptuneHasStep([display.eq(visible)]), ProfileStep,
&gt; NeptuneMemoryTrackerStep, OrderGlobalStep([[value(_createdOn),
&gt; desc]]), ProfileStep, RangeGlobalStep(0,1), ProfileStep,
&gt; PropertyMapStep(value), ProfileStep]])]
&gt;     
&gt;     WARNING: &gt;&gt; ProjectStep([lastActivity],[[VertexStep(IN,[activity],edge),
&gt; ProfileStep, NeptuneHasStep([display.eq(visible)]), ProfileStep,
&gt; NeptuneMemoryTrackerStep, OrderGlobalStep([[value(_createdOn),
&gt; desc]]), ProfileStep, RangeGlobalStep(0,1), ProfileStep,
&gt; PropertyMapStep(value), ProfileStep]]) &lt;&lt; (or one of its children) is
&gt; not supported natively yet
&gt;     
&gt;     
&gt;     Runtime (ms)
&gt;     ============
&gt;     Query Execution: 900.745
&gt;     
&gt;     Traversal Metrics
&gt;     =================
&gt;     Step                                                               Count  Traversers       Time (ms)    % Dur
&gt;     -------------------------------------------------------------------------------------------------------------
&gt;     NeptuneGraphQueryStep(Vertex)                                         53          53           1.226     0.14
&gt;     NeptuneTraverserConverterStep                                         53          53           0.604     0.07
&gt;     ProjectStep([lastActivity],[[VertexStep(IN,[...                       53          53         897.957    99.80
&gt;       VertexStep(IN,[activity],edge)                                   13671       13671          49.116
&gt;       NeptuneHasStep([display.eq(visible)])                            12680       12680         439.631
&gt;       OrderGlobalStep([[value(_createdOn), desc]])                        53          53         399.198
&gt;       RangeGlobalStep(0,1)                                                53          53           0.231
&gt;       PropertyMapStep(value)                                              53          53           7.728
&gt;                                                 &gt;TOTAL                     -           -         899.788        -
&gt;     
&gt;     Predicates
&gt;     ==========
&gt;     # of predicates: 403
&gt;     
&gt;     Results
&gt;     =======
&gt;     Count: 53
&gt;     
&gt;     Index Operations
&gt;     ================
&gt;     Query execution:
&gt;         # of statement index ops: 26460
&gt;         # of unique statement index ops: 26460
&gt;         Duplication ratio: 1.0
&gt;         # of terms materialized: 0
</code></pre>",0,2,2020-12-25 12:49:49.060000 UTC,,2021-02-27 19:49:05.537000 UTC,0,gremlin|amazon-neptune,291,2012-10-16 13:32:06.787000 UTC,2022-03-05 11:50:30.490000 UTC,,1019,32,0,419,,,,,,[]
Limit the data for spark dataframe,"<p>I am reading the data from NOSQL database using spark dataframe. Since there is limit to load maximum of 40MB data in databricks, i am looking for a solution to limit the data. I have tried using limit() or take() options but both are giving me an error since they read whole data first and limit later.</p>

<p>While reading itself it is throwing an error, is there anyway we can limit the data before reading entire dataset? We filtered the data and taking only two columns but still this data is huge.</p>

<pre><code>ReadData = spark.read.format(""com.mongodb.spark.sql.DefaultSource"").option(""uri"",connectionstring).option(""pipeline"",pipeline).load().limit(2000)
</code></pre>",1,3,2019-10-18 03:05:17.813000 UTC,,2019-10-28 21:14:26.853000 UTC,0,scala|apache-spark|apache-spark-sql|azure-databricks,347,2019-08-08 04:17:46.893000 UTC,2020-07-22 17:05:31.950000 UTC,,135,3,0,26,,,,,,[]
Version control system with working copy auto sync,"<p>I have laptop and desktop computer and I want my working copy to be in sync between them all the time.
For now I'm using git.</p>
<p>For now my workflow is to commit every time to remote git repo into my branch and than pull that branch from another computer.
There are two problems with this workflow:</p>
<ol>
<li>I have to commit partially finished code and write a commit message for it</li>
<li>I often forget to commit &amp; push before I swtich to laptop so I just can't continue my work on it.</li>
</ol>
<p>I wanted to automate it, but with git commit &amp; push every 5 seconds doesn't seem to be a good idea.</p>
<p><strong>The questions</strong></p>
<ul>
<li>Is it possible to do an automated working copies sync with git? How?</li>
<li>Do you know any other VCS can sync working copies automatically?</li>
</ul>",1,0,2021-01-31 07:37:00.650000 UTC,,,1,git|dvcs,48,2013-07-21 09:48:27.843000 UTC,2021-11-09 15:01:46.977000 UTC,"Russia, Moscow",911,29,4,155,,,,,,[]
"How to partition SQL Server table, where partition column is integer but in date format(20170101 to 20200306) using pyspark?","<p>I have integer column which is a date actually.</p>
<p>like this
20170101
20170103
20170102
.....</p>
<p>20200101</p>
<p>around 10 million rows in each partition.</p>
<p>how to read table using this field as partition column in pyspark?</p>",1,0,2020-06-26 10:00:05.933000 UTC,,,0,apache-spark|pyspark|apache-spark-sql|azure-databricks|data-partitioning,101,2017-10-03 06:05:08.900000 UTC,2020-08-07 08:57:34.397000 UTC,"Mumbai, Maharashtra, India",9,0,0,4,,,,,,[]
Unable to read Azure Eventhub topics from spark,"<p>Enviroment details</p>
<ol>
<li>spark version : 3.x</li>
<li>Python version 3.8 and java version 8</li>
<li>azure-eventhubs-spark_2.12-2.3.17.jar</li>
</ol>
<pre><code>import json
from pyspark.sql import SparkSession


#the below command getOrCreate() uses the SparkSession shared across the jobs instead of using one SparkSession per job.
spark = SparkSession.builder.appName('ntorq_eventhub_load').getOrCreate()

#ntorq adls checkpoint location.
ntorq_connection_string = &quot;connection-string&quot;

ehConf = {}
ehConf['eventhubs.connectionString'] = spark.sparkContext._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(ntorq_connection_string)
# ehConf['eventhubs.connectionString'] = ntorq_connection_string
ehConf['eventhubs.consumerGroup'] = &quot;$default&quot;

OFFSET_START = &quot;-1&quot;   # the beginning
OFFSET_END = &quot;@latest&quot;

# Create the positions
startingEventPosition = {
  &quot;offset&quot;: OFFSET_START ,
  &quot;seqNo&quot;: -1,            #not in use
  &quot;enqueuedTime&quot;: None,   #not in use
  &quot;isInclusive&quot;: True
}

endingEventPosition = {
  &quot;offset&quot;: OFFSET_END,           #not in use
  &quot;seqNo&quot;: -1,              #not in use
  &quot;enqueuedTime&quot;: None,
  &quot;isInclusive&quot;: True
}

# Put the positions into the Event Hub config dictionary
ehConf[&quot;eventhubs.startingPosition&quot;] = json.dumps(startingEventPosition)
ehConf[&quot;eventhubs.endingPosition&quot;] = json.dumps(endingEventPosition)


df = spark \
  .readStream \
  .format(&quot;eventhubs&quot;) \
  .options(**ehConf) \
  .load() \
  .selectExpr(&quot;cast(body as string) as body_str&quot;)

df.writeStream \
    .format(&quot;console&quot;) \
    .start()
</code></pre>
<p>error</p>
<pre><code>21/04/25 20:17:53 WARN Utils: Your hostname,resolves to a loopback address: 127.0.0.1; using 192.168.1.202 instead (on interface en0)
21/04/25 20:17:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/04/25 20:17:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to &quot;WARN&quot;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Traceback (most recent call last):
  File &quot;/Users/PycharmProjects/pythonProject/test.py&quot;, line 12, in &lt;module&gt;
    ehConf['eventhubs.connectionString'] = spark.sparkContext._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(ntorq_connection_string)
TypeError: 'JavaPackage' object is not callable
</code></pre>
<p>Code is working fine on databricks environment but unable to consume all messages from eventhub I tried clearing the default checkpointing folders before running every time but still facing the issue, so want to try on the local system.
When trying on local environment facing JavaPackage issue.
Appreciate any help.
thank you</p>",1,3,2021-04-25 15:09:53.520000 UTC,,,1,apache-spark|pyspark|azure-databricks,279,2019-09-02 13:45:52.790000 UTC,2022-02-14 13:44:15.477000 UTC,"Pune, Maharashtra, India",11,0,0,1,,,,,,[]
how to exclude Vertex which does not contain a value,"<p>I am using AWS Neptune gremlin, I have a vertex with a property stored multiple values</p>

<p>Vertex label 'apple' with property color, e.g. created a 'apple' vertex with property color multiple values ['red', 'white']</p>

<p><code>g.V().hasLabel('apple').has('color', TextP.notContaining('wh'))</code></p>

<p>the problem is it still returns this vertex, how can I exclude the V that contains 'wh'?</p>",1,0,2020-04-28 10:15:02.873000 UTC,,2020-04-28 12:20:43.643000 UTC,0,gremlin|amazon-neptune,235,2009-09-01 06:49:22.620000 UTC,2022-03-03 08:07:01.783000 UTC,"Shanghai, China",1317,18,2,72,,,,,,[]
(SPARK) What is the best way to partition data on which multiple filters are applied?,"<p>I am working in Spark (on azure databricks) with a 15 billion rows file that looks like this :</p>

<pre><code>+---------+---------------+----------------+-------------+--------+------+
|client_id|transaction_key|transaction_date|   product_id|store_id|spend|
+---------+---------------+----------------+-------------+--------+------+
|        1|  7587_20121224|      2012-12-24|     38081275|     787| 4.54|
|        1| 10153_20121224|      2012-12-24|         4011|    1053| 2.97|
|        2|  6823_20121224|      2012-12-24|    561122924|     683| 2.94|
|        3| 11131_20121224|      2012-12-24|     80026282|    1131|  0.4|
|        3|  7587_20121224|      2012-12-24|        92532|     787| 5.49|
</code></pre>

<p>This data is used for all my queries, which consist mostly in groupby (product_id for example), sum and count distinct :</p>

<pre><code>results = trx.filter(col(""transaction_date"") &gt; ""2018-01-01""
                     &amp; 
                     col(""product_id"").isin([""38081275"", ""4011""])
             .groupby(""product_id"")
             .agg(sum(""spend"").alias(""total_spend""),
                  countdistinct(""transaction_key"").alias(""number_trx""))
</code></pre>

<p>I never need to use 100% of this data, I always start with a filter on :</p>

<ul>
<li><em>transaction_date</em> (1 000 distinct values)</li>
<li><em>product_id</em> (1 000 000 distinct values)</li>
<li><em>store_id</em> (1 000 distinct values)</li>
</ul>

<p>==> What is the best way to partition this data in a parquet file ?</p>

<p>I initially partitionned the data on <em>transaction_date</em> :</p>

<pre><code>trx.write.format(""parquet"").mode(""overwrite"").partitionBy(""transaction_date"").save(""dbfs:/linkToParquetFile"")
</code></pre>

<p>This will create partitions that are approximately the same size.
However, most of the queries will require to keep at least 60% of the <em>transaction_date</em>, whereas only a few <em>product_id</em> are usually selected in 1 query.
(70% of the <em>store_id</em> kept usually)</p>

<p>==> Is there a way to build a parquet file taking this into account ?</p>

<p>It seems partitionning the data on <em>product_id</em> would create way too much partitions...</p>

<p>Thanks!</p>",1,0,2019-04-08 15:35:07.597000 UTC,1.0,,2,apache-spark|pyspark|filtering|data-partitioning|azure-databricks,242,2019-03-21 16:02:58.120000 UTC,2021-04-28 13:17:43.690000 UTC,,41,0,0,3,,,,,,[]
AWS Neptune (db.r5.large) can't handle more than 250 requests per second,"<p>So I have a Neptune (db.r5.large) on AWS, If I make more than 250 requests/second. I will get the error <code>HTTP 429: Too Many Requests</code>.</p>
<p>What should I do here? I was expecting db.r5.large instance type should able handle this kind of work load, and I don't have budget to move up another instance type.</p>
<p><a href=""https://i.stack.imgur.com/c8xzD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c8xzD.png"" alt=""enter image description here"" /></a></p>",1,2,2022-01-20 18:53:11.583000 UTC,,,0,amazon-web-services|amazon-neptune,38,2012-02-03 16:20:42.710000 UTC,2022-03-04 16:51:03.243000 UTC,,5628,119,6,508,,,,,,[]
How to pass Tumbling Window parameters to a Data Factory pipeline in the Data Factory UI?,"<p>I have defined a pipeline in <code>Azure Data Factory</code> with a Tumbling Window trigger, as seen below:</p>

<p><a href=""https://i.stack.imgur.com/LDuIE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LDuIE.png"" alt=""Pipeline with the trigger""></a></p>

<p>I would like for my activities to receive the Tumbling window parameters (<code>trigger().outputs.windowStartTime</code> and <code>trigger().outputs.windowEndTime</code>) however I did not find any examples in <a href=""https://docs.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger"" rel=""nofollow noreferrer"">the documentation</a> showing how to do this in the UI.</p>

<p><strong>Question</strong></p>

<p>How can I pass the Tumbling Window parameters to a Data Factory pipeline in the Data Factory UI?</p>",3,0,2019-04-01 13:06:15.200000 UTC,,,4,azure|azure-data-factory|azure-databricks,2732,2014-11-15 21:41:52.010000 UTC,2022-03-05 23:35:18.467000 UTC,"Montréal, QC, Canada",6248,797,24,742,,,,,,[]
"in Databricks, how to create a SQL function with dynamic variables in a notebook","<p>I would like to migrate SQL tables, procedures and functions to databrick's notebook. How can I create functions and procedures with parameters?</p>

<p>This is a new architecture, I used to save data from the Azure data lake to Azure database.</p>

<pre><code>CREATE TEMPORARY FUNCTION [db_name].table (@par1 int, @par2 varchar(10)) returns varchar(10) as
begin
      declare @var1  varchar(10);
      declare @var2 int;     
...
ends; 
</code></pre>

<p>I get this error</p>

<blockquote>
  <p>Error in SQL statement: ParseException: mismatched input 'begin'
  expecting {'(', 'SELECT', 'FROM', 'ADD', 'DESC', 'WITH', 'VALUES',
  'CREATE', 'TABLE', 'INSERT', 'DELETE', 'DESCRIBE', 'EXPLAIN', 'SHOW',
  'USE', 'DROP', 'ALTER', 'MAP', 'SET', 'RESET', 'START', 'COMMIT',
  'ROLLBACK', 'MERGE', 'UPDATE', 'CONVERT', 'REDUCE', 'REFRESH',
  'CLEAR', 'CACHE', 'UNCACHE', 'DFS', 'TRUNCATE', 'ANALYZE', 'LIST',
  'REVOKE', 'GRANT', 'LOCK', 'UNLOCK', 'MSCK', 'EXPORT', 'IMPORT',
  'LOAD', 'OPTIMIZE'}(line 1, pos 0)</p>
  
  <p>== SQL == declare @var1 ^^^   return 0</p>
</blockquote>",0,0,2019-06-14 10:03:25.017000 UTC,,2019-06-14 10:31:38.533000 UTC,1,python|sql-server|apache-spark|azure-databricks,599,2019-06-14 09:45:16.433000 UTC,2020-03-27 22:49:45.960000 UTC,,43,0,0,1,,,,,,[]
"Mercurial: enforce ""hg pull -u"" before ""hg commit""","<p>I have in some cases a need to enforce that Mercurial-users have run <code>hg pull -u</code> before any <code>hg commit</code> can be allowed, i.e., <code>hg pull</code> will mean that the incoming queue is empty — and furthermore I also want that the person is using the head version of the branch.</p>

<p>How can I set up such a restriction?</p>

<p>(I am fully aware that this goes against parts of the DVCS design core)</p>",3,3,2012-04-16 13:57:11.747000 UTC,,2012-04-16 15:36:56.707000 UTC,7,mercurial|dvcs|mercurial-hook,2214,2012-04-13 15:52:13.680000 UTC,2022-03-04 07:15:53.483000 UTC,Denmark,545,32,6,68,,,,,,[]
How to use Spark to copy from XML to SQL Server,"<p>I need to open and copy the content of multiple XML files stored on an Azure Datalake Store into an Azure SQL DB. This is the XML file structure:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;FileSummary xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" 
    xsi:noNamespaceSchemaLocation=""invoices.xsd""&gt;
      &lt;Header&gt;
      &lt;SequenceNumber&gt;1&lt;/SequenceNumber&gt;
      &lt;Description&gt;Hello&lt;/Description&gt;
      &lt;ShipDate&gt;20180101&lt;/ShipDate&gt;
     &lt;/Header&gt;
     &lt;FileInvoices&gt;
      &lt;InvoiceNumber&gt;000000A&lt;/InvoiceNumber&gt;
      &lt;InvoiceHeader&gt;
       &lt;InvoiceHeaderDate&gt;201800201&lt;/InvoiceHeaderDate&gt;
       &lt;InvoiceHeaderDescription&gt;XYZ&lt;/InvoiceHeaderDescription&gt;
      &lt;/InvoiceHeader&gt;
      &lt;InvoiceItems&gt;
       &lt;ItemId&gt;000001&lt;/ItemId&gt;
       &lt;ItemQuantity&gt;000010&lt;/ItemQuantity&gt;
       &lt;ItemPrice&gt;000100&lt;/ItemPrice&gt;
      &lt;/InvoiceItems&gt;
     &lt;/FileInvoices&gt;
     &lt;FileInvoices&gt;
      &lt;InvoiceNumber&gt;000000B&lt;/InvoiceNumber&gt;
      &lt;InvoiceHeader&gt;
       &lt;InvoiceHeaderDate&gt;201800301&lt;/InvoiceHeaderDate&gt;
       &lt;InvoiceHeaderDescription&gt;ABC&lt;/InvoiceHeaderDescription&gt;
      &lt;/InvoiceHeader&gt;
      &lt;InvoiceItems&gt;
       &lt;ItemId&gt;000002&lt;/ItemId&gt;
       &lt;ItemQuantity&gt;000020&lt;/ItemQuantity&gt;
       &lt;ItemPrice&gt;000200&lt;/ItemPrice&gt;
      &lt;/InvoiceItems&gt;
     &lt;/FileInvoices&gt;
&lt;/FileSummary&gt;
</code></pre>

<p>So I used Azure Databricks to mount the Datalake Store as ""/mnt/testdata"" and then I tried opening the sample file above with the following command</p>

<pre><code>dfXml = (sqlContext.read.format(""xml"") # requires maven library &lt;HyukjinKwon:spark-xml:0.1.1-s_2.11&gt;
         .options(rootTag='FileSummary')
         .load('/mnt/testdata/data/invoices_file1.xml')) 
dfXml.cache()
print (""Number of records in this dataframe: "" + str(dfXml.count())) 

dfXml.printSchema()
</code></pre>

<p>returns the following result:</p>

<pre><code>dfXml:pyspark.sql.dataframe.DataFrame
FileInvoices:array
element:struct
InvoiceHeader:struct
InvoiceHeaderDate:long
InvoiceHeaderDescription:string
InvoiceItems:struct
ItemId:long
ItemPrice:long
ItemQuantity:long
InvoiceNumber:string
Header:struct
Description:string
SequenceNumber:long
ShipDate:long
xmlns:xsi:string
xsi:noNamespaceSchemaLocation:string
Number of records in this dataframe: 1
root
 |-- FileInvoices: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- InvoiceHeader: struct (nullable = true)
 |    |    |    |-- InvoiceHeaderDate: long (nullable = true)
 |    |    |    |-- InvoiceHeaderDescription: string (nullable = true)
 |    |    |-- InvoiceItems: struct (nullable = true)
 |    |    |    |-- ItemId: long (nullable = true)
 |    |    |    |-- ItemPrice: long (nullable = true)
 |    |    |    |-- ItemQuantity: long (nullable = true)
 |    |    |-- InvoiceNumber: string (nullable = true)
 |-- Header: struct (nullable = true)
 |    |-- Description: string (nullable = true)
 |    |-- SequenceNumber: long (nullable = true)
 |    |-- ShipDate: long (nullable = true)
 |-- xmlns:xsi: string (nullable = true)
 |-- xsi:noNamespaceSchemaLocation: string (nullable = true)
</code></pre>

<p>So it looks like the command above does read the file correctly and of course I'm able to connect to my well-normalized Azure SQL DB and write records into the specific tables:</p>

<pre><code>dfXml.write.jdbc(url=jdbcUrl, table=""dest_table"", mode=""overwrite"", properties=connectionProperties)
</code></pre>

<p>, however this method requires setting some nested loops and a lot of manual tasks to keep track of each table' keys and respect referential integrity that do not leverage Spark architecture so I'm now wondering if there is the best practice (or pre-built library) which does this task in a more automated and scalable way. </p>

<p>I expect this is a common need so ideally I'd use a library which reads the full XML structure shown at the beginning, and automatically extracts the information to insert into the normalized tables.</p>

<p>Thanks so much for any suggestion.</p>

<p>Mauro</p>",3,1,2019-03-27 15:56:10.570000 UTC,2.0,,0,python|xml|pyspark|apache-spark-sql|azure-databricks,691,2019-03-27 15:20:30.127000 UTC,2022-02-17 14:45:42.693000 UTC,,21,0,0,10,,,,,,[]
How do I load data from azure databricks to an premise SQL Database tables?,"<p>First of all I have created an azure databricks notebook and cluster and able to access the data from the datalake.</p>
<p>Now I have to load this data into on premise SQL database tables.how can I load the data from azure databricks to one premise SQL.</p>
<p>I have seen there are spark connectors but not able to load it.</p>
<p>P.S. My destination is not Azure SQL database,it is on premise SQL database</p>",1,0,2020-09-25 13:07:51.867000 UTC,,,0,sql-server|azure-databricks,589,2019-10-07 17:51:59.957000 UTC,2021-07-07 17:22:04.543000 UTC,"Hyderabad, Telangana, India",3,0,0,3,,,,,,[]
I am trying to connect to abfss directly(without mounting to DBFS) and trying to open json file using open() in databricks,"<p>I am trying to connect to abfss directly(without mounting to DBFS) and trying to open json file using open() method in databricks.</p>
<p>json_file = open(&quot;abfss://@.dfs.core.windows.net/test.json') databricks is unable to open file present in azure blob container and getting below error :
FileNotFoundError: [Errno 2] No such file or directory: 'abfss://@.dfs.core.windows.net/test.json'</p>
<p>I have done all the configuration setting using service principal. Please suggest other way of opening file using abfss direct path.</p>",1,0,2021-04-09 15:30:10.363000 UTC,,,1,azure|pyspark|azure-blob-storage|azure-databricks|open-json,340,2021-04-09 15:24:06.157000 UTC,2021-05-18 08:21:46.210000 UTC,,11,0,0,0,,,,,,[]
Connect to Azure SQL Database from DataBricks using Service Principal,"<p>I have a requirement to connect to Azure SQL Database from Azure Databricks via Service Principal. Tried searching forums but unable to find the right approach. Any help is greatly appreciated.</p>

<p>Tried a similar approach with SQL User ID and Password with JDBC Connection and it worked successfully. Now looking into Service Principal approach.</p>

<p>P.S: The SP ID and Key should be placed in the Azure Key Vault and needs to be accessed here on Databricks.</p>",4,0,2019-04-02 16:39:17.703000 UTC,1.0,,3,azure|azure-sql-database|azure-databricks|service-principal,2756,2012-06-26 14:50:13.677000 UTC,2022-03-01 15:52:37.257000 UTC,,341,31,0,71,,,,,,[]
Disconnect Virtual environment from devops in VS CODE,"<p>I created an environment in vs code but I can't install any python packages on the environment because vscode is looking up on indexes in pkgs.dev.azure</p>
<pre><code>(dbconnect) PS C:\Users\test\OneDrive - \Desktop\dbconnect&gt; pip install -U &quot;databricks-connect==9.1.*&quot;
Looking in indexes: https://pkgs.dev.azure.com/mycloud-platform/_packaging/package-test-feed/pypi/simple/
User for pkgs.dev.azure.com: test@123.com
Password:
ERROR: Could not find a version that satisfies the requirement databricks-connect==9.1.* (from versions: none)
ERROR: No matching distribution found for databricks-connect==9.1.*
</code></pre>
<p>I want to disconnect this environment from devops.</p>",0,2,2022-01-04 09:55:33.837000 UTC,,2022-01-04 11:37:42.567000 UTC,1,python|azure|azure-devops|cloud|databricks-connect,79,2020-08-20 19:48:46.470000 UTC,2022-03-04 13:38:08.327000 UTC,Germany,13,0,0,7,,,,,,[]
"Using Git or Mercurial, how would you know when you do a clone or a pull, no one is checking in files (pushing it)?","<p>Using Git or Mercurial, how would you know when you do a clone or a pull, no one is checking in files (pushing it)?  It can be important that:</p>

<p>1) You never know it is in an inconsistent state, so you try for 2 hours trying to debug the code when your code is in a inconsistent state.</p>

<p>2) With all the framework code (such as Ruby on Rails) -- potentially hundreds of files -- if some files are inconsistent with the other, can't the <code>rake db:migrate</code> or <code>script/generate controller</code> cause some damage or inconsistencies to the code base?</p>",3,2,2010-06-05 13:26:11.373000 UTC,1.0,2010-08-16 20:20:18.453000 UTC,6,git|frameworks|mercurial|dvcs,283,2009-05-09 15:50:29.477000 UTC,2022-03-04 09:41:10.460000 UTC,,137341,1445,39,12817,,,,,,[]
How to run .Net spark jobs on Databricks from Azure Data Factory?,"<p>In Azure data factory, you have a Databricks Acvitiy. This activity supports running python, jar and notebooks. And These notebooks may be written in scala, python, java, and R but not c#/.net.</p>
<p>Is there inherent or direct support where I can write my .NET spark code and run it on Databricks from Data Factory?</p>
<p>Can I use .NET spark in Azure Databricks to its full extent?</p>",1,2,2020-08-05 08:06:59.007000 UTC,1.0,,3,c#|azure-data-factory|azure-databricks|.net-spark,311,2020-08-05 08:03:26.057000 UTC,2020-08-30 18:21:35.713000 UTC,,33,0,0,3,,,,,,[]
How to integrate DVCS in a python application,"<p>Hi I have a simple pyQt text editor, </p>

<p>Essentially I want to add mercurial support</p>

<p>I have seen in various other editors the ability to support a number of DVCS (Mercurial, GIT,Bazaar, etc), and they give the user the ability to perform functions like commit,update, etc</p>

<p>I really want to know what/how I can integrate mercurial in my pyQt text editor, so that it behaves more or less like other fancy Editors.</p>

<p>Any good tutorials/guides on how to get this done</p>",1,0,2016-10-19 17:18:48.737000 UTC,,,0,python|windows|mercurial|pyqt4|dvcs,50,2011-01-30 18:17:32.290000 UTC,2017-08-30 22:24:42.673000 UTC,,1483,7,0,289,,,,,,[]
How to send email Notification with Suject in Azure Databricks using Databricks CLI?,"<p>I have created Python script to send email notification as below</p>
<pre><code>&quot;email_notifications&quot;:{
           'on_failure':''
                  }
</code></pre>
<p>How I can include this in my subject?</p>",1,0,2021-05-19 08:13:48.593000 UTC,,2021-05-19 12:28:03.887000 UTC,1,azure-databricks,134,2020-05-24 10:04:28.790000 UTC,2022-03-04 11:06:39.240000 UTC,"Bangalore, Karnataka, India",21,0,0,0,,,,,,[]
Does it make sense to use a distributed VCS (DVCS) to work on research code?,"<p>I work in a small research team, and we have been debating whether is it worth it to use Git – or any other DVCS for that matter – to control our research code. The thing is, most of the research is developed in jupyter notebooks, and 99% of the time every member is working on its own code, which is never modified by other colleagues. With that in mind, does it make sense to account for all the Git “bureaucracy” involved in collaborative working (pulling, pushing, merging, etc.), or should we just have a centralized development environment?</p>",1,4,2020-06-19 14:36:07.417000 UTC,1.0,,0,git|version-control|jupyter-notebook|dvcs,37,2015-05-23 03:08:52.760000 UTC,2022-03-03 12:18:27.103000 UTC,,99,117,0,32,,,,,,[]
Azure data bricks spark streaming with autoloader,"<p>My source is azure datafactory which is copying files to <code>containerA --&gt; FolderA,FolderB, FolderC</code>. I am using below syntax to use the autoloader need to read the files as it comes to any one of the folder.</p>
<p>Mounting I have done till storage account</p>
<pre class=""lang-py prettyprint-override""><code>    source = &quot;abfss://containerA@storageaccount.dfs.core.windows.net/&quot;,
    mount_point = &quot;/mnt/containerA/&quot;,
    extra_configs = configs)
</code></pre>
<p>Streaming code:</p>
<pre class=""lang-py prettyprint-override""><code>df1=spark.readStream.format(&quot;cloudFiles&quot;) \
  .option(&quot;cloudFiles.format&quot;,&quot;Json&quot;) \
  .option(&quot;cloudFiles.useNotifications&quot;,&quot;True&quot;) \
  .option('cloudFiles.subscriptionId',&quot;xxxx-xxxx-xxxx-xxxx-xxx&quot;) \
  .option('cloudFiles.tenantId',&quot;xxxx-1cxxxx98-xxxx-xxxx-xxxx&quot;) \
  .option(&quot;cloudFiles.clientId&quot;,&quot;xxxx-xx-46d8-xx-xxx&quot;) \
  .option(&quot;cloudFiles.clientSecret&quot;,&quot;xxxxxxxxxx&quot;) \
  .option('cloudFiles.resourceGroup',&quot;xxxx-xxx&quot;) \
  .schema(Userdefineschema) \
  .load(&quot;/mnt/containerA/&quot;) \
  .withColumn(&quot;rawFilePath&quot;,input_file_name())
</code></pre>
<p>Above syntax is creating new queue always is there any way if I wanted to give name to the queue.</p>
<p>Issue when I am starting my stream and adf is copy data to folder A streaming is running fine. but when adf starts copy data to folder B streaming query is not fetchING records which is present in folder B in the same streaming session. But when I close the streaming cell and again start it will pick data for folder A and Folder B. My objective is to use autoloader when files comes in any of the folder stream starts automatically.</p>
<p>Kindly advice I am new to spark streaming.</p>
<p>Thanks Anuj gupta</p>",0,0,2021-12-20 11:29:37.333000 UTC,,2021-12-20 13:09:43.197000 UTC,0,pyspark|streaming|spark-streaming|azure-data-factory-2|azure-databricks,116,2020-05-08 07:52:45.147000 UTC,2022-02-02 07:13:06.297000 UTC,,84,5,0,85,,,,,,[]
Force pushing to new head when already pulled all changes,"<p>Is it possible to forcibly create new remote head when pushing ?</p>

<p>Suppose I have done some local commits on branch ""default"" then pulled and merged from remote.</p>

<p>Now, I would like to push my commits to remote creating new head and a bookmark but preserve existing remote head and tip - ie. my coworkers should not get my changes yet when doing <code>hg fetch</code>.</p>

<p>Basically this should be a short lived branch (thus not a named branch) for purpose of backup and code review by other before being fully merged into ""main"" head of default branch.</p>

<p>I've tried <code>--new-branch</code> but it didn't help - no new head was created and remote tip moved to my head.</p>",3,0,2012-02-13 17:05:30.947000 UTC,,2012-03-30 12:12:05.073000 UTC,4,mercurial|branch|push|dvcs,15768,2010-09-09 12:54:25.090000 UTC,2022-02-28 22:22:53.890000 UTC,"New York, NY, USA",4047,62,9,206,,,,,,[]
DevOps for Azure Databricks Jobs,"<p>I am trying to implement DevOps on Azure Databricks.</p>
<p>I have completed devops implementation for databricks notebooks and dbfs files.</p>
<p>I do have many databricks jobs running on my cluster based on schedule.
Some of these jobs points to notebook files and few points to jar file in the dbfs location.</p>
<p>Is there any way to implement devops process on the azure databricks jobs so that any change in any of the jobs in DEV will invoke build pipeline and deploy the same in PROD databricks instance.</p>
<p>First of all I wanted to know whether it is possible to implement devops on azure databricks jobs.</p>
<p>Any Leads Appreciated!</p>",1,2,2020-11-27 11:18:56.863000 UTC,,2021-02-08 06:50:50.753000 UTC,2,azure|azure-devops|azure-databricks,249,2017-04-04 05:54:01.927000 UTC,2022-03-05 05:59:02.213000 UTC,,743,45,1,284,,,,,,[]
Spark: executor heartbeat timed out,"<p>I am working in a databricks cluster that have <code>240GB</code> of memory and 64 cores. This the settings I defined.</p>

<pre><code>import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.types import *
import pyspark.sql.functions as fs
from pyspark.sql import SQLContext
from pyspark import SparkContext
from pyspark.sql.functions import count
from pyspark.sql.functions import col, countDistinct
from pyspark import SparkContext
from geospark.utils import GeoSparkKryoRegistrator, KryoSerializer
from geospark.register import upload_jars
from geospark.register import GeoSparkRegistrator
spark.conf.set(""spark.sql.shuffle.partitions"", 1000)
#Recommended settings for using GeoSpark
spark.conf.set(""spark.driver.memory"", ""20g"")
spark.conf.set(""spark.network.timeout"", ""1000s"")
spark.conf.set(""spark.driver.maxResultSize"", ""10g"")
spark.conf.set(""spark.serializer"", KryoSerializer.getName)
spark.conf.set(""spark.kryo.registrator"", GeoSparkKryoRegistrator.getName)
upload_jars()
SparkContext.setSystemProperty(""geospark.global.charset"",""utf8"")
spark.conf.set
</code></pre>

<p>I am working with large datasets and this is the error I get after hours of running.</p>

<pre><code>org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 10.0 failed 4 times, most recent failure: Lost task 3.3 in stage 10.0 (TID 6054, 10.17.21.12, executor 7): 

ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 170684 ms
</code></pre>",3,0,2020-03-31 06:53:37.530000 UTC,,,2,apache-spark|pyspark|azure-databricks,508,2014-04-30 16:00:53.307000 UTC,2022-03-03 14:37:36.293000 UTC,"Boston, MA",5561,98,7,794,,,,,,[]
Pass Typesafe config file to the Spark Submit Job in Azure Databricks,"<p>I am trying to pass a Typesafe config file to the spark submit task and print the details in the config file.</p>
<pre><code>import org.slf4j.{Logger, LoggerFactory}
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.sql.SparkSession
 
object Bootstrap extends MyLogging {
 
 val spark: SparkSession = SparkSession.builder.enableHiveSupport().getOrCreate()
 val config: Config = ConfigFactory.load(&quot;application.conf&quot;)
 
 def main(args: Array[String]): Unit = {
  val url: String = config.getString(&quot;db.url&quot;)
  val user: String = config.getString(&quot;db.user&quot;)
  println(url)
  println(user)
 }
}
</code></pre>
<p>application.conf file :</p>
<pre><code>db {
  url = &quot;jdbc:postgresql://localhost:5432/test&quot;
  user = &quot;test&quot;
}
</code></pre>
<p>I have uploaded the application.conf file to the dbfs and using the same path to create the job.</p>
<p>Spark submit job JSON :</p>
<pre><code>{
  &quot;new_cluster&quot;: {
    &quot;spark_version&quot;: &quot;6.4.x-esr-scala2.11&quot;,
    &quot;azure_attributes&quot;: {
      &quot;availability&quot;: &quot;ON_DEMAND_AZURE&quot;,
      &quot;first_on_demand&quot;: 1,
      &quot;spot_bid_max_price&quot;: -1
    },
    &quot;node_type_id&quot;: &quot;Standard_DS3_v2&quot;,
    &quot;enable_elastic_disk&quot;: true,
    &quot;num_workers&quot;: 1
  },
  &quot;spark_submit_task&quot;: {
    &quot;parameters&quot;: [
      &quot;--class&quot;,
      &quot;Bootstrap&quot;,
      &quot;--conf&quot;,
      &quot;spark.driver.extraClassPath=dbfs:/tmp/&quot;,
      &quot;--conf&quot;,
      &quot;spark.executor.extraClassPath=dbfs:/tmp/&quot;,
      &quot;--files&quot;,
      &quot;dbfs:/tmp/application.conf&quot;,
      &quot;dbfs:/tmp/code-assembly-0.1.0.jar&quot;
    ]
  },
  &quot;email_notifications&quot;: {},
  &quot;name&quot;: &quot;application-conf-test&quot;,
  &quot;max_concurrent_runs&quot;: 1
}
</code></pre>
<p>I have used above json to create the spark submit job and tried to run the spark-submit job using datbricks CLI commands.</p>
<p>Error :</p>
<pre><code>Exception in thread &quot;main&quot; com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'db'
    at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:124)
    at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:147)
    at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:159)
    at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:164)
    at com.typesafe.config.impl.SimpleConfig.getString(SimpleConfig.java:206)
    at Bootstrap$.main(Test.scala:16)
    at Bootstrap.main(Test.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)
</code></pre>
<p>I can see the below line in logs but the file is not getting loaded.</p>
<pre><code>21/09/22 07:21:43 INFO SparkContext: Added file dbfs:/tmp/application.conf at dbfs:/tmp/application.conf with timestamp 1632295303654
21/09/22 07:21:43 INFO Utils: Fetching dbfs:/tmp/application.conf to /local_disk0/spark-20456b30-fddd-42d7-9b23-9e4c0d3c91cd/userFiles-ee199161-6f48-4c47-b1c7-763ce7c0895f/fetchFileTemp4713981355306806616.tmp
</code></pre>
<p>Please help me in passing this typesafe config file to the spark-submit job using the appropriate spark submit job parameters.</p>
<p>We have tried below spark_submit_task parameters in the above json but still facing the same issue</p>
<pre><code>[
  &quot;--class&quot;,
  &quot;Bootstrap&quot;,
  &quot;--conf&quot;,
  &quot;spark.driver.extraClassPath=/tmp/application.conf&quot;,
  &quot;--files&quot;,
  &quot;dbfs:/tmp/application.conf&quot;,
  &quot;dbfs:/tmp/code-assembly-0.1.0.jar&quot;
]
</code></pre>
<pre><code>[
  &quot;--class&quot;,
  &quot;Bootstrap&quot;,
  &quot;--conf&quot;,
  &quot;spark.driver.extraClassPath=/tmp/&quot;,
  &quot;--conf&quot;,
  &quot;spark.executor.extraClassPath=/tmp/&quot;,
  &quot;--files&quot;,
  &quot;dbfs:/tmp/application.conf&quot;,
  &quot;dbfs:/tmp/code-assembly-0.1.0.jar&quot;
]
</code></pre>
<pre><code>[
  &quot;--class&quot;,
  &quot;Bootstrap&quot;,
  &quot;--conf&quot;,
  &quot;spark.driver.extraClassPath=dbfs:/tmp/application.conf&quot;,
  &quot;--conf&quot;,
  &quot;spark.executor.extraClassPath=dbfs:/tmp/application.conf&quot;,
  &quot;--files&quot;,
  &quot;dbfs:/tmp/application.conf&quot;,
  &quot;dbfs:/tmp/code-assembly-0.1.0.jar&quot;
]
</code></pre>
<pre><code>[
  &quot;--class&quot;,
  &quot;Bootstrap&quot;,
  &quot;--conf&quot;,
  &quot;spark.driver.extraClassPath=dbfs:/tmp/&quot;,
  &quot;--conf&quot;,
  &quot;spark.executor.extraClassPath=dbfs:/tmp/&quot;,
  &quot;--files&quot;,
  &quot;dbfs:/tmp/application.conf&quot;,
  &quot;dbfs:/tmp/code-assembly-0.1.0.jar&quot;
]
</code></pre>
<pre><code>[
  &quot;--class&quot;,
  &quot;Bootstrap&quot;,
  &quot;--conf&quot;,
  &quot;spark.driver.extraClassPath=dbfs:./&quot;,
  &quot;--conf&quot;,
  &quot;spark.executor.extraClassPath=dbfs:./&quot;,
  &quot;--files&quot;,
  &quot;dbfs:/tmp/application.conf&quot;,
  &quot;dbfs:/tmp/code-assembly-0.1.0.jar&quot;
]
</code></pre>
<pre><code>[
  &quot;--class&quot;,
  &quot;Bootstrap&quot;,
  &quot;--driver-java-options&quot;,
  &quot;-Dconfig.file=application.conf&quot;,
  &quot;--conf&quot;,
  &quot;spark.executor.extraJavaOptions=-Dconfig.file=application.conf&quot;,
  &quot;--files&quot;,
  &quot;dbfs:/tmp/application.conf&quot;,
  &quot;dbfs:/tmp/code-assembly-0.1.0.jar&quot;
]

</code></pre>
<pre><code>[
  &quot;--class&quot;,
  &quot;Bootstrap&quot;,
  &quot;--conf&quot;,
  &quot;spark.driver.extraJavaOptions=-Dconfig.file=application.conf&quot;,
  &quot;--conf&quot;,
  &quot;spark.executor.extraJavaOptions=-Dconfig.file=application.conf&quot;,
  &quot;--files&quot;,
  &quot;dbfs:/tmp/application.conf&quot;,
  &quot;dbfs:/tmp/code-assembly-0.1.0.jar&quot;
]
</code></pre>",1,0,2021-10-27 14:06:55.860000 UTC,,,1,azure|apache-spark|azure-databricks|spark-submit|typesafe-config,62,2020-05-19 07:51:53.797000 UTC,2021-12-01 12:54:29.063000 UTC,,21,0,0,3,,,,,,[]
Calling Python function on a column in DataFrame,"<p>I got below function to convert string to uppercase</p>

<pre><code>def changecase(col):
  return col.upper()
</code></pre>

<p>Trying to applying a column in dataframe throw error:</p>

<pre><code>df['Description'] = df['Description'].apply(changecase)
</code></pre>

<p><strong>Error:</strong></p>

<pre><code>TypeError: 'Column' object is not callable
</code></pre>

<p><a href=""https://i.stack.imgur.com/XBITr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XBITr.png"" alt=""enter image description here""></a></p>",1,8,2020-05-14 12:08:59.050000 UTC,,2020-05-14 12:48:51.373000 UTC,0,python-3.x|dataframe|azure-databricks,36,2009-04-02 00:53:40.477000 UTC,2022-03-01 22:30:30.100000 UTC,"Brisbane QLD, Australia",27501,66,1,1101,,,,,,[]
"Bigquery ""Getting IllegalArgumentException: Invalid Table ID"" error while reading part of bigquery table into pyspark dataframe using query","<p>I am trying to read a section of bigquery table using query in azure databricks spark.</p>
<pre><code>table_id = str(project_id) + &quot;.&quot; + str(schema) + &quot;.&quot; + str(table_name)
</code></pre>
<p>I am able to read complete table data using the following query.</p>
<pre><code> _data = spark.read.format(&quot;bigquery&quot;).option(&quot;credentials&quot;, ans). \
     option(&quot;parentProject&quot;, project_id). \
     option(&quot;project&quot;, project_id). \
     option(&quot;table&quot;, table_id). \
     option(&quot;dataset&quot;, schema).load()
</code></pre>
<p>But when i try to do the same using sql query in the following way,</p>
<pre><code>_query = &quot;&quot;&quot;select * from `{}` limit 2&quot;&quot;&quot;.format(table_id)
_data = spark.read.format(&quot;bigquery&quot;).option(&quot;credentials&quot;, ans). \
    option(&quot;parentProject&quot;, project_id). \
    option(&quot;project&quot;, project_id). \
    option(&quot;dataset&quot;, schema). \
    load(_query)

total = _data.count()
</code></pre>
<blockquote>
<p>IllegalArgumentException: Invalid Table ID 'select
col1 from `proj-164408.schema.mytable` limit 2'.
Must match '^(((\S+)[:.])?(\w+).)?([\S&amp;&amp;[^.:]]+)$$'</p>
</blockquote>
<p>I tried with differnt types of table ids like
<strong>proj-164408:schema.mytable</strong> , <strong>proj-164408:schema:mytable</strong></p>
<p>Attaching the stacktrace information.</p>
<pre><code>---------------------------------------------------------------------------
IllegalArgumentException                  Traceback (most recent call last)
&lt;command-755248569207678&gt; in &lt;module&gt;
     88     option(&quot;parentProject&quot;, project_id). \
     89     option(&quot;project&quot;, project_id).option(&quot;dataset&quot;, schema). \
---&gt; 90     load(_query)
     91 
     92 total = _data.count()

/databricks/spark/python/pyspark/sql/readwriter.py in load(self, path, format, schema, **options)
    176         self.options(**options)
    177         if isinstance(path, basestring):
--&gt; 178             return self._df(self._jreader.load(path))
    179         elif path is not None:
    180             if type(path) != list:

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-&gt; 1305             answer, self.gateway_client, self.target_id, self.name)
   1306 
   1307         for temp_arg in temp_args:

/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
    131                 # Hide where the exception came from that shows a non-Pythonic
    132                 # JVM exception message.
--&gt; 133                 raise_from(converted)
    134             else:
    135                 raise

/databricks/spark/python/pyspark/sql/utils.py in raise_from(e)
</code></pre>
<p><em>Databricks Run time version that i used: 7.3 LTS (includes Apache Spark 3.0.1, Scala 2.12)</em></p>
<p>There is already a question which talks about similar issue but could not apply that to my case.
Refer <a href=""https://stackoverflow.com/questions/19102896/bigquery-invalid-table-id"">BigQuery: Invalid table ID</a></p>
<p><strong>Update 1:</strong></p>
<p>Found one link for source code which throws the mentioned error.
<a href=""https://github.com/GoogleCloudDataproc/spark-bigquery-connector/blob/master/connector/src/main/java/com/google/cloud/bigquery/connector/common/BigQueryUtil.java"" rel=""nofollow noreferrer"">com.google.cloud.bigquery.connector.common.BigQueryUtil</a> Line number: 106</p>
<p>It seems we can only give fully qualified table name,</p>
<pre class=""lang-java prettyprint-override""><code>      private static final String PROJECT_PATTERN = &quot;\\S+&quot;;
      private static final String DATASET_PATTERN = &quot;\\w+&quot;;
      // Allow all non-whitespace beside ':' and '.'.
      // These confuse the qualified table parsing.
      private static final String TABLE_PATTERN = &quot;[\\S&amp;&amp;[^.:]]+&quot;;
      /**
       * Regex for an optionally fully qualified table.
       *
       * &lt;p&gt;Must match 'project.dataset.table' OR the legacy 'project:dataset.table' OR 'dataset.table'
       * OR 'table'.
       */
      private static final Pattern QUALIFIED_TABLE_REGEX =
          Pattern.compile(
              format(&quot;^(((%s)[:.])?(%s)\\.)?(%s)$$&quot;, PROJECT_PATTERN, DATASET_PATTERN, TABLE_PATTERN));

    Matcher matcher = QUALIFIED_TABLE_REGEX.matcher(rawTable);
    if (!matcher.matches()) {
      throw new IllegalArgumentException(
          format(&quot;Invalid Table ID '%s'. Must match '%s'&quot;, rawTable, QUALIFIED_TABLE_REGEX));

</code></pre>
<p>Since i gave SQL query instead of fq table name, i am getting mentioned error.</p>",1,4,2021-06-20 19:28:25.170000 UTC,,2021-06-22 08:11:19.853000 UTC,0,apache-spark|pyspark|google-bigquery|azure-databricks,408,2014-01-19 09:22:43.180000 UTC,2022-03-04 16:37:09.917000 UTC,,402,1394,2,86,,,,,,[]
Merge results from multiple queries in Gremlin,"<p>Let's say I want to get a few vertices from my database:</p>

<pre><code>g.V(1, 2, 3)
</code></pre>

<p>And then I have another set of vertices:</p>

<pre><code>g.V(4, 5, 6)
</code></pre>

<p>Imagine it's not just <code>g.V()</code>, but some more complicated traversal to get my vertices. But the traversal must start with <code>V()</code>, because I wanna pick from all of my nodes.</p>

<p>Let's also assume I wanna do this multiple times. So I might want to merge 7 different result sets. <strong>Each one can have completely different way to get its results.</strong></p>

<hr>

<p>Now I want to merge these two results into one result set. My first thought was this:</p>

<pre><code>g.V(1, 2, 3).fold().as('x').V(4, 5, 6).fold().as('x').select(all, 'x').unfold()
</code></pre>

<p>But this doesn't work. The second call to <code>fold</code> will clear out my ""local variables"", because it's a barrier step.</p>

<p>My current attempt is this:</p>

<pre><code>g.V(1, 2, 3).fold().union(identity(), V(4, 5, 6).fold()).unfold()
</code></pre>

<p>This works, but looks a bit too complicated. If I wanna repeat it 7 times, it'll make for a very complex query.</p>

<p><strong>Is there a better way to accomplish simple merging of results from two different queries?</strong></p>",2,0,2020-01-28 09:25:41.137000 UTC,1.0,,5,gremlin|tinkerpop|tinkerpop3|amazon-neptune|gremlinpython,1660,2010-03-29 14:47:38.683000 UTC,2021-10-27 18:26:52.203000 UTC,"Kosice, Slovakia",6794,130,33,320,,,,,,[]
how to explode a structs array,"<p>I have a schema like this</p>

<pre><code>root
 |-- CaseNumber: string (nullable = true)
 |-- Interactions: struct (nullable = true)
 |    |-- EmailInteractions: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |    |-- PhoneInteractions: array (nullable = true)
 |    |    |-- element: struct (containsNull = true)
 |    |    |    |-- Contact: struct (nullable = true)
 |    |    |    |    |-- id: string (nullable = true)
 |    |    |    |-- CreatedOn: string (nullable = true)
 |    |    |    |-- Direction: string (nullable = true)
 |    |-- WebInteractions: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
</code></pre>

<p>and I would like to explode the three arrays(EmailInteractions,PhoneInteractions,WebInteractions) and group with CaseNumber and create three tables and  execute this sql query</p>

<pre><code>Select casenumber,CreatedOn
from EmailInteration
where Direction = 'Outgoing'

union all

Select casenumber,CreatedOn
from PhoneInteraction
where Direction = 'Outgoing'

union all

Select casenumber,CreatedOn
from WebInteraction
where Direction = 'Outgoing'
</code></pre>

<p>the code to retrieve the schema</p>

<pre><code>val dl = spark.read.format(""com.databricks.spark.avro"").load(""adl://power.azuredatalakestore.net/Se/eventhubspace/eventhub/0_2020_01_20_*_*_*.avro"")
val dl1=dl.select($""body"".cast(""string"")).map(_.toString())
val dl2=spark.read.json(dl1)
val dl3=dl2.select($""Content.*"",$""RawProperties.UserProperties.*"")
</code></pre>

<p>I am new to databricks, any help would be appreciated. thanks in advance.</p>",0,2,2020-01-22 22:21:27.953000 UTC,,2020-01-23 00:09:31.093000 UTC,0,struct|apache-spark-sql|azure-databricks,105,2019-02-18 20:39:58.007000 UTC,2021-11-10 05:52:10.410000 UTC,,125,5,0,58,,,,,,[]
Is it possible to use Writestream directly to an API via spark,"<p>I build a code on Databricks to read a delta table in realtime (readstream) and then i need post this stream data to an API.<br>
In all paper that I read, writestream is used only to create files (.csv, .avro, .parquet, etc) or sent to an Event Hub. Is possible to use writestream to post to an API!?</p>

<p>My code:</p>

<pre><code>from pyspark.sql.functions import unix_timestamp, round, col
import json
import pandas as pd
from pyspark.sql.functions import lit
import requests

#tried with foreach_batch but it doens't work
def foreach_batch_function(df,epochId):
    r2 = requests.post('https://demo.api.com/index.php/api/v5/smsrequest/', data=str(df), verify=False)
    r2.json()
    pass

rs = spark.readStream.format(""delta"").option('path','/mnt/gen2/raw/mytable').load()
df = rs.select(round('id_cliente_fat').alias('id_cliente_fat'),'fone_fat','nome_fat',unix_timestamp('dt_nasc_fat','YYYY-MM-DD').cast('timestamp').cast('date').alias('birth_date'),'email_fat')

df2 = df.selectExpr('id_cliente_fat as identifier_code','fone_fat as phone_number','nome_fat as name','birth_date','email_fat as email')

data = {'authentication':{'username':'user','password':'pass'}}
r = requests.post('https://demo.api.com/index.php/api/v5/login/', data=json.dumps(data), verify=False).json()

df3 = df2.withColumn(""steps"", lit(""[1,2,4,7]"")).withColumn(""place_id"", lit(164)).withColumn(""token"", lit(r[""authentication""][""token""]))

df4 = df3.select(to_json(struct(struct(""token"").alias(""authentication""), struct(""identifier_code"", ""phone_number"", ""name"", ""birth_date"", ""email"",""steps"",""place_id"").alias(""smsrequest"").alias(""smsrequest""))).alias(""""))

df4.writeStream.foreachBatch(foreach_batch_function).start() 

</code></pre>",1,0,2020-02-07 10:54:20.080000 UTC,,2020-02-07 23:06:51.830000 UTC,-1,apache-spark|pyspark|spark-streaming|azure-databricks,81,2020-02-07 10:44:56.323000 UTC,2021-05-04 12:32:27.187000 UTC,,1,0,0,0,,,,,,[]
SparkException: Job aborted due to stage failure. Caused by: java.lang.ArrayIndexOutOfBoundsException,"<p>I am trying to read some data from parquet file using spark SQL and trying to put that data into some other table. But while writing data into another table I am getting the below error.</p>
<p>The parquet file is pulled from <code>event-hub</code> data. In the data I have one column of type Array of object e.g:</p>
<pre><code>[{MeassageTyep:string, Data:{Liftlink:int,MotionSensorLink:int}}]
</code></pre>
<p>This is how I have written the code to read the parquet file:</p>
<pre><code>try:
  spark.sql(&quot;&quot;&quot;DROP TABLE IF EXISTS stg_robustel.src_robustel_heartbeat&quot;&quot;&quot;)
  spark.sql(&quot;&quot;&quot;
CREATE TABLE IF NOT EXISTS stg_robustel.src_robustel_heartbeat
USING PARQUET
LOCATION '/mnt/RobustelLanding/OtisOne/robustel/heartbeat/2020/06/30/16/-1761854110_24d31143a42f4b6f9cec9aa576e2ddac_1.parquet' &quot;&quot;&quot;) 
except Exception as e:
  print('Error: ' + str(e))
</code></pre>
<p>And this is how I am trying to put the above data into another simple table</p>
<pre class=""lang-sql prettyprint-override""><code>%sql
drop table if exists stg_robustel.src_robustel_heartbeat_test;
create table stg_robustel.src_robustel_heartbeat_test as
select * from stg_robustel.src_robustel_heartbeat    
</code></pre>
<blockquote>
<p>Error in SQL statement: SparkException: Job aborted.
com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.SparkException: Job aborted.
at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:201)
at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:192)
at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:558)
at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:216)
at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:175)
at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:119)
at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:206)
at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:206)
at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3492)
at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3487)
at org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:112)
at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:241)
at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:98)
at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:171)
at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3487)
at org.apache.spark.sql.Dataset.(Dataset.scala:206)
at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:90)
at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:696)
at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:716)
at com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:88)
at com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:34)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.immutable.List.foreach(List.scala:392)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
at scala.collection.immutable.List.map(List.scala:296)
at com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:34)
at com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:141)
at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:385)
at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:362)
at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:251)
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:246)
at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)
at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:288)
at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)
at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:362)
at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)
at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)
at scala.util.Try$.apply(Try.scala:192)
at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)
at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)
at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)
at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)
at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)
at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)
at java.lang.Thread.run(Thread.java:748)
<em><strong>Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 10, 10.139.64.5, executor 0): org.apache.spark.SparkException: Task failed while writing rows.</strong></em>
at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:268)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:172)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
at org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)
at org.apache.spark.scheduler.Task.run(Task.scala:113)
at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)
at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
<em><strong>Caused by: java.lang.ArrayIndexOutOfBoundsException</strong></em></p>
</blockquote>
<p>Help is appreciated</p>",1,0,2020-07-24 07:59:14.540000 UTC,,2021-01-24 15:05:57.520000 UTC,0,apache-spark|pyspark|apache-spark-sql|azure-databricks,3262,2017-04-03 14:11:55.320000 UTC,2021-07-16 10:53:40.700000 UTC,,139,1,0,9,,,,,,[]
Setting fts maxresults to 1 means pagination returns no results in aws neptune,"<p>I am trying to paginate which works for the most part, but it breaks because of the fts.maxResults (If it is set too small, it won't return anything). What's the reason behind this?</p>
<pre><code>limit = 1
offset = 1

# This returns nothing
g.withSideEffect(
    &quot;Neptune#fts.endpoint&quot;, f&quot;{url}&quot;
)
.withSideEffect(&quot;Neptune#fts.queryType&quot;, &quot;query_string&quot;)
.withSideEffect(&quot;Neptune#fts.maxResults&quot;, limit) # if i set this to limit+1, I get 1 result as expected
.withSideEffect(&quot;Neptune#enableResultCache&quot;, enable_cache)
.withSideEffect(&quot;Neptune#fts.sortOrder&quot;, &quot;DESC&quot;)
.V()
.hasLabel(&quot;table&quot;)
.has(
    &quot;*&quot;,
    f&quot;Neptune#fts entity_type:&quot;table&quot; AND ({query})&quot;,
)
.range(offset, limit + offset)
</code></pre>",1,0,2021-12-17 02:32:56.127000 UTC,,,0,amazon-neptune|gremlin-server,24,2019-06-30 00:43:58.337000 UTC,2022-03-04 03:03:33.920000 UTC,,159,11,0,18,,,,,,[]
Error in loading in Cosmos through Spark job due to Bulk Upsert,"<p>Job aborted due to stage failure: Task 8 in stage 9387.0 failed 4 times, most recent failure: Lost task 8.3 in stage 9387.0 (TID 715923, 10.0.132.79, executor 2):
Errors encountered in bulk import API execution.</p>
<p>Record size are way too less than 2MB.
Job run is from Data bricks notebook.</p>",0,4,2021-08-06 12:00:25.410000 UTC,,2021-08-09 05:19:56.603000 UTC,0,azure-cosmosdb|azure-databricks,33,2021-01-22 15:30:27.883000 UTC,2021-10-04 07:56:47.870000 UTC,,1,0,0,2,,,,,,[]
Unable to read data in Azure Databricks,"<p>I want to read data from azure sql database in azure databricks through jdbc driver. I am writing notebooks in python. But I am unable to read data:</p>

<pre><code>jdbcUrl = ""jdbc:sqlserver://{0}:{1};database={2};user={3};password={4}"".format(jdbcHostname, jdbcPort, jdbcDatabase, username, password)


df = spark.read.jdbc(url=jdbcUrl, table=""dbo.CUSTOMER_HANDLES"", column=""Bank Name"", lowerBo

und=1, upperBound=100000, numPartitions=100)
display(df)
</code></pre>

<p>It is showing the following error:</p>

<pre><code>AnalysisException: 'Partition column type should be numeric, date, or timestamp, but string found.;'
</code></pre>",0,3,2019-08-02 11:39:58.787000 UTC,,2019-08-02 16:42:40.130000 UTC,0,python|apache-spark|jdbc|azure-databricks,64,2019-06-06 12:01:42.137000 UTC,2021-07-09 14:14:02.727000 UTC,,31,6,0,16,,,,,,[]
How to add azure blob storage to datasource from databricks notebook?,"<p>I am trying to add an azure storage account to my datasource using the following code snippet from my databricks notebook:</p>
<pre><code>datasource_config = {
    &quot;name&quot;: &quot;my_azure_datasource&quot;,
    &quot;class_name&quot;: &quot;Datasource&quot;,
    &quot;execution_engine&quot;: {
        &quot;class_name&quot;: &quot;SparkDFExecutionEngine&quot;,
        &quot;azure_options&quot;: {
            &quot;account_url&quot;: &quot;&lt;YOUR_ACCOUNT_URL&gt;&quot;,
            &quot;credential&quot;: &quot;&lt;YOUR_CREDENTIAL&gt;&quot;,
        },
    },
    &quot;data_connectors&quot;: {
        &quot;default_inferred_data_connector_name&quot;: {
            &quot;class_name&quot;: &quot;InferredAssetAzureDataConnector&quot;,
            &quot;azure_options&quot;: {
                &quot;account_url&quot;: &quot;&lt;YOUR_ACCOUNT_URL&gt;&quot;,
                &quot;credential&quot;: &quot;&lt;YOUR_CREDENTIAL&gt;&quot;,
            },
            &quot;container&quot;: &quot;&lt;YOUR_CONTAINER&gt;&quot;,
            &quot;name_starts_with&quot;: &quot;&lt;CONTAINER_PATH_TO_DATA&gt;&quot;,
            &quot;default_regex&quot;: {
                &quot;pattern&quot;: &quot;(.*)\\.csv&quot;,
                &quot;group_names&quot;: [&quot;data_asset_name&quot;],
            },
        },
    },
}
</code></pre>
<p>But when I try to add this datasource by running <code>context.add_datasource(**datasource_config)</code>, I am getting the following error:</p>
<p><code>Unable to load Azure BlobServiceClient (it is required for InferredAssetAzureDataConnector).</code></p>
<p>I am sure I am providing the account_url and credential correctly. Because I can connect to the storage account if I use <code>BlobServiceClient</code> separately.</p>
<p>Please help.</p>",0,0,2021-09-20 09:21:53.130000 UTC,,,0,azure-databricks|great-expectations,56,2015-02-24 20:39:41.363000 UTC,2022-03-03 15:02:46.483000 UTC,"Dhaka, Bangladesh",174,15,0,42,,,,,,[]
Azure Databricks: ImportError: No module named azure.storage.blob,"<p>When using the sample code example.py(provided with Azure documentation: Quickstart: Upload, download, and list blobs with Python), I get the following import error. </p>

<p>Link to documentation: <a href=""https://github.com/Azure-Samples/storage-blobs-python-quickstart/blob/master/example.py"" rel=""nofollow noreferrer"">https://github.com/Azure-Samples/storage-blobs-python-quickstart/blob/master/example.py</a></p>

<pre>
ImportError: No module named azure.storage.blob
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
 in ()
      1 import os, uuid, sys
----> 2 from azure.storage.blob import BlockBlobService, PublicAccess
      3 
</pre>

<p>Please help me resolve this issue. </p>

<p>Since it is running in a notebook on Azure cloud, there is no python installation involved. So, please do not revert with suggestion that I should use a different version of python.</p>

<pre><code>import os, uuid, sys
from azure.storage.blob import BlockBlobService, PublicAccess

def run_sample():
    try:
        # Create the BlockBlockService that is used to call the Blob service for the storage account
        block_blob_service = BlockBlobService(account_name='accountname', account_key='accountkey')

        # Create a container called 'quickstartblobs'.
        container_name ='quickstartblobs'
        block_blob_service.create_container(container_name)

        # Set the permission so the blobs are public.
        block_blob_service.set_container_acl(container_name, public_access=PublicAccess.Container)

        # Create a file in Documents to test the upload and download.
        local_path=os.path.expanduser(""~/Documents"")
        local_file_name =""QuickStart_"" + str(uuid.uuid4()) + "".txt""
        full_path_to_file =os.path.join(local_path, local_file_name)

        # Write text to the file.
        file = open(full_path_to_file,  'w')
        file.write(""Hello, World!"")
        file.close()

        print(""Temp file = "" + full_path_to_file)
        print(""\nUploading to Blob storage as blob"" + local_file_name)

        # Upload the created file, use local_file_name for the blob name
        block_blob_service.create_blob_from_path(container_name, local_file_name, full_path_to_file)

        # List the blobs in the container
        print(""\nList blobs in the container"")
        generator = block_blob_service.list_blobs(container_name)
        for blob in generator:
            print(""\t Blob name: "" + blob.name)

        # Download the blob(s).
        # Add '_DOWNLOADED' as prefix to '.txt' so you can see both files in Documents.
        full_path_to_file2 = os.path.join(local_path, str.replace(local_file_name ,'.txt', '_DOWNLOADED.txt'))
        print(""\nDownloading blob to "" + full_path_to_file2)
        block_blob_service.get_blob_to_path(container_name, local_file_name, full_path_to_file2)

        sys.stdout.write(""Sample finished running. When you hit &lt;any key&gt;, the sample will be deleted and the sample ""
                         ""application will exit."")
        sys.stdout.flush()
        input()

        # Clean up resources. This includes the container and the temp files
        block_blob_service.delete_container(container_name)
        os.remove(full_path_to_file)
        os.remove(full_path_to_file2)
    except Exception as e:
        print(e)

if __name__ == '__main__':
    run_sample()
</code></pre>",2,1,2019-01-09 11:53:32.620000 UTC,,2019-01-09 12:11:22.717000 UTC,1,azure|azure-databricks,4582,2017-10-02 11:09:58.067000 UTC,2020-06-01 06:41:46.553000 UTC,,117,4,0,12,,,,,,[]
Edge data not fully loaded into Neptune (via Python and Gremlin),"<p>I've uploaded my gremlin-csv formatted data to S3 and eventually figured out how to  run the loader gremlin script via Python Jupyter from the terrible AWS documentation, but while the Node data is fully loaded into the graph, the Edge data is not, but there is no error.</p>

<pre><code>from gremlin_python import statics
from gremlin_python.structure.graph import Graph
from gremlin_python.process.graph_traversal import __
from gremlin_python.process.strategies import *
from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection

graph = Graph()
url = 'wss://&lt;&lt;cluster&gt;&gt;.&lt;&lt;region&gt;&gt;.neptune.amazonaws.com:&lt;&lt;port&gt;&gt;/gremlin'
remoteConn = DriverRemoteConnection(url,'g')
g = graph.traversal().withRemote(DriverRemoteConnection(url,'g'))
remoteConn.close()
</code></pre>

<p>That creates an empty graph object, so now I load my data into it using:</p>

<pre><code>import requests
import json

url=""https://&lt;&lt;cluster&gt;&gt;.&lt;&lt;region&gt;&gt;.neptune.amazonaws.com:&lt;&lt;port&gt;&gt;/loader""
data = {
    ""source"" : ""s3://neptune-data/neptuneEdgeData.csv"",
    ""format"" : ""csv"",
    ""iamRoleArn"" : ""arn:aws:iam::&lt;&lt;###&gt;&gt;:role/NeptuneLoadFromS3"",
    ""region"" : ""&lt;&lt;region&gt;&gt;"",
    ""failOnError"" : ""FALSE"",
    ""parallelism"" : ""MEDIUM""
}
headers={'Content-Type': 'application/json'}
res = requests.post(url, data=json.dumps(data), headers=headers)
</code></pre>

<p>And this works fine for the Node data, but the EdgeData file doesn't get completely loaded.  One reason may be that the EdgeData file is 455 Mb.  There is a vague and uninformative note on the AWS Neptune limits page:</p>

<blockquote>
  <p>The total size of Gremlin and SPARQL HTTP requests must be less than 150 MB. If a request exceeds this size, Neptune returns HTTP 400: BadRequestException.</p>
</blockquote>

<p>I didn't get a <code>BadRequestException</code> (even when I set <code>failOnError</code> to <code>TRUE</code>) but it may be only loading ~1/3 of the data because of a size/time limit.  </p>

<p>Does anybody actually know why this might be happening and how to get the full edge data loaded?</p>",1,0,2019-06-12 06:08:38.310000 UTC,,,1,import|filesize|amazon-neptune|upload-max-filesize,826,2015-06-12 08:08:57.353000 UTC,2022-03-03 11:24:01.580000 UTC,,1129,1390,16,213,,,,,,[]
"AWS Lambda trigger on S3 bucket to Neptune ""Failed to start new load for the source""","<p>I wrote a lambda function which will trigger python code when a create event happens in S3 and Python script is supposed to read files from S3 and post them to Neptune server.  </p>

<p>When I test that, I am getting the following error.  </p>

<pre><code>{
 ""requestId"":""xxxxxxxx-1234-5678-9012-xxxxxxxxxxxxx"",  
 ""code"":""ThrottlingException"",  
 ""detailedMessage"":""Failed to start new load for the source s3://my-s3-url/file.ttl. 
    Max concurrent load limit breached. Limit is 1""
}
</code></pre>

<p><strong>Code:</strong></p>

<pre><code>def lambda_handler(event, context):
    file_names = [""a.ttl"", ""b.ttl"", ""c.ttl""]
    source_url = ""s3://my-s3.aws.com/""
    role = ""my-role""
    neptune_url = ""https://my-neptune-server.aws.com/loader""
    headers = {""Content-Type"": ""application/json""}

    for name in file_names:
        file = source_url+name
        data = {""source"": file, ""iamRoleArn"": role, ""region"": ""region-1"", ""failOnError"": ""FALSE"", ""format"": ""turtle""}
        loop = asyncio.get_event_loop()
        task = loop.create_task(post_async(neptune_url, json.dumps(data), headers))
        resp = loop.run_until_complete(task)
        print(resp)

async def post_async(neptune_url, data, headers):
    async with aiohttp.ClientSession() as session:
        async with session.post(neptune_url, data=data, headers=headers) as response:
            result = await response.text()
            return result
</code></pre>

<p>I tried both Synchronous and Asynchronous ways.  I am getting limited documentation in the web.  Can some one point me right direction ?</p>",1,0,2020-01-21 07:34:37.213000 UTC,,,1,python|amazon-web-services|amazon-s3|aws-lambda|amazon-neptune,332,2011-03-21 13:02:18.430000 UTC,2022-03-04 12:25:32.030000 UTC,"Hyderabad, India",1791,284,4,593,,,,,,[]
How do i execute the ScalaTest testcases written in my Azure Databricks notebook from Azure Devops build/release pipeline,"<p>Below is a basic &quot;ScalaTest&quot; testcase I've written in my Azure Databricks Notebook. I would like to execute this testcase from Azure DevOps Build or release pipeline and view the test results.</p>
<p>I searched for resources but no luck(Reference to one of the forums which talks about the same issue: <a href=""https://social.msdn.microsoft.com/Forums/en-US/5d31ef5b-5b18-4305-ad09-0107e47891be/automated-testing-in-azure-databricks?forum=AzureDataFactory"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/en-US/5d31ef5b-5b18-4305-ad09-0107e47891be/automated-testing-in-azure-databricks?forum=AzureDataFactory</a>)</p>
<pre><code>import org.scalatest.FunSuite
class SampleTestSuite extends FunSuite 
{
test(&quot;1 equals 1&quot;) 
{
    assert(1 == 1)
  }
}
</code></pre>",2,1,2020-07-05 15:48:39.980000 UTC,,2020-07-06 06:40:59.303000 UTC,1,azure|azure-devops|scalatest|azure-databricks,462,2017-10-25 09:07:40.903000 UTC,2022-03-03 11:34:36.470000 UTC,"Chennai, Tamil Nadu, India",55,11,0,14,,,,,,[]
Merging with an ancestor of the current commit,"<p>How does merging in a previously-rejected idea <em>from the same branch</em>, work in various revision control systems? Which offer support for this?</p>

<p>Here is my educated guess - is this summary accurate?</p>

<p>Git and centralised VCSs: Presumably, you reverted the commit that you want to merge in. So you would be literally trying to merge in an ancestor of the current commit. I haven't tried this but I don't imagine it would be supported too well. I guess you could do a 3-way merge with the relevant ancestor (the metaphorical ""branch point"" in hindsight) of the commit to merge in (which you'd have to identify, because there'd be no record of it) - but would the VCS support this or would you have to do it yourself?</p>

<p>Mercurial, using bookmarks (or using anonymous branching in a similar way to bookmarks): If you had used this approach diligently, you would have ""reversed course"" and branched from an ancestor of the old commit at the point in time when you previously rejected it. Therefore the commit, even though it was originally done on this branch, now is not on this branch (because bookmarks are dynamic, not immutable, and because you reversed course), so there's no problem at all. Merge away!</p>

<p>Darcs, if there are no tags between head and the old commit: While in darcs it looks to the casual glance as if you have a straight line of commits, what you actually have is a dependency graph between the first commit or the most recent tag, and head (which is not at all obvious and AFAIK there is still no tool to visualise this!). So if darcs thinks your old commit was independent of the current head (which is presumably quite likely, because you've reverted the old commit), they are invisibly separate branches and can be merged quite happily. If darcs thinks the old commit is depended on by head, then this situation is presumably like the git case, though I don't think anyone fully understands darcs patch theory, except maybe David Roundy.</p>

<p>Darcs, if there has been a tag in the meantime: I think this would be like git, because a tag bundles all the invisible heads together and makes a single head that depends on all of them (this is possibly one of the easiest-to-understand aspects of darcs patch theory)</p>

<p>Footnote: For brevity I've referred to ""the old commit"" throughout, although the previously rejected idea could of course have been a long line of commits.</p>",1,0,2012-02-07 21:52:54.663000 UTC,,2012-02-07 22:39:49.027000 UTC,-5,git|mercurial|branch|dvcs|darcs,784,2010-11-03 10:39:52.433000 UTC,2022-03-05 12:00:32.180000 UTC,"London, United Kingdom",30484,6811,992,5857,,,,,,[]
Unable to create edges,"<p>I have created an AWS AppSync graphql api which on being called will run an AWS Lambda function and that function will create a vertex and edge using query language Gremlin but I am unable create edge after vertex is successfully created and AWS AppSync giving me this error  <strong>&quot;message&quot;: &quot;g.addE(...).from is not a function&quot;</strong></p>
<p>This is my lambda function code please check it if there is any problem in my code?
tell me where is my mistake?</p>
<pre><code>import { process as gprocess } from 'gremlin';
import Post from './Post'
const gremlin = require('gremlin')
const DriverRemoteConnection = gremlin.driver.DriverRemoteConnection
const Graph = gremlin.structure.Graph
const uri = process.env.WRITER
const { t, P, } = gprocess;
const __ = gprocess.statics;

async function createPost(post: Post) {

    let dc = new DriverRemoteConnection(`wss://${uri}/gremlin`, {})

    const graph = new Graph()
    const g = graph.traversal().withRemote(dc)  

    let vertex = await g.addV('posts').property('title',post.title).property('content', post.content).property('id', post.id).next()

    let edge = await g.addE('post_to_post').from(g.V().hasLabel('posts').next()).to(g.V().hasLabel('posts').next()).next()

    dc.close()

    return post;
}

export default createPost
</code></pre>",1,0,2021-09-28 03:30:53.330000 UTC,,2021-09-28 11:18:16.707000 UTC,3,typescript|gremlin|aws-cdk|aws-appsync|aws-neptune,49,2021-09-27 21:53:13.177000 UTC,2022-03-05 22:31:57.767000 UTC,,31,0,0,1,,,,,,[]
Insert nodes if not exist else update,"<p>I am working on gremlin query in python. I want to insert node in my neptune graph only if they dont exist else I need to update them. Here is my query to insert node in batches</p>
<pre><code>[{id:1,'Name':'John','Age':25},
 {id:2,'Name':'Mike','Age':28},
 .......] 


self.g.inject(vertexes).unfold().as_('entity').\
        addV(label_name).as_('v').\
        sideEffect(__.select('entity').unfold().as_('kv').select('v').\
                    property(__.select('kv').by(Column.keys),
                            __.select('kv').by(Column.values)
                            )
                    ).iterate()
</code></pre>
<p>I have tried by adding coalesce operator</p>
<pre><code> //check if id already exist    
 self.g.inject(vertexes).has(&quot;Id&quot;,&quot;1&quot;).fold().coalesce(__unfold().as_('entity')).\
        addV(label_name).as_('v').\
        sideEffect(__.select('entity').unfold().as_('kv').select('v').\
                    property(__.select('kv').by(Column.keys),
                            __.select('kv').by(Column.values)
                            )
                    ).iterate()
</code></pre>
<p>How can I update this query to update the nodes if they exist</p>",0,1,2021-08-10 13:38:01.700000 UTC,,,0,python|amazon-neptune|gremlinpython,37,2012-08-21 12:31:20.977000 UTC,2022-02-12 07:51:43.010000 UTC,"Mumbai, India",9672,651,351,1058,,,,,,[]
Databricks INIT script errors with wget,"<p>I am trying to write an init script for a Databricks server. I have included a wget command which works from a Databricks Jupyter notebook. However when I include the command into an init script the init script fails and I do not know why.</p>
<p>Here is the script which I am using:</p>
<pre><code>#!/bin/bash

apt install libwayland-server0 -y
pip3 install selenium==3.141 pandas webdriver-manager pyvirtualdisplay msedge-selenium-tools
wget -q https://packages.microsoft.com/keys/microsoft.asc -O- | sudo apt-key add -
add-apt-repository &quot;deb [arch=amd64] https://packages.microsoft.com/repos/edge stable main&quot;
</code></pre>
<p>Does anybody have an idea why the last commands are not working?</p>
<p>Cheers</p>",2,1,2021-12-01 11:51:50.220000 UTC,,,0,azure-databricks,64,2012-09-19 21:34:07.720000 UTC,2022-03-03 05:34:12.637000 UTC,,628,2,1,76,,,,,,[]
What is the main advantage of GIT over SVN?,"<p>From my understand of Git Vs SVN, the main advantage of Git over SVN, apart from the distributed nature of Git, is that one can commit/push updates to the remote respository, without having to resolve conflicts at point of updateding the repository.</p>

<p>What do you think?</p>",1,3,2016-09-13 20:38:58.637000 UTC,,2016-09-14 01:37:44.040000 UTC,-3,git|version-control|dvcs,64,2011-11-09 13:17:09.143000 UTC,2022-03-03 13:38:39.660000 UTC,,767,25,1,116,,,,,,[]
How to filter Timestamp in Hours instead of Seconds?,"<p><a href=""https://i.stack.imgur.com/mEKvP.jpg"" rel=""nofollow noreferrer"">timestampbefore</a> <a href=""https://i.stack.imgur.com/Yl3XP.jpg"" rel=""nofollow noreferrer"">timestamp after @pissall's code</a> I have a Timestamp column with 0.5Hz frequency, that results in millions of rows. I am willing to reduce this data size by having a timestamp in an hourly manner. i.e 24 observations for a particular day. 
I already reduced the data size by filtering the data by year, month and day. but as it is still very big i want to reduce it now to hourly basis. </p>

<p>I am working on Databricks and using PySpark for the same.</p>

<p>i used following command to reduce my data size from years to a Day.  </p>

<p><code>df = df.filter(df.Timestamp.between('2019-09-03 00:00:00','2019-09-04 00:00:00'))</code></p>

<p>I would appreciate your help. 
Thanks</p>

<p><a href=""https://i.stack.imgur.com/J60a6.jpg"" rel=""nofollow noreferrer"">Java.util.gregori...</a></p>",1,4,2019-11-18 15:34:30.663000 UTC,,2019-11-18 17:31:50.273000 UTC,0,pyspark|timestamp|sampling|azure-databricks,215,2019-03-19 15:37:23.820000 UTC,2022-03-01 19:36:43.143000 UTC,"Friedrichshafen, Deutschland",23,0,0,11,,,,,,[]
Unable to build Spark application with multiple main classes for Databricks job,"<p>I have a spark application that contains multiple spark jobs to be run on Azure data bricks. I want to build and package the application into a fat jar. The application is able to compile successfully. While I am trying to package (command: sbt package) the application, it gives an error  <em><strong>&quot;[warn] multiple main classes detected: run 'show discoveredMainClasses' to see the list&quot;.</strong></em></p>
<p>How to build the application jar (Without specifying any main class) so that I can upload it to Databricks job and specify the main classpath over there?</p>",1,1,2021-12-11 10:27:14.813000 UTC,,,1,apache-spark|sbt|azure-databricks|spark-jobserver,36,2013-11-12 10:22:33.417000 UTC,2022-03-03 12:43:12.470000 UTC,"Bangalore, Karnataka, India",983,260,1,668,,,,,,[]
Python Requests OS Error 104 Connection Broken Error,"<p>Hi I am trying to a hit an API using requests module of python. The Api has to be hit 20000 times as the number of pages are around 20000. In every hit the data comes around 10 mb. By the end of the process it creates a json file of around 100gb. Here is the code I have written</p>

<pre><code>with open('file.json','wb',buffering=100*1048567) as f:
  while(next_page_cursor != """"):
    with request.get(url,headers=headers) as response:
      json_response = json.loads(response.content.decode('utf-8')) 
      """"""
      json response looks something like this 
      {
        content:[{},{},{}........50 dictionaries]
        next_page_cursor : ""abcd""
      }
      """"""
      next_page_cursor = json_response['next_page_cursor']

      for data in json_response['content']:
        f.write((json.dumps(data) + ""\n"").encode())
</code></pre>

<p>But after running successfully for few pages the code fails giving the below error:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;command-1206920060120926&gt;"", line 65, in &lt;module&gt;
    with requests.get(data_url, headers = headers) as response:
  File ""/databricks/python/lib/python3.7/site-packages/requests/api.py"", line 75, in get
    return request('get', url, params=params, **kwargs)
  File ""/databricks/python/lib/python3.7/site-packages/requests/api.py"", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/databricks/python/lib/python3.7/site-packages/requests/sessions.py"", line 533, in request
    resp = self.send(prep, **send_kwargs)
  File ""/databricks/python/lib/python3.7/site-packages/requests/sessions.py"", line 686, in send
    r.content
  File ""/databricks/python/lib/python3.7/site-packages/requests/models.py"", line 828, in content
    self._content = b''.join(self.iter_content(CONTENT_CHUNK_SIZE)) or b''
  File ""/databricks/python/lib/python3.7/site-packages/requests/models.py"", line 753, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: OSError(""(104, \'ECONNRESET\')"")', OSError(""(104, 'ECONNRESET')""))
</code></pre>",1,0,2020-04-01 09:23:13.333000 UTC,,2020-04-01 09:28:14.397000 UTC,1,python|python-3.x|python-requests|azure-databricks|chunked-encoding,1549,2018-02-12 11:55:37.653000 UTC,2022-02-25 08:07:16.973000 UTC,"Hyderabad, Telangana, India",495,24,0,92,,,,,,[]
When using a DVCS in front of Subversion should I merge or rebase before pushing?,"<p>My company uses Subversion officially, but I'm using Mercurial with the hgsubversion plugin for my daily work.</p>

<p>I tend to have more than one local feature branch (hg bookmark) going at a time. When I complete work on a branch I'll rebase it to the tip of the Subversion trunk and push the whole branch history into the Subversion repository.</p>

<p>I like that all of my granular changes get into the main repository - my coworkers can see why I make a change and not just the end result. But pushing to Subversion rewrites my commits, changing the timestamps to the moment of push, so I'm losing a little bit of fidelity from my work history.</p>

<p>I'm beginning to wonder if I should just be merging my local branches for the push to SVN and maintain my local commit history in branches in my Hg repository. If I move to a merge-oriented workflow, am I hurting my team? Is it realistically possible to sum up the changes that went into a merge with a single commit message?</p>

<p>Does Git->Subversion have this same sort of dichotomy? Aside from this philosophical issue, I'm quite happy with the Hg->Subversion workflow. But any Git users I've talked to seem to abhor the idea of using Git as a frontend for SVN for anything more than brief checkouts and patch submissions to open source projects.</p>",0,0,2012-05-07 15:33:42.090000 UTC,1.0,,4,git|svn|mercurial|workflow|dvcs,106,2010-02-25 18:17:41.513000 UTC,2021-11-09 22:53:57.250000 UTC,"Portland, OR",2900,415,3,169,,,,,,[]
Splitting existing repository & applying latest commits to another repository,"<p>Some time ago I have converted my SVN repository into Mercurial repository. It seemed to be converted properly and I have already committed many changes to the project. After a while (when digging in the history) I have realized that the conversion was not done properly - i.e old commits were not sorted by time, so I have all the revisions from SVN repository but not in a proper order (and on top of that I have my commits made after the conversion).</p>

<p>I would like to fix this. The only way of fixing this I can think of is to split the repository in two</p>

<ul>
<li>one containing revisions converted from old SVN repository (let's call it <code>A1</code>)</li>
<li>and one containing only new commits (let's call it <code>B1</code>)</li>
</ul>

<p>I would like to convert my SVN repository once again (properly this time) and then re-apply all the changes that was made by me since the repository was converted first time (<code>B1</code> part).</p>

<p>Just to sum everything up, what I need to do (or, what I think I need to do) is:</p>

<ol>
<li><p>Split existing repo in two - one piece that was created by converting from SVN repository (<code>A1</code>), and the other piece that contains proper commits (made by me after the repository conversion - <code>B1</code>).</p></li>
<li><p>Convert SVN repository to Mercurial (or Git, if for some reason all the other steps are not doable in Mercurial). So after the conversion I will have <code>A2</code>.</p></li>
<li><p>Apply the changes from <code>B1</code> to the newly converted repository from point 2 - <code>A2</code></p></li>
</ol>

<p>I think I know how to do the conversion (point 2) to get properly ordered commits (<code>A2</code>). I just need help with 1. (splitting existing repository into <code>A1</code> &amp; <code>B1</code>) &amp; 3 (applying commits from <code>B1</code> to <code>A2</code>).</p>

<p>Is that doable in Mercurial? If it is not doable in Mercurial - is it possible to achieve it using Git (as far as I know it is possible to convert SVN and Mercurial repository to Git repository with ease).</p>",3,0,2012-09-01 07:17:53.560000 UTC,,2012-09-01 19:22:19.820000 UTC,2,git|mercurial|repository|dvcs,110,2009-08-30 23:43:44.157000 UTC,2020-09-02 14:16:56.970000 UTC,"Wroclaw, Poland",1598,202,3,233,,,,,,[]
Using azure data factory to get data from delta tables to blob,"<p>I'm trying to get data from a delta table in Azure Databricks using Data Factory. I followed this doc <a href=""https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sp-access#access-adls-gen2-directly"" rel=""nofollow noreferrer"">link</a> to mount the storage and I'm using a service principal to access to the storage.</p>
<p>I can &quot;preview&quot; the data in the copy activity but when I try to transfer I get this error:</p>
<p>Failure to initialize configurationInvalid configuration value detected for fs.azure.account.key
Caused by: Invalid configuration value detected for fs.azure.account.key.</p>
<p><a href=""https://i.stack.imgur.com/FQ3Oa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FQ3Oa.png"" alt=""enter image description here"" /></a></p>
<p>I even give <strong>contributor</strong> role to the APP and the ADF in the Databricks resource</p>
<p><a href=""https://i.stack.imgur.com/91uuC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/91uuC.png"" alt=""enter image description here"" /></a></p>
<p>I'm running without more ideas, so any one has tried to do this ? Thanks in advance</p>",2,0,2022-01-26 19:29:16.607000 UTC,1.0,,1,azure|azure-data-factory-2|azure-databricks,58,2018-12-07 18:01:35.740000 UTC,2022-03-02 20:04:16.837000 UTC,"Santiago, Chile",380,20,1,43,,,,,,[]
ADF Mapping Dataflows performance very low as compared to Databricks when performing same set of transformations,"<p>I am doing same set of transformation using ADF Mapping Dataflows and Databricks. Dataflows takes around 2 hours whereas Databricks is taking only 8-10 mins for the same volume of data. As i understand they both use Apache Spark in the back-end, can someone please help me understand this behavior.</p>
<p>Dataflow Script:</p>
<pre><code>{
&quot;name&quot;: &quot;df_Test&quot;,
&quot;properties&quot;: {
    &quot;type&quot;: &quot;MappingDataFlow&quot;,
    &quot;typeProperties&quot;: {
        &quot;sources&quot;: [
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;ds_ST_SourceRefined&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;SalesTransactionUS&quot;
            },
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;ds_SC_SourceRefined&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;ShipToCustomer&quot;
            },
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;ds_PG_SourceRefined&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;PlantGlobal&quot;
            },
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;ds_M_SourceRefined&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;Materials&quot;
            },
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;ds_C_SourceRefined&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;Country&quot;
            },
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;ds_A_SourceRefined&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;Airport&quot;
            },
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;ds_SL_SourceRefined&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;ShipToLocation&quot;
            },
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;ds_LD_SourceRefined&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;LookupDivision&quot;
            },
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;ds_LCT_SourceRefined&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;LookupCurrencyType&quot;
            },
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;ds_PCH_SourceRefined&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;ProfitCenterHierarchy&quot;
            },
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;ds_DCal_SourceRefined&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;DateCalculations&quot;
            },
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;ds_CUOM_SourceRefined&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;CurrencyUOM&quot;
            },
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;ds_UUOM_SourceRefined&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;UnitUOM&quot;
            },
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;ds_LSDTS_SourceRefined&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;LookupSalesDocument&quot;
            }
        ],
        &quot;sinks&quot;: [
            {
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;ds_STS_SinkProduce&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;name&quot;: &quot;sinkToProduce&quot;
            }
        ],
        &quot;transformations&quot;: [
            {
                &quot;name&quot;: &quot;ProfitCentreJoin&quot;
            },
            {
                &quot;name&quot;: &quot;ShipToCustomerJoin&quot;
            },
            {
                &quot;name&quot;: &quot;PlantGlobalJoin&quot;
            },
            {
                &quot;name&quot;: &quot;MaterialsJoin&quot;
            },
            {
                &quot;name&quot;: &quot;AirportJoin&quot;
            },
            {
                &quot;name&quot;: &quot;ShipToLocationJoin&quot;
            },
            {
                &quot;name&quot;: &quot;LookupDivisionJoin&quot;
            },
            {
                &quot;name&quot;: &quot;LookupCurrencyTypeJoin&quot;
            },
            {
                &quot;name&quot;: &quot;CountryJoin&quot;
            },
            {
                &quot;name&quot;: &quot;DateCalculationJoin&quot;
            },
            {
                &quot;name&quot;: &quot;CurrencyUOMJoin&quot;
            },
            {
                &quot;name&quot;: &quot;UnitUOMJoin&quot;
            },
            {
                &quot;name&quot;: &quot;SelectColumns&quot;
            },
            {
                &quot;name&quot;: &quot;LookupSalesDocumentJoin&quot;
            }
        ],
        &quot;script&quot;: &quot;source(output(\n\t\tDateCalcKey as integer,\n\t\tCurrencyUOMKey as integer,\n\t\tUOMkey as integer,\n\t\tUnitUOMKey as integer,\n\t\tSalesTransactionKey as long,\n\t\tTransactionUniqueIdentifier as string,\n\t\tSourceCode as string,\n\t\tAccountingDocumentNumber as string,\n\t\tCompanyCode as string,\n\t\tTransactionType as string,\n\t\tSalesDocumentItem as integer,\n\t\tReferenceDocument as string,\n\t\tOrderNumber as string,\n\t\tPurchaseOrderNumber as string,\n\t\tReferenceItem as integer,\n\t\tSalesContractNumber as string,\n\t\tSalesContractType as string,\n\t\tSalesReferenceDocument as string,\n\t\tSenderCostCenter as string,\n\t\tCurrencyTypeKey as integer,\n\t\tProfitCenterKey as integer,\n\t\tDivisionKey as integer,\n\t\tPlantKey as integer,\n\t\tMaterialKey as integer,\n\t\tShipToKey as integer,\n\t\tSoldToConsolidatedCustomerNumber as string,\n\t\tSoldToKey as integer,\n\t\tPostingDayId as integer,\n\t\tSalesDocumentKey as integer,\n\t\tCountryKey as integer,\n\t\tRegionCode as string,\n\t\tAirportKey as integer,\n\t\tLocationKey as integer,\n\t\tShipToAirportCode as string,\n\t\tKPIElementCode as string,\n\t\tShipToCustomerPlantKey as integer,\n\t\tMeasures as decimal(19,3),\n\t\tSalesTransactionPostingMonthId as integer\n\t),\n\tallowSchemaDrift: false,\n\tvalidateSchema: false,\n\tformat: 'parquet') ~&gt; SalesTransactionUS\nsource(output(\n\t\tShipToKey as integer,\n\t\tShipToNumber as string,\n\t\tShipToSalesOrganization as string,\n\t\tShipToDistributionChannel as string,\n\t\tShipToDivision as string,\n\t\tAlternateSalesGroupIndicator as integer,\n\t\tSourceCode as string,\n\t\tShipToDisplayName as string,\n\t\tShipToName as string,\n\t\tShipToAddress as string,\n\t\tShipToCity as string,\n\t\tShipToState as string,\n\t\tShipToPostalCode as string,\n\t\tShipToCountryKey as integer,\n\t\tShipToLevel1 as string,\n\t\tShipToLevel1Description as string,\n\t\tShipToLevel1Cai as string,\n\t\tShipToLevel2 as string,\n\t\tShipToLevel2Description as string,\n\t\tShipToLevel2Cai as string,\n\t\tShipToLevel3 as string,\n\t\tShipToLevel3Description as string,\n\t\tShipToLevel3Cai as string,\n\t\tShipToSalesOffice as string,\n\t\tShipToSalesOfficeDescription as string,\n\t\tShipToSalesOfficeCai as string,\n\t\tShipToDistrict as string,\n\t\tShipToDistrictDescription as string,\n\t\tShipToDistrictCai as string,\n\t\tShipToSalesGroup as string,\n\t\tShipToSalesGroupDescription as string,\n\t\tShipToSalesGroupCai as string,\n\t\tShipToIndustry as string,\n\t\tShipToIndustryName as string,\n\t\tShipToIndustrySegment as string,\n\t\tShipToIndustrySegmentName as string,\n\t\tShipToIndustrySegmentLynxCode as string,\n\t\tShipToIndustrySegmentLynxName as string,\n\t\tShipToIndustrySegmentGroup as string,\n\t\tShipToIndustrySubSegment as string,\n\t\tShipToIndustrySubSegmentName as string,\n\t\tShipToIataCode as string,\n\t\tShipToAirportCode as string,\n\t\tShipToAirportName as string,\n\t\tShipToAirportCodeFlag as string,\n\t\tShipToDifferentialReferenceCode as string,\n\t\tSoldtoKey as integer,\n\t\tSoldToNumber as string,\n\t\tSoldToName as string,\n\t\tSoldToSalesOrganization as string,\n\t\tSoldToDistributionChannel as string,\n\t\tSoldToDivision as string,\n\t\tSoldToAddress as string,\n\t\tSoldToCity as string,\n\t\tSoldToState as string,\n\t\tSoldToPostalCode as string,\n\t\tSoldToCountryKey as integer,\n\t\tSoldToLevel1 as string,\n\t\tSoldToLevel1Description as string,\n\t\tSoldToLevel1Cai as string,\n\t\tSoldToLevel2 as string,\n\t\tSoldToLevel2Description as string,\n\t\tSoldToLevel2Cai as string,\n\t\tSoldToLevel3 as string,\n\t\tSoldToLevel3Description as string,\n\t\tSoldToLevel3Cai as string,\n\t\tSoldToSalesOffice as string,\n\t\tSoldToSalesOfficeDescription as string,\n\t\tSoldToSalesOfficeCai as string,\n\t\tSoldToDistrict as string,\n\t\tSoldToDistrictDescription as string,\n\t\tSoldToDistrictCai as string,\n\t\tSoldToSalesGroup as string,\n\t\tSoldToSalesGroupDescription as string,\n\t\tSoldToSalesGroupCai as string,\n\t\tSoldToIataCode as string,\n\t\tSoldToAirportCode as string,\n\t\tSoldToDifferentialReferenceCode as string,\n\t\tSoldToIndustry as string,\n\t\tSoldToIndustryName as string,\n\t\tSoldToIndustrySegment as string,\n\t\tSoldToIndustrySegmentName as string,\n\t\tSoldToIndustrySegmentLynxCode as string,\n\t\tSoldToIndustrySegmentLynxName as string,\n\t\tSoldToIndustrySegmentGroup as string,\n\t\tSoldToIndustrySubSegment as string,\n\t\tSoldToIndustrySubSegmentName as string,\n\t\tSoldToConsolidatedCustomerNumber as string,\n\t\tSoldToConsolidatedCustomerName as string,\n\t\tSubSourceCode as string,\n\t\tABCClassification as string,\n\t\tSoldToMosCustomerNumber as string,\n\t\tSoldToMosCustomerName as string,\n\t\tSoldToWithSalesGroup as string,\n\t\tShipToWithSalesGroup as string,\n\t\tShipToDeliveringPlant as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'parquet') ~&gt; ShipToCustomer\nsource(output(\n\t\tPlantKey as integer,\n\t\tPlant as string,\n\t\tValuationArea as string,\n\t\tCustomerNumber as string,\n\t\tPurchaseOrganization as string,\n\t\tSalesOrganization as string,\n\t\tDistributionChannel as string,\n\t\tSalesDivision as string,\n\t\tTaxIndicator as string,\n\t\tPlantTitle as string,\n\t\tName1 as string,\n\t\tName3 as string,\n\t\tName4 as string,\n\t\tCity as string,\n\t\tCountyCode as string,\n\t\tCounty as string,\n\t\tPostalCode as string,\n\t\tAddress as string,\n\t\tCountryKey as integer,\n\t\tCountryName as string,\n\t\tRegionName as string,\n\t\tState as string,\n\t\tAddressGroup as string,\n\t\tAddressSource as string,\n\t\tTaxJurisdictionCode as string,\n\t\tIATACode as string,\n\t\tOperationsArea as string,\n\t\tDeleteIndicator as string,\n\t\tRefineryArea as string,\n\t\tSourceCode as string,\n\t\tSubSourceCode as string,\n\t\tActiveIndicator as boolean,\n\t\tSapType as string,\n\t\tTerminalFlag as string,\n\t\tPlantType as string,\n\t\tOldCode as string,\n\t\tTerminalCenter as string,\n\t\tTransportationCenter as string,\n\t\tSupplyAreaCode as string,\n\t\tSupplyAreaName as string,\n\t\tDistributiveArea as string,\n\t\tPlantSubType as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'parquet') ~&gt; PlantGlobal\nsource(output(\n\t\tMaterialKey as integer,\n\t\tMaterialNumber as string,\n\t\tSourceCode as string,\n\t\tMaterialDescription as string,\n\t\tHierarchyLevel5 as string,\n\t\tHierarchyLevel5Description as string,\n\t\tHierarchyLevel4 as string,\n\t\tHierarchyLevel4Description as string,\n\t\tHierarchyLevel3 as string,\n\t\tHierarchyLevel3Description as string,\n\t\tHierarchyLevel2 as string,\n\t\tHierarchyLevel2Description as string,\n\t\tMaterialGroup as string,\n\t\tBrandCode as string,\n\t\tExternalMaterialGroup as string,\n\t\tSubSourceCode as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'parquet') ~&gt; Materials\nsource(output(\n\t\tCountryKey as integer,\n\t\tCountryCode as string,\n\t\tCountryName as string,\n\t\tDefaultCurrencyCode as string,\n\t\tDefaultVolumeUom as string,\n\t\tDefaultUomRate as string,\n\t\tDefaultSalesOrganization as string,\n\t\tCurrencyFormat as string,\n\t\tCurrencyFormatDecimalPlaces as string,\n\t\tRegionCode as string,\n\t\tRegionName as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'parquet') ~&gt; Country\nsource(output(\n\t\tAirportKey as integer,\n\t\tAirportCode as string,\n\t\tAirportName as string,\n\t\tAirportCodeName as string,\n\t\tAirportDescription as string,\n\t\tActiveIndicator as boolean,\n\t\tSortOrder as integer\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'parquet') ~&gt; Airport\nsource(output(\n\t\tLocationKey as integer,\n\t\tLocationId as string,\n\t\tSourceCode as string,\n\t\tStartDate as timestamp,\n\t\tEndDate as timestamp,\n\t\tName as string,\n\t\tAddress as string,\n\t\tCity as string,\n\t\tState as string,\n\t\tCountryKey as integer,\n\t\tCountryName as string,\n\t\tLocationType as string,\n\t\tLocationTypeName as string,\n\t\tLocationStatusKey as integer,\n\t\tLocationStatusName as string,\n\t\tPostalCode as string,\n\t\tProfitCenterKey as integer,\n\t\tCostCenter as string,\n\t\tPlantKey as integer,\n\t\tCountryCode as string,\n\t\tShipToKey as integer,\n\t\tSubsourcecode as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'parquet') ~&gt; ShipToLocation\nsource(output(\n\t\tDivisionKey as integer,\n\t\tDivision as string,\n\t\tDivisionName as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'parquet') ~&gt; LookupDivision\nsource(output(\n\t\tCurrencyTypeKey as integer,\n\t\tCurrencyType as string,\n\t\tCurrencyTypeName as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'parquet') ~&gt; LookupCurrencyType\nsource(output(\n\t\tProfitCenterKey as integer,\n\t\tProfitCenter as string,\n\t\tProfitCenterName as string,\n\t\tBusinessAreaName as string,\n\t\tCountryCode as string,\n\t\tCountryName as string,\n\t\tRegionCode as string,\n\t\tRegionName as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'parquet') ~&gt; ProfitCenterHierarchy\nsource(output(\n\t\tDateCalcKey as integer,\n\t\tDateCalcDesc as string,\n\t\tSortOrder as integer\n\t),\n\tallowSchemaDrift: false,\n\tvalidateSchema: true,\n\tformat: 'parquet') ~&gt; DateCalculations\nsource(output(\n\t\tCurrencyUOMKey as integer,\n\t\tCurrencyUOMName as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'parquet') ~&gt; CurrencyUOM\nsource(output(\n\t\tUnitUOMKey as integer,\n\t\tUnitUOMName as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'parquet') ~&gt; UnitUOM\nsource(output(\n\t\tSalesDocumentKey as integer,\n\t\tSalesDocumentType as string,\n\t\tSalesDocumentName as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'parquet') ~&gt; LookupSalesDocument\nSalesTransactionUS, ProfitCenterHierarchy join(SalesTransactionUS@ProfitCenterKey == ProfitCenterHierarchy@ProfitCenterKey,\n\tjoinType:'left',\n\tbroadcast: 'auto')~&gt; ProfitCentreJoin\nCountryJoin, ShipToCustomer join(SalesTransactionUS@ShipToKey == ShipToCustomer@ShipToKey,\n\tjoinType:'inner',\n\tbroadcast: 'auto')~&gt; ShipToCustomerJoin\nShipToCustomerJoin, PlantGlobal join(SalesTransactionUS@PlantKey == PlantGlobal@PlantKey,\n\tjoinType:'inner',\n\tbroadcast: 'auto')~&gt; PlantGlobalJoin\nPlantGlobalJoin, Materials join(SalesTransactionUS@MaterialKey == Materials@MaterialKey,\n\tjoinType:'inner',\n\tbroadcast: 'auto')~&gt; MaterialsJoin\nMaterialsJoin, Airport join(SalesTransactionUS@AirportKey == Airport@AirportKey,\n\tjoinType:'inner',\n\tbroadcast: 'auto')~&gt; AirportJoin\nAirportJoin, ShipToLocation join(SalesTransactionUS@LocationKey == ShipToLocation@LocationKey,\n\tjoinType:'inner',\n\tbroadcast: 'auto')~&gt; ShipToLocationJoin\nShipToLocationJoin, LookupDivision join(SalesTransactionUS@DivisionKey == LookupDivision@DivisionKey,\n\tjoinType:'inner',\n\tbroadcast: 'auto')~&gt; LookupDivisionJoin\nLookupDivisionJoin, LookupCurrencyType join(SalesTransactionUS@CurrencyTypeKey == LookupCurrencyType@CurrencyTypeKey,\n\tjoinType:'left',\n\tbroadcast: 'auto')~&gt; LookupCurrencyTypeJoin\nProfitCentreJoin, Country join(SalesTransactionUS@CountryKey == Country@CountryKey,\n\tjoinType:'inner',\n\tbroadcast: 'auto')~&gt; CountryJoin\nLookupCurrencyTypeJoin, DateCalculations join(SalesTransactionUS@DateCalcKey == DateCalculations@DateCalcKey,\n\tjoinType:'inner',\n\tbroadcast: 'auto')~&gt; DateCalculationJoin\nDateCalculationJoin, CurrencyUOM join(SalesTransactionUS@CurrencyUOMKey == CurrencyUOM@CurrencyUOMKey,\n\tjoinType:'inner',\n\tbroadcast: 'auto')~&gt; CurrencyUOMJoin\nCurrencyUOMJoin, UnitUOM join(SalesTransactionUS@UnitUOMKey == UnitUOM@UnitUOMKey,\n\tjoinType:'inner',\n\tbroadcast: 'auto')~&gt; UnitUOMJoin\nLookupSalesDocumentJoin select(mapColumn(\n\t\tDateCalcDesc,\n\t\tSortOrder = DateCalculations@SortOrder,\n\t\tCurrencyUOMName,\n\t\tUOMkey,\n\t\tUnitUOMName,\n\t\tSalesTransactionKey,\n\t\tTransactionUniqueIdentifier,\n\t\tSourceCode = SalesTransactionUS@SourceCode,\n\t\tAccountingDocumentNumber,\n\t\tCompanyCode,\n\t\tTransactionType,\n\t\tSalesDocumentItem,\n\t\tReferenceDocument,\n\t\tOrderNumber,\n\t\tPurchaseOrderNumber,\n\t\tReferenceItem,\n\t\tSalesContractNumber,\n\t\tSalesContractType,\n\t\tSalesReferenceDocument,\n\t\tSenderCostCenter,\n\t\tCurrencyType,\n\t\tCurrencyTypeName,\n\t\tProfitCenter,\n\t\tProfitCenterName,\n\t\tBusinessAreaName,\n\t\tProfitCentreCountryCode = ProfitCenterHierarchy@CountryCode,\n\t\tProfitCentreCountryName = ProfitCenterHierarchy@CountryName,\n\t\tProfitCentreRegionCode = ProfitCenterHierarchy@RegionCode,\n\t\tProfitCentreRegionName = ProfitCenterHierarchy@RegionName,\n\t\tDivision,\n\t\tDivisionName,\n\t\tPlant,\n\t\tMaterialNumber,\n\t\tShipToNumber,\n\t\tSoldToConsolidatedCustomerNumber = SalesTransactionUS@SoldToConsolidatedCustomerNumber,\n\t\tSoldToNumber,\n\t\tPostingDayId,\n\t\tPostingMonthId = SalesTransactionPostingMonthId,\n\t\tSalesDocumentName,\n\t\tRegionCode = SalesTransactionUS@RegionCode,\n\t\tCountryCode = Country@CountryCode,\n\t\tCountryName = Country@CountryName,\n\t\tDefaultCurrencyCode,\n\t\tDefaultVolumeUom,\n\t\tDefaultUomRate,\n\t\tDefaultSalesOrganization,\n\t\tCurrencyFormat,\n\t\tCurrencyFormatDecimalPlaces,\n\t\tRegionName = Country@RegionName,\n\t\tAirportCode,\n\t\tLocationId,\n\t\tShipToAirportCode = SalesTransactionUS@ShipToAirportCode,\n\t\tKPIElementCode,\n\t\tShipToCustomerPlantKey,\n\t\tMeasures\n\t),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~&gt; SelectColumns\nUnitUOMJoin, LookupSalesDocument join(SalesTransactionUS@SalesDocumentKey == LookupSalesDocument@SalesDocumentKey,\n\tjoinType:'inner',\n\tbroadcast: 'auto')~&gt; LookupSalesDocumentJoin\nSelectColumns sink(input(\n\t\tAirportKey as integer,\n\t\tAirportCode as string,\n\t\tAirportName as string,\n\t\tAirportCodeName as string,\n\t\tAirportDescription as string,\n\t\tActiveIndicator as boolean,\n\t\tSortOrder as integer\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'parquet',\n\tpartitionFileNames:['SalesTransactionUS.parquet'],\n\tpartitionBy('hash', 1),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~&gt; sinkToProduce&quot;
    }
}
</code></pre>
<p>}</p>",0,3,2020-07-27 13:40:56.693000 UTC,,2020-08-11 07:05:20.743000 UTC,1,performance|azure-data-factory|azure-databricks,46,2016-01-22 07:17:36.903000 UTC,2022-02-08 02:43:40.270000 UTC,"Pune, Maharashtra, India",11,0,0,1,,,,,,[]
Azure Databricks - Generate SQL Select Statement with Columns,"<p>I have tables in Azure Databricks that I am using SQL to interact with via a notebook. I need to select all columns from a table with 200 columns, I need to select all of them but I need to modify some for a select insert (To modify specific columns for a PK). Therefore I can not use a select *. (There are multiple scenarios this is just my current objective)</p>
<p>How can I generate a select statement on a table with all the column names in a sql statement. This would be equivalent of a 'Select top N' in SSMS where it generates a select for the table I can than edit.</p>
<p>I have seen functions like describe and show but they can't build a select statement.
I am new to Databricks. Any help is appreciated.</p>",0,5,2021-08-20 14:30:34.170000 UTC,,,0,sql|azure-databricks,102,2015-06-30 12:51:41.647000 UTC,2021-10-27 20:18:57.073000 UTC,,11,0,0,4,,,,,,[]
To load data from Azure Delta Lake & Azure Data Bricks to Postgres SQL in single transaction( To insert 10 tables of data and rollback for any errors),"<p>I would like to load data (around 10 tables) from Azure delta lake to azure postgressql. I have tried jdbc connectivity in azure data bricks.I can able to insert and read data from delta lake to postgressql. But not able to achieve atomicity (to insert all tables data in a single transaction). Could you help better way to connect between azure delta lake to postgressql and how to achieve atomicity?</p>
<p>Data frequency is hourly feeds from source and volume of data is very huge. Thank you.</p>",1,0,2021-07-23 14:58:59.033000 UTC,,,1,azure-databricks|delta-lake|azure-postgresql,163,2021-07-23 14:46:59.753000 UTC,2022-02-17 13:30:19.320000 UTC,,11,0,0,4,,,,,,[]
Remove Trailing Zero for float columns in Azure Databricks - Scala,"<p>I have created a Table in Azure Databricks with few columns that have Datatype as Float. The table has about 250 columns and out of which about 50 columns are float. I am extracting the output of that table to a tab delimited file. All rows are extracted to the file correctly but when I look at the float columns they are appended with a decimal value by default. I do not see these decimal values in the table when I perform a select but they only appear in the output file that is generated. <em><strong>Is there a way that I can eliminate the zero decimals that is generated as default for all float type columns ?</strong></em></p>
<p>Table in my Azure Databricks:</p>
<pre><code>DataType: 
Year: int
MOM: float
AGG: float
State: string
*******************

|Year|MOM|AGG   |State|
|2021|10 |9.9   |CA   |
|2020|18 |10    |CA   |
|2019|987|952.35|CA   |
|2018|1  |1     |CA   |
</code></pre>
<p>After I generate the output in a tsv file it looks like below, &quot;.0&quot; is appended to every value where the column is a float type.</p>
<pre><code>|Year  |MOM  |AGG     |State|
|2021  |10.0 |9.9     |CA   |
|2020  |18.0 |10.0    |CA   |
|2019  |987.0|952.35  |CA   |
|2018  |1.0  |1.0     |CA   |

*******************
</code></pre>
<p>Required Output:</p>
<pre><code>|Year  |MOM  |AGG     |State|
|2021  |10   |9.9     |CA   |
|2019  |18   |10      |CA   |
|2019  |987  |952.35  |CA   |
|2018  |1    |1       |CA   |
</code></pre>
<p>Code Snippet</p>
<pre><code>df = spark.sql(&quot;Select * from prod.summarysales&quot;)

df_oupput = df.na.fill(&quot;&quot;)

df_oupput
.coalesce(1)
.write
.format(&quot;csv&quot;)
.mode(&quot;append&quot;)
.option(&quot;header&quot;, &quot;true&quot;)
.option(&quot;delimiter&quot;, &quot;\t&quot;)
.option(&quot;sep&quot;, &quot;\t&quot;)
.option(&quot;emptyValue&quot;, &quot;&quot;)
.csv(&quot;/output/summarysalesreport.tsv&quot;)
</code></pre>",0,0,2021-04-29 15:00:39.903000 UTC,,,0,azure-databricks,48,2016-04-18 10:30:07.417000 UTC,2022-01-07 17:27:49.823000 UTC,"Bangalore, Karnataka, India",45,2,0,61,,,,,,[]
"Odd Request Size is too large error, Python + Databricks + CosmosDB","<p>I have the following script in my databricks notebook, I am having an issue with the request size being too large when saving my dataframe to cosmos.</p>
<pre><code>    writeConfig = {
      &quot;Endpoint&quot;: &quot;https://mycosmosdbendpoint:443/&quot;,
      &quot;Masterkey&quot;: &quot;*****&quot;,
      &quot;Database&quot;: database,
      &quot;Collection&quot;: collection,
      &quot;WritingBatchSize&quot;: &quot;1&quot;,
      &quot;Upsert&quot;: &quot;true&quot;, 
    }
example_df = spark.sql(f'SELECT * FROM {myData}.temp_cosmos_export')
example_df.write.format(&quot;com.microsoft.azure.cosmosdb.spark&quot;).options(**writeConfig).mode(&quot;overwrite&quot;).save()
</code></pre>
<p>This gives the following error</p>
<blockquote>
<p>org.apache.spark.SparkException: Job aborted due to stage failure:
Task 0 in stage 77285.0 failed 4 times, most recent failure: Lost task
0.3 in stage 77285.0 (TID 676380, executor 209): java.lang.Exception: Errors encountered in bulk import API execution.
PartitionKeyDefinition:
{&quot;paths&quot;:[&quot;/id&quot;],&quot;kind&quot;:&quot;Hash&quot;},
Number of failures corresponding to exception of type:
com.microsoft.azure.documentdb.DocumentClientException = 1; FAILURE:
com.microsoft.azure.documentdb.DocumentClientException:
{&quot;Errors&quot;:[&quot;Request size is too large&quot;]}</p>
</blockquote>
<p>While debugging this issue I have reduced the number of items in my dataframe to just 1 item and it still fails!. What is really odd is that if I add the following line of code <code>display(example_df )</code> e.g.</p>
<pre><code>example_df = spark.sql(f'SELECT * FROM {myData}.temp_cosmos_export')
display(example_df )
example_df.write.format(&quot;com.microsoft.azure.cosmosdb.spark&quot;).options(**writeConfig).mode(&quot;overwrite&quot;).save()
</code></pre>
<p>Then the single record is sent to Cosmos successfully!
When I check the size of the object in CosmosDB it is only about 250k well below the 2MB limit for Cosmos.</p>
<p>Why does the act of displaying the dataframe cause it to work? How can my request size be too large for a 250kb document?</p>
<p>Has anyone else come across anything like this? Any help or suggestions would be appreciated.</p>",0,0,2021-03-29 17:13:28.603000 UTC,,2021-03-29 17:34:01.877000 UTC,0,python|azure-cosmosdb|azure-databricks,92,2013-04-05 08:11:31.227000 UTC,2022-03-03 18:27:15.987000 UTC,"London, UK",1552,60,1,155,,,,,,[]
How to load a .msg file into pyspark RDD?,"<p>I am relatively new to pyspark programming, hence looking out for a way to read a bunch of outlook (.msg) files from databricks file system (dbfs). While executing the below line of code, I'm getting some junk unicode data as shown in output below:</p>

<pre><code>rdd = sc.wholeTextFiles(""dbfs:/......./*.msg"")
</code></pre>

<p>Output:</p>

<pre><code>[(u'dbfs:/........./file1.msg', u'\ufffd\ufffd\x11\u0871\x1a\ufffd\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00&gt;\x00\x03\x00\ufffd\ufffd\t\x00\x06\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00 \x00\x00\x00\x05\x00\x00\x00\ufffd\ufffd\ufffd\ufffd\x00\x00\x00\x00\x03\x00\x00\x00\ufffd\x00\x00\x00\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd')]
</code></pre>

<p>Can anybody help me in implementing a pairedRDD wherein ""key= path of email msg file"" and ""value= content of email msg file."" ? Also, is there any alternative function that can be used to achieve the same?</p>",1,0,2019-07-25 06:59:09.387000 UTC,,,0,apache-spark|pyspark|outlook|rdd|azure-databricks,180,2019-03-13 06:11:32.237000 UTC,2020-07-27 06:44:16.873000 UTC,India,13,0,0,31,,,,,,[]
Neptune is returning a 500 error on Lambda cold starts,"<p>I'm using Gremlin on Lambda (Node 12.x) to traverse Neptune. Occasionally on a cold start, Neptune will return a 500 error. From CloudWatch:</p>

<pre><code>{
    ""errorType"": ""ResponseError"",
    ""errorMessage"": ""Server error: {\""requestId\"":\""2a15ac95-ca36-406b-b210-5f7f98b6dd1d\"",\""detailedMessage\"":\""An unexpected error has occurred in Neptune.\"",\""code\"":\""InternalFailureException\""} (500)"",
    ""name"": ""ResponseError"",
    ""statusCode"": 500,
    ""statusMessage"": ""{\""requestId\"":\""2a15ac95-ca36-406b-b210-5f7f98b6dd1d\"",\""detailedMessage\"":\""An unexpected error has occurred in Neptune.\"",\""code\"":\""InternalFailureException\""}"",
    ""statusAttributes"": {},
    ""stack"": [
        ""ResponseError: Server error: {\""requestId\"":\""2a15ac95-ca36-406b-b210-5f7f98b6dd1d\"",\""detailedMessage\"":\""An unexpected error has occurred in Neptune.\"",\""code\"":\""InternalFailureException\""} (500)"",
        ""    at Connection._handleMessage (/var/task/src/services/getFeed/index.js:8774:9)"",
        ""    at WebSocket.&lt;anonymous&gt; (/var/task/src/services/getFeed/index.js:8634:43)"",
        ""    at WebSocket.emit (events.js:210:5)"",
        ""    at WebSocket.EventEmitter.emit (domain.js:476:20)"",
        ""    at Receiver._receiver.onmessage (/var/task/src/services/getFeed/index.js:21851:47)"",
        ""    at Receiver.dataMessage (/var/task/src/services/getFeed/index.js:21069:14)"",
        ""    at Receiver.getData (/var/task/src/services/getFeed/index.js:21019:12)"",
        ""    at Receiver.startLoop (/var/task/src/services/getFeed/index.js:20854:16)"",
        ""    at Receiver.add (/var/task/src/services/getFeed/index.js:20828:10)"",
        ""    at Socket.&lt;anonymous&gt; (/var/task/src/services/getFeed/index.js:21848:22)""
    ]
}
</code></pre>

<p>Logging is enabled in Neptune but I don't see anything being captured in CloudWatch. Subsequent requests to Neptune run fine. How can I further debug this?</p>",0,5,2019-12-22 01:19:53.790000 UTC,1.0,,0,gremlin|amazon-neptune,246,2010-08-23 23:03:20.887000 UTC,2022-03-03 17:19:08.790000 UTC,,4656,718,14,204,,,,,,[]
Can not figure out how checkout works in git,"<p>I have created a new repository and commited a file <code>a.pl</code>. All ok.<br>
If I do <code>git status</code> that there is nothing added or untracked changes. I do a <code>git log</code> and I see the <code>hash</code> of my last (and actually only) commit in this repository.<br>
Now I modify the <code>a.pl</code> and I append a new line in the end of the file. E.g <code>print ""1"";</code><br>
I save it and if I do <code>git status</code> it reports untracked changes.<br>
Now If I do: <code>git checkout &lt;commit_id&gt;</code> using the hash reported in <code>git log</code> I expect to go to the version I commited, so my recent untracked modification should be deleted.<br>
I do <code>git checkout 1d739</code> and I get:  </p>

<blockquote>
  <p>Note: checking out '1d739'.  </p>
  
  <p>You are in 'detached HEAD' state. You can look around, make
  experimental   changes and commit them, and you can discard any
  commits you make in this   state without impacting any branches by
  performing another checkout. etc</p>
</blockquote>

<p>When I look in the file, I see that the <code>print ""1"";</code> I added is still there!<br>
Now in this checkout version I append another line in the file: <code>print ""2"";</code> and save the file.  </p>

<p>Then I do: <code>git checkout master</code>. This would take me back to the latest commit, right? So I expected that the last append I did <code>print ""2"";</code> would be lost.<br>
Well after I get: <code>Switched to branch 'master'</code> I see the file and it has both <code>print ""1"";</code> and <code>print ""2"";</code>.<br>
This is really confusing. Shouldn't I be seeing the latest commit now? And both the modifications should be lost?</p>",3,0,2013-05-20 11:49:58.093000 UTC,,2017-11-10 15:24:02.287000 UTC,1,git|version-control|dvcs|git-checkout,51,2010-07-06 15:28:06.603000 UTC,2019-08-24 14:27:35.867000 UTC,,51096,1580,21,3821,,,,,,[]
Failing to create Azure Databricks cluster because of unreachable instances,"<p>I'm trying to create a cluster in Azure Databricks and getting a such error messgae</p>

<pre><code>Resources were not reachable via SSH. If the problem persists, this usually indicates a network environment misconfiguration. Please check your cloud provider configuration, and make sure that Databricks control plane can reach Spark clusters instances.
</code></pre>

<p>I have such the default configuration:</p>

<p>Cluster mode: <code>Standard</code></p>

<p>Pool: <code>None</code></p>

<p>Runtime version: <code>5.5 LTS</code></p>

<p>Autoscaling enabled</p>

<p>Worker Type: <code>Standard_DS3_v2</code></p>

<p>Driver Type: <code>Standard_DS3_v2</code></p>

<p>From Logs Analytics I see Azure tried to create virtual machines and without any reason (I suppose because they were unreachable) had to delete all of them.</p>

<p>Did anyone face such issue?</p>",2,0,2019-10-23 14:51:46.717000 UTC,,,1,azure|azure-virtual-network|azure-databricks,2162,2010-12-03 11:53:40.213000 UTC,2022-03-05 00:13:00.713000 UTC,United States,4415,114,2,452,,,,,,[]
How do you execute a python Wheel Class/Method(not a script) in Azure Data Factory using an Azure Databricks activity?,"<p>Is it possible to execute a python Wheel Class/Method(not a script) in Azure Data Factory using an Azure Databricks activity like you would execute if it were a java packaged method in a .jar?  Unlike a script, this would have the ability to return a value(s), without doing something like burying them stdout.   </p>

<p>I haven't been able to search anything and I tried using the jar activity with no luck which didn't surprise me but worth a try.</p>

<p>If not, what I am looking for is a way to use Azure Databricks compute and return a small set of values back from the python job.  I have successfully used the ADF activity for databricks python script.</p>

<p>TIA!</p>",1,0,2020-01-10 14:07:29.713000 UTC,,,2,python|interop|azure-data-factory|azure-databricks|python-wheel,612,2013-06-20 21:18:56.280000 UTC,2022-01-28 12:46:32.663000 UTC,,81,0,0,3,,,,,,[]
How to Insert Data into table with select query in Databricks using spark temp table,"<p>I would like to insert the results of a Spark table into a new SQL Synapse table using SQL within Azure Data Bricks.</p>
<p>I have tried the following explanation [https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/language-manual/sql-ref-syntax-ddl-create-table-datasource] but I'm having no luck.</p>
<p>The Synapse table must be created as a result of a <strong>SELECT</strong> statement. The source should be a Spark / Data Bricks temporary view or Parquet source.</p>
<p>e.g. <strong>Temp Table</strong></p>
<pre><code>    # Load Taxi Location Data from Azure Synapse Analytics
        
        jdbcUrl = &quot;jdbc:sqlserver://synapsesqldbexample.database.windows.net:number;
database=SynapseDW&quot; #Replace &quot;suffix&quot; with your own  
        connectionProperties = {
          &quot;user&quot; : &quot;usernmae1&quot;,
          &quot;password&quot; : &quot;password2&quot;,
          &quot;driver&quot; : &quot;com.microsoft.sqlserver.jdbc.SQLServerDriver&quot;
        }
        
        pushdown_query = '(select * from NYC.TaxiLocationLookup) as t'
        dfLookupLocation = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)
        
        dfLookupLocation.createOrReplaceTempView('NYCTaxiLocation')
        
        display(dfLookupLocation)
</code></pre>
<p>e.g. <strong>Source Synapse DW</strong></p>
<p><em>Server:</em> synapsesqldbexample.database.windows.net</p>
<p><em>Database:</em>[SynapseDW]</p>
<p><em>Schema:</em> [NYC]</p>
<p><em>Table:</em> [TaxiLocationLookup]</p>
<p><strong>Sink / Destination Table (not yet in existence):</strong></p>
<p><em>Server:</em> synapsesqldbexample.database.windows.net</p>
<p><em>Database:</em>[SynapseDW]</p>
<p><em>Schema:</em> [NYC]</p>
<p><em>New Table:</em> [TEST_NYCTaxiData]</p>
<p><strong>SQL Statement I tried:</strong></p>
<pre><code>%sql
CREATE TABLE if not exists TEST_NYCTaxiLocation 
select *
from NYCTaxiLocation
limit 100
</code></pre>",2,10,2020-10-31 23:01:09.630000 UTC,2.0,,0,sql-server|apache-spark|create-table|azure-databricks|azure-synapse,2309,2019-08-11 06:54:05.713000 UTC,2021-01-23 13:00:03.407000 UTC,United Kingdom,11,0,0,3,,,,,,[]
Git show commit number,"<p>Is there a way to show index numbers of commit in </p>

<pre><code>    git log
</code></pre>

<p>command,
git log only show me SHA1 check sum.
Thanks</p>",1,3,2014-01-23 11:14:00.023000 UTC,,,1,git|version-control|dvcs,2068,2012-07-15 03:00:46.963000 UTC,2014-07-09 10:56:05.800000 UTC,,99,3,0,49,,,,,,[]
SQL statement to count all rows that changed from previous day for 'Column_a',"<p>I am looking for a SQL query to count all rows that changed from the previous day for 'column_a'. The table has one date column 'historyDate'.
Thanks in advance.</p>",1,0,2020-12-15 12:59:22.560000 UTC,,,0,sql|azure-databricks,16,2018-10-18 12:31:30.660000 UTC,2022-03-04 16:58:48.507000 UTC,"Chennai, Tamil Nadu, India",173,28,0,23,,,,,,[]
Converting Spark DF too Pandas DF and other way - Performance,"<p>Trying to convert Spark DF with 8m records to Pandas DF</p>

<pre><code>spark.conf.set(""spark.sql.execution.arrow.enabled"", ""true"")
sourcePandas = srcDF.select(""*"").toPandas()
</code></pre>

<p>Takes almost 2 minutes</p>

<p>And other way from Pandas to Spark DF </p>

<pre><code>finalDF = spark.createDataFrame(sourcePandas)
</code></pre>

<p>takes too long and never finishes.</p>

<p>sourcePandas</p>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 10 entries, 0 to 9
Data columns (total 42 columns):
CONSIGNMENT_PK     10 non-null int32
CERTIFICATE_NO     10 non-null object
ACTOR_NAME         10 non-null object
GENERATOR_FK       10 non-null int32
TRANSPORTER_FK     10 non-null int32
RECEIVER_FK        10 non-null int32
REC_POST_CODE      0 non-null object
WASTEDESC          10 non-null object
WASTE_FK           10 non-null int32
GEN_LICNUM         0 non-null object
VOLUME             10 non-null int32
MEASURE            10 non-null object
WASTE_TYPE         10 non-null object
WASTE_ADD          0 non-null object
CONTAMINENT1_FK    0 non-null float64
CONTAMINENT2_FK    0 non-null float64
CONTAMINENT3_FK    0 non-null float64
CONTAMINENT4_FK    0 non-null float64
TREATMENT_FK       10 non-null int32
ANZSICODE_FK       10 non-null int32
VEH1_REGNO         10 non-null object
VEH1_LICNO         0 non-null object
VEH2_REGNO         0 non-null object
VEH2_LICNO         0 non-null object
GEN_SIGNEE         0 non-null object
GEN_DATE           10 non-null datetime64[ns]
TRANS_SIGNEE       0 non-null object
TRANS_DATE         10 non-null datetime64[ns]
REC_SIGNEE         0 non-null object
REC_DATE           10 non-null datetime64[ns]
DATECREATED        10 non-null datetime64[ns]
DISCREPANCY        0 non-null object
APPROVAL_NUMBER    0 non-null object
TR_TYPE            10 non-null object
REC_WASTE_FK       10 non-null int32
REC_WASTE_TYPE     10 non-null object
REC_VOLUME         10 non-null int32
REC_MEASURE        10 non-null object
DATE_RECEIVED      10 non-null datetime64[ns]
DATE_SCANNED       0 non-null datetime64[ns]
HAS_IMAGE          10 non-null object
LASTMODIFIED       10 non-null datetime64[ns]
dtypes: datetime64[ns](7), float64(4), int32(10), object(21)
memory usage: 3.0+ KB
</code></pre>

<p>srcDF</p>

<pre><code>|-- CONSIGNMENT_PK: integer (nullable = true)
 |-- CERTIFICATE_NO: string (nullable = true)
 |-- ACTOR_NAME: string (nullable = true)
 |-- GENERATOR_FK: integer (nullable = true)
 |-- TRANSPORTER_FK: integer (nullable = true)
 |-- RECEIVER_FK: integer (nullable = true)
 |-- REC_POST_CODE: string (nullable = true)
 |-- WASTEDESC: string (nullable = true)
 |-- WASTE_FK: integer (nullable = true)
 |-- GEN_LICNUM: string (nullable = true)
 |-- VOLUME: integer (nullable = true)
 |-- MEASURE: string (nullable = true)
 |-- WASTE_TYPE: string (nullable = true)
 |-- WASTE_ADD: string (nullable = true)
 |-- CONTAMINENT1_FK: integer (nullable = true)
 |-- CONTAMINENT2_FK: integer (nullable = true)
 |-- CONTAMINENT3_FK: integer (nullable = true)
 |-- CONTAMINENT4_FK: integer (nullable = true)
 |-- TREATMENT_FK: integer (nullable = true)
 |-- ANZSICODE_FK: integer (nullable = true)
 |-- VEH1_REGNO: string (nullable = true)
 |-- VEH1_LICNO: string (nullable = true)
 |-- VEH2_REGNO: string (nullable = true)
 |-- VEH2_LICNO: string (nullable = true)
 |-- GEN_SIGNEE: string (nullable = true)
 |-- GEN_DATE: timestamp (nullable = true)
 |-- TRANS_SIGNEE: string (nullable = true)
 |-- TRANS_DATE: timestamp (nullable = true)
 |-- REC_SIGNEE: string (nullable = true)
 |-- REC_DATE: timestamp (nullable = true)
 |-- DATECREATED: timestamp (nullable = true)
 |-- DISCREPANCY: string (nullable = true)
 |-- APPROVAL_NUMBER: string (nullable = true)
 |-- TR_TYPE: string (nullable = true)
 |-- REC_WASTE_FK: integer (nullable = true)
 |-- REC_WASTE_TYPE: string (nullable = true)
 |-- REC_VOLUME: integer (nullable = true)
 |-- REC_MEASURE: string (nullable = true)
 |-- DATE_RECEIVED: timestamp (nullable = true)
 |-- DATE_SCANNED: timestamp (nullable = true)
 |-- HAS_IMAGE: string (nullable = true)
 |-- LASTMODIFIED: timestamp (nullable = true)
</code></pre>

<p>Cluster size</p>

<p><a href=""https://i.stack.imgur.com/rJp87.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rJp87.png"" alt=""enter image description here""></a></p>",1,2,2020-05-19 01:53:16.587000 UTC,,2020-05-19 09:11:20.797000 UTC,1,pandas|azure-databricks|pyspark-dataframes,40,2009-04-02 00:53:40.477000 UTC,2022-03-01 22:30:30.100000 UTC,"Brisbane QLD, Australia",27501,66,1,1101,,,,,,[]
acessing azure databricks data from kubernetes,"<p>I am running the following command from a kubernetes cluster to access a file from azure databricks</p>
<pre><code>spark-submit --packages io.delta:delta-core_2.12:0.7.0 --conf &quot;spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension&quot; --conf &quot;spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog&quot; --conf &quot;spark.delta.logStore.class=org.apache.spark.sql.delta.storage.HDFSLogStore&quot; script.py
</code></pre>
<p>I am getting this error.</p>
<pre><code>Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem not found
    at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)
    at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)
</code></pre>
<p>Do I need to install any jars from hadoop azure. Please guide me</p>",0,2,2021-03-03 11:39:11.240000 UTC,,,2,kubernetes|azure-databricks,83,2016-02-22 19:30:51.497000 UTC,2022-03-03 14:14:05.103000 UTC,United Kingdom,1792,54,0,252,,,,,,[]
How to connect Azure Data Factory with SQL Endpoints instead of interactive cluster?,<p>Is it possible to connect Azure Data Factory with Azure Databricks SQL Endpoints (Delta table and views) instead of interactive cluster. I tried with Azure delta lake connector but it has options for cluster and not Endpoints?</p>,1,3,2021-12-21 09:58:40.780000 UTC,,,1,azure|azure-data-factory|azure-databricks,50,2020-11-30 12:09:34.780000 UTC,2022-03-02 13:59:27.700000 UTC,"Hyderabad, Telangana, India",11,0,0,0,,,,,,[]
Repository fragmentation when targetting a project,"<p>What is relevant when you think about separating a project into several repositories or keeping it in a single repository with separated folders?</p>

<p>In a project with several different parts, those separated repositories would target the following components:</p>

<ul>
<li>Data structures</li>
<li>Core functionality</li>
<li>Sort of plugins family (one for each)</li>
<li>Different backends for some components (one for each)</li>
</ul>

<p>Also considering the option of making a github organisation to store those repositories, because a few of them would be somehow dependant on others. Furthermore, that would help in keeping the personal accout clear.</p>

<p>I've seen all sort of approaches when looking at other projects: from using one big repository to separated small ones. I'm considering going the fragmented way because it's somehow more organised, but it might be an overkill if those parts are not too big.</p>",1,0,2015-09-12 17:10:27.173000 UTC,,2015-09-12 17:19:45.310000 UTC,0,git|github|version-control|dvcs,52,2011-05-06 13:47:02.643000 UTC,2021-03-24 10:20:37.477000 UTC,,538,20,1,31,,,,,,[]
pyspark function understanding - conversion factor,"<p>I'm coding in PySpark on Apache Spark, Databricks.</p>
<p>I have a DataFrame <strong>DF</strong> and the DataFrame contains the following columns [A, B, C, D, E, F, G, H, I, J].</p>
<p>The following validates the dataframe has the required columns</p>
<pre><code>has_columns(very_large_dataframe, ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'])
</code></pre>
<p>There is a requirement to apply conversion factor of 2.5 to Column F i.e. Value 2, conversion factor 2.5 = 5.</p>
<p>The full context of the code is as follows:</p>
<blockquote>
<p>very_large_dataframe 250 GB of CSV files from client which must have
only 10 columns [A, B, C, D, E, F, G, H, I, J], [A, B] contains string
data [C, D, E, F, G, H, I, J], contains decimals with precision 5,
scale 2 (i.e. 125.75) [A, B, C, D, E], should not be null [F, G, H, I,
J] should may be null</p>
</blockquote>
<pre><code>very_large_dataset_location = '/Sourced/location_1'
very_large_dataframe = spark.read.csv(very_large_dataset_location, header=True, sep=&quot;\t&quot;)
</code></pre>
<p><strong>validate column count</strong></p>
<pre><code>if column_count(very_large_dataframe) != 10:
        raise Exception('Incorrect column count: ' + column_count(very_large_dataframe))
</code></pre>
<p><strong>validate that dataframe has all required columns</strong></p>
<pre><code>has_columns(very_large_dataframe, ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'])
</code></pre>
<p>However, I have never come across applying a conversion factor to column.</p>
<p>Is anyone familiar with applying a conversion factor with PySpark? (or any language for that matter)</p>",0,4,2022-01-14 21:29:42.643000 UTC,,2022-01-14 23:56:50.470000 UTC,0,apache-spark|pyspark|azure-databricks,44,2021-03-31 08:36:43.863000 UTC,2022-03-05 23:03:22.193000 UTC,"London, UK",651,58,0,93,,,,,,[]
Can I integrate Log Analytics with log4j2?,"<p><strong>Use-case:</strong> Running Spark job in Databricks and monitoring logs in LogAnalytics.</p>
<p><strong>Issue:</strong> In <em>Veracode</em> report, I am getting security vulnerabilities(<em>CRLF</em>) with <em>Slf4j</em> logger.</p>
<p>I am looking for other approaches, with <em>Log4j2</em> I don't see any issues.</p>
<p>Now my question is, I want to integrate <em><strong>LogAnalytics with Log4j2</strong></em>, is that possible in my case?</p>",0,1,2020-08-21 16:45:04.223000 UTC,,,2,log4j2|slf4j|azure-databricks|azure-log-analytics|veracode,141,2020-08-21 12:24:27.960000 UTC,2020-08-24 10:09:25.000000 UTC,,21,0,0,1,,,,,,[]
AWS Neptune HTTP REST change response serializer from GraphSON to JSON,"<p>I'm using this <a href=""https://docs.aws.amazon.com/neptune/latest/userguide/iam-auth-connecting-python.html"" rel=""nofollow noreferrer"">sample</a> to make HTTP REST queries from Lambda.</p>
<p>The output format named as <code>GraphSON</code></p>
<pre><code>{
  &quot;requestId&quot;: &quot;104803b-e5d-4e49-bad8-e95e32fe7f0&quot;,
  &quot;status&quot;: {
    &quot;message&quot;: &quot;&quot;,
    &quot;code&quot;: 200,
    &quot;attributes&quot;: {
      &quot;@type&quot;: &quot;g:Map&quot;,
      &quot;@value&quot;: []
    }
  },
  &quot;result&quot;: {
    &quot;data&quot;: {
      &quot;@type&quot;: &quot;g:List&quot;,
      &quot;@value&quot;: []
    },
    &quot;meta&quot;: {
      &quot;@type&quot;: &quot;g:Map&quot;,
      &quot;@value&quot;: []
    }
  }
}
</code></pre>
<p>and my general question how to specify <em>serializer</em> and get well-formatted <code>JSON</code> output instead of <code>GraphSON</code> without <em>g:Map</em>, <em>@value</em>, etc.</p>
<p>What I've tried: set particular values for <code>Content-Type</code>:</p>
<ul>
<li><code>application/json</code></li>
<li><code>application/x-amz-json-1.1</code></li>
<li><code>application/vnd.gremlin-v3.0+json</code></li>
</ul>
<p>Also I've tried to use values above as parameter for <code>serializer</code> in <code>GET</code> params, such as:</p>
<blockquote>
<p>https://{host}:{port}/gremlin/?gremlin={query}&amp;serializer=application/vnd.gremlin-v3.0+json</p>
</blockquote>
<p>But returned data format still GraphSON.</p>",1,0,2021-07-20 15:52:22.743000 UTC,,,0,gremlin|amazon-neptune,35,2015-07-14 07:38:54.847000 UTC,2022-02-23 20:28:15.570000 UTC,"Kyiv city, Ukraine",4226,1151,43,354,,,,,,[]
how to delete data from a delta file in databricks?,"<p>I want to delete data from a delta file in databricks.
Im using these commands<br />
Ex:</p>
<pre><code>PR=spark.read.format('delta').options(header=True).load('/mnt/landing/Base_Tables/EventHistory/')
PR.write.format(&quot;delta&quot;).mode('overwrite').saveAsTable('PR')
spark.sql('delete from PR where PR_Number=4600')
</code></pre>
<p>This is deleting data from the table but not from the actual delta file. And i want to delete the data in the file without using merge operation, because the join condition is not matching. Can anyone please help me in resolving this issue.</p>
<p>Thanks</p>",3,0,2020-12-07 10:03:02.377000 UTC,,,3,sql|pyspark|apache-spark-sql|azure-databricks|delta-lake,2443,2018-09-06 04:18:09.453000 UTC,2022-03-04 07:54:38.533000 UTC,"Hyderabad, Telangana, India",300,12,0,56,,,,,,[]
Which service I have to use from azure for real time streaming provided by azure?,"<p>I'm trying to do real-time analytics with Azure, and when I have gone through services, I have seen three services provided by Azure are HDInsight(Kafka), Azure stream Analytics, and Azure Events hub what are the services do I have to use.
I'm trying stream data on real-time either from SQL server or from twitter or some other and to store in on Azure Data warehouse or Data Lake.</p>",1,0,2019-07-11 10:07:48.693000 UTC,1.0,,0,apache-kafka|azure-hdinsight|azure-eventhub|azure-stream-analytics|azure-databricks,106,2016-07-23 12:23:27.930000 UTC,2022-03-04 17:17:58.200000 UTC,,351,12,0,82,,,,,,[]
Can I use databricks notebook to re-structure blobs and save it in another azure storage account?,"<p>I have incoming blobs in azure storage account for every day-hour, now I want to modulate the structure of the JSON inside the blobs and injest them into azure data lake.</p>

<p>I am using azure data factory and databricks.</p>

<p>Can Someone let me know how to proceed with it? I have mounted blob to databricks but now how to create a new structure and then do the mapping?</p>",0,0,2019-08-04 21:28:22.417000 UTC,,,1,dataframe|pyspark|jupyter-notebook|azure-data-factory|azure-databricks,33,2018-03-28 10:15:09.603000 UTC,2020-05-13 12:54:29.620000 UTC,,59,1,0,17,,,,,,[]
Run a spark scala script in a remote cluster setup on Azure databricks,<p>I have written a spark scala(sbt) application in intelliJ which I want to run on a remote cluster hosted on Azure databricks. What all steps to follow to avoid manually uploading jars into dbfs always to test the code</p>,1,0,2021-12-23 08:50:23.517000 UTC,,,0,azure|apache-spark|azure-databricks|databricks-connect,40,2018-01-03 09:41:01.033000 UTC,2022-03-04 18:48:23.360000 UTC,"Hyderabad, Telangana, India",43,3,0,24,,,,,,[]
Azure Data Factory appending large number of files having different schema from csv files,"<p>We have 500 CSV files uploaded to an Azure storage container. These files use 4 different schemas, meaning that they have few different columns and some columns are common across all files.</p>
<p>We are using ADF and schema drift to map columns in sink and source and be able to wrote the files.</p>
<p>But this is not working and it only uses schema for the 1st file it processes for every file and this is causing data issues. Please advise on this issue.</p>
<p>We ran the pipeline for three scenarios but the issue is not resolved. Same issue as mentioned below occurring in all three cases:</p>
<p>1.Incorrect Mapping i.e. the Description and PayClass from A type get mapped to WBSname and Activity Name
2. If one less column in one of file (missing column) that also disturbs the mapping i.e. one files does not have resource type that maps Group incorrectly to other column.</p>
<p>Case 1
No Schema Drift at source and sink
Empty Dummy File with all columns created and uploaded at source
Derived table with column Pattern</p>
<p>Case 2 :
Schema Drift at source and sink
Dummy File with all columns created and uploaded at source
Derived table with column Pattern</p>
<p>Case 3 :Schema Drift at Source /No Schema Drift at Sink
Dummy File with all columns created and uploaded at source
Derived table with column Pattern</p>",1,0,2020-11-26 05:19:45.533000 UTC,1.0,,2,azure|azure-sql-database|azure-data-factory|azure-databricks|azure-synapse,399,2020-11-26 04:56:08.377000 UTC,2020-12-01 05:21:02.620000 UTC,India,21,0,0,1,,,,,,[]
How to connect local and remote Mercurial repos?,"<p>Typically, in Mercurial, I create a new project by:</p>

<ol>
<li>Create a new remote repo</li>
<li>Clone the repo locally</li>
<li>Make changes to the local repo</li>
<li>Push those changes to the remote repo</li>
</ol>

<p>The ""remote repo"" here is actually our ""central/originating"" DVCS server (<code>http://ourhg.ourorg.example.com</code>, etc.).</p>

<p>I am now in a situation where I had to use a code generation tool to produce the source code for a simple web app. So the source code exists <em>before</em> the remote repo exists on our hg server. I'm looking for the exact shell commands I need to execute to get this properly pushed to the remote repo.</p>

<p>I <em>believe</em> it should be something like this:</p>

<ol>
<li>Use the code generator to generate the code, say, at <code>/home/myuser/myapp</code>.</li>
<li>Initialize an hg repo for <code>myapp</code> locally on my machine (<code>hg init</code>)</li>
<li>Add the generated source code for <code>myapp</code> to this local repo (<code>hg add</code>, then <code>hg commit</code>)</li>
<li>On <code>ourhg.ourorg.example.com</code>, create the new remote repo (manual steps)</li>
<li><strong>???</strong></li>
<li>Push the changes sitting in my local repo to the remote repo (<code>hg push</code>)</li>
</ol>

<p>I <em>know</em> there is something missing in between Step #4 (creation of the remote repo) and Step #6 (pushing to the remote repo). There surely needs to be some ""connection"" step where my local repo and the remote repo realize they represent the same project/source code/etc. This is my hangup here, so I ask: <strong>what is Step #5</strong>?</p>",3,4,2014-07-18 18:36:52.233000 UTC,,,0,linux|version-control|mercurial|dvcs,508,2011-08-12 15:08:13.160000 UTC,2014-12-24 16:06:51.757000 UTC,Uranus,52373,783,37,4060,,,,,,[]
How to do a combined boolean operation in neptune using gremlin and python?,"<p>I am trying to do a query where I have multiple boolean operations to be done but can't figure out how to do it.</p>
<p>The query is something like <code>(A and B) or (C and D)</code></p>
<p>I first tried</p>
<pre><code>g.V()\
.has(&quot;attra&quot;, P.lte(20))\
.has(&quot;attrb&quot;, P.gte(10))\
.or_()\
.has(&quot;attrc&quot;, P.lte(20))\
.has(&quot;attrd&quot;, P.gte(10))
</code></pre>
<p>but it turns out anything after the <code>or_()</code> in the query is <code>or</code>ed and that is not what I want. Because I have other complex boolean logic down the line as well.</p>
<p>and I also tried</p>
<pre><code>g.V()\
.or_(
  has(&quot;attra&quot;, P.lte(20)).and().has(&quot;attrb&quot;, P.gte(10)),
  has(&quot;attrc&quot;, P.lte(20)).and().has(&quot;attrd&quot;, P.gte(10))
)
</code></pre>
<p>but it says that <code>has</code> is not defined. Is this how you do it? Where is this has even defined?</p>
<p>Help would be really appreciated</p>
<p>EDIT: I have the following imports in my file</p>
<pre><code>from __future__ import print_function
from gremlin_python.structure.graph import Graph
from gremlin_python.process.strategies import *
from gremlin_python.process.traversal import *
from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection
from gremlin_python.process.graph_traversal import __ as AnonymousTraversal
</code></pre>",1,0,2020-09-26 01:01:29.220000 UTC,,,0,python|gremlin|amazon-neptune,65,2014-11-20 03:05:50.800000 UTC,2022-03-04 18:04:09.467000 UTC,"Huntsville, AL, USA",3320,70,31,370,,,,,,[]
Overwriting log4j property with my configuration fails to start the databricks cluster,"<p>I want  redirect spark jobs (written  in java)logs to Azure  Application Insights.For that I am trying to overwrite log4j default property executor file in azure databricks cluster with my configuration.I have  created an init  script like below  so the app insights appender is added to default property file of executors in spark.
dbutils.fs.put(&quot;/databricks/init_scripts/ACDP-Dev/verbose_logging.sh&quot;, &quot;&quot;&quot;
#!/bin/bash
log4j.appender.aiAppender=com.microsoft.applicationinsights.log4j.v1_2.ApplicationInsightsAppender
log4j.appender.aiAppender.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n</p>
<p>log4j.logger.com.varian.uap.Main=TRACE, aiAppender&quot; &gt;&gt; /home/ubuntu/databricks/spark/dbconf/log4j/executor/log4j.properties
&quot;&quot;&quot;, True)</p>
<p>When I restart the cluster ,the script fails to execute. Please let me know if I am missing something in the script or if the appenders are right.</p>",0,0,2021-03-15 07:45:09.127000 UTC,,2021-03-16 09:20:46.337000 UTC,0,apache-spark|logging|azure-application-insights|azure-databricks,133,2021-01-29 04:43:48.793000 UTC,2021-03-30 07:24:32.597000 UTC,"Pune, Maharashtra, India",1,0,0,9,,,,,,[]
Connecting OneDrive data to Azure Databricks,<p>I have created an Azure Databricks cluster and would like to connect to a SharePoint folder to read and upload files. I cannot seem to find any solution to this. Please advise.</p>,0,0,2020-06-17 11:24:09.237000 UTC,,2021-05-24 21:10:41.577000 UTC,2,azure|sharepoint|onedrive|azure-databricks,657,2020-06-17 11:17:31.803000 UTC,2020-09-24 15:30:36.423000 UTC,,21,0,0,4,,,,,,[]
Is there a way to show visual diff from a Mercurial Web Server?,"<p>A Mercurial Server can be started up just by</p>

<pre><code>hg serve
</code></pre>

<p>and going to</p>

<pre><code>localhost:8000
</code></pre>

<p>but the diff is a text type of diff.  I wonder if there is any way to get a visual diff right from the server (probably by clicking on a file or a changeset), the type of visual diff that is like on kdiff3 or Tortoise SVN diff.  If it can be done, that should be quite cool.</p>",1,0,2010-08-03 02:14:38.680000 UTC,,2010-08-03 02:27:05.280000 UTC,2,mercurial|dvcs|hgserve,391,2009-05-09 15:50:29.477000 UTC,2022-03-04 09:41:10.460000 UTC,,137341,1445,39,12817,,,,,,[]
Azure Databricks externalize metastore - MSFT Script not running,"<p>I am trying to set up azure databricks with external hive metastore on AzureSQL.
While doing the setup, I created Azure SQL. And now I have to run a MSFT given sql <a href=""https://github.com/apache/hive/blob/master/metastore/scripts/upgrade/mssql/hive-schema-2.3.0.mssql.sql"" rel=""nofollow noreferrer"">script</a> which has table and indices creation sql.</p>
<p>When I ran it was able to create new tables but failed in Index creation. I have full access on Database. May be some grant is missing. Also why MSFT or Databrick has such lengthy process?</p>
<p>OR if there a better way to externalize metedata.Please help.</p>",1,1,2021-08-09 21:59:58.477000 UTC,1.0,,0,azure|azure-sql-database|azure-databricks,116,2013-04-18 17:00:15.787000 UTC,2022-03-04 22:41:54.547000 UTC,,453,11,0,104,,,,,,[]
Mercurial project hosting,"<p>I need a project hosting with:</p>

<ul>
<li>mercurial support;</li>
<li>lightweight, but usable issue tracker (github`s one is pretty close);</li>
<li>both public and private repositories/trackers/wiki and so on support;</li>
<li>cool customizable dashboard;</li>
<li>API and hooks (optional, but nice to have).</li>
</ul>

<p>I've found <a href=""http://codebasehq.com"" rel=""nofollow"">codebasehq</a> which looks promising, but It doesn't allow to publish anything, but commit history.</p>

<p>Some points:</p>

<ul>
<li>I'm fine with paying for good project hosting;</li>
<li>bitbucket is not an option (huge list of reasons is omitted);</li>
<li>usability and reliability is the most important things for me.</li>
</ul>

<p>Thanks in advance.</p>

<p><strong>UPD1: Primary reasons I want move from bitbucket:</strong></p>

<ul>
<li>totally unusable issue tracker: it makes me cry every time I use one;</li>
<li>lack of <a href=""https://bitbucket.org/site/master/issue/4580/list-of-my-issues"" rel=""nofollow"">list of my assigned tickets for all projects</a>, suggested solution seems early alpha (have accessibility problems and lack basic features);</li>
<li>not well-designed notification system (sends me notifications about mine comments and so on)</li>
<li>dashboard sucks;</li>
<li>slow and ""wonfixish"" reaction (IMHO) on feature requests and bug reports.</li>
</ul>

<p><strong>UPD2:</strong></p>

<p>Assembla is more appropriate project hosing then fogbugz + klin according my criteria
list, so I'm granting bounty to @lazybadger`s answer, but not accepting it, because Assembla works very bad with mercurial repositories and doesn't care about users.</p>

<p>Thanks for your suggestions, guys!</p>",3,2,2012-09-13 19:12:11.217000 UTC,,2012-09-23 18:50:25.097000 UTC,0,web-applications|mercurial|dvcs|project-hosting,192,2010-11-13 20:23:33.147000 UTC,2015-12-23 19:59:26.337000 UTC,,252,76,0,80,,,,,,[]
Can AWS Glue catalog point to a data location in Azure ADLS?,"<p>We are trying configure AWS Databricks Runtime to use the AWS Glue Data Catalog as its metastore. In this environment ,Azure ADLS is one of the source system.In that case,Can AWS Glue catalog point to a data location in Azure ADLS?</p>",1,0,2021-03-03 22:17:10.380000 UTC,,,0,azure-storage|azure-databricks|aws-glue-data-catalog|aws-databricks,45,2018-04-04 20:47:38.883000 UTC,2022-03-04 22:08:30.013000 UTC,"San Antonio, TX, USA",1183,1,8,266,,,,,,[]
Writing parquet file throws...An HTTP header that's mandatory for this request is not specified,"<p>I have two ADLSv2 storage accounts, both are hierarchical namespace enabled.
In my Python Notebook, I'm reading a CSV file from one storage account and writing as parquet file in another storage, after some enrichment.</p>
<p>I am getting below error when writing the parquet file...</p>
<pre><code>StatusCode=400, An HTTP header that's mandatory for this request is not 
</code></pre>
<p>Any help is greatly appreciated.</p>
<p>Below is my Notebook code snippet...</p>
<pre><code># Databricks notebook source
# MAGIC %python
# MAGIC 
# MAGIC STAGING_MOUNTPOINT = &quot;/mnt/inputfiles&quot;
# MAGIC if STAGING_MOUNTPOINT in [mnt.mountPoint for mnt in dbutils.fs.mounts()]:
# MAGIC   dbutils.fs.unmount(STAGING_MOUNTPOINT)
# MAGIC 
# MAGIC PERM_MOUNTPOINT = &quot;/mnt/outputfiles&quot;
# MAGIC if PERM_MOUNTPOINT in [mnt.mountPoint for mnt in dbutils.fs.mounts()]:
# MAGIC   dbutils.fs.unmount(PERM_MOUNTPOINT)

STAGING_STORAGE_ACCOUNT = &quot;--------&quot;
STAGING_CONTAINER = &quot;--------&quot;
STAGING_FOLDER = --------&quot;
PERM_STORAGE_ACCOUNT = &quot;--------&quot;
PERM_CONTAINER = &quot;--------&quot;

configs = {
 &quot;fs.azure.account.auth.type&quot;: &quot;OAuth&quot;,
 &quot;fs.azure.account.oauth.provider.type&quot;: 
 &quot;org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider&quot;,
 &quot;fs.azure.account.oauth2.client.id&quot;: &quot;#####################&quot;,
 &quot;fs.azure.account.oauth2.client.secret&quot;: dbutils.secrets.get(scope=&quot;DemoScope&quot;,key=&quot;DemoSecret&quot;),
 &quot;fs.azure.account.oauth2.client.endpoint&quot;: 
 &quot;https://login.microsoftonline.com/**********************/oauth2/token&quot;}

STAGING_SOURCE = 
&quot;abfss://{container}@{storage_acct}.blob.core.windows.net/&quot;.format(container=STAGING_CONTAINER, 
storage_acct=STAGING_STORAGE_ACCOUNT)

try:
 dbutils.fs.mount(
  source=STAGING_SOURCE,
  mount_point=STAGING_MOUNTPOINT,
  extra_configs=configs)
except Exception as e:
 if &quot;Directory already mounted&quot; in str(e):
 pass # Ignore error if already mounted.
else:
 raise e

print(&quot;Staging Storage mount Success.&quot;)

inputDemoFile = &quot;{}/{}/demo.csv&quot;.format(STAGING_MOUNTPOINT, STAGING_FOLDER)
readDF = (spark
          .read.option(&quot;header&quot;, True)
          .schema(inputSchema)
          .option(&quot;inferSchema&quot;, True)
          .csv(inputDemoFile))

LANDING_SOURCE = 
 &quot;abfss://{container}@{storage_acct}.blob.core.windows.net/&quot;.format(container=LANDING_CONTAINER, 
 storage_acct=PERM_STORAGE_ACCOUNT)

try:
 dbutils.fs.mount(
 source=PERM_SOURCE,
 mount_point=PERM_MOUNTPOINT,
 extra_configs=configs)
except Exception as e:
 if &quot;Directory already mounted&quot; in str(e):
  pass # Ignore error if already mounted.
 else:
  raise e

print(&quot;Landing Storage mount Success.&quot;)

outPatientsFile = &quot;{}/patients.parquet&quot;.format(outPatientsFilePath)
print(&quot;Writing to parquet file: &quot; + outPatientsFile)

***Below call is failing…error is 
StatusCode=400
StatusDescription=An HTTP header that's mandatory for this request is not specified.
ErrorCode=
ErrorMessage=***

(readDF
 .coalesce(1)
 .write
 .mode(&quot;overwrite&quot;)
 .option(&quot;header&quot;, &quot;true&quot;)
 .option(&quot;compression&quot;, &quot;snappy&quot;)
 .parquet(outPatientsFile)
)
</code></pre>",2,5,2020-10-12 23:33:04.997000 UTC,,,0,parquet|azure-databricks|azure-blob-storage|spark-notebook,513,2013-11-23 02:42:25.790000 UTC,2021-11-03 17:05:24.283000 UTC,,81,0,0,42,,,,,,[]
Gremlin NodeJS query returns different results than Groovy,"<p>I have a query that I originally wrote in the console:</p>
<pre><code>    g.V().hasLabel('group')
      .has('type', 'PowerUsers')
      .local(__.union(
         __.project('group').by(__.valueMap().by(__.unfold())),
         __.inE().outV().project('user').by(__.valueMap().by(__.unfold())))
      .fold()).unfold().toList()
</code></pre>
<p>I get something like:</p>
<pre><code>==&gt;{group={owner=A, group_id=21651399-91fd-4da4-8608-1bd30447e773, name=Group 8, type=PowerUsers}}
==&gt;{user={name=John, user_id=91f5e306-77f1-4aa1-b9d0-23136f57142d}}
==&gt;{user={name=Jane, user_id=7f133d0d-47f3-479d-b6e7-5191bea52459}}
==&gt;{group={owner=A, group_id=ef8c81f7-7066-49b2-9a03-bad731676a8c, name=Group B, type=PowerUsers}}
==&gt;{user={name=Max, user_id=acf6abb8-08b3-4fc6-a4cb-f34ff523d628}}
==&gt;{group={owner=A, group_id=07dff798-d6db-4765-8d74-0c7be66bec05, name=Group C, type=PowerUsers}}
==&gt;{user={name=John, user_id=91f5e306-77f1-4aa1-b9d0-23136f57142d}}
==&gt;{user={name=Max, user_id=acf6abb8-08b3-4fc6-a4cb-f34ff523d628}}
</code></pre>
<p>When I run that query with NodeJS, I was expecting to get a similar result, but I don't. I get something like this:</p>
<pre><code>[ { group:
     { owner: 'A',
       group_id: '21651399-91fd-4da4-8608-1bd30447e773',
       name: 'Group 8',
       type: 'PowerUsers' } },
  { user:
     { name: 'John',
       user_id: '91f5e306-77f1-4aa1-b9d0-23136f57142d'} },
  { user:
     { name: 'John',
       user_id: '91f5e306-77f1-4aa1-b9d0-23136f57142d'} },
  { user:
     { name: 'Jane',
       user_id: '7f133d0d-47f3-479d-b6e7-5191bea52459'} },
  { user:
     { name: 'Jane',
       user_id: '7f133d0d-47f3-479d-b6e7-5191bea52459'} },
  { group:
     { owner: 'A',
       group_id: 'ef8c81f7-7066-49b2-9a03-bad731676a8c',
       name: 'Group B',
       type: 'PowerUsers' } },
  { user:
     { name: 'Max',
       user_id: 'acf6abb8-08b3-4fc6-a4cb-f34ff523d628' } },
  ...
</code></pre>
<p>Because I have the same users in different groups, I can't use dedup(), and if the results where the same in NodeJS as Groovy, that'd be perfect. Unfortunately, they are not, and I don't understand why the results in NodeJS are all messed up, considering that the query is exactly the same</p>",1,4,2020-11-21 00:13:36.993000 UTC,,2020-11-27 11:18:50.900000 UTC,1,node.js|gremlin|amazon-neptune,89,2013-07-16 20:58:07.570000 UTC,2022-03-04 01:20:13.760000 UTC,,1005,18,2,53,,,,,,[]
how to comparing dataset to new data coming every week using SQL in Databricks,"<p>I want to find the number of ids changing their value every week and month</p>
<p>every week a new dataset get entered into the database, every week same ids are added with their values, some weeks values for some ids change.</p>
<p>I want to find the amount of ids change per month and week for all the data I have over the last 2 years.</p>
<p>all of this is being done in databricks</p>
<p>I have attached an example data-set, where data is entered for 3 ids for two months and 2 ids changed their value. the desired output shows what I need is that the second month showing 2 changes and the first showing 0 changes.</p>
<p>dataset</p>
<p><a href=""https://i.stack.imgur.com/x8R7l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x8R7l.png"" alt="""" /></a></p>
<p>output needed</p>
<p><a href=""https://i.stack.imgur.com/k0iQU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/k0iQU.png"" alt="""" /></a></p>",1,3,2021-08-10 16:56:03.470000 UTC,,2021-08-19 07:26:55.727000 UTC,0,sql|databricks-sql-analytics,72,2021-02-20 21:21:11.720000 UTC,2021-11-09 14:20:59.240000 UTC,"England, UK",1,0,0,3,,,,,,[]
Read JSON file in pyspark to create schema struct type in python,"<p>This is in a Microsoft Azure data lake running on azure databricks.  I'm trying to read a JSON file, that I did not create, which has the schema, or name and type information for CSV's that I can read, but have no header in the CSV.</p>
<pre><code>df1 = spark.read.json('/mnt/jsontest/...PATH.../SalesTable.cdm.json', multiLine=True)
df1.printSchema() 
</code></pre>
<p>That loads a pyspark DataFrame and prints this.</p>
<p>What I want is Struct Schema I can use to read the CSV files.
I haven't been able to get a dataframe of the definitions array, that I could loop and create the struct elements.</p>
<pre><code>root
 |-- definitions: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- displayName: string (nullable = true)
 |    |    |-- entityName: string (nullable = true)
 |    |    |-- hasAttributes: array (nullable = true)
 |    |    |    |-- element: struct (containsNull = true)
 |    |    |    |    |-- dataFormat: string (nullable = true)
 |    |    |    |    |-- name: string (nullable = true)
 |    |    |    |    |-- purpose: string (nullable = true)
 |-- imports: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- corpusPath: string (nullable = true)
 |-- jsonSchemaSemanticVersion: string (nullable = true)
</code></pre>
<p>If I could loop the hasAttributes, I would have the dataFormat and name field I think to make the schema structure.
I also can't read the json, directly as json. Its always a DataFrame.  If I try to use a normal python3 read, the file can't be found.
What is the best most normal way to traverse a json to get to the fields I want?</p>
<p>Most examples do something like this - new_schema = StructType.fromJson(first_json)
But because the JSON I have doesn't have fields, I get an error. hasAttributes is where fields typically is or contains generally the same information.</p>
<p>Any help in how to create the struct or read this json as json, or format (explode/flatten) the DataFrame would be great.  I'd think there must be something more simple in pyspark to do something so simple in base python3.</p>",0,7,2021-10-25 19:21:09.493000 UTC,,2021-10-25 19:44:18.733000 UTC,0,python|json|azure|pyspark|azure-databricks,182,2012-08-01 03:19:00.723000 UTC,2022-02-14 19:37:10.600000 UTC,,1,0,0,1,,,,,,[]
Gremlin Concurrent Modification Exception on Neptune,"<p>I have seen many concurrent modification exceptions emerging since I changed a small Gremlin operation in a .Net setting, with Neptune as a the back-end.  I'm struggling to understand what could be causing it.</p>
<p>This is my new query</p>
<pre><code>await Graph.V(itemId)
                .ActiveLinkedVertex(DbLabels.ComponentEdge)
                .OutE(DbLabels.ComponentEdge).HasNot(DbLabels.EdgeToProperty)
                .MarkToDate(_operationTime)
                .Promise(t =&gt; t.Iterate());
</code></pre>
<p>and this is my ActiveLinkedVertex extension</p>
<pre><code>public static GraphTraversal&lt;Vertex, Vertex&gt; ActiveLinkedVertex(this GraphTraversal&lt;Vertex, Vertex&gt; traversal, string edgeLabel)
{
            traversal = traversal.OutE(edgeLabel)
                .HasNot(DbLabels.EdgeToProperty)
                .InV();

            return traversal;
}
</code></pre>
<p>And here's my 'MarkToDate'</p>
<pre><code>public static GraphTraversal&lt;S, E&gt; MarkToDate&lt;S, E&gt;(this GraphTraversal&lt;S, E&gt; traversal, DateTime utcUpdateTime)
{
            traversal = traversal.Property(DbLabels.EdgeToProperty, utcUpdateTime).Property(DbLabels.EdgeToTicksProperty, utcUpdateTime.Ticks);
            return traversal;
}
   
</code></pre>
<p>The query is getting hit in a multithreaded setting, but never on the same itemId or even itemId's parent.  Each thread is working on an isolated subgraph.</p>
<p>Is there anything in the internal implementation of Neptune/Gremlin that I'm missing that could cause a broad lock to be taken, for example, or is my query open to concurrent mods in some way?</p>
<p><strong>Additional Information</strong> (edit)</p>
<p>The new query replaced a buggy version, that was</p>
<pre><code>await Graph.V(itemId)
                .Out(DbLabels.ComponentEdge)
                .OutE(DbLabels.ComponentEdge)
                .MarkToDate(_operationTime)
                .Promise(t =&gt; t.Iterate());
</code></pre>
<p>This is is simplified view of what the graph is.  Everything below a group is being done on a separate thread and in-order.  There can be x-links between Vertices labelled as 'Part' in different groups, but the edges we're mutating are the Component edges between the green and purple verts at the bottom (MarkToDate on the edges)</p>
<p>I've read on the Neptune docs that range locks taken on neighbouring nodes can cause CMEs, but I am finding it hard to understand whether a CME is possible when mutating the edges at the bottom below, under separate Groups.  If I put back the original (bugged) update, then there are no CMEs.</p>
<p><a href=""https://i.stack.imgur.com/bWSlC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bWSlC.png"" alt=""enter image description here"" /></a></p>",1,4,2022-01-24 11:23:42.853000 UTC,,2022-01-27 01:41:32.747000 UTC,0,gremlin|amazon-neptune,70,2016-05-05 08:35:43.540000 UTC,2022-03-04 13:23:57.727000 UTC,,27,5,0,6,,,,,,[]
Databricks fails accessing a Data Lake Gen1 while trying to enumerate a directory,"<p>I am using (well... trying to use) Azure Databricks and I have created a notebook.</p>

<p>I would like the notebook to connect my Azure Data Lake (Gen1) and transform the data. I followed the <a href=""https://docs.azuredatabricks.net/spark/latest/data-sources/azure/azure-datalake.html#access-adls-using-the-dataframe-api"" rel=""nofollow noreferrer"">documentation</a> and put the code in the first cell of my notebook: </p>

<pre><code>spark.conf.set(""dfs.adls.oauth2.access.token.provider.type"", ""ClientCredential"")
spark.conf.set(""dfs.adls.oauth2.client.id"", ""**using the application ID of the registered application**"")
spark.conf.set(""dfs.adls.oauth2.credential"", ""**using one of the registered application keys**"")
spark.conf.set(""dfs.adls.oauth2.refresh.url"", ""https://login.microsoftonline.com/**using my-tenant-id**/oauth2/token"")

dbutils.fs.ls(""adl://**using my data lake uri**.azuredatalakestore.net/tenantdata/events"")
</code></pre>

<p>The execution fails with this error:</p>

<blockquote>
  <p>com.microsoft.azure.datalake.store.ADLException: Error enumerating
  directory /</p>
  
  <p>Operation null failed with exception java.io.IOException : Server
  returned HTTP response code: 400 for URL:
  <a href=""https://login.microsoftonline.com/"" rel=""nofollow noreferrer"">https://login.microsoftonline.com/</a><strong>using my-tenant-id</strong>/oauth2/token
  Last encountered exception thrown after 5 tries.</p>
  
  <p>[java.io.IOException,java.io.IOException,java.io.IOException,java.io.IOException,java.io.IOException]
  [ServerRequestId:null]    at
  com.microsoft.azure.datalake.store.ADLStoreClient.getExceptionFromResponse(ADLStoreClient.java:1169)
    at
  com.microsoft.azure.datalake.store.ADLStoreClient.enumerateDirectoryInternal(ADLStoreClient.java:558)
    at
  com.microsoft.azure.datalake.store.ADLStoreClient.enumerateDirectory(ADLStoreClient.java:534)
    at
  com.microsoft.azure.datalake.store.ADLStoreClient.enumerateDirectory(ADLStoreClient.java:398)
    at
  com.microsoft.azure.datalake.store.ADLStoreClient.enumerateDirectory(ADLStoreClient.java:384)</p>
</blockquote>

<p>I have given the registered application the <code>Reader</code> role to the Data Lake:</p>

<p><a href=""https://i.stack.imgur.com/2AZDm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2AZDm.png"" alt=""enter image description here""></a></p>

<p><strong>Question</strong></p>

<p>How can I allow Spark to access the Data Lake?</p>

<p><strong>Update</strong></p>

<p>I have granted both the <code>tenantdata</code> and <code>events</code> folders <code>Read</code> and <code>Execute</code> access:</p>

<p><a href=""https://i.stack.imgur.com/jiHd8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jiHd8.jpg"" alt=""Granted persmissions on folder""></a></p>",1,1,2019-03-22 12:52:50.117000 UTC,,2019-04-01 12:01:00.780000 UTC,0,scala|azure|apache-spark|azure-data-lake|azure-databricks,2759,2014-11-15 21:41:52.010000 UTC,2022-03-05 23:35:18.467000 UTC,"Montréal, QC, Canada",6248,797,24,742,,,,,,[]
Write data to specific partitions in Azure Dedicated SQL pool,"<p>At the moment ,we are using steps in the below article to do a full load of the data from one of our spark data sources(delta lake table) and write them to a table on SQL DW.</p>
<p><a href=""https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/synapse-analytics"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/synapse-analytics</a>
Specifically, the write is carried out using,</p>
<pre><code>df.write \
  .format(&quot;com.databricks.spark.sqldw&quot;) \
  .option(&quot;url&quot;, &quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;) \
  .option(&quot;forwardSparkAzureStorageCredentials&quot;, &quot;true&quot;) \
  .option(&quot;dbTable&quot;, &quot;&lt;your-table-name&gt;&quot;) \
  .option(&quot;tempDir&quot;, &quot;wasbs://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.blob.core.windows.net/&lt;your-directory-name&gt;&quot;) \
  .option(&quot;maxStrLength&quot;,4000).mode(&quot;overwrite&quot;).save()
</code></pre>
<p>Now,our source data,by virture of it being a delta lake, is partitioned on the basis of countryid. And we would to load/refresh only certain partitions to the SQL DWH, instead of the full drop table and load(because we specify &quot;overwrite&quot;) that is happening now.I tried adding an adding a additional option (partitionBy,countryid) to the above script,but that doesnt seem to work.</p>
<p>Also the above article doesn't mention partitioning.</p>
<p>How do I work around this?</p>",1,0,2021-09-21 16:15:55.043000 UTC,0.0,2021-09-26 06:14:36.730000 UTC,0,apache-spark|azure-sql-database|azure-databricks|azure-synapse|azure-sqldw,251,2019-08-17 17:37:31.640000 UTC,2022-03-04 18:02:15.673000 UTC,India,147,80,0,33,,,,,,[]
Nul sign when saving .tab delimited file in databricks,"<p>I have csv template saved on ADL. I'm reading that template and I'm appending some data from data frame and save it like tab delimited file. This is working but I have this Nul signs at the beginning and end of each row. How can I save this without these signs.
<a href=""https://i.stack.imgur.com/caDjV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/caDjV.png"" alt=""Saved result with NUL sign""></a></p>

<p>Part of the code which I'm using:</p>

<pre><code>    def saveToSingleTxt(df, saveLocation):
      txtLocation = saveLocation+'temp.folder'
      fileLocation = saveLocation

      df.coalesce(1).write.format(""csv"").option(""header"", ""false"").option(""delimiter"",""\t"").option(""quote"", ""\u0000"").option(""charset"",""UTF-8"").mode(""overwrite"").save(txtLocation)

      file = dbutils.fs.ls(txtLocation)[-1].path  
      dbutils.fs.cp(file, fileLocation)
      dbutils.fs.rm(txtLocation, recurse=True)

df = getDimContent(rootPayLoadDataV)
dftempV28 = sqlContext.read.format(""com.databricks.spark.csv"").option(""header"", ""false"").load(sourceADLSFullPathTemp + fileNameTempV28)
dftempV28 = dftempV28.replace('## SC\tD:YYYY-MM-DD hh:mm:ss\tA:300092594:128', '## SC\tD:' + str(curentDate.strftime(""%Y-%m-%d %H:%M:%S"")) + '\tA:300092594:128')

df = df.withColumn(""KeyContentName"", concat(df[""ContentID""], lit(""\t""), df[""ContentName""]))

appendV28 = dftempV28.union(df.select(df[""KeyContentName""]))

saveToSingleTxt(df, destinationADLSFullPath + fileNameV28)
</code></pre>

<p>How can I save this without this NUL ('\x00' hexadecimal) sign?
Thanks!</p>",1,1,2020-01-16 10:18:37.467000 UTC,,,0,azure-databricks,49,2018-04-25 11:58:40.420000 UTC,2022-03-03 14:36:02.647000 UTC,"Belgrade, Serbia",81,1,0,8,,,,,,[]
Iterating through a particular column values in dataframes using pyspark in azure databricks,<p>Hi is it possible to iterate through the values in the dataframe using pyspark code in databricks notebook?</p>,1,0,2021-12-08 16:15:04.670000 UTC,,,0,pyspark|azure-databricks,29,2021-12-08 14:19:16.330000 UTC,2021-12-09 09:42:20.823000 UTC,,1,0,0,2,,,,,,[]
Create Azure Keyvault backed secret scope for Databricks,<p>I need to create a secret scope for Azure Databricks workspace backed by Azure keyvault. How can i get it created.</p>,1,0,2020-05-17 14:40:58.797000 UTC,,,-2,azure|azure-databricks,295,2020-05-17 11:43:11.237000 UTC,2020-05-21 11:36:41.567000 UTC,,1,0,0,0,,,,,,[]
git cherry confusion - doesn't work as described in doc,"<p>Documentation says: ""Because git-cherry compares the changeset rather than the commit id (sha1), you can use git-cherry to find out if a commit you made locally has been applied  under a different commit id.""</p>

<p>Let's see: </p>

<pre><code>$ git cherry master release-1.1.0 | head -1
- 533e2559342910fbffa2be5b38fdd7f2ddb2ed53
$ git show 533e2559342910fbffa2be5b38fdd7f2ddb2ed53
...
(cherry picked from commit 409c61b3304373a73c787fdf9c08cc338934b74d)
...
</code></pre>

<p>git show shows the same changeset for 409c.. and 533e</p>

<pre><code>$ git br --contains 533e2559342910fbffa2be5b38fdd7f2ddb2ed53
release-1.1.0
$ git br --contains 409c61b3304373a73c787fdf9c08cc338934b74d
master
release-1.0.4
</code></pre>

<p>That means that the changeset is in both master and release-1.1.0. So how come git cherry shows 533e.. ?</p>",1,0,2009-06-24 10:18:23.890000 UTC,0.0,,5,git|dvcs,574,2009-06-24 07:15:01.547000 UTC,2022-03-05 08:08:05.217000 UTC,,644,7,1,78,,,,,,[]
Best way to export a large dataframe to a single file that can be consumed by Power BI,"<p>I'm processing a large number of input files in Azure Databricks. My final dataframe has approximately 98million rows.</p>

<p>I need to export this out from Databricks so that I can import it to Power BI for reporting.</p>

<p>Power BI does not currently seem to have a connector that can interpret the partitioned nature if I simply write the dataframe to e.g. CSV.  Making use of coalesce or converting to a pandas dataframe and exporting to CSV is very slow and prone to resource limitations on the cluster.</p>

<p>I've tried both of the above approaches with little success.</p>

<p>What other options do I have to efficiently export my dataframe in a way that Power BI can understand?  I don't mind if this is either from the Databricks side or the Power Query side of the processing.</p>",1,3,2019-04-29 07:04:57.847000 UTC,,,0,pyspark|powerbi|azure-databricks,359,2018-10-09 23:52:37.947000 UTC,2022-02-20 07:09:18.110000 UTC,"Melbourne VIC, Australia",1463,14,20,124,,,,,,[]
How to return data from azure databricks notebook in Azure Data Factory,"<p>I have a requirement where I need to transform data in azure databricks and then return the transformed data. Below is notebook sample code where I am trying to return some json.</p>
<pre><code>from pyspark.sql.functions import *
from pyspark.sql.types import *
import json
import pandas as pd



# Define a dictionary containing ICC rankings
rankings = {'test': ['India', 'South Africa', 'England',
                            'New Zealand', 'Australia'],
              'odi': ['England', 'India', 'New Zealand',
                            'South Africa', 'Pakistan'],
               't20': ['Pakistan', 'India', 'Australia',
                              'England', 'New Zealand']}
   
# Convert the dictionary into DataFrame
rankings_pd = pd.DataFrame(rankings)
   
# Before renaming the columns
   
rankings_pd.rename(columns = {'test':'TEST'}, inplace = True)
rankings_pd.rename(columns = {'odi':'ODI'}, inplace = True)
rankings_pd.rename(columns = {'t20':'twenty-20'}, inplace = True)  
# After renaming the columns
#print(rankings_pd.to_json())
dbutils.notebook.exit(rankings_pd.to_json())
</code></pre>
<p>In order to achieve the same, I created a job under a cluster for this notebook and then I had to create a custom connector too following this article <a href=""https://medium.com/@poojaanilshinde/create-azure-logic-apps-custom-connector-for-azure-databricks-e51f4524ab27"" rel=""nofollow noreferrer"">https://medium.com/@poojaanilshinde/create-azure-logic-apps-custom-connector-for-azure-databricks-e51f4524ab27</a>. Using the connectors with API endpoint <strong>'/2.1/jobs/run-now'</strong> and then <strong>'/2.1/jobs/runs/get-output'</strong> in Azure Logic App, I am able to get the return value but after the job is executed successfully, sometimes I just get the status as running with no output. I need to get the output when job is executed successfully with transformation.
Please suggest a way better way for this if I am missing anything.</p>",1,0,2022-02-02 10:33:12.260000 UTC,,,0,azure-data-factory|azure-logic-apps|azure-databricks,59,2015-12-31 11:35:08.190000 UTC,2022-03-05 08:53:33.627000 UTC,,692,59,13,88,,,,,,[]
Use bzr/git/hg to move SVN repo to a sub-dir in another repo,"<p>I have 2 repos that I would like to merge into one as the projects should be together.
They look something like this:</p>

<pre><code>Repo1
   --branches
   --tags
   --trunk
       --Projects
           --Project1
</code></pre>

<p>--</p>

<pre><code>Repo2
   --branches
   --tags
   --trunk
       --Project2
</code></pre>

<p>I want to end up with something like this:</p>

<pre><code>Repo1
   --branches
   --tags
   --trunk
       --Projects
           --Project1
           --Project2
</code></pre>

<p>Basically move Project2 into Repo1 whilst keeping its history (not interested in branches though).</p>

<p>I think that this could be done with svnadmin and svndumpfilter but my svn admin says he doesn't want the hassle and that I should just commit the latest revision :(</p>

<p>Can I get around this by using bzr or git or hg to clone Repo2 then push it to Repo1 in the correct directory?</p>

<p>I'm not familiar with any of the DVCSs, so commands to do it with any of them are welcome.</p>",1,1,2011-11-29 01:20:50.113000 UTC,,,0,svn|clone|dvcs,57,2011-10-17 21:24:19.280000 UTC,2022-03-03 22:53:48.760000 UTC,,6312,357,26,741,,,,,,[]
Python - Connecting to AWS Neptune,"<p>I have created a neptune instance in aws. How can I connect to it now?</p>

<p>I tried the the example given in the <a href=""https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-python.html"" rel=""noreferrer"">documentation</a> locally from my laptop.</p>

<pre><code>from gremlin_python.structure.graph import Graph
from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection

graph = Graph()

g = graph.traversal().withRemote(DriverRemoteConnection('ws://my_endpoint:8182/gremlin','g'))

print(g.V().limit(2).toList())
</code></pre>

<p>But I get Timeout exception with the following stacktrace</p>

<pre><code>File ""/home/cegprakash/.virtualenvs/cegprakash-6Wq6Rd61/lib/python3.5/site-packages/gremlin_python/driver/driver_remote_connection.py"", line 45, in __init__
    password=password)
  File ""/home/cegprakash/.virtualenvs/cegprakash-6Wq6Rd61/lib/python3.5/site-packages/gremlin_python/driver/client.py"", line 76, in __init__
    self._fill_pool()
  File ""/home/cegprakash/.virtualenvs/cegprakash-6Wq6Rd61/lib/python3.5/site-packages/gremlin_python/driver/client.py"", line 88, in _fill_pool
    conn = self._get_connection()
  File ""/home/cegprakash/.virtualenvs/cegprakash-6Wq6Rd61/lib/python3.5/site-packages/gremlin_python/driver/client.py"", line 101, in _get_connection
    self._transport_factory, self._executor, self._pool)
  File ""/home/cegprakash/.virtualenvs/cegprakash-6Wq6Rd61/lib/python3.5/site-packages/gremlin_python/driver/connection.py"", line 40, in __init__
    self.connect()
  File ""/home/cegprakash/.virtualenvs/cegprakash-6Wq6Rd61/lib/python3.5/site-packages/gremlin_python/driver/connection.py"", line 46, in connect
    self._transport.connect(self._url)
  File ""/home/cegprakash/.virtualenvs/cegprakash-6Wq6Rd61/lib/python3.5/site-packages/gremlin_python/driver/tornado/transport.py"", line 33, in connect
    lambda: websocket.websocket_connect(url))
  File ""/home/cegprakash/.virtualenvs/cegprakash-6Wq6Rd61/lib/python3.5/site-packages/tornado/ioloop.py"", line 458, in run_sync
    return future_cell[0].result()
  File ""/home/cegprakash/.virtualenvs/cegprakash-6Wq6Rd61/lib/python3.5/site-packages/tornado/concurrent.py"", line 238, in result
    raise_exc_info(self._exc_info)
  File ""&lt;string&gt;"", line 4, in raise_exc_info
  File ""/home/cegprakash/.virtualenvs/cegprakash-6Wq6Rd61/lib/python3.5/site-packages/tornado/stack_context.py"", line 316, in wrapped
    ret = fn(*args, **kwargs)
  File ""/home/cegprakash/.virtualenvs/cegprakash-6Wq6Rd61/lib/python3.5/site-packages/tornado/simple_httpclient.py"", line 307, in _on_timeout
    raise HTTPError(599, error_message)
tornado.httpclient.HTTPError: HTTP 599: Timeout while connecting
</code></pre>

<p>Is there any authentication that I'm missing for the DB to get connected?</p>",3,10,2018-08-14 14:28:04.783000 UTC,2.0,,7,python|amazon-web-services|graph-databases|amazon-neptune,3115,2012-01-08 21:32:19.453000 UTC,2022-03-04 13:09:59.873000 UTC,"Chennai, India",2585,434,41,2372,,,,,,[]
Fail azure data factory pipeline if notebook execution skipped,"<p><a href=""https://i.stack.imgur.com/FAUGB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FAUGB.png"" alt=""enter image description here""></a>I have build an ADF pipline and i am executing multiple databricks notebooks in the pipeline. When one of the notebook fails the remianing command in the notebook skipped but the pipeline did not fail. I want to make sure that the pipeline execution stops if the notebook fails and next command skipped</p>

<p>I have tried trigering an error so that the pipeline fails</p>

<p>NA</p>

<p>I want to make sure that the pipeline execution stops if the notebook fails and next command skipped</p>",2,0,2019-11-08 16:19:40.113000 UTC,,2019-11-11 09:35:20.860000 UTC,2,azure-data-factory-2|azure-databricks,710,2019-08-15 11:56:48.253000 UTC,2019-11-12 15:50:21.033000 UTC,,33,0,0,56,,,,,,[]
"dvcs partial merge (git, hg merge tracking)","<p>I've one question about general DVCS, including Git and Hg.</p>

<p>In both Git and Hg merge tracking is done at the ""commit"" level instead of the ""file/directory"" level.</p>

<p>One of the ""side effects"" is that you can't easily do a ""partial merge"":</p>

<ul>
<li>You've modified 30 files in your branch ""feature_branch_x""</li>
<li>You want to merge ONLY the files under (let's say) /kernel/gui</li>
</ul>

<p>With ""item based merge tracking"" (Perforce, ClearCase, Plastic SCM &lt;= 3.0) you can just select a few files to merge, then checkin, then later repeat the merge and the pending files will show up.</p>

<p>With Hg, Git: once you merge (there're ways to keep files out without being merged) the ""tracking"" is set and if you repeat the merge no candidates to merge are left.</p>

<p>My question is how do you feel about it?? </p>

<p>Are there cases where you feel ""partial merge"" is mandatory? Can you live without it? (merge with commit/cset level tracking is much faster).</p>

<p>DISCLAIMER: I work for <a href=""http://www.plasticscm.com"" rel=""nofollow"">Plastic SCM</a> and we've moved to ""cset"" level tracking in 4.0, but we're wondering whether it could be a good idea to stay with ""item level merge tracking"" or even allow both.</p>",5,1,2011-09-06 17:20:04.053000 UTC,1.0,2012-02-24 22:21:37.837000 UTC,7,git|mercurial|dvcs|plasticscm,900,2009-03-01 13:44:11.950000 UTC,2020-07-08 10:53:13.417000 UTC,"Boecillo, Spain",6292,593,18,939,,,,,,[]
"Databricks ""extraneous input expecting EOF"" error","<p>This took me a while to figure out so I thought I'd share to save someone else the pain. This is obviously dummy code to illustrate the issue.</p>
<p>This doesn't work:</p>
<pre><code>%sql

Select 'A' as A -- I won't need this
  , '1' as B;

Select 'Magic';
</code></pre>
<p>Error message:</p>
<pre><code>Error in SQL statement: ParseException: 
extraneous input 'Select' expecting {&lt;EOF&gt;, ';'}(line 4, pos 0)

== SQL ==
Select 'A' as A -- I won't need this
  , '1' as B;

Select 'Magic';
^^^
</code></pre>
<p>This does work:</p>
<pre><code>%sql

Select 'A' as A -- I wont need this
  , '1' as B;

Select 'Magic';
</code></pre>
<p>And the difference in the single-quote in the comment on line 3.</p>",1,0,2020-10-21 02:05:54.270000 UTC,,,1,apache-spark-sql|azure-databricks,3439,2020-10-21 01:56:21.107000 UTC,2020-11-09 04:33:17.983000 UTC,,11,0,0,0,,,,,,[]
Same query on the same graph returns different results on Neptune vs Gremlify,"<p>I have a large production graph on Neptune which I query using Gremlin Python. I have written a query that should return paths that contain all vertices and edges connected in any way to the queried vertex (root vertex).</p>
<p>It seemed as though this query wasn't always returning all the vertices and edges I expected it to, so I made a small sample graph in Neptune, and the same graph on Gremlify: <a href=""https://gremlify.com/d4xp81hwd1g"" rel=""nofollow noreferrer"">https://gremlify.com/d4xp81hwd1g</a></p>
<p>I then ran the query on Neptune and Gremlify twice with a different root vertex for each. In both cases, the paths returned from Neptune were always a subset of the paths returned from Gremlify.</p>
<p><strong>Neptune graph creation:</strong></p>
<pre><code>g.addV(&quot;c&quot;).property(id, &quot;c139&quot;).iterate()
g.addV(&quot;c&quot;).property(id, &quot;c121&quot;).iterate()
g.addV(&quot;c&quot;).property(id, &quot;c146&quot;).iterate()
g.addV(&quot;c&quot;).property(id, &quot;c128&quot;).iterate()

g.addV(&quot;d&quot;).property(id, &quot;d5E0&quot;).iterate()
g.addV(&quot;d&quot;).property(id, &quot;d546&quot;).iterate()
g.addV(&quot;d&quot;).property(id, &quot;d434&quot;).iterate()
g.addV(&quot;d&quot;).property(id, &quot;dDE5&quot;).iterate()
g.addV(&quot;d&quot;).property(id, &quot;dFFE&quot;).iterate()

g.addE(&quot;has_d&quot;).from_(g.V(&quot;c139&quot;)).to(g.V(&quot;d5E0&quot;)).property(id, &quot;c139_d5E0&quot;).iterate()
g.addE(&quot;has_d&quot;).from_(g.V(&quot;c139&quot;)).to(g.V(&quot;d546&quot;)).property(id, &quot;c139_d546&quot;).iterate()
g.addE(&quot;has_d&quot;).from_(g.V(&quot;c121&quot;)).to(g.V(&quot;d5E0&quot;)).property(id, &quot;c121_d5E0&quot;).iterate()
g.addE(&quot;has_d&quot;).from_(g.V(&quot;c121&quot;)).to(g.V(&quot;d546&quot;)).property(id, &quot;c121_d546&quot;).iterate()
g.addE(&quot;has_d&quot;).from_(g.V(&quot;c121&quot;)).to(g.V(&quot;dDE5&quot;)).property(id, &quot;c121_dDE5&quot;).iterate()
g.addE(&quot;has_d&quot;).from_(g.V(&quot;c121&quot;)).to(g.V(&quot;dFFE&quot;)).property(id, &quot;c121_dFFE&quot;).iterate()
g.addE(&quot;has_d&quot;).from_(g.V(&quot;c121&quot;)).to(g.V(&quot;d434&quot;)).property(id, &quot;c121_d434&quot;).iterate()
g.addE(&quot;has_d&quot;).from_(g.V(&quot;c146&quot;)).to(g.V(&quot;dDE5&quot;)).property(id, &quot;c146_dDE5&quot;).iterate()
g.addE(&quot;has_d&quot;).from_(g.V(&quot;c128&quot;)).to(g.V(&quot;dFFE&quot;)).property(id, &quot;c128_dFFE&quot;).iterate()
g.addE(&quot;has_d&quot;).from_(g.V(&quot;c128&quot;)).to(g.V(&quot;d434&quot;)).property(id, &quot;c128_d434&quot;).iterate()
</code></pre>
<p><strong>Neptune query:</strong></p>
<pre><code>paths = (
    g.V('c121')
    .repeat(bothE().simplePath().otherV())
    .until(not_(bothE().simplePath()))
    .path()
    .by(valueMap(True))
    .toList()
)

for path in paths:
    for element in path:
        print(element[id], end = ' -&gt; ')
    print('')
</code></pre>
<p>The Gremlify graph and query can be seen in the link above.</p>
<p><strong>Results:</strong></p>
<pre><code>ROOT VERTEX: c121

Neptune:  c121 -&gt; c121_dDE5 -&gt; dDE5 -&gt; c146_dDE5 -&gt; c146
Gremlify: c121 -&gt; c121_dDE5 -&gt; dDE5 -&gt; c146_dDE5 -&gt; c146

Neptune:  c121 -&gt; c121_d5E0 -&gt; d5E0 -&gt; c139_d5E0 -&gt; c139
Gremlify: c121 -&gt; c121_d5E0 -&gt; d5E0 -&gt; c139_d5E0 -&gt; c139 -&gt; c139_d546 -&gt; d546 -&gt; c121_d546 -&gt; c121

Neptune:  c121 -&gt; c121_dFFE -&gt; dFFE -&gt; c128_dFFE -&gt; c128
Gremlify: c121 -&gt; c121_dFFE -&gt; dFFE -&gt; c128_dFFE -&gt; c128 -&gt; c128_d434 -&gt; d434 -&gt; c121_d434 -&gt; c121

Neptune:  c121 -&gt; c121_d434 -&gt; d434 -&gt; c128_d434 -&gt; c128
Gremlify: c121 -&gt; c121_d434 -&gt; d434 -&gt; c128_d434 -&gt; c128 -&gt; c128_dFFE -&gt; dFFE -&gt; c121_dFFE -&gt; c121

Neptune:  c121 -&gt; c121_d546 -&gt; d546 -&gt; c139_d546 -&gt; c139 -&gt;
Gremlify: c121 -&gt; c121_d546 -&gt; d546 -&gt; c139_d546 -&gt; c139 -&gt; c139_d5E0 -&gt; d5E0 -&gt; c121_d5E0 -&gt; c121

ROOT VERTEX: c139

Neptune:  c139 -&gt; c139_d546 -&gt; d546 -&gt; c121_d546 -&gt; c121
Gremlify: c139 -&gt; c139_d546 -&gt; d546 -&gt; c121_d546 -&gt; c121 -&gt; c121_d5E0 -&gt; d5E0 -&gt; c139_d5E0 -&gt; c139

Neptune:  c139 -&gt; c139_d546 -&gt; d546 -&gt; c121_d546 -&gt; c121
Gremlify: c139 -&gt; c139_d546 -&gt; d546 -&gt; c121_d546 -&gt; c121 -&gt; c121_dDE5 -&gt; dDE5 -&gt; c146_dDE5 -&gt; c146

Neptune:  c139 -&gt; c139_d5E0 -&gt; d5E0 -&gt; c121_d5E0 -&gt; c121
Gremlify: c139 -&gt; c139_d5E0 -&gt; d5E0 -&gt; c121_d5E0 -&gt; c121 -&gt; c121_d546 -&gt; d546 -&gt; c139_d546 -&gt; c139

Neptune:  c139 -&gt; c139_d5E0 -&gt; d5E0 -&gt; c121_d5E0 -&gt; c121
Gremlify: c139 -&gt; c139_d5E0 -&gt; d5E0 -&gt; c121_d5E0 -&gt; c121 -&gt; c121_dDE5 -&gt; dDE5 -&gt; c146_dDE5 -&gt; c146

Neptune:  c139 -&gt; c139_d546 -&gt; d546 -&gt; c121_d546 -&gt; c121 -&gt; c121_dFFE -&gt; dFFE
Gremlify: c139 -&gt; c139_d546 -&gt; d546 -&gt; c121_d546 -&gt; c121 -&gt; c121_dFFE -&gt; dFFE -&gt; c128_dFFE -&gt; c128 -&gt; c128_d434 -&gt; d434 -&gt; c121_d434 -&gt; c121

Neptune:  c139 -&gt; c139_d546 -&gt; d546 -&gt; c121_d546 -&gt; c121 -&gt; c121_d434 -&gt; d434
Gremlify: c139 -&gt; c139_d546 -&gt; d546 -&gt; c121_d546 -&gt; c121 -&gt; c121_d434 -&gt; d434 -&gt; c128_d434 -&gt; c128 -&gt; c128_dFFE -&gt; dFFE -&gt; c121_dFFE -&gt; c121

Neptune:  c139 -&gt; c139_d5E0 -&gt; d5E0 -&gt; c121_d5E0 -&gt; c121 -&gt; c121_dFFE -&gt; dFFE
Gremlify: c139 -&gt; c139_d5E0 -&gt; d5E0 -&gt; c121_d5E0 -&gt; c121 -&gt; c121_dFFE -&gt; dFFE -&gt; c128_dFFE -&gt; c128 -&gt; c128_d434 -&gt; d434 -&gt; c121_d434 -&gt; c121

Neptune:  c139 -&gt; c139_d5E0 -&gt; d5E0 -&gt; c121_d5E0 -&gt; c121 -&gt; c121_d434 -&gt; d434
Gremlify: c139 -&gt; c139_d5E0 -&gt; d5E0 -&gt; c121_d5E0 -&gt; c121 -&gt; c121_d434 -&gt; d434 -&gt; c128_d434 -&gt; c128 -&gt; c128_dFFE -&gt; dFFE -&gt; c121_dFFE -&gt; c121
</code></pre>
<p>Why am I seeing these different results? And how can I achieve the expected results I am getting in Gremlify with my Python queries on Neptune?</p>",1,0,2020-09-17 09:24:10.387000 UTC,,2020-09-18 09:10:52.977000 UTC,3,python|graph|gremlin|amazon-neptune,99,2015-02-13 15:27:11.157000 UTC,2022-03-01 10:24:41.690000 UTC,,3342,80,2,467,,,,,,[]
How to read csv file for which data contains double quotes and comma seperated using spark dataframe in databricks,"<p>I'm trying to read csv file using spark dataframe in databricks. The csv file contains double quoted with comma separated columns. I tried with the below code and not able to read the csv file. But if I check the file in datalake I can see the file.</p>

<p>The input and  output is as follows</p>

<pre class=""lang-python prettyprint-override""><code>df = spark.read.format(""com.databricks.spark.csv"")\
     .option(""header"",""true"")\
     .option(""quoteAll"",""true"")\
     .option(""escape"",'""')\
     .csv(""mnt/A/B/test1.csv"")
</code></pre>

<p>The input file data:header: </p>

<pre class=""lang-python prettyprint-override""><code>""A"",""B"",""C""
""123"",""dss"",""csc""
""124"",""sfs"",""dgs""
</code></pre>

<p>Output: </p>

<pre class=""lang-python prettyprint-override""><code>""A""|""B""|""C""|
</code></pre>",0,0,2019-03-22 16:34:59.780000 UTC,,2019-03-22 16:38:31.623000 UTC,1,pyspark|apache-spark-sql|azure-databricks,230,2018-10-04 16:30:46.120000 UTC,2021-02-25 15:56:54.010000 UTC,,113,3,0,83,,,,,,[]
Messages not loading into SilverTable from Topic,"<p>Trying to load messages from Topic into a silverTable in the WriteStream. But the messages are not loading into silverTable. How to read the messages into silverTable?</p>
<pre class=""lang-scala prettyprint-override""><code>var df = spark
  .readStream
  .format(&quot;kafka&quot;)
  .option(&quot;kafka.bootstrap.servers&quot;, &quot;10.19.9.4:1111&quot;)
  .option(&quot;subscribe&quot;, &quot;testTopic&quot;)
  .load()
df = df.select($&quot;value&quot;,$&quot;topic&quot;)

// select the avro encoded value and the topic name from the topic
df.writeStream
  .foreachBatch( (batchDF: DataFrame, batchId: Long)=&gt;
    {
      batchDF.persist()
      val topics = batchDF.select(&quot;topic&quot;).distinct().collect().map(
         (row)=&gt;row.getString(0))
      topics.foreach((topix)=&gt;{
        val silverTable = mappings(topix)
        // filter out message for the current topic
        var writeDF = batchDF.where(s&quot;topic = '${topix}'&quot;)
        //  decode the avro records to a spark struct
        val schemaReg = schemaRegistryMappings(topix)
        writeDF = writeDF.withColumn(&quot;avroconverted&quot;,
           from_avro($&quot;value&quot;, topix+&quot;-value&quot;, schemaReg))
        // append to the sliver table
        writeDF.write.format(&quot;delta&quot;).mode(&quot;append&quot;).saveAsTable(&quot;silverTable&quot;)
      })
    }
</code></pre>",0,0,2021-12-17 05:42:23.637000 UTC,,2021-12-17 06:55:03.540000 UTC,1,scala|apache-spark|apache-kafka|spark-streaming|azure-databricks,16,2017-12-13 04:33:03.000000 UTC,2022-01-10 19:26:21.457000 UTC,"Irvine, CA, United States",11,0,0,1,,,,,,[]
SSLError: HTTPSConnectionPool when registering ML model on Azure,"<p>Please I have my Databricks instance in a VNET. I’m trying to deploy my Machine learning model using the Azure ML workspace on Azure Container Instance (ACI).</p>
<p>I’m able to create an ML workspace. I get an SSLERROR when I try to register the Model using Model.register().</p>
<p>Using  this code -</p>
<pre><code>from azureml.core import Workspace
from azureml.core.model import Model
import azureml.core
from azureml.core.workspace import Workspace
from azureml.core.model import Model

from azureml.core import Workspace
   ws = Workspace.create(name='myworkspace',
               subscription_id='&lt;azure-subscription-id&gt;',
               resource_group='myresourcegroup',
               location='eastus'
               )

model_reg = Model.register(model_path = “./model_dir”,
                       model_name = &quot;ModelX&quot;,
                      workspace = ws)
</code></pre>
<p>Find below the error when I try to deploy my model.</p>
<p> 
SSL Error:</p>
<pre><code>SSLError: HTTPSConnectionPool(host='eastus.experiments.azureml.net', port=443): Max retries exceeded with url: /discovery (Caused by SSLError(SSLError(&quot;bad handshake: SysCallError(104, 'ECONNRESET')&quot;)))
 
</code></pre>
<p>Please note only the Azure Databricks is in a VNET on Azure. How do I resolve it and deploy my model as a webservice on ACI.</p>
<p>Thank you.</p>",1,2,2020-09-10 04:32:24.547000 UTC,,,2,azure|ssl-certificate|azure-databricks|azure-machine-learning-service|azure-machine-learning-workbench,653,2018-09-18 19:45:24.587000 UTC,2021-05-03 07:29:12.890000 UTC,"Laurel, MD, USA",329,3,0,40,,,,,,[]
Azure Databricks with ADLS Gen 2 Secure Access,"<p>I'm running into some frustration among the teams at our small company surrounding the use of Azure Data Lake Storage Gen 2 as the backend for our delta tables in Azure Databricks (I'm new at this company and to Databricks, so anything I may explain was decided before my time and I realize some of it may be questionable, but not looking for perspectives on that).</p>
<p>Essentially, the engineering team is building data ingestion pipelines (as python files, not notebooks) that run on and are scheduled by Azure Databricks (Jobs API). This means the pipelines must be able to access the ADLS Gen 2 storage resource - and so we authenticate directly using a Service Principal (SPN) and OAuth 2.0, as described in this <a href=""https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/azure-datalake-gen2"" rel=""nofollow noreferrer"">Microsoft doc</a>, setting the following configs via spark.conf.set():</p>
<pre><code>fs.azure.account.auth.type.[STORAGE_ACCT].dfs.core.windows.net
fs.azure.account.oauth.provider.type.[STORAGE_ACCT].dfs.core.windows.net
fs.azure.account.oauth2.client.id.[STORAGE_ACCT].dfs.core.windows.net
fs.azure.account.oauth2.client.secret.[STORAGE_ACCT].dfs.core.windows.net
fs.azure.account.oauth2.client.endpoint.[STORAGE_ACCT].dfs.core.windows.net
</code></pre>
<p>Given that engineering is working with this for our pipelines, and we use different storage accounts for dev/prod, the codebase detects for which environment we're running in (dev or prod) and sets the appropriate storage account configs. Perhaps unconventional, but no problems on our side.</p>
<p><strong>The issue</strong>:
Our data science team is running on Databricks Notebooks, and also require access to the data tables backed by ADLS Gen 2 - so, they must authenticate also. However, their code is not engineering code, so they do not account for the change in environment. Thus they are frustrated that on promotion to production, they must make a small tweak which will allow it to work in prod, but then does not work in dev, as these environments are in their own VNETs and have their own storage accounts.</p>
<p><strong>The Ask</strong>:
How do we securely allow table access <em>without</em> sacrificing access controls, and <em>without</em> having to set these configs in the notebooks/codebase? Is this even possible with Databricks?</p>
<p>If other teams are using the above method, how do they swap container on promotion to production without breaking the code in dev? Is it just a matter of telling the DS team they <em>have</em> to use our logic?</p>
<p><strong>What I've tried</strong>:</p>
<ul>
<li>ADLS Credential Passthrough - since we schedule their models to run with Databricks Jobs API - this has a hard limitation on credential passthrough, so cannot use</li>
<li>Mounting ADLS - since this gives anyone with access to the cluster access to the ADLS, team is against this approach</li>
<li>Cluster config (UI) - we use ephemeral clusters for jobs, but since this would list any retrieved secrets in plaintext, all are against this approach</li>
</ul>
<hr />
<p>For now, I've implemented engineering teams logic in their notebook for detecting the environment, but I cannot stress enough how livid they are that engineering code is implemented in their notebook.</p>
<p>I understand there may not be any straight shot answer, but any help would be appreciated.</p>",1,0,2020-12-02 17:51:14.480000 UTC,,,0,azure|azure-databricks|access-control|azure-data-lake-gen2,198,2013-05-25 00:11:58.963000 UTC,2022-03-03 19:25:00.207000 UTC,,105,5,0,20,,,,,,[]
show the file timestamp and select latest file from the directory using Scala in azure databricks,"<p>I want to select the latest file from the directory and show the timestamp of all the files using scala code in azure databricks .</p>

<p>Can you please help me on this .</p>",1,0,2019-10-29 13:22:30.517000 UTC,,,0,scala|azure-databricks,248,2019-10-28 10:46:32.500000 UTC,2021-04-18 19:25:34.100000 UTC,,11,0,0,2,,,,,,[]
How to include my LOGS of my airflow task in the email triggered on failure,"<p>I have a Dag with DataBricks Run Operator Task. Now on failure of this Dag, it notifies the user that this task is failed. Now i want to include the execution of Databricks Notebook URL into the Mail , it will be available if we go to the logs manually.</p>
<p>Can anyone help me how to get that property with link of execution of Databricks Notebook</p>",0,2,2021-06-21 10:24:10.257000 UTC,,,0,airflow|azure-databricks,73,2021-05-26 06:28:38.243000 UTC,2022-03-03 06:39:00.907000 UTC,"Hyderabad, Telangana, India",71,0,0,10,,,,,,[]
ParseExpection: no viable alternative at input,"<p>I was trying to run the below query in Azure data bricks.</p>
<pre class=""lang-py prettyprint-override""><code>query=s&quot;&quot;&quot;WITH pre_file_user AS(
            SELECT id,
            typeid,
          CASE when dttm is null or dttm='' then cast('1900-01-01 00:00:00.000' as timestamp)
          else cast(dttm as timestamp)
          end as dttm
          from dde_pre_file_user_supp
)&quot;&quot;&quot;

spark.sql(query)
</code></pre>
<p>then I was getting the following error</p>
<blockquote>
<p>ParseException:no viable alternative at input 'with pre_file_users AS
(\n select id, \n typid, in case\n when dttm is null or dttm = '' then
cast('1900-01-01 00:00:00.000 as timestamp)\n end as dttm\n from
dde_pre_file_user_supp\n  )'</p>
</blockquote>
<p>Can I use WITH clause in data bricks or is there any alternative?</p>",1,1,2021-12-31 09:32:15.733000 UTC,1.0,2021-12-31 10:26:57.727000 UTC,1,sql|apache-spark|pyspark|apache-spark-sql|azure-databricks,1088,2019-02-07 08:01:16.953000 UTC,2022-03-05 12:09:35.997000 UTC,"Vijayawada, Andhra Pradesh, India",51,1,0,7,,,,,,[]
Different image for nodes in Neptune Jupyter notebook,"<p>I loaded my node.csv and edge.csv successfully to AWS Neptune.<br>
I'm trying to display the vertices as an image accordingly to the corresponding label. <br>
For example, there's an edge between 198.51.100.0 &amp; adesai and 198.51.100.0 is also connecting to gramirez.<br>
So, I would like to display a address.png for the IPAddress and a person.png for adesai &amp; gramirez.</p>
<p>I'm using Gremlin lanugage and I have tried below but it's not working.</p>
<pre><code> %%graph_notebook_vis_options&lt;br&gt;
{&lt;br&gt;
  &quot;nodes&quot;:{  &quot;id&quot;:&quot;0&quot;,   &quot;label&quot;: &quot;User&quot;, &quot;shape&quot;: &quot;circularImage&quot;, &quot;image&quot;: &quot;person.png&quot;},&lt;br&gt;
  &quot;nodes&quot;:{  &quot;id&quot;:&quot;1&quot;,   &quot;label&quot;: &quot;User&quot;,&quot;shape&quot;: &quot;circularImage&quot;, &quot;image&quot;: &quot;person.png&quot;},&lt;br&gt;
  &quot;nodes&quot;:{  &quot;id&quot;:&quot;2&quot;,   &quot;label&quot;: &quot;Restaurant&quot;,&quot;shape&quot;: &quot;circularImage&quot;, &quot;image&quot;: &quot;restaurant.png&quot;},&lt;br&gt;
  &quot;nodes&quot;:{  &quot;id&quot;:&quot;3&quot;,   &quot;label&quot;: &quot;Restaurant&quot;,&quot;shape&quot;: &quot;circularImage&quot;, &quot;image&quot;: &quot;restaurant.png&quot;},&lt;br&gt;
  &quot;nodes&quot;:{  &quot;id&quot;:&quot;4&quot;,   &quot;label&quot;: &quot;IPAddress&quot;,&quot;shape&quot;: &quot;circularImage&quot;, &quot;image&quot;: &quot;address.png&quot;}&lt;br&gt;
}

</code></pre>
<p>The nodes are just showing the same image.
Can anyone advise? thank you</p>
<p><a href=""https://i.stack.imgur.com/hGUVz.png"" rel=""nofollow noreferrer"">Showing the node and the graph</a></p>",1,2,2021-11-12 10:13:32.563000 UTC,,2021-11-20 17:55:01.720000 UTC,0,amazon-web-services|gremlin|graph-databases|amazon-neptune|graph-notebook,95,2021-11-12 09:53:05.117000 UTC,2022-02-25 10:51:10.820000 UTC,,1,0,0,3,,,,,,[]
How to connect to Azure Databricks using Service Principal?,"<p>I am trying to launch a cluster using Azure DataBricks using portal but I am getting an issue saying ""Subnet provided does not have security group associated to it."" 
But I want to connect it using the service Principal.</p>

<p>Please help!!</p>",1,0,2019-10-22 06:21:21.130000 UTC,,,0,azure|azure-databricks|service-principal,221,2019-10-22 06:17:40.513000 UTC,2019-12-03 05:42:00.717000 UTC,,1,0,0,1,,,,,,[]
"EM dash (""–"") gets replaced by character â€” upon update in WinCvs","<p>In WinCvs, when I take update of a file (in my case its .sql file) the EM Dash (<strong>–</strong>) gets replaced by character <strong>â€”</strong>. How can I get rid of this?
I am using <strong>WinCvs version - 2.1.1.1(Build 1)</strong></p>",0,4,2018-10-04 07:31:24.197000 UTC,,2018-10-26 08:04:46.297000 UTC,0,version-control|repository|cvs|dvcs|cvsnt,72,2014-04-18 16:16:07.120000 UTC,2021-11-30 13:12:30.957000 UTC,,1,0,0,10,,,,,,[]
How to install job dependent libraries and .whl package on Databricks Job cluster,"<p>We have the .whl package(customised module) that needs to be installed on the Databricks job cluster the moment when the new job cluster spins-up for ADF jobs. Let me know how we can push the .whl package to the newly spined-up cluster.</p>
<p>Any pointers would help.</p>",1,0,2022-01-21 14:11:10.200000 UTC,,2022-01-22 12:41:43.023000 UTC,1,azure-data-factory|azure-databricks,62,2016-01-22 09:29:22.083000 UTC,2022-03-05 17:22:10.320000 UTC,"Bengaluru, India",475,44,0,111,,,,,,[]
Azure Data Factory architecture with Azure SQL database to Power BI,"<p>I'm no MS expert - recently hopped onto the Azure train and apologies in advance if I get some information wrong.</p>

<p>Basically need some input in <strong>Azure's architecture</strong> utilising <strong>Azure Data Factory</strong> (as the ETL/ELT tool) and <strong>Azure SQL database</strong> (as the storage), to a BI output - Power BI. My situation is this;</p>

<ul>
<li>I have on-premise data sources such as <em>Oracle DB, Oracle Cloud SSAS, MS SQL server db</em></li>
<li>I'd like to have a MS cloud infrastructure solution for reporting purposes. </li>
<li>No data migration needed - merely pumping on-prem data onto cloud and producing a BI reporting solution</li>
</ul>

<p>Based on my limited knowledge and Google research, Azure Data Factory caters for all my on-prem sources, as well as the future cloud Azure SQL database. If future analysis is needed, <strong>Azure Storage</strong> and <strong>Azure Databricks</strong> can be added in to this architecture. I have sketched out the architecture of my proposed solution.
<a href=""https://i.stack.imgur.com/NVctm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NVctm.png"" alt=""Proposed Azure Architecture""></a></p>

<p>Just confirming my understanding</p>

<ol>
<li><strong>Without</strong> Azure Storage &amp; Databricks (the 2 pink boxes), the 2 Azure component (DF &amp; SQL database) is sufficient to take data from on-premise sources, process on cloud &amp; output into Power BI.</li>
<li><strong>With</strong> Azure Storage &amp; Databricks (the 2 pink boxes), processing will be more efficient as their summarised function is to store training data models &amp; act as an analytics processing engine.</li>
<li><strong>Azure SQL database</strong> is more suitable, as compared to <strong>Azure SQL datawarehouse</strong> as my data sources does not exceed 1TB; cost-wise is cheaper AND one of my data sources contain data from call centers, hence OLTP is more suitable. Plus I have Azure Databricks to support the analytical bit that SQL datawarehouse does (OLAP).</li>
</ol>

<p>Any other comments to help me understand this whole architecture will be great!</p>",2,0,2019-11-19 15:00:07.030000 UTC,,,2,azure|azure-sql-database|azure-storage|azure-data-factory|azure-databricks,256,2018-01-17 06:20:01.660000 UTC,2019-11-24 13:47:39.737000 UTC,Kuala Lumpur Malaysia,35,0,0,20,,,,,,[]
Using version control to keep multiple code branches up to date,"<p>I'm hoping that someone will be able to help me better understand version control in this scenario. Currently we're thinking of either implementing VisualSVN or Mercurial, but want to get our strategy figured-out first. </p>

<p>I'm part of a two man development team working on porting and maintaining a bespoke CMS system. We're looking to keep a ""stable"" branch, and each have our own dev branches as required.</p>

<p>Where we're not sure is that each customer requires their own customisations (templates, stylesheets, etc). I'd expect these to then be branches from stable. Ideally we'd like a way to ensure that any changes made to stable (such as merging a dev branch back in or adding a hot-fix) get pushed out to the customer branches. </p>

<ul>
<li><p>Is there a way to ensure that these customer branches always get automatically brought forward to the most recent changeset ? </p></li>
<li><p>Can we choose to exclude certain files (such as *.css) from getting pushed to the client branches after the initial branch operation ?</p></li>
</ul>

<p>Looking at this you'd be able to go into each customer branch and pull from stable, merging the latest changes in. In the event that we end up with lots of customer branches, is there a better way to do this ?</p>

<p>I've found some questions that seem to offer good advice</p>

<ul>
<li><p><a href=""https://stackoverflow.com/questions/4852124/using-version-control-with-non-hierarchical-code"">Using version control with non-hierarchical code?</a> </p></li>
<li><p><a href=""https://stackoverflow.com/questions/404948/practical-way-to-commit-changes-in-source-control-to-multiple-branches"">Practical way to commit changes in source control to multiple branches</a></p></li>
<li><p><a href=""https://stackoverflow.com/questions/4095184/version-control-for-multiple-instances-of-a-developing-code"">Version control for multiple instances of a developing code</a></p></li>
<li><p><a href=""https://stackoverflow.com/questions/2563887/how-do-i-keep-my-branches-up-to-date-with-the-default-branch-under-mercurial"">How do I keep my branches up to date with the &#39;default&#39; branch under Mercurial?</a></p></li>
</ul>

<p>Apologies if I'm coming at this backwards, but all suggestions would be appreciated.</p>",1,0,2012-02-01 14:37:12.630000 UTC,,2017-05-23 10:28:31.123000 UTC,2,svn|version-control|mercurial|branch|dvcs,161,2010-05-12 14:30:18.703000 UTC,2020-05-20 13:22:41.070000 UTC,United Kingdom,61,16,0,10,,,,,,[]
Databricks Reading Only metadata from JDBC Source but not Data,"<p>Tried different ways</p>
<p>sql</p>
<pre><code>  CREATE TABLE oracle_table
  USING org.apache.spark.sql.jdbc
  OPTIONS (
    dbtable 'persons',
    driver 'oracle.jdbc.driver.OracleDriver',
    user '&lt;user&gt;',
    password '&lt;pass&gt;',
    url 'jdbc:oracle:thin://@&lt;host&gt;:1521/orcl')
</code></pre>
<p>above code returned OK</p>
<pre><code>select * from oracle_table
</code></pre>
<p>throwed java.sql.SQLRecoverableException: IO Error: The Network Adapter could not establish the connection</p>
<p>python</p>
<pre><code>jdbcUrl = &quot;jdbc:oracle:thin:@&lt;host&gt;:1521/orcl&quot;
properties = {
    &quot;user&quot;: &quot;&lt;user&gt;&quot;,
    &quot;password&quot;: &quot;&lt;password&gt;&quot;,
    &quot;driver&quot;: &quot;oracle.jdbc.driver.OracleDriver&quot; 
}

pushdown_query = &quot;( SELECT * FROM persons ) emp_alias&quot;
df = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=properties)
df.printSchema()
</code></pre>
<p>returned</p>
<pre><code> |-- PERSON_ID: decimal(38,10) (nullable = true)
 |-- FIRST_NAME: string (nullable = true)
 |-- LAST_NAME: string (nullable = true)
</code></pre>
<p>But for below code</p>
<pre><code>df.show()
</code></pre>
<p>throwed java.sql.SQLRecoverableException: IO Error: The Network Adapter could not establish the connection</p>
<p>help me!</p>",1,7,2020-07-21 12:36:03.837000 UTC,,2020-07-21 12:46:12.493000 UTC,1,oracle|pyspark|apache-spark-sql|azure-databricks,179,2015-09-24 17:19:46.610000 UTC,2022-03-06 04:27:12.623000 UTC,"Hyderabad, Telangana, India",41,3,0,11,,,,,,[]
Implementing 'max' for a Gremlin sack,"<p>I have a parent-child relationship in which there is a numeric property on the edge. I would like to <code>sack</code> the maximum value of this property and carry it through the reset of the traversal, referring to it as needed.<br></p>
<p>Current implementation looks like this:</p>
<pre><code>g.withSack(1).V(33131).outE(&quot;parent_to&quot;).values(&quot;order&quot;).max().unfold().sack(Operator.max)
    .project(&quot;count&quot;,&quot;sack&quot;)
        .by(V().count())
        .by(sack())
</code></pre>
<p><a href=""https://gremlify.com/zt7u6a2qx18/1"" rel=""nofollow noreferrer"">Gremlify</a></p>
<p>Is there a cleaner way to do this? The projection here is only for example sake.</p>",1,0,2021-10-20 13:25:46.353000 UTC,,2021-10-26 15:57:27.897000 UTC,0,gremlin|amazon-neptune,46,2020-03-22 17:13:51.963000 UTC,2022-03-04 21:15:00.600000 UTC,Earth,73,4,0,19,,,,,,[]
An error occurred while calling o7344.save,"<p>I am running spark jobs using datafactory in azure databricks.
My cluster vesion is <strong>9.1 LTS ML (includes Apache Spark 3.1.2, Scala 2.12)</strong>.
I am writing data on azure blob storage.</p>
<p>While writing job gives me error.
It shows error on line
<strong>AllDataDF.coalesce(1).write.format(&quot;delta&quot;).mode(&quot;append&quot;).option(&quot;mergeSchema&quot;, &quot;true&quot;).save(path)</strong></p>
<p>And traceback in as below.</p>
<pre><code>/databricks/spark/python/pyspark/sql/readwriter.py in save(self, path, format, mode, partitionBy, **options)
   1134             self._jwrite.save()
   1135         else:
-&gt; 1136             self._jwrite.save(path)
   1137 
   1138     @since(1.4)

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1302 
   1303         answer = self.gateway_client.send_command(command)
-&gt; 1304         return_value = get_return_value(
   1305             answer, self.gateway_client, self.target_id, self.name)
   1306 

/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
    115     def deco(*a, **kw):
    116         try:
--&gt; 117             return f(*a, **kw)
    118         except py4j.protocol.Py4JJavaError as e:
    119             converted = convert_exception(e.java_exception)

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    324             value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
    325             if answer[1] == REFERENCE_TYPE:
--&gt; 326                 raise Py4JJavaError(
    327                     &quot;An error occurred while calling {0}{1}{2}.\n&quot;.
    328                     format(target_id, &quot;.&quot;, name), value)

Py4JJavaError: An error occurred while calling o7344.save.
: org.apache.spark.SparkException: Job aborted.
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:307)
    at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$5(TransactionalWriteEdge.scala:349)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
    at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
    at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$1(TransactionalWriteEdge.scala:296)
    at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
    at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:122)
    at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:395)
    at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:484)
    at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:504)
    at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)
    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
    at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)
    at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)
    at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:20)
    at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)
    at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)
    at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:20)
    at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:479)
    at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:404)
    at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:20)
    at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:395)
    at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:367)
    at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:20)
    at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:57)
    at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:137)
    at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:71)
    at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:58)
    at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:98)
    at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:429)
    at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:408)
    at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:98)
    at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:120)
    at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:104)
    at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:98)
    at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:213)
    at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:207)
    at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:98)
    at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:389)
    at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:382)
    at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:98)
    at com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:158)
    at com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:155)
    at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:98)
    at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:175)
    at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.$anonfun$run$2(WriteIntoDelta.scala:91)
    at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.$anonfun$run$2$adapted(WriteIntoDelta.scala:83)
    at com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:206)
    at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:83)
    at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:1413)
    at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.run(WriteIntoDelta.scala:82)
    at com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:164)
    at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:96)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:213)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:257)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:253)
    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:209)
    at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:167)
    at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:166)
    at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:1080)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)
    at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)
    at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1080)
    at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:469)
    at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:386)
    at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:304)
    at sun.reflect.GeneratedMethodAccessor531.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
    at py4j.Gateway.invoke(Gateway.java:295)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:251)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8352.0 failed 4 times, most recent failure: Lost task 0.3 in stage 8352.0 (TID 10576) (10.139.64.18 executor 17): org.apache.spark.SparkException: Task failed while writing rows.
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:396)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:284)
    at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)
    at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
    at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)
    at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)
    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)
    at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)
    at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
    at org.apache.spark.scheduler.Task.run(Task.scala:91)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:813)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1620)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:816)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:672)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.NoSuchElementException
    at org.apache.spark.sql.vectorized.ColumnarBatch$1.next(ColumnarBatch.java:69)
    at org.apache.spark.sql.vectorized.ColumnarBatch$1.next(ColumnarBatch.java:58)
    at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:44)
    at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$4.next(ArrowConverters.scala:401)
    at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$4.next(ArrowConverters.scala:382)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage138.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:757)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage183.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:757)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$2(FileFormatWriter.scala:374)
    at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1654)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:383)
    ... 19 more
    Suppressed: java.io.IOException: can not write PageHeader(type:DICTIONARY_PAGE, uncompressed_page_size:176, compressed_page_size:101, crc:-1384879305, dictionary_page_header:DictionaryPageHeader(num_values:22, encoding:PLAIN_DICTIONARY))
        at org.apache.parquet.format.Util.write(Util.java:224)
        at org.apache.parquet.format.Util.writePageHeader(Util.java:61)
        at org.apache.parquet.format.converter.ParquetMetadataConverter.writeDictionaryPageHeader(ParquetMetadataConverter.java:1190)
        at org.apache.parquet.hadoop.ParquetFileWriter.writeDictionaryPage(ParquetFileWriter.java:374)
        at org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writeToFileWriter(ColumnChunkPageWriteStore.java:238)
        at org.apache.parquet.hadoop.ColumnChunkPageWriteStore.flushToFileWriter(ColumnChunkPageWriteStore.java:316)
        at org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:202)
        at org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:127)
        at org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:41)
        at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:58)
        at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.abort(FileFormatDataWriter.scala:84)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$3(FileFormatWriter.scala:380)
        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1665)
        ... 20 more
    Caused by: shaded.parquet.org.apache.thrift.transport.TTransportException: java.io.IOException: Stream is closed!
        at shaded.parquet.org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:147)
        at shaded.parquet.org.apache.thrift.transport.TTransport.write(TTransport.java:107)
        at shaded.parquet.org.apache.thrift.protocol.TCompactProtocol.writeByteDirect(TCompactProtocol.java:482)
        at shaded.parquet.org.apache.thrift.protocol.TCompactProtocol.writeByteDirect(TCompactProtocol.java:489)
        at shaded.parquet.org.apache.thrift.protocol.TCompactProtocol.writeFieldBeginInternal(TCompactProtocol.java:252)
        at shaded.parquet.org.apache.thrift.protocol.TCompactProtocol.writeFieldBegin(TCompactProtocol.java:234)
        at org.apache.parquet.format.InterningProtocol.writeFieldBegin(InterningProtocol.java:74)
        at org.apache.parquet.format.PageHeader$PageHeaderStandardScheme.write(PageHeader.java:1068)
        at org.apache.parquet.format.PageHeader$PageHeaderStandardScheme.write(PageHeader.java:966)
        at org.apache.parquet.format.PageHeader.write(PageHeader.java:847)
        at org.apache.parquet.format.Util.write(Util.java:222)
        ... 33 more
    Caused by: java.io.IOException: Stream is closed!
        at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream.close(AbfsOutputStream.java:344)
        at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.PageCRCVerifyingAbfsOutputStream.close(PageCRCVerifyingAbfsOutputStream.java:48)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
        at com.databricks.backend.daemon.data.common.CancellableOutputStream.cancel(CancellableOutputStream.scala:24)
        at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$create$4(DatabricksFileSystemV2.scala:630)
        at org.apache.spark.SparkUtils$.$anonfun$onTaskFailure$2(SparkUtils.scala:58)
        at org.apache.spark.SparkUtils$.$anonfun$onTaskFailure$2$adapted(SparkUtils.scala:57)
        at org.apache.spark.TaskContext$$anon$2.onTaskFailure(TaskContext.scala:177)
        at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskFailureListeners$1(TaskContextImpl.scala:150)
        at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskFailureListeners$1$adapted(TaskContextImpl.scala:150)
        at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:197)
        at org.apache.spark.TaskContextImpl.invokeTaskFailureListeners(TaskContextImpl.scala:150)
        at org.apache.spark.TaskContextImpl.markTaskFailed(TaskContextImpl.scala:127)
        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1663)
        ... 20 more

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2828)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2775)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2769)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2769)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1305)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1305)
    at scala.Option.foreach(Option.scala:407)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1305)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3036)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2977)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2965)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1067)
    at org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2477)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2460)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:274)
    ... 88 more
Caused by: org.apache.spark.SparkException: Task failed while writing rows.
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:396)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:284)
    at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)
    at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
    at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)
    at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)
    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)
    at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)
    at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
    at org.apache.spark.scheduler.Task.run(Task.scala:91)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:813)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1620)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:816)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:672)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    ... 1 more
Caused by: java.util.NoSuchElementException
    at org.apache.spark.sql.vectorized.ColumnarBatch$1.next(ColumnarBatch.java:69)
    at org.apache.spark.sql.vectorized.ColumnarBatch$1.next(ColumnarBatch.java:58)
    at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:44)
    at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$4.next(ArrowConverters.scala:401)
    at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$4.next(ArrowConverters.scala:382)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage138.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:757)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage183.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:757)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$2(FileFormatWriter.scala:374)
    at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1654)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:383)
    ... 19 more
    Suppressed: java.io.IOException: can not write PageHeader(type:DICTIONARY_PAGE, uncompressed_page_size:176, compressed_page_size:101, crc:-1384879305, dictionary_page_header:DictionaryPageHeader(num_values:22, encoding:PLAIN_DICTIONARY))
        at org.apache.parquet.format.Util.write(Util.java:224)
        at org.apache.parquet.format.Util.writePageHeader(Util.java:61)
        at org.apache.parquet.format.converter.ParquetMetadataConverter.writeDictionaryPageHeader(ParquetMetadataConverter.java:1190)
        at org.apache.parquet.hadoop.ParquetFileWriter.writeDictionaryPage(ParquetFileWriter.java:374)
        at org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writeToFileWriter(ColumnChunkPageWriteStore.java:238)
        at org.apache.parquet.hadoop.ColumnChunkPageWriteStore.flushToFileWriter(ColumnChunkPageWriteStore.java:316)
        at org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:202)
        at org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:127)
        at org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:41)
        at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:58)
        at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.abort(FileFormatDataWriter.scala:84)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$3(FileFormatWriter.scala:380)
        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1665)
        ... 20 more
    Caused by: shaded.parquet.org.apache.thrift.transport.TTransportException: java.io.IOException: Stream is closed!
        at shaded.parquet.org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:147)
        at shaded.parquet.org.apache.thrift.transport.TTransport.write(TTransport.java:107)
        at shaded.parquet.org.apache.thrift.protocol.TCompactProtocol.writeByteDirect(TCompactProtocol.java:482)
        at shaded.parquet.org.apache.thrift.protocol.TCompactProtocol.writeByteDirect(TCompactProtocol.java:489)
        at shaded.parquet.org.apache.thrift.protocol.TCompactProtocol.writeFieldBeginInternal(TCompactProtocol.java:252)
        at shaded.parquet.org.apache.thrift.protocol.TCompactProtocol.writeFieldBegin(TCompactProtocol.java:234)
        at org.apache.parquet.format.InterningProtocol.writeFieldBegin(InterningProtocol.java:74)
        at org.apache.parquet.format.PageHeader$PageHeaderStandardScheme.write(PageHeader.java:1068)
        at org.apache.parquet.format.PageHeader$PageHeaderStandardScheme.write(PageHeader.java:966)
        at org.apache.parquet.format.PageHeader.write(PageHeader.java:847)
        at org.apache.parquet.format.Util.write(Util.java:222)
        ... 33 more
    Caused by: java.io.IOException: Stream is closed!
        at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream.close(AbfsOutputStream.java:344)
        at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.PageCRCVerifyingAbfsOutputStream.close(PageCRCVerifyingAbfsOutputStream.java:48)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
        at com.databricks.backend.daemon.data.common.CancellableOutputStream.cancel(CancellableOutputStream.scala:24)
        at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$create$4(DatabricksFileSystemV2.scala:630)
        at org.apache.spark.SparkUtils$.$anonfun$onTaskFailure$2(SparkUtils.scala:58)
        at org.apache.spark.SparkUtils$.$anonfun$onTaskFailure$2$adapted(SparkUtils.scala:57)
        at org.apache.spark.TaskContext$$anon$2.onTaskFailure(TaskContext.scala:177)
        at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskFailureListeners$1(TaskContextImpl.scala:150)
        at org.apache.spark.TaskContextImpl.$anonfun$invokeTaskFailureListeners$1$adapted(TaskContextImpl.scala:150)
        at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:197)
        at org.apache.spark.TaskContextImpl.invokeTaskFailureListeners(TaskContextImpl.scala:150)
        at org.apache.spark.TaskContextImpl.markTaskFailed(TaskContextImpl.scala:127)
        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1663)
        ... 20 more
</code></pre>
<p>Job runs every month so it should not be a code error.</p>",0,2,2022-02-23 10:04:49.700000 UTC,,,0,apache-spark|pyspark|azure-databricks,37,2020-05-21 13:27:25.520000 UTC,2022-03-03 17:44:42.927000 UTC,,15,0,0,8,,,,,,[]
can not connect to azure sql dw using azure databricks,"<p>I am trying to connect to Azure SQL data warehouse but receiving this error. ""com.databricks.spark.sqldw.SqlDWSideException: SQL DW failed to execute the JDBC query produced by the connector""</p>

<p>further more it says that ""The connection string contains a badly formed name or value. [ErrorCode = 0] [SQLState = null]""</p>

<p>Please help!</p>",1,0,2019-12-20 13:25:07.823000 UTC,,,0,azure|azure-databricks|azure-sql-data-warehouse,1358,2019-08-28 09:16:12.017000 UTC,2021-09-20 09:13:53.217000 UTC,,25,0,0,89,,,,,,[]
How to dynamically pass ADLS gen2 folder/filename to Databricks Notebook in ADF or Databricks,"<p>I am using a Databricks Notebook Activity in ADF to transform files in ADLS gen2 folder. This folder is dynamic and a new folder is created on daily basis with daynumber. So I want my Databricks Notebook Activity to pickup foldername dynamically for each day to process files in that folder.</p>
<p>Can we do this ADF or within Databricks Notebook Activity?</p>",1,0,2020-08-25 17:13:08.193000 UTC,1.0,2020-08-27 19:09:32.213000 UTC,0,azure|azure-data-factory-2|azure-databricks,415,2015-11-10 15:15:50.220000 UTC,2021-06-14 14:16:06.627000 UTC,,23,1,0,15,,,,,,[]
Migration from StarTeam to distributed source control,"<p>We're currently using StarTeam for our project, and the license is expiring soon.  We're looking at moving to Team Foundation Server or something similar, but there's a push (mostly from myself and one other person) to move to some form of distributed version control.  One of the problems is that our change manager wants to be able to track and release by change request (as in StarTeam), and I don't believe this can be easily done with Mercurial or Git out-of-the-box (please correct me if I'm wrong).</p>

<p>This is a 15+ year-old product with thousands of java files and PL/SQL packages, maintained by 5-6 development subteams with a total of 30-40 developers.  To me, that seems like it would make a distributed repository a nightmare with the ""commit early, commit often"" mindset.  The fact that each team would be working on a completely different subsystem in the same repository makes it even nastier in my mind.</p>

<p>Our current StarTeam process for any change is as such:</p>

<ol>
<li>Lock the file(s) you want to work on,</li>
<li>Make your entire change and get it reviewed from a copy on a network drive,</li>
<li>Check in (and unlock) the changed files, hoping that someone hasn't forcibly broken your lock,</li>
<li>Hope that your change hasn't broken the build from someone else's change to other files</li>
</ol>

<p>Personally, I think the idea of ""locking"" files is ridiculous.  There should be enough communication between teams such that you don't need to.</p>

<p>Does anyone here have experience with distributed repositories on projects of a similar size?  Could you make any suggestions as to version control and/or change management solutions?  The main requirement is that the system be able to manage customer-initiated change requests and force developers to link their checkins to one.</p>

<p>Thanks.</p>",6,0,2011-12-06 23:42:30.960000 UTC,2.0,2011-12-07 18:51:43.417000 UTC,5,git|mercurial|dvcs|starteam,6757,2009-11-09 23:22:49.197000 UTC,2022-03-02 04:33:31.103000 UTC,Australia,1200,334,22,148,,,,,,[]
"Runtime error : SWIG std::function invocation failed, in azure databricks","<p>While using the Routing solver of the google or-tools,a runtime error is thrown. There was a no changes made in the code segment,before and after getting this error. Previously, it was working. But recently after a DB connection modification was made, I am getting this error. 
(Although, I doubt how a dB connection modification could affect the routing solver)</p>

<p>I am using the Azure Databricks notebook. As I am new to operations research, I have taken the example given in the <a href=""https://developers.google.com/optimization/routing/pickup_delivery#complete_programs"" rel=""nofollow noreferrer"">https://developers.google.com/optimization/routing/pickup_delivery#complete_programs</a> page, as my reference. </p>

<p>This is Vehicle Routing with Pick and Delivery problem.</p>

<pre><code>from __future__ import print_function
from ortools.constraint_solver import routing_enums_pb2
from ortools.constraint_solver import pywrapcp

def create_data_model():
    """"""Stores the data for the problem.""""""
    data = {}
    data['distance_matrix'] = dist
    data['pickups_deliveries'] = nodes_pickup_delivery
    data['num_vehicles'] = 2
    data['depot'] = 0
    return data

solution_list = []
def print_solution(data, manager, routing, assignment):
    """"""Prints assignment on console.""""""
    total_distance = 0 
    for vehicle_id in range(data['num_vehicles']):
        index = routing.Start(vehicle_id)
        plan_output = 'Route for vehicle {}:\n'.format(vehicle_id)
        route_distance = 0
        i = []
        while not routing.IsEnd(index):
            i.append(manager.IndexToNode(index))
            plan_output += ' {} -&gt; '.format(str(cityList[manager.IndexToNode(index)]))
            previous_index = index
            index = assignment.Value(routing.NextVar(index))
            route_distance += routing.GetArcCostForVehicle(previous_index, index, vehicle_id)
        solution_list.append(i)
        plan_output += '{}\n'.format(str(cityList[manager.IndexToNode(index)]))
        plan_output += 'Distance of the route: {} miles\n'.format(route_distance)
        #print(plan_output)
        total_distance += route_distance 
    #print('Total Distance of all routes: {} miles'.format(total_distance))

def main():
    """"""Entry point of the program.""""""
    # Instantiate the data problem.
    data = create_data_model()

    # Create the routing index manager.
    manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']), data['num_vehicles'], data['depot'])

    # Create Routing Model.
    routing = pywrapcp.RoutingModel(manager)

    # Define cost of each arc.
    def distance_callback(from_index, to_index):
        """"""Returns the manhattan distance between the two nodes.""""""
        # Convert from routing variable Index to distance matrix NodeIndex.
        from_node = manager.IndexToNode(from_index)
        to_node = manager.IndexToNode(to_index)
        return data['distance_matrix'][from_node][to_node]

    transit_callback_index = routing.RegisterTransitCallback(distance_callback)
    routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)

    # Add Distance constraint.
    dimension_name = 'Distance'
    routing.AddDimension(
        transit_callback_index,
        0,  # no slack
        40,  # vehicle maximum travel distance
        True,  # start cumul to zero
        dimension_name)
    distance_dimension = routing.GetDimensionOrDie(dimension_name)
    distance_dimension.SetGlobalSpanCostCoefficient(100)

    # Define Transportation Requests.
    for request in data['pickups_deliveries']:
        pickup_index = manager.NodeToIndex(request[0])
        delivery_index = manager.NodeToIndex(request[1])
        routing.AddPickupAndDelivery(pickup_index, delivery_index)
        routing.solver().Add(routing.VehicleVar(pickup_index) == routing.VehicleVar(delivery_index))
        routing.solver().Add(distance_dimension.CumulVar(pickup_index) &lt;= distance_dimension.CumulVar(delivery_index))

    # Setting first solution heuristic.
    search_parameters = pywrapcp.DefaultRoutingSearchParameters()
    #search_parameters.time_limit.seconds = 90
    search_parameters.first_solution_strategy = (routing_enums_pb2.FirstSolutionStrategy.PARALLEL_CHEAPEST_INSERTION)

    # Solve the problem.
    assignment = routing.SolveWithParameters(search_parameters)

    # Print solution on console.
    if assignment:
        print_solution(data, manager, routing, assignment)


if __name__ == '__main__':
    main()
</code></pre>

<p>The error I am getting is pointing to the following code segment:  'plan_output = 'Route for vehicle {}:\n'.format(vehicle_id)'
The error thrown is:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>RuntimeError: SWIG std::function invocation failed.

RuntimeErrorTraceback (most recent call last)
&lt;command-2714173895177597&gt; in &lt;module&gt;()
     89 
     90 if __name__ == '__main__':
---&gt; 91     main()

&lt;command-2714173895177597&gt; in main()
     85     # Print solution on console.
     86     if assignment:
---&gt; 87         print_solution(data, manager, routing, assignment)
     88 
     89 

&lt;command-2714173895177597&gt; in print_solution(data, manager, routing, assignment)
     18     for vehicle_id in range(data['num_vehicles']):
     19         index = routing.Start(vehicle_id)
---&gt; 20         plan_output = 'Route for vehicle {}:\n'.format(vehicle_id)
     21         route_distance = 0
     22         i = []

RuntimeError: SWIG std::function invocation failed.</code></pre>
</div>
</div>
</p>

<p>Kindly help. </p>",1,4,2019-07-05 09:22:16.433000 UTC,1.0,2019-07-05 09:38:31.800000 UTC,0,swig|azure-databricks|or-tools|google-developer-tools|vehicle-routing,408,2017-03-29 18:50:29.467000 UTC,2021-02-23 05:52:42.913000 UTC,"Hyderabad, Telangana, India",25,1,0,60,,,,,,[]
Merging files within the Azrue Data Lake container,"<p>Consider below scenario:</p>
<p>I want my data flow like following
import container ---&gt; databricks (Transform)--&gt; export container</p>
<p><strong>Current situation after I am done with the transformation process
container:</strong></p>
<pre><code>---import
    --folder
        --mydata.csv
---export
    --folder
        --part-1-transformed-mydata.csv
        --part-2-transformed-mydata.csv
        --part-3-transformed-mydata.csv
        --initial.txt
        --success.txt
        --finish.txt
</code></pre>
<p><strong>I want below structure:</strong></p>
<pre><code>---import
    --folder
        --mydata.csv
---export
    --folder
        --transformed-mydata.csv
</code></pre>
<p>What should be preferred way (considering data is of few GBs &lt;10) within data-bricks or I am happy to use any functionality in Data Factory as I am using this data-bricks notebook as a step in pipeline .</p>
<p><strong>Note</strong> : I am using Apache Spark 3.0.0, Scala 2.12 in data-bricks with 14 GB Memory, 4 Cores. Cluster type is standard</p>",1,0,2020-11-08 14:35:51.717000 UTC,,,0,pyspark|azure-data-factory-2|azure-databricks,25,2020-03-30 15:32:10.280000 UTC,2021-05-29 18:38:18.123000 UTC,"Bangalore, Karnataka, India",37,5,0,15,,,,,,[]
Mount ADLS Gen2 to Databricks when firewall is enabled,"<p>When I am trying to mount ADLS Gen2 to Databricks, I have this issue : ""StatusDescription=This request is not authorized to perform this operation"" if the ADLS Gen2 firewall is enabled. But the request works fine if the firewall is disabled. </p>

<p>Someone can help please ?</p>

<pre><code>configs = {""fs.azure.account.auth.type"": ""OAuth"",
               ""fs.azure.account.oauth.provider.type"": ""org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"",
               ""fs.azure.account.oauth2.client.id"": clientID,
               ""fs.azure.account.oauth2.client.secret"": keyID,
               ""fs.azure.account.oauth2.client.endpoint"": ""https://login.microsoftonline.com/"" + tenantID + ""/oauth2/token""}

dbutils.fs.mount(
  source = ""abfss://"" + fileSystem + ""@"" + accountName + "".dfs.core.windows.net/"",
  mount_point = ""/mnt/adlsGen2"",
  extra_configs = configs)

StatusCode=403
StatusDescription=This request is not authorized to perform this operation.
ErrorCode=
ErrorMessage=
    at shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:134)
    at shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:498)
    at shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getIsNamespaceEnabled(AzureBlobFileSystemStore.java:164)
    at shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:445)
    at shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:362)
    at com.databricks.backend.daemon.dbutils.DBUtilsCore.verifyAzureFileSystem(DBUtilsCore.scala:486)
    at com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:435)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
    at py4j.Gateway.invoke(Gateway.java:295)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:251)
    at java.lang.Thread.run(Thread.java:748)
</code></pre>",4,0,2019-05-21 14:00:38.647000 UTC,,,1,azure|azure-data-lake|azure-databricks,1740,2018-04-25 15:51:11.310000 UTC,2020-06-09 12:39:29.650000 UTC,,11,0,0,3,,,,,,[]
Import error for csv file azure databricks,"<p>I have a csv file which has a lot of text data. I am trying to import it in azure databricks using python pandas but it is giving me a long list of errors but primarily its telling me this:- ERROR: Internal Python error in the inspect module. However, when I am putting file in local desktop and then importing it on local desktop using jupyter/spyder it is imported without any errors.
I have also put in option of encoding UTF-8 while importing it in azure databricks but its still showing error. Any idea how to tackle this?</p>",1,0,2021-08-04 09:58:50.533000 UTC,,,-1,python|pandas|azure|azure-databricks,119,2021-01-10 11:52:48.370000 UTC,2022-03-03 09:47:15.547000 UTC,,7,0,0,3,,,,,,[]
Append only new aggregates based on groupby keys,"<p>I have to process some files which arrive to me daily. The information have primary key <code>(date,client_id,operation_id)</code>. So I created a Stream which append only new data into a delta table:</p>

<pre class=""lang-py prettyprint-override""><code>operations\
        .repartition('date')\
        .writeStream\
        .outputMode('append')\
        .trigger(once=True)\
        .option(""checkpointLocation"", ""/mnt/sandbox/operations/_chk"")\
        .format('delta')\
        .partitionBy('date')\
        .start('/mnt/sandbox/operations')
</code></pre>

<p>This is working fine, but i need to summarize this information grouped by <code>(date,client_id)</code>, so i created another streaming from this operations table to a new table:</p>

<pre class=""lang-py prettyprint-override""><code>summarized= spark.readStream.format('delta').load('/mnt/sandbox/operations')

summarized= summarized.groupBy('client_id','date').agg(&lt;a lot of aggs&gt;)

summarized.repartition('date')\
        .writeStream\
        .outputMode('complete')\
        .trigger(once=True)\
        .option(""checkpointLocation"", ""/mnt/sandbox/summarized/_chk"")\
        .format('delta')\
        .partitionBy('date')\
        .start('/mnt/sandbox/summarized')
</code></pre>

<p>This is working, but every time I got new data into <code>operations</code> table, spark recalculates <code>summarized</code> all over again. I tried to use the append mode on the second streaming, but it need watermarks, and the date is DateType.</p>

<p>There is a way to only calculate new aggregates based on the group keys and append them on the <code>summarized</code>?</p>",1,0,2019-09-25 13:54:56.873000 UTC,0.0,2019-09-25 17:18:29.717000 UTC,0,apache-spark|pyspark|spark-structured-streaming|azure-databricks|delta-lake,185,2013-12-19 03:32:00.203000 UTC,2022-03-04 22:14:52.737000 UTC,Brasil,779,96,5,92,,,,,,[]
How to read bulk excel file data and load into spark dataframe in Databricks,"<p>i want to read the bulk excel data which contains 800k records and 230 columns in it. I have read data using spark and pandas dataframe , but while reading the data using spark data frame i'm getting the following message.</p>

<blockquote>
  <p>Message: The spark driver has stopped unexpectedly and is restarting. Your notebook will be automatically reattached.</p>
</blockquote>

<p>I have used below code using spark.</p>

<pre><code>df=spark.read.format(""com.crealytics.spark.excel"").option(""useheader"",""true"").option(""treatEmptyValuesAsNulls"",""true"").option(""inferSchema"", ""true"").option(""addColorColumns"", ""False"").option(""location"",""/dbfs/FileStore/test/abc.xlsx"").load()

Using scala:

import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.DataFrame
def readExcel(file: String): DataFrame = sqlContext.read
   .format(""com.crealytics.spark.excel"")
   .option(""location"", file)
   .option(""useHeader"", ""true"")
   .option(""treatEmptyValuesAsNulls"", ""true"")
   .option(""inferSchema"", ""true"")
   .option(""addColorColumns"", ""False"")
   .load()

val data = readExcel(""/dbfs/test/abc.xlsx"")
data.show(false)
</code></pre>",1,0,2019-06-04 15:34:39.353000 UTC,,2019-06-04 16:24:02.160000 UTC,0,python-3.x|pandas|pyspark|azure-databricks,655,2019-06-04 15:19:09.203000 UTC,2020-01-06 09:01:08.227000 UTC,"Hyderabad, Telangana, India",1,0,0,3,,,,,,[]
Spark Delta Table Add new columns in middle Schema Evolution,"<p>Have to ingest a file with new column into a existing table structure.</p>
<pre><code>create table sch.test (
name string ,
address string 
) USING DELTA 
--OPTIONS ('mergeSchema' 'true')
PARTITIONED BY (name)
LOCATION  '/mnt/loc/fold'
TBLPROPERTIES (delta.autoOptimize.optimizeWrite = true, delta.autoOptimize.autoCompact = true);
</code></pre>
<p>Code to read the file:
val df = spark.read
.format(&quot;com.databricks.spark.csv&quot;)
.option(&quot;header&quot;, &quot;true&quot;)
.load(&quot;/mnt/loc/fold&quot;)</p>
<pre><code>display(df)
</code></pre>
<p>File in path contains below data</p>
<pre><code>name,address
raghu,india
raj,usa
</code></pre>
<p>On writing it to a table,</p>
<pre><code> import org.apache.spark.sql.functions._
df.withColumn(&quot;az_insert_ts&quot;, current_timestamp())
.withColumn(&quot;exec_run_id&quot;,lit(&quot;233&quot;))
.withColumn(&quot;az_inp_file_name&quot;,lit(&quot;24234filename&quot;))
     .coalesce(12)
     .write
     .mode(&quot;append&quot;)
     .option(&quot;mergeSchema&quot;, &quot;true&quot;)
     .format(&quot;delta&quot;)
     .saveAsTable(&quot;sch.test&quot;)
display(spark.read.table(&quot;sch.test&quot;))
</code></pre>
<p><a href=""https://i.stack.imgur.com/ef9TJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ef9TJ.png"" alt=""enter image description here"" /></a></p>
<p>Adding a new column,</p>
<pre><code>name,address,age
raghu,india,12
raj,usa,13
</code></pre>
<p>Read the file,</p>
<pre><code>    val df = spark.read
        .format(&quot;com.databricks.spark.csv&quot;)
        .option(&quot;header&quot;, &quot;true&quot;)
        .load(&quot;/mnt/loc/fold&quot;)

display(df)
</code></pre>
<p>While writing into the table using insertInto,</p>
<pre><code>import org.apache.spark.sql.functions._
df.withColumn(&quot;az_insert_ts&quot;, current_timestamp())
.withColumn(&quot;exec_run_id&quot;,lit(&quot;233&quot;))
.withColumn(&quot;az_inp_file_name&quot;,lit(&quot;24234filename&quot;))
     .coalesce(12)
     .write
     .mode(&quot;append&quot;)
     .option(&quot;mergeSchema&quot;, &quot;true&quot;)
     .format(&quot;delta&quot;)
     .insertInto(&quot;sch.test&quot;)
display(spark.read.table(&quot;sch.test&quot;))
</code></pre>
<p>Getting the below error,</p>
<p><a href=""https://i.stack.imgur.com/LF37U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LF37U.png"" alt=""enter image description here"" /></a></p>",1,1,2021-06-29 00:05:52.197000 UTC,1.0,,0,azure|apache-spark|azure-databricks|delta-lake,742,2013-03-12 04:29:36.407000 UTC,2022-03-06 01:52:55.413000 UTC,,1339,366,25,444,,,,,,[]
tinkerpop gremlin clear console not working,"<p>i'm a newbie to tinkerpop gremlin. querying neptune DB using apache-tinkerpop-gremlin-console</p>
<blockquote>
<p>gremlin&gt; :c</p>
<p>gremlin&gt; :clear</p>
</blockquote>
<p>is not clear the screen / console as &quot;cls&quot; command clears the screen in windows cmd. Please comment / give suggestions to clear the gremlin console</p>",1,0,2020-04-01 15:55:52.563000 UTC,,2020-06-20 09:12:55.060000 UTC,2,gremlin|tinkerpop|amazon-neptune,265,2015-02-04 05:25:51.150000 UTC,2022-01-12 09:48:00.607000 UTC,"Chennai, Tamil Nadu, India",185,44,0,104,,,,,,[]
You may get a different result due to the upgrading of Spark 3.0: Fail to parse 'JUN 2001' in the new parser,"<p>I'm working on converting a string to date in SparkSQL. The string format is &quot;Jun 2001', I tried to convert to date format as 'MMMyyyy' by using the query below</p>
<pre><code>SELECT 
       TO_DATE(CAST('JUN 2001'AS STRING),'MMMyyyy')
</code></pre>
<p>However, it kept giving me error as below,</p>
<pre><code>*Error in SQL statement: SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse 'JUN 2001' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.*
</code></pre>
<p>Is there any solution for it?</p>
<p>Thanks!</p>",0,1,2022-01-20 20:47:16.317000 UTC,,,1,sql|apache-spark-sql|azure-databricks,95,2022-01-11 16:44:03.893000 UTC,2022-03-03 15:44:24.857000 UTC,,11,0,0,0,,,,,,[]
Databricks: Querying Production Cluster table from Development Server,"<p>Disclaimer: Not a code query, but directly related to it.</p>
<p>I find it difficult in Databricks to handle such scenarios where there's no shell prompt; just the notebooks. I have two clusters on Azure dev &amp; prod. The database &amp; tables can be accessed via Databricks Notebooks of separate environments.</p>
<p>The problem arises when I want to:</p>
<ol>
<li>Query data in dev, but from prod environment &amp; vice-versa. On a sql prompt, it just seems impossible to achieve this.</li>
<li>If I want to populate dev table from prod table; there's no way to establish a connection from within the dev notebook to query the table of prod environment.</li>
</ol>
<p>The workaround I've established for now to copy the prod data into dev is:</p>
<ul>
<li>Download full dump from production in <code>csv</code> in my local machine.</li>
<li>Upload to <code>DBFS</code> in dev environment.</li>
<li>Create temp table/directly insert the <code>csv</code> in the dev table.</li>
</ul>
<p>Any comments on how I remove this download-upload process &amp; query prod directly from dev notebook?</p>",1,0,2020-07-16 04:53:49.180000 UTC,,,-1,azure|azure-databricks,50,2012-03-02 07:05:46.313000 UTC,2022-03-03 21:52:24.853000 UTC,,780,78,7,213,,,,,,[]
Spark dataframe with complex & nested data,"<p>I have 3 dataframes currently
Call them dfA, dfB, and dfC</p>

<p>dfA has 3 cols</p>

<hr>

<h2>|Id | Name | Age |</h2>

<p>dfB has say 5 cols. the 2nd col, is a FK reference back to dFA record.</p>

<hr>

<h2>|Id | AId | Street | City | Zip |</h2>

<p>Similarily dfC has 3 cols, also with a reference back to dfA</p>

<hr>

<h2>|Id  | AId  |  SomeField  |</h2>

<p>Using Spark SQL i can do an JOIN across the 3</p>

<pre><code>%sql

SELECT * FROM dfA
INNER JOIN dfB ON dfA.Id = dfB.AId
INNER JOIN dfC ON dfA.Id = dfC.AId
</code></pre>

<p>And I'll get my resultset, but it's been ""flattened"" as SQL would do with tabular results like this.</p>

<p>I want to load it in to a complex schema like this </p>

<pre><code>val destinationSchema = new StructType()
  .add(""id"", IntegerType)
  .add(""name"", StringType)
  .add(""age"", StringType)
  .add(""b"", 
       new StructType()
        .add(""street"", DoubleType, true)
        .add(""city"", StringType, true)
    .add(""zip"", StringType, true)
      )
  .add(""c"",
       new StructType()
        .add(""somefield"", StringType, true)
      )
</code></pre>

<p>Any ideas how to take the results of the SELECT and save to dataframe with specifying the schema? </p>

<p>I ultimately want to save the complex StructType, or JSON, and load this is to Mongo DB using the Mongo Spark Connector.</p>

<p>Or, is there a better way to accomplish this from the 3 seperate dataframes (which were originally 3 seperate CSV files that were read in)?</p>",2,2,2019-04-18 21:54:03.777000 UTC,3.0,2019-04-19 04:24:36.253000 UTC,1,scala|apache-spark|apache-spark-sql|azure-databricks,506,2014-08-05 01:24:49.907000 UTC,2022-03-02 03:20:10.327000 UTC,"Redmond, WA",2634,121,20,353,,,,,,[]
Implement DevOps on DataBricks DBFS files,"<p>I am trying to implement DevOps on Azure Data Factory and Azure Databricks.</p>
<p>I have completed the devops implementation for ADF DevOps and Databricks notebook files.</p>
<p>After deployment it seems like there are some issues with ADF pipelines which are fetching jar files stored in dbfs location.</p>
<p>One of the pipeline is shown below.
<a href=""https://i.stack.imgur.com/6vICo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6vICo.png"" alt=""enter image description here"" /></a>
the path to the jar file is
<code>dbfs:/FileStore/jars/xxx............xxx1_0_SNAPSHOT-c073a.jar </code>
After deploying the ADF pipelines into the PROD environment, where it is pointing to the PROD databricks instance, the jar file with same name will not be available. This is causing the pipeline to fail in PROD.</p>
<p>How to fetch the jars from DEV dbfs location and deploy this into PROD dbfs location with same name in order to make the ADF pipeline run?</p>
<p>Below given is the method I am now following to implement DevOps.</p>
<ol>
<li>I have created a build pipeline which point to the git repository and build the jar files by executing pom.xml file</li>
<li>Created a release pipeline which will copy the jar files from build artifact to <code>FileStore/jars/</code></li>
<li>Now the ADF pipeline will point to the jar available in <code>FileStore/jars/</code></li>
</ol>
<p>Is there any alternative method to resolve this or is this the proper approach?</p>",1,0,2020-11-24 14:48:06.173000 UTC,,2020-11-26 07:29:16.323000 UTC,0,azure|azure-devops|azure-data-factory|azure-data-factory-2|azure-databricks,312,2017-04-04 05:54:01.927000 UTC,2022-03-05 05:59:02.213000 UTC,,743,45,1,284,,,,,,[]
Limiting gremlin query duration on the client side in NodeJs,"<p>We are using the gremlin SDK for NodeJs and experiencing situations where the queries get stuck.<br />
I was wondering if there is a way to limit the query duration on the client-side, so if there are some networking issues or similar stuff the application won't get stuck.<br />
I've seen the <a href=""https://stackoverflow.com/q/47718808/927477"">Promise.race</a> option, but I'm concerned about sockets leak.</p>
<ul>
<li>Just to be clear, I'm not looking to limit the server-side</li>
</ul>",0,0,2021-09-02 12:26:33.837000 UTC,,2021-09-02 12:41:51.010000 UTC,0,node.js|gremlin|amazon-neptune,26,2011-09-04 11:21:23.543000 UTC,2022-03-06 00:01:34.273000 UTC,,6121,565,1,373,,,,,,[]
Gremlin Comparing DateTime,"<p>I have the following model</p>

<p>(Club)-HAS-(Match)-AT-(Datetime)
And
(Club)-HAS-(Player)-UNAVAILABLE-(Datetime)</p>

<p>I am using Amazon Neptune to run this, and i am running in the following situation:</p>

<p>I should be able to for each match that a club has, identify which player are available</p>

<ol>
<li>I need to check the date of each match Match</li>
<li>For each match, identify the club</li>
<li>From club, get players</li>
<li>From player, check if he has an unavailable state on the same match-DateTime </li>
</ol>

<p>How could I run this in Neptune?</p>

<p>thanks</p>",1,0,2019-10-18 13:02:29.107000 UTC,,2019-11-03 16:06:46.820000 UTC,0,graph-databases|gremlin|tinkerpop|amazon-neptune,387,2011-12-15 15:51:09.583000 UTC,2020-09-25 07:57:48.100000 UTC,Neverland,1112,20,1,139,,,,,,[]
Deleting records in a table with billion records using spark or scala,"<p>we have a table in Azure Data Warehouse with 17 billion records. Now we have a scenario where we have to delete records from this table based on some where condition. We are writing Spark in Scala language in Azure Databricks notebooks.</p>

<p>We searched for different options to do this in Spark, but all suggested to first read the entire table, delete records from this and then overwrite the entire table in Data Warehosue. However this approach will not work in our case due to huge number of records in our table.</p>

<p>Can you please suggest how we can achieve this functionality using spark/scala?</p>

<p>1) checked if we can call stored procedure through spark/scala code in azure databricks but Spark do not support stored procedures.</p>

<p>2) Tried reading the entire table first to delete the records but it goes into never ending loop.</p>",1,5,2019-07-30 18:47:34.843000 UTC,,2019-07-31 10:25:00.277000 UTC,0,sql|scala|apache-spark|azure-databricks,317,2019-07-30 18:35:25.937000 UTC,2020-03-31 14:24:47.587000 UTC,,1,0,0,1,,,,,,[]
How to call notebook or run jobs from C# in databricks using Mobius?,"<p>I'm new to Databricks.Is it possible to send the code passing through API (like Mobius) from C# to run jobs in Databricks ?</p>

<p>Could you possibly give me some code example ? 
such as if I want to run some job in notebook which contain the NoSql code in there.</p>

<p>Thank you.</p>",1,1,2019-03-13 15:35:33.007000 UTC,,,1,c#|azure-hdinsight|azure-databricks|mobius,754,2019-03-13 15:23:44.617000 UTC,2020-10-16 09:59:25.910000 UTC,,11,0,0,2,,,,,,[]
Mapping dataflow creating output files in root directory irrespective of given dataset path,"<p>I am trying to split a file having month data into separate day wise files using dataflow. 
I am storing the filename as a derived column and using this column as fileName in sink settings as shown below. The filename in the column is like Transactions_[date].csv</p>

<p><a href=""https://i.stack.imgur.com/lGJaY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lGJaY.png"" alt=""enter image description here""></a></p>

<p>Then i am partitioning the file based on this column in optimise -> partition type- key -> Unique value per partition ->key column -> fileName</p>

<p><a href=""https://i.stack.imgur.com/yQHUl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yQHUl.png"" alt=""enter image description here""></a></p>

<p>PROBLEM:
The dataflow runs perfectly. I can see the temporary files in my desired location during runtime but after completion, the files end up in the root location(inside container) and not inside my desired folder. </p>

<p><a href=""https://i.stack.imgur.com/5YdB6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5YdB6.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/ysuux.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ysuux.png"" alt=""enter image description here""></a></p>

<p>Update:
Dataset settings (hardcoded now but will be using dataset param)
<a href=""https://i.stack.imgur.com/NuHvk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NuHvk.png"" alt=""enter image description here""></a></p>",0,3,2020-01-21 14:05:46.263000 UTC,,2020-01-22 09:56:47.733000 UTC,1,azure|azure-data-factory|azure-data-factory-2|azure-databricks|azure-data-flow,157,2017-03-01 14:17:38.240000 UTC,2022-03-02 07:48:09.767000 UTC,"Bengaluru, Karnataka, India",71,2,0,5,,,,,,[]
Connection between Azure Data Factory and Databricks,"<p>I'm wondering what is the most appropriate way of accessing databricks from Azure data factory. </p>

<p>Currently I've got databricks as a linked service to which I gain access via a generated token.</p>",1,2,2019-11-01 08:50:46.350000 UTC,,,0,azure|azure-data-factory|azure-data-factory-2|azure-databricks,151,2018-11-29 12:34:52.167000 UTC,2022-01-11 15:44:10.807000 UTC,Romania,59,5,0,7,,,,,,[]
Unable to retrieve data from AWS Neptune,"<p>I have created an AWS Appsync graphql api which on being called will run an aws lambda function and that function will query data from AWS Neptune using query language Gremlin but i am unable to retrieve data from aws neptune.</p>
<p>This is my lambda function</p>
<pre><code>// import * as gremlin from &quot;gremlin&quot;;
const gremlin = require(&quot;gremlin&quot;)

const DriverRemoteConnection = gremlin.driver.DriverRemoteConnection;
const Graph = gremlin.structure.Graph;
const uri = process.env.READER_ENDPOINT


const HighestRatedByCuisine = async () =&gt; {

    let dc = new DriverRemoteConnection(`wss://${uri}/gremlin`, {});
    const graph = new Graph();
    const g = graph.traversal().withRemote(dc)

    try {
        let data = await g.V().has(&quot;Person&quot;, &quot;name&quot;, &quot;Muhammad Ahsen Riaz&quot;).
            out(&quot;lives&quot;).in_(&quot;within&quot;).where(__.out(&quot;serves&quot;).
                has(&quot;name&quot;, P.within(&quot;Burgers&quot;))).
            where(__.inE(&quot;about&quot;)).
            group().
            by(__.identity()).
            by(__.in_(&quot;about&quot;).values(&quot;rating&quot;)).
            order().
            by(values, Order.desc).
            unfold().
            order().by(values, Order.desc).limit(1).
            project(&quot;name&quot;, &quot;address&quot;, &quot;rating_average&quot;, &quot;cuisine&quot;).
            by(__.select(Column.keys).values(&quot;name&quot;)).
            by(__.select(Column.keys).values(&quot;address&quot;)).
            by(__.select(Column.values)).
            by(__.select(Column.keys).out(&quot;serves&quot;).values(&quot;name&quot;)).next()

            console.log(&quot;Onlydata&quot; , data)
            console.log(&quot;Data&gt;&gt;&gt;&quot; , data.value)
            console.log(&quot;data&gt;&gt;&gt;&gt;&gt;&quot; , data.done)
            console.log(&quot;JsonData&quot; , JSON.stringify(data))


        return data.value
     
    }

    catch (err) {
        console.log(&quot;there is an error &quot; , err)
        return err
    };

}

export default HighestRatedByCuisine


</code></pre>
<p>where as this is my schema</p>
<pre><code>type HighestRatedRestaurantWithSpecificCuisine {
    name : String
    address : String
    cuisine : String
    rating_average : String
}

type Query {
 getHighestRatedRestaurantWithSpecificCuisine : HighestRatedRestaurantWithSpecificCuisine
}

</code></pre>
<p>Whenever a query throught appsync it returns null but when i console it in AWS CloudWatch it contains data in the form</p>
<pre><code>    Data&gt;&gt;&gt; Map {
  'name' =&gt; 'Burger Lab',
  'address' =&gt; '3648 Alek Forge',
  'rating_average' =&gt; '4',
  'cuisine' =&gt; 'Burgers' }


</code></pre>",0,4,2021-06-08 11:35:20.993000 UTC,,2021-06-15 19:02:41.557000 UTC,0,amazon-web-services|amazon-cloudwatch|gremlin|aws-appsync|amazon-neptune,124,2020-12-14 18:15:37.840000 UTC,2021-12-25 22:51:11.007000 UTC,Pakistan,1,0,0,3,,,,,,[]
View Spark UI for Jobs executed via Azure ADF,"<p>I am not able to view the spark-ui for databricks jobs executed through notebook activity in Azure datafactory.</p>
<p>Does anyone know which permissions needs to be added to enable the same?</p>
<p><a href=""https://i.stack.imgur.com/FY4PH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FY4PH.png"" alt=""enter image description here"" /></a></p>",1,3,2021-11-10 01:56:11.407000 UTC,,2021-11-11 05:17:24.687000 UTC,0,permissions|azure-data-factory|azure-databricks|spark-ui,65,2013-03-12 04:29:36.407000 UTC,2022-03-06 01:52:55.413000 UTC,,1339,366,25,444,,,,,,[]
Sending time ordered events into Kafka,"<p>I am using Autoloader (from Databricks) to ingest some parquet files and send them later to a Kafka topic.</p>
<p>I am able to read the files and write them without any problem but I have doubts about the order.</p>
<p>These files contain a <code>timestamp</code> field inside the payload which indicates the modification date of the file.</p>
<p>Is it possible to write each of the events that I receive with the autoloader in the Kafka sink ordered by that date?</p>
<p>I would like to be able to write in Kafka from the oldest to the newest events based on this <code>timestamp</code>.</p>
<p>I have considered to define a function that is going to be invoked the <code>foreachBatch</code> in which it makes a simple <code>orderBy</code> for each batch.
Something like this:</p>
<pre class=""lang-scala prettyprint-override""><code>
def orderByFunc ( batchDF:DataFrame, batchID:Long ) : Unit = {

  val rodered_df=batchDF.orderBy($&quot;some_field&quot;.desc) // order by the timestamp field
  rodered_df.write.format(&quot;kafka&quot;).option(...) // write into Kafka
 
}

streamingInputDF
                .writeStream
                .queryName(job_name)
                .option(&quot;checkpointLocation&quot;, checkpoint_path)
                .foreachBatch(orderByFunc _)
                .start()
</code></pre>
<p>Is there a less cumbersome way? am I missing something?</p>
<p>Thank you very much to all</p>",0,3,2021-06-28 11:27:36.877000 UTC,,,0,azure|apache-kafka|spark-structured-streaming|azure-databricks,50,2020-05-04 14:44:15.777000 UTC,2022-03-03 15:42:10.357000 UTC,,105,7,0,31,,,,,,[]
Migrating away from Clearcase,"<p>We are migrating from Clearcase to another VCS (probably either SVN or Mercurial).  For companies that have made this transition, what factors did they find important in selecting another VCS tool, and what practices did they find eased the transition?</p>",7,0,2009-08-06 15:22:12.737000 UTC,,2009-09-06 00:19:54.307000 UTC,8,svn|mercurial|clearcase|dvcs,1277,2008-12-24 15:09:09.530000 UTC,2018-10-10 19:49:48.643000 UTC,"New York, NY",81,0,0,72,,,,,,[]
DELETE WHERE not working with property path sequences sparql,"<p>PART 1:</p>

<p>These are the triplets which already exists.</p>

<pre><code>&lt;http:o1&gt;   &lt;http:name&gt;   ""name""^^xsd:string
&lt;http:o1&gt;   &lt;http:place&gt;   ""place""^^xsd:string
&lt;http:o1&gt;   &lt;http:hasContained&gt;   &lt;http:o2&gt;
&lt;http:o2&gt;   &lt;http:name&gt;   ""name1""^^xsd:string
&lt;http:o2&gt;   &lt;http:place&gt;   ""place2""^^xsd:string
&lt;http:o2&gt;   &lt;http:hasContained&gt;   &lt;http:o3&gt;
&lt;http:o3&gt;   &lt;http:name&gt;   ""name3""^^xsd:string
&lt;http:o3&gt;   &lt;http:place&gt;   ""place3""^^xsd:string
</code></pre>

<p>I want to delete node properties which are 2 nodes away from the o1 node.</p>

<pre><code>delete where { &lt;http:o1&gt; &lt;http:hasContained&gt;/&lt;http:hasContained&gt; ?s. ?s ?p ?o}
</code></pre>

<p>I came up with this query to remove o3 node related triplets. But when I run this query, I am getting some errors.</p>

<pre><code>Malformed query: Encountered "" ""/"" ""/ """" at line 1, column 731.
Was expecting one of:
    ""("" ...
    ""["" ...
    &lt;NIL&gt; ...
    &lt;ANON&gt; ...
    ""true"" ...
    ""false"" ...
    &lt;Q_IRI_REF&gt; ...
    &lt;PNAME_NS&gt; ...
    &lt;PNAME_LN&gt; ...
    &lt;BLANK_NODE_LABEL&gt; ...
    &lt;VAR1&gt; ...
    &lt;VAR2&gt; ...
    &lt;INTEGER&gt; ...
    &lt;INTEGER_POSITIVE&gt; ...
    &lt;INTEGER_NEGATIVE&gt; ...
    &lt;DECIMAL&gt; ...
    &lt;DECIMAL_POSITIVE&gt; ...
    &lt;DECIMAL_NEGATIVE&gt; ...
    &lt;DOUBLE&gt; ...
    &lt;DOUBLE_POSITIVE&gt; ...
    &lt;DOUBLE_NEGATIVE&gt; ...
    &lt;STRING_LITERAL1&gt; ...
    &lt;STRING_LITERAL2&gt; ...
    &lt;STRING_LITERAL_LONG1&gt; ...
    &lt;STRING_LITERAL_LONG2&gt; ... 
</code></pre>

<p>With some alternate queries, I could do the job. </p>

<p>But what is the mistake in the above query?</p>

<p>Why path property query not working with delete where?  </p>

<p>PART 2:</p>

<p>For the same triplets data, the query to remove all the triplets used is</p>

<pre><code>delete {?s ?p ?o} where { &lt;http:o1&gt; (&lt;http:hasContained&gt;/&lt;http:hasContained&gt;?)? ?s. ?s ?p ?o}
</code></pre>

<p>which is not deleting any data from the triple store. Whereas by using construct, I am able to retrieve the data with the same where clause.</p>

<pre><code>construct {?s ?p ?o} where { &lt;http:o1&gt; (&lt;http:hasContained&gt;/&lt;http:hasContained&gt;?)? ?s. ?s ?p ?o}
</code></pre>

<p>What is the issue in these queries, Am I missing something?</p>",2,0,2020-04-28 05:37:57.757000 UTC,,2020-04-29 16:52:39.503000 UTC,0,sparql|amazon-neptune|rdf4j,244,2017-04-06 18:53:11.143000 UTC,2021-12-10 06:59:36.113000 UTC,"Gurgaon, Haryana, India",302,31,0,52,,,,,,[]
Optimize NeptuneDB Gremlin query,"<p>vehicles --&gt; accounts --&gt; organizations &lt;-- users</p>
<p>We have the above graph structure where vechicles , accounts, organizations and users are vertex labels and the arrows indicate the edge direction.</p>
<p>Consider the following number of vertices :</p>
<pre><code>organizations = 1
accounts per organizations = 2
vehciles per account = 5000
users per organizations = 100
</code></pre>
<p>Our requirement is , given two vertexIds , find a set of all users and vehicles that satisfy the above graph.</p>
<p>For example if I have vertex1 = accounts:1 and vertex2 = organizations:1 , find the set of users and vehicles that are part of these two vertices.</p>
<p>We have the following query</p>
<pre><code>g.V('accounts:1').outE().otherV().hasId('organizations:1')
.V('accounts:1').inE().otherV().as('B')
.V('organizations:1').inE().otherV().as('A')
.select('A', 'B')
</code></pre>
<p>While this works , the query takes ~3.5 seconds to complete , now we know that there are going to be 500000 traversers for this query.</p>
<p>Is there a better way to do this ?</p>
<p>Thanks for the help</p>
<p>Edit #1 : Attaching the query's profile API response</p>
<pre><code>  Optimized Traversal
===================
Neptune steps:
[
    NeptuneGraphQueryStep(VertexId)@[A, B] {
        JoinGroupNode {
            JoinGroupNode {
                PatternNode[(?1=&lt;accounts:1&gt;, &lt;lifestate&gt;, &quot;ACTIVE&quot;, ?) . project ?1 .], {estimatedCardinality=1799504, expectedTotalOutput=1, indexTime=0, joinTime=1, numSearches=1, actualTotalOutput=1}
                PatternNode[(?1, ?5, ?3=&lt;organizations:1&gt;, ?6) . project ?1,?6,?3 . IsEdgeIdFilter(?6) .], {estimatedCardinality=102, expectedTotalOutput=1, indexTime=0, joinTime=0, numSearches=1, actualTotalOutput=1}
                PatternNode[(?6, &lt;lifestate&gt;, &quot;ACTIVE&quot;, ?) . project ask .], {estimatedCardinality=3341886, expectedTotalOutput=1, indexTime=0, joinTime=2, numSearches=1, actualTotalOutput=1}
                PatternNode[(?3, &lt;lifestate&gt;, &quot;ACTIVE&quot;, ?) . project ask .], {estimatedCardinality=1799504, expectedTotalOutput=1, indexTime=0, joinTime=2, numSearches=1, actualTotalOutput=1}
            }, finishers=[dedup(?3)]
            PatternNode[(?8=&lt;organizations:1&gt;, &lt;~label&gt;, ?9, &lt;~&gt;) . project distinct ?8 .], {estimatedCardinality=INFINITY, expectedTotalOutput=1, indexTime=0, joinTime=0, numSearches=1, actualTotalOutput=1}
            PatternNode[(?8, &lt;lifestate&gt;, &quot;ACTIVE&quot;, ?) . project ask .], {estimatedCardinality=1799504, expectedTotalOutput=1, indexTime=0, joinTime=0, numSearches=1, actualTotalOutput=1}
            PatternNode[(?10, ?12, ?8, ?13) . project ?8,?13,?10 . IsEdgeIdFilter(?13) .], {estimatedCardinality=INFINITY, expectedTotalOutput=102, indexTime=0, joinTime=1, numSearches=1, actualTotalOutput=102}
            PatternNode[(?13, &lt;lifestate&gt;, &quot;ACTIVE&quot;, ?) . project ask .], {estimatedCardinality=3341886, expectedTotalOutput=102, indexTime=0, joinTime=128, numSearches=102, actualTotalOutput=102}
            PatternNode[(?13, &lt;role&gt;, &quot;admin&quot;, ?) . project ask .], {estimatedCardinality=113376, expectedTotalOutput=100, indexTime=1, joinTime=6, numSearches=102, actualTotalOutput=100}
            PatternNode[(?10, &lt;~label&gt;, ?14=&lt;users&gt;, &lt;~&gt;) . project ask .], {estimatedCardinality=2326404, expectedTotalOutput=100, indexTime=0, joinTime=128, numSearches=100, actualTotalOutput=100}
            PatternNode[(?10, &lt;lifestate&gt;, &quot;ACTIVE&quot;, ?) . project ask .], {estimatedCardinality=1799504, expectedTotalOutput=100, indexTime=0, joinTime=83, numSearches=100, actualTotalOutput=100}
            PatternNode[(?10, &lt;~label&gt;, ?15=&lt;users&gt;, &lt;~&gt;) . project ?10 .], {estimatedCardinality=2326404, expectedTotalOutput=100, indexTime=1, joinTime=1, numSearches=1, actualTotalOutput=100}
            PatternNode[(?16=&lt;accounts:1&gt;, &lt;~label&gt;, ?17, &lt;~&gt;) . project distinct ?16 .], {estimatedCardinality=INFINITY, expectedTotalOutput=100, indexTime=0, joinTime=1, numSearches=1, actualTotalOutput=100}
            PatternNode[(?16, &lt;lifestate&gt;, &quot;ACTIVE&quot;, ?) . project ask .], {estimatedCardinality=1799504, expectedTotalOutput=100, indexTime=0, joinTime=0, numSearches=1, actualTotalOutput=100}
            PatternNode[(?18, ?20, ?16, ?21) . project ?16,?21,?18 . IsEdgeIdFilter(?21) .], {estimatedCardinality=INFINITY, expectedTotalOutput=1000, indexTime=0, joinTime=119, numSearches=1, actualTotalOutput=500000}
            PatternNode[(?21, &lt;lifestate&gt;, &quot;ACTIVE&quot;, ?) . project ask .], {estimatedCardinality=3341886, expectedTotalOutput=1000, indexTime=194, joinTime=142, numSearches=5000, actualTotalOutput=500000}
            PatternNode[(?18, &lt;~label&gt;, ?22=&lt;vehicles&gt;, &lt;~&gt;) . project ask .], {estimatedCardinality=238260, expectedTotalOutput=1000, indexTime=183, joinTime=499, numSearches=5000, actualTotalOutput=500000}
            PatternNode[(?18, &lt;lifestate&gt;, &quot;ACTIVE&quot;, ?) . project ask .], {estimatedCardinality=1799504, expectedTotalOutput=1000, indexTime=193, joinTime=858, numSearches=5000, actualTotalOutput=500000}
            PatternNode[(?18, &lt;~label&gt;, ?23=&lt;vehicles&gt;, &lt;~&gt;) . project ?18 .], {estimatedCardinality=238260, indexTime=360, joinTime=1372, numSearches=500}
        }, annotations={path=[Vertex(?1):GraphStep, Edge(?6):VertexStep, Vertex(?3):EdgeOtherVertexStep, Vertex(?8):GraphStep, Edge(?13):VertexStep, Vertex(?10):EdgeOtherVertexStep, VertexId(?10):IdStep@[A], Vertex(?16):GraphStep, Edge(?21):VertexStep, Vertex(?18):EdgeOtherVertexStep, VertexId(?18):IdStep@[B]], joinStats=true, optimizationTime=329, maxVarId=24, executionTime=6279}
    },
    NeptuneTraverserConverterStep
]
+ not converted into Neptune steps: [SelectStep(last,[A, B])]

WARNING: &gt;&gt; SelectStep(last,[A, B]) &lt;&lt; (or one of its children) is not supported natively yet

Physical Pipeline
=================
NeptuneGraphQueryStep@[A, B]
    |-- StartOp
    |-- JoinGroupOp
        |-- JoinGroupOp
            |-- SpoolerOp(1000)
            |-- DynamicJoinOp(PatternNode[(?1=&lt;accounts:1&gt;, &lt;lifestate&gt;, &quot;ACTIVE&quot;, ?) . project ?1 .], {estimatedCardinality=1799504, expectedTotalOutput=1})
            |-- SpoolerOp(1000)
            |-- DynamicJoinOp(PatternNode[(?1, ?5, ?3=&lt;organizations:1&gt;, ?6) . project ?1,?6,?3 . IsEdgeIdFilter(?6) .], {estimatedCardinality=102, expectedTotalOutput=1})
            |-- SpoolerOp(1000)
            |-- DynamicJoinOp(PatternNode[(?6, &lt;lifestate&gt;, &quot;ACTIVE&quot;, ?) . project ask .], {estimatedCardinality=3341886, expectedTotalOutput=1})
            |-- SpoolerOp(1000)
            |-- DynamicJoinOp(PatternNode[(?3, &lt;lifestate&gt;, &quot;ACTIVE&quot;, ?) . project ask .], {estimatedCardinality=1799504, expectedTotalOutput=1})
            |-- FilterOp
        |-- SpoolerOp(1000)
        |-- DynamicJoinOp(PatternNode[(?8=&lt;organizations:1&gt;, &lt;~label&gt;, ?9, &lt;~&gt;) . project distinct ?8 .], {estimatedCardinality=INFINITY, expectedTotalOutput=1})
        |-- SpoolerOp(1000)
        |-- DynamicJoinOp(PatternNode[(?8, &lt;lifestate&gt;, &quot;ACTIVE&quot;, ?) . project ask .], {estimatedCardinality=1799504, expectedTotalOutput=1})
        |-- SpoolerOp(1000)
        |-- DynamicJoinOp(PatternNode[(?10, ?12, ?8, ?13) . project ?8,?13,?10 . IsEdgeIdFilter(?13) .], {estimatedCardinality=INFINITY, expectedTotalOutput=102})
        |-- SpoolerOp(1000)
        |-- DynamicJoinOp(PatternNode[(?13, &lt;lifestate&gt;, &quot;ACTIVE&quot;, ?) . project ask .], {estimatedCardinality=3341886, expectedTotalOutput=102})
        |-- SpoolerOp(1000)
        |-- DynamicJoinOp(PatternNode[(?13, &lt;role&gt;, &quot;admin&quot;, ?) . project ask .], {estimatedCardinality=113376, expectedTotalOutput=100})
        |-- SpoolerOp(1000)
        |-- DynamicJoinOp(PatternNode[(?10, &lt;~label&gt;, ?14=&lt;users&gt;, &lt;~&gt;) . project ask .], {estimatedCardinality=2326404, expectedTotalOutput=100})
        |-- SpoolerOp(1000)
        |-- DynamicJoinOp(PatternNode[(?10, &lt;lifestate&gt;, &quot;ACTIVE&quot;, ?) . project ask .], {estimatedCardinality=1799504, expectedTotalOutput=100})
        |-- SpoolerOp(1000)
        |-- DynamicJoinOp(PatternNode[(?10, &lt;~label&gt;, ?15=&lt;users&gt;, &lt;~&gt;) . project ?10 .], {estimatedCardinality=2326404, expectedTotalOutput=100})
        |-- SpoolerOp(1000)
        |-- DynamicJoinOp(PatternNode[(?16=&lt;accounts:1&gt;, &lt;~label&gt;, ?17, &lt;~&gt;) . project distinct ?16 .], {estimatedCardinality=INFINITY, expectedTotalOutput=100})
        |-- SpoolerOp(1000)
        |-- DynamicJoinOp(PatternNode[(?16, &lt;lifestate&gt;, &quot;ACTIVE&quot;, ?) . project ask .], {estimatedCardinality=1799504, expectedTotalOutput=100})
        |-- SpoolerOp(1000)
        |-- DynamicJoinOp(PatternNode[(?18, ?20, ?16, ?21) . project ?16,?21,?18 . IsEdgeIdFilter(?21) .], {estimatedCardinality=INFINITY, expectedTotalOutput=1000})
        |-- SpoolerOp(1000)
        |-- DynamicJoinOp(PatternNode[(?21, &lt;lifestate&gt;, &quot;ACTIVE&quot;, ?) . project ask .], {estimatedCardinality=3341886, expectedTotalOutput=1000})
        |-- SpoolerOp(1000)
        |-- DynamicJoinOp(PatternNode[(?18, &lt;~label&gt;, ?22=&lt;vehicles&gt;, &lt;~&gt;) . project ask .], {estimatedCardinality=238260, expectedTotalOutput=1000})
        |-- SpoolerOp(1000)
        |-- DynamicJoinOp(PatternNode[(?18, &lt;lifestate&gt;, &quot;ACTIVE&quot;, ?) . project ask .], {estimatedCardinality=1799504, expectedTotalOutput=1000})
        |-- SpoolerOp(1000)
        |-- DynamicJoinOp(PatternNode[(?18, &lt;~label&gt;, ?23=&lt;vehicles&gt;, &lt;~&gt;) . project ?18 .], {estimatedCardinality=238260})

Runtime (ms)
============
Query Execution: 6283.262
Serialization:   2120.104

Traversal Metrics
=================
Step                                                               Count  Traversers       Time (ms)    % Dur
-------------------------------------------------------------------------------------------------------------
NeptuneGraphQueryStep(VertexId)@[A, obje...                500000      500000        2502.636    41.43
NeptuneTraverserConverterStep                                     500000      500000        2580.098    42.71
SelectStep(last,[A, B])                              500000      500000         958.328    15.86
                                            &gt;TOTAL                     -           -        6041.062        -

Predicates
==========
# of predicates: 37

WARNING: reverse traversal with no edge label(s) - .in() / .both() may impact query performance

Results
=======
Count: 500000
Output: &lt;Removed for space&gt;
Response serializer: application/vnd.gremlin-v3.0+gryo
Response size (bytes): 64,000,045


Index Operations
================
Query execution:
    # of statement index ops: 15915
    # of unique statement index ops: 15915
    Duplication ratio: 1.0
    # of terms materialized: 0
Serialization:
    # of statement index ops: 0
    # of terms materialized: 0
</code></pre>",1,0,2022-02-24 14:24:57.903000 UTC,,2022-02-25 13:43:00.513000 UTC,0,gremlin|graph-databases|tinkerpop|amazon-neptune,47,2017-11-04 09:01:23.687000 UTC,2022-03-05 18:24:04.733000 UTC,,166,2,0,15,,,,,,[]
Azure Synapse Analytics failed to execute the JDBC query produced by the connector with Databricks on Apache Spark,"<p>I am trying to write to my Azure Synapse Server from Databricks, but I keep getting the error:</p>
<p>Azure Synapse Analytics failed to execute the JDBC query produced by the connector</p>
<p>The code is as follows:</p>
<pre><code>blobStorage = &quot;*******.blob.core.windows.net&quot;
blobContainer = &quot;synapsestagecontainer&quot;
blobAccessKey = &quot;***************&quot;

tempDir = &quot;wasbs://&quot; + blobContainer + &quot;@&quot; + blobStorage +&quot;/tempDirs&quot;

acntInfo = &quot;fs.azure.account.key.&quot;+ blobStorage
sc._jsc.hadoopConfiguration().set(acntInfo, blobAccessKey)

dwDatabase = &quot;carlspool&quot;
dwServer = &quot;carlssynapseworkspace&quot;
dwUser = &quot;techadmin@carlssynapseworkspace&quot;
dwPass = &quot;*******&quot;
dwJdbcPort = &quot;1433&quot;
dwJdbcExtraOptions = &quot;encrypt=true;trustServerCertificate=true;hostNameInCertificate=*.database.windows.net;loginTimeout=30;&quot;
sqlDwUrl = &quot;jdbc:sqlserver://&quot; + dwServer + &quot;.database.windows.net:&quot; + dwJdbcPort + &quot;;database=&quot; + dwDatabase + &quot;;user=&quot; + dwUser+&quot;;password=&quot; + dwPass + &quot;;$dwJdbcExtraOptions&quot;
sqlDwUrlSmall = &quot;jdbc:sqlserver://&quot; + dwServer + &quot;.database.windows.net:&quot; + dwJdbcPort + &quot;;database=&quot; + dwDatabase + &quot;;user=&quot; + dwUser+&quot;;password=&quot; + dwPass


spark.conf.set(
   &quot;spark.sql.parquet.writeLegacyFormat&quot;,
   &quot;true&quot;)

example1.write.format(&quot;com.databricks.spark.sqldw&quot;).option(&quot;url&quot;, sqlDwUrlSmall).option(&quot;dbtable&quot;, &quot;SampleTable12&quot;).option(&quot;forward_spark_azure_storage_credentials&quot;,&quot;True&quot;) .option(&quot;tempdir&quot;, tempDir).mode(&quot;overwrite&quot;).save()
</code></pre>
<p>The full stack trace is a follows:</p>
<pre><code>Py4JJavaError                             Traceback (most recent call last)
&lt;command-3898875195714724&gt; in &lt;module&gt;
      4    &quot;true&quot;)
      5 
----&gt; 6 example1.write.format(&quot;com.databricks.spark.sqldw&quot;).option(&quot;url&quot;, sqlDwUrlSmall).option(&quot;dbtable&quot;, &quot;SampleTable12&quot;).option(&quot;forward_spark_azure_storage_credentials&quot;,&quot;True&quot;) .option(&quot;tempdir&quot;, tempDir).mode(&quot;overwrite&quot;).save()

/databricks/spark/python/pyspark/sql/readwriter.py in save(self, path, format, mode, partitionBy, **options)
   1132             self.format(format)
   1133         if path is None:
-&gt; 1134             self._jwrite.save()
   1135         else:
   1136             self._jwrite.save(path)

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1302 
   1303         answer = self.gateway_client.send_command(command)
-&gt; 1304         return_value = get_return_value(
   1305             answer, self.gateway_client, self.target_id, self.name)
   1306 

/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
    115     def deco(*a, **kw):
    116         try:
--&gt; 117             return f(*a, **kw)
    118         except py4j.protocol.Py4JJavaError as e:
    119             converted = convert_exception(e.java_exception)

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    324             value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
    325             if answer[1] == REFERENCE_TYPE:
--&gt; 326                 raise Py4JJavaError(
    327                     &quot;An error occurred while calling {0}{1}{2}.\n&quot;.
    328                     format(target_id, &quot;.&quot;, name), value)

Py4JJavaError: An error occurred while calling o1761.save.
: com.databricks.spark.sqldw.SqlDWSideException: Azure Synapse Analytics failed to execute the JDBC query produced by the connector.
Underlying SQLException(s):
  - com.microsoft.sqlserver.jdbc.SQLServerException: HdfsBridge::recordReaderFillBuffer - Unexpected error encountered filling record reader buffer: HadoopSqlException: String or binary data would be truncated. [ErrorCode = 107090] [SQLState = S0001]
         
    at com.databricks.spark.sqldw.Utils$.wrapExceptions(Utils.scala:686)
    at com.databricks.spark.sqldw.DefaultSource.createRelation(DefaultSource.scala:89)
    at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:96)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:196)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:240)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:236)
    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:192)
    at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:167)
    at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:166)
    at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:1079)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:126)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:267)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)
    at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:217)
    at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1079)
    at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:468)
    at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:438)
    at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:311)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
    at py4j.Gateway.invoke(Gateway.java:295)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:251)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Exception thrown in awaitResult: 
    at com.databricks.spark.sqldw.JDBCWrapper.executeInterruptibly(SqlDWJDBCWrapper.scala:137)
    at com.databricks.spark.sqldw.JDBCWrapper.$anonfun$executeInterruptibly$1(SqlDWJDBCWrapper.scala:115)
    at com.databricks.spark.sqldw.JDBCWrapper.$anonfun$executeInterruptibly$1$adapted(SqlDWJDBCWrapper.scala:115)
    at com.databricks.spark.sqldw.JDBCWrapper.withPreparedStatement(SqlDWJDBCWrapper.scala:362)
    at com.databricks.spark.sqldw.JDBCWrapper.executeInterruptibly(SqlDWJDBCWrapper.scala:115)
    at com.databricks.spark.sqldw.SqlDwWriter.$anonfun$saveToSqlDW$6(SqlDwWriter.scala:239)
    at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
    at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:377)
    at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:363)
    at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
    at com.databricks.spark.sqldw.SqlDwWriter.$anonfun$saveToSqlDW$1(SqlDwWriter.scala:197)
    at com.databricks.spark.sqldw.SqlDwWriter.$anonfun$saveToSqlDW$1$adapted(SqlDwWriter.scala:73)
    at com.databricks.spark.sqldw.JDBCWrapper.withConnection(SqlDWJDBCWrapper.scala:340)
    at com.databricks.spark.sqldw.SqlDwWriter.saveToSqlDW(SqlDwWriter.scala:73)
    at com.databricks.spark.sqldw.DefaultSource.$anonfun$createRelation$3(DefaultSource.scala:122)
    at com.databricks.spark.sqldw.Utils$.wrapExceptions(Utils.scala:655)
    ... 34 more
Caused by: com.microsoft.sqlserver.jdbc.SQLServerException: HdfsBridge::recordReaderFillBuffer - Unexpected error encountered filling record reader buffer: HadoopSqlException: String or binary data would be truncated.
    at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:262)
    at com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1632)
    at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(SQLServerPreparedStatement.java:602)
    at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecCmd.doExecute(SQLServerPreparedStatement.java:524)
    at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7418)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3272)
    at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:247)
    at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:222)
    at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.execute(SQLServerPreparedStatement.java:505)
    at com.databricks.spark.sqldw.JDBCWrapper.$anonfun$executeInterruptibly$2(SqlDWJDBCWrapper.scala:115)
    at com.databricks.spark.sqldw.JDBCWrapper.$anonfun$executeInterruptibly$2$adapted(SqlDWJDBCWrapper.scala:115)
    at com.databricks.spark.sqldw.JDBCWrapper.$anonfun$executeInterruptibly$3(SqlDWJDBCWrapper.scala:129)
    at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
    at scala.util.Success.$anonfun$map$1(Try.scala:255)
    at scala.util.Success.map(Try.scala:213)
    at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
    at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
</code></pre>
<p>I know there are other people that have experienced this problem with Databricks, and I have try to apply the answers to my situation but I can't get it to work.</p>
<p>The full error is:</p>
<pre><code>com.databricks.spark.sqldw.SqlDWSideException: Azure Synapse Analytics failed to execute the JDBC query produced by the connector.
</code></pre>
<p>I am running Runtime 8.3</p>",0,1,2021-10-12 21:46:59.273000 UTC,,2021-10-12 21:53:08.463000 UTC,0,apache-spark|azure-databricks|azure-synapse,680,2021-03-31 08:36:43.863000 UTC,2022-03-05 23:03:22.193000 UTC,"London, UK",651,58,0,93,,,,,,[]
Is this the best method to load and merge data into an existing Delta Table on Databricks?,"<p>I'm new to using Databricks and I'm trying to test the validity of continuously loading an hourly file into primary that will be used for reporting. Each hourly file is roughly 3-400gb and contains ~1-1.3b records. I would like to have the primary table store ~48 hours worth of data, but I really only need 6 hourly files to complete a view of my data. </p>

<p>My current process is below and it seems to work ok. The csv hourly file is stored on Azure DataLake (Gen1) and the primary table is using ADL Gen2 as the storage. Are these the best options? Does this process seem sound or am I doing something horribly wrong? :)</p>

<pre><code>csvdata = spark.read.format(""csv"").option(""header"",""true"").option(""ignoreLeadingWhiteSpace"",""true"").option(""ignoreTrailingWhiteSpace"",""true"").option(""timestampFormat"",""yyyy-MM-dd HH:mm:ss.SSS"").option(""delimiter"",""|"").option(""inferSchema"",""true"").option(""mode"",""FAILFAST"").csv(""adl://pathToCsv"").createOrReplaceTempView(""tempdata"").cache()

```sql Merge
MERGE INTO primaryTable
USING tempdata
ON primaryTable.UserGuid = tempdata.UserGuid AND primaryTable.OrgGuid = tempdata.OrgGuid
WHEN MATCHED AND cast(unix_timestamp(primaryTable.timestamp,'yyyy/MM/dd HH:mm:ss.SSS') AS timestamp) &lt; cast(unix_timestamp(tempdata.timestamp,'yyyy/MM/dd HH:mm:ss.SSS') AS timestamp) THEN
  UPDATE SET *
WHEN NOT MATCHED
  THEN INSERT *
</code></pre>",0,1,2019-05-02 15:20:47.677000 UTC,,2019-05-07 06:41:46.903000 UTC,1,apache-spark|apache-spark-sql|azure-databricks|delta-lake,137,2015-06-25 14:34:28.207000 UTC,2019-05-18 00:48:40.070000 UTC,,23,0,0,3,,,,,,[]
Reading Blob Into Pyspark,"<p>I'm trying to read in a series of json files stored in an azure blob into spark using the databricks notebook. I set the conf() with my account and key but it always returns the error</p>

<pre><code>shaded.databricks.org.apache.hadoop.fs.azure.AzureException: java.lang.IllegalArgumentException: The String is not a valid Base64-encoded string.
</code></pre>

<p>I've followed along with the information provided here:</p>

<p><a href=""https://docs.databricks.com/_static/notebooks/data-import/azure-blob-store.html"" rel=""nofollow noreferrer"">https://docs.databricks.com/_static/notebooks/data-import/azure-blob-store.html</a></p>

<p>and here:</p>

<p><a href=""https://luminousmen.com/post/azure-blob-storage-with-pyspark"" rel=""nofollow noreferrer"">https://luminousmen.com/post/azure-blob-storage-with-pyspark</a></p>

<p>I can pull the data just fine using the azure sdk for python</p>

<pre><code>storage_account_name = ""name""
storage_account_access_key = ""key""

spark.conf.set(
  ""fs.azure.account.key.""+storage_account_name+"".blob.core.windows.net"",
  storage_account_access_key)

file_location = ""wasbs://loc/locationpath""
file_type = ""json""

df = spark.read.format(file_type).option(""inferSchema"", ""true"").load(file_location)
</code></pre>

<p>Should return a dataframe of the json file</p>",0,3,2019-08-13 21:50:21.493000 UTC,,,4,azure-databricks,2086,2018-01-09 05:32:55.453000 UTC,2020-04-03 12:48:38.867000 UTC,,61,0,0,13,,,,,,[]
Optimize withColumn when clause,"<p>I have the following code</p>

<pre><code>def GetCompletionForS4(location: String): DataFrame = {

var dfSubSystem = GetTasksFor(location, ""S4"").as(""Tasks"")
.join(GetCertsFor(location, ""S4"").as(""Certs""),$""Tasks.SystemX"" === $""Certs.SystemX"" &amp;&amp; $""Tasks.StageX"" === $""Certs.StageX"" , ""outer"")
.join(GetPTTasksFor(location, ""S4"").as(""PT""), $""Tasks.SystemX"" === $""PT.SystemX"" &amp;&amp; $""Tasks.StageX""=== $""PT.StageX"", ""outer"")
  .withColumn(""SystemizationId"", coalesce(col(""Tasks.SystemX""), col(""Certs.SystemX""), col(""PT.SystemX"")))
  .withColumn(""CommissioningStage"", coalesce(col(""Tasks.StageX""), col(""Certs.StageX""), col(""PT.StageX"")))
  .withColumn(""fPercentageClosed"", when((col(""PT.SystemX"")).isNull,  coalesce(col(""Tasks.CountX""), lit(0)).cast(""double"") * 0.9  + coalesce(col(""Certs.CountX""), lit(0)).cast(""double"") * 0.1)
                                   .otherwise(coalesce(col(""Tasks.CountX""), lit(0)).cast(""double"") * 0.6 + coalesce(col(""PT.CountX""), lit(0)).cast(""double"") * 0.3  + coalesce(col(""Certs.CountX""), lit(0)).cast(""double"") * 0.1)
             )
.withColumn(""fActualStartDate"", when(col(""Tasks.ActualStartDateX"").isNull,
                                     when(col(""Certs.ActualStartDateX"").isNull, col(""PT.ActualStartDateX""))
                                     .otherwise(
                                                 when(col(""PT.ActualStartDateX"").isNull, col(""Certs.ActualStartDateX""))
                                                 .otherwise(
                                                             when(col(""Certs.ActualStartDateX"")&lt; col(""PT.ActualStartDateX""), col(""Certs.ActualStartDateX"")).otherwise(col(""PT.ActualStartDateX""))
                                                           )
                                               )
                                    )
                                    .otherwise(
                                                when(col(""Certs.ActualStartDateX"").isNull, 
                                                     when(col(""PT.ActualStartDateX"").isNull, col(""Tasks.ActualStartDateX"")).otherwise(
                                                                                                                                       when(col(""PT.ActualStartDateX"") &lt; col(""Tasks.ActualStartDateX""), col(""PT.ActualStartDateX"")).otherwise(col(""Tasks.ActualStartDateX"")) 
                                                                                                                                     ) 
                                                    )
                                                .otherwise(
                                                            when(col(""PT.ActualStartDateX"").isNull,
                                                                  when(col(""Certs.ActualStartDateX"") &lt; col(""Tasks.ActualStartDateX""), col(""Certs.ActualStartDateX"")).otherwise(col(""Tasks.ActualStartDateX""))
                                                                )
                                                            .otherwise(
                                                                        when(col(""Certs.ActualStartDateX"") &lt; col(""Tasks.ActualStartDateX"") , 
                                                                             when(col(""Certs.ActualStartDateX"") &lt; col(""PT.ActualStartDateX""), col(""Certs.ActualStartDateX"")).otherwise(col(""PT.ActualStartDateX""))
                                                                            )
                                                                        .otherwise(
                                                                                    when(col(""Tasks.ActualStartDateX"") &lt; col(""PT.ActualStartDateX""), col(""Tasks.ActualStartDateX"")).otherwise(col(""PT.ActualStartDateX""))
                                                                                  )

                                                                      )
                                                          )
                                              )

           )
.withColumn(""fActualEndDate"", when(col(""PT.SystemX"").isNull,
                                   when(col(""Tasks.ActualEndDateX"").isNull,null)
                                   .otherwise(
                                      when(col(""Certs.ActualEndDateX"").isNull, null)
                                     .otherwise(
                                       when(col(""Tasks.ActualEndDateX"") &gt; col(""Certs.ActualEndDateX""), col(""Tasks.ActualEndDateX""))
                                       .otherwise(col(""Certs.ActualEndDateX""))
                                     )
                                    )
                                  )
                              .otherwise(
                                          when(col(""PT.ActualEndDateX"").isNull || col(""Certs.ActualEndDateX"").isNull || col(""Tasks.ActualEndDateX"").isNull, null)
                                         .otherwise(
                                                    when(col(""Tasks.ActualEndDateX"") &gt; col(""Certs.ActualEndDateX""),
                                                           when(col(""Tasks.ActualEndDateX"") &gt; col(""PT.ActualEndDateX"") , col(""Tasks.ActualEndDateX"")).otherwise(col(""PT.ActualEndDateX""))
                                                        )
                                                    .otherwise(
                                                                 when(col(""Certs.ActualEndDateX"") &gt; col(""PT.ActualEndDateX"") , col(""Certs.ActualEndDateX"")).otherwise(col(""PT.ActualEndDateX""))
                                                              )
                                                   )
                                        )
           )
  .select(""SystemizationId"",
          ""CommissioningStage"",
          ""fPercentageClosed"",
          ""fActualStartDate"",
          ""fActualEndDate""
         )


return dfSubSystem
}
</code></pre>

<p>How do I optimize this insane nested whens inside the withColumn? Its not readable or maintainable. What is a better way to write this query. There are new requirements to add more complexity to these nested whens so not sure what the best approach to do this is.</p>

<p>Thank you in advance</p>",0,2,2020-01-30 02:09:17.480000 UTC,,2020-01-30 14:38:33.583000 UTC,0,scala|apache-spark|azure-databricks,54,2013-03-30 16:25:18.897000 UTC,2021-07-21 01:43:58.397000 UTC,,73,0,0,14,,,,,,[]
How to setup distributed version control in a company,"<p>From my experience with distributed version control systems (DVCS) like git and mercurial in open source projects, most setup models use a centralized setup for their project (think GitHub).</p>

<p>When introducing distributed VCS into a company, <strong>do you have a centralized setup model</strong>?</p>",4,0,2009-03-19 09:15:16.260000 UTC,2.0,,5,git|mercurial|dvcs,803,2009-03-09 11:06:28.033000 UTC,2021-11-02 14:05:28.680000 UTC,"Berlin, Germany",2085,45,4,296,,,,,,[]
Why should a business use distributed version control?,"<p>It seems that many people read about distributed version control and implicitly understand why it is a good thing for open source development, with many distributed developers all acting independently and in accordance with their own choices instead of mandate from management.  But from this impression many people form the idea that DVCS is useful <em>only</em> in an open-source environment; they can't see how it would help an organization that releases a proprietary product and doesn't make its version control system accesible externally, or how it would help a single developer.</p>

<p>What are some benefits a business can see if it chooses to use distributed version control such as git, darcs, or Mercurial instead of centralized version control such as CVS or Subversion?</p>",7,0,2009-03-31 20:27:11.920000 UTC,3.0,,10,git|dvcs,1082,2008-09-18 18:52:09.940000 UTC,2020-06-03 01:38:13.653000 UTC,Texas,88848,1651,102,2287,,,,,,[]
AWS Full-Text Search with Elastic: Invalid endpoint provided for external service query,"<p>Currently I'm playing with AWS ElasticSearch (OpenSearch) on top of AWS Neptune. They are classically integrated through Neptune streams. When I'm trying to execute Gremlin query:</p>
<pre><code>t.withSideEffect(&quot;Neptune#fts.endpoint&quot;, &quot;https://vpc-DOMAIN-ID.REGION.es.amazonaws.com&quot;)
  .V()
  .hasLabel(&quot;person&quot;)
  .has(&quot;firstName&quot;, &quot;Neptune#fts Viki~&quot;)
</code></pre>
<p>I'm getting Neptune API exception:</p>
<pre><code>org.apache.tinkerpop.gremlin.driver.exception.ResponseException:
{&quot;code&quot;:&quot;InvalidParameterException&quot;,&quot;requestId&quot;:&quot;e2427e4a-333d-4952-b8a1-929e7c72fc96&quot;,&quot;detailedMessage&quot;:&quot;Invalid endpoint provided for external service query.&quot;}
</code></pre>
<p>Both Neptune and ElasticSearch reside within the same private subnet. Elastic is configured to use VPC endpoint. Any help would be appreciated, including how to debug/profile such a behaviour etc</p>",0,3,2021-12-13 22:26:03.357000 UTC,,2021-12-13 23:56:17.337000 UTC,0,amazon-web-services|elasticsearch|gremlin|amazon-neptune|opensearch,43,2021-12-13 21:51:21.337000 UTC,2022-03-04 12:06:09.903000 UTC,,1,0,0,2,,,,,,[]
Reading Json file from Azure datalake as a file using Json.load in Azure databricks /Synapse notebooks,"<p>I am trying to parse Json data with multi nested level. I am using the approach is giving filename and using open(File-name) to load the data. when I am providing datalake path, it is throwing error that file path not found. I am able to read data in dataframes but How can I read file from data lake without converting to dataframes and reading it as a file and open it?</p>
<p>Current code approach on local machine which is working:</p>
<pre class=""lang-py prettyprint-override""><code>f = open(File_Name.Json)
data = json.load(f)
</code></pre>
<p>Failing scenario when provding datalake path:</p>
<pre><code>f = open(Datalake path/File_Name.Json)
data = json.load(f)
</code></pre>",2,0,2021-12-29 16:09:52.197000 UTC,,2021-12-30 08:25:03.373000 UTC,1,pyspark|apache-spark-sql|rdd|azure-databricks|azure-data-lake,169,2021-05-25 18:23:34.360000 UTC,2021-12-29 18:47:35.467000 UTC,,11,0,0,1,,,,,,[]
copy activity alternative in Azure Data Factory,"<p>I have output from web activity in Azure data factory and I want to store the data in blob storage.
Is there any way other than Copy Activity and Function app that I can use to store data in blob storage?</p>",1,0,2021-08-12 12:38:23.950000 UTC,,2021-08-16 09:09:10.567000 UTC,1,azure|azure-functions|azure-data-factory-2|azure-databricks,115,2018-08-30 15:36:46.320000 UTC,2022-03-02 05:48:53.100000 UTC,,102,4,0,42,,,,,,[]
How to generate databricks token through RESTAPI using basic authentication,"<p>I am trying to generate Databricks token through RESTAPI using basic aunthentication. I am unable to generate the token.</p>
<p>Below is the RESTAPI</p>
<pre><code>https://......azuredatabricks.net/basic-auth/api/2.0/token/list
</code></pre>",1,0,2021-06-04 13:41:37.350000 UTC,,2021-06-04 14:49:37.410000 UTC,1,postman|azure-databricks|rest,98,2020-05-24 10:04:28.790000 UTC,2022-03-04 11:06:39.240000 UTC,"Bangalore, Karnataka, India",21,0,0,0,,,,,,[]
How to create a Databricks job using a Python file outside of dbfs?,"<p>I am fairly new to Databricks, so forgive me for the lack of knowledge here. I am using the Databricks resource in Azure. I mainly use the UI right now, but I know some features are only available using databricks-cli, which I have setup but not used yet.</p>
<p>I have cloned my Git repo in Databricks Repos using the UI. Inside my repo, there is a Python file that I will like to run as a job.</p>
<p>Can I use Databricks Jobs to create a job that will call this Python file directly ? The only way that I have been able to make this work is to create and upload to dbfs another Python file that will call the file in my Databricks Repo.</p>
<p>Maybe it cannot be done, or maybe the path I use is incorrect. I tried with the following path structure when creating a job using a Python file and it did not work, unfortunately.</p>
<pre><code>file:/Workspace/Repos/&lt;user_folder&gt;/&lt;repo_name&gt;/my_python_file.py
</code></pre>",2,2,2021-11-24 12:50:25.183000 UTC,,,1,python|git|azure-databricks,375,2014-07-10 19:41:32.927000 UTC,2022-01-07 19:52:06.413000 UTC,"Québec City, QC, Canada",218,4,1,54,,,,,,[]
How to select a subset of Avro files from Azure Data Lake Gen2 by data content,"<p>I have lots of Avro files in an Azure Data Lake Gen2 storage sent by an Event Hub service with capture enabled. These Avro files contain data from different sensors and engines. The structure of the directory is organized in folders with the following path format (typical of Azure Blobs): </p>

<p><code>namespace/eventhub/partition/year/month/day/hour/minute/file.avro</code></p>

<p>I need to access to some of these files, in order to get data to 1) pre-process and 2) train or re-train a machine learning model. I'd like to know what procedure could I follow to download or mount just the files containing data of a particular engine and/or sensor, given that not data from all of them are present in all Avro files. Let's assume I'm interested just in files containing data from:</p>

<pre><code>Engine = engine_ID_4012
Sensor = sensor_engine_4012_ID_0114
</code></pre>

<p>I'm aware that Spark offers some advantages working with Avro files, so I could consider to carry out this task using Databricks. Otherwise the option is Azure Machine Learning service, but maybe there are other possiblities, for instance a combination. The goal is to speed up the data ingestion process, avoiding to read files with no needed data.</p>

<p>Thanks.</p>",1,0,2020-04-10 16:45:07.233000 UTC,1.0,,0,avro|azure-data-lake|azure-eventhub|azure-databricks|azure-machine-learning-service,166,2018-09-07 11:08:23.563000 UTC,2022-02-23 20:11:57.943000 UTC,,150,56,0,16,,,,,,[]
Can we update the property of existing nodes and edges of a graph in AWS Neptune using S3 Bulk uploader,<p>I know using S3 CSV Bulk uploader we can create new vertexes and edges in AWS Neptune GraphDB. What I want to know if it is possible to update some of the properties of a edges and nodes in the Neptune graph using the S3 CSV bulk uploader.</p>,1,0,2020-08-15 10:57:20.470000 UTC,,2020-08-15 16:08:14.530000 UTC,0,amazon-s3|gremlin|graph-databases|amazon-neptune,653,2015-05-21 15:41:11.467000 UTC,2021-03-26 02:01:30.827000 UTC,"Jaipur, Rajasthan, India",956,115,40,268,,,,,,[]
Load data in to Amazon Neptune DB,"<p>I have used the below code to load the data in to neptune db access.</p>
<pre><code>curl -X POST \
    -H 'Content-Type: application/json' \
    https://your-neptune-endpoint:port/loader -d '
    {
      &quot;source&quot; : &quot;s3://bucket-name/object-key-name&quot;,
      &quot;format&quot; : &quot;format&quot;,
      &quot;iamRoleArn&quot; : &quot;arn:aws:iam::account-id:role/role-name&quot;,
      &quot;region&quot; : &quot;region&quot;,
      &quot;failOnError&quot; : &quot;FALSE&quot;,
      &quot;parallelism&quot; : &quot;MEDIUM&quot;,
      &quot;updateSingleCardinalityProperties&quot; : &quot;FALSE&quot;,
      &quot;queueRequest&quot; : &quot;TRUE&quot;,
      &quot;dependencies&quot; : [&quot;load_A_id&quot;, &quot;load_B_id&quot;]
    }'
</code></pre>
<p>i have an csv file in s3 bucket which i am trying to add in the neptune DB. This gave me the response 200. But I could not se any data in my neptune DB instance in aws. Where do i view the uploaded data in aws neptune DB?</p>
<p>And also does loading data in to neptune DB always needs an s3 in the middle?</p>",2,1,2020-09-16 08:34:12.590000 UTC,1.0,2020-09-16 12:21:03.817000 UTC,0,amazon-neptune,861,2020-02-14 06:33:45.640000 UTC,2020-12-08 05:55:03.317000 UTC,,23,0,0,18,,,,,,[]
How to enable smart commit for custom workflow in JIRA?,"<p>We're using custom workflow as attached. 
In the example from JIRA official we have a clear and easy to understand instruction on how to trigger the smart commit and update the ticket status in JIRA. Unfortunately seem like it's not working with the custom workflow or is there any way to work around? advise is appreciated. </p>

<pre><code>JRA-090 #close Fixed this today
</code></pre>

<p>Reference from <a href=""https://confluence.atlassian.com/display/JIRACLOUD/Processing+JIRA+issues+with+commit+messages"" rel=""nofollow noreferrer"">here</a></p>

<p><a href=""https://i.stack.imgur.com/guzp3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/guzp3.png"" alt=""Our custom workflow""></a></p>",2,0,2015-07-31 19:10:45.320000 UTC,,,0,git|jira|bitbucket|dvcs,754,2009-12-19 13:54:54.280000 UTC,2020-08-16 01:45:07.850000 UTC,Malaysia,3416,264,6,442,,,,,,[]
how to implement scd type 2 with type1 in spark sql,"<p>I have requirement  to capture old records along with new changes in  azure databricks notebook.
ex:</p>
<pre class=""lang-py prettyprint-override""><code>|----------------------------------------------------------------------
|EMPNO       ROW_EFF_DT     STATE_CODE      CITY_NAME     ROW_EXPIRY_DT
|----------------------------------------------------------------------|
|1550    19-10-2020      TA            FORTWORTH          20-10-2020  |
|-----------------------------------------------------------------------|
|1550    20-10-2020      TX             RONOLE            21-1-2020    |
|-----------------------------------------------------------------------|
|1550    21-1-2020      TX             GRAPEVIN            NULL         |
|------------------------------------------------------------------------|
I hope given useful information,Please feel free to ask if need some more info.


Any inputs for the above scenario will be helpful.
</code></pre>",0,3,2020-10-19 14:54:58.117000 UTC,1.0,2020-10-19 16:20:44.743000 UTC,0,pyspark|apache-spark-sql|azure-databricks,306,2016-10-16 10:12:02.053000 UTC,2020-11-07 11:51:55.887000 UTC,,3,0,0,20,,,,,,[]
ADF change columns of data type in Sink using copy data,"<p>Does anyone have experienced on transforming data type of columns when copying from source and sink with the specific data type in adf.</p>

<p>From Microsoft documentation, i noticed that the adf copy data can perform
1. Convert from native source types to Azure Data Factory interim data types
2. Convert from Azure Data Factory interim data types to native sink type</p>

<p>Currently, I want to copy data from a hive table which store all the column as String, so when using adf to copy data and landed it as parquet with correct data type, in which some column could be int, datetime, string and so on. Therefore, I have using dynamic mapping when copying data by creating dynamic json code.</p>

<pre><code>""translator"": {
        ""type"": ""TabularTranslator"",
        ""mappings"": [
            {
                ""source"": {
                    ""name"": ""commentid"",
                    ""type"": ""Int32""
                },
                ""sink"": {
                    ""name"": ""commentid"",
                    ""type"": ""Int32""
                }
            },
            {
                ""source"": {
                    ""name"": ""comment"",
                    ""type"": ""String""
                },
                ""sink"": {
                    ""name"": ""comment"",
                    ""type"": ""String""
                }
            },
            {
                ""source"": {
                    ""name"": ""commenteduser"",
                    ""type"": ""String""
                },
                ""sink"": {
                    ""name"": ""commenteduser"",
                    ""type"": ""String""
                }
            },
            {
                ""source"": {
                    ""name"": ""commenteddatetime"",
                    ""type"": ""String""
                },
                ""sink"": {
                    ""name"": ""commenteddatetime"",
                    ""type"": ""String""
                }
            }
        ]
    }
</code></pre>

<p>However, as a result, the copying process is successful without any error message. But when I have create table on top of these parquet file, the data type of these columns are still showing all as String.</p>

<p>Does anyone here have faced this issues before. Thanks.</p>",1,0,2020-06-04 12:43:14.830000 UTC,,2020-06-05 13:56:24.843000 UTC,0,azure|azure-data-factory|azure-data-lake|azure-databricks,886,2020-01-16 01:54:22.807000 UTC,2021-01-11 08:30:22.850000 UTC,,3,0,0,11,,,,,,[]
Unable to load empty string values for vertex and edge properties using Neptune Loader,"<p>I am unable to load empty string values for vertex and edge properties, as far as I know I am following Gremlin Load Format and providing the right request JSON to Neptune loader endpoint.</p>
<p>This is how the vertex csv file(generated using pandas) on s3 looks</p>
<p><a href=""https://i.stack.imgur.com/01W5i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/01W5i.png"" alt=""enter image description here"" /></a></p>
<p>The request payload sent to loader endpoint(http://database:8182/loader)</p>
<pre><code>{
   &quot;source&quot;: &quot;s3://bucket/dir/or/object/containing/csvfile/&quot;,
   &quot;format&quot; : &quot;csv&quot;,
   &quot;iamRoleArn&quot; : &quot;arn:sample&quot;,
   &quot;region&quot; : &quot;us-east-1&quot;,
   &quot;failOnError&quot; : &quot;FALSE&quot;,
   &quot;parserConfiguration&quot; : {
       &quot;allowEmptyStrings&quot;: true
     }
}
</code></pre>
<p>The data is loaded successfully.</p>
<ol>
<li>Vertex is created.</li>
<li><em>'label</em>' and 'id' fields are assigned with values mentioned in the csv.</li>
<li><em>'key2'</em> property shows its value as 'value'(mentioned in csv).</li>
<li><strong><em>'key1'</em> property is not found/loaded in database in spite of providing allowEmptyStrings as true in
request payload.</strong></li>
</ol>",1,0,2020-12-16 17:40:44.700000 UTC,,,0,pandas|amazon-web-services|gremlin|amazon-neptune,129,2017-02-08 22:58:22.030000 UTC,2021-10-21 07:49:25.313000 UTC,"Hyderabad, Telangana, India",74,25,0,22,,,,,,[]
auzre databricks install an application but a command cannot be entered from the databricks notebook,"<p>I am trying to install an application on Azure databricks from python3. </p>

<p>From the databricks notebook:</p>

<pre><code>%sh
cd  /dbfs/my_path/app_files/my_app/ #( there is a makefile here)
make

Enter soft-link target file or directory for ""lib/include/xxx_app_name"" (return if not needed): 
</code></pre>

<p>I cannot use shell to access azure databricks, how I can enter the necessary command for the interactive command line ? </p>

<p>thanks</p>",0,3,2020-05-09 18:53:47.683000 UTC,,,1,python|linux|ubuntu|azure-databricks,30,2014-03-21 20:02:48.577000 UTC,2022-03-03 17:48:26.460000 UTC,,1119,60,0,315,,,,,,[]
Parsing a delimited message (event message from IBM MQ) received in Azure Event hub,"<p>I have events being received in Azure Event Hub which are in the Format of IBM MQ Text. Source is Mainframe Oracle tables so data is being routed through Oracle Golden Date on the IBM MQ and arrives in Event Hub. 
I am interested in the body of the message which has an Event Name followed by Delimiter and also Tags which are followed by delimited data. There are no column names associated with the data in the message. The idea is to be able parse, map  and further transform this data and  make it available to other applications.
The problem is  how to Parse the Body of the event message which has delimited text and map it to a fixed schema using Azure Data Bricks or Stream Analytics.
Is it only possible by creating a custom parser in Python or Java or another coding language?</p>

<p>Here is the body of a sample message - </p>

<pre><code>{""GDSFE001\u00031\u0003N\u00030\u0003Confirm_Shipment_Closed\u00035572214\u0003B\u0003I7EPM0XV1Z8KB\u0003TAG0000\u0001\u000220190516\u00011409\u0001GCSS\u0001Message Broker\u0001\u0001\u0001\u0001O\u0001\u0001\u0001N\u0001BKG\u0001\u0001\u0001\u000163.0\u0002TAGT100\u0001HDZKG4XV1Z9KB\u0001BNILG4XV1Z9KB\u0001\u0001\u0001N\u0001N\u00010\u0001Y\u0001N\u00010\u00010\u0001SGPPM0XV1Z8KB\u0002TAG0100\u0001I7EPM0XV1Z8KB\u0001\u0001\u0001O\u0001\u0001\u00011\u0001Order Handling\u00011\u0001Active\u00013\u0001Ordinary Transport Order\u00012019-05-16 13.49.24.683955\u0001NayanKumar\u00012019-05-16 14.09.32.539936\u0001NayanKumar\u00012019-05-16 14.09.32.539936\u0001NayanKumar\u00011\u0001MSL\u00012019-05-16 13.49.24.683955\u0001NayanKumar\u00012019-05-16 14.09.32.539936\u0001NayanKumar\u0001\u0001Y\u0001N\u00011\u0001Booking Confirmation\u0001\u0001\u0001\u0001\u0001\u0001\u0001\u00012019-05-16 13.49.24.683955\u0001NayanKumar\u0001\u0001\u0001\u0001\u0001\u0001\u0001A1\u0001\u0001\u0001\u0001T2\u00015\u0001GCSS\u00011\u0001CY\u00011\u0001CY\u0001\u0001\u0001\u00014\u0001Telephone\u0001\u00012019-05-09\u0001\u0001\u0001\u000110\u0001Transdoc Invoicing\u0001\u0001\u0001\u0001Y\u00012019-05-16 13.49.24.683955\u0001\u0001\u0001\u0001\u0001N\u0001\u0001\u0001Y\u0001SGPPM0XV1Z8KB\u0001\u00012019-05-09 21.00.00.000000\u0001T2\u0001233\u0001North Europe - United States\u0001MAEU\u0001\u0001\u0001\u0001N\u0001Y\u00012019-05-16 13.49.24.683955\u00011\u0001Fixed Date\u00016\u0001Failed\u0001W\u0001N\u0001Y\u0001N\u0001Y\u0001N\u0002TAG0110\u0001J20PM0XV1Z8KB\u0001\u0001\u0001O\u00011\u0001Booking Management\u00012019-05-16 13.49.24.683955\u0001\u0001\u0001\u0001\u00012019-05-16 13.49.24.683955\u0001NayanKumar\u00012019-05-16 13.54.16.577754\u0001AUTOUSER\u000112084\u0001Export Order Handling\u0001AUTOUSER\u0001AUTOUSER\u00011021\u0001AP Moller Copenhagen (MSL)\u0001panama_kvinder@hotmail.com\u0001DKCPHMSL1\u00013D9XA9RMF1PJ2\u0001DK\u0001Denmark\u0002TAG0120\u0001OKZHN0XV1Z8KB\u0001\u0001\u0001O\u00015HKDN0XV1Z8KB\u0001\u0001\u0001\u00010\u00011\u0001Booking Number\u0001\u0001\u0001510185665\u00012019-05-16 13.49.24.683955\u0001NayanKumar\u00012019-05-16 13.49.24.683955\u0001NayanKumar\u0001SGPPM0XV1Z8KB\u0001\u0001\u0001\u0001101\u0001Shipment\u0002TAG0120\u0001D8BMG4XV1Z9KB\u0001\u0001\u0001O\u0001\u0001\u0001\u0001\u00010\u00015\u0001Transport Document Number\u0001\u0001\u0001510185665\u00012019-05-16 13.51.42.301735\u0001NayanKumar\u00012019-05-16 13.51.42.301735\u0001NayanKumar\u0001\u0001\u0001BNILG4XV1Z9KB\u0001\u0001151\u0001Transport_Doc
</code></pre>",0,4,2019-06-13 13:57:57.717000 UTC,,2019-06-14 07:48:13.533000 UTC,0,ibm-mq|text-parsing|azure-eventhub|azure-databricks,156,2019-06-13 13:32:03.453000 UTC,2022-02-21 08:14:49.390000 UTC,"Copenhagen, Denmark",56,0,0,14,,,,,,[]
Databricks-Connect 6.6 doesn't add custom modules to Spark Context,"<p>I'm experiencing a strange behaviour in Databricks-Connect 6.6 and was wondering if anybody has seen this before and knows what causes this problem.</p>
<p>I have created a local Spark Context via Databricks-Connect and can successfully connect to my Cluster and execute any script. But, as soon as I try to add a custom module to my Spark Context via <code>sc.addPyFile()</code> and use a custom class/function from it, the execution fails with the <code>ModuleNotFoundError</code>.</p>
<p>I'm aware of how to add a Python file (or a ZIP file containing a package) to the Spark Context and it worked some months ago when I used an earlier version of Databricks-Connect (I think 6.2 which was depreciated, so I had to update). Also, if I pack the package as a Wheel and install it on the cluster, everything works fine. Actually, even if I add the package to the Spark Context while running the script on a Databricks compute target via Azure ML, it works fine. It just seems to be broken if I use Databricks-Connect.</p>
<p>While debugging, I inspected <code>sys.path</code> and the package/module was listed there, so it seems that even though the package is added to the Spark Context, it doesn't get shipped to the worker nodes.</p>
<p>While I first experienced the problem when working with <code>joblibspark</code>, it also occurs when I call</p>
<p><code>sc.parallelize([1, 2, 3, 4]).mapPartitions(test_function).collect()</code></p>
<p>I already tested everything which has been suggested for similar problems and changed all general parameters (Databricks cluster, runtime, local environment, ...) but the error remains and can be reproduced easily by running <code>sc.parallelize()</code>... via Databricks-Connect (and providing test_function through a module which is added to the Spark Context via <code>sc.addPyFile()</code>.</p>
<p>Does anybody have any idea how to solve this?</p>",0,3,2020-07-16 06:13:02.333000 UTC,,2020-08-19 16:33:25.240000 UTC,1,pyspark|modulenotfounderror|databricks-connect,127,2020-07-16 05:39:33.727000 UTC,2022-01-24 09:55:29.407000 UTC,Germany,51,0,0,2,,,,,,[]
How to Trigger an Databricks Notebook using Python Flask Api,"<p>I want to Trigger a Databricks notebbok in my azure account from a flask api function which is running in my local VS Code. I have all the connection parameters like host URL, cluster_id, Token_id except Org_id which i am not able to find and the path of Notebook. How to write a Flask Api to basically trigger (Run) that Databricks Notebook. </p>",1,2,2020-04-29 09:36:32.837000 UTC,,2020-04-30 13:19:40.793000 UTC,1,azure|triggers|flask-restful|azure-databricks,1065,2020-04-28 10:45:44.217000 UTC,2020-05-14 16:18:13.020000 UTC,,11,0,0,0,,,,,,[]
Neptune Graph Database Performance Cost to List Edges of a Type,"<p>The use case for the graph database is to have users and contents (vertices) linked by likes, favorites and reports relations (edges). The problem I have is that I will sometimes need to show the reported contents (from any users). Since this is not a standard graph traversal, I fear this would have a big performance hit.</p>

<p>Is it possible to index the edges of type ""reports"" to quickly get the list of all contents that have been reported? Is there a better way to do this?</p>",1,0,2018-09-21 15:35:30.193000 UTC,,,2,performance|database-performance|graph-databases|amazon-neptune,737,2014-05-27 17:50:57.360000 UTC,2022-03-04 18:14:22.340000 UTC,"Montreal, QC, Canada",3429,111,14,387,,,,,,[]
How to write a Gremlin query when the number of input vertices is unknown?,"<p>How to write a Gremlin query when the number of input vertices is unknown?</p>
<p>Scenario:</p>
<pre><code>1. Poll -&gt; Already existing vertex.
2. Poll Question -&gt; User might send multiple questions that are unknown and multiple options to the question (unknown list).
</code></pre>
<p>I am using <code>GremlinPython</code> and it's not supporting manual transactions with <code>AWS Neptune</code>. How to write it in a single query?</p>
<p>Or Switch to Java?</p>",1,3,2021-06-11 06:20:29.273000 UTC,,,0,aws-lambda|transactions|gremlin|amazon-neptune|gremlinpython,43,2014-03-07 07:02:31.267000 UTC,2022-03-04 04:37:05.967000 UTC,"Bangalore, India",4909,538,406,442,,,,,,[]
Does saveAsTable doubles memory?,"<p>I'm reading quite some data (2.3TB) into a spark dataframe.
All CSV files prepared for a prediction model.</p>

<p>Once loaded we use a temporary view to store it</p>

<pre><code>dSales = spark.read.option(""delimiter"","","").option(""header"", ""true"").option(""inferSchema"", ""true"").csv(""/mnt/"" + sourceMountName + ""/"")
dSales.createOrReplaceTempView(""dSales"")
</code></pre>

<p>After that we produce several other tables with joins and write them all to the database. These tables are used in PowerBI.</p>

<p>My question is, how can I get that big Sales dataframe and the Tempview out of memory once everything are processed?</p>",0,2,2019-04-18 16:13:33.070000 UTC,,2019-04-18 19:28:49.163000 UTC,0,pyspark|pyspark-sql|azure-databricks,72,2017-02-15 10:52:22.493000 UTC,2022-02-22 15:32:07.020000 UTC,"Brussel, België",595,16,0,100,,,,,,[]
How to use AWSRequestSigningApacheInterceptor with AWS SDK2,"<p>I am trying to use REST calls to Neptune SPARQL on existing Java code which already uses Apache HTTP clients.   I'd like to not mix and match AWS SDK1 and SDK2 (which I use for the S3 portion of loading owl to Neptune).</p>

<p>I see these solutions:</p>

<ul>
<li><p><strong>AWSRequestSigningApacheInterceptor</strong> that works with SDK1, but can't find the equivalent in SDK2.</p></li>
<li><p><a href=""https://github.com/awslabs/aws-request-signing-apache-interceptor/issues/5"" rel=""nofollow noreferrer"">aws-request-signing-apache-interceptor</a> on github for building an adaptor class so it can be used in SDK 2 with mix-and-match SDK 1 &amp; 2</p></li>
<li><p><a href=""https://github.com/javaquery/Examples"" rel=""nofollow noreferrer"">javaquery/Examples</a> where Vicky Thakor has gone even more generic and just implemented the V4 signing for any Java REST implementation</p></li>
</ul>

<p>But none of these is what I expected: an AWS or Apache implmentation of an Apache Interceptor for AWS SDK 2.  </p>

<p>Is there such a thing? or is one of the above solutions the best available at the moment?</p>",2,2,2019-05-20 15:36:23.747000 UTC,1.0,2019-05-21 12:44:07.473000 UTC,4,amazon-web-services|rest|sparql|apache-httpclient-4.x|amazon-neptune,5029,2014-04-22 14:51:34.737000 UTC,2022-03-04 16:49:24.140000 UTC,"Schenectady, NY, USA",353,628,5,60,,,,,,[]
Hive managed table drop doesn't delete files on HDFS. Any solutions?,"<p>While deleting managed tables from the hive, its associated files from hdfs are not being removed (on azure-databricks). I am getting the following error:</p>

<blockquote>
  <p>[Simba]SparkJDBCDriver ERROR processing query/statement. Error Code: 0, SQL state: org.apache.spark.sql.AnalysisException: Can not create the managed table('`schema`.`XXXXX`'). The associated location('dbfs:/user/hive/warehouse/schema.db/XXXXX) already exists</p>
</blockquote>

<p>This issue is occurring intermittently. Looking for a solution to this.</p>",1,2,2019-03-18 12:43:35.540000 UTC,1.0,2019-03-18 21:43:57.233000 UTC,2,hadoop|hive|hdfs|azure-databricks,2572,2018-10-30 09:24:06.647000 UTC,2020-04-02 08:04:33.843000 UTC,"Gurgaon, Haryana, India",21,0,0,1,,,,,,[]
Using Dask with fbprophet on Databricks gives error,"<p>I am trying to use <code>Dask</code> in a <code>databricks</code> notebook to perform <code>FBprophet</code> cross validation while tuning hyper parameters using grid search. Here is the code I am using:</p>
<pre class=""lang-py prettyprint-override""><code>import itertools
param_grid = {'seasonality_mode':('multiplicative','additive'),
               'changepoint_prior_scale':[0.001, 0.01, 0.1,0.5],
              'holidays_prior_scale':[0.1,0.5,5]
                    }

# Generate all combinations of parameters
all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]
rmses = []  # Store the RMSEs for each params here

# Prepare cross validation data 
train_df = train_data 

if frequency == &quot;Monthly&quot;:
  holiday = holidays_m
else:
  holiday = holidays_d

client = Client(n_workers=6)  # connect to the cluster 
  
# Train 
# Use cross validation to evaluate all parameters
for params in all_params:
    
    prophet = Prophet(growth = &quot;linear&quot;, holidays=holiday , 
    seasonality_mode = params['seasonality_mode'] , daily_seasonality = False, weekly_seasonality = False, yearly_seasonality = False , changepoint_prior_scale = params['changepoint_prior_scale'],
                         holidays_prior_scale = params['holidays_prior_scale'])
    
    
    prophet.fit(train_df)   # Fit model with given params
     
    
    df_cv = cross_validation(prophet, initial='730 days', period='90 days', horizon = '180 days' , parallel=&quot;dask&quot;)    
    df_p = performance_metrics(df_cv)

</code></pre>
<p>Here is the error I get:</p>
<pre><code>    Train data is created for DFW
/databricks/python/lib/python3.7/site-packages/distributed/client.py:1100: VersionMismatchWarning:

Mismatched versions found

+---------+--------+-----------+---------+
| Package | client | scheduler | workers |
+---------+--------+-----------+---------+
| pandas  | 1.0.1  | 1.0.1     | 1.3.3   |
+---------+--------+-----------+---------+

INFO:fbprophet:Making 5 forecasts with cutoffs between 2020-03-09 00:00:00 and 2021-03-04 00:00:00
INFO:fbprophet:Applying in parallel with &lt;Client: 'tcp://127.0.0.1:40555' processes=6 threads=18, memory=26.35 GiB&gt;
distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File &quot;/databricks/python/lib/python3.7/site-packages/distributed/protocol/core.py&quot;, line 112, in loads
    frames[0], object_hook=_decode_default, use_list=False, **msgpack_opts
  File &quot;msgpack/_unpacker.pyx&quot;, line 195, in msgpack._cmsgpack.unpackb
  File &quot;/databricks/python/lib/python3.7/site-packages/distributed/protocol/core.py&quot;, line 104, in _decode_default
    sub_header, sub_frames, deserializers=deserializers
  File &quot;/databricks/python/lib/python3.7/site-packages/distributed/protocol/serialize.py&quot;, line 475, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File &quot;/databricks/python/lib/python3.7/site-packages/distributed/protocol/serialize.py&quot;, line 407, in deserialize
    return loads(header, frames)
  File &quot;/databricks/python/lib/python3.7/site-packages/distributed/protocol/serialize.py&quot;, line 86, in pickle_loads
    return pickle.loads(x, buffers=new)
  File &quot;/databricks/python/lib/python3.7/site-packages/distributed/protocol/pickle.py&quot;, line 75, in loads
    return pickle.loads(x)
AttributeError: Can't get attribute 'new_block' on &lt;module 'pandas.core.internals.blocks' from '/databricks/python/lib/python3.7/site-packages/pandas/core/internals/blocks.py'&gt;
</code></pre>",0,0,2021-09-14 00:12:17.233000 UTC,,,0,python|parallel-processing|cross-validation|azure-databricks|facebook-prophet,72,2018-08-09 15:10:42.580000 UTC,2022-01-31 22:55:30.150000 UTC,"Fort Worth, TX, USA",1,0,0,3,,,,,,[]
Most optimal Neptune query for getting all related vertices with the shortest distances,"<p>I need to get all related vertices with shortest distances to the given vertex, which distances does not exceed some maximum distance value.</p>

<p>I came up with the following query for maximum distance of 4, but is it possible to optimise this query more? Maybe there is some algorithm for graph distance searching in Neptune?</p>

<p><code>g.V('XXX').repeat(both().dedup()).emit().times(4)
    .project('id', 'count').by(id()).by(path().count(local))</code></p>",1,4,2019-01-04 12:31:09.567000 UTC,,,2,amazon-web-services|gremlin|tinkerpop|tinkerpop3|amazon-neptune,499,2010-09-23 02:49:23.053000 UTC,2019-08-29 14:28:54.740000 UTC,"Tallinn, Estonia",183,7,0,49,,,,,,[]
Facing issue to use spark inside python function,"<p>Im reading data from event hub through databricks. the data from event hub is json messages.Im using a foreach funtion to save the json message to adls. Im getting error as below can anyone help ?
<a href=""https://i.stack.imgur.com/nXDrQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nXDrQ.png"" alt=""enter image description here"" /></a></p>
<p>and process_row is a simple functions which takes json message and removes unnecessary keys saves that to adls. Can anyone help me how to use spark in this function.</p>
<pre><code>def process_row(row):
    a = json.loads(row['newBody'])
    final_df = spark.createDataFrame(pd.DataFrame(a))
</code></pre>",1,0,2022-02-16 05:48:57.223000 UTC,,,0,pyspark|azure-databricks|azure-eventhub,15,2021-05-26 06:28:38.243000 UTC,2022-03-03 06:39:00.907000 UTC,"Hyderabad, Telangana, India",71,0,0,10,,,,,,[]
fossil dvcs difference between update and checkout commands,"<p>After reading the builtin help, it seems to me that both commads can be used for modifying the workspace to match a certain revision. But I don't understand the differences between update and checkout. Please include some trivial workflows in your answer which show when update/checkout are appropriate.</p>",1,0,2010-12-07 12:39:07.117000 UTC,1.0,,7,dvcs|fossil,486,2010-12-07 12:39:07.117000 UTC,2010-12-08 02:46:24.497000 UTC,,71,0,0,0,,,,,,[]
How to access shared Google Drive files through Python?,"<p>I try to access shared Google Drive files through Python.</p>
<p>I have created an OAuth 2.0 ClientID as well as the OAuth consent.</p>
<p>I have copy-pasted this code: <a href=""https://github.com/googleworkspace/python-samples/blob/master/drive/quickstart/quickstart.py"" rel=""nofollow noreferrer"">https://github.com/googleworkspace/python-samples/blob/master/drive/quickstart/quickstart.py</a></p>
<p>The authorization is successful, however, the Python code returns a blank list indicating there are no files in Google Drive, although there are many.</p>
<p>Should there be a difference because I am trying to access a shared folder, if yes, could it cause the error, and how this can be solved?</p>
<p>If not, is this the right approach? I read about API keys and Service Accounts as well, would it make sense to use either of them? Later this service I create will be used by other users on Databricks (running on AWS), and I do not know which solution would be the best one.</p>
<p>Thank you for your help!</p>",2,1,2021-05-20 15:49:53.673000 UTC,,,3,python|google-drive-api|google-drive-shared-drive|aws-databricks,1309,2018-10-31 10:22:40.003000 UTC,2022-03-05 10:59:07.923000 UTC,,99,7,0,20,,,,,,[]
Read CosmosDb items from pyspark (Databricks) with an inconsistent model,"<p>Let's say I have two items in CosmosDb:</p>
<pre><code>{
  &quot;Test&quot;: {
    &quot;InconsistentA&quot;: 10
  },
  &quot;Common&quot;: 1
}
</code></pre>
<pre><code>{
  &quot;Test&quot;: {
    &quot;InconsistentB&quot;: 10
  },
  &quot;Common&quot;: 2
}
</code></pre>
<p>How to read this data so to have the following schema:</p>
<ul>
<li>Test: string (the JSON string of the inconsistent part of the model)</li>
<li>Common: int (the consistent part of the model)</li>
</ul>
<p>I don't know in advance what the model is and the spark CosmosDb driver (com.microsoft.azure.cosmosdb.spark) only reads X first items in CosmosDb to infer the schema.</p>
<p>What I have tried is enforcing the schema this way:</p>
<pre><code>|-- Test: string (nullable = true)
|-- Common: integer (nullable = true)
</code></pre>
<p>But the result of the Test column is:</p>
<pre><code>'{ InconsistentA=10 }'
</code></pre>
<p>Instead of:</p>
<pre><code>'{ &quot;InconsistentA&quot;: 10 }'
</code></pre>",0,2,2022-01-31 17:16:26.490000 UTC,,2022-02-01 08:49:10.577000 UTC,0,pyspark|azure-cosmosdb|azure-databricks,41,2014-11-04 14:00:32.750000 UTC,2022-03-03 11:11:47.893000 UTC,"Paris, France",307,6,0,37,,,,,,[]
Create Spark context from Python in order to run databricks sql,"<p>I've been following this <a href=""https://docs.databricks.com/dev-tools/python-sql-connector.html"" rel=""nofollow noreferrer"">tutorial</a> which lets me connect to Databricks from Python and then run delta table queries. However, I've stumbled upon a problem. When I run it for the FIRST time, I get the following error:</p>
<blockquote>
<p>Container container-name in account
storage-account.blob.core.windows.net not found, and we can't create
it using anoynomous credentials, and no credentials found for them in
the configuration.</p>
</blockquote>
<p>When I go back to my Databricks cluster and run this code snippet</p>
<pre><code>from pyspark import SparkContext
spark_context =SparkContext.getOrCreate()

if StorageAccountName is not None and StorageAccountAccessKey is not None:
  print('Configuring the spark context...')
  spark_context._jsc.hadoopConfiguration().set(
    f&quot;fs.azure.account.key.{StorageAccountName}.blob.core.windows.net&quot;,
    StorageAccountAccessKey)
</code></pre>
<p>(where <code>StorageAccountName</code> and <code>AccessKey</code> are known) then run my Python app once again, it runs successfully without throwing the previous error. I'd like to ask, is there a way to run this code snippet from my Python app and at the same time reflect it on my Databricks cluster?</p>",1,2,2021-11-29 07:34:13.203000 UTC,,2021-11-29 07:42:25.607000 UTC,1,python|apache-spark|azure-databricks|databricks-connect|databricks-sql,133,2016-10-26 12:03:43.167000 UTC,2022-03-02 12:42:37.167000 UTC,,376,43,5,44,,,,,,[]
How to access key value from AWS Key Management Service in data bricks,"<p>I am creating a solution in AWS data bricks and wanted to access the userID and password of RDS from AWS KMS.</p>
<p>Anyone has created this scenario please help.</p>",1,2,2020-09-14 12:31:26.613000 UTC,,,0,aws-kms|key-management|aws-databricks,133,2017-05-30 03:43:03.657000 UTC,2020-10-21 16:24:10.000000 UTC,"Gurgaon, Haryana, India",273,0,0,93,,,,,,[]
How to resolve this sql error of schema_of_json,"<p>I need to find out the schema of a given JSON file, I see sql has schema_of_json <a href=""https://spark.apache.org/docs/latest/api/sql/index.html"" rel=""nofollow noreferrer"">function</a>
and something like this works flawlessly</p>
<pre><code>&gt; SELECT schema_of_json('[{&quot;col&quot;:0}]');
 ARRAY&lt;STRUCT&lt;`col`: BIGINT&gt;&gt;
</code></pre>
<p>But if I query for my table name, it gives me the following error</p>
<pre><code>&gt;SELECT schema_of_json(Transaction) as json_data from table_name;
Error in SQL statement: AnalysisException: cannot resolve 'schemaofjson(`Transaction`)' due to data type mismatch: The input json should be a string literal and not null; however, got `Transaction`.; line 1 pos 7;
</code></pre>
<p>The Transaction is one of the columns in my table and after checking it manually I can attest that it is of String type(json).</p>
<p>The SQL statement has it to give me the schema of the JSON, how to do it?</p>",1,3,2021-04-15 10:10:34.953000 UTC,1.0,,1,sql|azure-databricks,92,2014-10-22 06:51:47.737000 UTC,2022-02-03 10:25:35.743000 UTC,,11,0,0,4,,,,,,[]
Gremlin with low response time on 50M edge on AWS Neptune,"<p>I am implementing a recommendation system with gremlin. I have 2M users and 20k books. When a user buys a book, it creates an edge to the book. I have total of 50M edge over the graph. I am using basic queries but it runs too slow. I am planing to give responses in the real time.</p>
<pre><code>query = graph.V().hasLabel('user').hasId(user_id).match(
    _.as('u1').out().hasLabel('book').dedup().fold().as_('u1_books'),
    _.as('u1').V().hasLabel('user').as_('u2'),
    _.as('u2').out().hasLabel('book').dedup().fold().as_('u2_books')) \
    .where('u1', neq('u2')).as_('m') \
    .project('u1', 'u2', 'b1', 'b2') \
    .by(select('u1').id_()) \
    .by(select('u2').id_()) \
    .by(select('u1_books'').as_('n').select('m').select('u2_books'').unfold().where(within('n')).count()) \
    .by(union(select('u1_books''), select('u2_books'')).unfold().dedup().count()) \
    .project('u1', 'u2', 'int', 'un', 'similarity') \
    .by('u1') \
    .by('u2') \
    .by('b1') \
    .by('b2') \
    .by(__.math('b1/b2')).order().by('similarity', Order.desc).limit(LIMIT)
</code></pre>
<p>This is my query. I cannot find a way to expedite the response. Any help is appreciated.</p>
<p>EDIT</p>
<pre><code>    query = g.V().hasId(user).as_('u1').out().aggregate('u1_books').in_().where(neq('u1')).as_('u2')\
    .project('u1', 'u2', 'int', 'un') \
    .by(select('u1').id_()) \
    .by(select('u2').id_()) \
    .by(select('u1_books').as_('n').select('u2').out().where(within('n')).count()) \
    .by(union(select('u1_books'), select('u2').out()).unfold().dedup().count()).toList()
</code></pre>
<p>I considered your responses and make adjustments. The query that I'm trying to run is this now, even simpler. However, I cannot get a response. It gives a timeout error.</p>",1,8,2021-07-26 16:55:09.743000 UTC,,2021-07-27 09:26:42.373000 UTC,1,gremlin|amazon-neptune|gremlin-server|gremlinpython,190,2021-07-26 16:35:39.997000 UTC,2022-03-02 18:11:56.617000 UTC,,21,0,0,1,,,,,,[]
Gremlin Server : Getting ClosedChannelException for some cases . How to resolve this exception?,"<p>I am using withRemote to connect my java application to gremlin server. I'm running gremlin query for my inputs . But for some inputs I'm facing this issue :</p>

<pre><code>[junit] java.lang.IllegalStateException: java.nio.channels.ClosedChannelException
    [junit]     at org.apache.tinkerpop.gremlin.process.remote.traversal.step.map.RemoteStep.processNextStart(RemoteStep.java:77)
    [junit]     at org.apache.tinkerpop.gremlin.process.traversal.step.util.AbstractStep.next(AbstractStep.java:130)
    [junit]     at org.apache.tinkerpop.gremlin.process.traversal.step.util.AbstractStep.next(AbstractStep.java:38)
    [junit]     at org.apache.tinkerpop.gremlin.process.traversal.Traversal.fill(Traversal.java:179)
    [junit]     at org.apache.tinkerpop.gremlin.process.traversal.Traversal.toList(Traversal.java:117)
    [junit]     at com.lineage.scripts.neptune.client.NeptuneQueryClientImpl.getVertexList(NeptuneQueryClientImpl.java:92)
    [junit]     at com.lineage.scripts.neptune.client.NeptuneQueryClientImpl.getUpstreamTable(NeptuneQueryClientImpl.java:70)
    [junit]     at com.lineage.scripts.neptune.client.NeptuneQueryClientImpl.getUpstreamTableDepthForVertexId(NeptuneQueryClientImpl.java:23)
    [junit]     at com.lineage.scripts.djs.intermediatetable.InterMediateTableForProvider.getUpstreamForATable(InterMediateTableForProvider.java:150)
    [junit]     at com.lineage.scripts.djs.intermediatetable.InterMediateTableForProvider.lambda$null$1(InterMediateTableForProvider.java:105)
    [junit]     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    [junit]     at java.lang.Thread.run(Thread.java:749)
    [junit] Caused by: java.nio.channels.ClosedChannelException
    [junit]     at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:958)
    [junit]     at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:866)
    [junit]     at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1379)
    [junit]     at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:716)
    [junit]     at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:708)
    [junit]     at io.netty.channel.AbstractChannelHandlerContext.access$1700(AbstractChannelHandlerContext.java:56)
    [junit]     at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1102)
    [junit]     at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1149)
    [junit]     at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1073)
    [junit]     at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
    [junit]     at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:510)
    [junit]     at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:518)
    [junit]     at io.netty.util.concurrent.SingleThreadEventExecutor$6.run(SingleThreadEventExecutor.java:1044)
    [junit]     at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    [junit]     ... 1 more
</code></pre>

<p>I looked in the gremlin docs and few stackoverflow articles and changed the configuration as well. </p>

<p>'''</p>

<pre><code>            maxWaitForConnection(3000).
            reconnectInterval(1000).
            maxWaitForSessionClose(120000).
            maxContentLength(65536000).
            keepAliveInterval(180000).
            maxConnectionPoolSize(500)
</code></pre>

<p>I'm still not able to figure out why I'm getting exception here? Thanks in advance.</p>",0,8,2020-04-17 13:26:11.533000 UTC,,2020-04-28 11:02:40.687000 UTC,1,java|exception|gremlin|gremlin-server|amazon-neptune,182,2017-05-01 20:34:19.193000 UTC,2021-08-10 10:22:35.753000 UTC,,63,1,0,10,,,,,,[]
"Azure Databricks Notebook unable to find ""dbutils"" when it is in package","<p>I am creating a class in for communicating with azure storage blobs and it is working fine but if I try to put this class in package it is giving me an error <strong>""error: not found: value dbutils""</strong>. It is working fine if I remove the <strong>""package Libraries.Custom""</strong> above my class.</p>

<p>I am creating a class in azure databricks notebook using Scala. Could anybody help me out. The code is stated below:</p>

<p><strong>Class Code</strong></p>

<pre><code>package Libraries.Custom

import org.apache.spark.sql._

class BlobContext {
  // Basic Azure Storage Configurations
  val blob_account_name = ""REPLACE_BY_ACTUAL_VALUE""
  val blob_account_access_key = ""REPLACE_BY_ACTUAL_VALUE""
  val blob_container_name = ""REPLACE_BY_ACTUAL_VALUE""
  val blob_server = s""REPLACE_BY_ACTUAL_VALUE""
  val blob_wasbs = s""REPLACE_BY_ACTUAL_VALUE""

  val spark = SparkSession
    .builder()
    .appName(""Path SOM"")
    .master(""local[*]"")
    .config(""spark.ui.enabled"", ""false"")
    .getOrCreate()

  def SetConfigurations {
    spark.conf.set(blob_server, blob_account_access_key)
  }

  def ReadData(fileName: String, fileType: String): DataFrame = {
    SetConfigurations
    val dataFrame = spark.read.format(fileType).option(""header"", ""true"").load(s""${blob_wasbs}${fileName}.${fileType}"")    
    return dataFrame
  }

  def WriteData(fileDataFrame: DataFrame, fileName: String, fileType: String) {
    val absTempFilePath = s""${blob_wasbs}SPARK_NEW_${fileName}.temp""
    val absFilePath = s""${blob_wasbs}SPARK_NEW_${fileName}.${fileType}""

    fileDataFrame.repartition(1).write.format(fileType).mode(""overwrite"").option(""header"", ""true"").option(""inferSchema"", ""true"").option(""delimiter"", "","").csv(absTempFilePath)

    val partition_path = dbutils.fs.ls(absTempFilePath + ""/"").filter(file=&gt;file.name.endsWith("".csv""))(0).path
    dbutils.fs.cp(partition_path, absFilePath)
    dbutils.fs.rm(absTempFilePath,recurse=true)
  }      
}
</code></pre>

<p><strong>Error</strong></p>

<pre><code>&lt;notebook&gt;:37: error: not found: value dbutils
    val partition_path = dbutils.fs.ls(absTempFilePath + ""/"").filter(file=&gt;file.name.endsWith("".csv""))(0).path
                         ^
&lt;notebook&gt;:38: error: not found: value dbutils
    dbutils.fs.cp(partition_path, absFilePath)
    ^
&lt;notebook&gt;:39: error: not found: value dbutils
    dbutils.fs.rm(absTempFilePath,recurse=true)
    ^
&lt;notebook&gt;:39: error: not found: value recurse
    dbutils.fs.rm(absTempFilePath,recurse=true)
                                  ^
Compilation failed.
</code></pre>",1,0,2018-12-27 13:03:27.290000 UTC,1.0,2018-12-27 14:23:35.523000 UTC,4,scala|azure|apache-commons-dbutils|azure-databricks,1241,2017-12-02 07:43:45.223000 UTC,2020-09-30 13:33:02.697000 UTC,"Karachi, Sindh, Pakistan",141,0,0,21,,,,,,[]
Can't write to Azure Sql DataWarehouse from databricks pyspark workers,"<p>I am trying to simply write data to azure sql DataWarehouse, while using azure blob storage for staging.</p>

<p>There is a very straight forward tutorial at azure databricks documentation <a href=""https://docs.databricks.com/spark/latest/data-sources/azure/sql-data-warehouse.html"" rel=""nofollow noreferrer"">azure/sql-data-warehouse</a>, which works, if you follow it step by step. </p>

<p>However in my scenario, I have to do the writing from a worker that is executing a foreach.</p>

<p>Here some links related to the issue:</p>

<p><a href=""https://stackoverflow.com/questions/57082556/error-using-pyspark-with-wasb-connecting-pyspark-with-azure-blob"">error-using-pyspark-with-wasb-connecting-pyspark-with-azure-blob</a></p>

<p><a href=""https://github.com/Azure/mmlspark/issues/456"" rel=""nofollow noreferrer"">github.com/Azure/mmlspark/issues/456</a></p>

<p><a href=""https://stackoverflow.com/questions/47352813/pyspark-java-io-ioexception-no-filesystem-for-scheme-https"">pyspark-java-io-ioexception-no-filesystem-for-scheme-https</a></p>

<p>So, this code below <strong>WORKS</strong>:</p>

<pre><code>  spark = SparkSession.builder.getOrCreate()      
  spark.conf.set(""fs.azure.account.key.&lt;storageAccountName&gt;.blob.core.windows.net"", ""myKey"")  
  df = spark.createDataFrame([(1, 2, 3, 4), (5, 6, 7, 8)], ('a', 'b', 'c', 'd'))  

  (df.write 
  .format(""com.databricks.spark.sqldw"") 
  .option(""url"", ""jdbc:sqlserver:..."") 
  .option(""user"", ""user@server"") 
  .option(""password"", ""pass"") 
  .option(""forwardSparkAzureStorageCredentials"", ""true"") 
  .option(""dbTable"", ""dbo.table_teste"") 
  .option(""tempDir"", ""wasbs://&lt;container&gt;@&lt;storageAccountName&gt;.blob.core.windows.net/"") 
  .mode(""append"")
  .save())
</code></pre>

<p>However it fails when I insert the code above inside a foreach, just like below:</p>

<pre><code>from pyspark.sql.session import SparkSession
from pyspark.sql import Row

spark = SparkSession.builder.getOrCreate()          

def iterate(row):
   # The code above

dfIter = spark.createDataFrame([(1, 2, 3, 4)], ('a', 'b', 'c', 'd'))
dfIter.rdd.foreach(iterate)
</code></pre>

<p>Executing it will generate this exception:</p>

<blockquote>
  <p>py4j.protocol.Py4JJavaError: An error occurred while calling o54.save.
  : com.databricks.spark.sqldw.SqlDWConnectorException: Exception
  encountered in SQL DW connector code.</p>
  
  <p>Caused by: java.io.IOException: No FileSystem for scheme: wasbs</p>
</blockquote>

<p>I have had the same kind of issue when saving on delta tables: <a href=""https://stackoverflow.com/questions/56783356/pyspark-saving-is-not-working-when-called-from-inside-a-foreach"">pyspark-saving-is-not-working-when-called-from-inside-a-foreach</a></p>

<p>But in that case, I just needed to setup '/dbfs/' at the begining of the delta table location, so the worker would be able to save it in the right place.</p>

<p><strong>Based on that, I believe something is missing in the worker, and that is why it is not properly executing this saving. Maybe a library that I should setup into spark config.</strong></p>

<p>I also looked into databricks community: <a href=""https://forums.databricks.com/questions/8620/i-want-to-save-the-results-of-a-query-to-azure-blo.html"" rel=""nofollow noreferrer"">save-the-results-of-a-query-to-azure-blo</a> and they managed to solve the issue by setting this config:</p>

<pre><code>sc.hadoopConfiguration.set(""fs.wasbs.impl"",""org.apache.hadoop.fs.azure.NativeAzureFileSystem"")
</code></pre>

<p>PySpark:</p>

<pre><code>spark.sparkContext._jsc.hadoopConfiguration().set(""fs.wasbs.impl"",""org.apache.hadoop.fs.azure.NativeAzureFileSystem"")
</code></pre>

<p>But it didn't work and I got this exception:</p>

<blockquote>
  <p>Caused by: java.lang.RuntimeException:
  java.lang.ClassNotFoundException: Class
  org.apache.hadoop.fs.azure.NativeAzureFileSystem not found</p>
</blockquote>

<p><strong>org.apache.hadoop:hadoop-azure:3.2.0 is installed.</strong></p>

<p>Well, any help?</p>",1,2,2019-07-17 23:09:19.540000 UTC,1.0,,0,python-3.x|azure|apache-spark|pyspark|azure-databricks,1750,2015-09-15 08:29:24.193000 UTC,2021-08-26 20:22:38.807000 UTC,"São Paulo, State of São Paulo, Brazil",328,306,3,79,,,,,,[]
Saving CSV with custom two character line separator per Spark SQL,"<p>You must save the csv file with the &quot;\n\r&quot; delimiter. However, Spark does not allow you to add a delimiter with more than 1 character. The data volumes are large. Please tell me how to get around this limitation. Sample code below.</p>
<p>Error: requirement failed: 'lineSep' can contain only 1 character.</p>
<p>The spark documentation states that the separator &quot;\n\r&quot; can be used.</p>
<pre><code>spark.sql(&quot;SELECT * FROM SS_LSR_OUT&quot;)
  .coalesce(1)       
  .write
  .format(&quot;csv&quot;)             
  .mode(&quot;overwrite&quot;) 
  .option(&quot;header&quot;, &quot;true&quot;)
  .option(&quot;encoding&quot;, &quot;cp1251&quot;)
  .option(&quot;multiline&quot;, &quot;true&quot;)
  .option(&quot;lineSep&quot;,&quot;\n\r&quot;)
  .option(&quot;sep&quot;, &quot;;&quot;)
  .csv(path_out + &quot;.tmp&quot;)
</code></pre>",0,1,2021-06-03 13:06:31.317000 UTC,,2021-06-21 10:25:36.633000 UTC,0,python|scala|csv|apache-spark-sql|azure-databricks,79,2020-09-21 23:17:35.923000 UTC,2021-06-03 20:20:45.897000 UTC,"St Petersburg, Russia",1,0,0,2,,,,,,[]
Accessing ADLS Gen 2 storage from Databricks,"<p>I am trying to read files in ADLS Gen 2 Storage from Databricks Notebook using Python.</p>
<p>The storage container however has it's public access level set to &quot;Private&quot;.</p>
<p>I have Storage Account Contributor and Storage Blob Data Contributor access.</p>
<p>How can the Databricks be allowed to read and write into ADLS Storage ?</p>",1,2,2020-12-24 03:08:07.693000 UTC,,,0,python-3.x|azure|azure-databricks|azure-data-lake-gen2,130,2018-04-11 04:10:34.993000 UTC,2022-02-24 05:17:06.103000 UTC,,491,46,0,249,,,,,,[]
Fossil SCM: Merge leafs back to trunk,"<p>I have been working for some time with Fossil SCM but I still see something I don't quite get.</p>

<p><img src=""https://i.stack.imgur.com/VNDnn.png"" alt=""fossil timeline""></p>

<p>In the screenshot you can see that I have two Leaves that are present in the repository, but sadly I can't find the way to merge them back into trunk (is annoying to have the 'Leaf' mark in all my commits).</p>

<p>I had Leaves before and I normally merged them by doing </p>

<pre><code>fossil update trunk
fossil merge &lt;merged_changeset_id&gt;
</code></pre>

<p>But now I just get the message:</p>

<pre><code>fossil: cannot find a common ancestor between the current checkout and ...
</code></pre>

<p>Update: This repository is a complete import from a git repository, I'm gonna try to reproduce the exception.</p>",2,2,2013-09-30 21:53:36.297000 UTC,,2013-10-01 22:11:00.140000 UTC,4,dvcs|fossil,978,2010-02-22 13:34:23.463000 UTC,2022-03-03 18:40:14.040000 UTC,"Santa Cruz, Bolivia",510,792,2,199,,,,,,[]
"Distributed Version Control ""killer applications""","<p>Considering switching to Mercurial or Git? We are too. 
I'm currently studying the benefits of DVCS which turn out to be vast, lust and must.</p>

<p>I would love to hear from the community typical usage patterns.</p>

<p>Let's create a ""Top N"" productivity feature list for DVCS (based on Mercurial, Git or alike).</p>

<p>Please describe work flows that prove to be productive for you / your team, procedures that DVCS helped you achieve/improve as well as blunt ""good stuff"" that DVCS gives you (don't assume stuff are clear to the novice user).</p>

<p>I think that such a list could help folks approaching the team with a DVCS suggestion.</p>

<p>This question is community wiki, obviously.</p>",7,2,2010-10-10 10:54:29.887000 UTC,6.0,2017-08-14 12:30:16.627000 UTC,11,git|version-control|mercurial|dvcs,1372,2008-12-20 23:15:39.067000 UTC,2020-06-21 15:07:54.750000 UTC,Israel,27462,331,17,1587,,,,,,[]
Python UTC to iso format now working- cast error,"<p>I am getting datetime in UTC format from datafactory in databricks.
I am trying to convert it into databricks timestamp and insert into database.</p>
<pre><code>Format that i am  receiving: 2020-11-02T01:00:00Z
Convert into : 2020-11-02T01:00:00.000+0000 (iso format)
</code></pre>
<p>i tried to convert the string into isoformat()</p>
<pre><code>df.selectExpr(&quot;make_timestamp(YEAR, MONTH, DAY, HOUR, MINUTE, SECOND) as MAKE_TIMESTAMP&quot;)
</code></pre>
<p>and then</p>
<pre><code>spark.sql(&quot;INSERT INTO test VALUES (1, 1, 'IMPORT','&quot;+ slice_date_time.isoformat() +&quot;','deltaload',0, '0')&quot;)
</code></pre>
<p>But when i try to insert it I am receiving error:</p>
<ul>
<li>Cannot safely cast 'start_time': string to timestamp</li>
<li>Cannot safely cast 'end_time': string to timestamp;</li>
</ul>
<p>I also tried making timestamp. but still the same error.</p>",1,0,2020-11-03 09:05:50.557000 UTC,,,0,python-3.x|pyspark|apache-spark-sql|azure-databricks,183,2016-01-19 23:58:20.783000 UTC,2022-03-02 11:59:52.213000 UTC,,153,18,0,56,,,,,,[]
Execute msdb stored procedure with pyspark jdbc,"<p>PySpark has <code>df = spark.read.jdbc()</code></p>

<p>It also has <code>df.write.jdbc()</code></p>

<p>Does it have some fashion of <code>spark.{execute or call or whatever}.jdbc()</code>?</p>",0,3,2019-04-11 12:48:14.150000 UTC,,2019-05-04 08:33:47.497000 UTC,2,jdbc|pyspark|azure-databricks,1376,2018-11-20 14:05:55.320000 UTC,2019-06-12 22:21:23.793000 UTC,,97,2,0,46,,,,,,[]
How to set current datetime in gremlin console?,"<p>How to set current DateTime in the gremlin console?</p>
<p>I need to add an audit field <code>created_on</code> with <code>current_timestamp</code> as default value</p>
<p>Eg:</p>
<pre><code>g.addV('student').property('name', 'Thirumal').property('created_on', datetime())
</code></pre>
<p>In, the above query, I am getting an error on <code>datetime()</code>. What is the syntax to add default date time?</p>",1,0,2022-02-10 09:21:29.447000 UTC,1.0,2022-02-10 16:08:34.547000 UTC,0,datetime|tinkerpop|amazon-neptune|tinkerpop3|gremlinpython,12,2014-03-07 07:02:31.267000 UTC,2022-03-04 04:37:05.967000 UTC,"Bangalore, India",4909,538,406,442,,,,,,[]
"What is a practical use example of ""hg copy file file2"" using Mercurial?","<p>Mercurial has an</p>

<pre><code>hg copy file file2
</code></pre>

<p>command and the change can propagate change at the first merge.  The O'Reilly Mercurial book says that Mercurial is the only source control system that does that.</p>

<p>What is a practical use of this?  The book mentioned making a copy of the file and do bug fix, so the bug fix can propagate back to the original file, but with version control, don't we usually edit the file directly, and if the bug fix works, then directly commit that file?  Or even if for some reason we need to make a copy, we can <code>cp file file2</code>, test the fix, and <code>mv file2 file</code> to move that file back to the original file, and commit the file.  What is a good example of using the <code>hg copy</code> feature?</p>",3,1,2010-10-16 02:13:48.113000 UTC,0.0,2011-12-16 09:28:11.313000 UTC,4,mercurial|dvcs,911,2009-05-09 15:50:29.477000 UTC,2022-03-04 09:41:10.460000 UTC,,137341,1445,39,12817,,,,,,[]
"Change data capture with databricks delta using ""INSERT OVERWRITE""","<p>Instead of deleting and selecting the data that I want to load into my Delta table, I would like to use INSERT OVERWRITE for performance purposes. 
I have a couple of queries e.g.</p>

<p>query1 = ""DELETE FROM asl_process.otr WHERE substring(<code>Requested_delivery_d</code>,1,6) in ('{}' , '{}' , '{}' , '{}')"".format(month_M1, month, month_P1, month_P2) ;</p>

<p>query2 = ""DELETE FROM asl_process.otr WHERE substring(<code>Requested_delivery_d</code>,1,6) in (select distinct substring(<code>Requested_delivery_d</code>,1,6) from df_otrcurr)</p>

<p>that I want to summarise. </p>

<p>I tried to rewrite the 2 queries in a sql statement like:</p>

<pre><code>%sql

INSERT OVERWRITE TABLE asl_process.otr
    PARTITION(Ord_Planned_GI_date)
    SELECT * FROM asl_process.otr 
    WHERE substring(`Requested_delivery_d`,1,6) &lt; &gt; ('month_M1', 'month', 'month_P1', 'month_P2')
          AND substring(`Requested_delivery_d`,1,6) &lt; &gt; (select distinct substring(`Requested_delivery_d`,1,6) from df_otrcurr)
</code></pre>

<p>The problem is that I get the following error:
""SyntaxError: invalid syntax"" at INSERT OVERWRITE.</p>

<p>What am I doing wrong? It doesn't like that I have the schema before the name of the table?..</p>",1,0,2019-07-02 12:27:31.507000 UTC,,2019-07-02 21:44:52.107000 UTC,1,sql|apache-spark|hive|azure-databricks,465,2016-10-06 18:30:13.367000 UTC,2022-02-15 07:09:58.987000 UTC,,125,12,0,18,,,,,,[]
is it possible access neptune DB from local machine via ssh tunnel/port forward while neptune IAM DB authorization enabled?,"<p>i am unable to connect to Neptune DB from local system via ssh-tunnel EC2(ec2 exists in same vpc as neptune db) where neptune DB connectivity is established when Neptune IAM DB authorization is enabled.</p>
<p>With disabling IAM DB authorization i can able to access neptune DB from local machine.I could not find enough documentation on this. Can someone please help.</p>",1,3,2021-02-24 10:51:25.487000 UTC,,2021-03-07 19:21:30.920000 UTC,1,amazon-web-services|amazon-iam|amazon-neptune,175,2021-02-11 11:52:26.593000 UTC,2021-07-16 10:45:48.563000 UTC,,23,0,0,3,,,,,,[]
GremlinPython: ConnectionResetError: Cannot write to closing transport,"<p>I am facing the below problem very often in AWS lambda - Neptune. using the latest version <code>gremlinpython==3.5.1</code>. How to fix? even in the previous version - same problem</p>
<pre><code>  user_available = g().V(cognito_username).hasNext()
  File &quot;/var/task/gremlin_python/process/traversal.py&quot;, line 80, in hasNext
    self.traversal_strategies.apply_strategies(self)
  File &quot;/var/task/gremlin_python/process/traversal.py&quot;, line 548, in apply_strategies
    traversal_strategy.apply(traversal)
  File &quot;/var/task/gremlin_python/driver/remote_connection.py&quot;, line 63, in apply
    remote_traversal = self.remote_connection.submit(traversal.bytecode)
  File &quot;/var/task/gremlin_python/driver/driver_remote_connection.py&quot;, line 59, in submit
    result_set = self._client.submit(bytecode, request_options=self._extract_request_options(bytecode))
  File &quot;/var/task/gremlin_python/driver/client.py&quot;, line 123, in submit
    return self.submitAsync(message, bindings=bindings, request_options=request_options).result()
  File &quot;/var/lang/lib/python3.6/concurrent/futures/_base.py&quot;, line 432, in result
    return self.__get_result()
  File &quot;/var/lang/lib/python3.6/concurrent/futures/_base.py&quot;, line 384, in __get_result
    raise self._exception
  File &quot;/var/task/gremlin_python/driver/connection.py&quot;, line 66, in cb
    f.result()
  File &quot;/var/lang/lib/python3.6/concurrent/futures/_base.py&quot;, line 425, in result
    return self.__get_result()
  File &quot;/var/lang/lib/python3.6/concurrent/futures/_base.py&quot;, line 384, in __get_result
    raise self._exception
  File &quot;/var/lang/lib/python3.6/concurrent/futures/thread.py&quot;, line 56, in run
    result = self.fn(*self.args, **self.kwargs)
  File &quot;/var/task/gremlin_python/driver/protocol.py&quot;, line 86, in write
    self._transport.write(message)
  File &quot;/var/task/gremlin_python/driver/aiohttp/transport.py&quot;, line 86, in write
    self._loop.run_until_complete(async_write())
  File &quot;/var/lang/lib/python3.6/asyncio/base_events.py&quot;, line 488, in run_until_complete
    return future.result()
  File &quot;/var/task/gremlin_python/driver/aiohttp/transport.py&quot;, line 83, in async_write
    await self._websocket.send_bytes(message)
  File &quot;/var/task/aiohttp/client_ws.py&quot;, line 155, in send_bytes
    await self._writer.send(data, binary=True, compress=compress)
  File &quot;/var/task/aiohttp/http_websocket.py&quot;, line 685, in send
    await self._send_frame(message, WSMsgType.BINARY, compress)
  File &quot;/var/task/aiohttp/http_websocket.py&quot;, line 598, in _send_frame
    raise ConnectionResetError(&quot;Cannot write to closing transport&quot;)
ConnectionResetError: Cannot write to closing transport
</code></pre>",1,0,2021-08-23 11:54:29.793000 UTC,1.0,2021-08-28 13:22:16.443000 UTC,0,graph|amazon-neptune|gremlinpython|tinkerpop,278,2014-03-07 07:02:31.267000 UTC,2022-03-04 04:37:05.967000 UTC,"Bangalore, India",4909,538,406,442,,,,,,[]
How to add dynamic connection string in Azure Data Factory,<p>Can someone help me on achieving dynamic connections to databases using azure data factory?</p>,1,1,2019-03-21 11:21:54.907000 UTC,,,-1,azure|azure-sql-database|azure-cosmosdb|azure-data-factory|azure-databricks,1475,2018-08-01 14:58:33.147000 UTC,2019-09-20 09:43:02.913000 UTC,,111,0,0,11,,,,,,[]
AWS Neptune Gremlin query gives MemoryLimitExceededException,"<p>We have 3.7M nodes and 11.2M relations in AWS Neptune. Here we need these nodes: <code>Organization</code>, <code>Member</code>, <code>Proposal</code>, <code>Vote</code> and relations:
<code>Member-[:IN]-&gt;Organization</code>, <code>Vote-[:BELONGS_TO]-&gt;Proposal</code>, <code>Vote-[:VOTED_BY]-&gt;Member</code>, <code>Member-[:IN]-&gt;Organization</code>.</p>
<p>The goal is to build the query which finds pairs of members in organization and count of proposals they voted with the same <code>vote.choice</code>. Here is the query:</p>
<pre><code>g.V().has('Organization','id','${id}').as('d').
match(
 as('d').in('IN').hasLabel('Vote').as('v1').out('BELONGS_TO').hasLabel('Proposal').as('p'),
 as('d').in('IN').hasLabel('Vote').as('v2').out('BELONGS_TO').hasLabel('Proposal').as('p1').
  select('p1','p').where('p1',eq('p')
 ),
 as('v1').out('VOTED_BY').hasLabel('Member').as('m1'),
 as('v2').out('VOTED_BY').hasLabel('Member').as('m2').
  select('v1','v2').by('choice').where('v1',eq('v2'))).
  select('m1','m2').by('address').where('m1',lt('m2')).
   group().by(
    select('m1','m2')).by(select('p').count()
   ).
   order(local).by(values, desc).
   limit(local, 20)
</code></pre>
<p>The issue is that the query returns this error for Organizations with a lot of Members and Votes:</p>
<pre><code>{&quot;code&quot;:&quot;MemoryLimitExceededException&quot;,&quot;detailedMessage&quot;:&quot;Query cannot be completed due to memory limitations.&quot;,&quot;requestId&quot;:&quot;e8f8a361-40c4-4db9-8da4-a618d0e20d92&quot;}
</code></pre>
<p>Can the memory config be adjusted on AWS Neptune? Or should the query be optimized and what are ways to optimize it?</p>
<p><strong>UPDATE:</strong></p>
<p>The query without <code>match</code>:</p>
<pre><code>g.V().has('Dao','organizationId','${id}').as('d').in('IN').hasLabel('Vote').as('v1').out('BELONGS_TO').hasLabel('Proposal').as('p'). 
 select('d').in('IN').hasLabel('Vote').as('v2').out('BELONGS_TO').hasLabel('Proposal').as('p1').where(eq('p')).select('v1').out('VOTED_BY').hasLabel('Member').as('m1'). 
 select('v2').out('VOTED_BY').hasLabel('Member').as('m2').where('v1',eq('v2')).by('choice').
 select('m1','m2').by('address').where('m1',lt('m2')).
   group().by( 
     select('m1','m2') 
   ).by(
     select('p').count()
   ).
 order(local).by(values, desc).
 limit(local,20)
</code></pre>
<p>Original Cypher query:</p>
<pre><code>CALL {
    MATCH (o:Organization { id: '${id}' })&lt;-[:IN]-(p:Proposal)
    RETURN COUNT(p) AS total_proposals_count
}
WITH total_proposals_count
MATCH (o:Organization { id: '${id}' })&lt;-[:IN]-(v1:Vote)-[:BELONGS_TO]-&gt;(p:Proposal)
MATCH (o)&lt;-[:IN]-(v2:Vote)-[:BELONGS_TO]-&gt;(p)
MATCH (v1)-[:VOTED_BY]-&gt;(m1:Member)
MATCH (v2)-[:VOTED_BY]-&gt;(m2:Member)
WHERE m1.address &lt; m2.address AND v1.choice = v2.choice
RETURN [m1, m2] AS members, COUNT(p) AS voted_together, total_proposals_count
ORDER BY voted_together DESC
LIMIT 20
</code></pre>",0,8,2022-02-02 13:53:41.063000 UTC,,2022-02-07 18:47:10.320000 UTC,1,gremlin|amazon-neptune|memory-limit|aws-neptune,71,2022-01-27 14:38:10.857000 UTC,2022-03-01 09:35:54.453000 UTC,"Lviv, Lviv Oblast, Ukraine",23,2,0,5,,,,,,[]
Ways to import data from SQL Server 2016 to Azure Data Lake Gen 2,"<p>I am finding the safest way to import several dimension and fact tables from SQL Server to Azure Data Lake Gen 2. This is what I found:</p>
<p><strong>Option 1</strong>:  Azure Data Factory
This involves a cost and therefore not preferable solution for me at the moment.</p>
<p><strong>Option 2</strong>:  Python from Azure Databricks</p>
<p><strong>2a) Apache Spark Connector</strong></p>
<pre><code>jdbcDF = spark.read \
        .format(&quot;com.microsoft.sqlserver.jdbc.spark&quot;) \
        .option(&quot;url&quot;, url) \
        .option(&quot;dbtable&quot;, table_name) \
        .option(&quot;user&quot;, username) \
        .option(&quot;password&quot;, password).load()
</code></pre>
<p><strong>2b) Built-in JDBC Spark SQL Connector</strong></p>
<p><strong>2c) ODBC driver and pyodbc package</strong></p>
<p><strong>2d) pymssql package</strong></p>
<p><strong>2e) JayDeBeApi</strong></p>
<p><strong>Option 3: SSIS package</strong></p>
<p>I am not sure which of these I should use. What are the pros and cons of the above approaches?</p>
<p>Once I read the data into a data frame using one of the above approaches, how do I save them to the Data Lake Gen2 storage ?</p>",1,4,2020-12-16 08:46:18.810000 UTC,,2020-12-16 09:05:30.230000 UTC,1,python|azure-data-lake|azure-databricks|azure-data-lake-gen2,201,2018-04-11 04:10:34.993000 UTC,2022-02-24 05:17:06.103000 UTC,,491,46,0,249,,,,,,[]
How to write CSV to Azure Storage Gen2 with Databricks(Python),"<p>I would like write reqular CSV file to Storage, but what I get is folder &quot;sample_file.csv&quot; and 4 files under it. How to create normal csv file from data frame to Azure Storage Gen2?</p>
<p>I'm happy with any advice or link to article.</p>
<p>df.coalesce(1).write.option(&quot;header&quot;, &quot;true&quot;).csv(TargetDirectory + &quot;/sample_file.csv&quot;)</p>",1,0,2020-12-01 13:55:09.587000 UTC,,,0,azure-databricks,1279,2016-11-04 09:17:30.693000 UTC,2022-03-04 08:19:44.720000 UTC,Finland,1157,106,0,333,,,,,,[]
Override partitioned Delta table schema,"<p>A Delta table defined like this:</p>
<pre><code>CREATE TABLE IF NOT EXISTS test
(
    name                    string,
    age                     bigint,
    city                    string,
    country                 string
)
USING DELTA
LOCATION '/mnt/data';
</code></pre>
<p>In spark, I partitioned this table based on city and country columns.</p>
<p>Now, I need to reverse this and delete this partitioning.</p>
<p>One way to do it is via Spark:</p>
<pre><code>val input = spark.read.table(&quot;test&quot;)
input.write.format(&quot;delta&quot;)
  .mode(&quot;overwrite&quot;)
  .option(&quot;overwriteSchema&quot;, &quot;true&quot;)
  .saveAsTable(&quot;test&quot;)
</code></pre>
<p>Is there another way to do it using only SQL? (Like Replace Table maybe?)</p>",0,0,2021-12-14 22:39:10.793000 UTC,1.0,,0,apache-spark|azure-databricks|delta-lake,81,2015-04-22 08:46:46.853000 UTC,2022-03-04 16:02:32.660000 UTC,"Paris, France",1,0,0,3,,,,,,[]
Snowflake Table from Databricks using Python/Scala,"<p>Can anyone help me out please? I want to create a table and lad data into it in Snowflake from Databricks using Python/Scala.
Below is my code snippet. I'm getting the below error. Could you please let me know how I can create the table first if not exists in Databricks notebook using Python or Scala and then load the data?
If so, what functions do I need to use. Below gives me an error. Thanks!</p>
<p>'''
df1.write.format(&quot;snowflake&quot;).options(sfOptions).option(&quot;dbtable&quot;,&quot;TEST_TABLE&quot;)
.mode(SaveMode.Append)
'''</p>",1,1,2021-10-12 10:28:14.853000 UTC,,2021-10-12 14:42:17.383000 UTC,1,scala|snowflake-cloud-data-platform|azure-databricks,64,2018-01-09 11:01:06.050000 UTC,2022-03-04 17:05:34.420000 UTC,,117,0,0,28,,,,,,[]
Geomesa on Databricks - library installation fails,"<p>I would like to install the Geomesa library on Databricks, but choosing any geomesa library goes automatically to &quot;failed&quot;. I have tried all of the available versions. What am I doing wrong?</p>
<p><a href=""https://i.stack.imgur.com/1qKfo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1qKfo.png"" alt=""enter image description here"" /></a></p>",0,4,2022-01-05 08:31:11.563000 UTC,,,1,azure-databricks|geomesa,52,2021-04-28 19:38:01.237000 UTC,2022-03-04 13:43:39.593000 UTC,,39,5,0,30,,,,,,[]
How can spark infer types of some specific column only?,"<p>I'm using <code>.options(&quot;inferSchema&quot; , &quot;true&quot;)</code> to infer the schema of the dataframe and make a schema check.</p>
<p>But i would like to deactivate this inferschema for some specific column only.</p>
<p>The reason is that we receive sporadically a <code>string</code> in a column expected as <code>integer</code> , and it fails.</p>
<p>When the file contains only <code>integer</code> ==&gt; spark infers it as <code>integer</code> ==&gt; all good
But sometimes and it is rare, we receive a couple of <code>string</code> in this column. Then it fails.</p>
<p>I cannot expect a <code>string</code> instead every day ==&gt; because <code>spark</code> will infer it as an <code>integer</code> most of the time, and the schema check would fail.</p>
<p>That why i would like to check if spark can infer type check only for some column, rather than the all dataframe. And let the other column as <code>strings</code>.</p>
<p>Altenative would be to set up the <code>option(&quot;quote&quot;,&quot;&quot;)</code> parameter only on some of the fields.</p>",0,1,2021-10-25 12:32:43.970000 UTC,,2021-10-26 20:06:03.457000 UTC,0,csv|apache-spark|pyspark|schema|azure-databricks,88,2018-06-02 16:53:24.290000 UTC,2022-03-05 21:11:03.483000 UTC,,684,199,1,121,,,,,,[]
"to_json in pyspark is exluding null values, but I need the null values as blank","<p>I am converting a struct column in dataframe to json column using to_json in pyspark, but null values in few of the struct fields are ignored in json, I dont want the null values to be ignored.</p>",1,1,2020-10-14 12:00:17.163000 UTC,,2020-10-14 19:22:38.057000 UTC,0,python|struct|pyspark|azure-databricks|to-json,383,2018-07-12 08:30:50.097000 UTC,2021-05-12 10:19:05.273000 UTC,"Bengaluru, Karnataka, India",1,0,0,0,,,,,,[]
pyspark set ttl on rows in Cassandra table,"<p>I'm using PySpark to manipulate data in Cassandra and want to setup ttl on rows. I tried the following code it ran fine, but after the run i checked the table, the record/row was still there. </p>

<pre><code>def set_ttl(tableName, keyspaceName, dataDf):
  (dataDf.write
          .format(""org.apache.spark.sql.cassandra"")
          .options(table = tableName, keyspace = keyspaceName, ttl = str(1))
          .mode(""append"")
          .save())


emails='abc@test.com'.split("","")
df = read_table(my_table, my_keyspace,""*"").where(col(""email"").isin(emails))

set_ttl(my_table, my_keyspace, df)
</code></pre>

<p>I googled around and got conflict answers, some said it's doable but other said it's supported. since I didn't get any error when I ran the script so I assume it should work? Please help.</p>",0,3,2020-01-06 22:39:50.823000 UTC,,2020-01-07 14:34:58.930000 UTC,1,pyspark|azure-databricks,49,2010-10-07 00:08:05.620000 UTC,2022-03-06 01:26:06.733000 UTC,,4305,127,0,340,,,,,,[]
Can you push / pull between two identical repositories with EGit?,"<p>I've got a repository A that I would like to copy to create B.
Then I'd like to be able to pull A->B, do some merging, then push B->A.</p>

<p>Is this possible with EGit?</p>",1,0,2013-04-12 18:42:16.603000 UTC,,,0,eclipse|git|dvcs|egit,43,2012-05-16 22:22:19.213000 UTC,2016-04-01 22:24:11.060000 UTC,,247,8,0,25,,,,,,[]
Do you know a good program to draw DVCS graphs?,"<p>Recently I was trying to introduce improvements to a DVCS workflow in the company I work for. To make it happen I need to write a document describing the changes - cause it's for managers - the more pictures / graphs the better.</p>

<p>Do you know any program (for Windows preferably) in which it's easy to draw graphs representing branches, commits and merges? I've tried Visio but it's not exactly what I expected (or maybe I just need new stencils).</p>

<p>EDIT: The result I would like to accomplish is similar to this one: <a href=""http://nvie.com/posts/a-successful-git-branching-model/"" rel=""nofollow"">http://nvie.com/posts/a-successful-git-branching-model/</a> The author didn't answer to the questions about the software used in the comment's though. </p>",2,0,2011-01-27 12:06:46.647000 UTC,2.0,2011-01-27 13:09:49.533000 UTC,1,graph|dvcs,186,2009-04-30 17:27:41.523000 UTC,2022-02-08 14:31:04.303000 UTC,Poland,4256,93,1,317,,,,,,[]
Reading azure databricks logs json file using azure databricks,"<p>I have sent the data bricks logs to storage account by enabling diagnostic setting, Now I have to read those logs using azure data bricks for advance analytics. when I try to mount the path it works but reads wont work .</p>
<pre class=""lang-py prettyprint-override""><code>step 1- 

containerName = &quot;insights-logs-jobs&quot;
storageAccountName = &quot;smk&quot;
config = &quot;fs.azure.sas.&quot; + containerName+ &quot;.&quot; + storageAccountName + &quot;.blob.core.windows.net&quot;
sas = &quot;sp=r&amp;st=2021-12-07T08:07:08Z&amp;se=2021-12-07T16:07:08Z&amp;spr=https&amp;sv=2020-08-04&amp;sr=b&amp;sig=3skdlskdlkf5tt3FiR%2FLM%3D&quot;
spark.conf.set(config,sas)

step 2 

df = spark.read.json(&quot;wasbs://insights-logs-jobs.gtoollogging.blob.core.windows.net/resourceId=/SUBSCRIPTIONS/xxxBD-3070-4AFD-A44C-3489956CE077/RESOURCEGROUPS/xxxx-xxx-RG/PROVIDERS/MICROSOFT.DATABRICKS/WORKSPACES/xxx-ADB/y=2021/m=12/d=07/h=00/m=00/*.json&quot;)


Getting below error

 shaded.databricks.org.apache.hadoop.fs.azure.AzureException: Unable to access container $root in account insights-logs-jobs.gtjjjng.blob.core.windows.net using anonymous credentials, and no credentials found for them  in the configuration.
    at shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.connectUsingAnonymousCredentials(AzureNativeFileSystemStore.java:796)
    at shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.createAzureStorage.
</code></pre>
<p>Tried many approach but getting below error.
[![enter image description here][1]][1]</p>",2,2,2021-11-23 11:50:29.570000 UTC,,2021-12-07 10:45:39.137000 UTC,1,azure|logging|pyspark|azure-blob-storage|azure-databricks,122,2020-05-08 07:52:45.147000 UTC,2022-02-02 07:13:06.297000 UTC,,84,5,0,85,,,,,,[]
AWS Lambda access Neptune on the same VPC,"<p>I have the following lambda code to try to access Neptune from the same VPC on AWS. Which most copied from <a href=""https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-python.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-python.html</a>.</p>
<p>But I am getting 403 Forbidden error.</p>
<p>The source code:</p>
<pre><code>from gremlin_python import statics
from gremlin_python.structure.graph import Graph
from gremlin_python.process.graph_traversal import __
from gremlin_python.process.strategies import *
from gremlin_python.process.traversal import T
from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection

wss = 'wss://{}:{}/gremlin'.format(host, port)
remoteConn = DriverRemoteConnection(wss, 'g')
print(&quot;wss--&gt; {}&quot;.format(wss))

# grapch
graph = Graph()
g = graph.traversal().withRemote(remoteConn)

# get
info = g.V().hasLabel('my').outE().inV().path().toList()
print(info)
return info
</code></pre>
<p>Error traceback:</p>
<pre><code>{
  &quot;errorMessage&quot;: &quot;HTTP 403: Forbidden&quot;,
  &quot;errorType&quot;: &quot;HTTPClientError&quot;,
  &quot;stackTrace&quot;: [
    &quot;  File \&quot;/var/task/gremlin_python/process/traversal.py\&quot;, line 58, in toList\n    return list(iter(self))\n&quot;,
    &quot;  File \&quot;/var/task/gremlin_python/process/traversal.py\&quot;, line 48, in __next__\n    self.traversal_strategies.apply_strategies(self)\n&quot;,
    &quot;  File \&quot;/var/task/gremlin_python/process/traversal.py\&quot;, line 573, in apply_strategies\n    traversal_strategy.apply(traversal)\n&quot;,
    &quot;  File \&quot;/var/task/gremlin_python/driver/remote_connection.py\&quot;, line 149, in apply\n    remote_traversal = self.remote_connection.submit(traversal.bytecode)\n&quot;,
    &quot;  File \&quot;/var/task/gremlin_python/driver/driver_remote_connection.py\&quot;, line 56, in submit\n    result_set = self._client.submit(bytecode, request_options=self._extract_request_options(bytecode))\n&quot;,
    &quot;  File \&quot;/var/task/gremlin_python/driver/client.py\&quot;, line 127, in submit\n    return self.submitAsync(message, bindings=bindings, request_options=request_options).result()\n&quot;,
    &quot;  File \&quot;/var/task/gremlin_python/driver/client.py\&quot;, line 148, in submitAsync\n    return conn.write(message)\n&quot;,
    &quot;  File \&quot;/var/task/gremlin_python/driver/connection.py\&quot;, line 55, in write\n    self.connect()\n&quot;,
    &quot;  File \&quot;/var/task/gremlin_python/driver/connection.py\&quot;, line 45, in connect\n    self._transport.connect(self._url, self._headers)\n&quot;,
    &quot;  File \&quot;/var/task/gremlin_python/driver/tornado/transport.py\&quot;, line 40, in connect\n    self._ws = self._loop.run_sync(\n&quot;,
    &quot;  File \&quot;/var/task/tornado/ioloop.py\&quot;, line 576, in run_sync\n    return future_cell[0].result()\n&quot;
  ]
</code></pre>",0,2,2021-07-15 18:26:24.750000 UTC,,,0,python-3.x|amazon-web-services|gremlin|amazon-neptune|gremlinpython,169,2012-02-03 16:20:42.710000 UTC,2022-03-04 16:51:03.243000 UTC,,5628,119,6,508,,,,,,[]
Is it possible to connect to databricks deltalake tables from adf,"<p>I'm looking for a way to be able to connect to Databricks deltalake tables from ADF and other Azure Services(like Data Catalog).  I don't see databricks data store listed in ADF data sources. </p>

<p>On a similar question  - <a href=""https://stackoverflow.com/questions/54185630/is-possible-to-read-an-azure-databricks-table-from-azure-data-factory"">Is possible to read an Azure Databricks table from Azure Data Factory?</a></p>

<p>@simon_dmorias seems to have suggested using ODBC connection to connect to databricks tables.</p>

<p>I tried to set up the ODBC connection but it requires IR to be setup. There are 2 options I see when creating the IR. Self-hosted and linked Self-hosted. I tried to create the Self-hosted IR but it requires installation on my local desktop and probably is more meant for an on-premise odbc connection. I couldn't use the IR on my linked Services.</p>

<p>I have been able to connect powerbi with databricks deltalake tables and plan to use the same creds here. Here is the reference link -</p>

<p><a href=""https://docs.azuredatabricks.net/user-guide/bi/power-bi.html"" rel=""nofollow noreferrer"">https://docs.azuredatabricks.net/user-guide/bi/power-bi.html</a></p>

<p>Any guidance will be helpful</p>",3,0,2019-09-13 05:46:33.390000 UTC,,,1,azure-data-factory|azure-databricks|delta-lake,1961,2019-02-23 19:50:55.663000 UTC,2020-12-19 07:31:00.450000 UTC,,31,5,0,10,,,,,,[]
How can I do CICD of Databricks Notebook in Azure Devops?,"<p>I want to do CICD of my Databricks Notebook.                                                                                                                      Steps I followed.</p>
<ol>
<li>I have integrated my Databricks with Azure Repos.</li>
<li>Created a Build Artifact using YAML script which will hold my Notebook.</li>
<li>Deployed Build Artifact into Databricks workspace in YAML.</li>
</ol>
<p>Now I want to</p>
<ol>
<li>Execute and Schedule the Databricks notebook from the Azure DevOps pipeline itself.</li>
<li>How can setup multiple Environments like Stage, Dev, and Prod using YAML.</li>
<li>My Notebook itself call other notebooks. can I do this?</li>
</ol>
<p>How can I solve this?</p>",1,0,2022-02-14 14:39:57.347000 UTC,1.0,2022-02-14 14:57:08.667000 UTC,1,azure|azure-devops|azure-pipelines|azure-databricks|cicd,35,2019-02-07 08:01:16.953000 UTC,2022-03-05 12:09:35.997000 UTC,"Vijayawada, Andhra Pradesh, India",51,1,0,7,,,,,,[]
How to fetch all rows data from spark dataframe to a file using pyspark in databricks,"<p>I'm trying to fetch all rows data from spark dataframe to a file in databricks. I'm able to write df data to a file with only few counts. Suppose if i'm getting the count in df as 100 , then in file its 50 count so it's skipping the data.How can i load completed data from dataframe to a file without skipping the data. I have created a udf that udf will open the file and append the data to it.I have called that udf in spark sql df.</p>
<p>Can someone help me on this issue?</p>",1,1,2020-09-29 18:20:35.860000 UTC,,,0,python|pyspark|apache-spark-sql|azure-databricks,327,2018-10-04 16:30:46.120000 UTC,2021-02-25 15:56:54.010000 UTC,,113,3,0,83,,,,,,[]
GraphDB account modeling: user access relationship attribute or relationship?,"<p>I am attempting to model account access in a graph DB.</p>

<p>The account can have multiple users and multiple features. A user can have access to many accounts. Each account can give access to only part of the features for each user.</p>

<p>One way I see it is to represent access for each user through relationship attributes, this allows having a shared feature node.</p>

<p>user_1 has access to account_1-feature_1 and account_2-feature-2. user_1 does not have access to account_1-feature_2 even though it is enabled for the account. </p>

<p><a href=""https://i.stack.imgur.com/Qs973.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qs973.png"" alt=""""></a></p>

<p>Another way to model the same access, but without relationship attribute is to create account specific feature nodes. </p>

<p><a href=""https://i.stack.imgur.com/knUHm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/knUHm.png"" alt=""""></a></p>

<p>Question 1: which of these 2 ways is a more 'proper' modeling in the graph DB world?</p>

<p>Now to make things more interesting the account can also have parts which can be accessed by multiple accounts and a certain feature should be able to be scoped down to only be accessible for specific part by user.</p>

<p>In this example user_1 can access account_1 only for part_a feature_1. </p>

<p><a href=""https://i.stack.imgur.com/tTFVR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tTFVR.png"" alt=""enter image description here""></a></p>

<p>To me it seems like defining an attribute on relationship is the way to go for being able to scope down user access by feature &amp; by part of the account. However, reading neo4j powerpoints this would be one of the code smells of relationships having ""Lots of attribute-like properties"". Is there a better way to approach such problem in a graph?</p>",1,1,2019-04-16 05:06:07.413000 UTC,1.0,2019-04-23 08:49:46.923000 UTC,1,graph|neo4j|graph-databases|amazon-neptune,121,2011-06-26 19:28:09.523000 UTC,2019-07-18 01:36:05.230000 UTC,,4649,115,3,196,,,,,,[]
How to define schema & database for a Neptune database using Python / Gremlin?,"<p>I've recently started out building a graph database using AWS Neptune. Is there a way to define schema and create database using <a href=""https://pypi.org/project/gremlinpython/"" rel=""nofollow noreferrer"">gremlin_python</a>?</p>
<p>Any tutorial recommendation would be helpful too.</p>",1,0,2021-06-17 09:09:14.320000 UTC,,,0,python|amazon-web-services|graph|gremlin|amazon-neptune,122,2015-10-20 14:09:42.793000 UTC,2022-03-03 13:22:26.710000 UTC,"New Delhi, Delhi, India",385,25,1,30,,,,,,[]
Install conda package manually Databricks ML runtime,"<p>I have a Databricks ML Runtime cluster. I am trying to install fbprophet library using cluster init_script. I am following the example in the Databricks documentation.</p>
<pre class=""lang-sh prettyprint-override""><code>#!/bin/bash
set -x
. /databricks/conda/etc/profile.d/conda.sh
conda info --envs
conda activate /databricks/python
conda install -y --offline /dbfs/conda_packages/linux-64/fbprophet.tar.bz2
</code></pre>
<p>But the init_script logs show that it cannot activate conda env,  and cannot locate package from given path.</p>
<pre><code>: invalid option
set: usage: set [-abefhkmnptuvxBCHP] [-o option-name] [--] [arg ...]
bash: line 2: /databricks/conda/etc/profile.d/conda.sh
: No such file or directory
usage: conda [-h] [-V] command ...
conda: error: unrecognized arguments: --envs

CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init &lt;SHELL_NAME&gt;

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.



PackagesNotFoundError: The following packages are not available from current channels:

  - /dbfs/conda_packages/linux-64/fbprophet.tar.bz2

Current channels:

  - https://repo.anaconda.com/pkgs/main/linux-64
  - https://repo.anaconda.com/pkgs/main/noarch
  - https://repo.anaconda.com/pkgs/free/linux-64
  - https://repo.anaconda.com/pkgs/free/noarch
  - https://repo.anaconda.com/pkgs/r/linux-64
  - https://repo.anaconda.com/pkgs/r/noarch

To search for alternate channels that may provide the conda package you're
looking for, navigate to

    https://anaconda.org

and use the search bar at the top of the page.
</code></pre>
<p>Can you please guide me what is wrong with the script, and how can I install the conda package from a path on DBFS.</p>
<p>Or is there any other way I can install conda packages. Because if I try to install using UI, or library API, it fails to install fbprophet package.</p>
<p>Regards</p>",0,0,2021-04-08 08:28:32.440000 UTC,,,3,python|bash|conda|azure-databricks|facebook-prophet,240,2013-12-15 09:42:24.757000 UTC,2022-03-05 05:58:58.247000 UTC,,77,11,0,13,,,,,,[]
plot_tree returns empty chart,"<p>Working on python azure databricks, I would like to plot a tree (lightgbm) classifier diagram by using plot_tree. Here is what I did and got:</p>
<p><a href=""https://i.stack.imgur.com/KNA7y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KNA7y.png"" alt=""enter image description here"" /></a></p>
<p>Apparently, there is no error but an empty chart. I also tried using &quot;create_tree_digraph&quot;. Sadly, I can't install graphviz library since after installing by following this <a href=""https://docs.microsoft.com/en-us/azure/databricks/libraries/cluster-libraries#install-workspace-libraries"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/databricks/libraries/cluster-libraries#install-workspace-libraries</a> all the commands cannot be run. So I've decided to use plot_tree rather than create_tree_digraph.</p>",0,2,2021-07-21 06:19:31.623000 UTC,,,1,azure-databricks|lightgbm,29,2021-07-21 06:08:21.537000 UTC,2022-02-22 11:43:16.987000 UTC,,111,0,0,0,,,,,,[]
How to implement DevOps on ADF with Databricks Activity,"<p>I am trying to implement DevOps on ADF and it was successful with pipelines having activities which fetch data from ADLS location and SQL.</p>
<p>But now I have a pipeline in which one of the activity is to run a jar file from dbfs location as shown below.</p>
<p><a href=""https://i.stack.imgur.com/WI46U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WI46U.png"" alt=""enter image description here"" /></a></p>
<p>This pipeline will run a jar file which is in the dbfs location and proceed.</p>
<p>The connection parameters for the cluster is as shown below.
<a href=""https://i.stack.imgur.com/uxbta.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uxbta.png"" alt=""enter image description here"" /></a></p>
<p>While deploying the ARM template from dev ADF to UAT instance, which is having UAT instance of databricks, I was not able to override any of the cluster connection details from <strong>arm_template_parameter.json</strong> file.</p>
<ol>
<li><p>How to configure the workspace URL and clusterID for UAT/PROD environment at the time of ARM deployment? There is no entry for any of the cluster details in the <strong>arm_template_parameter.json</strong> file.</p>
</li>
<li><p>As shown in the first picture, if there is an activity which picks the jar file from DEV instance dbfs loaction, with system generated jar file name, Will it fail when the ARM template for this pipeline is deployed in other environments? If so How to deploy the same jar file with same name in DEV/PROD databricks dbfs location?</p>
</li>
</ol>
<p>Any leads appreciated!</p>",2,0,2020-11-13 09:02:58.217000 UTC,,2020-11-18 06:45:14.617000 UTC,0,azure|azure-data-factory|azure-data-factory-2|azure-databricks|arm-template,364,2017-04-04 05:54:01.927000 UTC,2022-03-05 05:59:02.213000 UTC,,743,45,1,284,,,,,,[]
Can I merge the ADF_Publish branch to a different branch?,"<p>We are designing a branching strategy for an data application which uses ADF and Data Brick .</p>
<p>Question is if one can merge the ADF_publish branch to the release branch and then from the release branch run the deployment . Is it possible or we must use default created ADF_Publish branch .</p>",1,2,2021-09-30 20:46:48.903000 UTC,,,0,azure|azure-devops|azure-data-factory|azure-databricks,105,2018-07-06 13:13:14.910000 UTC,2021-10-18 14:09:56.710000 UTC,,23,0,0,39,,,,,,[]
What are the merits of using the various VCS (Version Control Systems) that exist to track Drupal projects?,"<p>I'm trying to find the best version control strategy for my workflow with Drupal. I have multiple sites per install and I'm pretty sure I'll have a separate repository for each website. As far as I know, the VCSs worth considering are:</p>

<ul>
<li>SVN</li>
<li>Bazaar (bzr)</li>
<li>Git</li>
<li>Mercurial (hg)</li>
</ul>

<p>I know how these compare to each other in general, but want to learn their merits/demerits for Drupal. If you're using (or have used) any of these for Drupal:</p>

<ul>
<li>What is your setup?</li>
<li>What about the VCS you chose works well for managing Drupal projects?</li>
<li>What doesn't?</li>
</ul>",5,1,2010-02-09 12:08:44.123000 UTC,3.0,2010-03-29 04:40:55.967000 UTC,6,version-control|drupal|dvcs,326,2009-12-30 14:00:56.577000 UTC,2022-02-23 21:24:04.657000 UTC,United Kingdom,8646,216,53,467,,,,,,[]
Can git operate without remote server in team environment?,"<p>Git is a DVCS and each local git user holds a complete copy of the repository.</p>

<p>I am trying to set up a repository for our company's project (of small team of less than 10 people). We selected git so that we dont need to set up a server like SVN. We are using git with Visual Studio 2017. It seems that it requires us to set up / push to a remote repository either using VSTS or other remote repository such as GitHub or BitBucket, so that my teammate can clone from it.</p>

<p>I wonder can git work using just local repository copies in a team environment? (With no remote repository such as GitHub needed)</p>

<p>I have set up the repository of the project locally in my machine, how can I have my teammate clone it from me (without needing remote repository such as GitHub)?</p>",3,7,2018-03-22 07:14:23.193000 UTC,,2018-03-22 08:16:31.520000 UTC,0,git|version-control|visual-studio-2017|dvcs,152,2018-02-27 07:15:21.413000 UTC,2018-07-26 06:04:08.610000 UTC,,11,0,0,23,,,,,,[]
Windows to Neptune connection succeeded- How to use it further?,"<p>I have followed this :<a href=""https://stackoverflow.com/questions/52575630"">Connect to Neptune on AWS from local machine</a></p>
<p>Instead of changing this file C:\Windows\System32\drivers\etc\hosts, I changed C:\Users\user_name.ssh\config file as below:</p>
<pre><code>Host 10.100.128.00 
  Hostname 10.100.128.00
  Port 22
  User ec2-user
  IdentityFile ~/.ssh/my-ec2.pem
</code></pre>
<p>then I ran: <code>ssh -L 8182:&lt;neptune endpoint&gt;:8182 &lt;bastion server ip&gt;</code></p>
<p>Now the <strong>status</strong> check worked but how can I use this connection to achieve other things(create vertices/edge) in Neptune? Can someone help with that?</p>",1,0,2022-01-27 08:53:09.950000 UTC,,,0,amazon-web-services|amazon-neptune,17,2018-12-17 18:48:13.643000 UTC,2022-02-18 06:47:43.153000 UTC,,189,18,1,35,,,,,,[]
How to use Multiprocessing pool in Databricks with pandas,"<p>My dataframe has million of lines, I have a function in python 3 pandas that proccess a dataframe :</p>
<pre><code>def OpportunityDetector (df_Opportunity):
    
    df_Opportunity['BillingCity_Det']=df_Opportunity['BillingCity'].apply(ISColFilled)                                                                           
    df_Opportunity['BillingPostalCode_Det']=df_Opportunity['BillingPostalCode'].apply(ISColFilled)                                                                           
    df_Opportunity['BillingStreet_Det']=df_Opportunity['BillingStreet'].apply(ISColFilled)                                                                           
    
    #1
    df_Opportunity['BillingCity_Det_StageName_Det']=df_Opportunity.apply(lambda x: IsBillingValid(x.StageName, x.BillingCity_Det), axis=1)
    #2
    df_Opportunity['BillingPostalCode_Det_StageName_Det']=df_Opportunity.apply(lambda x: IsBillingValid(x.StageName, x.BillingPostalCode_Det), axis=1)
    #3
    df_Opportunity['BillingStreet_StageName_Det']=df_Opportunity.apply(lambda x: IsBillingValid(x.StageName, x.BillingStreet_Det), axis=1)
     
    return df_Opportunity
</code></pre>
<p>How Can I use multiprocessing in order to parallelise my compute and get my result faster  ?</p>
<pre><code>`#somethinks like 

df_new= Parrelelise_Function(My_Pandas_dataframe,OpportunityDetector)
</code></pre>
<p>I have tried this but it is not working:</p>
<pre><code>from multiprocessing import  Pool

def parallelize_dataframe(df, func, n_cores=7): 
    df_split = np.array_split(df, n_cores) # on divise la donnée en n_cores
    pool = Pool(n_cores)  
    df = pd.concat(pool.map(func, df_split),ignore_index=True) 
    pool.close()
    pool.join() 
    return df 


df_new = parallelize_dataframe(df_OpportunityA, OpportunityDetector)
</code></pre>
<p>I have this error :</p>
<pre><code>PicklingError: Can't pickle &lt;function OpportunityDetector at 0x7f7e51db1bf8&gt;: attribute lookup OpportunityDetector on __main__ failed
---------------------------------------------------------------------------
PicklingError                             Traceback (most recent call last)
&lt;command-2041881674588859&gt; in &lt;module&gt;
----&gt; 1 trainpportunityA = parallelize_dataframe(df_OpportunityA, OpportunityDetector)

&lt;command-2041881674588850&gt; in parallelize_dataframe(df, func, n_cores)
      2     df_split = np.array_split(df, n_cores) # on divise la donnée en n_cores
      3     pool = Pool(n_cores) # lancement du traintement
----&gt; 4     df = pd.concat(pool.map(func, df_split),ignore_index=True) #on rassemble toutes les données
      5     pool.close() #fin du traitement
      6     pool.join() # on join les données

/usr/lib/python3.7/multiprocessing/pool.py in map(self, func, iterable, chunksize)
    266         in a list that is returned.
    267         '''
--&gt; 268         return self._map_async(func, iterable, mapstar, chunksize).get()
    269 
    270     def starmap(self, func, iterable, chunksize=None):

/usr/lib/python3.7/multiprocessing/pool.py in get(self, timeout)
    655             return self._value
</code></pre>
<p>Thanks in advance</p>",0,0,2021-01-19 14:56:16.117000 UTC,,2021-01-19 15:54:20.760000 UTC,1,python-3.x|pandas|pyspark|azure-databricks,304,2016-11-23 12:18:08.873000 UTC,2022-02-04 15:41:52.470000 UTC,USA,339,21,0,98,,,,,,[]
Installing SQL Server ODBC library throwing error in Azure databricks runtime 8.3,"<p>I am trying to install ODBC library for SQL Server 2017 to be used to connect to SQL Server using pyodbc Python package. I got this piece of code from StackOverflow only. I am getting error while installing.</p>
<p>This is the script I am running:</p>
<pre><code>%sh
curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -
curl https://packages.microsoft.com/config/ubuntu/16.04/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list
sudo apt-get update
sudo ACCEPT_EULA=Y apt-get -q -y install msodbcsql17
</code></pre>
<p>And I get this error:</p>
<pre><code>% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: apt-key output should not be parsed (stdout is not a terminal)
E: This command can only be used by root.

100   983  100   983    0     0   3640      0 --:--:-- --:--:-- --:--:--  3640
**(23) Failed writing body**
/bin/bash: line 1: /etc/apt/sources.list.d/mssql-release.list: Permission denied
sudo: no tty present and no askpass program specified
sudo: no tty present and no askpass program specified
</code></pre>
<p>I tried another script as below.</p>
<pre><code>%sh
curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -
curl https://packages.microsoft.com/config/ubuntu/16.04/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list 
apt-get update
ACCEPT_EULA=Y apt-get install msodbcsql17
apt-get -y install unixodbc-dev
sudo apt-get install python3-pip -y
pip3 install --upgrade pyodbc
</code></pre>
<p>This one also failing as below.</p>
<pre><code> % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: apt-key output should not be parsed (stdout is not a terminal)
E: This command can only be used by root.

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100   983  100   983    0     0   1790      0 --:--:-- --:--:-- --:--:--  1787
(23) Failed writing body
/bin/bash: line 1: /etc/apt/sources.list.d/mssql-release.list: Permission denied
Reading package lists...
E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)
E: Unable to lock directory /var/lib/apt/lists/
**E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)
E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?
E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)
E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?**
sudo: no tty present and no askpass program specified
Defaulting to user installation because normal site-packages is not writeable
Collecting pyodbc
  Downloading pyodbc-4.0.32.tar.gz (280 kB)
Building wheels for collected packages: pyodbc
  Building wheel for pyodbc (setup.py): started
  Building wheel for pyodbc (setup.py): finished with status 'done'
  Created wheel for pyodbc: filename=pyodbc-4.0.32-cp38-cp38-linux_x86_64.whl size=317312 sha256=baca7539ec5fb88f940fb9ba944609c211a1c84d73d02f659c9deaa0f463b700
  Stored in directory: /home/spark-fcbd5404-644a-45c3-a4b1-9a/.cache/pip/wheels/68/de/de/65a129482924e96fb701c51488b907953acf25fe623bb177b3
Successfully built pyodbc
Installing collected packages: pyodbc
Successfully installed pyodbc-4.0.32
WARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.
You should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.
</code></pre>
<p>Not sure how to install SQL Server ODBC and pyodbc properly. Please help.</p>",1,3,2021-08-19 21:35:28.167000 UTC,,2021-08-20 04:06:39.310000 UTC,2,python|sql-server|pyodbc|azure-databricks,212,2017-11-22 17:17:39.857000 UTC,2022-03-05 03:46:01.603000 UTC,USA,21,0,0,4,,,,,,[]
Can we do type cast in gremlin?,"<p>I have two vertices with Label='Node'.  Both vertices has name and value properties.  One vertex value is numeric and other vertex value is string.</p>
<p>I want to search all vertices with value as &quot;something&quot;.  Since one of the vertices is numeric, it is not allowing me to compare against numeric property.</p>
<p>Is there a way to type cast numeric property value to string?</p>
<p>Here is the example:</p>
<p>Vertex#1:  addV('Node').property('name','col1').property('value',123)</p>
<p>Vertex#2:  addV('Node').property('name','col2').property('value',&quot;ABC&quot;)</p>
<p>I want to find all vertices with Label &quot;Node&quot; where value can be any given string (&quot;123&quot; or &quot;ABC&quot;)</p>",0,3,2020-12-07 06:57:08.093000 UTC,,2020-12-07 14:25:45.793000 UTC,0,gremlin|tinkerpop|amazon-neptune,113,2020-12-01 06:40:33.493000 UTC,2021-10-29 09:35:04.783000 UTC,,3,0,0,5,,,,,,[]
Amazon Neptune IAM Auth Via Websocket,<p>Has anyone successfully connected to an Amazon Neptune cluster with IAM authentication turned on? I am currently using trying to submit gremlin via bytecode which can only be sent via websocket. I have been unsuccessful connecting to the neptune cluster though because all examples of this are using HTTP. Has anyone successfully done this? I could use some guidance.</p>,1,0,2018-10-26 01:08:36.967000 UTC,,2018-10-26 19:54:38.917000 UTC,0,amazon-web-services|gremlin|amazon-neptune,399,2015-04-16 19:03:51.700000 UTC,2022-02-08 18:17:58.507000 UTC,Cincinnati,85,2,0,20,,,,,,[]
Create Azure databricks notebook from storage account,"<p>We have python script stored in Azure storage account in blob. We want to deploy / create this python script (as notebook) in azure databricks cluster so later we can run Azure data factory pipeline and pipeline can execute notebook created/deployed in databricks.</p>
<p>We want to create / deploy this script  only one time as and when its available in blob.</p>
<p>I have tried to search over the web but couldn't find proper solution for this.</p>
<p>Is it possible to deploy/create notebook from storage account? if yes, how?</p>
<p>Thank you.</p>",2,0,2021-05-28 07:37:24.580000 UTC,,,1,azure|azure-data-factory-2|azure-databricks|azure-storage-account,130,2013-04-26 08:31:41.507000 UTC,2022-03-03 10:18:41.033000 UTC,"Hyderabad, Telangana, India",307,14,0,59,,,,,,[]
How to install odbc package to Databricks cluster?,"<p>I need to access an Azure SQL Database from an R notebook in Databricks. To do this I aimed to use the odbc package, which installed fine on my local instance of R.</p>

<p>I have tried to install the package to the cluster using Databricks' interface, which always fails. I have also tried the following code within a notebook:</p>

<pre><code>install.packages(""odbc"")
</code></pre>

<p>which results in:</p>

<pre><code>Installing package into ‘/databricks/spark/R/lib’
(as ‘lib’ is unspecified)
trying URL 'https://cloud.r-project.org/src/contrib/odbc_1.1.6.tar.gz'
Content type 'application/x-gzip' length 288033 bytes (281 KB)
==================================================
downloaded 281 KB

* installing *source* package ‘odbc’ ...
** package ‘odbc’ successfully unpacked and MD5 sums checked
PKG_CFLAGS=
PKG_LIBS=-lodbc
&lt;stdin&gt;:1:17: fatal error: sql.h: No such file or directory
compilation terminated.
------------------------- ANTICONF ERROR ---------------------------
Configuration failed because odbc was not found. Try installing:
 * deb: unixodbc-dev (Debian, Ubuntu, etc)
 * rpm: unixODBC-devel (Fedora, CentOS, RHEL)
 * csw: unixodbc_dev (Solaris)
 * brew: unixodbc (Mac OSX)
To use a custom odbc set INCLUDE_DIR and LIB_DIR manually via:
R CMD INSTALL --configure-vars='INCLUDE_DIR=... LIB_DIR=...'
--------------------------------------------------------------------
ERROR: configuration failed for package ‘odbc’
* removing ‘/databricks/spark/R/lib/odbc’

The downloaded source packages are in
    ‘/tmp/RtmpqHp2QM/downloaded_packages’
</code></pre>

<p>I have also tried installing from github:</p>

<pre><code>library(devtools)
devtools::install_github(""r-dbi/odbc"")
</code></pre>

<p>Which gives a different error:</p>

<pre><code>Downloading GitHub repo r-dbi/odbc@master
Installing 3 packages: assertthat, BH, Rcpp
Installing packages into ‘/databricks/spark/R/lib’
(as ‘lib’ is unspecified)
trying URL 'https://cloud.r-project.org/src/contrib/assertthat_0.2.1.tar.gz'
Content type 'application/x-gzip' length 12742 bytes (12 KB)
==================================================
downloaded 12 KB

trying URL 'https://cloud.r-project.org/src/contrib/BH_1.69.0-1.tar.gz'
Content type 'application/x-gzip' length 12378154 bytes (11.8 MB)
==================================================
downloaded 11.8 MB

trying URL 'https://cloud.r-project.org/src/contrib/Rcpp_1.0.1.tar.gz'
Content type 'application/x-gzip' length 3661123 bytes (3.5 MB)
==================================================
downloaded 3.5 MB

* installing *source* package ‘assertthat’ ...
** package ‘assertthat’ successfully unpacked and MD5 sums checked
** R
** preparing package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded
* DONE (assertthat)
* installing *source* package ‘BH’ ...
** package ‘BH’ successfully unpacked and MD5 sums checked
** inst
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded
* DONE (BH)
* installing *source* package ‘Rcpp’ ...
** package ‘Rcpp’ successfully unpacked and MD5 sums checked
** libs
g++  -I/usr/share/R/include -DNDEBUG -I../inst/include/     -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c Date.cpp -o Date.o
g++  -I/usr/share/R/include -DNDEBUG -I../inst/include/     -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c Module.cpp -o Module.o
g++  -I/usr/share/R/include -DNDEBUG -I../inst/include/     -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c Rcpp_init.cpp -o Rcpp_init.o
g++  -I/usr/share/R/include -DNDEBUG -I../inst/include/     -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c api.cpp -o api.o
g++  -I/usr/share/R/include -DNDEBUG -I../inst/include/     -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c attributes.cpp -o attributes.o
g++  -I/usr/share/R/include -DNDEBUG -I../inst/include/     -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c barrier.cpp -o barrier.o
g++ -shared -L/usr/lib/R/lib -Wl,-Bsymbolic-functions -Wl,-z,relro -o Rcpp.so Date.o Module.o Rcpp_init.o api.o attributes.o barrier.o -L/usr/lib/R/lib -lR
installing to /databricks/spark/R/lib/Rcpp/libs
** R
** inst
** preparing package for lazy loading
** help
*** installing help indices
** building package indices
** installing vignettes
** testing if installed package can be loaded
* DONE (Rcpp)

The downloaded source packages are in
    ‘/tmp/RtmpqHp2QM/downloaded_packages’
Error in processx::run(bin, args = real_cmdargs, stdout_line_callback = real_callback(stdout),  : 
  System command error
In addition: Warning messages:
1: In install.packages(""odbc"") :
  installation of package ‘odbc’ had non-zero exit status
2: In install.packages(""odbc"") :
  installation of package ‘odbc’ had non-zero exit status
</code></pre>

<p>Any idea why this package will not install on Databricks when it works fine locally and when all other packages I have tried install on Databricks work fine using the same syntax?</p>",1,2,2019-04-04 14:04:50.600000 UTC,,,3,r|odbc|azure-databricks,2138,2017-03-09 14:43:18.847000 UTC,2021-10-14 10:10:11.463000 UTC,"Bristol, UK",620,44,1,101,,,,,,[]
Databricks structured streaming with Snowflake as source?,"<p>Is it possible to use a Snowflake table as a source for spark structured streaming in Databricks? When I run the following pyspark code:</p>

<pre><code>    options = dict(sfUrl=our_snowflake_url,
              sfUser=user,
              sfPassword=password,
              sfDatabase=database,
              sfSchema=schema,
              sfWarehouse=warehouse)

    df = spark.readStream.format(""snowflake"") \
              .schema(final_struct) \
              .options(**options) \
              .option(""dbtable"", ""BASIC_DATA_TEST"") \
              .load()
</code></pre>

<p>I get this warning:</p>

<p>java.lang.UnsupportedOperationException: Data source snowflake does not support streamed reading</p>

<p>I haven't been able to find anything in the Spark Structured Streaming Docs that explicitly says Snowflake is supported as a source, but I'd like to make sure I'm not missing anything obvious.</p>

<p>Thanks!</p>",1,2,2020-02-19 22:20:01.627000 UTC,,,0,apache-spark|pyspark|spark-structured-streaming|snowflake-cloud-data-platform|azure-databricks,330,2020-02-19 22:08:18.097000 UTC,2021-01-04 14:38:22.810000 UTC,,1,0,0,1,,,,,,[]
What is the best Mercurial clone / repository strategy?,"<p>There can be:</p>

<p>1) just clone from remote repo as needed  (each new one can take 20 minutes and 500MB)</p>

<p>2) clone 2 local ones from remote repo, both 500MB, total 1GB, so always have 2 local repo to work with</p>

<p>3) clone 1 local one from remote repo, called it 'master', and then don't touch this master, but clone other local ones from this master as needed</p>

<p>I started off using (1), but when there is a quick bug fix, I need to do a clone and it is 20 minutes, so then method (2) is better, because there are 2 independent local repos all the time.</p>

<p>But then sometimes a repo becomes ""weird"" because there are merges that do damages and when it is fixed on the remote repo, any local repo's merge that shows up in <code>hg outgoing</code> will cause damage later when we push again, so we just remove that local repo and clone from remote again to start ""fresh"", taking 20 minutes again.  (Actually, we can use local repo 2 first, rename local repo 1 as repo_old, and then before sleep or before going home, do a clone again)</p>

<p>Is (3) the best option?  Because on a Mac, the master takes 500MB and 20 minutes, but the other local clones are super fast and takes much less than 500MB because it uses hard link on a Mac (how to find out how much disk space without the hard linked content?).  And if using (3), how do we do commits and push?  Suppose we clone from remote repo to local as ""master"", and then clone local ones as ""clone01"", ""clone02"", 03, etc, then do we work inside of clone01, and then when an urgent fix is needed, we go to master, do an <code>hg pull</code>, and <code>hg update</code>, and go to clone02 and also do <code>hg pull</code> and <code>hg update</code>, and fix it on clone02, test it, and <code>hg commit</code>, <code>hg push</code> to the master, and then go to master, and do an <code>hg push</code> there?   And then when clone01's project is done, again go to master, pull, update, go to clone01, pull, update, merge, test, commit, push, go to master, push to remote repo?   That's a lot of steps!</p>",3,0,2010-08-18 23:00:47.917000 UTC,0.0,,4,mercurial|dvcs,1269,2009-05-09 15:50:29.477000 UTC,2022-03-04 09:41:10.460000 UTC,,137341,1445,39,12817,,,,,,[]
data frame writer overwrite the data in all Azure Datalake gen2 storage containers,"<p>In my Datalake storage account there is a one container which is having multiple folders and these folders contains files that which are being generated on a daily basis.</p>
<p>By accident someone writes the dataframe in the container only by below command.</p>
<pre><code>df.coalesce(1).write.option(mode,'overwrite').csv('mnt/container name')
</code></pre>
<p>No folder name was provided in the CSV file path which result in overwrite all the files which were already there in the container by _success,_commited and _vaccum files.</p>
<p>Is there any way I can restore the origiñal files or can undo what have been done.</p>",0,2,2021-11-28 03:15:33.013000 UTC,,,0,pyspark|azure-databricks|azure-data-lake-gen2,42,2021-04-10 16:49:14.333000 UTC,2022-03-03 17:02:30.540000 UTC,,51,0,0,6,,,,,,[]
Azure Databricks - Receive error Zip bomb detected! The file would exceed the max. ratio of compressed file size to the size of the expanded data,"<p>I have been through many a links to solve this problem. However, none have helped me. Primarily because I am facing this error on Azure Databricks.</p>
<p>I am trying to read Excel files located on ADLS Curated zone. There are about 25 of the excel files. My program loops through the excel files and reads them into a PySpark Dataframe. However, after reading about 9 excel files, I receive the below error -</p>
<pre><code>Py4JJavaError: An error occurred while calling o1481.load.
: java.io.IOException: Zip bomb detected! The file would exceed the max. ratio of compressed file size to the size of the expanded data.
This may indicate that the file is used to inflate memory usage and thus could pose a security risk.
You can adjust this limit via ZipSecureFile.setMinInflateRatio() if you need to work with files which exceed this limit.
Uncompressed size: 6111064, Raw/compressed size: 61100, ratio: 0.009998
</code></pre>
<p>I installed the maven - org.apache.poi.openxml4j but when I try to call it using the simple following import statement, I receive the error &quot;No module named 'org'&quot;
import org.apache.poi.openxml4j.util.ZipSecureFile</p>
<p>Any ideas anyone about how to set the ZipSecureFile.setMinInflateRatio() to 0 in Azure Databricks?</p>
<p>Best regards,
Sree</p>",1,1,2020-07-23 11:59:02.023000 UTC,,,0,maven|azure-databricks,461,2012-10-03 09:11:58.107000 UTC,2022-03-04 16:50:42.890000 UTC,"Mumbai, Maharashtra, India",81,0,0,16,,,,,,[]
Forking vs Branching in an enterprise context,"<p>I'm curious to know what DVCS strategy people use in enterprises.</p>

<p>The Github model is based on forks because in open source projects you have some trust issues and your probably don't want anyone to be able to update your code. However, in an enterprise context the trust issue is not as critical and branches might be a more suitable option for DVCS in terms of maintenance (less repos, less permissions to manage).</p>

<p>So, the question is simple: do you replicate Github model and use forks for new features or release with some sort of pull requests mechanism or would you rather work with branches.</p>

<p>This post might help people to get some insights about the strategy they should use if they think about going to DVCS in their own company.</p>",2,0,2011-07-20 23:48:17.963000 UTC,,2012-05-11 20:11:22.447000 UTC,2,branch|dvcs|fork,259,2011-01-05 23:34:08.633000 UTC,2013-07-29 13:49:34.093000 UTC,"Paris, France",46,1,0,14,,,,,,[]
Using git or Mercurial over a slow Internet connection,<p>I am working on a team of about 8 software developers.  The main team is located in a country with a very slow internet connection.  We are currently using SVN hosted on a server located with the main team.  Our repository is about 5gb and cannot be all transferred across our internet connection.  We would like to have developers work remotely and would like to convert to Git or Mercurial.  Once we get a copy of the repository to each remote developer how do we keep everybody in sync and how much bandwidth would be required.</p>,2,2,2013-04-16 20:22:38.043000 UTC,,,0,dvcs,175,2009-02-02 01:54:03.813000 UTC,2022-03-04 19:06:18.657000 UTC,Papua New Guinea,643,124,0,220,,,,,,[]
Databricks notebook workflows - Log Entries from Child notebook not recorded in Azure App,"<p>I have implemented a small Databricks Notebook workflow where a parent notebook invokes another notebook. I  also have a shared notebook, which is included in both notebooks. In this shared notebook I have a function to get a logger instance like this:</p>
<pre><code>def initializeLogging(first_time):

  logger = logging.getLogger(__name__)

  if  not first_time:
    # Set the log level
    logger.setLevel(logging.INFO)

    # TODO: Load the instrumentation key from the Key Vault
    instrumentation_key = '&lt;key here&gt;'

    # Add our Handler for Azure Monitor
    logger.addHandler(AzureLogHandler(connection_string=f'InstrumentationKey={instrumentation_key}'))
    
  return logger
</code></pre>
<p>In the parent notebook I call <code>initializeLogging(True)</code>, and in the child notebook I call it with false (<code>initializeLogging(False)</code>, since I don't want to re-initialize the logger</p>
<p>I can see the Parent log entries is Application Insights, but I can never see the child notebook's log entry, and I am not sure why.</p>
<p>I have tried passing True to my initializeLogging method from the child notebook, with the same result (no App Insights log entries on the child notebook).</p>
<p>Has anybody tried this before?</p>",0,2,2021-06-07 00:06:35.837000 UTC,,,1,python|azure|azure-databricks,33,2017-06-29 23:25:36.990000 UTC,2022-03-04 17:35:31.803000 UTC,,11,0,0,2,,,,,,[]
"Gremlin function doesn't exist in Javascript, but works in console","<p>I'm having trouble transitioning from the console to Javascript. Starting with one Vertex:</p>

<p><code>
g.addV('airport').property('code','AUS').as('aus')
</code></p>

<p>I want to add another airport and then add routes between the two. In the Gremlin console, this works:</p>

<p><code>
g.V().has(""airport"",""code"",""AUS"").as(""aus"").addV(""airport"").property(""code"",""ZZZ"").as(""zzz"").addE(""route"").from(""aus"").to(""zzz"")
</code>
<code>
g.V().has(""airport"", ""code"", ""AUS"").out(""route"").has(""airport"", ""code"", ""ZZZ"").hasNext()
==&gt;true
</code></p>

<p>But, if I try the same process in my Lambda Javascript, the traversal fails:</p>

<p><code>
g.V()
      .has('airport', 'code', 'AUS')
      .as('aus')
      .addV('airport')
      .property('code', 'QQQ')
      .as('qqq')
      .addE('route')
      .from('aus')
      .to('qqq')
      .next();
</code>
<code>
{""errorMessage"":""error updating event: TypeError: g.V(...).has(...).as(...).addV(...).property(...).as(...).addE(...).from is not a function""}
</code></p>

<p>Is there an implementation detail I'm missing between the console and JS? I'm using AWS Neptune and <code>""gremlin"": ""^3.2.9""</code>.</p>",0,2,2018-08-14 22:18:17.720000 UTC,,2018-08-14 23:24:46.157000 UTC,1,gremlin|amazon-neptune,358,2010-08-23 23:03:20.887000 UTC,2022-03-03 17:19:08.790000 UTC,,4656,718,14,204,,,,,,[]
Installing R packages on Azure failed: non-zero exit status,"<p>I just started using RStudio on Azure Databricks. After I select the cluster and click on the Libraries tab I managed to install few extra packages (timetk, tidymodels, rules, doFutures). But when I tried to install the modelitme package I got the following error:</p>
<blockquote>
<p>java.lang.RuntimeException: Installation failed with message:
Error installing R package: Could not install package with error: installation of package ‘V8’ had non-&gt;zero exit status installation of package ‘rstan’ had non-zero exit status installation of package &gt;‘prophet’ had non-zero exit status installation of package ‘modeltime’ had non-zero exit status Full &gt;error log available at /databricks/driver/library-install-logs/r-package-installation-2020-10-&gt;03T10:31:18Z-vcap9s0e.log</p>
<p>Error installing R package: Could not install package with error: installation of package ‘V8’ had non-&gt;zero exit status installation of package ‘rstan’ had non-zero exit status installation of package &gt;‘prophet’ had non-zero exit status installation of package ‘modeltime’ had non-zero exit status</p>
</blockquote>
<p>How do I solve problem like this?</p>",0,2,2020-10-04 07:29:14.867000 UTC,,,1,r|azure|azure-databricks,244,2016-08-26 19:27:45.253000 UTC,2022-03-03 20:57:51.067000 UTC,,255,16,0,64,,,,,,[]
How to write a spark.sql.dataframe into a S3 bucket in databricks?,"<p>I am using databricks and I am reading .csv file from a bucket.</p>

<pre><code>MOUNT_NAME = ""myBucket/""
ALL_FILE_NAMES = [i.name for i in dbutils.fs.ls(""/mnt/%s/"" % MOUNT_NAME)] \
dfAll = spark.read.format('csv').option(""header"", ""true"").schema(schema).load([""/mnt/%s/%s"" % (MOUNT_NAME, FILENAME) for FILENAME in ALL_FILE_NAMES])
</code></pre>

<p>I would like at the same time to write a table there.</p>

<pre><code>myTable.write.format('com.databricks.spark.csv').save('myBucket/')
</code></pre>",1,2,2020-03-26 15:38:21.387000 UTC,,,0,python|amazon-s3|pyspark|azure-databricks,1031,2014-04-30 16:00:53.307000 UTC,2022-03-03 14:37:36.293000 UTC,"Boston, MA",5561,98,7,794,,,,,,[]
"For Mercurial, if I don't commit for 7 days, I can diff all changes made by me, but not if I have ever committed?","<p>I have the habit of diff'ing all the changes I made before pushing to any repo.  But I found that if I never commit for that period of development, such as for 5 days, then I can merge with other people's code, do testing, etc, and I can diff or kdiff3 and see all the changes I made, remove any debug code, fix any small things, and then push to the repo.</p>

<p>However, if I ever committed within this 5-day period, then it seems there is no easy way to ""show all my changes"".</p>

<p>The closest solution I have is:</p>

<pre><code>hg log -u MyUserName -r tip:4322 --style ~/hg-style.txt | sort | uniq | xargs hg vdiff -r 4322
</code></pre>

<p>where <code>~/hg-style.txt</code> is</p>

<pre><code>changeset = ""{files}""
file = ""{file}\n""
</code></pre>

<p>and 4322 is before I start the 5-day development work.  Then the above can diff all the files that was changed by me, but it also includes the changes of my teammates if they made changes to those files, and there can be lots of changes too.</p>

<p>Is there any easy solution?</p>",1,0,2010-10-02 01:58:29.147000 UTC,,2010-10-02 02:18:06.570000 UTC,1,mercurial|dvcs|tortoisehg,119,2009-05-09 15:50:29.477000 UTC,2022-03-04 09:41:10.460000 UTC,,137341,1445,39,12817,,,,,,[]
Snappy compression and decompression in Databricks,"<p>I'm working on a problem where I have to compress a column data (datatype is string). While I can see that the data is getting compressed, decompressing it doesn't seem to display the original data which I compressed. I'm new to compression/decompression, so I'm having a hard time trying to work out the issue.</p>
<pre><code>from snappy import * 
def compress_to_snappy(data):
  return snappy.compress(data,encoding=&quot;utf-8&quot;)
def decompress(data):
  return snappy.uncompress(data)
cSnappy = udf(compress_to_snappy, StringType())
dSnappy = udf(decompress, StringType())

comprsd_df = df.withColumn(&quot;compressed_col&quot;,cSnappy(&quot;metadata_col&quot;))
</code></pre>
<p>Output: <a href=""https://i.stack.imgur.com/aymEH.png"" rel=""nofollow noreferrer"">the list has been converted to string prior the compression</a></p>
<pre><code>decomprsd_df = comprsd_df.withColumn(&quot;decompressed_col&quot;,dSnappy(&quot;compressed_col&quot;))
</code></pre>
<p>Output: <a href=""https://i.stack.imgur.com/fpui8.png"" rel=""nofollow noreferrer"">Decompression not happening?</a></p>",0,2,2021-08-12 07:04:36.457000 UTC,,,0,pyspark|compression|azure-databricks,41,2021-08-12 06:45:37.970000 UTC,2021-12-16 05:35:29.460000 UTC,,43,0,0,3,,,,,,[]
How to achieve more parallelism while reading data from oracle db using spark,"<p>I am running spark on azure data-bricks. My requirement is I need to pull the data from oracle db and push the output it to files. </p>

<pre><code>Spark verison - 2.4
Databricks cluster size - 8 nodes,8 cores for each node.

</code></pre>

<p>So to achieve more parallelism, I used hashing algorithm on partition column in oracle query.</p>

<pre><code>example - mod(ora_hash(id), 64) as partition_key
</code></pre>

<p>Problem is, though I have 64 cores available in Data-bricks cluster, only 8 cores are running to pull the data from oracle.</p>

<p>Please find the attached screen shot for reference.</p>

<p><a href=""https://i.stack.imgur.com/Qa05x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qa05x.png"" alt=""executors tab""></a>
<a href=""https://i.stack.imgur.com/ReBEM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ReBEM.png"" alt=""executor tab with task level""></a></p>

<p>following is the code - </p>

<pre><code>     spark
      .read
      .format(""jdbc"")
      .option(""url"", jdbc_url)
      .option(""dbtable"",crmquery)
      .option(""lowerBound"", 0)
      .option(""upperBound"", 64)
      .option(""partitionColumn"", ""partition_key"")
      .option(""numPartitions"", 64)
      .option(""Driver"",driverClass)
      .option(""user"", user)
      .option(""password"", pswd)
      .option(""fetchsize"",1000)
      .load()
      .write
      .option(""header"", ""true"")
      .option(""escape"","""")
      .option(""inferSchema"", ""false"")
      .csv(path)

</code></pre>

<p>Can someone help me , how to increase more connections to oracle DB while reading it? I can make use cores till 56.</p>

<p>Thanks in advance.</p>",1,0,2019-04-17 07:49:04.373000 UTC,,2019-04-17 07:56:39.957000 UTC,0,apache-spark|ojdbc|azure-databricks,671,2015-10-11 15:43:58.547000 UTC,2020-05-29 10:18:11.063000 UTC,,189,3,0,86,,,,,,[]
Java code executed by Databricks/Spark job - settings for one executor,"<p>In our current system there is one Java code that is reading one file and it will generate many JSON documents for the full day - 24h; all JSON docs are written to CosmosDB. When I execute it in the console everything is OK. I have tried to schedule a Databricks job by using the uber-jar file and it failed with the following error: </p>

<pre><code>""Resource with specified id or name already exists.""
</code></pre>

<p>It seems ok... IMO because the default settings of the existing cluster contain many executors - so each executor will try to write to CosmosDB the same set of JSON docs.
So I changed the main method as below: </p>

<pre><code>public static void main(String[] args) {

 SparkConf conf01 = new SparkConf().set(""spark.executor.instances"",""1"").set(""spark.executor.cores"",""1"");
 SparkSession spark = SparkSession.builder().config(conf01).getOrCreate(); 
 ...
}
</code></pre>

<p>But I received the same error <em>""Resource with specified id or name already exists""</em> from the CosmosDB.
I would like to have only one executor for this specific Java code, how to use only one spark executor?</p>

<p>Any help (link/doc/url/code) will be  appreciated. 
Thank you !</p>",0,0,2020-01-05 06:12:10.013000 UTC,,2020-01-05 22:30:35.193000 UTC,1,apache-spark|azure-databricks,50,2012-11-03 01:35:53.010000 UTC,2022-03-05 02:52:05.803000 UTC,,288,0,0,51,,,,,,[]
How to execute a Gremlin query in Python,"<p>In Python, I am trying to connect using Gremlin and execute a query.</p>
<p>Here I could connect to Neptune DB using gremlin and fetch vertices count using only print statement: print(g.V().has(&quot;system.tenantId&quot;, &quot;hLWmgcH61m0bnaI9KpUj6z&quot;).count().next())</p>
<p>but while reading from a file and storing in a variable and passing in gremlin query is not working</p>
<p>Code</p>
<pre><code>from __future__  import print_function  # Python 2/3 compatibility

from gremlin_python import statics
from gremlin_python.structure.graph import Graph
from gremlin_python.process.graph_traversal import __
from gremlin_python.process.strategies import *
from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection

graph = Graph()

remoteConn = DriverRemoteConnection('wss://&lt;URL&gt;:&lt;port&gt;/gremlin','g')
g = graph.traversal().withRemote(remoteConn)

with open ('Orgid1.txt','r') as file:
#    orgstr = file.readlines()
#    orgstr = [orgstr.rstrip() for line in orgstr]
    for line in file:
        print(g.V().has(&quot;system.tenantId&quot;, line.rstrip()).count().next())

#        print(neptune)




#print(g.V().count())

#print(g.V().has(&quot;system.tenantId&quot;, &quot;xyz123&quot;).count().next())
remoteConn.close()
</code></pre>
<p>Adding more details:</p>
<p>This is the code I am using:</p>
<h2>input:</h2>
<pre><code>with open ('Orgid1.txt','r') as file:
    for line in file:
         print(g.V().has(&quot;system.tenantId&quot;, line.rstrip()).out().count().next())
</code></pre>
<h2>output:</h2>
<pre><code>$ python3 gremlinexample.py
0
</code></pre>
<h2>if I directly print it its working.
input:</h2>
<pre><code>print(line.rstrip())
</code></pre>
<h2>output:</h2>
<pre><code>$ python3 gremlinexample.py
0
xyz123
</code></pre>
<p>ps: xyz123 present in input file named as Orgid1.txt</p>",1,3,2021-11-03 06:46:08.723000 UTC,,2021-11-03 17:14:29.110000 UTC,0,python|gremlin|amazon-neptune,131,2017-08-25 16:52:22.053000 UTC,2022-03-03 14:41:53.160000 UTC,,25,1,0,1,,,,,,[]
Import library not found in databricks notebook,"<p>using Azure Devops pipeline task, I'm importing azure.databricks.cicd.tools library and installing azure-identity and azure-keyvault-secrets. These libraries are installed fine on to a cluster when I add it to a cluster using a bearer token and cluster id however when I run the notebook it says Import module not found. Can you help me where I'm going wrong please?</p>
<pre><code>    - task: AzurePowerShell@5
      inputs:
        azureSubscription: xxxxx
        ScriptType: InlineScript
        Inline: |
          Install-Module -Name azure.databricks.cicd.tools -Force -Scope CurrentUser
        azurePowerShellVersion: LatestVersion   

    - task: AzurePowerShell@5
      inputs:
        azureSubscription: xxxxx
        ScriptType: InlineScript
        Inline: |
          Import-Module -Name azure.databricks.cicd.tools
          Add-DatabricksLibrary -BearerToken $(az-bearer-token) -Region $(az-region) -LibraryType &quot;pypi&quot; -LibrarySettings 'azure-identity' -ClusterId 'xxxxxx'
          Add-DatabricksLibrary -BearerToken $(az-bearer-token) -Region $(az-region) -LibraryType &quot;pypi&quot; -LibrarySettings 'azure.keyvault.secrets' -ClusterId 'xxxxx'
        azurePowerShellVersion: LatestVersion
</code></pre>
<p>followed by .....</p>
<pre><code>   - task: configuredatabricks@0
   - task: DataThirstLtd.databricksDeployScriptsTasks.databricksDeployScriptsTask.databricksDeployScripts@0
   - task: executenotebook@0 
</code></pre>
<p><a href=""https://i.stack.imgur.com/alO3g.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/alO3g.png"" alt=""Library successfully installed on to the cluster"" /></a></p>
<pre><code>Databricks notebook:
from azure.identity import ClientSecretCredential
from azure.keyvault.secrets import SecretClient
credential = ClientSecretCredential(directory_id, sp_client_id, sp_client_secret, 'login.microsoftonline.com')

Error message:

ImportError: No module named azure.identity
</code></pre>",1,0,2021-11-13 10:25:20.427000 UTC,,2021-11-13 14:45:37.583000 UTC,1,azure|azure-pipelines|azure-databricks,101,2021-10-16 10:29:33.423000 UTC,2022-03-02 09:26:27.257000 UTC,,47,1,0,1,,,,,,[]
Databricks Delta Lake Structured Streaming Performance with event hubs and ADLS g2,"<p>I'm currently attempting to process telemetry data which has a volume of around 4TB a day using Delta Lake on Azure Databricks.</p>
<p>I have a dedicated event hub cluster where the events are written to and I am attempting to ingest this eventhub into delta lake with databricks structured streaming. there's a relatively simple job that takes the event hub output and extracts a few columns and then writes with a stream writer to ADLS gen2 storage that is mounted to the DBFS partitioned by date and hour.</p>
<p>Initially on a clean delta table directory the performance keeps up with the event hub writing around 18k records a second but after a few hours this drops to 10k a second and then further till it seems to stabilize around 3k records a second.</p>
<p>tried a few things on the databricks side with different partition schemes and the day hour partitions seemed to perform the best for the longest but still, after a pause and restart in this case the performance dropped and started to lag behind the event hub.</p>
<p>looking for some suggestions as to how I might be able to maintain performance.</p>",1,2,2021-02-10 16:32:32.857000 UTC,,,0,spark-structured-streaming|azure-databricks|azure-eventhub|delta-lake,442,2016-09-22 07:35:04.253000 UTC,2022-03-02 12:01:26.130000 UTC,,53,4,0,7,,,,,,[]
How to read a .gz/.zip compressed file containing pipe seperated . txt file as a pyspark dataframe,"<p>Im trying to read a .gz compressed file as a Pyspark dataframe.</p>
<pre><code>folder_path1 =&quot;dbfs:/mnt/my_path/test_08072021.gz&quot;
dbutils.fs.ls(folder_path1)
df=spark.read.option(&quot;inferSchema&quot;,&quot;true&quot;).option(&quot;header&quot;,&quot;true&quot;).option('inferSchema','true').format(&quot;csv&quot;).load(folder_path1)
</code></pre>
<p>Im not able to read it properly shows garbage text :-
<a href=""https://i.stack.imgur.com/MqxsL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MqxsL.png"" alt=""enter image description here"" /></a>
How do I read a .gz compressed csv file through pyspark correctly?
Im able to do it with read_csv in pandas but owing to huge volume of data want to read in pyspark</p>",0,4,2022-02-14 17:15:32.143000 UTC,,2022-02-15 03:20:38.167000 UTC,-1,apache-spark|pyspark|azure-databricks,30,2020-09-28 09:07:05.687000 UTC,2022-03-05 17:42:38.343000 UTC,"Bangalore, Karnataka, India",332,31,4,118,,,,,,[]
Optimize traversal and look up in gremlin query of connected components,"<p>I'm trying to find all the nodes in a connected component in a graph, which contains around ~130M vertices and ~350M edges.</p>
<p>Following is the query I'm using to find the count of nodes in connected components -</p>
<p>Input - starting vertex id/name</p>
<p>Output - count of nodes in the connected component.</p>
<p>Query - <code>g.v().has(&quot;name&quot;, &quot;driver1&quot;).repeat(where(without (&quot;a&quot;)).store(&quot;a&quot;).both().simplePath().dedup()).emit().hasLabel(&quot;driver&quot;).count().fold()</code></p>
<p>The above query is taking around ~ 52 sec</p>
<p>RepeatStep is taking around ~ 29 sec</p>
<p>Is there a way we can optimize the linear traversal in Repeatstep or lookup in WherePredicateatep?</p>
<p>Profile output of above query-</p>
<pre><code>&quot;dur&quot;: 29008.200345, &quot;counts&quot;: (&quot;traverserCount&quot;: 13809,&quot;elementCount&quot;: 13809}, name: &quot;RepeatStep ([Where PredicateStep (without ([a])), Profilestep, Storestep (a), Profilestep, JanuaGraphVertexStep(BOTH, vertex), ProfileStep, PathFilterstep(simple), Profilestep, RepeatEndstep, Profilestep], until(false), emit(true))&quot;, &quot;annotations&quot;:{ &quot;percentDur&quot;: 52.557919400750215}, &quot;id&quot;: &quot;2.0.0()&quot;, &quot;metrics&quot;: [ { &quot;dur&quot;: 38.137699, &quot;counts&quot;:{ traverserCount: 13810, elementCount: 13810}, &quot;name&quot;:&quot;WherePredicateStep(without([a]))&quot;, &quot;id&quot;: &quot;0.1.0 (2.0.0())&quot; }, { &quot;dur&quot;: 28628.594393, &quot;counts&quot;: { &quot;traverserCount&quot;: 252428, &quot;elementCount&quot;: 252428 }, name: &quot;JanusGraphVertexStep (BOTH, vertex)&quot;, &quot;annotations&quot;
</code></pre>",1,3,2021-07-22 14:54:34.360000 UTC,,,1,gremlin|janusgraph|amazon-neptune|gremlin-server|connected-components,111,2021-04-26 14:22:15.553000 UTC,2022-03-02 14:20:43.727000 UTC,"Bengaluru, Karnataka, India",73,3,0,14,,,,,,[]
PySpark read multiple txt files as json format from a directory,"<p>I am currently doing a task in distributed databases classes for reading txt files of tweet data written as json file format and load the tweets into a dataframe (and later run them through pyspark's kmeans algorith). 
I have the directory however i dont know the names of the specific text file (i am pretty sure all the files in the directory are relevant text files to the task).</p>

<p>In addition because I am supposed to run it through a kmeans algorithm, how do i get the non numeral features to be used in the run?</p>

<p>Any assitance would be appreciated.</p>",1,0,2020-05-31 17:45:48.957000 UTC,,,1,python|json|pyspark|k-means|azure-databricks,71,2018-11-06 08:45:26.337000 UTC,2022-02-27 16:44:55.723000 UTC,,39,0,0,12,,,,,,[]
Azure DevOps pipelines for Azure Databricks,"<p>No idea what all from Azure databricks can be based on Azure DevOps pipeline. We are planning to use github as repository.</p>

<p>Like can Azure databricks be coded in file and then that file i can manage in git repo?</p>

<p>Can we use Azure DevOps CD pipeline for deployment in Azure Databricks?  </p>",1,2,2019-07-15 04:04:31.437000 UTC,1.0,2019-07-15 17:59:48.890000 UTC,6,azure|azure-devops|azure-databricks,914,2018-11-16 20:13:16.843000 UTC,2021-08-23 00:20:08.877000 UTC,,220,35,1,55,,,,,,[]
Displaying samples based on ONLY the first 1000 rows in azure databricks,"<pre><code>dfResult = spark.readStream.format(""delta"").load(PATH)
dfResult.createOrReplaceTempView(""Stream"")
</code></pre>

<p>Trying to read streaming data from the delta table where I had put all data into and visualize them by doing :</p>

<pre><code>%sql
SELECT Time, score
From Stream 
</code></pre>

<p>However, only the first 1000 rows are shown on the graph or table.</p>

<p>Are there any ways to see the last 1000 rows or display the whole data instead of the first 1,000? </p>",2,0,2020-02-21 10:39:06.093000 UTC,,2020-02-24 00:30:39.947000 UTC,0,apache-spark|pyspark|azure-databricks,2306,2018-07-19 16:04:21.447000 UTC,2022-03-06 02:44:51.003000 UTC,,163,20,0,56,,,,,,[]
How can I completely replace a bitbucket repository with another repository?,"<p>I recently had to make a <a href=""https://stackoverflow.com/questions/9560286/how-can-i-make-names-in-a-mercurial-revision-history-consistent"">couple of changes which required rebuilding the repository using hg convert</a>. Unfortunately, this means the bitbucket repo is now inconsistent with the copy I have locally. I don't want to just ""blow away"" the repo as it exists on bitbucket, because that gets rid of all the other customizations / issue tracking etc. that are associated with the project there.</p>

<p>Is it possible to completely wipe the repository from Bitbucket's view of things and push everything from my local (fixed) repo there?</p>",2,0,2012-03-09 02:02:12.007000 UTC,8.0,2017-05-23 10:29:21.620000 UTC,15,mercurial|dvcs|bitbucket,4773,2009-03-25 00:56:36.320000 UTC,2022-02-26 03:52:15.813000 UTC,"Redmond, WA",100291,5170,938,13334,,,,,,[]
How to configure Azure Storage Gen 2 for Azure Databricks,"<p>I'm trying to mount data lake with Databricks. My goal is to build data lake. I wonder why format of my url is different from documentation.What is meaning of filesystem and dfs?</p>
<p>I tried to create data lake with Azure Storage Gen2. Enabled hierarchy and started to create directories.
I noticed that file url includes word &quot;blob&quot;.</p>
<p>This is my url currently:
<a href=""https://datalakestagingtest.blob.core.windows.net/staging/manufacturers/nissan/micra.csv"" rel=""nofollow noreferrer"">https://datalakestagingtest.blob.core.windows.net/staging/manufacturers/nissan/micra.csv</a></p>
<p>I see that format is different in DataLake documentation where url may
be abfss://@.dfs.core.windows.net/</p>
<p>Reference:
<a href=""https://docs.databricks.com/data/data-sources/azure/azure-datalake-gen2.html"" rel=""nofollow noreferrer"">https://docs.databricks.com/data/data-sources/azure/azure-datalake-gen2.html</a></p>",1,3,2020-11-18 14:50:55.460000 UTC,,,0,azure-storage|azure-databricks,1286,2016-11-04 09:17:30.693000 UTC,2022-03-04 08:19:44.720000 UTC,Finland,1157,106,0,333,,,,,,[]
"How can I have two ""streams of development"" (branches?) track each other while remaining different in particular ways?","<p>BRIEF:</p>

<p>I want to have two (or more) ""streams of development"" / environments track each other, sending changes between each other in both directions, without converging totally - while preserving certain key, essential, differences?</p>

<p>DETAIL, ONE PARTICULAR EXAMPLE:</p>

<p>Here is one particular example:</p>

<p>I have been version controlling my home directory, glew-home, for, oh, 28 years.  RCS, SCCS, many RCS wrappers, CVS, SVN, a brief period of experimentation with early DVCS like Monotone and Darcs, bzr, git, and now Mercurial. At the moment I am mainly happy using Mercurial, although I'll jump back to git or bzr as needed.</p>

<p>Now, my home directory is similar, but not identical, on many systems.  The biggest differences are between Cygwin and the various Linuxes at work. I try to make them as smilar as possible, but the differences arise, and often need to persist.</p>

<p>Here is a trivial example of the differences:  on Cygwin, on my personally owned laptop, ~/LOG is a symlink to ~/LOG.dir/LOG.cygwin.glew-home, while at work ~/LOG is a symlink to something like ~/work/LOG.dir/LOG.work.</p>

<p>Reason: anything proprietary needs to stay at work.  ~/work is a separate repository, ~/work/.hg, and is NOT pushed/pulled or otherwise synchronized with my personal computer(s).</p>

<p>Problem:  I want to keep these symlinks (and several other files) different.  But I want to synchronize all other files.  I make changes to my environment in both places.  If I make a change to my ~/.emacs at work I want to send it home, and vice versa.</p>

<p>Q: how can I most conveniently do this?</p>

<p>In the bad old days I would use a common repository, say a common CVS repo.  CVS doesn't handle symlinks, but say that I had a script that generated symlinks from a template stored in CVS.  I would arrange for the symlink template for ~/LOG to have different branches for my cygwin-laptop and for work. I would create workspaces with most files pointing to the same branch of their corresponding RCS/CVS repos, while the files that differed between cygwin-linux and work would have their corresponding branches checked out into their corresponding workspaces.  </p>

<p>This worked, although it was a bit of a pain to maintain.</p>

<p>I have not figured out a good way to do this with a modern DVCS, like Mercurial (or Got, or Bzr).</p>

<p>These modern DVCS tools do whole-repo branching, not per-file branching.  They do not understand the notion of two branches that are identical for most files, but which differ in only certain files.</p>

<p>When I try to track two branches, I always end up with the essential differences being propagated.</p>

<p>Some have suggested Makefiles.  Not attractive.</p>

<p>I have considered making the essential changes base revs, and constantly rebasing.  But I don't like rebasing that much.</p>

<p>Better ideas appreciated. </p>

<p>Quilt?</p>",1,1,2012-08-03 05:39:23.143000 UTC,1.0,,6,git|mercurial|dvcs|bazaar,627,2011-11-17 05:59:52.473000 UTC,2022-03-06 03:58:37.633000 UTC,"Portland, OR",6708,493,39,2234,,,,,,[]
Find largest connected components AWS Neptune,"<p>In an AWS Neptune graph with billions of nodes and edges, how would one go about finding the largest connected components efficiently? The reason I am trying to find the answer to this question is because usually large connected components in my domain indicate fraud. Most nodes in my graph only are connected to like tens of other nodes. It is suspicious when nodes are connected to hundreds or thousands of other nodes.</p>
<p>I have several questions:</p>
<ol>
<li>Is AWS Neptune an appropriate database for finding large connected components in a graph with billions of nodes and edges?</li>
<li>Would it be more efficient to calculate PageRank for the graph? A high PageRank would similarly indicate fraud I believe. If so, how would I go about calculating PageRank?</li>
<li>What architecture and algorithm could find the largest connected components?</li>
<li>I am not just trying to find fraud that happened in the past but I am also trying to identify fraud in real time. As data is ingested, what would be a good way to identify a fraudulent node in real time? I am thinking that Neptune Streams and doing DFS on the node to get the entire connected component would be appropriate here.</li>
<li>Eventually, years from now, when I've identified enough fraud, I am thinking I could do some sort of supervised machine learning. Not sure what the benefit of this would be though since most large connected components are fraud. It might be better at identifying harder to distinguish cases?</li>
<li>Similarly to connected components and PageRank, are there other graph attributes I should look into that might indicate fraud in my case? I know this might be difficult to answer since I haven't revealed my domain.</li>
</ol>
<p>Any help is much appreciated!</p>",1,3,2022-01-16 23:41:34.013000 UTC,,,1,graph-theory|gremlin|tinkerpop|amazon-neptune,136,2015-03-21 20:11:45.577000 UTC,2022-02-28 22:34:55.397000 UTC,,47,20,0,14,,,,,,[]
Writing from pandas dataframe to DataBricks database table,"<p>I have a database table in Azure DataBricks that already has data in it - I need to append data to that table.</p>
<p>I have my pandas dataframe (df_allfeatures) that I want to append to my database</p>
<p>The function that I use to write to my database table:</p>
<pre><code>def write_to_delta(df_name, db_name, table_name, write_mode, num_part=10):
    df_name \
        .repartition(num_part) \
        .write \
        .mode(write_mode) \
        .insertInto(&quot;{}.{}&quot;.format(db_name, table_name), overwrite=True)
</code></pre>
<p>When using this function to write into by database:</p>
<pre><code>df_allfeatures = spark.createDataFrame(df_allfeatures)
write_to_delta(df_allfeatures, 'production', 'feed_to_output_all_features', 'append', num_part=10)
</code></pre>
<p>However I keep getting the error &quot;
AnalysisException: Cannot write incompatible data to table 'production.feed_to_output_all_features': &quot;</p>
<p>The columns that are singled out are the following:</p>
<p>&quot;AnalysisException: Cannot write incompatible data to table 'production.feed_to_output_all_features':</p>
<ul>
<li>Cannot safely cast 'LEAD_CONCENTRATE_GRADES_PB': string to double</li>
<li>Cannot safely cast 'TAILINGS_RECOVERIES_PB': timestamp to double</li>
<li>Cannot safely cast 'DATE': double to timestamp&quot;</li>
</ul>
<p>I have already changed the datatypes to rectify this error:</p>
<pre><code>df_allfeatures = df_allfeatures.astype({&quot;LEAD_CONCENTRATE_GRADES_PB&quot;: 'float32'}) 
df_allfeatures = df_allfeatures.astype({&quot;TAILINGS_RECOVERIES_PB&quot;: 'float32'})
df_allfeatures['DATE'] = pd.to_datetime(df_allfeatures['DATE'])
</code></pre>
<p>But I keep getting the same error</p>",1,0,2021-09-15 07:53:26.387000 UTC,,,1,python|pandas|azure-databricks,141,2021-07-08 14:31:33.563000 UTC,2021-11-29 12:02:48.537000 UTC,,11,0,0,0,,,,,,[]
Is there a way to combine text from an SQL table with a case statement,"<p>I have a list of validation rules in an SQL Server table e.g. <code>LENGTH(@input) &gt;= 50, LENGTH(@input) &lt;= 20</code>
(I have changed LEN to LENGTH so it is compatible with Spark SQL)</p>
<p>I am trying to combine in Databricks, a case statement (<code>CASE WHEN &lt;rule&gt; THEN 1 ELSE 0 END</code>) with the rule (e.g. <code>LENGTH(@input) &gt;= 50</code>)</p>
<pre><code>dfh = df_rules2.withColumn(&quot;Validation2&quot;,expr(&quot;concat('CASE WHEN ', df_rules.RuleName, ' THEN 1 ELSE 0 END')&quot;)) 
</code></pre>
<p>It is not accepting the code because it is expecting a boolean expression and I can't seem to combine the validation rule from the SQL Server table with the case statement.</p>",0,0,2021-05-24 15:53:06.427000 UTC,,2021-05-24 16:08:04.087000 UTC,0,python|pyspark|azure-databricks,28,2021-05-24 15:38:28.263000 UTC,2022-02-07 11:26:22.280000 UTC,,1,0,0,1,,,,,,[]
How to handle mobile version on github,"<p>So I want to develop a mobile version of a software I created recently and which is hosted on github. This version should be a own project and isn't supposed to be merged with the original one. I don't have major experience with version controlling software and this I am a bit confused how to proceed with this.</p>

<p>Should I create a new branch for it, fork my own repo (as shown <a href=""http://bitdrift.com/post/4534738938/fork-your-own-project-on-github"" rel=""nofollow"">here</a>) or are there other possibilities?</p>",1,3,2014-02-19 13:19:40.987000 UTC,,,0,git|version-control|github|dvcs,53,2012-08-20 22:24:18.320000 UTC,2021-02-27 17:36:52.767000 UTC,,1289,19,0,104,,,,,,[]
git how branching works inside,"<p>I'm using github and interesting, how git branching works inside. Is it copy some data when creating new branch or not. How and where it store the difference of the branches and how it allow to switching between them fast. What's happened with my files on disk when I'm changing the brunch?</p>",3,0,2013-02-01 14:08:53.647000 UTC,,2013-02-01 17:51:10.610000 UTC,0,git|branch|dvcs,86,2011-12-05 09:23:12.863000 UTC,2022-02-09 14:17:21.980000 UTC,"Belarus, Minsk",9735,884,16,758,,,,,,[]
Proxy git commands to only one clone,"<p>I've got a couple of processes which use the same git repositories, but they insist on treating them as separate things. That leads to me having a number of clones of the same repository under different local locations.</p>

<p>Is there any existing wrapper for git that would ensure I have only one repo with hard links? What I'd expect it to do is:</p>

<ul>
<li>On <code>clone</code>, check in <code>$GIT_CLONES_PATH</code> if a given url has been checked out already. If it's not there, clone to that location, then hardlink-clone to requested path.</li>
<li>On <code>pull</code>, first pull in <code>$GIT_CLONES_PATH</code>, then pull locally.</li>
<li>On <code>push</code>, push to the remote of <code>$GIT_CLONES_PATH</code> (to prevent conflicts after local push), then pull in the common location <code>$GIT_CLONES_PATH</code>.</li>
</ul>

<p>I guess there are hundreds of edge cases I didn't think about here, so: does anyone know some existing project that does this?</p>",1,0,2012-10-29 18:15:48.363000 UTC,,,0,git|dvcs,32,2008-10-27 00:19:37.773000 UTC,2022-02-27 04:40:09.233000 UTC,United Kingdom,32024,1325,330,2727,,,,,,[]
Gremlin Python: Count vertices and its children in one query,"<p>I am trying to get the count of all vertices with a particular label and all its children in one query like so:</p>

<pre><code>g.V().hasLabel('folder').has('folder_name', 'root').as_('a', 'b').select('a', 'b').by(__.count()).by(__.in_('items_belongs_to_folder').count()).toList()
</code></pre>

<p>This should ideally return [{a: 200, b: 400}], but I am instead getting a list like so:</p>

<pre><code>[{a: 1, b: 0},{a: 1, b: 0},{a: 1, b: 0},{a: 1, b: 0},{a: 1, b: 0},{a: 1, b: 0},{a: 1, b: 0},....{a: 1, b: 0}]
</code></pre>

<p>How exactly can I achieve the desired result?</p>

<p>gremlinpython: 3.4.4 (latest)</p>

<p>Python 3.7</p>

<p>graph database: AWS Neptune</p>",1,0,2019-11-22 06:13:57.180000 UTC,,,4,gremlin|tinkerpop|amazon-neptune|gremlinpython,506,2014-10-15 13:58:41.620000 UTC,2022-02-10 10:50:33.530000 UTC,"Bengaluru, Karnataka, India",359,5,0,12,,,,,,[]
Can we work with external APIs in Azure Databricks?,"<p>Being newbie to Databricks just exploring ways to access third party APIs in Databricks.</p>

<pre><code>   Example : 
      One of the sceanario is checking whether json file which is being 
      processed via Databricks whether its in correct Json format or not?
      We have one API which validate this format, question is can we consume
      this API in Databricks notebook ?
</code></pre>",1,0,2019-02-26 05:27:19.380000 UTC,1.0,,2,api|azure-databricks,799,2015-07-29 05:11:03.417000 UTC,2022-02-04 13:12:20.040000 UTC,,301,2,0,64,,,,,,[]
Turtle files data processing from s3 to Neptune,"<p>I have copied turtle files from s3 to Neptune through <code>curl post</code> method commands as suggested in the AWS document. <code>curl</code> command executed and job is successfully completed. Post job execution, I have checked the job id status to know the details of load status; my job showing overall status completed but record count is zero records. My turtle files in S3 have lots of records. Why did my <code>curl post</code> method not transfer a single record from S3 to Neptune? Please someone help if you have successfully moved turtle files from S3 to Neptune.</p>",1,3,2018-08-16 07:53:22.803000 UTC,1.0,2018-08-17 01:43:25.553000 UTC,0,sparql|amazon-neptune,199,2017-07-06 04:54:35.257000 UTC,2018-08-23 09:11:46.100000 UTC,"Hyderabad, Telangana, India",29,0,0,30,,,,,,[]
How to deal with Mercurial when disk corruption/restore has returned the master-repo to an old state,"<p>I wonder if anyone can give me advice on how to cope with the aftermath of one of our disks being corrupted, and the restored to an old state. Here's the story:</p>

<p>I have some code I manage with Mercurial. There is a ""master"" repository on disk A, and
a branch/clone on disk B. The timeline looks like this</p>

<ol>
<li>Start with master repository. There are various extant branches/clones for
prototyping new features.</li>
<li>clone master repo on disk A--> another new branch on disk B</li>
<li>commit changes to master repo, and push to new branch</li>
<li>disk A is corrupted</li>
<li>disk A is restored to state at timepoint 1.</li>
<li>Disk B is unaffected</li>
</ol>

<p>What should I do?</p>

<p>Option1:
 - Very little has happened on the branch since the actual branching. So just blow away
   the master repo, and start using the new branch as my new master. If I do this, won't
   I have problems merging my old clones (mentioned at timepoint 0) back in?</p>

<p>Option2:
 - Just manually make the changes in my master that have been lost my diffing with the new branch. Even if I do this, how am I going to carry on pushing to the new branch?</p>

<p>Option3:
 - Just manually make the changes in my master that have been lost my diffing with the new branch. Then delete the new branch, and clone a new one.</p>

<p>Any advice welcome
cheers
Zam</p>",1,2,2012-02-16 10:21:14.850000 UTC,,2012-03-30 12:13:03.903000 UTC,3,version-control|mercurial|dvcs|restore|corruption,97,2012-02-16 10:07:09.873000 UTC,2017-03-02 21:51:26.417000 UTC,,88,27,0,10,,,,,,[]
writing appending text file from databricks to azure adls gen1,"<p>I want to write kind of a log file back to azure adls gen1
I can write (not append) using</p>
<pre><code>dbutils.fs.put(filename,&quot;random text&quot;)
</code></pre>
<p>but i cant append it using</p>
<pre><code>with open(&quot;/dbfs/mnt/filename.txt&quot;,&quot;a&quot;):
f.write(&quot;random text&quot;)
</code></pre>
<p>it give me error</p>
<pre><code>1 with  open(&quot;/dbfs/mnt/filename.txt&quot;, &quot;a&quot;) as f:
----&gt; 2   f.write(&quot;append values&quot;)

OSError: [Errno 95] Operation not supported
</code></pre>
<p>alternatively, i tried using logger.basicconfig(logging.basicConfig(filename='dbfs:/mnt/filename.txt', filemode='w')</p>
<p>but looks like its not writing into the path.
can anyone help please</p>",1,0,2020-10-29 08:28:17.087000 UTC,,2020-11-02 07:01:24.903000 UTC,1,python-3.x|file|append|azure-data-lake|azure-databricks,1152,2018-03-14 11:04:47.907000 UTC,2022-03-03 08:42:27.243000 UTC,,43,18,0,4,,,,,,[]
How does Git achieve storage efficiency when storing multiple versions of a file?,<p>Git stores each version of a file in a new object. This data model is not very storage efficient when storing multiple versions of a file. How does Git achieve storage efficiency anyway?</p>,1,0,2017-06-21 07:16:50.407000 UTC,,,1,git|github|dvcs,555,2017-05-28 09:26:33.643000 UTC,2019-12-03 22:03:07.507000 UTC,"Sydney, New South Wales, Australia",11,0,0,1,,,,,,[]
DVCS Choices - What's good for Windows?,"<p>So I want to get a project on a distributed version control system, such as mercurial, git, or bazaar.  The catch is that I need the Windows support to be good, i.e. no instructions that start off with ""install cygwin..."".  Now I've <em>heard</em> that git's Windows support is decent these days, but don't have any first hand experience.  Also, it sounds like the bazaar team has an explicit goal of making it as multiplatform as possible.
Can I get any recommendations?</p>",8,1,2008-08-07 12:57:32.377000 UTC,3.0,2010-08-16 20:52:57.217000 UTC,20,version-control|dvcs,3102,2008-08-07 12:15:58.840000 UTC,2010-03-08 16:41:04.410000 UTC,"Portland, OR",1001,44,1,97,,,,,,[]
Posting Dynamic values as a request body,"<p>I am fetching the ID from some other API hitting the endpoint of that API.
I am fetching Dynamic ID from that API.
Now, In this code I which have posted I should send a request body that should accept dynamic ID that I am fetching from other API which I am storing in <strong>str</strong> and Iterate till the length of the ID and get JSON response for each ID.</p>

<pre><code>   for(int i=0;i&lt;array.length();i++) 
   {         
   str.add(array.getJSONObject(i).getString(""id""));
   }
   for(String s:str)
   {
    HttpPost request1 = new HttpPost(""/2.0/clusters/events"");
    //StringEntity params=new StringEntity(array[i]);

    ***StringEntity params =new StringEntity(""{\""id\"":\""+s+\""}"");*** 
    //main concern to send dynamic ID as a body  

    request1.addHeader(""Authorization"",bearerToken);
    request1.addHeader(""content-type"", ""application/json"");          
    request1.addHeader(""Accept"",""application/json"");
    request1.setEntity(params);
    System.out.println(params);
    HttpResponse response1 = httpClient.execute(request1);
    System.out.println(""Response Code:"" + 
    response1.getStatusLine().getStatusCode());
    String json1 = EntityUtils.toString(response1.getEntity());
    System.out.println(json1);
    JSONObject event = new JSONObject(json1);
    JSONArray arrays=event.getJSONArray(""events"");
    for (int k=0;k&lt; arrays.length();k++){
    JSONObject ids = arrays.getJSONObject(k);
    System.out.println(ids.get(""id"").toString()); 
    System.out.println(ids.get(""timestamp"").toString()); 
    System.out.println(ids.get(""type"").toString()); 
    System.out.println(ids.get(""details"").toString()); 
    }
    }
</code></pre>

<p><strong>The main question is how to send an value dynamically as a request body using String Entity or any other using JAVA.</strong>
I should fetch the ID and Its repose for each ID till its length.</p>",3,1,2019-09-05 08:50:49.263000 UTC,,,0,java|http-post|azure-functions|azure-databricks,1617,2018-04-23 09:39:30.760000 UTC,2020-04-21 09:20:51.433000 UTC,,41,0,0,51,,,,,,[]
Broadcast Timeout on Azure Databricks Delta Delete,"<p>Hi I am trying to delete records from a delta table. It is causing a broadcast timeout error from time to time.
Can someone please help with this</p>
<pre><code>spark.sql(s&quot;&quot;&quot;DELETE FROM stg.bl  WHERE concat(key,':',revision) in 
   (Select distinct concat(bl.key,':',bl.revision) from stg.bl bl left semi join
    tgt.bl tgt ON bl.key = tgt.key and bl.revision = tgt.revision)&quot;&quot;&quot;)
</code></pre>
<pre><code>org.apache.spark.SparkException: Could not execute broadcast in 300 secs. 
You can increase the timeout for broadcasts via spark.sql.broadcastTimeout or 
disable broadcast join 
by setting spark.sql.autoBroadcastJoinThreshold to -1
</code></pre>
<p><a href=""https://i.stack.imgur.com/dVwfe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dVwfe.png"" alt=""Error"" /></a></p>",0,5,2021-06-01 16:54:14.140000 UTC,,2021-06-01 17:49:50.380000 UTC,0,apache-spark|azure-databricks|delta-lake,161,2013-03-12 04:29:36.407000 UTC,2022-03-06 01:52:55.413000 UTC,,1339,366,25,444,,,,,,[]
Spark query in R,"<p>I'm translating a spark query from Python to R in databricks.What is the equivalent of below code in R.</p>
<pre><code>categories= spark.sql(&quot;select name,place from table where XYZ &quot;)
</code></pre>
<p>I tried to replicate the above code as following
categories &lt;- sql(select name,place from table where XYZ)</p>
<p>but getting syntax error.</p>",2,1,2021-03-23 18:11:11.130000 UTC,,,0,azure-databricks|sparkr,76,2020-04-23 16:11:33.367000 UTC,2021-12-16 18:24:22.537000 UTC,,57,15,0,14,,,,,,[]
Multi Processing in Azure Data Bricks - Python,"<p>I need to run multiple notebooks in Azure Data Bricks parallelly, but getting error when trying to run below code:</p>
<pre><code>from concurrent.futures import ProcessPoolExecutor

with ProcessPoolExecutor() as executor:
  p1 = executor.submit(dbutils.notebook.run,'/Notebook1',0)
  p2 = executor.submit(dbutils.notebook.run,'/Notebook2',0)
  
print(p1.result(),p2.result())
</code></pre>
<p>Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.</p>
<p>However, I am able to successfully implement using multithreading:</p>
<pre><code>with ThreadPoolExecutor() as executor:
  p1 = executor.submit(dbutils.notebook.run,'/Notebook1',0)
  p2 = executor.submit(dbutils.notebook.run,'/Notebook2',0)
  
print(p1.result(),p2.result())
</code></pre>
<p>But need to leevrage multiprocessing, any guidance is appreciated.</p>
<p>Thanks
Kaushal</p>",0,6,2021-08-07 15:16:50.800000 UTC,,,1,python|multithreading|multiprocessing|azure-databricks,374,2020-11-17 12:22:53.260000 UTC,2022-03-04 12:33:00.743000 UTC,,13,0,0,4,,,,,,[]
Is there a way to pass parameters to a job dynamically in Azure Databricks?,"<p>I'm looking for a solution to trigger an azure databricks job, keep the job running all the time and then passing parameters to functions dynamically.Is there a way to achieve the same?</p>",1,0,2019-05-15 08:24:26.587000 UTC,,,1,python|scheduler|jobs|azure-databricks,1111,2016-01-27 15:07:06.643000 UTC,2020-11-09 18:39:49.130000 UTC,,309,8,0,25,,,,,,[]
Pyspark - name 'when' is not defined,"<p>Could someone please help me with the below.</p>
<pre><code>joinDf = join_df2(df_tgt_device_dim.withColumn(&quot;hashvalue&quot;, F.sha2(F.concat_ws(&quot;,&quot;, *valColumns), 256)).alias(&quot;target&quot;),
                      df_final.withColumn(&quot;hashvalue&quot;, F.sha2(F.concat_ws(&quot;,&quot;, *valColumns), 256)).alias(&quot;source&quot;),
                      conditions,
                      &quot;full_outer&quot;,
                      keyColumns)

deltaDf = get_active_records(joinDf, common_cols, &quot;Type2&quot;)

wind_spc = Window.partitionBy(*keyColumns).orderBy(col(&quot;effective_start_ts&quot;).desc())

df_device_new = deltaDf.withColumn(&quot;Rank&quot;, F.row_number().over(wind_spc))
deltaDf_final = df_device_new .filter( col(&quot;diff&quot;) != 'unchanged_act_records').withColumn(&quot;crnt_ind&quot;,when(df_device_new .Rank == 1 ,lit('Y'))\
                                                             .when(df_device_new.Rank != 1 ,lit('N'))).drop(&quot;Rank&quot;)

deltaDf_final.union(deltaDf.filter(col(&quot;diff&quot;) == 'unchanged_act_records').withColumn(&quot;crnt_ind&quot;,lit('N'))).createOrReplaceTempView(f&quot;device_delta&quot;)
</code></pre>
<p>Below is the error.</p>
<pre><code>NameError: name 'when' is not defined
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
&lt;command-104590&gt; in &lt;module&gt;
     17 wind_spc = Window.partitionBy(*keyColumns).orderBy(col(&quot;effective_start_ts&quot;).desc())
     18 df_device_new = deltaDf.withColumn(&quot;Rank&quot;, F.row_number().over(wind_spc))
---&gt; 19 deltaDf_final = df_device_new .filter( col(&quot;diff&quot;) != 'unchanged_act_records').withColumn(&quot;crnt_ind&quot;,when(df_device_new .Rank == 1 ,lit('Y'))\
                                                             .when(df_device_new.Rank != 1 
     21 deltaDf_final.union(deltaDf.filter(col(&quot;diff&quot;) == 'unchanged_act_records').withColumn(&quot;crnt_ind&quot;,lit('N'))).createOrReplaceTempView(f&quot;device_delta&quot;)

NameError: name 'when' is not defined
</code></pre>
<p>I have tried F.when, but it did not work.</p>
<p>Could someone please assist thank you.</p>",0,4,2021-09-29 21:19:13.930000 UTC,,,0,python|pyspark|azure-databricks,473,2021-04-18 14:14:23.533000 UTC,2022-03-01 19:02:10.977000 UTC,,101,10,0,49,,,,,,[]
I need to have an azure_databricks job fail on an exception and it's not doing that,"<p>I've written a ""CriticalException"" handler to handle cases where my notebook has created unusable data or some other error has happen below. My intention is to have the notebook's execution halt with a ""failed"" status but it's not doing that. </p>

<p><strong>What am I missing in my routine?</strong> </p>

<p>This method is defined in a ""helper"" notebook I ""run"" from all my notebooks. This method gets called but the notebook continues to execute and finishes with a success status instead of halting with a failed status.</p>

<pre><code>def CriticalException(err_msg):
  sys.stderr.write(err_msg) 
  raise RuntimeError(err_msg)
  return None
</code></pre>",1,0,2019-07-26 18:29:10.750000 UTC,,,0,azure-databricks,11,2016-09-30 22:52:37.447000 UTC,2021-05-04 23:36:42.850000 UTC,"Oregon, United States",57,20,0,28,,,,,,[]
How to install a python wheel file with optional/extra requirements?,"<p>Specifically I want to install a wheel file that has extras_require defined.<br />
I can install the package directly with <code>pip install mypackage[myextradependency]</code>.</p>
<p>But if I create a wheel file out of it, it does not allow me to do <code>pip install mypackage-1.0-py2.py3-none-any.whl[myextradependency]</code> and gives me a url syntax error. It seems like I am using the wrong syntax, but I was unable to find the correct syntax on the <a href=""https://pip.pypa.io/en/latest/user_guide/#installing-from-wheels"" rel=""nofollow noreferrer"">documentation</a>.</p>
<p>I am trying to install the wheel on a databricks job cluster started via Azure data factory. Therefor I do not have access to additional pip commands.</p>
<p>The exact error message is
<code>Could not parse URI. Please double check your input.</code></p>
<p>Thanks for your help</p>",3,1,2021-01-26 18:44:05.730000 UTC,,2021-01-27 15:12:00.820000 UTC,0,python|pip|azure-data-factory|azure-databricks|python-wheel,694,2013-03-18 14:32:08.630000 UTC,2022-03-02 20:30:27.630000 UTC,,468,91,1,54,,,,,,[]
How to Override parameters for CI/CD pipeline for azure daafactory For Databricks (Workspace Url and Cluster ID),"<p>I am doing CI/CD integration for one data factory to another I am Successfully able to Create the release and abe to copy from my Dev to UAT environment I am able to copy my pipelines Triggers and the Link Service
The problem I am facing In just copying Databricks Links Service</p>
<p>As we know we have to override the parameters of our environment, In Databricks Option comes only to override for an Access token.
And Databricks require three parameters workspace URL and ClusterID, As there is no option to override these two. My workspace URL and Cluster ID is in the production environment is copied of MY Dev environment. Although the Token is copied Successfully which results in the Unsuccessful Connection due to workspace URL NA clusterId.</p>
<p><a href=""https://i.stack.imgur.com/TM8Lk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TM8Lk.png"" alt=""enter image description here"" /></a></p>
<p>As you see inside of pic for data bricks three params are required
and Override Template parameter is giving for an only access token</p>",1,1,2020-10-13 10:37:00.100000 UTC,,2020-10-17 18:04:44.173000 UTC,0,azure|azure-data-factory|azure-databricks,361,2017-06-16 10:44:32.290000 UTC,2021-08-09 10:48:58.890000 UTC,"Indore, Madhya Pradesh, India",31,5,0,39,,,,,,[]
The spark driver has stopped unexpectedly and is restarting. Your notebook will be automatically reattached,"<p>I try to analyze a dataset of 500Mb in Databricks. These data are stored in Excel file. The first thing that I did was to install Spark Excel package <code>com.crealytics.spark.excel</code> from Maven (last version - 0.11.1).</p>

<p>These are the parameters of the cluster:</p>

<p><a href=""https://i.stack.imgur.com/0YZIa.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/0YZIa.png"" alt=""""></a></p>

<p>Then I executed the following code in Scala notebook:</p>

<pre><code>val df_spc = spark.read
          .format(""com.crealytics.spark.excel"")
          .option(""useHeader"", ""true"")
          .load(""dbfs:/FileStore/tables/test.xlsx"")
</code></pre>

<p>But I got error about the Java heap size and then I get another error ""java.io.IOException: GC overhead limit exceeded"". Then I executed this code again and got another error after 5 minutes running:</p>

<blockquote>
  <p>The spark driver has stopped unexpectedly and is restarting. Your
  notebook will be automatically reattached.</p>
</blockquote>

<p>I do not understand why it happens. In fact the data set is quite small for the distributed computing and the cluster size should be ok to process these data. What should I check to solve it?</p>",1,1,2019-06-16 12:39:14.033000 UTC,,2019-06-16 15:22:54.837000 UTC,5,excel|scala|apache-spark|azure-databricks,6338,2019-06-09 19:46:19.880000 UTC,2022-03-04 13:50:45.330000 UTC,,2243,194,7,327,,,,,,[]
"git garbage-size out of control, need understanding","<p>we are using git as our DVCS for a very large project (yes, I know git it's not always pointed at as the best for these situations), and there's something I don't quite understand about my repo. </p>

<p>This is my count-objects output:</p>

<pre><code>count: 53
size: 1.57 MiB
in-pack: 26444
packs: 2
size-pack: 42.49 GiB
prune-packable: 0
garbage: 8
size-garbage: 32.22 GiB
</code></pre>

<p>as you can see the size is less than 2Mb, the size-pack is 43Gb (what is this, exactly?), but the size-garbage is 32Gb! What is that? Can I remove it? How?</p>

<p>I tried many options found on the internet with very poor understanding of what they do on a separate repository with basically no gains or major changes. Like:</p>

<pre><code>git reflog expire --all --expire=now
git gc --prune=now --aggressive
git gc
git repack -a -d --depth=250 --window=250
</code></pre>",1,0,2015-06-23 10:09:45.727000 UTC,,2015-06-23 10:23:45.830000 UTC,0,git|garbage-collection|dvcs|unreal-engine4,1672,2012-06-11 13:42:34.280000 UTC,2020-10-04 13:16:52.683000 UTC,Torino,129,2,0,43,,,,,,[]
Get a secret from AWS secret manager using DefaultAWSCredentialsProviderChain,"<p>Is there a way to retrieve a secret from the AWS secret store using <strong>DefaultAWSCredentialsProviderChain</strong> java class?</p>

<p>If not please suggest a way to retrieve it? (I need this in the context of doing signature V4 signing the request to connect with AWS Neptune. For signature signing, I am using <a href=""https://github.com/aws/amazon-neptune-sparql-java-sigv4/blob/master/src/main/java/com/amazonaws/neptune/client/rdf4j/NeptuneRdf4JSigV4Example.java"" rel=""nofollow noreferrer"">this</a> example. But my secrets are in AWS secret manager, So How can I retrieve the secret from the secret store with <strong>DefaultAWSCredentialsProviderChain</strong>)</p>",2,1,2020-02-27 08:37:34.807000 UTC,,,0,amazon-web-services|aws-java-sdk|amazon-neptune,1039,2017-04-06 18:53:11.143000 UTC,2021-12-10 06:59:36.113000 UTC,"Gurgaon, Haryana, India",302,31,0,52,,,,,,[]
Is there a way to do a true sql type merge of 2 dataframes,"<p>For starter I'll admit that I'm quite new to dataframes/databricks having worked with them for only a few months.</p>

<p>I have two dataframes read from parquet files (full format). In reviewing the documentation it appears that what in pandas is called merge is in fact only a join.</p>

<p>in SQL I would write this step as:</p>

<pre><code> ml_RETURNS_U = sqlContext.sql(""""""
  MERGE INTO U2 as target
    USING U as source
    ON (
        target.ITEMNUMBER = source.ITEMNUMBER
        and target.PRODUCTCOLORID = source.PRODUCTCOLORID
        and target.WEEK_ID = source.WEEK_ID
        )
    WHEN MATCHED THEN
      UPDATE SET target.RETURNSALESQUANTITY = target.RETURNSALESQUANTITY + source.QTY_DELIVERED
    WHEN NOT MATCHED THEN
      INSERT (ITEMNUMBER, PRODUCTCOLORID, WEEK_ID, RETURNSALESQUANTITY)
      VALUES (source.ITEMNUMBER, source.PRODUCTCOLORID, source.WEEK_ID, source.QTY_DELIVERED)
"""""")
</code></pre>

<p>When I run this command I get the following error: u'MERGE destination only supports Delta sources.\n;'</p>

<p>So I have two questions: Is there a way I can preform this operation using pandas or pySpark?</p>

<p>if not, how can I resolve this error?</p>",1,0,2019-01-16 18:32:24.273000 UTC,,,0,pandas|dataframe|pyspark-sql|azure-databricks,843,2016-09-30 22:52:37.447000 UTC,2021-05-04 23:36:42.850000 UTC,"Oregon, United States",57,20,0,28,,,,,,[]
How to execute Intellij Spark Code on Databricks Cluster,"<p>I'm trying to launch my Spark code that i've written in Intellij and run it on Databricks, so I've found that it can be done by <strong>""sbt-databricks""</strong> plugin.</p>

<p>Here is my <em>build.sbt</em> file : </p>

<pre><code>name := ""DatabricksTest""

version := ""1.0""

scalaVersion := ""2.11.8""

libraryDependencies ++= Seq(""org.apache.spark"" %% ""spark-core"" % ""2.4.0"" //% ""provided""
  ,""org.apache.spark"" %% ""spark-sql"" % ""2.4.0"" //% ""provided"",
 )

dbcUsername := ""token""//

dbcPassword := ""xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx""// 

dbcApiUrl := ""https://westeurope.azuredatabricks.net/api/1.2""//

dbcClusters += ""Cluster1""
</code></pre>

<p>And here is my <em>plugins.sbt</em> file:</p>

<pre><code>addSbtPlugin(""com.databricks"" %% ""sbt-databricks"" % ""0.1.5"")
</code></pre>

<p>When I try to list the clusters using <strong>dbcListClusters</strong> I have the following error </p>

<pre><code>[trace] Stack trace suppressed: run 'last *:dbcFetchClusters' for the full output.
[error] (*:dbcFetchClusters) org.apache.http.client.HttpResponseException: Bad Request
[error] Total time: 3 s, completed 28 mars 2019 16:18:54
</code></pre>

<p>Can you relate to this error ?</p>

<p>Thanks</p>",0,0,2019-03-28 15:34:12.073000 UTC,2.0,,2,scala|apache-spark|intellij-idea|sbt|azure-databricks,687,2016-04-28 20:54:23.423000 UTC,2020-09-10 14:23:37.627000 UTC,,437,11,0,14,,,,,,[]
Having issue susing the MAX command in Databricks,"<p>Just running a simple query</p>
<pre><code>select COL1
from ( 
    select *
    , monotonically_increasing_id() as row_id 
    from db00sparkmigration_landingzone_template.tbl_Ingestion_note_load_errors_pipes 
    ) aa
where row_id &gt; 1 and row_id &lt; max(row_id)
</code></pre>
<p>but getting the following error, any ideas?</p>
<blockquote>
<p>Error in SQL statement: UnsupportedOperationException: Cannot evaluate expression: max(input[1, bigint, false])</p>
</blockquote>",1,0,2020-08-03 10:44:59.403000 UTC,,2020-08-03 10:48:56.810000 UTC,0,sql|window-functions|azure-databricks,18,2019-08-28 12:05:37.513000 UTC,2022-03-04 12:12:43.257000 UTC,"Bournemouth, UK",69,0,0,20,,,,,,[]
Mounting Azure Blob Storage to Azure Databricks without using cluster,"<p>We have a requirement that while  provisioning the Databricks service thru CI/CD pipeline in Azure DevOps we should able to mount a blob storage to DBFS without connecting to a cluster. Is it possible to mount object storage to DBFS cluster by using a bash script from Azure DevOps ?</p>
<p>I looked thru various forums but they all mention about doing this using <strong>dbutils.fs.mount</strong> but the problem is we cannot run this command in Azure DevOps CI/CD pipeline.</p>
<p>Will appreciate any help on this.</p>
<p>Thanks</p>",1,0,2020-08-14 07:36:21.097000 UTC,,2020-08-14 08:16:10.630000 UTC,1,azure-devops|azure-databricks,877,2018-02-20 23:43:12.553000 UTC,2021-03-20 22:40:01.657000 UTC,,11,0,0,11,,,,,,[]
Using AWS Appsync with AWS Neptune,"<p>I'm currently using Aws Appsync, Aws Lambda, and Aws Neptune for an application. My Lambda function uses NodeJS 12. Right now my problem is getting back the appropriate JSON format from Neptune (more specifically gremlin) for my graphql api (appsync) when I do a mutation and eventually a query (I want to make sure the mutations are working first). For example:</p>

<ol>
<li>Here is my type <strong>Post</strong> for my graphql schema with <strong>addPost</strong> mutation right above it:
<a href=""https://i.stack.imgur.com/TDzf7.png"" rel=""noreferrer"">Post schema</a></li>
<li>The <strong>addPost</strong> mutation maps to this resolver: <a href=""https://i.stack.imgur.com/iVQQH.png"" rel=""noreferrer"">Post Mutation Resolver</a></li>
<li>Which then runs this piece of code: <a href=""https://i.stack.imgur.com/Gx9f5.png"" rel=""noreferrer"">Lambda Code</a></li>
</ol>

<p>When I run this test query to add a post, I get the following error with data being <strong>null</strong>: <a href=""https://i.stack.imgur.com/BzwE5.png"" rel=""noreferrer""><strong>addPost</strong> test query and result</a></p>

<p>Does adding a vertex in gremlin return data/an object? If so, how do I get the appropriate JSON format for my appsync graphql api? I've been reading <a href=""http://kelvinlawrence.net/book/Gremlin-Graph-Guide.html#servertweaks"" rel=""noreferrer"">Practical Gremlin</a> and searching the web but no luck. Thank you in advance.</p>",1,0,2020-04-16 21:24:02.850000 UTC,1.0,,6,amazon-web-services|aws-lambda|graphql|aws-appsync|amazon-neptune,963,2017-06-15 00:19:37.513000 UTC,2020-08-28 20:22:40.297000 UTC,,79,1,0,8,,,,,,[]
Getting timeout error while connecting with AWS Neptune database from lambda function,"<p>I am trying to connect with the AWS Neptune database using gremlinpython library from the AWS Lambda function.</p>
<pre><code>from gremlin_python.driver import client

query = &quot;g.V().count()&quot;
uri = &quot;wss://aws-neptune-endpoint:8182/gremlin&quot;
clientt = client.Client(uri, 'g')
response = clientt.submit(query)
if response != None:
   print('Response is OK')
   return response.result()
else:
   print('Response is not good ')
   return None
</code></pre>
<p><strong>Note</strong>: AWS Lambda function is outside of Neptune DB VPC.
but When we are trying the following code, we are able to connect and get results.</p>
<pre><code>from __future__  import print_function  # Python 2/3 compatibility
from gremlin_python.structure.graph import Graph
from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection
from gremlin_python.driver import serializer
from gremlin_python.process.anonymous_traversal import traversal

graph = Graph()

remoteConn = DriverRemoteConnection('wss://aws-neptune-endpoint:8182/gremlin','g',
                                    pool_size=1,
    message_serializer=serializer.GraphSONSerializersV2d0())
g = traversal().withRemote(remoteConn)

print(g.V().hasLabel('Company'))
remoteConn.close()
</code></pre>
<p>Please help, Thanks in Advance</p>",1,6,2022-02-01 13:03:59.690000 UTC,,2022-02-01 14:32:27.640000 UTC,0,amazon-web-services|amazon-neptune|gremlinpython,42,2017-01-03 11:53:12.290000 UTC,2022-03-04 14:49:07.200000 UTC,"Gurugram, Haryana, India",410,57,1,69,,,,,,[]
automating runs in databricks,"<p>I am new to the databricks environment and a bit also not much familiar with the
jargon. Here's my question.</p>

<p>I have a jar file called <code>xx.jar</code> and a property file with several arguments to run the jar file
called <code>pp.properties</code>. The <code>pp.properties</code> have several arguments including path to the all the data
required to run the <code>xx.jar</code>. These data are stored in my azure storage explorer. This is how I
run a job in the databricks.</p>

<ol>
<li>Created the cluster <code>mycluster</code></li>
<li>Create a job and upload <code>xx.jar</code> in the section Task by clicking <code>Set Jar</code>. In the argument
section, I give the path to <code>pp.properties</code>.</li>
<li>Click Run and wait for the job to finish.</li>
</ol>

<p>I often need to multiple iterations by changing the values in the <code>pp.properties</code>, restarting the
cluster and click Run again. I was wondering if there is any way of automating this.
Basically, what I am looking to do is to modify the <code>pp.properties</code>, run the jar and repeat this for
n iterations. Any tutorials will also be much appreciated.</p>",1,2,2019-08-15 09:42:22.927000 UTC,,,0,r|apache-spark|azure-databricks,136,2014-01-23 10:22:40.150000 UTC,2022-03-04 17:07:33.650000 UTC,,2685,227,3,607,,,,,,[]
Issue with parsing CSV file and error handling in PySpark,"<p>I am working with Azure Databricks and PySpark 2.4.3 trying to build a  robust approach to file import from Blob storage to a cluster file. Things <em>mostly</em> work but parsing is not raising errors as I expect. </p>

<p>I have a 7GB csv file that I know has a number records with issues that are causing rows to be skipped (found by reconciling the count of read records in the output parquet file written from the Dataframe versus the source CSV.)   I am attempting to use the badRecordsPath option and there no output is being generated (that I can find.). Can anyone share advice on how to troubleshoot file loading when there is bad data - and to create a robust process that will handle parsing errors not permissively in the future?</p>

<p>One  issue tacked is around embedded newlines where I've found wholeFile and multiline options have helped - but I am now having challenges in getting insight to what records are not being accepted.</p>

<p>The python code that I am using to load the file looks like this.</p>

<pre><code>myDf = spark.read.format(""csv"")\
.option(""inferSchema"", ""true"")\
.option(""header"", ""true"")\
.option(""wholeFile"", ""true"")\
.option(""multiline"",""true"")\
.option(""ignoreLeadingWhiteSpace"", ""true"")\
.option(""ignoreTrailingWhiteSpace"", ""true"")\
.option(""parserLib"",""univocity"")\
.option('quote', '""')\
.option('escape', '""')\
.option(""badRecordsPath"",""/tmp/badRecordsPath"")\
.load(BLOB_FILE_LOCATION)
</code></pre>

<p>What I see is that about a half million records out of more than 10 million records being dropped. I am currently unable to easily tell which ones or know that failures are occurring or what they are (without exporting and comparing data which would be OK for a one-time load - but not acceptable for the production system). I've also tried the other read modes without luck (it always seems to be behaving like DROPMALFORMED is set which is not the case (even trying mode set ""FAILFAST"" in an experiment.)</p>

<p>Many thanks for any insight / advice.</p>",0,3,2019-09-12 20:47:25.307000 UTC,,,1,pyspark|azure-databricks,444,2019-09-12 20:35:44.437000 UTC,2020-07-29 14:24:36.317000 UTC,"Pittsburgh, PA, USA",11,0,0,7,,,,,,[]
Getting vertices and their edges upon a condition,"<p>I have a not so complex set of data, but I am struggling to make a query.</p>
<p>Let's say that I have a main vertex with id 1.
That vertex has edges to vertices 10, 11, and 12. Each of those vertices have edges to 100 and 101.</p>
<p>If I do something like this:</p>
<pre><code>g.V(1).inE('type').project('e', 'v').by().by(outV().valueMap())
</code></pre>
<p>I get a list of vertices: 10, 11 and 12, but I don't get the list of 100 and 101.
I assume that I can do something with coalesce or something, but I can't figure out the proper way of doing this.</p>
<p>Also, down the line I want to filter the 10, 11 and 12 vertices based on one 100 and 101 property. Meaning that if 100 has a property like x = 1 and 101 has a property x = 2, I want to get only the vertices (in the 10-12 set) that point to the vertex that has x = 1.</p>",1,0,2021-07-23 18:24:31.817000 UTC,,,0,gremlin|amazon-neptune,27,2013-07-16 20:58:07.570000 UTC,2022-03-04 01:20:13.760000 UTC,,1005,18,2,53,,,,,,[]
AttributeError: 'GroupedData' object has no attribute 'applyInPandas' in Azure Databricks,"<p>I got stuck with the error while running some jobs on a DBR 6.6 ML cluster on Azure Databricks.
I've gone through the spark Grouped Map documentation and everything seems fine from my end.</p>",1,0,2020-11-26 17:04:56.537000 UTC,,,0,pyspark|azure-databricks,256,2017-04-21 12:26:58.817000 UTC,2022-03-02 22:50:28.417000 UTC,"Lagos, NG",43,14,0,22,,,,,,[]
Multiprocess python module seems to not work as expected for Spark MLlib when running in a Databricks cluster with 32 CPU cores,"<p>Please note that due to the length of this post I advise you to look at the description of each function presented. This is because, the functions are executed successfully without any bug. I just present them for the reader to get a general view of the code executed. So pay more attention on my Theoretical part and My Question sections and not so much on the Technical section.</p>

<p><strong>[Theoretical part - Explain the situation]</strong></p>

<p>Initially, I would like to pinpoint that this is an execution-time related question. The code illustrated works perfectly, although the time execution is something I am concerned with.</p>

<p>I would like your opinion on a matter I have experienced the last few days working on <strong>threading</strong> and <strong>multiprocessing</strong> Python modules on a Databricks cluster with 32 CPU cores. Very briefly, I have created a function <em>(presented below)</em> which takes as input a Spark Dataframe and trains two SPARK MLlib classifiers. Prior to the training some extra cleaning and preparation is applied to the Spark Dataframe using again Spark alike commands. Both the time taken to train and preprocess each spark dataframe will be presented. This function, which includes the functions of training and preprocessing, is applied 15 times (aka to 15 different spark dataframes). Thus, you can understand that my goal in using <em>threading and multiprocessing</em> was to execute those 15 iterations at once and not sequentially (one after the other). Just only imagine that those 15 iterations will become 1500 in the near future. So, this is a benchmark of what is coming in terms of data scale up.</p>

<p>Before moving forward I would like to illustrate some of the conclusions I made while working on threading and multiprocessing. Based on this <a href=""https://medium.com/@bfortuner/python-multithreading-vs-multiprocessing-73072ce5600b"" rel=""nofollow noreferrer"">article</a> by Brendan Fortuner, threading is mostly used for I/O bound tasks which are subject to GIL limitations (prevent two threads from executing simultaneously in the same program). On the opposite side, the multiprocessing module uses Processes to speed up Python operations that are CPU intensive because they benefit from multiple cores and avoid the GIL. Thus, even though I initially created a threading alike application that applies my function 15 times concurrently, I later changed to multiprocessing approach due to the reason written above.</p>

<p><strong>[Technical part]</strong></p>

<p><em>The Spark dataframe</em></p>

<pre class=""lang-py prettyprint-override""><code>spark_df= pd.DataFrame({    'IMEI' : ['358639059721529', '358639059721529', '358639059721529', '358639059721529', '358639059721529', '358639059721735', '358639059721735', '358639059721735', '358639059721735', '358639059721735'],
                            'PoweredOn': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
                            'InnerSensorConnected': [1.0,  1.0,  1.0,  1.0,  1.0,  1.0, 1.0,  1.0,  1.0,  1.0,  1.0],
                            'averageInnerTemperature': [2.5083397819149877, 12.76785419845581, 2.5431994716326396, 2.5875612214150556, 2.5786447594332143, 2.6642078435610212, 12.767857551574707, 12.767857551574707, 2.6131772499486625, 2.5172743565284166]
                            'OuterSensorConnected':[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 
                            'OuterHumidity':[31.784826, 32.784826, 33.784826, 43.784826, 23.784826, 54.784826, 31.784826, 31.784826],
                            'EnergyConsumption': [70.0, 70.0, 70.0, 70.0, 70.0, 70.0, 70.0, 70.0, 70.0, 70.0],
                            'DaysDeploymentDate': [10.0, 20.0, 21.0, 31.0, 41.0, 11.0, 19.0, 57.0, 44.0, 141.0],
                            'label': [0, 0, 1, 1, 1, 0, 0, 1, 1, 1]
                      }
                    )
spark_df= spark.createDataFrame(spark_df)
</code></pre>

<p>The dataframe is just presented to keep in mind the spark dataframe used. Imagine that those 10 rows is 7000 rows and the 2 IMEI are actually 15 unique IMEIs, because as I told you, I have 15 spark dataframes 1 per IMEI (['358639059721529', '358639059721735']).</p>

<p>*[The function applied]</p>

<pre class=""lang-py prettyprint-override""><code>def training_models_operation_multiprocess(asset_id, location, asset_total_number, timestamp_snap, joined_spark_dataset):
 #-------------------------------------------------------------------------------------------------------------------------
    # KEYWORDS INITIALIZATION
    #-------------------------------------------------------------------------------------------------------------------------
    device_length=int(asset_total_number)
    list_string_outputs=list()
    max_workers=16*2
    training_split_ratio=0.5
    testing_split_ratio=0.5
    cross_validation_rounds=2
    optimization_metric=""ROC_AUC""
    features_column_name=""features""
    disable_logging_value=1 # a value that prevents standard output to be logged at Application insights
    logger_initialization=instantiate_logger(instrumentation_key_value) #a logger instance

    # Time format
    date_format = '%Y-%m-%d %H-%M-%S'
    #-------------------------------------------------------------------------------------------------------------------------
    # KEYWORDS INITIALIZED
    #-------------------------------------------------------------------------------------------------------------------------

    try:

        print(""{0}: START EXECUTION PLAN OF ASSET ID {1}: {2}/{3}"".format(datetime.utcnow().strftime(date_format), asset_id, location, device_length))
        begin_time_0 = time.time()

        #1.1 Filter the rows related to the current asset
        begin_time_1 = time.time()

        filtered_dataset=joined_spark_dataset.where(joined_spark_dataset.IMEI.isin([asset_id]))
        filtered_dataset=apply_repartitioning(filtered_dataset, max_workers)

        end_time_1 = time.time() - begin_time_1
        list_string_outputs.append(""{0}: FINISH Step 1.1 asset id {1}: {2}/{3} in: {4}\n"".format(datetime.utcnow().strftime(date_format), asset_id, location, device_length, format_timespan(end_time_1)))

        #------------------------
        # FUNCTION: 1.2 Preprocess
        begin_time_2 = time.time()

        target_column_name=None
        target_column_name='label'
        preprocessed_spark_df=preprocess_data_pipeline(filtered_dataset, drop_columns_not_used_in_training, target_column_name, executor)
        preprocessed_spark_df=apply_repartitioning(preprocessed_spark_df, max_workers)

        end_time_2 = time.time() - begin_time_2
        list_string_outputs.append(""{0}: FINISH Step 1.2 asset id {1}: {2}/{3} in: {4}\n"".format(datetime.utcnow().strftime(date_format), asset_id, location, device_length, format_timespan(end_time_2)))

        #------------------------
        #FUNCTION: 1.3 Train-Test split
        begin_time_3 = time.time()

        target_column_name=None
        target_column_name='target'
        training_data, testing_data=spark_train_test_split(asset_id, preprocessed_spark_df, training_split_ratio, testing_split_ratio, target_column_name, disable_logging_value, logger_initialization)
        training_data=apply_repartitioning(training_data, max_workers)
        testing_data=apply_repartitioning(testing_data, max_workers)

        end_time_3 = time.time() - begin_time_3
        list_string_outputs.append(""{0}: FINISH Step 1.3 asset id {1}: {2}/{3} in: {4}\n"".format(datetime.utcnow().strftime(date_format), asset_id, location, device_length, format_timespan(end_time_3)))

        #FUNCTION: 1.4 Train the algorithms
        begin_time_4 = time.time()

        best_classifier_asset_id=spark_ml_classification(asset_id, cross_validation_rounds, training_data, testing_data, target_column_name, features_column_name, optimization_metric, disable_logging_value, 
                                                         logger_initialization)

        end_time_4 = time.time() - begin_time_4
        list_string_outputs.append(""{0}: FINISH Step 1.4 asset id {1}: {2}/{3} in: {4}\n"".format(datetime.utcnow().strftime(date_format), asset_id, location, device_length, format_timespan(end_time_4)))

        end_time_0 = time.time() - begin_time_0
        list_string_outputs.append(""{0}: END EXECUTION PLAN OF ASSET ID {1}: {2}/{3} in: {4}\n"".format(datetime.utcnow().strftime(date_format), asset_id, location, device_length, format_timespan(end_time_0)))

    except Exception as e:
            custom_logging_function(logger_initialization, disable_logging_value, ""ERROR"", ""ERROR EXCEPTION captured in asset id {0}: {1}"".format(asset_id, e))
            raise
    print("" "".join(list_string_outputs))
</code></pre>

<p>[FUNCTION 1.1]: Filter the dataset per IMEI
<em>Description: From the whole dataset with all the IMEI ids filter only the rows belonging to the IMEI per iteration number</em></p>

<p>device_ids=['358639059721529', '358639059721735']
filtered_dataset=spark_df.where(spark_df.IMEI.isin([device_ids]))</p>

<p>[FUNCTION 1.2]: Preprocess Spark DF
<em>Description: Apply VectorAssembler to trainable features and StringIndexer to the label</em></p>

<pre class=""lang-py prettyprint-override""><code>   def preprocess_data_pipeline(spark_df, target_variable)
    stages = []

    # Convert label into label indices using the StringIndexer
    label_stringIdx = StringIndexer(inputCol=target_variable, outputCol=""target"").setHandleInvalid(""keep"") #target variable shoule be IntegerType
    stages += [label_stringIdx]

    numeric_columns=[""PoweredOn"", ""InnerSensorConnected"", ""averageInnerTemperature"", ""OuterSensorConnected"", ""OuterHumidity"", ""EnergyConsumption"", ""DaysDeploymentDate""]

    # Vectorize trainable features
    assemblerInputs = numeric_columns
    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=""features"").setHandleInvalid(""keep"")
    stages += [assembler]

    partialPipeline = Pipeline().setStages(stages)
    pipelineModel = partialPipeline.fit(spark_df)
    preppedDataDF = pipelineModel.transform(spark_df)

    # Keep relevant columns
    selectedcols = [""target"", ""features""]
    dataset = preppedDataDF.select(selectedcols)
    dataset=dataset.drop(target_variable)

    #dataset.printSchema()
    return dataset
</code></pre>

<p>[FUNCTION 1.3: Train-Test split]
* Split the data to train and test spark df's using a Stratified kind of approach*</p>

<pre class=""lang-py prettyprint-override""><code>def spark_train_test_split(device_id, prepared_spark_df, train_split_ratio, test_split_ratio, target_variable):

    trainingData = prepared_spark_df.sampleBy(target_variable, fractions={0: train_split_ratio, 1: train_split_ratio}, seed=10)
    testData = prepared_spark_df.subtract(trainingData)

    return trainingData, testData
</code></pre>

<p>[FUNCTION 1.4: Train the ML algorithms]
<em>Description: Train two classification algorithms and then select the one with the highest ROC_AUC score. Each classifier is trained using the CrossValidator class of Spark MLlib...For the first classifier (Random Forest) I cross validate 4 models while for the second classifier (Gradient Boosted Trees) I cross validate 8 models. To speed up things on this, I have set the parallelism parameter of the cross validator class equal to 8 (explanation <a href=""https://spark.apache.org/docs/latest/ml-tuning.html#model-selection-aka-hyperparameter-tuning"" rel=""nofollow noreferrer"">here</a>)</em></p>

<pre class=""lang-py prettyprint-override""><code>def machine_learning_estimator_initialization(model_name, target_variable, features_variable):

    try:
        dictionary_best_metric={}
        dictionary_best_estimator={}
        list_of_classifiers=[""RandomForest Classifier"", ""GradientBoost Classifier""]

        begin_time_train=time.time()
        for i in list_of_classifiers:

            pipeline_object, paramGrid, evaluator=machine_learning_estimator_initialization(i, target_column, features_column)

            start_time_classifier=time.time()

            # THE MOST TIME CONSUMING PART OF MY EXECUTION
            classification_object = CrossValidator(estimator=pipeline_object, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=cross_validation_rounds, parallelism=8)

            classificationModel = classification_object.fit(training_dataset)
            end_time_classifier=time.time()-start_time_classifier
            print(""Time passed to complete training for classifier {0} of asset id {1}: {2}"".format(i, device_id, format_timespan(end_time_classifier)))

            predictions = classificationModel.transform(testing_dataset)
            evaluation_score_classifier=evaluator.evaluate(predictions, {evaluator.metricName: ""areaUnderROC""})
            y_true = predictions.select([target_column]).collect()
            y_pred = predictions.select(['prediction']).collect()

            confusion_mat=confusion_matrix(y_true, y_pred)
            confusion_table=pd.DataFrame(confusion_mat,
                                         columns=['0','1'],
                                         index=['0','1'])

            accuracy_value=accuracy_score(y_true, y_pred)
            f1_value=f1_score(y_true, y_pred, zero_division=1)
            precision_value=precision_score(y_true, y_pred, zero_division=1)
            recall_value=recall_score(y_true, y_pred, zero_division=1)
            hamming_loss_value=hamming_loss(y_true, y_pred)
            zero_one_loss_value=zero_one_loss(y_true, y_pred, normalize=False)

            list_of_metrics=['ROC_AUC', 'accuracy', 'f1', 'precision', 'recall', 'hamming_loss', 'zero_one_loss']
            list_of_metric_values=[evaluation_score_classifier, accuracy_value, f1_value, precision_value, recall_value, hamming_loss_value, zero_one_loss_value]

            evaluation_metric_name_index=list_of_metrics.index(evaluation_metric) # With this index I can locate any value selected for the evaluation metric

            if evaluation_metric=='ROC_AUC':
                dictionary_best_metric.update({""{0}_best_score"".format(i): evaluation_score_classifier}) #alternative hamming_loss_value

            else:
                dictionary_best_metric.update({""{0}_best_score"".format(i): list_of_metric_values[evaluation_metric_name_index]})

            dictionary_best_estimator.update({""{0}_best_estimator"".format(i): classificationModel.bestModel})

        end_time_train=time.time()-begin_time_train
        print(""Total time of training execution of two MLlib algorithms for the asset {0}: {1}"".format(device_id, format_timespan(end_time_train)))

        maximum_metrics=['ROC_AUC', 'accuracy', 'f1', 'precision', 'recall']
        minimum_metrics=['hamming_loss', 'zero_one_loss']

        if evaluation_metric in maximum_metrics:
            index_of_best_model_score=list(dictionary_best_metric.keys()).index(max(dictionary_best_metric, key=dictionary_best_metric.get))

        else:
            index_of_best_model_score=list(dictionary_best_metric.keys()).index(min(dictionary_best_metric, key=dictionary_best_metric.get))

        classification_model_for_scoring=list(dictionary_best_estimator.values())[index_of_best_model_score]

    except Exception as e:
        print(e)

    return classification_model_for_scoring
</code></pre>

<p>The four functions presented above are those applied 15 times (15 spark dataframes, 1 per IMEI the unique id). For what I am concern is the time taken to execute those 15 iterations of the functions presented. As already said I have initially implemented an approach by following the threading module. The approach is as follows:</p>

<p><strong>[THREADING APPROACH]</strong></p>

<pre class=""lang-py prettyprint-override""><code>import threading

#Creating a list of threads
device_ids=spark_df.select(sql_function.collect_set('IMEI').alias('unique_IMEIS')).collect()[0]['unique_IMEIS']
device_ids=device_ids[-15:]
location=range(1, len(device_ids)+1, 1)
devices_total_number=len(device_ids)

date_format = '%Y-%m-%d %H-%M-%S'
timestamp_snapshot=datetime.utcnow()
timestamp_snap=timestamp_snapshot.strftime(date_format)

thread_list = list()

# #looping all objects, creating a thread for each element in the loop, and append them to thread_list
for location, i in enumerate(device_ids, 1):

    try:
        thread = threading.Thread(target=training_models_operation_multiprocess, args=(i, location, asset_total_number, timestamp_snap, spark_df,)
        thread_list.append(thread)
        thread.start()

    except Exception as e:
        print(e)

**[BENCHMARK OF MULTIPROCESSING APPROACH]**


#--------------------------------------
# Wait for all threads to finish
for thread in thread_list:
    thread.join()

print(""Finished executing all threads"")
</code></pre>

<p><strong>BENCHMARK: ON A CLUSTER WITH 32 CPU CORES: ~16m</strong></p>

<p>However, as already mentioned the treading is not used as my final approach. In the end, having read a couple of things about multiprocessing, I chose this approach.</p>

<p><strong>[MULTIPROCESSING APPROACH]</strong></p>

<pre class=""lang-py prettyprint-override""><code>from multiprocessing.pool import ThreadPool as Pool
from multiprocessing import freeze_support
from itertools import cycle

if __name__ == '__main__':
    freeze_support()

    device_ids=datalake_spark_dataframe_downsampled.select(sql_function.collect_set('IMEI').alias('unique_IMEIS')).collect()[0]['unique_IMEIS']
    device_ids=device_ids[-15:] #15 UNIQUE IMEI's 
    location=range(1, len(device_ids)+1, 1)
    devices_total_number=len(device_ids)

    pool_list=list()

    with Pool(mp.cpu_count()) as pool:
        start_time = time.time()
        tasks = [*zip(device_ids, location, cycle([str(devices_total_number)]), cycle([timestamp_snap]), cycle([datalake_spark_dataframe_downsampled]))]

        pool.starmap(training_models_operation_multiprocess,
                     iterable=(tasks),
                     chunksize=1)
        pool.close()
        pool.join()
        pool.terminate()
        end_time = time.time()
        secs_per_iteration = (end_time - start_time) / len(device_ids)
        print(""Time per iteration: {0}"".format(format_timespan(secs_per_iteration)))
</code></pre>

<p><strong>[BENCHMARK ON A CLUSTER WITH 32 CPU cores]</strong></p>

<p>On average for every IMEI and its relevant spark df, RandomForest and GradientBoostedTrees took that time to execute: 5minutes and 6 minutes respectively.
<a href=""https://i.stack.imgur.com/ZtgsE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZtgsE.png"" alt=""enter image description here""></a></p>

<p>Below you will notice the time take to execute each of the above 4 sub-fucntions (1.1, 1.2, 1.3, 1.4)
<a href=""https://i.stack.imgur.com/NDMu7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NDMu7.png"" alt=""enter image description here""></a></p>

<p><strong>[My Question]</strong></p>

<p>Having presented all the facts and results from my experiment it's time to write my question.</p>

<p>How is it possible with a cluster of 32 CPU cores (2 workers with 16 cores each) to get such a time consuming result? Is it possible a Pool execution to take almost 12 minutes to run a cross validation on a dataframe with approximately 467 rows... My spark df had 7,000 rows in total and with 15 ids I get 467 rows per IMEI id. In my humble mind 32 CPU cores are great computational power, yet they execute a sequence of functions in 15 minutes.</p>

<p>Thus, I want to understand why is this happening: </p>

<p>Is it a problem of Spark? Meaning, that it cannot allocate properly 32 CPU cores to execute 4 simple functions? Combining the Multiprocessing module I would expect 15 iterations to complete in much less time. Maybe it's something that I have not yet understood about multiprocessing and my execution can only achieve up to this execution time.</p>

<p>I would really appreciate your opinion on this matter, because maybe I miss the point of multiprocessing. I cannot digest the fact that I have 32 cpu cores and my execution takes 1 minute, per Pool, for Spark to complete it. Please, don't take into consideration the fact that I use Spark to train data of 500 rows dataframe because in the near future this df will have 100,000 rows and more. So I have in mind the disadvantages of Spark over Python on such small number of rows. But it's the multiprocessing approach I want to understand more.</p>",0,9,2020-06-13 21:59:17.067000 UTC,1.0,,1,python|multithreading|apache-spark|multiprocessing|azure-databricks,434,2018-11-08 11:32:43.047000 UTC,2022-03-05 23:08:13.960000 UTC,"Athens, Greece",1012,546,2,167,,,,,,[]
Login to Azure ML workspace from Azure Databricks notebook,"<p>I am writing a python notebook in Azure Databricks cluster to perform an Azure Machine learning experiment. I have created an Azure ML workspace and instantiating a workspace object in my notebook as follows:</p>

<pre><code>id = InteractiveLoginAuthentication(force=False, tenant_id=AzureTenantId)
ws = Workspace(SubscriptionId, ResourceGroupName, WorkspaceName, auth = id)
</code></pre>

<p>I am trying to perform an interactive login to azure to access the workspace but when I run the notebook I get the following error. the notebook is written in python</p>

<pre><code>Falling back to use azure cli credentials. This fall back to use azure cli credentials will be removed in the next release. 
Make sure your code doesn't require 'az login' to have happened before using azureml-sdk, except the case when you are specifying AzureCliAuthentication in azureml-sdk.
Performing interactive authentication. Please follow the instructions on the terminal.
From cffi callback &lt;function _verify_callback at 0x7f4736825d08&gt;:
Traceback (most recent call last):
  File ""/databricks/python/lib/python3.5/site-packages/OpenSSL/SSL.py"", line 309, in wrapper
    _lib.X509_up_ref(x509)
AttributeError: module 'lib' has no attribute 'X509_up_ref'
</code></pre>

<p>Could someone help me resolve this issue? Is it really an OpenSSL issue?</p>",3,0,2019-02-11 23:51:50.793000 UTC,,,2,python-3.x|azure-machine-learning-studio|azure-databricks,1118,2014-12-22 23:46:46.047000 UTC,2021-11-01 23:21:04.773000 UTC,,91,0,0,17,,,,,,[]
org.apache.spark.SparkException: Writing job aborted on Databricks,"<p>I have used Databricks to ingest data from Event Hub and process it in real time with Pyspark Streaming. The code is working fine, but after this line:</p>
<pre><code>df.writeStream.trigger(processingTime='100 seconds').queryName(&quot;myquery&quot;)\
  .format(&quot;console&quot;).outputMode('complete').start()
</code></pre>
<p>I'm getting the following error:</p>
<pre><code>org.apache.spark.SparkException: Writing job aborted.
Caused by: java.io.InvalidClassException: org.apache.spark.eventhubs.rdd.EventHubsRDD; local class incompatible: stream classdesc
</code></pre>
<p>I have read that this could be due to low processing power, but I am using a Standard_F4 machine, standard cluster mode with autoscaling enabled.</p>
<p>Any ideas?</p>",1,3,2021-12-26 01:27:57.510000 UTC,,2021-12-26 08:39:14.977000 UTC,1,apache-spark|pyspark|azure-databricks|azure-eventhub,238,2021-04-28 19:38:01.237000 UTC,2022-03-04 13:43:39.593000 UTC,,39,5,0,30,,,,,,[]
Subqueries in Python,"<p>I am trying to use subqueries to run matching against multiple tables and move the unmatched records to new table.</p>

<p>I have written SQL subqueries but the only issue i am facing is the performace, it is taking lot of time to process.</p>

<pre><code>create table UnmatchedRecord
(select a.* 
 from HashedValues a 
 where a.Address_Hash not in(select b.Address_Hash 
                             from HashAddress b) 
 and a.Person_Hash not in(select d.Person_Hash 
                          from HashPerson d) 
 and a.HH_Hash not in(select f.HH_Hash 
                      from HashHH f) 
 and a.VehicleRegistration not in(select VehicleRegistration 
                                  from MasterReference) 
 and a.EmailAddress not in (select EmailAddress 
                            from MasterReference) 
 and a.PhoneNumber not in (select PhoneNumber 
                           from MasterReference) 
 and a.NationalInsuranceNo not in (select NationalInsuranceNo 
                                   from MasterReference))
</code></pre>",1,2,2019-10-08 09:41:10.937000 UTC,,2019-10-08 09:55:39.380000 UTC,0,sql|azure-databricks|pyspark-dataframes,69,2019-08-15 11:56:48.253000 UTC,2019-11-12 15:50:21.033000 UTC,,33,0,0,56,,,,,,[]
Merge in Spark SQL - WHEN NOT MATCHED BY SOURCE THEN,"<p>I am coding Python and Spark SQL in Databricks and I am using spark 2.4.5.</p>
<p>I have two tables.</p>
<pre><code>Create table IF NOT EXISTS db_xsi_ed_faits_shahgholi_ardalan.Destination
(
  id Int,
  Name string,
  Deleted int
) USING Delta;

Create table IF NOT EXISTS db_xsi_ed_faits_shahgholi_ardalan.Source
(
  id Int,
  Name string,
  Deleted int
) USING Delta;
</code></pre>
<p>I need to ran a Merge command between my source and destination. I wrote below command</p>
<pre><code>%sql
MERGE INTO db_xsi_ed_faits_shahgholi_ardalan.Destination AS D
USING db_xsi_ed_faits_shahgholi_ardalan.Source AS S
ON (S.id = D.id)
-- UPDATE
WHEN MATCHED AND S.Name &lt;&gt; D.Name THEN 
  UPDATE SET 
    D.Name = S.Name
-- INSERT    
WHEN NOT MATCHED THEN 
  INSERT (id, Name, Deleted)
  VALUES (S.id, S.Name, S.Deleted)
 -- DELETE
WHEN NOT MATCHED BY SOURCE THEN 
  UPDATE SET 
     D.Deleted = 1
</code></pre>
<p>When i ran this command i have below error:</p>
<p><a href=""https://i.stack.imgur.com/TT7Dx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TT7Dx.png"" alt=""enter image description here"" /></a></p>
<p>It seems that we do not have <code>NOT MATCHED BY SOURCE</code> in spark! I need a solution to do that.</p>",1,0,2020-10-22 15:13:08.347000 UTC,,,1,apache-spark|apache-spark-sql|azure-databricks,1193,2013-02-12 05:03:00.843000 UTC,2022-03-03 20:20:02.957000 UTC,"Montreal, Canada",10809,3254,76,2088,,,,,,[]
Is it possible to change the Great Expectations logo shown in Great_Expectations data docs,"<p>I received tremendous help visualizing Great_Expectations Data Docs from Apache Spark both with Databricks and Synapse see <a href=""https://stackoverflow.com/questions/68023413/how-to-save-great-expectations-results-to-file-from-apache-spark-with-data-doc/68364259#68364259"">How to Save Great Expectations results to File From Apache Spark - With Data Docs</a></p>
<p>I was wondering if it was possible to customize the logo shown, see image.</p>
<p><a href=""https://i.stack.imgur.com/kb5tc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kb5tc.png"" alt=""enter image description here"" /></a></p>
<p>I would like to change the logo to my own logo - or would that be breaking somekind of copyright</p>",0,2,2021-07-18 22:31:03.103000 UTC,,,0,apache-spark|azure-databricks|great-expectations,76,2021-03-31 08:36:43.863000 UTC,2022-03-05 23:03:22.193000 UTC,"London, UK",651,58,0,93,,,,,,[]
Flattening complex JSON file in Azure,"<p>I am trying to flatten a JSON file into a CSV using Azure. I have tried a data flow in Azure Data Factory, and also using Python in Azure Databricks. The desired columns I need are below, I need to have these against each stage in 'stage_descriptions'and each date in 'data':</p>
<pre><code>order
applicable_scales/feekes
applicable_scales/haun
applicable_scales/zadoks
air_temp_min
air_temp_max
gdd
agdd
harvest_window/start
harvest_window/end
</code></pre>
<p>So I really need to completely flatten it into a CSV including the dates. The data I need is in the 'data' and 'stage_description' columns. I have tried splitting the file in 2 but I cannot find a way there too. The JSON looks like this (note, in reality there are many more dates e.g. '2021-05-05' and 7 stages e.g. '0', '1', '2' etc.):</p>
<pre><code>   &quot;attribution&quot;:[
      &quot;Growth model based on PhenologyMMS, but modified. PhenologyMMS is a model developed by the USDA-ARS, Water Management and Systems Research Unit, Fort Collins, Colorado.&quot;
   ],
   &quot;stage_descriptions&quot;:{
      &quot;0&quot;:{
         &quot;advisor&quot;:&quot;Nutrient&quot;,
         &quot;description&quot;:&quot;The radicle (primary root) is appearing from the seed.&quot;,
         &quot;images&quot;:[
            
         ],
         &quot;name&quot;:&quot;Germination&quot;,
         &quot;order&quot;:&quot;1&quot;,
         &quot;applicable_scales&quot;:{
            &quot;feekes&quot;:&quot;n/a&quot;,
            &quot;haun&quot;:&quot;n/a&quot;,
            &quot;zadoks&quot;:&quot;05&quot;
         }
      },
      &quot;1&quot;:{
         &quot;advisor&quot;:&quot;Nutrient&quot;,
         &quot;description&quot;:&quot;Emergence occurs when the seedling pushes through the soil surface.&quot;,
         &quot;images&quot;:[
            {
               &quot;attribution&quot;:&quot;Courtesy of David L. Hansen, University of Minnesota.&quot;,
               &quot;caption&quot;:&quot;Emergence&quot;,
               &quot;url&quot;:&quot;static.clearapis.com/images/growth_wheat/Coleoptile.jpg&quot;
            }
         ],
         &quot;name&quot;:&quot;Emergence&quot;,
         &quot;order&quot;:&quot;2&quot;,
         &quot;applicable_scales&quot;:{
            &quot;feekes&quot;:&quot;1&quot;,
            &quot;haun&quot;:&quot;0.1&quot;,
            &quot;zadoks&quot;:&quot;10&quot;
         }
      },
      &quot;2&quot;:{
         &quot;advisor&quot;:&quot;Nutrient&quot;,
         &quot;description&quot;:&quot;The first tiller is visible.&quot;,
         &quot;images&quot;:[
            {
               &quot;attribution&quot;:&quot;Courtesy of David L. Hansen, University of Minnesota.&quot;,
               &quot;caption&quot;:&quot;First tiller&quot;,
               &quot;url&quot;:&quot;static.clearapis.com/images/growth_wheat/Zadoks+21.jpg&quot;
            }
         ],
         &quot;name&quot;:&quot;First Tiller&quot;,
         &quot;order&quot;:&quot;3&quot;,
         &quot;applicable_scales&quot;:{
            &quot;feekes&quot;:&quot;2&quot;,
            &quot;haun&quot;:&quot;1.9-2.7&quot;,
            &quot;zadoks&quot;:&quot;21&quot;
         }
      },
      &quot;3&quot;:{
         &quot;advisor&quot;:&quot;Nutrient&quot;,
         &quot;description&quot;:&quot;The shoot apex shape changes from dome to more elongated, and leaf primordia begin to form a ridge around the apex.&quot;,
         &quot;images&quot;:[
            
         ],
         &quot;name&quot;:&quot;Single Ridge&quot;,
         &quot;order&quot;:&quot;4&quot;,
         &quot;applicable_scales&quot;:{
            &quot;feekes&quot;:&quot;n/a&quot;,
            &quot;haun&quot;:&quot;n/a&quot;,
            &quot;zadoks&quot;:&quot;n/a&quot;
         }
      },
        &quot;data&quot;:{
      &quot;43.9691581726,24.8167858124&quot;:{
         &quot;growth&quot;:{
            &quot;2021-05-05&quot;:{
               &quot;air_temp_max&quot;:71.0,
               &quot;air_temp_min&quot;:50.0,
               &quot;gdd&quot;:15.82,
               &quot;agdd&quot;:15.82,
               &quot;last_stage&quot;:&quot;n/a&quot;,
               &quot;new_stage&quot;:&quot;n/a&quot;,
               &quot;agdd_envelope_max_year&quot;:&quot;2020&quot;,
               &quot;agdd_envelope_min_year&quot;:&quot;2020&quot;,
               &quot;agdd_envelope_max&quot;:15.82,
               &quot;agdd_envelope_min&quot;:15.82
            },
            &quot;2021-05-06&quot;:{
               &quot;air_temp_max&quot;:73.0,
               &quot;air_temp_min&quot;:51.0,
               &quot;gdd&quot;:16.57,
               &quot;agdd&quot;:32.38,
               &quot;last_stage&quot;:&quot;n/a&quot;,
               &quot;new_stage&quot;:&quot;n/a&quot;,
               &quot;agdd_envelope_max_year&quot;:&quot;2018&quot;,
               &quot;agdd_envelope_min_year&quot;:&quot;2011&quot;,
               &quot;agdd_envelope_max&quot;:54.07,
               &quot;agdd_envelope_min&quot;:36.2
            },
            &quot;2021-05-07&quot;:{
               &quot;air_temp_max&quot;:78.0,
               &quot;air_temp_min&quot;:50.0,
               &quot;gdd&quot;:17.66,
               &quot;agdd&quot;:50.04,
               &quot;last_stage&quot;:&quot;n/a&quot;,
               &quot;new_stage&quot;:&quot;n/a&quot;,
               &quot;agdd_envelope_max_year&quot;:&quot;2018&quot;,
               &quot;agdd_envelope_min_year&quot;:&quot;2011&quot;,
               &quot;agdd_envelope_max&quot;:86.50999999999999,
               &quot;agdd_envelope_min&quot;:56.69
            },
                    },
         &quot;harvest_window&quot;:{
            &quot;start&quot;:&quot;n/a&quot;,
            &quot;end&quot;:&quot;n/a&quot;
         }
      }
   }
}```

Maybe there are some key functions/ libraries/ documents that I simply can't find. There may also be a problem with the json structure.
</code></pre>",0,1,2021-05-08 11:02:59.267000 UTC,,2021-05-08 14:16:14.397000 UTC,0,python|json|azure-data-factory|azure-databricks,74,2020-11-15 17:53:54.283000 UTC,2021-06-08 11:45:00.867000 UTC,"London, UK",3,0,0,4,,,,,,[]
AWS Neptune - gremlin property order,"<p>i create the property for a vertex, as 
<code>g.addV('sth').property('p1', '1').property('p2', '2').property('p3', '3')</code></p>

<p>however when I query the vertex, like 
<code>g.V().hasLabel('sth').valueMap(true)</code>
or 
<code>g.V().hasLabel('sth').properties()</code></p>

<p>the order of the properties is lost, I get
p3, p1, p2, how can I make sure I can order the property like the order I created.</p>",1,0,2020-02-11 05:18:10.757000 UTC,,2020-02-11 11:07:00.803000 UTC,1,gremlin|amazon-neptune,276,2009-09-01 06:49:22.620000 UTC,2022-03-03 08:07:01.783000 UTC,"Shanghai, China",1317,18,2,72,,,,,,[]
Update existing records of parquet file in Azure,"<p>I am converting my table into parquet file format using Azure Data Factory. Performing query on parquet file using databricks for reporting. I want to update only existing records which are updated in original sql server table. Since I am performing it on very big table and daily I don't want to perform truncate and reload entire table as it will be costly.</p>
<p>Is there any way I can update those parquet file without performing truncate and reload operation.</p>",1,0,2021-09-13 09:11:41.483000 UTC,,,1,sql|azure|parquet|azure-databricks,159,2021-03-19 07:40:40.587000 UTC,2022-02-24 08:03:42.390000 UTC,India,11,0,0,7,,,,,,[]
Python/Neptune: getting a traversal object,"<p>I'm using the <code>client.Client</code> class from <code>gremlin_python.driver</code> to connect to AWS Neptune. See the following</p>
<pre><code>    def _prepare_request(method, url, *, data=None, params=None, headers=None, service='neptune-db'):
        _ = requests.Session()
        request = requests.Request(method=method, url=url, data=data, params=params, headers=headers)

        credentials = Session().get_credentials()
        frozen_creds = credentials.get_frozen_credentials()

        req = AWSRequest(method=method, url=url, data=data, params=params, headers=headers)
        SigV4Auth(frozen_creds, service, os.environ['AWS_REGION']).add_auth(req)
        prepared_iam_req = req.prepare()
        request.headers = dict(prepared_iam_req.headers)
        return request.prepare()

    # https
    http_protocol = 'https'
    uri = f'{http_protocol}://{self.host}:{self.port}/gremlin'
    request = _prepare_request('GET', uri)

    # wss
    ws_url = 'wss://{}:{}/gremlin'.format(self.host, self.port)
    ws_request = httpclient.HTTPRequest(ws_url, headers=dict(request.headers))
    self.conn = client.Client(ws_request, 'g')
</code></pre>
<p>Now my question how can I used the client.Client object from above to get a traversal object &quot;g&quot;.</p>
<p>There is a similar example at <a href=""https://pypi.org/project/gremlinpython/#description"" rel=""nofollow noreferrer"">https://pypi.org/project/gremlinpython/#description</a> showing this. But I can't use the DriverRemoteConnection in the above code.</p>
<pre><code>&gt;&gt;&gt; from gremlin_python.process.anonymous_traversal import traversal
&gt;&gt;&gt; from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection
&gt;&gt;&gt; g = traversal().withRemote(DriverRemoteConnection('ws://localhost:8182/gremlin','g'))
</code></pre>",1,1,2021-08-18 22:09:05.050000 UTC,,2021-08-18 22:49:04.010000 UTC,0,python|amazon-web-services|gremlin|amazon-neptune,84,2012-02-03 16:20:42.710000 UTC,2022-03-04 16:51:03.243000 UTC,,5628,119,6,508,,,,,,[]
"How should I manage ""reverting"" a branch done with bookmarks in mercurial?","<p>I have an open source project on bitbucket. Recently, I've been working on an experimental branch which I (for whatever reason) didn't make an actual branch for. Instead what I did was use bookmarks. </p>

<p>So I made two bookmarks at the same revision</p>

<ul>
<li>test --the new code I worked on that should now be abandoned(due to an experiment failure)</li>
<li>main -- the stable old code that works</li>
</ul>

<p>I worked in test. I also pushed from test to my server, which ended up switching the <code>tip</code> tag to the new unstable code, when I really would've rather it stayed at main. I ""switched"" back to the main bookmark by doing a <code>hg update main</code> and then committing an insignificant change. So, I pushed this with <code>hg push -f</code> and now my source control is ""correct"" on the server.</p>

<p>I know that there should be a cleaner way to ""switch"" branches. What should I do in the future for this kind of operation? </p>",2,1,2012-10-27 03:46:41.597000 UTC,1.0,,4,version-control|mercurial|dvcs,67,2009-02-23 04:34:53.537000 UTC,2020-04-14 16:57:59.873000 UTC,"Denver, CO, United States",59331,2896,255,4747,,,,,,[]
"spark trying to convert .Mf4 to save as .txt files, getting issue","<p>I have a spark code which reads .mf4 files and write as a .txt file. Also running this code in databricks.</p>
<p>Could you please suggest? Even after increasing the executor memory in data bricks in cluster, still having the issue.</p>
<pre><code>pip install asammdf

from pyspark import SparkContext
from pyspark.sql import SparkSession
from asammdf import MDF
import io
import os
import sys
from pyspark.sql.functions import col



os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable


spark = SparkSession \
    .builder \
    .appName(&quot;mdf4&quot;) \
    .getOrCreate()

sc = spark.sparkContext

def decodeBinary(val):
    file_stream = io.BytesIO(val)
    mdf = MDF(file_stream)
    location = mdf.whereis(test_1)
    return location
print(&quot;1&quot;)
input_hdfs_path_to_mdf4 = &quot;dbfs:/FileStore/inputmfd4/&quot;
channel_name = &quot;test_1&quot;
local_or_hdfs_output_path = &quot;dbfs:/FileStore/outputmfd4/opp4.txt&quot;
print(&quot;2&quot;)
raw_binary = sc.binaryFiles(input_hdfs_path_to_mdf4)
print(&quot;3&quot;)
decoded_binary = raw_binary.map(lambda r: r[1]).map(decodeBinary)
print(&quot;4&quot;)
decoded_binary.saveAsTextFile(local_or_hdfs_output_path)
print(&quot;5&quot;)
print(decoded_binary)
</code></pre>
<p>I am running this code in databricks , I have 5Gb mf4 file as a input. When I try to run small file there is no issue. But when I use this 5GB mf4 file getting</p>
<pre><code>Caused by org.apache.spark.api.python.PythonException: asammdf.blocks.utils.MdfException: &lt;_io.BytesIO object at 0x7efed84482c0&gt; is not a valid ASAM MDF file: magic header is b\xff\xd8\xff\xe1H\xe6Ex from command-2705692180399242 line 24 Full traceback below
Traceback (most recent call last)
</code></pre>",1,0,2022-01-31 15:59:22.773000 UTC,,,2,apache-spark|pyspark|azure-databricks|asammdf,52,2014-11-05 09:39:22.233000 UTC,2022-03-04 04:10:16.437000 UTC,,89,9,0,37,,,,,,[]
How to implement Gremlin query corresponding to Neo4j cypher query?,"<p>I have the following Cypher query(neo4j) and want to convert it to a Gremlin query.</p>
<pre><code>MATCH d=(a:Actor {id:&quot; + entityId +'})-[r:ACTING_IN*0..2]-(m) WITH d, 
RELATIONSHIPS(d) AS rels WHERE NONE (rel in r WHERE rel.Type = &quot;Hollywood&quot;) RETURN *
UNION
MATCH d=(aa:Actor{id: &quot; + entityId + &quot;})-[rel:PRODUCER_OF*0..2]-&gt;(mm:Movie) WITH d, 
RELATIONSHIPS(d) AS rels return *
</code></pre>
<p>Please help, Thanks :)</p>",1,0,2022-03-01 18:10:10.113000 UTC,,2022-03-01 18:19:02.577000 UTC,0,neo4j|cypher|gremlin|amazon-neptune|gremlinpython,26,2017-01-03 11:53:12.290000 UTC,2022-03-04 14:49:07.200000 UTC,"Gurugram, Haryana, India",410,57,1,69,,,,,,[]
Azure Databricks waiting for cluster to start when cluster is already running,"<p>I have an Azure Data Factory pipeline that runs few Azure Databricks Notebooks every day. I keep having this problem that the notebook instance keeps running for a long time. When I checked, I see &quot;Waiting for the cluster to start&quot; in the cell output. But, when I checked the cluster, its in a running state. I thought this might be temporary and waited for another 30 minutes and it the cell output was still &quot;Waiting for the cluster to start&quot;. Is there a way to force re-check cluster status inside the notebook? if Not, what's the correct way to handle this error</p>",0,6,2021-02-24 09:02:45.507000 UTC,1.0,,2,azure|azure-data-factory|azure-databricks,1302,2017-11-07 06:10:43.770000 UTC,2022-03-04 08:36:29.987000 UTC,"Hyderabad, Telangana, India",4447,1490,316,639,,,,,,[]
Spark write inserting certain records twice,"<p>I am currently using azure databricks to read from a large file and insert each record into a table.  My code looks like:</p>
<pre><code>var readNewFile = spark.emptyDataFrame

readNewFile = spark.read
  .option(&quot;header&quot;, header)
  .option(&quot;multiLine&quot;, &quot;true&quot;)
  .schema(schema)
  .csv(landingPath)

readNewFile = readNewFile.repartition(50)

var lastDf = readNewFile.withColumn(&quot;type_data&quot;, lit(type))

  val props = new Properties()

props.put(&quot;user&quot;, s&quot;${username}&quot;)
props.put(&quot;password&quot;, s&quot;${password}&quot;)
  
   lastDf.write
  .mode(SaveMode.Append) // &lt;--- Append to the existing table
  .jdbc(s&quot;jdbc:mysql://mydb&quot;,databaseSystem+&quot;.&quot;+tableName, props)

</code></pre>
<p>depending on my file size the number of records is always different after about 50k.  If my file size if 50k or less it inserts the exact amount.  If the file gets larger, certain records are inserted multiple times.  I added the repartition code to see if that will work, but it still fails sometimes.  I do not understand repartition enough to know how to fix this issue.  I was told to change the save mode to overwrite, but I feel like there has to be a better way to do this than to overwrite records that have already been inserted.</p>",0,2,2021-05-10 17:44:19.637000 UTC,,,0,scala|apache-spark|azure-databricks,64,2012-11-29 21:14:27.167000 UTC,2022-03-04 18:06:15.353000 UTC,,283,21,0,96,,,,,,[]
Flatten complex json using Databricks and ADF,"<p>I have following json which I have flattened partially using explode</p>
<pre><code>{
   &quot;result&quot;:[
      {
         &quot;employee&quot;:[
            {
               &quot;employeeType&quot;:{
                  &quot;name&quot;:&quot;[empName]&quot;,
                  &quot;displayName&quot;:&quot;theName&quot;
               },
               &quot;groupValue&quot;:&quot;value1&quot;
            },
            {
               &quot;employeeType&quot;:{
                  &quot;name&quot;:&quot;#bossName#&quot;,
                  &quot;displayName&quot;:&quot;theBoss&quot;
               },
               &quot;groupValue&quot;:[
                  {
                     &quot;id&quot;:&quot;1&quot;,
                     &quot;type&quot;:{
                        &quot;name&quot;:&quot;firstBoss&quot;,
                        &quot;displayName&quot;:&quot;CEO&quot;
                     },
                     &quot;name&quot;:&quot;Martha&quot;
                  },
                  {
                     &quot;id&quot;:&quot;2&quot;,
                     &quot;type&quot;:{
                        &quot;name&quot;:&quot;secondBoss&quot;,
                        &quot;displayName&quot;:&quot;cto&quot;
                     },
                     &quot;name&quot;:&quot;Alex&quot;
                  }
               ]
            }
         ]
      }
   ]
}
</code></pre>
<p>I need to get following fields:</p>
<pre><code>employeeType.name
groupValue
</code></pre>
<p>I am able to extract those fields and value. But, if name value starts with # like in <code>&quot;name&quot;:&quot;#bossName#&quot;,</code> I am getting groupValue as string from which I need to extract id and name.</p>
<pre><code>&quot;groupValue&quot;:[
                  {
                     &quot;id&quot;:&quot;1&quot;,
                     &quot;type&quot;:{
                        &quot;name&quot;:&quot;firstBoss&quot;,
                        &quot;displayName&quot;:&quot;CEO&quot;
                     },
                     &quot;name&quot;:&quot;Martha&quot;
                  },
                  {
                     &quot;id&quot;:&quot;2&quot;,
                     &quot;type&quot;:{
                        &quot;name&quot;:&quot;secondBoss&quot;,
                        &quot;displayName&quot;:&quot;cto&quot;
                     },
                     &quot;name&quot;:&quot;Alex&quot;
                  }
               ]
</code></pre>
<p>How to convert this string to json and get the values.</p>
<p>My code so far:</p>
<pre><code>from pyspark.sql.functions import *
db_flat = (df.select(explode(&quot;result.employee&quot;).alias(&quot;emp&quot;))
.withColumn(&quot;emp_name&quot;, col(emp.employeeType.name))
.withColumn(&quot;emp_val&quot;,col(&quot;emp.groupValue&quot;)).drop(&quot;emp&quot;))
</code></pre>
<p>How can I extract groupValue from db_flat and get id and name from it. Maybe use python panda library.</p>",1,4,2021-12-23 02:14:14.940000 UTC,,2021-12-23 13:02:46.287000 UTC,0,pandas|azure|azure-data-factory|azure-data-factory-2|azure-databricks,97,2013-05-23 16:37:01.490000 UTC,2022-03-06 01:56:13.083000 UTC,,1007,135,1,111,,,,,,[]
update the delta table in databricks with adding value to existing columns,"<p>I am having a piece of scala code which will take count of signals at 3 different stages with respect to an <strong>id_no</strong> and an <strong>identifier</strong>.
The output of the code will be as shown below.</p>
<pre><code>+-----+----------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+----------------------------+
|id_no|identifier|signal01_total|signal01_without_NaN|signal01_total_valid|signal02_total|signal02_without_NaN|signal02_total_valid|signal03_total|signal03_without_NaN|signal03_total_valid|load_timestamp              |
+-----+----------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+----------------------------+
|050  |ident01   |25            |23                  |20                  |45            |43                  |40                  |66            |60                  |55                  |2021-08-10T16:58:30.054+0000|
|051  |ident01   |78            |70                  |68                  |15            |14                  |14                  |10            |10                  |9                   |2021-08-10T16:58:30.054+0000|
|052  |ident01   |88            |88                  |86                  |75            |73                  |70                  |16            |13                  |13                  |2021-08-10T16:58:30.054+0000|
+-----+----------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+----------------------------+

</code></pre>
<p>There will be more than 100 signals, so that number of columns will be more than 300.</p>
<p>This dataframe is written to the delta table location as shown below.</p>
<pre><code>statisticsDf.write.format(&quot;delta&quot;).option(&quot;mergeSchema&quot;, &quot;true&quot;).mode(&quot;append&quot;).partitionBy(&quot;id_no&quot;).save(statsDestFolderPath)
</code></pre>
<p>For the next week data i have again executed this code and get the data as shown below.</p>
<pre><code>+-----+----------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+----------------------------+
|id_no|identifier|signal01_total|signal01_without_NaN|signal01_total_valid|signal02_total|signal02_without_NaN|signal02_total_valid|signal03_total|signal03_without_NaN|signal03_total_valid|load_timestamp              |
+-----+----------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+----------------------------+
|050  |ident01   |10            |8                   |7                   |15            |15                  |14                  |38            |38                  |37                  |2021-08-10T16:58:30.054+0000|
|051  |ident01   |10            |10                  |9                   |16            |15                  |15                  |30            |30                  |30                  |2021-08-10T16:58:30.054+0000|
|052  |ident01   |26            |24                  |24                  |24            |23                  |23                  |40            |38                  |36                  |2021-08-10T16:58:30.054+0000|
|053  |ident01   |25            |24                  |23                  |20            |19                  |19                  |25            |25                  |24                  |2021-08-10T16:58:30.054+0000|
+-----+----------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+----------------------------+
</code></pre>
<p>But the output I expect is if the <strong>id_no</strong> ,<strong>identifier</strong> and <strong>signal name</strong> is already present in the table, then it should add the count with existing data, If the <strong>id_no</strong>, <strong>identifier</strong> and <strong>signal name</strong> are new, then it should add to the final table.</p>
<p>The output I receive now is as shown below, where data gets appended for each run.</p>
<pre><code>+-----+----------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+----------------------------+
|id_no|identifier|signal01_total|signal01_without_NaN|signal01_total_valid|signal02_total|signal02_without_NaN|signal02_total_valid|signal03_total|signal03_without_NaN|signal03_total_valid|load_timestamp              |
+-----+----------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+----------------------------+
|050  |ident01   |25            |23                  |20                  |45            |43                  |40                  |66            |60                  |55                  |2021-08-10T16:58:30.054+0000|
|051  |ident01   |78            |70                  |68                  |15            |14                  |14                  |10            |10                  |9                   |2021-08-10T16:58:30.054+0000|
|052  |ident01   |88            |88                  |86                  |75            |73                  |70                  |16            |13                  |13                  |2021-08-10T16:58:30.054+0000|
|050  |ident01   |10            |8                   |7                   |15            |15                  |14                  |38            |38                  |37                  |2021-08-10T16:58:30.054+0000|
|051  |ident01   |10            |10                  |9                   |16            |15                  |15                  |30            |30                  |30                  |2021-08-10T16:58:30.054+0000|
|052  |ident01   |26            |24                  |24                  |24            |23                  |23                  |40            |38                  |36                  |2021-08-10T16:58:30.054+0000|
|053  |ident01   |25            |24                  |23                  |20            |19                  |19                  |25            |25                  |24                  |2021-08-10T16:58:30.054+0000|
+-----+----------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+----------------------------+
</code></pre>
<p>But I am expecting the output as shown below.</p>
<pre><code>+-----+----------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+----------------------------+
|id_no|identifier|signal01_total|signal01_without_NaN|signal01_total_valid|signal02_total|signal02_without_NaN|signal02_total_valid|signal03_total|signal03_without_NaN|signal03_total_valid|load_timestamp              |
+-----+----------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+----------------------------+
|050  |ident01   |35            |31                  |27                  |60            |58                  |54                  |38            |38                  |37                  |2021-08-10T16:58:30.054+0000|
|051  |ident01   |88            |80                  |77                  |31            |29                  |19                  |30            |30                  |30                  |2021-08-10T16:58:30.054+0000|
|052  |ident01   |114           |102                 |110                 |99            |96                  |93                  |40            |38                  |36                  |2021-08-10T16:58:30.054+0000|
|053  |ident01   |25            |24                  |23                  |20            |19                  |19                  |25            |25                  |24                  |2021-08-10T16:58:30.054+0000|
+-----+----------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------+--------------------+--------------------+----------------------------+
</code></pre>
<p>Got a hint using upsert command as below.</p>
<pre><code>val updatesDF = ...  // define the updates DataFrame[id_no, identifier, sig01_total, sig01_NaN, sig01_final, sig02_total,.......]

DeltaTable.forPath(spark, &quot;/data/events/&quot;)
  .as(&quot;events&quot;)
  .merge(
    updatesDF.as(&quot;updates&quot;),
    &quot;events.id_no = updates.id_no&quot; &amp;&amp;
    &quot;events.identifier = updates.identifier&quot;)
  .whenMatched
  .updateExpr(
    Map(&quot;sig01_total&quot; -&gt; &quot;updates.sig01_total&quot;
                      -&gt; 
                      -&gt;........))
  .whenNotMatched
  .insertExpr(
    Map(
      &quot;id_no&quot; -&gt; &quot;updates.id_no&quot;,
      &quot;identifier&quot; -&gt; &quot;updates.identifier&quot;,
      &quot;sig01_total&quot; -&gt; &quot;updates.sig01_total&quot;
                    -&gt;
                    -&gt;
                     .....))
  .execute()
</code></pre>
<p>But in my case the number of columns may vary each time, if a new signal is added to the id, then we have to add the same. If one of the signal for existing id is not available for current week process, that signal value alone should keep same and rest should be updated.</p>
<p>Is there any option to achieve this requirement using delta table merge or by updating the above code or any other ways?
Any leads appreciated!</p>",1,0,2021-08-11 07:51:46.667000 UTC,,2021-08-12 08:17:24.967000 UTC,0,scala|apache-spark|azure-databricks|delta,1594,2017-04-04 05:54:01.927000 UTC,2022-03-05 05:59:02.213000 UTC,,743,45,1,284,,,,,,[]
How do you prevent your Mercurial (or Git) branches from being locked up?,"<p>I want to throw this scenario out there and see what the most objective, vanilla-Mercurial way to fix this would be.</p>

<p>Suppose I have the following branches in my centralized Mercurial repository for my centralized, non-distributed web-app:</p>

<pre><code>repository
    default
    feature1
    feature2
    bugs
</code></pre>

<p>Suppose ten developers have committed fixes to the 'bugs' branch and pushed. Now let's say it comes time to release. So, I do some final tests, and suddenly, I notice a problem with a bug fix--and this problem is going to take a few additional hours to fix. So, now I have super critical bug fixes that have to be released, but the 'bugs' branch is locked up, and I can't merge into the 'default' branch and release because the half-fixed bug will also be released, and I can't let that happen.</p>

<p>So what do I do?</p>

<p>Should I revert the change in the bugs branch, merge bugs into default, and release? What if there are ten commits associated with this particular problem-bug-fix--find and revert all those? That sounds hairy, but let's say I do that. What about when the developer who has to fix the problem-bug-fix pulls and updates his repository? His changes will all be reverted. He COULD un-revert the revert…is that the way to go?</p>

<p>Another option would be to do some kind of cherry picking and merge just the changes I want to release from the 'bugs' branch, minus the problem-bug-fix commits, into the 'default' branch. But this sounds even hairier than the previously-proposed solution.</p>

<p>What do you all do?</p>",1,0,2010-05-18 18:57:16.753000 UTC,,,2,git|mercurial|dvcs|release-management,130,2009-03-28 02:01:50.307000 UTC,2022-03-05 02:36:25.400000 UTC,"Grand Rapids, MI, USA",19805,1276,42,1693,,,,,,[]
cannot get selenium webdriver to work in azure databricks,"<p>We have some python scripts that scrape websites and work well. Now we want to do this in Azure Databricks.
We thought we had the solution to do this with the following post in the Databricks forum, but unfortunately, it doesn't work.
(<a href=""https://forums.databricks.com/questions/15480/how-to-add-webdriver-for-selenium-in-databricks.html?childToView=21347#answer-21347"" rel=""nofollow noreferrer"">https://forums.databricks.com/questions/15480/how-to-add-webdriver-for-selenium-in-databricks.html?childToView=21347#answer-21347</a>)</p>

<p>The error we get after running the last bit of code is :
WebDriverException: Message: unknown error: cannot find Chrome binary (Driver info: chromedriver=73.0.3683.68 (47787ec04b6e38e22703e856e101e840b65afe72),platform=Linux 4.15.0-1050-azure x86_64)</p>

<p>The last bit of code looks like this:</p>

<pre><code>    %py

    from selenium import webdriver

    chrome_options = webdriver.ChromeOptions()

    chrome_options.add_argument('--no-sandbox')

    chrome_options.add_argument('--headless')

    chrome_options.add_argument('--disable-dev-shm-usage')

    chrome_driver = ""/tmp/chromedriver/chromedriver""

    driver = webdriver.Chrome(chrome_driver, 
    chrome_options=chrome_options)

    driver.get(""https://www.google.com"")
</code></pre>

<p>I have found a post where I have to give the location of the binary:
<a href=""https://stackoverflow.com/questions/46026987/selenium-gives-selenium-common-exceptions-webdriverexception-message-unknown"">Selenium gives &quot;selenium.common.exceptions.WebDriverException: Message: unknown error: cannot find Chrome binary&quot; on Mac</a></p>

<pre><code>    options.binary_location = ""/Applications/Google 
    Chrome.app/Contents/MacOS/Google Chrome""
</code></pre>

<p>But I don't know the file location in Azure Databricks for this binary.</p>",1,1,2019-09-12 10:05:53.213000 UTC,1.0,2020-03-16 05:49:30.073000 UTC,3,binary|selenium-chromedriver|azure-databricks,1472,2016-07-29 12:16:09.277000 UTC,2022-02-01 09:53:13.433000 UTC,,243,0,0,27,,,,,,[]
Apache Spark : Including the partition columns in the parquet file,"<p>I have a huge data set partitioned by month. I am able to write the parquet files using the spark.write.parquet method. It works fine when trying to read using the spark itself. 
Parquet files don't have the partition columns and it is represented by the folders they reside. When trying to read the parquet files using external programs (like polybase), we cannot tell the month to which the file belongs to. </p>

<p>Is there any way to force spark to include the partition columns in the parquet files? Is there any other alternatives?</p>",0,2,2019-03-10 09:40:40.567000 UTC,1.0,,0,apache-spark|apache-spark-sql|azure-databricks,282,2015-10-16 21:46:59.017000 UTC,2022-01-11 03:57:26.417000 UTC,,225,8,0,20,,,,,,[]
Databricks notebook command skipped only when scheduled as a job,"<p>I have a databricks notebook with some code (Python) to upload a file from dbfs to a SharePoint location. The notebook runs correctly when executed stand-alone, and the file is uploaded, but when I try to schedule it using ADF or a Databricks job, the command for the SharePoint upload gets skipped.</p>
<p>Other commands are executed okay. I'm using O365 REST Python client for the Sharepoint upload. I'm not sure if my choice of library is causing this to happen.</p>
<p>Has anyone faced something similar?</p>",1,2,2021-08-23 11:06:45.900000 UTC,1.0,,1,python|sharepoint|azure-databricks|office365-rest-client,339,2014-06-29 11:07:50.943000 UTC,2022-03-03 13:52:38.363000 UTC,,93,3,0,9,,,,,,[]
Move first 5 files out of 100 from one directory to another using Python code,"<p>Is it possible to move the first 5 files from one directory to another using python code?
I have to run the code on the data bricks notebook.</p>
<p>The scenario is: I have to pick any first 5 files present in the directory (total files is 100) and move those 5 files to another directory, this process will be repeated till all the files moved to another folder.</p>",1,4,2021-04-07 05:22:30.017000 UTC,,2021-04-07 05:39:59.797000 UTC,0,python|azure-databricks,593,2020-01-30 05:08:22.337000 UTC,2022-03-04 08:17:44.810000 UTC,,121,17,0,28,,,,,,[]
DVCS - tag multiple revisions,"<p>I have in mind such release workflow:<br>
 - One branch is a <code>trunk</code> branch - it may has sub-branches for features and other purposes.<br>
 - Second branch is <code>stable</code> - it represents releases for production.<br>
 - Revisions of <code>trunk</code> - are tagged for versions of releases. If I want to make a release - I must nominate one or multiple changesets for release version and then merge them into <code>stable</code>, and end final revision of this multi-merge will be a new <code>stable</code> version.<br></p>

<p>There is an options - use some external tool for track to which version revisions are belong, or write this information in commit-messages - but I don't like them, because I want to store this information in DVCS, and not depend on any external software for release management.</p>

<p>And so my questions is:<br>
 - Is it good scheme at all?<br>
 - Is there such tool for any popular DVSC, for mass-tagging of revisions?<br></p>",2,1,2013-09-02 11:13:14.397000 UTC,,2013-09-03 01:22:58.903000 UTC,0,git|version-control|mercurial|merge|dvcs,64,2012-11-12 16:07:20.193000 UTC,2022-03-03 12:33:42.817000 UTC,,10532,754,9,2794,,,,,,[]
How to add a validation in azure data factory pipeline to check file size?,"<p>I have multiple data sources I want to add a validation in azure data factory before loading into tables it should check for file size so that it is not empty. So if the file size is more than 10 kb or if it is not empty loading should start and if it is empty then loading should not start.
I checked validation activity in Azure Data Factory but it is not showing size for multiple files in a folder.
Any suggestions appreciated basically if I can add any python notebook for this validation will also do.</p>",2,0,2020-09-29 17:49:50.590000 UTC,1.0,,1,azure|pyspark|azure-data-factory|azure-data-lake|azure-databricks,1826,2019-10-24 09:30:32.113000 UTC,2022-02-02 13:58:02.710000 UTC,"Bangalore, Karnataka, India",53,7,0,80,,,,,,[]
"Is there a SparkR function equivalent to unique and orderBy from R, that brings all the columns?","<p>I'm one month old in the Data world and my goal is to refactor existing local R scripts to work with SparkR on Databricks.</p>
<p>This is the R code:</p>
<pre><code>minmaxAcctDates &lt;- intLoadFiles(&quot;Accounts_BalanceEOD&quot;, monthID)
minmaxAcctDates$CUSTOMER_NUMBER &lt;- as.integer(minmaxAcctDates$CUSTOMER_NUMBER)
  
AccountsFilteredMin &lt;- unique(setDT(minmaxAcctDates)[order(BOOK_BALANCE_LCY)], by = &quot;CUSTOMER_NUMBER&quot;) 
AccountsFilteredMax &lt;- unique(setDT(minmaxAcctDates)[order(BOOK_BALANCE_LCY, decreasing = TRUE)], by = &quot;CUSTOMER_NUMBER&quot;)
  
AccountsFilteredMin$MIN_AC_BAL_DATE&lt;-AccountsFilteredMin$CALENDAR_DATE
AccountsFilteredMax$MAX_AC_BAL_DATE&lt;-AccountsFilteredMax$CALENDAR_DATE
  
AccountsFilteredMin &lt;- AccountsFilteredMin[, c(&quot;CUSTOMER_NUMBER&quot;, &quot;MIN_AC_BAL_DATE&quot;)]
AccountsFilteredMax &lt;- AccountsFilteredMax[, c(&quot;CUSTOMER_NUMBER&quot;, &quot;MAX_AC_BAL_DATE&quot;)]
</code></pre>
<p>What I tried to reproduce in SparkR:</p>
<pre><code>minmaxAcctDates &lt;- read.df(&quot;abfss://x@adlsstorage.dfs.core.windows.net/x/Accounts_BalanceEOD.csv&quot;, source = &quot;csv&quot;, header=&quot;true&quot;, inferSchema = &quot;true&quot;)

AccountsFilteredMin &lt;- (SparkR::distinct(minmaxAcctDates))
AccountsFilteredMin = groupBy(minmaxAcctDates, minmaxAcctDates$CUSTOMER_NUMBER) %&gt;% agg(min(minmaxAcctDates$BOOK_BALANCE_LCY))
SparkR::collect(AccountsFilteredMin)
AccountsFilteredMin = join(AccountsFilteredMin, minmaxAcctDates)
dropDuplicates(AccountsFilteredMax, &quot;CUSTOMER_NUMBER&quot;)

AccountsFilteredMax &lt;- (SparkR::distinct(minmaxAcctDates))
AccountsFilteredMax = groupBy(minmaxAcctDates, minmaxAcctDates$CUSTOMER_NUMBER) %&gt;% agg(max(minmaxAcctDates$BOOK_BALANCE_LCY))
SparkR::collect(AccountsFilteredMax)
AccountsFilteredMax = join(AccountsFilteredMax, minmaxAcctDates)
dropDuplicates(AccountsFilteredMax, &quot;CUSTOMER_NUMBER&quot;)

AccountsFilteredMin &lt;- AccountsFilteredMin[, c(&quot;CUSTOMER_NUMBER&quot;, &quot;MIN_AC_BAL_DATE&quot;)]
AccountsFilteredMax &lt;- AccountsFilteredMax[, c(&quot;CUSTOMER_NUMBER&quot;, &quot;MAX_AC_BAL_DATE&quot;)]
</code></pre>
<p>From trial and error, the issues that I am facing are:</p>
<ul>
<li>Duplicate columns</li>
<li>Ambiguous column for CUSTOMER_NUMBER</li>
</ul>
<p>Grateful if I could receive some help/guidance please, thanks.</p>",0,0,2021-09-15 06:00:30.900000 UTC,,,1,r|dataframe|data-science|azure-databricks|sparkr,41,2016-01-25 11:33:31.067000 UTC,2022-03-02 11:32:43.823000 UTC,,36,0,0,21,,,,,,[]
Why is mercurial dumb when merging? How can I make pulling/merging changes simpler?,"<p>I just started to use Mercurial and I think I'm trying to do something very simple, something that should be quite typical, but I'm stumped on why it's so complicated, and why it doesn't just work the way it should (IMO). </p>

<p>I share some a repository with a friend, he makes some changes and checks in several files and pushes them.  Now in svn I'm used to just updating my working copy and getting his changes, no hassle.  But with mercurial apparently I have to merge.  What I don't get is: shouldn't mercurial be smart enough to figure out that if my friend made the most recent changes, and I haven't touched the files, that it should just use his version of the file??  Apparently it can't figure that out, and instead it tries to merge the files which utterly fails (actually I have installed Beyond Compare which automatically opened, so I can't blame mercurial entirely for the bad merge).  </p>

<p>At any rate, I don't know why it even has to ""merge"" the files when it's obvious (to me) that it should just have taken the remote (i.e. most recent) changes.  Am I doing anything wrong in how I'm using the tool, or is there anything I can do to make it work in a simpler way (the way I'm used to it just working in subversion)... is there any configuration settings, any tips on command line flags I can use to have it work better?</p>",1,7,2011-02-24 05:01:59.307000 UTC,1.0,,2,mercurial|merge|dvcs,779,2010-06-21 20:11:22.080000 UTC,2022-03-05 03:44:18.923000 UTC,,1638,432,1,154,,,,,,[]
distributed source control with working directory model?,<p>I'm looking for a distributed source control system that will let me do a subversion type checkout. I have several different projects and would like to combine them into one repository. I'd like to be able to have working directorys though much as subversion does so each project doesn't have to be it's own repository. From looking at Mercurial this is not possible since  any files have to be stored in a repository in order to track there changes. If anyone knows of a distributed source control system that will let me use working directorys I'd appreciate it.</p>,5,0,2009-02-03 17:58:29.747000 UTC,,2010-01-19 22:00:04.230000 UTC,0,version-control|dvcs,213,2008-09-17 02:29:39.840000 UTC,2022-02-28 03:06:39.317000 UTC,"Detroit, MI, United States",38731,385,43,5439,,,,,,[]
Get argument from Dropdown menu of Notebook,"<p>I create a 'dropdown' menu in the notebook and tried to use it to update the graph from display function.</p>

<p>I can use it in the filter function.</p>

<pre class=""lang-py prettyprint-override""><code>display(df.filter(df.year == getArgument(""Year""))
</code></pre>

<p>However, when I tried to use it in the select statement, it started to not work.</p>

<pre class=""lang-py prettyprint-override""><code>display(df.select(getArgument(""Year""))
</code></pre>

<p>Do you know why?</p>",0,3,2020-04-16 16:24:19.193000 UTC,,,0,pyspark|pyspark-sql|azure-databricks|pyspark-dataframes,88,2016-01-19 03:14:42.883000 UTC,2021-02-26 21:21:27.440000 UTC,,335,0,0,48,,,,,,[]
Move Files from Azure Files to ADLS Gen 2 and Back using Databricks,"<p>I have a Databricks process which currently generate a bunch of text files which gets stored in Azure Files. These files need to be moved to ADLS Gen 2 on a scheduled basis and back to File Share.</p>
<p>How this can be achieved using Databricks?</p>",1,3,2021-08-17 01:36:46.080000 UTC,,,1,azure|azure-databricks|azure-data-lake-gen2|azure-files,121,2021-07-29 17:00:19.153000 UTC,2021-09-22 00:28:58.580000 UTC,,45,3,0,1,,,,,,[]
PYODBC connection going to sleep mode,"<p>I am trying to execute stored procedure from data bricks by using PYODBC connection, after all transactions happened, status is going to sleep mode. Please help me on that,
I tried all the possibilities auto-commit and connection timeout etc. but nothing is working.</p>
<pre><code>import pyodbc
import datetime
username = &quot;usrname&quot;
password = &quot;password&quot;
server = &quot;server&quot;
database_name = &quot;dbname&quot;
port = &quot;1433&quot;
conn=pyodbc.connect('Driver={ODBC Driver 17 for SQL server};SERVER=tcp:'+server+','+port+';DATABASE='+ database_name +';UID='+ username +';PWD='+ password)
#conn.timeout = 600
cursor=conn.cursor()
# conn.autocommit = True

sql = &quot;set nocount on; exec proc_name&quot;
print(&quot;Connection Started at &quot;+str(datetime.datetime.now()))
cursor.execute(sql)
print(&quot;Connection closed at &quot;+str(datetime.datetime.now()))
conn.commit()
cursor.close()
conn.close()

print(datetime.datetime.now())
</code></pre>
<p>Notebook is still in Running process please have a look on below pic</p>
<p><a href=""https://i.stack.imgur.com/nPNXk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nPNXk.png"" alt=""enter image description here"" /></a></p>
<p>Status from database for that SPID initially status is RUNNABLE once all transaction got completed from proc(insertion, delete, update of data) status is updating as sleeping mode
<a href=""https://i.stack.imgur.com/mq9uR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mq9uR.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/RLtbL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RLtbL.png"" alt=""enter image description here"" /></a></p>
<p>because of this sleeping status data bricks note book is not completing, it's keep on running. please help me out from issue. Thanks in Advance.</p>",1,0,2022-02-17 10:50:36.037000 UTC,,,0,python-3.x|azure-sql-database|pyodbc|azure-databricks|azure-sql-server,36,2018-11-21 07:07:54.753000 UTC,2022-03-03 10:08:28.680000 UTC,,97,1,0,15,,,,,,[]
Create an object with property names in azure databricks with python,"<p>I am new azure databricks and python too.
I am reading excel which has column names/path of properties (JSON), I have to create an object using those properties</p>
<pre><code>import json


#Load Data from DBFS
mappingFilePath = &quot;dbfs:/FileStore/tables/MappingNew.xlsx&quot;
#flags required for reading the excel
isHeaderOn = &quot;true&quot;
isInferSchemaOn = &quot;false&quot;
#sheet address in excel
mappingAddress = &quot;'Sheet1'!A1&quot;
#read excelfile
finalMapping = {};
targetArr = []
mappingDF = (spark.read.format(&quot;com.crealytics.spark.excel&quot;)
    .option(&quot;header&quot;, isHeaderOn)
    .option(&quot;inferSchema&quot;, isInferSchemaOn)
    .option(&quot;treatEmptyValuesAsNulls&quot;, &quot;false&quot;)
    .option(&quot;dataAddress&quot;, mappingAddress)
    .load(mappingFilePath))

mapping_json = mappingDF.toJSON()
for map1 in mapping_json.collect():
                mapping = json.loads(map1)
                if 'Target' in mapping and 'Source' in mapping:
                    src = mapping['Source'].replace(&quot; &quot;, &quot;_&quot;)
                    target = mapping['Target']
                    propertyNames = target.split('.')
                    #print(propertyNames)
                    #temp = {}
                    #for prop in propertyNames:
                        #print(prop)
                    #finalMapping[target] = target
print(finalMapping)
</code></pre>
<p>Here</p>
<pre class=""lang-py prettyprint-override""><code>mapping_json = {'UniqueId', 'Description', 'Info[0].value',
 'Info[1].value', 'categoryInfo[0].category.code', 
 'categoryInfo[0].category.name'};
</code></pre>
<p>And Output should be</p>
<pre class=""lang-json prettyprint-override""><code>{'UniqueId': &quot;&quot;, 'Description' : &quot;&quot;, 
'Info' : [{'value' : &quot;&quot;}, {'value' : &quot;&quot;} ], 
'categoryInfo' : [{'category' : { 'code' : &quot;&quot;, 'name' : &quot;&quot; }}]}
</code></pre>",0,0,2022-01-19 14:13:04.237000 UTC,,2022-01-19 18:20:43.697000 UTC,0,python|azure-databricks,19,2015-12-31 11:35:08.190000 UTC,2022-03-05 08:53:33.627000 UTC,,692,59,13,88,,,,,,[]
What's so great about git?,"<p>For someone coming from a more conventional VCS background (CVS/SVN), what are the most compelling reasons to learn and migrate to git?</p>

<p>Please comment upon a team's required technical ability in order to make git work.  I've seen smart people climb the learning curve and still lose some hair over it.  Can anyone climb this curve, or is git not for all teams?</p>

<p>Of course I also want to hear about functional benefits, tool support, integration other systems (CI, etc)...</p>

<p><em>(This seems like an obvious question, but I didn't find a duplicate of this despite a few searches)</em></p>

<p><strong>EDIT</strong> Links to good resources also appreciated.</p>",8,0,2009-02-13 08:23:26.420000 UTC,11.0,2009-06-26 04:44:55.240000 UTC,32,git|version-control|dvcs,4651,2008-10-03 15:02:28.817000 UTC,2022-03-03 00:08:26.300000 UTC,"Melbourne, Australia",280610,5125,171,13365,,,,,,[]
What is the proper way to access SparkSession from worker,"<p>I just realized that I am calling the following code many times and that seems not right:</p>

<pre><code>spark = SparkSession.builder.getOrCreate()
</code></pre>

<p>Some steps of my code run on worker context. So the spark session created while in the driver is not available to the worker.</p>

<p>I know that the getOrCreate() method checks if there is any global session available for use, so it may not always create a new one, but this forces me to ask for the spark session again and again.</p>

<p>I checked around and I saw people sending the session as argument of UDF or foreach functions, but couldn't find much about it.</p>

<p>So, what is the proper way to access spark while inside the worker?</p>

<p><strong>EDIT: Added my use case below / Changed steps details</strong></p>

<p>Maybe my use case gets clearer with the list below:</p>

<pre><code> 1. Get data from eventhub. 
 2. Save data to delta table
 3. Query distinct IDs
 4. Foreach ID
  4.1. Query other database to get info about the body based on the ID
  4.2. For each row using UDF function (CSV)
   4.2.1. Transform csv into dataframe and return list of tuples
  4.3. Merge all dataframes using flatMap on the rows
  4.4. Write to somewhere
</code></pre>

<p>I am receiving messages from eventhub and each message has a CSV body and an ID.</p>

<p>Each message may be completely different from another and if so, at the end of all, I am going to save each one in a different DW table.</p>

<p>So in order to do that I chose the following strategy: </p>

<p>First, Save all the CSV body and IDs in a generic Delta table, just like they came (I am partitioning by ID)</p>

<p>Now I can query all the data related to each ID one by one and this makes possible to process all the data related to that ID in a single batch.</p>

<p>When I query all body data of a specific ID, I have X rows, and I need to iterate over them transforming the CSV body of each row to a Dataframe. </p>

<p>After that, I merge all the dataframes into one and save it to the right table in DW.</p>

<p>For each dinstinct ID, I use spark to get the info about the body, and each read of CSV or write to DW is already beeing performed from inside a worker.</p>

<p><strong>EDIT: Added some code for the people</strong></p>

<p>4 Foreach ID</p>

<pre><code># dfSdIds is a dataframe containing all distinct ids that I want to iterate over
dfSdIds.rdd.foreach(SaveAggregatedBodyRows)
</code></pre>

<p>4.2 For each row using UDF function (CSV)</p>

<pre><code># mapping: is a json structure that is going to generate the dataframe schema of the CSV inside the udf function
# argSchema: is the expected udf returning structure ArrayType(StructType(...))
def SaveAggregatedBodyRows(row): 

...

spark = SparkSession.builder.getOrCreate()
dfCsvBody = spark.sql('select body from delta.`/dbfs/' + allInOneLocation + '` where SdIds = {}'.format(sdid))

UdfConvertCSVToDF = udf(lambda body, mapping: ConvertCSVToDF(body, mapping), argSchema)
dfConvertedBody = dfCsvBody.withColumn('body', UdfConvertCSVToDF(dfCsvBody.body, lit(mapping)))
</code></pre>

<p>4.2.1 Transform csv into dataframe and return list of tuples</p>

<pre><code>def ConvertCSVToDF(body, mapping): 

...

spark = SparkSession.builder.getOrCreate()           
csvData = spark.sparkContext.parallelize(splittedBody)

df = (spark.read
.option(""header"", True)
.option(""delimiter"", delimiter)
.option(""quote"", quote)
.option(""nullValue"", nullValue)
.schema(schema)
.csv(csvData))

return list(map(tuple, df.select('*').collect()))
</code></pre>

<p>4.3 Merge all dataframes using flatMap on the rows</p>

<pre><code># mapSchema is the same as argSchema but without ArrayType
flatRdd = dfConvertedBody.rdd.flatMap(lambda x: x).flatMap(lambda x: x)      
dfMerged = flatRdd.toDF(mapSchema)
</code></pre>

<p>4.4 Write to somewhere</p>

<pre><code>(dfMerged.write
   .format(savingFileFormat)
   .mode(""append"")
   .option(""checkpointLocation"", checkpointLocation)
   .save(tableLocation)) 
</code></pre>

<p>I know there is a lot to improve in this code, but I am doing as I am learning pyspark.</p>

<p>This question has become way more than I expected, but the point of it is that I called </p>

<pre><code>spark = SparkSession.builder.getOrCreate() 
</code></pre>

<p>at the driver, inside the method SaveAggregatedBodyRows AND inside the method ConvertCSVToDF. </p>

<p>People said it wouldn't work, but it is.</p>",0,14,2019-07-05 22:30:44.820000 UTC,,2019-07-12 20:39:58.893000 UTC,1,python-3.x|apache-spark|pyspark|azure-databricks,1755,2015-09-15 08:29:24.193000 UTC,2021-08-26 20:22:38.807000 UTC,"São Paulo, State of São Paulo, Brazil",328,306,3,79,,,,,,[]
How to make mercurial sort the output of `hg log -G` by commit date?,"<p>How can I make mercurial order the graph log output of <code>hg log -G</code> by commit date?</p>

<p>Using <code>hg log -r 'sort(all(), -date)'</code> works without the graph log option as answered in [<a href=""https://stackoverflow.com/questions/47269015/how-to-make-mercurial-sort-the-output-of-hg-log-by-commit-date"">1</a>], but that solution doesn't seem to have any effect on the order of the graph log.</p>

<p>I'm looking for a mercurial equivalent of <code>git log --graph --date-order</code>.</p>",0,1,2017-11-13 18:19:41.980000 UTC,,,3,mercurial|dvcs,139,2014-02-05 14:04:47.243000 UTC,2022-03-03 20:25:08.123000 UTC,"Alexandria, VA, USA",459,470,10,18,,,,,,[]
Preserve detailed Gremlin error message when running Gremlin query with eval(),"<p>in my script I do the following:<br>
<code>eval(""query"")</code><br>
and get:<br>
<code>unexpected EOF while parsing (&lt;string&gt;, line 1)</code></p>

<p>in Jupyter i do:<br>
<code>query</code><br>
and get:  </p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""false"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>GremlinServerError: 499: {""requestId"":""2602387d-f9a1-4478-a90d-3612d1943b71"",""code"":""ConstraintViolationException"",""detailedMessage"":""Vertex with id already exists: ba48297665fc3da684627c0fcb3bb1fd6738e7ad8eb8768528123904b240aaa7b21f66624de1fea84c87e5e2707995fe52435f1fb5fc4c2f9eaf85a605c6877a""}</code></pre>
</div>
</div>
</p>

<p>Is there a way to <strong>preserve the detailed error message</strong> whilst doing Gremlin queries with the <code>eval(""querystring"")</code> approach?<br>
I need to concatenate many strings into one query, that is why.<br>
Also, the detailed error message allows me to catch the errors like this <code>ConstraintViolationException</code>   </p>

<p><strong>Details:</strong><br>
I am interacting with Neptune with Python.<br>
I have this at the beginning of my script:<br>
<code>from gremlin_python import statics</code><br>
<code>statics.load_statics(globals())</code><br>
<code>from gremlin_python.structure.graph import Graph</code><br>
<code>from gremlin_python.process.graph_traversal import __</code><br>
<code>from gremlin_python.process.strategies import *</code><br>
<code>from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection</code><br>
which is from the official documentation on how to connect with Python.  </p>",1,2,2018-10-24 05:49:25.427000 UTC,,2018-10-24 09:52:28.493000 UTC,0,gremlin|gremlin-server|amazon-neptune,191,2014-09-04 10:50:32.543000 UTC,2022-02-28 05:27:58.523000 UTC,Singapore,3641,2251,46,484,,,,,,[]
How to read excel as a pyspark dataframe,"<p>I am able to read all the files and formats like csv, parquet, delta from adls2 account with oauth2 cred.</p>
<p>However when I am trying to read excel file like below,</p>
<pre><code>df = spark.read.format(&quot;com.crealytics.spark.excel&quot;) \
.option(&quot;header&quot;, &quot;true&quot;) \
.option(&quot;inferSchema&quot;, &quot;true&quot;) \
.option(&quot;dataAddress&quot;, &quot;'excel sheet name'!A1&quot;) \
.load(filepath)
</code></pre>
<p>I am getting below error</p>
<blockquote>
<p>Failure to initialize configurationInvalid configuration value detected for fs.azure.account.key</p>
</blockquote>
<p>Note: I have installed external library &quot;com.crealytics:spark-excel_2.11:0.12.2&quot; to read excel as a dataframe.</p>
<p>Can anyone help me with error here?</p>",1,3,2021-08-02 11:38:36.620000 UTC,1.0,,0,azure|pyspark|azure-databricks|azure-authentication|azure-data-lake-gen2,167,2020-09-15 09:30:00.583000 UTC,2022-02-03 07:01:59.150000 UTC,,35,3,0,22,,,,,,[]
Grant Access to Azure Databricks using Azure Devops,"<p>I am fairly new to Azure Devops and Azure Databricks.</p>
<p>I have created Azure Databricks workspace using Azure Devops CI/CD Pipeline.
Now I am looking for a solution to Add Users to Azure Databricks workspace using DevOps Pipeline.</p>",1,0,2021-10-31 16:48:51.003000 UTC,,2021-10-31 18:16:24.630000 UTC,2,azure-devops|azure-pipelines|azure-databricks,192,2013-02-03 14:24:10.400000 UTC,2022-02-18 11:23:30.937000 UTC,"Gurugram, Haryana, India",49,0,0,13,,,,,,[]
Lock prevents from pushing in local mercurial repo,"<p>Im trying to use mercurial for a project that will be developed on some disconnected PCs. I managed to setup the initial repo, and commit. But when I try to push, I get this message: </p>

<pre><code>D:\work\havana2059-dvcs&gt;hg push
pushing to D:\work\havana2059-dvcs
searching for changes
no changes found
waiting for lock on repository D:\work\havana2059-dvcs held by 'serenity:4816'
</code></pre>

<p>I have to interrupt it, because it simply stays there. How can I solve this? </p>",1,1,2017-02-02 15:33:56.133000 UTC,,,0,mercurial|dvcs,376,2013-10-28 19:17:21.297000 UTC,2017-12-07 17:01:43.757000 UTC,,1,0,0,2,,,,,,[]
Read files from multiple folders from ADLS gen2 storage via databricks and create single target file,<p>I'm using databricks service for analysis. I have built a connection with ADLS gen2 storage and create a mountpoint and now that container contains multiple folder for years and months  and having parquet files for each month inside month folders. I have to read all those files and create a single target file with complete months data. How do I achieve it can anyone suggest?</p>,1,3,2021-11-03 19:34:21.347000 UTC,,,1,python|azure-databricks|azure-data-lake,95,2015-02-17 10:58:13.730000 UTC,2022-02-24 08:34:44.460000 UTC,,41,0,0,6,,,,,,[]
how to copy xml fiiles amazon from s3 to neptune,"<p>I am trying to move xml files from s3 to Neptune with bellow steps.</p>

<p><a href=""https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html</a></p>

<p>from the below path I have followed below topic to move the data from s3 to neptune
<a href=""https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-tutorial-IAM.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-tutorial-IAM.html</a></p>

<p><strong>•Creating an IAM Role to Allow Amazon Neptune to Access Amazon S3 resources</strong>
I have created a role name ""NeptuneLoadfroms3"" as suggested in above topic</p>

<p><strong>•Adding the IAM Role to an Amazon Neptune Cluster</strong>
I have followed the steps as suggested but in step5 mentioned that we need to add the role which created in previous topic , but I could not find the role name which was created Neputneloadfroms3.
It is showing role name is AWSServiceRoleForRDS</p>

<p><strong>•Creating the Amazon S3 VPC Endpoint</strong></p>

<p>Can some one help me to understand the role assignment  process .</p>

<p>My final goal is to copy xml file from s3 to Neptune.</p>

<p>Regards,
SP</p>",1,1,2018-07-09 11:23:16.117000 UTC,,2018-08-21 05:32:16.037000 UTC,0,amazon-s3|amazon-neptune,97,2017-07-06 04:54:35.257000 UTC,2018-08-23 09:11:46.100000 UTC,"Hyderabad, Telangana, India",29,0,0,30,,,,,,[]
Performance issue in select query jdbc conncetion to Mysql in Azure databricks,"<p>I am doing an JDBC connection to Mysql database in Azure databricks env. Then trying retrieve the count(id) with date range of 24 hrs and IN filter with specific country, but its taking hell lot of time. How can I improve the performance?</p>
<p>Query:</p>
<pre><code>pushdown_query = &quot;&quot;&quot;(select count(id) from Mysql_database where time &gt; &quot;{}&quot; and time &lt; &quot;{}&quot; and country IN ('FRA','AUT','DEU','CZE')) alias&quot;&quot;&quot;.format(From_date,To_date)

df = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)
display(df)
</code></pre>",1,0,2021-06-07 11:42:51.030000 UTC,,2021-06-07 14:45:14.673000 UTC,0,mysql|jdbc|apache-spark-sql|azure-databricks,41,2018-05-28 09:46:44.583000 UTC,2022-03-02 08:05:29.517000 UTC,Hyderabad,103,20,0,6,,,,,,[]
Databricks cluster Stages are not starting,"<pre><code>spark.sql(
          s&quot;&quot;&quot;
select
nb.*,
facl_bed.hsid as facl_decn_bed_hsid,
facl_bed.bed_day_decn_seq_nbr as facl_decn_bednbr,
case when facl_bed.hsid is not null then concat(substr(nb.cse_dt, 1, 10), ' 00:00:00.000') else cast(null as string) end as en_dt
from nobed nb
left outer join decn_bed_interim facl_bed on
(nb.hsid=facl_bed.hsid and nb.facl_decn_hsid=facl_bed.hsid)
where nb.facl_decn_hsc_id is not null
union all
select
nb.*,
cast(null as int) as facl_decn_bed_hsid,
cast(null as int) as facl_decn_bednbr,
cast(null as string) as en_dt
from nobed nb
where nb.facl_decn_hsid is null
&quot;&quot;&quot;)
</code></pre>
<p>I was executing the above snippet on databricks spark cluster. I was taking so much of time to complete the task.
here My tables are having very large data.</p>
<p>From my DAG I can see it is taking much in union all.
<a href=""https://i.stack.imgur.com/OwOuJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OwOuJ.png"" alt=""enter image description here"" /></a><br />
Here Stage 205 and 206 are taking so much of time to complete.</p>
<p><a href=""https://i.stack.imgur.com/wnfKi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wnfKi.png"" alt=""enter image description here"" /></a></p>
<p>What could be the reason for this and how can I solve this?</p>",0,0,2022-03-02 15:02:35.423000 UTC,,,1,apache-spark|hive|bigdata|query-optimization|azure-databricks,7,2022-01-21 03:11:50.470000 UTC,2022-03-04 06:13:20.257000 UTC,"Hyderabad, Telangana, India",33,0,0,2,,,,,,[]
Azure DevOps CD Pipeline to Deploy Library to Databricks DBFS 403 Forbidden Error,"<p>I'm following the tutorial <a href=""https://docs.microsoft.com/en-us/azure/databricks/dev-tools/ci-cd/ci-cd-azure-devops"" rel=""nofollow noreferrer"">Continuous integration and delivery on Azure Databricks using Azure DevOps</a> to automate the process to deploy and install library on an Azure Databricks cluster. However, I'm stucked in the step &quot;Deploy the library to DBFS&quot; using task <a href=""https://marketplace.visualstudio.com/items?itemName=DataThirstLtd.databricksDeployScriptsTasks"" rel=""nofollow noreferrer"">Databricks files to DBFS</a> in <a href=""https://marketplace.visualstudio.com/items?itemName=DataThirstLtd.databricksDeployScriptsTasks"" rel=""nofollow noreferrer"">Databricks Script Deployment Task</a> extension by Data Thirst.</p>
<p>It continuously gives me this error:</p>
<pre><code>##[error]The remote server returned an error: (403) Forbidden.
</code></pre>
<p>The configuration of this task is shown below:
<img src=""https://i.stack.imgur.com/knnov.png"" alt=""configuration of Databricks files to DBFS"" /></p>
<p>I've checked with my token that it works fine when I try to upload the libraries manually through Databricks CLI. Thus, the problem shouldn't be due to the permission of the token.</p>
<p>Can anyone suggest any solution to this? Or is there any alternative way to deploy libraries to clusters on Azure Databricks via the release CD pipelines on Azure DevOps?</p>",3,4,2020-07-01 13:21:48.883000 UTC,,2020-08-06 10:05:11.033000 UTC,1,azure|azure-devops|azure-pipelines|azure-databricks,880,2017-09-27 15:30:57.253000 UTC,2022-03-02 14:17:38.753000 UTC,"London, 英国",11,0,0,2,,,,,,[]
Mercurial/Hg commit message prefixed with current branch directory,"<p>We are using Mercurial with cloned repositories for our ""branches"". The ""branch"" in each clone is ""default"". </p>

<p>The structure is:</p>

<pre><code>repos/Test
repos/Trunk
repos/Live
repos/NewFeature
</code></pre>

<p>When finished with work in Trunk the changes are pulled into the Test clone. As each commit is done to <code>default</code> there is now way I can see to see where the changes were originally made, i.e. in the Trunk or the Test repo. </p>

<p>I would like to know how to automatically prefix each commit message with say <code>[Trunk]</code> or <code>[Test]</code> — then the logs would be easier to view. </p>

<p>I would like this to happen when committing both from the cmd line and from Netbeans.</p>",3,0,2012-04-23 16:10:06.660000 UTC,,2012-04-23 17:27:38.947000 UTC,1,netbeans|mercurial|branch|dvcs,475,2011-12-21 16:12:11.170000 UTC,2022-03-02 15:22:23.723000 UTC,,41,17,0,9,,,,,,[]
How to eliminate header and trailer in apache spark dataframe in python,"<p>My dataframe looks like below in .dat format</p>
<pre><code>_c0
This*is*header
siva*2121*123821*3123
sankar*2121*123821*3123
hraju*2121*123821*3123
santhi*2121*123821*3123
This*is*trailer
</code></pre>
<p>I want to remove first row This<em>is</em>header and last row This<em>is</em>trailer and '*' here is delimitter. I want to create dataframe as below with 4 columns. Please provide solution in python pyspark lbrary</p>
<p>Expected output in tabular format:</p>
<pre><code>C1     C2     C3       C4
siva   2121   123821   3123
sankar 2121   123821   3123
hraju  2121   123821   3123
santhi 2121   123821   3123
</code></pre>",1,0,2020-09-03 12:38:42.727000 UTC,,2020-09-03 13:38:07.307000 UTC,0,python-3.x|pyspark|azure-databricks|pyspark-dataframes,65,2020-09-03 12:17:34.990000 UTC,2020-09-24 03:02:00.120000 UTC,"Hyderabad, Telangana, India",1,0,0,0,,,,,,[]
uncompress snappy parquet files in Azure Databricks,"<p>i have a bunch OF snappy parquet files in a folder in azure data lake
Does anyone have code that i can use to uncompress snappy parquet files to parquet using Azure Databricks.</p>

<p>Thanks</p>",1,0,2020-05-14 12:36:25.543000 UTC,,,0,parquet|azure-databricks|snappy,993,2010-01-30 21:29:46.473000 UTC,2020-10-07 12:04:41.213000 UTC,,958,2,1,231,,,,,,[]
Get last modified date of Folders and Files in Azure Databricks,"<p>I need to get last modified dates of all Folders and Files in DBFS mount point (of ADLS Gen1) under Azure Databricks.
Folder structure is like:</p>
<pre><code>Not containing any files, Empty folders:
/dbfs/mnt/ADLS1/LANDING/parent/child/subfolder1
/dbfs/mnt/ADLS1/LANDING/parent/child/subfolder2/subfolder3

Containing some files:
/dbfs/mnt/ADLS1/LANDING/parent/XYZ/subfolder4/File1.txt
/dbfs/mnt/ADLS1/LANDING/parent/XYZ/subfolder5/subfolder6/File2.txt

</code></pre>
<p>Used following Python code to get last modified date:</p>
<pre><code>root_dir = &quot;/dbfs/mnt/ADLS1/LANDING/parent&quot;

def get_directories(root_dir):

    for child in Path(root_dir).iterdir():

        if child.is_file():
            print(child, datetime.fromtimestamp(getmtime(child)).date())
      
        else:
            print(child, datetime.fromtimestamp(getmtime(child)).date())
            get_directories(child)
</code></pre>
<p>From above code, I am getting correct modified date for all folders containing files.</p>
<p><em><strong>But for empty folders, it is giving current date. Not last modified date</strong></em>.</p>
<p>Whereas, when I hardcode the path for empty folder, it is giving correct modified date:</p>
<p><code>print(datetime.fromtimestamp(getmtime(&quot;/dbfs/mnt/ADLS1/LANDING/parent/child/subfolder1&quot;)).date())</code></p>
<p>Can someone please help me out, what am I missing here in loop?</p>",1,0,2021-08-25 06:39:51.173000 UTC,1.0,,0,python-3.x|azure|azure-databricks|azure-data-lake|pathlib,405,2019-03-23 12:24:21.823000 UTC,2022-03-01 14:55:58.710000 UTC,,155,2,0,67,,,,,,[]
JSON parse error: Unexpected character while running curl statement to upload file to neptune database from EC2,"<p>Any idea what's wrong with the following curl statement? I am using this to upload files to a neptune database from an EC2 instance.</p>
<pre><code>curl -X POST \
    -H 'Content-Type: application/json' \
    https://*my neptune endpoint*:8182/loader -d '
    {​​​​
    &quot;source&quot; : &quot;s3://&lt;file path&gt;/&lt;file name&gt;.nq&quot;,
      &quot;format&quot; : &quot;nquads&quot;,
      &quot;iamRoleArn&quot; : &quot;arn:aws:iam::##########:role/NeptuneLoadFromS3&quot;,
      &quot;region&quot; : &quot;us-east-1&quot;,
      &quot;failOnError&quot; : &quot;FALSE&quot;,
      &quot;parallelism&quot; : &quot;MEDIUM&quot;,
      &quot;updateSingleCardinalityProperties&quot; : &quot;FALSE&quot;,
      &quot;queueRequest&quot; : &quot;TRUE&quot;
    }​​​​'
</code></pre>
<p>I have used this command template multiple times before without issue. The only things that i have changed here are the neptune endpoint and the file location on s3. When i run it now, i get the following error:</p>
<pre><code>{&quot;detailedMessage&quot;:&quot;Json parse error: Unexpected character ('​' (code 8203 / 0x200b)): was expecting double-quote to
 start field name\n at [Source: (String)\&quot;{​​​​\n    \&quot;source\&quot; : \&quot;s3://&lt;file path&gt;/&lt;file name&gt;.nq\&quot;,\n      \&quot;format\&quot; 
: \&quot;nquads\&quot;,\n      \&quot;iamRoleArn\&quot; : \&quot;arn:aws:iam::#########:role/NeptuneLoadFromS3\&quot;,\n      \&quot;region\&quot; 
: \&quot;us-east-1\&quot;,\n      \&quot;failOnError\&quot; : \&quot;FALSE\&quot;,\n      \&quot;parallelism\&quot; : \&quot;MEDIUM\&quot;,\n      
\&quot;updateSingleCardinalityProperties\&quot; : \&quot;FALSE\&quot;,\n      \&quot;queueRequest\&quot; : \&quot;TRUE\&quot;\n    }​​​​\&quot;; line: 1, column: 3]&quot;,
&quot;requestId&quot;:&quot;4ebb82c9-107d-8578-cf84-8056817e504e&quot;,&quot;code&quot;:&quot;BadRequestException&quot;}
</code></pre>
<p>Anything that i change in the statement does not seem to have an effect on the outcome. Is there something really obvious that i am missing here?</p>",0,4,2021-01-15 10:30:36.283000 UTC,1.0,2021-03-07 19:23:40.707000 UTC,0,json|curl|amazon-neptune,874,2016-10-28 07:28:58.800000 UTC,2022-02-09 10:03:18.453000 UTC,,23,1,0,1,,,,,,[]
how to convert pandas dataframe to koalas with mixed data types,"<p>I m using Azure Databricks to convert a pandas Dataframe into a koalas data frame...</p>
<pre><code>kdf = ks.DataFrame(pdf)
</code></pre>
<p>This results in an error message of &quot;an integer is required (got type str)&quot;</p>
<p>I tried adding a dtype of str to force the koalas dataframe to be of type string. ..</p>
<pre><code> df = ks.DataFrame(pdf, dtype='str')
</code></pre>
<p>adding the dtype works perfectly in vs code using the databricks extention but results in an <em>AssertionError</em> when executed in azure databricks workspace.</p>
<p>It seems like azure databricks must be using a different version of koalas than the vs code databricks extention.</p>
<p>How can I get this to work in azure databricks?</p>
<p>How can I find out what version of koalas azure databricks is using and what version of koalas the databricks vs code extention is using?</p>
<p>I cant just use <em>pip list</em> to find vs code version of koalas because it is an extention, rather than an installed package.</p>
<p>Any help on this would be gratefully received</p>
<p>Laura</p>",1,0,2020-08-15 21:10:31.793000 UTC,,2020-08-15 23:11:42.030000 UTC,0,pandas|dataframe|azure-databricks|spark-koalas,1193,2017-03-03 14:33:39.280000 UTC,2020-11-16 19:59:20.940000 UTC,,457,10,0,95,,,,,,[]
Guarantees of traversal order in AWS Neptune graph database,"<p>I'm working with Gremlin and AWS Neptune graph database.</p>
<p>I was wondering if any query, when it is applied in different times (but the query stays exactly the same), could potentially result in different traversal order and therefore maybe return the same items in a different order.
I know that Gremlin doesn't guarantee any such order but I couldn't find anything regarding the matter on AWS documentation.
I know that to guarantee some order I could always use order() but it is obviously less efficient.</p>
<p>Thanks.</p>",1,0,2021-12-02 09:18:52.347000 UTC,,,1,amazon-web-services|gremlin|graph-databases|tinkerpop|aws-neptune,39,2019-09-10 11:36:54.153000 UTC,2022-03-03 15:53:38.500000 UTC,"Rehovot, Israel",11,0,0,2,,,,,,[]
Upload to Azure Storage container from FileServer using Azure databricks,"<p>I want to upload binary files from Windows FileSystem to Azure blob. I achieved it with Azure data factory with the below steps</p>
<ol>
<li>Installed integration run time on the FileSystem</li>
<li>Created a linked service in ADF as FileSystem</li>
<li>Created a binary dataset with the above linked service</li>
<li>Use CopyData activity in a ADF Pipeline, set the binary dataset as source and Azure Blob as Sink</li>
</ol>
<p>Post upload, I am performing some ETL activities. So my ADF pipeline has two components,</p>
<ol>
<li>Copy Data</li>
<li>Databricks Notebook</li>
</ol>
<p>I am wondering if I could move the Copy Data fragment to Databricks?
Can we upload binary files from Windows FileSystem to Azure blob using Azure Databricks?</p>",1,0,2021-05-12 10:44:36.637000 UTC,,,0,azure|azure-blob-storage|azure-data-factory|azure-databricks,35,2012-03-25 11:04:52.163000 UTC,2022-01-30 23:42:54.737000 UTC,"Chennai, India",1351,113,11,161,,,,,,[]
Writing in json format as an array of json object in Scala Spark,"<p>Currently I am using this to write output in single partition.</p>
<pre><code> df.coalesce(1).write
.format(&quot;json&quot;)
.mode(&quot;overwrite&quot;)
.option(&quot;path&quot;,writePath)
.save
</code></pre>
<p>Ouput file is currently in this format :</p>
<p>{obj1}
{obj2}</p>
<p>I want this as an array of json object.
[{obj1},
{obj2}]</p>",1,0,2022-02-18 07:44:25.133000 UTC,,,1,scala|apache-spark|azure-databricks,24,2021-12-27 12:57:16.570000 UTC,2022-03-05 16:19:13.280000 UTC,,11,0,0,2,,,,,,[]
Bazaar manage multiple branches at once,"<p>I'm using Bazaar quite some time now, but at the moment I'm searching a solution to the following problem:</p>

<p>Assuming you've got several developers with everyone developing in its own branch, like this:</p>

<p>Project<br>
|<br>
|----Branch 1<br>
|<br>
|----Branch 2<br>
|<br>
...<br></p>

<p>Now, we've got a project manager who wants to have an overview over all branches.</p>

<p>Is there any possibility (using only bzr functions) that he can manage those branches at once?</p>

<p>With ""manage"", I mean update, commit and perhaps even checkout (last one could perhaps be done with multi-pull but I think this would overwrite existing local data)</p>

<p>Greetings Florian</p>

<p>P.S. I know that this use-case could easily be achieved with SVN (by simply using subdirectories - but without the features of a dvcs) or more or less easily with shell-scripts (something like bzr list-branches|xargs bzr update), but I'd prefer a built-in bzr function</p>",2,0,2013-02-24 18:22:00.427000 UTC,1.0,,0,version-control|dvcs|bazaar,189,2010-02-18 16:54:45.107000 UTC,2022-01-26 16:46:47.457000 UTC,,211,4,0,16,,,,,,[]
Databricks Kusto connector: ModuleNotFoundError: No module named 'azure',"<p>I am using Databricks on Azure and trying to run this one simple line:</p>
<pre><code>from azure.kusto.data import KustoClient, KustoConnectionStringBuilder
</code></pre>
<p>However it gives me the error:</p>
<pre><code>ModuleNotFoundError: No module named 'azure'
</code></pre>
<p>I have installed the following Maven libraries:</p>
<pre><code>com.microsoft.azure.kusto:kusto-data:2.1.1
com.microsoft.azure.kusto:kusto-ingest:2.1.1
com.microsoft.azure.kusto:spark-kusto-connector1.1.5
</code></pre>",1,0,2020-06-27 15:42:31.313000 UTC,,,0,azure-databricks,830,2012-10-20 16:18:44.083000 UTC,2022-03-03 10:41:10.747000 UTC,,5331,89,4,402,,,,,,[]
Writing a spark dataframe to Azure Sql Server is causing duplicate records intermittently,"<p>We are using JDBC option to insert transformed data in a spark DataFrame to a table in Azure SQL Server. Below is the code snippet we are using for this insert. However, we noticed on few occasions that some records are being duplicated in the destination table. This is happening for large tables. e.g. if a DataFrame has 600K records, after inserting data into the table, we get around 620K records. This is very rare occurrence, but we still want to understand why that's happening.</p>

<pre><code>DataToLoad.write.jdbc(url = jdbcUrl, table = targetTable, mode = ""overwrite"", properties = jdbcConnectionProperties)
</code></pre>

<p>Only reason we could think of is that while inserts are happening in distributed fashion, if one of the executors fail in between, they are being re-tried and could be inserting duplicate records. This could be totally meaningless but just to see if that could be an issue.</p>",0,3,2020-01-24 02:58:05.500000 UTC,,2020-01-24 05:31:25.057000 UTC,0,apache-spark|pyspark|azure-databricks,403,2013-11-11 05:59:55.433000 UTC,2021-06-25 20:51:24.590000 UTC,,43,3,0,3,,,,,,[]
Access data from ADLS using Azure Databricks,"<p>I am trying to access data files stored in ADLS location via Azure Databricks using storage account access keys.
To access data files, I am using python notebook in azure databricks and below command works fine,</p>
<pre><code>spark.conf.set(
  &quot;fs.azure.account.key.&lt;storage-account-name&gt;.dfs.core.windows.net&quot;,
  &quot;&lt;access-key&gt;&quot;
)
</code></pre>
<p>However, when I try to list the directory using below command, it throws an error</p>
<pre><code>dbutils.fs.ls(&quot;abfss://&lt;container-name&gt;@&lt;storage-account-name&gt;.dfs.core.windows.net&quot;)
</code></pre>
<p>ERROR:</p>
<pre><code>ExecutionError: An error occurred while calling z:com.databricks.backend.daemon.dbutils.FSUtils.ls.
: Operation failed: &quot;This request is not authorized to perform this operation using this permission.&quot;, 403, GET, https://&lt;storage-account-name&gt;.dfs.core.windows.net/&lt;container-name&gt;?upn=false&amp;resource=filesystem&amp;maxResults=500&amp;timeout=90&amp;recursive=false, AuthorizationPermissionMismatch, &quot;This request is not authorized to perform this operation using this permission. RequestId:&lt;request-id&gt; Time:2021-08-03T08:53:28.0215432Z&quot;
</code></pre>
<p>I am not sure on what permission would it require and how can I proceed with it.</p>
<p>Also, I am using ADLS Gen2 and Azure Databricks(Trial - premium).</p>
<p>Thanks in advance!</p>",2,9,2021-08-03 09:09:22.353000 UTC,,,0,azure|azure-databricks|azure-data-lake|azure-data-lake-gen2,483,2020-10-22 11:59:37.027000 UTC,2022-03-03 11:05:35.673000 UTC,,1,0,0,2,,,,,,[]
Parallel running python notebooks in Azure Data Bricks,"<p>I need help on parallelly running some python notebooks in ADB. Approach I am currently following is, if I have 5 notebooks to run, then I am calling them from a main file sequentially:</p>
<pre><code>dbutils.notebook.run('Notebook1')
dbutils.notebook.run('Notebook2')
dbutils.notebook.run('Notebook3')
dbutils.notebook.run('Notebook4')
dbutils.notebook.run('Notebook5')
</code></pre>
<p>This ends up taking more than two days for all models to complete training. Is there a way I can put these to run parallely, given that all of these are independent models and can run concurrently.</p>",0,0,2021-08-06 09:39:12.353000 UTC,,2021-08-06 09:49:46.957000 UTC,0,python|parallel-processing|azure-databricks,38,2020-11-17 12:22:53.260000 UTC,2022-03-04 12:33:00.743000 UTC,,13,0,0,4,,,,,,[]
Conflict versions when running spark application on Databricks runtime 10.3,"<p>I'm running scala/spark application on Databricks cluster (it tries to read data from an Event Hub topic using EventHub schemaregistry) and getting the following error:</p>
<pre><code>22/02/17 07:36:14 ERROR JacksonVersion: Version '1.0.19-SNAPSHOT' of package 'jackson-dataformat-xml' is not supported (older than earliest supported version - `2.10.0`), please upgrade.
22/02/17 07:36:14 ERROR JacksonVersion: Version '1.0.19-SNAPSHOT' of package 'jackson-datatype-jsr310' is not supported (older than earliest supported version - `2.10.0`), please upgrade.
22/02/17 07:36:14 INFO JacksonVersion: Package versions: jackson-annotations=2.12.3, jackson-core=2.12.3, jackson-databind=2.12.3, jackson-dataformat-xml=1.0.19-SNAPSHOT, jackson-datatype-jsr310=1.0.19-SNAPSHOT, azure-core=1.8.1, Troubleshooting version conflicts: https://aka.ms/azsdk/java/dependency/troubleshoot
22/02/17 07:36:14 ERROR ScalaDriverLocal: User Code Stack Trace: 
java.lang.NoSuchMethodError: reactor.netty.http.client.HttpClient.resolver(Lio/netty/resolver/AddressResolverGroup;)Lreactor/netty/transport/ClientTransport;
</code></pre>
<p>Looks that maven is trying to use main artificat version (1.0.19-SNAPSHOT) to 'jackson-dataformat-xml' and 'jackson-datatype-jsr310':</p>
<pre><code>22/02/17 07:36:14 ERROR JacksonVersion: Version '1.0.19-SNAPSHOT' of package 'jackson-dataformat-xml' is not supported (older than earliest supported version - `2.10.0`), please upgrade.
22/02/17 07:36:14 ERROR JacksonVersion: Version '1.0.19-SNAPSHOT' of package 'jackson-datatype-jsr310' is not supported (older than earliest supported version - `2.10.0`), please upgrade.
</code></pre>
<p>The project uses Databricks 10.3 runtime and this pom:</p>
<pre><code>&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
     xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;
&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

&lt;parent&gt;
    &lt;groupId&gt;group&lt;/groupId&gt;
    &lt;artifactId&gt;parent&lt;/artifactId&gt;
    &lt;version&gt;3.1-SNAPSHOT&lt;/version&gt;
&lt;/parent&gt;

&lt;artifactId&gt;ingestion&lt;/artifactId&gt;
&lt;version&gt;${revision}&lt;/version&gt;
&lt;packaging&gt;jar&lt;/packaging&gt;
&lt;name&gt;ingestion&lt;/name&gt;
&lt;organization&gt;
    &lt;name&gt;company&lt;/name&gt;
&lt;/organization&gt;

&lt;properties&gt;
    &lt;revision&gt;1.0.19-SNAPSHOT&lt;/revision&gt;
    &lt;encoding&gt;UTF-8&lt;/encoding&gt;
&lt;/properties&gt;

&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;
        &lt;artifactId&gt;log4j-core&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-core_${scala.compact.version}&lt;/artifactId&gt;
        &lt;scope&gt;provided&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-sql_${scala.compact.version}&lt;/artifactId&gt;
        &lt;scope&gt;provided&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-streaming_${scala.compact.version}&lt;/artifactId&gt;
        &lt;scope&gt;provided&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-avro_${scala.compact.version}&lt;/artifactId&gt;
        &lt;scope&gt;provided&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.delta&lt;/groupId&gt;
        &lt;artifactId&gt;delta-core_${scala.compact.version}&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.scalatest&lt;/groupId&gt;
        &lt;artifactId&gt;scalatest_${scala.compact.version}&lt;/artifactId&gt;
        &lt;version&gt;3.2.11&lt;/version&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.mockito&lt;/groupId&gt;
        &lt;artifactId&gt;mockito-scala_${scala.compact.version}&lt;/artifactId&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.yaml&lt;/groupId&gt;
        &lt;artifactId&gt;snakeyaml&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.spray&lt;/groupId&gt;
        &lt;artifactId&gt;spray-json_${scala.compact.version}&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.databricks&lt;/groupId&gt;
        &lt;artifactId&gt;dbutils-api_${scala.compact.version}&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.github.mrpowers&lt;/groupId&gt;
        &lt;artifactId&gt;spark-daria_${scala.compact.version}&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.github.mrpowers&lt;/groupId&gt;
        &lt;artifactId&gt;spark-fast-tests_${scala.compact.version}&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;joda-time&lt;/groupId&gt;
        &lt;artifactId&gt;joda-time&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.circe&lt;/groupId&gt;
        &lt;artifactId&gt;circe-yaml_${scala.compact.version}&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.circe&lt;/groupId&gt;
        &lt;artifactId&gt;circe-generic_${scala.compact.version}&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.jvm.uuid&lt;/groupId&gt;
        &lt;artifactId&gt;scala-uuid_2.12&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;
        &lt;artifactId&gt;httpclient&lt;/artifactId&gt;
        &lt;scope&gt;provided&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
        &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.typesafe&lt;/groupId&gt;
        &lt;artifactId&gt;config&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.azure&lt;/groupId&gt;
        &lt;artifactId&gt;azure-identity&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.azure&lt;/groupId&gt;
        &lt;artifactId&gt;azure-core&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.fasterxml.jackson.dataformat&lt;/groupId&gt;
        &lt;artifactId&gt;jackson-dataformat-xml&lt;/artifactId&gt;
        &lt;version&gt;2.12.3&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.fasterxml.jackson.datatype&lt;/groupId&gt;
        &lt;artifactId&gt;jackson-datatype-jsr310&lt;/artifactId&gt;
        &lt;version&gt;2.12.3&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.azure&lt;/groupId&gt;
        &lt;artifactId&gt;azure-data-schemaregistry&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;commons-cli&lt;/groupId&gt;
        &lt;artifactId&gt;commons-cli&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.azure&lt;/groupId&gt;
        &lt;artifactId&gt;azure-core-http-netty&lt;/artifactId&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;

&lt;build&gt;
    &lt;resources&gt;
        &lt;resource&gt;
            &lt;directory&gt;src/main/resources&lt;/directory&gt;
            &lt;filtering&gt;false&lt;/filtering&gt;
        &lt;/resource&gt;
        &lt;resource&gt;
            &lt;directory&gt;src/main/resources-filtered&lt;/directory&gt;
            &lt;filtering&gt;true&lt;/filtering&gt;
        &lt;/resource&gt;
    &lt;/resources&gt;

    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;
            &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;
            &lt;executions&gt;
                &lt;execution&gt;
                    &lt;id&gt;scala-compile&lt;/id&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;compile&lt;/goal&gt;
                        &lt;goal&gt;testCompile&lt;/goal&gt;
                    &lt;/goals&gt;
                &lt;/execution&gt;
            &lt;/executions&gt;
            &lt;configuration&gt;
                &lt;scalaVersion&gt;${scala.version}&lt;/scalaVersion&gt;
                &lt;jvmArgs&gt;
                    &lt;jvmArg&gt;-Xms64m&lt;/jvmArg&gt;
                    &lt;jvmArg&gt;-Xmx1024m&lt;/jvmArg&gt;
                &lt;/jvmArgs&gt;
            &lt;/configuration&gt;
        &lt;/plugin&gt;

        &lt;plugin&gt;
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
            &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;
            &lt;version&gt;3.0.2&lt;/version&gt;
            &lt;configuration&gt;
                &lt;archive&gt;
                    &lt;manifestEntries&gt;
                        &lt;Implementation-Title&gt;${project.artifactId}&lt;/Implementation-Title&gt;
                        &lt;Implementation-Version&gt;${project.version}&lt;/Implementation-Version&gt;
                        &lt;Specification-Vendor&gt;${project.groupId}&lt;/Specification-Vendor&gt;
                        &lt;Specification-Title&gt;${project.artifactId}&lt;/Specification-Title&gt;
                        &lt;Implementation-Vendor-Id&gt;${project.groupId}&lt;/Implementation-Vendor-Id&gt;
                        &lt;Specification-Version&gt;${project.version}&lt;/Specification-Version&gt;
                        &lt;Implementation-Vendor&gt;${project.groupId}&lt;/Implementation-Vendor&gt;
                    &lt;/manifestEntries&gt;
                &lt;/archive&gt;
            &lt;/configuration&gt;
        &lt;/plugin&gt;

        &lt;plugin&gt;
            &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;
            &lt;artifactId&gt;flatten-maven-plugin&lt;/artifactId&gt;
            &lt;version&gt;1.2.7&lt;/version&gt;
            &lt;configuration&gt;
                &lt;updatePomFile&gt;true&lt;/updatePomFile&gt;
                &lt;flattenMode&gt;resolveCiFriendliesOnly&lt;/flattenMode&gt;
            &lt;/configuration&gt;
            &lt;executions&gt;
                &lt;execution&gt;
                    &lt;id&gt;flatten&lt;/id&gt;
                    &lt;phase&gt;process-resources&lt;/phase&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;flatten&lt;/goal&gt;
                    &lt;/goals&gt;
                &lt;/execution&gt;
                &lt;execution&gt;
                    &lt;id&gt;flatten.clean&lt;/id&gt;
                    &lt;phase&gt;clean&lt;/phase&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;clean&lt;/goal&gt;
                    &lt;/goals&gt;
                &lt;/execution&gt;
            &lt;/executions&gt;
        &lt;/plugin&gt;

        &lt;plugin&gt;
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
            &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
            &lt;version&gt;3.3.0&lt;/version&gt;
            &lt;executions&gt;
                &lt;execution&gt;
                    &lt;id&gt;make-assembly&lt;/id&gt;
                    &lt;phase&gt;package&lt;/phase&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;single&lt;/goal&gt;
                    &lt;/goals&gt;
                &lt;/execution&gt;
            &lt;/executions&gt;
            &lt;configuration&gt;
                &lt;finalName&gt;${project.artifactId}-${project.version}-assembly&lt;/finalName&gt;
                &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt;
                &lt;descriptorRefs&gt;
                    &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
                &lt;/descriptorRefs&gt;
                &lt;archive&gt;
                    &lt;manifestEntries&gt;
                        &lt;Implementation-Title&gt;${project.artifactId}&lt;/Implementation-Title&gt;
                        &lt;Implementation-Version&gt;${project.version}&lt;/Implementation-Version&gt;
                        &lt;Specification-Vendor&gt;${project.groupId}&lt;/Specification-Vendor&gt;
                        &lt;Specification-Title&gt;${project.artifactId}&lt;/Specification-Title&gt;
                        &lt;Implementation-Vendor-Id&gt;${project.groupId}&lt;/Implementation-Vendor-Id&gt;
                        &lt;Specification-Version&gt;${project.version}&lt;/Specification-Version&gt;
                        &lt;Implementation-Vendor&gt;${project.groupId}&lt;/Implementation-Vendor&gt;
                    &lt;/manifestEntries&gt;
                &lt;/archive&gt;
            &lt;/configuration&gt;
        &lt;/plugin&gt;

        &lt;plugin&gt;
            &lt;groupId&gt;org.scalatest&lt;/groupId&gt;
            &lt;artifactId&gt;scalatest-maven-plugin&lt;/artifactId&gt;
            &lt;version&gt;2.0.0&lt;/version&gt;
            &lt;configuration&gt;
                &lt;reportsDirectory&gt;${project.build.directory}/surefire-reports&lt;/reportsDirectory&gt;
            &lt;/configuration&gt;
            &lt;executions&gt;
                &lt;execution&gt;
                    &lt;id&gt;test&lt;/id&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;test&lt;/goal&gt;
                    &lt;/goals&gt;
                &lt;/execution&gt;
            &lt;/executions&gt;
        &lt;/plugin&gt;

        &lt;plugin&gt;
            &lt;groupId&gt;org.scalastyle&lt;/groupId&gt;
            &lt;artifactId&gt;scalastyle-maven-plugin&lt;/artifactId&gt;
            &lt;executions&gt;
                &lt;execution&gt;
                    &lt;phase&gt;compile&lt;/phase&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;check&lt;/goal&gt;
                    &lt;/goals&gt;
                &lt;/execution&gt;
            &lt;/executions&gt;
        &lt;/plugin&gt;
    &lt;/plugins&gt;
&lt;/build&gt;
</code></pre>

<p>and this is the parent project:</p>
<pre><code>&lt;groupId&gt;group&lt;/groupId&gt;
&lt;artifactId&gt;parent&lt;/artifactId&gt;
&lt;version&gt;3.1-SNAPSHOT&lt;/version&gt;
&lt;packaging&gt;pom&lt;/packaging&gt;
&lt;properties&gt;
    &lt;scala.version&gt;2.12.14&lt;/scala.version&gt;
    &lt;scala.compact.version&gt;2.12&lt;/scala.compact.version&gt;
    &lt;spark.version&gt;3.2.1&lt;/spark.version&gt;
    &lt;databricks.dbutils.api.version&gt;0.0.5&lt;/databricks.dbutils.api.version&gt;
    &lt;delta.version&gt;1.1.0&lt;/delta.version&gt;
    &lt;postgresql.version&gt;42.2.9&lt;/postgresql.version&gt;
    &lt;typesafe.config.version&gt;1.4.0&lt;/typesafe.config.version&gt;
    &lt;snakeyaml.version&gt;1.25&lt;/snakeyaml.version&gt;
    &lt;spray-json.version&gt;1.3.5&lt;/spray-json.version&gt;
    &lt;h2.version&gt;1.4.200&lt;/h2.version&gt;
    &lt;mockito.version&gt;1.16.46&lt;/mockito.version&gt;
    &lt;scalacheck.version&gt;1.14.0&lt;/scalacheck.version&gt;
    &lt;adal4j.version&gt;1.6.6&lt;/adal4j.version&gt;
    &lt;scalastyle.version&gt;0.8.0&lt;/scalastyle.version&gt;
    &lt;maven-surefire-plugin.version&gt;2.7&lt;/maven-surefire-plugin.version&gt;
    &lt;scalatest-maven-plugin.version&gt;1.0&lt;/scalatest-maven-plugin.version&gt;
    &lt;spark-daria.version&gt;1.2.3&lt;/spark-daria.version&gt;
    &lt;spark-fast-tests.version&gt;1.1.0&lt;/spark-fast-tests.version&gt;
    &lt;kafka.version&gt;3.1.0&lt;/kafka.version&gt;
    &lt;scala-maven-plugin.version&gt;4.4.0&lt;/scala-maven-plugin.version&gt;
    &lt;cats-core.version&gt;2.0.0&lt;/cats-core.version&gt;
    &lt;pureconfig_2.11.version&gt;0.12.3&lt;/pureconfig_2.11.version&gt;
    &lt;jgitflow-maven-plugin.version&gt;1.0-m5.1&lt;/jgitflow-maven-plugin.version&gt;
    &lt;joda-time.version&gt;2.10.6&lt;/joda-time.version&gt;
    &lt;mssql-jdbc.version&gt;8.2.2.jre8&lt;/mssql-jdbc.version&gt;
    &lt;circe-yaml.artifact&gt;circe-yaml_2.12&lt;/circe-yaml.artifact&gt;
    &lt;circe-yaml.version&gt;0.13.1&lt;/circe-yaml.version&gt;
    &lt;circe-generic.artifact&gt;circe-generic_2.12&lt;/circe-generic.artifact&gt;
    &lt;circe-generic.version&gt;0.13.0&lt;/circe-generic.version&gt;
    &lt;scala-uuid_2.12.version&gt;0.3.1&lt;/scala-uuid_2.12.version&gt;
    &lt;spark-testing-base.version&gt;0.14.0&lt;/spark-testing-base.version&gt;
    &lt;scalatest.version&gt;3.0.5&lt;/scalatest.version&gt;
    &lt;httpclient.version&gt;4.5.6&lt;/httpclient.version&gt;
    &lt;azure-identity.version&gt;1.4.0&lt;/azure-identity.version&gt;
    &lt;azure-core.version&gt;1.23.0&lt;/azure-core.version&gt;
    &lt;azure-data-schemaregistry.version&gt;1.0.0&lt;/azure-data-schemaregistry.version&gt;
    &lt;log4j-core.version&gt;2.17.1&lt;/log4j-core.version&gt;
    &lt;commons-cli.version&gt;1.2&lt;/commons-cli.version&gt;
    &lt;reactor-netty.version&gt;1.0.15&lt;/reactor-netty.version&gt;
    &lt;azure-core-http-netty.version&gt;1.11.7&lt;/azure-core-http-netty.version&gt;
&lt;/properties&gt;
&lt;dependencyManagement&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;
            &lt;artifactId&gt;log4j-core&lt;/artifactId&gt;
            &lt;version&gt;${log4j-core.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-core_${scala.compact.version}&lt;/artifactId&gt;
            &lt;version&gt;${spark.version}&lt;/version&gt;
            &lt;scope&gt;provided&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-sql_${scala.compact.version}&lt;/artifactId&gt;
            &lt;version&gt;${spark.version}&lt;/version&gt;
            &lt;scope&gt;provided&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-streaming_${scala.compact.version}&lt;/artifactId&gt;
            &lt;version&gt;${spark.version}&lt;/version&gt;
            &lt;scope&gt;provided&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-mllib_${scala.compact.version}&lt;/artifactId&gt;
            &lt;version&gt;${spark.version}&lt;/version&gt;
            &lt;scope&gt;provided&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
            &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
            &lt;version&gt;${kafka.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
            &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt;
            &lt;version&gt;${kafka.version}&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;
                    &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-avro_${scala.compact.version}&lt;/artifactId&gt;
            &lt;version&gt;${spark.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;
            &lt;artifactId&gt;scala-library&lt;/artifactId&gt;
            &lt;version&gt;${scala.version}&lt;/version&gt;
            &lt;scope&gt;provided&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.microsoft.azure&lt;/groupId&gt;
            &lt;artifactId&gt;adal4j&lt;/artifactId&gt;
            &lt;version&gt;${adal4j.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.databricks&lt;/groupId&gt;
            &lt;artifactId&gt;dbutils-api_2.12&lt;/artifactId&gt;
            &lt;version&gt;${databricks.dbutils.api.version}&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;
                    &lt;artifactId&gt;scala-library&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
            &lt;scope&gt;provided&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;joda-time&lt;/groupId&gt;
            &lt;artifactId&gt;joda-time&lt;/artifactId&gt;
            &lt;version&gt;${joda-time.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;
            &lt;artifactId&gt;httpclient&lt;/artifactId&gt;
            &lt;version&gt;${httpclient.version}&lt;/version&gt;
            &lt;scope&gt;provided&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;io.delta&lt;/groupId&gt;
            &lt;artifactId&gt;delta-core_${scala.compact.version}&lt;/artifactId&gt;
            &lt;version&gt;${delta.version}&lt;/version&gt;
            &lt;scope&gt;provided&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.typesafe&lt;/groupId&gt;
            &lt;artifactId&gt;config&lt;/artifactId&gt;
            &lt;version&gt;${typesafe.config.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.yaml&lt;/groupId&gt;
            &lt;artifactId&gt;snakeyaml&lt;/artifactId&gt;
            &lt;version&gt;${snakeyaml.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;io.circe&lt;/groupId&gt;
            &lt;artifactId&gt;${circe-yaml.artifact}&lt;/artifactId&gt;
            &lt;version&gt;${circe-yaml.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;io.circe&lt;/groupId&gt;
            &lt;artifactId&gt;${circe-generic.artifact}&lt;/artifactId&gt;
            &lt;version&gt;${circe-generic.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;io.jvm.uuid&lt;/groupId&gt;
            &lt;artifactId&gt;scala-uuid_2.12&lt;/artifactId&gt;
            &lt;version&gt;${scala-uuid_2.12.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;io.spray&lt;/groupId&gt;
            &lt;artifactId&gt;spray-json_${scala.compact.version}&lt;/artifactId&gt;
            &lt;version&gt;${spray-json.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.mockito&lt;/groupId&gt;
            &lt;artifactId&gt;mockito-scala_${scala.compact.version}&lt;/artifactId&gt;
            &lt;version&gt;${mockito.version}&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.typelevel&lt;/groupId&gt;
            &lt;artifactId&gt;cats-core_2.12&lt;/artifactId&gt;
            &lt;version&gt;${cats-core.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.scalacheck&lt;/groupId&gt;
            &lt;artifactId&gt;scalacheck_${scala.compact.version}&lt;/artifactId&gt;
            &lt;version&gt;${scalacheck.version}&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.holdenkarau&lt;/groupId&gt;
            &lt;artifactId&gt;spark-testing-base_${scala.compact.version}
            &lt;version&gt;2.4.5_${spark-testing-base.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.scalatest&lt;/groupId&gt;
            &lt;artifactId&gt;scalatest_${scala.compact.version}&lt;/artifactId&gt;
            &lt;version&gt;${scalatest.version}&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.github.mrpowers&lt;/groupId&gt;
            &lt;artifactId&gt;spark-fast-tests_${scala.compact.version}&lt;/artifactId&gt;
            &lt;version&gt;${spark-fast-tests.version}&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.github.mrpowers&lt;/groupId&gt;
            &lt;artifactId&gt;spark-daria_${scala.compact.version}&lt;/artifactId&gt;
            &lt;version&gt;${spark-daria.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.azure&lt;/groupId&gt;
            &lt;artifactId&gt;azure-identity&lt;/artifactId&gt;
            &lt;version&gt;${azure-identity.version}&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;com.azure&lt;/groupId&gt;
                    &lt;artifactId&gt;azure-core-http-netty&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.azure&lt;/groupId&gt;
            &lt;artifactId&gt;azure-core&lt;/artifactId&gt;
            &lt;version&gt;${azure-core.version}&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;com.fasterxml.jackson.dataformat&lt;/groupId&gt;
                    &lt;artifactId&gt;jackson-dataformat-xml&lt;/artifactId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;com.fasterxml.jackson.datatype&lt;/groupId&gt;
                    &lt;artifactId&gt;jackson-datatype-jsr310&lt;/artifactId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;io.projectreactor.netty&lt;/groupId&gt;
                    &lt;artifactId&gt;reactor-netty&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.azure&lt;/groupId&gt;
            &lt;artifactId&gt;azure-core-http-netty&lt;/artifactId&gt;
            &lt;version&gt;${azure-core-http-netty.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.azure&lt;/groupId&gt;
            &lt;artifactId&gt;azure-data-schemaregistry&lt;/artifactId&gt;
            &lt;version&gt;${azure-data-schemaregistry.version}&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;com.azure&lt;/groupId&gt;
                    &lt;artifactId&gt;azure-core-http-netty&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;commons-cli&lt;/groupId&gt;
            &lt;artifactId&gt;commons-cli&lt;/artifactId&gt;
            &lt;version&gt;${commons-cli.version}&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/dependencyManagement&gt;
&lt;build&gt;
    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;external.atlassian.jgitflow&lt;/groupId&gt;
            &lt;artifactId&gt;jgitflow-maven-plugin&lt;/artifactId&gt;
            &lt;version&gt;${jgitflow-maven-plugin.version}&lt;/version&gt;
            &lt;configuration&gt;
                &lt;pushFeatures&gt;true&lt;/pushFeatures&gt;
                &lt;pushHotfixes&gt;true&lt;/pushHotfixes&gt;
                &lt;pushReleases&gt;true&lt;/pushReleases&gt;
                &lt;enableSshAgent&gt;true&lt;/enableSshAgent&gt;
                &lt;noDeploy&gt;true&lt;/noDeploy&gt;
                &lt;flowInitContext&gt;
                    &lt;masterBranchName&gt;master&lt;/masterBranchName&gt;
                    &lt;developBranchName&gt;develop&lt;/developBranchName&gt;
                    &lt;featureBranchPrefix&gt;feature/&lt;/featureBranchPrefix&gt;
                    &lt;releaseBranchPrefix&gt;release/&lt;/releaseBranchPrefix&gt;
                    &lt;hotfixBranchPrefix&gt;hotfix/&lt;/hotfixBranchPrefix&gt;
                    &lt;versionTagPrefix&gt;${project.artifactId}-&lt;/versionTagPrefix&gt;
                &lt;/flowInitContext&gt;
            &lt;/configuration&gt;
        &lt;/plugin&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;
            &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;
            &lt;version&gt;${scala-maven-plugin.version}&lt;/version&gt;
            &lt;executions&gt;
                &lt;execution&gt;
                    &lt;phase&gt;compile&lt;/phase&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;compile&lt;/goal&gt;
                        &lt;goal&gt;testCompile&lt;/goal&gt;
                    &lt;/goals&gt;
                &lt;/execution&gt;
            &lt;/executions&gt;
            &lt;configuration&gt;
                &lt;jvmArgs&gt;
                    &lt;jvmArg&gt;-Xms1024M&lt;/jvmArg&gt;
                    &lt;jvmArg&gt;-Xmx2048M&lt;/jvmArg&gt;
                    &lt;jvmArg&gt;-XX:+CMSClassUnloadingEnabled&lt;/jvmArg&gt;
                &lt;/jvmArgs&gt;
            &lt;/configuration&gt;
        &lt;/plugin&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.scalastyle&lt;/groupId&gt;
            &lt;artifactId&gt;scalastyle-maven-plugin&lt;/artifactId&gt;
            &lt;version&gt;${scalastyle.version}&lt;/version&gt;
            &lt;configuration&gt;
                &lt;verbose&gt;true&lt;/verbose&gt;
                &lt;failOnViolation&gt;true&lt;/failOnViolation&gt;
                &lt;includeTestSourceDirectory&gt;true&lt;/includeTestSourceDirectory&gt;
                &lt;failOnWarning&gt;false&lt;/failOnWarning&gt;
                &lt;sourceDirectory&gt;${project.basedir}/src/main/scala&lt;/sourceDirectory&gt;
                &lt;testSourceDirectory&gt;${project.basedir}/src/test/scala&lt;/testSourceDirectory&gt;
                &lt;configLocation&gt;${project.basedir}/scalastyle_config.xml&lt;/configLocation&gt;
                &lt;outputFile&gt;${project.basedir}/scalastyle-output.xml&lt;/outputFile&gt;
                &lt;outputEncoding&gt;UTF-8&lt;/outputEncoding&gt;
            &lt;/configuration&gt;
            &lt;executions&gt;
                &lt;execution&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;check&lt;/goal&gt;
                    &lt;/goals&gt;
                &lt;/execution&gt;
            &lt;/executions&gt;
        &lt;/plugin&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
            &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;
            &lt;version&gt;${maven-surefire-plugin.version}&lt;/version&gt;
            &lt;configuration&gt;
                &lt;skipTests&gt;true&lt;/skipTests&gt;
            &lt;/configuration&gt;
        &lt;/plugin&gt;
    &lt;/plugins&gt;
&lt;/build&gt;
&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;jitpack.io&lt;/id&gt;
        &lt;url&gt;https://jitpack.io&lt;/url&gt;
        &lt;snapshots&gt;
            &lt;enabled&gt;false&lt;/enabled&gt;
        &lt;/snapshots&gt;
    &lt;/repository&gt;
    &lt;repository&gt;
        &lt;id&gt;dl.binatry.com&lt;/id&gt;
        &lt;url&gt;http://dl.bintray.com/spark-packages/maven&lt;/url&gt;
        &lt;snapshots&gt;
            &lt;enabled&gt;false&lt;/enabled&gt;
        &lt;/snapshots&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;
</code></pre>

<p>When I run it locally it works fine, the issue arrives when trying to run it in the cluster.</p>
<p>I try to exclude some conflict versions artificats but the issue persists.</p>
<p>Any help?</p>",0,0,2022-02-17 08:28:58.993000 UTC,,,0,scala|maven|apache-spark|azure-databricks,42,2022-02-17 07:38:59.607000 UTC,2022-03-04 07:15:26.923000 UTC,,1,0,0,1,,,,,,[]
How to handle errors for AIRFLOW (DatabricksSubmitRunOperator),"<p>I have Airflow dag , which runs series of Databricks notebook.</p>
<p>Now What i want is , if any one the notebook got failed? How to send mail to user that this notebook got failed with few details like execution date.</p>
<p>and is there any error handling way?</p>",2,1,2021-06-21 06:56:58.323000 UTC,,,3,airflow|azure-databricks,151,2021-05-26 06:28:38.243000 UTC,2022-03-03 06:39:00.907000 UTC,"Hyderabad, Telangana, India",71,0,0,10,,,,,,[]
Azure Data Flow compare two string with lookup,"<p>I'm using Azure Data flow to do some transformation on the data but I'm facing some challenges.
I have a use case where I have two streams, these two streams have some common data, and what I'm looking for is to output the common data between these two streams.
I do matching data with some common fields(product_name(string) and brand(string)), I have not got ID.
to do the matching, I picked lookup activity and I tried to compare the brand  in the two streams, but THE RESULT IS NOT CORRECT because for example:</p>
<blockquote>
<p>left stream : the brand = Estēe Lauder</p>
</blockquote>
<blockquote>
<p>right stream. : the brand = Estée Lauder</p>
</blockquote>
<p>for me this is the same brand, but they have different text format, I wanted to use 'like' operator but lookup activity does not support it, I'm using '==' operator to compare.</p>
<p>is there a way to override this problem please ?
<a href=""https://i.stack.imgur.com/z5z17.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z5z17.png"" alt=""enter image description here"" /></a></p>",1,0,2021-12-02 11:21:16.260000 UTC,,2021-12-02 11:27:24.320000 UTC,0,azure-data-factory|azure-databricks|azure-data-lake,57,2019-12-07 15:00:02.513000 UTC,2022-03-04 14:58:41.847000 UTC,France,201,0,0,39,,,,,,[]
Uploading data to Azure eventhub on daily basis,"<p>I have a job scheduler which is running daily in Azure Databricks notebook and output generated to a parquet file in Databricks.</p>
<p>I am creating Azure Eventhub where daily output of the parquet table will be uploaded.</p>
<p>My question is lets say on day1 data is uploaded to Eventhub and on day2 when i am trying to upload the data it should only append the data of day2, it should not upload the data of day1 and day2 together again.</p>
<p>Can you help me with the sample code?</p>",1,0,2021-10-06 11:14:52.077000 UTC,1.0,,1,azure-databricks|azure-eventhub,31,2018-05-28 09:46:44.583000 UTC,2022-03-02 08:05:29.517000 UTC,Hyderabad,103,20,0,6,,,,,,[]
Is there a better tool for performing merges in SVN (capable of using merge tracking info)?,"<p>From <a href=""https://stackoverflow.com/questions/2613525/what-makes-merging-in-dvcs-easy"">other questions</a> I gather that the strength of the DVCS, the ease of merges, comes from the fact that each revision knows its parents. Ever since SVN 1.5 came out <a href=""http://subversion.apache.org/docs/release-notes/release-history.html"" rel=""nofollow noreferrer"">over 2 years ago</a>, this is also available for SVN. So is there some tool which can use this information and make merges as easy in SVN as it is in DVCS?</p>",1,0,2010-12-22 09:03:36.340000 UTC,1.0,2017-05-23 12:18:40.967000 UTC,1,svn|merge|dvcs,137,2008-11-27 12:59:33.427000 UTC,2022-03-04 16:51:07.947000 UTC,Latvia,100434,673,74,5416,,,,,,[]
DROP multiple tables at once in Azure Databricks,"<p>I have two tables as below
Table 1: AllTablesInDatabase
Table 2: RequiredTablesInDatabase
Now, i want to drop all the table at a time which is not matching between these two tables using python or scala.</p>",1,0,2021-06-07 10:47:26.330000 UTC,,,0,python|azure-databricks,162,2018-08-18 09:36:18.800000 UTC,2021-06-25 04:52:49.920000 UTC,"Chennai, Tamil Nadu, India",148,28,0,31,,,,,,[]
How to remove case sensitive folder from Mercurial repo?,"<p>In BitBucket Mercurial repo, I have two folders: converter and Converter. </p>

<p>But when I clone the repo, I have only Converter folder that has content of both (MacOS). </p>

<p>I want to remove Converter folder. How can I do it?</p>",1,1,2012-04-17 12:50:06.950000 UTC,,,0,mercurial|dvcs,75,2012-04-17 12:28:06.103000 UTC,2015-05-25 16:26:04.137000 UTC,,1,0,0,2,,,,,,[]
Azure Databricks and Python - extract only those rows containing an email address from a string column,"<p>I am using Azure Databricks with Python 3.</p>
<p>I have a dataframe with several columns one being a string column called 'BodyJson' - this column is complex and one example column value is...</p>
<p>{&quot;Timestamp&quot;:3679584000,&quot;Sender&quot;:&quot;10.87.35.7:32768:wifivm0002EF&quot;,&quot;Type&quot;:&quot;1.3.6.1.4.1.9.9.599.0.8&quot;,&quot;CaptureTime&quot;:637616395229104341,&quot;Variables&quot;:[{&quot;Key&quot;:&quot;1.3.6.1.4.1.9.9.513.1.2.1.1.1.0&quot;,&quot;Value&quot;:&quot;1&quot;},{&quot;Key&quot;:&quot;1.3.6.1.4.1.9.9.513.1.1.1.1.5.104.188.12.52.58.0&quot;,&quot;Value&quot;:{&quot;Hex&quot;:&quot;6C6E646F6E622D7761703034&quot;,&quot;String&quot;:&quot;test@hotmail.com&quot;}},{&quot;Key&quot;:&quot;1.3.6.1.4.1.9.9.599.1.3.2.1.2.0&quot;,&quot;Value&quot;:1},{&quot;Key&quot;:&quot;1.3.6.1.4.1.9.9.599.1.3.2.1.3.0&quot;,&quot;Value&quot;:{&quot;Hex&quot;:&quot;0A410CA3&quot;,&quot;String&quot;:&quot;\nA\f?&quot;}},{&quot;Key&quot;:&quot;1.3.6.1.4.1.9.9.599.1.3.1.1.27.150.152.153.127.33.193&quot;,&quot;Value&quot;:{&quot;Hex&quot;:&quot;31323334313045678393338343430373340776C616E2E6D6E633031302E6D63633233342E336770706E6574776F723456ghy67&quot;,&quot;String&quot;:&quot;456789993@wlan.xxxx.mcc234.3gppnetwork.org&quot;}},{&quot;Key&quot;:&quot;1.3.6.1.4.1.9.9.599.1.3.1.1.28.150.152.153.127.33.193&quot;,&quot;Value&quot;:{&quot;Hex&quot;:&quot;57696669204578747261&quot;,&quot;String&quot;:&quot;Wifi Extra&quot;}},{&quot;Key&quot;:&quot;1.3.6.1.4.1.9.9.599.1.3.1.1.38.150.152.153.127.33.193&quot;,&quot;Value&quot;:{&quot;Hex&quot;:&quot;36306562373064302F39363A39383A39393A37663A32313A63312F3734313830383138&quot;,&quot;String&quot;:&quot;60eb70d0/96:67:99:8f:29:d1/74180818&quot;}},{&quot;Key&quot;:&quot;1.3.6.1.4.1.9.9.599.1.3.1.1.8.150.152.153.127.33.193&quot;,&quot;Value&quot;:{&quot;Hex&quot;:&quot;68DE0C663A00&quot;,&quot;String&quot;:&quot;h?\f4:\u0000&quot;}}]}</p>
<p>I want to extract all rows to a new dataframe where column 'BodyJson' contains an email address. The email addresses can be from anywhere in the world - &quot;test@hotmail.com&quot; or &quot;test1@yahoo.co.fr&quot; etc...
There are thousands of rows in this dataframe. Ignore those rows that don't contain an email.</p>
<p>Code so</p>
<p>#Load Avro files (multiple files loaded using wildcards: /<em>/</em>.avro )
avroDf = spark.read.format(&quot;com.databricks.spark.avro&quot;).load(InputRawFiles).withColumn(&quot;FileName&quot;,input_file_name())</p>
<p>avroDf=avroDf.withColumn(
&quot;BodyJson&quot;,
avroDf.Body.cast(&quot;string&quot;)
)
avroDf=avroDf.drop(avroDf.Body)</p>
<p>WRITE CODE TO SEARCH FOR EMAIL ADDRESSES IN COLUMN &quot;BodyJson&quot; HERE...</p>
<p>How do I do this maybe using a regular expression from library 're'?</p>",0,0,2021-07-12 10:45:52.347000 UTC,,2021-07-14 07:29:18.323000 UTC,0,python-3.x|azure-databricks,30,2012-03-04 21:19:21.190000 UTC,2022-02-10 15:17:16.487000 UTC,,117,9,0,37,,,,,,[]
"Python/Neptune/Gremlin: ""'list' object has no attribute 'next'""","<p>I trying to mirror the following gremlin code in Python to do pagination.</p>
<pre><code>gremlin&gt; t = g.V().hasLabel('person');[]
gremlin&gt; t.next(2)
==&gt;v[1]
==&gt;v[2]
gremlin&gt; t.next(2)
==&gt;v[4]
==&gt;v[6]
</code></pre>
<p>Here are the Python code</p>
<pre><code>    from neptune_python_utils.gremlin_utils import GremlinUtils
    from neptune_python_utils.endpoints import Endpoints

    GremlinUtils.init_statics(globals())

    endpoints = '...'
    gremlin_utils = GremlinUtils(endpoints)

    conn = gremlin_utils.remote_connection()
    g = gremlin_utils.traversal_source(connection=conn)

    t = g.V().hasLabel('my-label')

    cnt, ipp = True, 100
    while cnt:
        r = t.next(ipp)
        if not r: 
            cnt = False
</code></pre>
<p>But I'm getting error</p>
<pre><code>  &quot;errorMessage&quot;: &quot;'list' object has no attribute 'next'&quot;,
  &quot;errorType&quot;: &quot;AttributeError&quot;
  on line ---&gt; r = t.next(ipp)
</code></pre>
<p>The trace show that the first iteration for <code>r = t.next(ipp)</code> actually ran, but it returned a list object, so there is no .next()
anymore. How can I keep the traversal in the iterations?</p>",0,3,2021-08-19 13:56:56.007000 UTC,,2021-08-22 14:20:57.730000 UTC,1,gremlin|amazon-neptune|gremlinpython|neptune-python-utils,93,2012-02-03 16:20:42.710000 UTC,2022-03-04 16:51:03.243000 UTC,,5628,119,6,508,,,,,,[]
I am trying to validate phone numbers in a dataframe,"<p>I am trying to validate the Phone number in my dataframe. The code should flag the number as home or Mobile or Invalid</p>

<p>This is specific to UK phone number validation. And i tried Regex but it wont flag the number.</p>

<pre><code>import pandas as pd 
import re
# display(df)
# df.head()
df['Phonenumber']=df(df.withColumn('Phone_Number_Validity', if(isValid(df)):       
       # print (""Mobile Number"")      
       else : 
       # print (""Home Number"")))
display(df)  
</code></pre>

<p>Getting Syntax error</p>",2,4,2019-08-19 09:49:25.213000 UTC,,2019-08-20 08:39:40.907000 UTC,0,python|pandas|dataframe|azure-databricks,1629,2019-08-15 11:56:48.253000 UTC,2019-11-12 15:50:21.033000 UTC,,33,0,0,56,,,,,,[]
Distributed Source Control - pushing individual changesets,"<p>Working on a bit of a sticky problem and was hoping for some help from the community.  Basically, our dev team is split up into two teams, lets say ""Red"" and ""Blue""</p>

<p>3 repos:<br>
1: Master<br>
2: Red >> Clone of master<br>
3: Blue >> Clone of master  </p>

<p>Each developer is cloning red or blue on their local machine where they are working.  </p>

<p>Both teams are working on various tasks for our main application.  Each team has a clone of our Shared ""Master"" Repository on which they are applying their changesets.  The changesets are verified at that level, at which point they are ready to be pushed into the Master.</p>

<p>To simplify, lets say developer A and B are both on Red team.</p>

<p>So the problem comes when developer A pushes changeset 1, then developer B pushes changeset 2.   Then changeset 1 is verified and ready to go into Master but changeset 2 is not.  </p>

<p>I want to push changeset 1 to Master as soon as possible, and not wait for verification to changeset 2, especially since changeset 3 could be being introduced in the meantime.</p>

<p>We're currently using mercurial and I like it - I would be willing to switch to git if the workflow for what I want to do would be easier.  </p>

<p>Am i thinking about this wrong?  I appreciate the help. </p>",3,3,2010-05-04 18:51:31.897000 UTC,2.0,2010-05-04 19:17:45.883000 UTC,2,git|version-control|mercurial|dvcs|patch,262,2009-06-05 15:09:08.120000 UTC,2021-10-07 21:50:32.793000 UTC,"Austin, TX, United States",957,83,29,218,,,,,,[]
Git conflict with pull --rebase on unrelated file,"<p>I have a repo where I'm seeing behavior that I don't understand. </p>

<p>Ill refer to the repo that this is occuring on as the 'bad repo', all of the following sequences were run on that one.</p>

<p>It doesn't matter what commit I reset to, same conflicting behavior. The files it reports as conflicting seem to be the ones that were changed in the commit I reset to.</p>

<p>1) Sequence 2 is only happening on the 'bad repo', the same sequence of commands results in no conflicts on a fresh clone, or any one else's. What is it about the one persons repo could be causing this?</p>

<p>2) Why would adding an arbitrary file in sequence 2 cause the pull --rebase to conflict? It worked fine as in sequence 1 when there was no changes.</p>

<p>3) Basically, I don't understand why sequence 2 is causing conflicts, given that 1,3,4 all work fine.</p>

<pre>
.git/config has:

[branch ""media""]
    remote = origin
    merge = refs/heads/media
</pre>

<p>Here are the sequences of commands I'm running and the results:</p>

<h2>Sequence 1(reset and pull)</h2>

<pre>
$ git reset --hard 68a170d
HEAD is now at 68a170d Fixes issues with nested attribute sites

$ git status
# On branch media
# Your branch is behind 'origin/media' by 7 commits, and can be fast-forwarded.
#
nothing to commit (working directory clean)

$ git pull --rebase
First, rewinding head to replay your work on top of it...
Fast-forwarded media to 4c7d9cf046368d4c7770d3b590bf3c1d1f14d480.
</pre>

<h2>Sequence 2(reset add file pull)</h2>

<pre>
$ git reset --hard 68a170d
HEAD is now at 68a170d Fixes issues with nested attribute sites

$ touch someblahrandomfile
$ git add someblahrandomfile
$ git commit -m 'blah'
[media 9bf2bfb] blah
 0 files changed, 0 insertions(+), 0 deletions(-)
 create mode 100644 someblahrandomfile

$ git status
# On branch media
# Your branch and 'origin/media' have diverged,
# and have 1 and 7 different commit(s) each, respectively.
#
nothing to commit (working directory clean)

$ git pull --rebase
First, rewinding head to replay your work on top of it...
Applying: Fixed verify methods
Using index info to reconstruct a base tree...
Falling back to patching base and 3-way merge...
Auto-merging app/controllers/jet_controller.rb
Auto-merging app/models/claim.rb
Auto-merging app/models/site.rb
Auto-merging app/models/user.rb
CONFLICT (content): Merge conflict in app/models/user.rb
Failed to merge in the changes.
Patch failed at 0001 Fixed verify methods

When you have resolved this problem run ""git rebase --continue"".
If you would prefer to skip this patch, instead run ""git rebase --skip"".
To restore the original branch and stop rebasing run ""git rebase --abort"".
</pre>

<h2>Sequence 3(reset add file pull with extra params)</h2>

<pre>
$ git reset --hard 68a170d
HEAD is now at 68a170d Fixes issues with nested attribute sites

$ touch zz
$ git add zz
$ git commit -m 'blah4'
[media c79214d] blah4
 0 files changed, 0 insertions(+), 0 deletions(-)
 create mode 100644 zz

$ git status
# On branch media
# Your branch and 'origin/media' have diverged,
# and have 1 and 7 different commit(s) each, respectively.
#
nothing to commit (working directory clean)

$ git pull --rebase -- origin media
 * branch            media      -> FETCH_HEAD
First, rewinding head to replay your work on top of it...
Applying: blah4
</pre>

<h2>Sequence 4(reset and rebase)</h2>

<pre>
$ git reset --hard 68a170d
HEAD is now at 68a170d Fixes issues with nested attribute sites

$ touch vv
$ git add vv
$ git commit -m 'blah7'
[media 6c3f42b] blah7
 0 files changed, 0 insertions(+), 0 deletions(-)
 create mode 100644 vv

$ git status
# On branch media
# Your branch and 'origin/media' have diverged,
# and have 1 and 7 different commit(s) each, respectively.
#
nothing to commit (working directory clean)

$ git rebase origin/media
First, rewinding head to replay your work on top of it...
Applying: blah7
</pre>

<h2>Additional Info</h2>

<p>a) The bad repo is on a Mac osx 10.6.4<br>
b) Git 1.7.1<br>
c)  </p>

<pre>
color.branch=auto
color.diff=auto
color.status=auto
color.branch.current=yellow reverse
color.branch.local=yellow
color.branch.remote=green
color.diff.meta=yellow bold
color.diff.frag=magenta bold
color.diff.old=red bold
color.diff.new=green bold
color.status.added=yellow
color.status.changed=green
color.status.untracked=cyan
merge.tool=opendiff
mergetool.tool=opendiff
difftool.difftool=opendiff
gui.recentrepo=/git/MYREPO
user.name=USER
user.email=EMAIL
alias.wtf=git-wtf
alias.lg=log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr)%Creset' --abbrev-commit --date=relative
core.repositoryformatversion=0
core.filemode=true
core.bare=false
core.logallrefupdates=true
core.ignorecase=true
remote.origin.fetch=+refs/heads/*:refs/remotes/origin/*
remote.origin.url=URL
branch.master.remote=origin
branch.master.merge=refs/heads/master
branch.media.remote=origin
branch.media.merge=refs/heads/media
</pre>

<h2>Update 2 (first set of >>> should be pointing the other way but couldn't get them displaying correctly)</h2>

<pre>
$ git diff
diff --cc app/models/user.rb
index e1c31e2,f4923e6..0000000
--- a/app/models/user.rb
+++ b/app/models/user.rb

    has_many :coupon_codes
    accepts_nested_attributes_for :coupon_codes

 >>>>>>> HEAD
 =======
    has_many :sites, :dependent => :destroy
    accepts_nested_attributes_for :sites, :allow_destroy => true

 >>>>>>> Fixed verify methods
</pre>",1,8,2010-07-21 02:37:04.033000 UTC,1.0,2010-07-22 03:57:09.293000 UTC,2,git|version-control|dvcs,3041,2009-12-28 23:22:02.420000 UTC,2012-12-08 07:02:16.333000 UTC,"Santa Clara, CA",40,5,0,60,,,,,,[]
Does partition strategy helps on Gremlin traversal performance,"<p>I tried to play around with the partition strategy as what was mentioned here <a href=""https://tinkerpop.apache.org/docs/current/reference/"" rel=""nofollow noreferrer"">https://tinkerpop.apache.org/docs/current/reference/</a> .Initially, I expect that when I define a specific partition key for a zone and write some vertices on it, it would index that specific zones and improve the vertex lookup. Eventually, I realize that the partition key is just like another property value define within a vertex. In other words, these codes is nothing more but just a property value lookup which leads to full graph traversal scan:</p>
<blockquote>
<pre><code>g.withStrategies(new PartitionStrategy(partitionKey: &quot;_partition&quot;, writePartition: &quot;a&quot;, 
                                     readPartitions: [&quot;a&quot;]));
</code></pre>
</blockquote>
<p>I'm not sure what are the underlying logic of this partitionstrategy, but it does not seems to be improve the lookup if it really does full graph scan. Correct me if i;m wrong</p>",1,0,2021-11-19 07:02:14.710000 UTC,,,0,amazon-web-services|gremlin|amazon-neptune|azure-cosmosdb-gremlinapi|gremlinnet,81,2014-04-11 10:08:58.807000 UTC,2022-03-05 14:39:26.440000 UTC,,619,1,0,17,,,,,,[]
gremlin query using promise. Can't use it to return desired result,"<p>When using the following Gremlin statement to query AWS Neptune, the completableFuture is not returning the desired result. As i tried the following:</p>
<p><code>graphTraversal.promise(Traversal::hasNext());</code></p>
<p><strong>Scenario 1:</strong></p>
<p>It works for single vertex <code>g.V('id-1').promise(Traversal::hasNext())</code> returns true</p>
<p>But, when i use <code>g.V('id-1','id-2').promise(Traversal::hasNext())</code> it still returns <strong>true</strong> rather than <strong>false</strong> since <code>id-2</code> is not available in db</p>
<p><strong>Scenario 2:</strong></p>
<p>when creating list of edges as follows, how to make the query return false if any of the edges is not created.</p>
<pre><code>CompletableFuture&lt;Boolean&gt; method1() {
    List&lt;Edge&gt; edgeList = new ArrayList();  
    edgeList.add(edge1);// vertex2 -&gt; vertex3`  
    edgeList.add(edge2);// vertex1(not present) -&gt; vertex2  
    edgeList.add(edge3);// vertex3 -&gt; vertex4
    
    GraphTraversal g = null;
    
    loop(edgeList: e) {
        g = graphTraversalQueryMethod(g, e);
    }
    
    return g.promise(Traversal::hasNext);

}

GraphTraversal graphTraversalQueryMethod(GraphTraversal g, Edge e) {
    
    g.V(e.sourceVertexId).addE(&quot;EDGE_LABEL&quot;).property(e.propKey, e.propValue).to(e.destVertexId);
    
}
</code></pre>
<p>Its returning false but, the useCase: (suppose the source / destination vertices of the edge is not present)</p>
<ul>
<li>edge1 is getting created</li>
<li>edge2 not created since source vertex1 is missing</li>
<li>edge3 also not created even if it has both source and destination vertices available in the db</li>
<li>so result is returned as false with creating 1 (edge1) valid edge out of 2 (edge1, edge3)</li>
</ul>
<p>Is there any option to rollback the whole transaction using gremlin in neptune? to get this query executed after the missing vertex1 got created by some other way.</p>
<p>Or any idea to handle / capture the missing transaction. Please correct me if i need to change the query formation.</p>
<p>Note: At-least i need to capture the edges / any of the transaction is not succeeded on query execution. This is using Java.</p>
<p>Would be helpful, if got suggestion for any best practices to achieve this!</p>",1,0,2020-08-05 11:00:41.657000 UTC,,2020-08-05 18:11:57.440000 UTC,0,java|gremlin|amazon-neptune,182,2015-02-04 05:25:51.150000 UTC,2022-01-12 09:48:00.607000 UTC,"Chennai, Tamil Nadu, India",185,44,0,104,,,,,,[]
AWS Lambda finished before return call or a timeout,"<p>I have only seen this one time, but want to understand the reason behind it.</p>
<p>I have a AWS lambda function written in nodeJS that just add vertex to AWS neptune.</p>
<p>Here is the relevant code</p>
<pre><code>module.exports.userSignedUp = (event, context, callback) =&gt; {
  console.log('Event: ', JSON.stringify(event))
  console.log('context: ', context)
  context.callbackWaitsForEmptyEventLoop = false
  var dataTmp = JSON.parse(event.Records[0].Sns.Message)
  var errorData = {}
  var logData = {}
  var id

  var now = moment()  

  if (!dataTmp) {
    id = shortid.getShortId()
    
  } else {
    console.log('dataTmp: ', dataTmp)
    id = dataTmp.dupId    
    updateDoc(dataTmp, callback, logData, errorData)
  }
}

async function updateDoc (dataTmp, callback, logData, errorData) {
  try {
    console.log('inside updateDoc function')
    console.log('dc', dc)
    console.log('g', g)
    var userAdded = await g.addV('user').property('userId', dataTmp.userId).property('firstName', dataTmp.firstName).next()
    console.log('userAdded', userAdded)
    return callback(null)
  } catch (e) {
    console.log('error', e)        
    return callback(null)
  }
}
</code></pre>
<p>Timeout set for this function is 20 seconds.
Most of the time this functions just works fine and return in 11 to 50 ms.</p>
<p>This one time, I saw function just stopped execution after printing third console.log statement. Initially, I thought, lambda might be waiting for Neptune to return (await statement), but in that case lambda should timeout and retry again.</p>
<p>Here is the output from cloudwatch</p>
<p>Notice that after last console.log, execution ends</p>
<pre><code>INFO    g GraphTraversalSource { [+5ms]
  graph: Graph {},
  traversalStrategies: TraversalStrategies { strategies: [ [RemoteStrategy] ] },
  bytecode: Bytecode { sourceInstructions: [], stepInstructions: [] },
  graphTraversalSourceClass: [Function: GraphTraversalSource],
  graphTraversalClass: [Function: GraphTraversal]
}
REPORT  Duration: 42.51 ms  Billed Duration: 100 ms Memory Size: 1024 MB    Max Memory Used: 103 MB  [+46ms]
</code></pre>
<p>Usual output of the lambda looks like this</p>
<pre><code>g GraphTraversalSource { [+4ms]
  graph: Graph {},
  traversalStrategies: TraversalStrategies { strategies: [ [RemoteStrategy] ] },
  bytecode: Bytecode { sourceInstructions: [], stepInstructions: [] },
  graphTraversalSourceClass: [Function: GraphTraversalSource],
  graphTraversalClass: [Function: GraphTraversal]
}
2 hours ago INFO    userAdded { [+46ms]
  value: Vertex {
    id: '9cb979cb-427e-97d2-f265-fee7b8b94780',
    label: 'user',
    properties: undefined
  },
  done: false
}
REPORT  Duration: 43.74 ms  Billed Duration: 100 ms Memory Size: 1024 MB    Max Memory Used: 103 MB  [+47ms]
</code></pre>
<p>Any thoughts on what might be happening here and why this lambda is returning before the timeout.</p>
<p>Thanks</p>",0,3,2020-06-27 03:58:48.733000 UTC,1.0,2020-12-07 14:40:25.073000 UTC,1,node.js|amazon-web-services|aws-lambda|async-await|amazon-neptune,242,2014-06-30 22:12:58.957000 UTC,2022-03-05 14:04:30.870000 UTC,Massachusetts,551,11,0,48,,,,,,[]
"Git equivalent of ""hg id""?","<p>Does Git have any command equivalent to Mercurial's ""hg id""? I.e. a command that prints the parent commit's hash and a plus sign if there are changes in the working directory?</p>",5,3,2013-01-15 08:01:33.313000 UTC,3.0,2013-01-15 20:48:22.047000 UTC,21,git|version-control|mercurial|dvcs,3615,2009-06-26 11:10:05.430000 UTC,2022-02-23 15:08:54.143000 UTC,,1232,99,3,154,,,,,,[]
Using p4 zip and unzip to export files from one perforce server to another,"<p>I was trying to export files along with their revision history inside my depot folder from 2015.2 to 2019 perforce server.Also , I would want perforce to create new user on my new server corresponding to the commiter/submitter on my original 2015 repo.</p>

<p>Perforce replicate looked like overkill for my current task and then I came across this read on perforce's <a href=""https://community.perforce.com/s/article/15336"" rel=""nofollow noreferrer"">website</a> that mentioned P4 zip.</p>

<p>This looked like it will solve my problem, but the article has a few issues I could not understand.</p>

<p>Let's say I am moving data from server1_ip:port --> server2_ip:port</p>

<p>I am currently following these steps</p>

<ol>
<li><p>Making zip of folder to be copied using </p>

<ul>
<li><code>p4 remote my_remote_spec</code> , setting </li>
<li><code>Address: server1_ip:port</code> </li>
<li><code>DepotMap://depot/... //depot2/...</code></li>
</ul></li>
<li><p><code>p4 -p server1_ip:port zip -o test.zip -r my_remote_spec -A //depot/...</code>. But on this step I get permission denied error. This is weird to me because the user although not super/admin has access to files i ask to get zipped.</p></li>
</ol>

<p>Also, when i did try with a super user, i could not find test.zip even though i was not prompted any errors. </p>

<ol>
<li>Isn't the above command supposed to generate a zip file inside the directory which i run it from? </li>
<li>Is the unzip command supposed to be run after a p4 login from user of second server? </li>
<li>Lastly, from the document why is a third port , 1667 mentioned in the transfer of files from server running on 1666 and 1777.</li>
</ol>",1,0,2020-05-19 16:45:48.950000 UTC,,,2,version-control|perforce|dvcs|perforce-client-spec|p4java,475,2015-07-05 12:23:42.183000 UTC,2021-11-16 03:03:05.313000 UTC,,331,41,0,85,,,,,,[]
Unable to read CSV in pyspark with special characters [ ] included,"<p>I have been trying to read from a CSV with pyspark where the filename contains special characters. However, spark throws me an error . Is there any way to read this file using spark ?.</p>

<p>I have tried reading the file with name ""[dbo].[sample_To_text_Common].csv"". I also tried with escaping character as well.The same file has read in Azure Data Bricks Notebook using spark by playing with filename. </p>

<p>Code that worked in azure data bricks</p>

<pre><code>
data_file = ""[dbo].[sample_To_text_Common].csv"".replace(""["",""?"").replace(""]"",""?"")

df = spark.read.option(""header"",""true"").csv(data_file)

</code></pre>

<p>In spark local mode it throws me an error like below ( i tried the above in local mode as well ) </p>

<pre><code>df = spark.read.option(""header"",""true"").csv(""[dbo].[sample_To_text_Common].csv"")


pyspark.sql.utils.AnalysisException: u""Path does not exist: '[dbo].[sample_To_text_Common].csv';""

</code></pre>",0,2,2019-07-29 14:06:53.553000 UTC,,,0,python|csv|apache-spark|pyspark|azure-databricks,340,2017-07-31 08:21:15.850000 UTC,2020-02-12 12:31:17.360000 UTC,"Cochin, Kerala, India",86,0,0,20,,,,,,[]
neptune drop vertex property with acknowledgement,"<p>I tried Gremlin Tinkerpop query for Amazon neptune to drop properties of Vertex. It is working fine, but couldn't get acknowledgement either TRUE / FALSE on dropping properties.</p>

<p>But, i could manage get boolean for updating / adding properties of the Vertex</p>

<p><em><code>g.V('id').properties('property_1','property_2').drop()</code></em></p>

<p>I'm expecting it to return TRUE / FALSE on successful query execution.</p>

<p>It would be more helpful if someone gives heads Up on this.</p>

<p>Note: Since trying CompletableFuture for Query execution, looking on boolean result on successful execution.</p>",2,0,2020-03-12 12:23:13.273000 UTC,,,2,gremlin|completable-future|tinkerpop|amazon-neptune,352,2015-02-04 05:25:51.150000 UTC,2022-01-12 09:48:00.607000 UTC,"Chennai, Tamil Nadu, India",185,44,0,104,,,,,,[]
Error in Post Requst (Kusto & Databricks),"<p>I'm trying to follow the example code <a href=""https://docs.microsoft.com/en-us/azure/data-explorer/spark-connector"" rel=""nofollow noreferrer"">here</a>:</p>

<p>Here is my code:</p>

<pre><code>import com.microsoft.kusto.spark.datasource.KustoOptions
import com.microsoft.kusto.spark.sql.extension.SparkExtension._
import org.apache.spark.SparkConf
import org.apache.spark.sql._

val cluster = dbutils.secrets.get(scope = ""key-vault-secrets"", key = ""ClusterName"")
val client_id = dbutils.secrets.get(scope = ""key-vault-secrets"", key = ""ClientId"")
val client_secret = dbutils.secrets.get(scope = ""key-vault-secrets"", key = ""ClientSecret"")
val authority_id = dbutils.secrets.get(scope = ""key-vault-secrets"", key = ""TenantId"")
val database = ""db""
val table = ""tablename""

val conf: Map[String, String] = Map(
      KustoOptions.KUSTO_AAD_CLIENT_ID -&gt; client_id,
      KustoOptions.KUSTO_AAD_CLIENT_PASSWORD -&gt; client_secret,
      KustoOptions.KUSTO_QUERY -&gt; s""$table | top 100""      
    )

// Simplified syntax flavor
import org.apache.spark.sql._
import com.microsoft.kusto.spark.sql.extension.SparkExtension._
import org.apache.spark.SparkConf

val df = spark.read.kusto(cluster, database, """", conf)
display(df)
</code></pre>

<p>However this gives me this error:</p>

<pre><code>com.microsoft.azure.kusto.data.exceptions.DataServiceException: Error in post request
    at com.microsoft.azure.kusto.data.Utils.post(Utils.java:106)
    at com.microsoft.azure.kusto.data.ClientImpl.execute(ClientImpl.java:89)
    at com.microsoft.azure.kusto.data.ClientImpl.execute(ClientImpl.java:45)
    at com.microsoft.kusto.spark.utils.KustoDataSourceUtils$.getSchema(KustoDataSourceUtils.scala:103)
    at com.microsoft.kusto.spark.datasource.KustoRelation.getSchema(KustoRelation.scala:102)
    at com.microsoft.kusto.spark.datasource.KustoRelation.schema(KustoRelation.scala:36)
    at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:450)
    at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)
    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:283)
    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:201)
    at com.microsoft.kusto.spark.sql.extension.SparkExtension$DataFrameReaderExtension.kusto(SparkExtension.scala:19)
    at linef172a4a7eaa6435fa4ff9fec071cf03535.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-1810687702746193:25)
    at linef172a4a7eaa6435fa4ff9fec071cf03535.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-1810687702746193:86)
    at linef172a4a7eaa6435fa4ff9fec071cf03535.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-1810687702746193:88)
    at linef172a4a7eaa6435fa4ff9fec071cf03535.$read$$iw$$iw$$iw.&lt;init&gt;(command-1810687702746193:90)
    at linef172a4a7eaa6435fa4ff9fec071cf03535.$read$$iw$$iw.&lt;init&gt;(command-1810687702746193:92)
    at linef172a4a7eaa6435fa4ff9fec071cf03535.$read$$iw.&lt;init&gt;(command-1810687702746193:94)
    at linef172a4a7eaa6435fa4ff9fec071cf03535.$read.&lt;init&gt;(command-1810687702746193:96)
    at linef172a4a7eaa6435fa4ff9fec071cf03535.$read$.&lt;init&gt;(command-1810687702746193:100)
    at linef172a4a7eaa6435fa4ff9fec071cf03535.$read$.&lt;clinit&gt;(command-1810687702746193)
    at linef172a4a7eaa6435fa4ff9fec071cf03535.$eval$.$print$lzycompute(&lt;notebook&gt;:7)
</code></pre>

<p>Any ideas?</p>",2,1,2019-07-22 18:23:04.877000 UTC,,,0,azure-databricks|azure-data-explorer,578,2012-10-20 16:18:44.083000 UTC,2022-03-03 10:41:10.747000 UTC,,5331,89,4,402,,,,,,[]
Not able to connect to Azure database for MySQL server from databricks cluster using jdbc and spark connector,"<p>I am trying to connect to Azure database for MySQL server using databricks clusters.
I tried using 2 ways described below-</p>

<ol>
<li><p>using jdbc</p>

<pre><code>val jdbcHostname = ""&lt;serverName&gt;.mysql.database.azure.com""
val jdbcPort = 3306
val jdbcDatabase = ""&lt;db&gt;""
val jdbcUrl = s""jdbc:mysql://${jdbcHostname}:${jdbcPort}/${jdbcDatabase}?useSSL=true&amp;requireSSL=false""

import java.sql.DriverManager
val connection = DriverManager.getConnection(jdbcUrl, ""&lt;user&gt;@&lt;serverName&gt;"", ""&lt;password&gt;"")
</code></pre></li>
</ol>

<p>but I got the error saying 
<code>""Client with IP address 'SOME_IP_ADDRESS' is not allowed to connect to this MySQL server""</code></p>

<p>I added this ip to firewall rules of Azure database for MySQL server and was able to access then. But everytime cluster restarts, ip address changes and it throws error.
I don't want to <code>""Allow access to Azure services""</code> in mySQL server as it will allow users from another subscription as well. </p>

<ol start=""2"">
<li><p>using Spark connector- downlaoded ""com.microsoft.azure:azure-sqldb-spark:1.0.2"" jar</p>

<pre><code>val config = Config(Map(
""driver""         -&gt; ""org.mariadb.jdbc.Driver"",
""url""            -&gt; ""&lt;serverName&gt;.mysql.database.azure.com:3306"",
""databaseName""   -&gt; ""&lt;db&gt;"",
""dbTable""        -&gt; ""&lt;dbtable&gt;"",
""user""           -&gt; ""&lt;user&gt;@&lt;serverName&gt;"",
""password""       -&gt; ""&lt;password&gt;""
))
val data = spark.read.sqlDB(config)
</code></pre></li>
</ol>

<p>But it throws error saying
<code>""java.lang.IllegalArgumentException: requirement failed: The driver could not open a JDBC connection. Check the URL: jdbc:sqlserver://&lt;serverName&gt;.mysql.database.azure.com:3306""</code></p>

<p>I tried in this way also-</p>

<pre><code>    val df = spark.read.format(""jdbc"").option(""driver"", ""org.mariadb.jdbc.Driver"")
   .option(""url"", ""jdbc:mysql://&lt;serverName&gt;.mysql.database.azure.com:3306/&lt;db&gt;?useSSL=true&amp;requireSSL=false"")
   .option(""databaseName"", ""&lt;db&gt;"")
   .option(""dbTable"", ""&lt;dbtable&gt;"")
   .option(""user"", ""&lt;user&gt;@&lt;serverName&gt;"")
   .option(""password"", ""&lt;password&gt;"")
   .load()
</code></pre>

<p>but it also throws error
<code>""Client with IP address 'SOME_IP_ADDRESS' is not allowed to connect to this MySQL server""</code></p>",1,2,2020-02-06 05:33:23.243000 UTC,,,1,mysql|azure|apache-spark|azure-databricks|azure-sql-server,1024,2020-01-09 10:43:22.670000 UTC,2022-03-03 11:31:16.833000 UTC,,23,0,0,5,,,,,,[]
Does gremlin support the bulkloader with neptune?,<p>Neptune's newest update includes bulkload with a CSV file. Does the js lv of Gremlin support this?</p>,1,1,2019-07-31 17:52:37.113000 UTC,,2019-08-04 12:21:29.840000 UTC,0,gremlin|amazon-neptune,210,2019-05-31 15:11:19.277000 UTC,2021-04-13 00:22:44.427000 UTC,"Portland, OR, USA",41,0,0,7,,,,,,[]
Using case class to import csv in scala,"<p>Team,
In databricks, currently we are using StructType to create the data schema and load the csv file to a dataframe, doing necessary operations and exporting it into CSV.
We would like to see if there are any other approaches to increase performance. 
Can we create objects and load file using case classes? 
If there is any such process, can some one share link/documentation?</p>

<p>and Which one will be more efficient: Object or Dataframe?</p>

<p>Thanks,
Srini</p>",0,4,2019-10-07 17:09:45.647000 UTC,,,0,scala|case-class|azure-databricks|import-csv,420,2018-06-18 02:04:37.287000 UTC,2022-01-20 20:06:06.133000 UTC,"San Diego, CA, USA",17,9,0,15,,,,,,[]
How to access on premise Teradata from Azure Databricks,"<p>We need to connect to on premise Teradata from Azure Databricks .</p>

<p>Is that possible at all ? </p>

<p>If yes please let me know how .</p>",3,1,2019-02-15 17:41:24.240000 UTC,,,1,teradata|azure-virtual-network|azure-databricks,738,2014-07-17 03:21:33.427000 UTC,2020-12-14 16:44:41.383000 UTC,"Kolkata, West Bengal, India",379,1,0,20,,,,,,[]
Optimising AWS Neptune (Gremlin) Elastic Search query,"<p>I have a graph structure detailed in the image below (with 900k vertices and 2.2m edges). running on AWS Neptune. The aim is to return a set of item ids that match the text search and remove any excluded items (based on items that are restricted from the group, items that are &quot;exclusive items&quot; tied to another group, or items that have been purchased by the user). Current performance of the query is 550ms to 700ms, I need to get performance to under 100ms.</p>
<p><a href=""https://i.stack.imgur.com/cJTPK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cJTPK.png"" alt=""enter image description here"" /></a></p>
<p>I understand generally to optimise Neptune queries you should try use native steps e.g. In the profiler it states <code>fold</code> is not supported natively by Neptune however removing fold actually makes performance worse (around 950ms) and results in the warning that <code>ChooseStep... is not Supported natively</code>, whilst removing the <code>chooseStep</code> further degrades performance and results in <code>SelectOneStep(last,items) is not natively supported...</code></p>
<p>Is there a way to rewrite this query with Neptune native steps or generally improve the performance?</p>
<pre><code>   g.withSideEffect(&quot;Neptune#fts.endpoint&quot;, &quot;elastic search endpoint&quot;)
    .V().hasLabel('item').has('*', 'Neptune#fts test_item')
    .aggregate('items')
    .V().hasLabel('group')
    .where(out('exclusive_item_edge'))
    .not(hasId('group_id'))
    .out('exclusive_item_edge')
    .dedup()
    .aggregate('excludedItems')
    .fold()
    .V(&quot;group_id&quot;)
    .out('group_restriction_edge')
    .choose(
      has('name', 'intersection_restriction'),
      out()
        .out()
        .groupCount()
        .unfold()
        .where(select(values).is(gt(1)))
        .select(keys),
      out().out()
    )
    .dedup()
    .not(in('exclusive_item_edge')
         .hasId(within(&quot;group_id&quot;)))
    .aggregate('excludedItems')
    .fold()
    .V(&quot;user_id&quot;)
    .out('user_item_purchase_edge')
    .dedup()
    .aggregate('excludedItems')
    .fold()
    .select('items')
    .unfold()
    .where(without('excludedItems'))
    .id()
</code></pre>",0,0,2021-05-07 15:52:58.880000 UTC,,2021-05-07 16:02:00.947000 UTC,0,amazon-web-services|elasticsearch|gremlin|amazon-neptune,147,2017-07-24 21:33:44.633000 UTC,2022-01-07 10:48:45.223000 UTC,,13,0,0,1,,,,,,[]
Configure BigQuery dataset location in Azure Databricks,"<p>I need to query a table from Big Query using Azure Databricks. When I follow the official documentation (code sample below, full documentation on <a href=""https://docs.databricks.com/data/data-sources/google/bigquery.html?_ga=2.161210173.1875630365.1604488036-1915266480.1595344991#example-notebooks"" rel=""nofollow noreferrer"">https://docs.databricks.com/data/data-sources/google/bigquery.html?_ga=2.161210173.1875630365.1604488036-1915266480.1595344991#example-notebooks</a>), I receive an error: <em>com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Not found: Dataset [projectId]:[datasetId] was not found in location US</em>.</p>
<p>I found out that for a dataset with location US the code below works, so it seems US is the default location. But my main dataset has a different location (EU) and so I need to be able to set the location.</p>
<pre class=""lang-py prettyprint-override""><code>tableName = projectId + '.' + datasetId + '.' + tableId
tempLocation = 'databricks_testing'
query = &quot;SELECT count(*) FROM `{table}`&quot;.format(table = tableName)

# load the result of a SQL query on BigQuery into a DataFrame
df = (spark.read.format(&quot;bigquery&quot;) 
      .option(&quot;materializationDataset&quot;, tempLocation)
      .option(&quot;query&quot;, query) 
      .load() 
      .collect()
     )

display(df)
</code></pre>",0,2,2020-11-04 12:28:06.773000 UTC,,,0,python|google-bigquery|azure-databricks,315,2020-02-25 14:24:37.827000 UTC,2022-03-04 12:44:34.930000 UTC,,35,4,0,8,,,,,,[]
How to mount data with Azure Blob Storage?,"<p>I'm brand new to Azure Databricks, and my mentor suggested I complete the Machine Learning Bootcamp at </p>

<p><a href=""https://aischool.microsoft.com/en-us/machine-learning/learning-paths/ai-platform-engineering-bootcamps/custom-machine-learning-bootcamp"" rel=""nofollow noreferrer"">https://aischool.microsoft.com/en-us/machine-learning/learning-paths/ai-platform-engineering-bootcamps/custom-machine-learning-bootcamp</a></p>

<p>Unfortunately, after successfully setting up Azure Databricks, I've run into some issues in step 2.  I successfully added the 1_01_introduction file to my workspace as a notebook.  However, while the tutorial talks about teaching how to mount data in Azure Blob Storage, it seems to skip that step, which causes all of the next tutorial coding steps to throw errors.  The first code bit (which the tutorial tells me to run), and the error that comes up afterwards, are included below.</p>

<p>%run ""../presenter/includes/mnt_blob""</p>

<p>Notebook not found: presenter/includes/mnt_blob. Notebooks can be specified via a relative path (./Notebook or ../folder/Notebook) or via an absolute path (/Abs/Path/to/Notebook). Make sure you are specifying the path correctly.</p>

<p>Stacktrace:
  /1_01_introduction: python</p>

<p>As far as I can tell, the Azure Blob storage just isn't set up yet, and so the code I run (as well as the code in all of the following steps) can't find the tutorial items that are supposed to be stored in the blob.  Any help you fine folks can provide would be most appreciated.</p>",1,0,2019-06-26 13:48:03.237000 UTC,1.0,,1,python|azure|azure-blob-storage|azure-databricks,5192,2019-06-26 13:31:56.990000 UTC,2020-12-08 08:29:52.200000 UTC,,11,0,0,3,,,,,,[]
Partition by columns: data being truncated to another partition,"<p>I'm using <code>repartitionByRange</code> in PySpark while saving over 2,000+ CSV's.</p>
<pre><code>df.repartitionByRange(&lt;no of unique values of col&gt;, col).write\
        .option(&quot;sep&quot;, &quot;|&quot;)\
        .option(&quot;header&quot;, &quot;true&quot;)\
        .option(&quot;quote&quot;,  '&quot;')\
        .option(&quot;escape&quot;, '&quot;')\
        .option(&quot;nullValue&quot;, &quot;null&quot;)\
        .option(&quot;quoteAll&quot;, &quot;true&quot;)\
        .mode('overwrite')\
        .csv(path)
</code></pre>
<p>And then renaming each partition with the unique id of column  that they contain. However, around 1-2% of the CSV's being generated have more than one unique id. Please assist resolving this issue of incorrect repartitioning.</p>",1,0,2020-12-17 19:30:33.873000 UTC,,2020-12-19 00:00:36.237000 UTC,0,pyspark|partitioning|azure-databricks,48,2020-12-17 19:15:17.910000 UTC,2022-01-12 18:07:22.397000 UTC,,1,0,0,0,,,,,,[]
Neptune AWS sortOrder asc and desc don't give reverses of each other,"<p>I am searching for with <code>my_table</code> and I get all the tables I expect, sorted by score (though I am not sure if it's in the correct order since I don't know how to get the scores from elasticsearch).</p>
<p>When I reverse it (i.e. from sortOrder DESC to ASC), I do not get the exact reverse. What is the reason for this?</p>
<pre><code>When I search with sortOrder DESC, I get the results [my_table, table_2, my_table].
When I search with sortOrder ASC , I get the results [my_table, my_table, table_2].
Something weird is that I would expect both my_table to be ranked equally and table_2 to be ranked lower but that doesn't seem to be the case based on the ordering.


limit = 100
g.withSideEffect(
    &quot;Neptune#fts.endpoint&quot;, f&quot;{url}&quot;
)
.withSideEffect(&quot;Neptune#fts.queryType&quot;, &quot;query_string&quot;)
.withSideEffect(&quot;Neptune#fts.maxResults&quot;, limit)
.withSideEffect(&quot;Neptune#enableResultCache&quot;, enable_cache)
.withSideEffect(&quot;Neptune#fts.sortOrder&quot;, &quot;DESC&quot;). # &lt;- switching this doesn't flip the order
.V()
.hasLabel(&quot;table&quot;)
.has(
    &quot;*&quot;,
    f&quot;Neptune#fts entity_type:&quot;table&quot; AND ({query})&quot;,
)
</code></pre>",1,3,2021-12-20 07:21:30.547000 UTC,,,0,gremlin|amazon-neptune|gremlinpython,41,2019-06-30 00:43:58.337000 UTC,2022-03-04 03:03:33.920000 UTC,,159,11,0,18,,,,,,[]
Git tool to remove lines from staging if they consist only of changes in whitespace,"<p>The point in removing trailing whitespace is that if everyone does it always then you end up with a diff that is minimal, ie. it consists only of code changes and not whitespace changes.</p>

<p>However when working with other people who do not practice this, removing all trailing whitespace with your editor or a pre-commit hook results in an <em>even worse</em> diff. You are doing the opposite of your intention.</p>

<p>So I am asking here if there is a tool that I can run manually before I commit that unstages lines from staging that are only changes in whitespace.</p>

<p>Also a bonus would be to change the staged line to have trailing whitespace removed for lines that have code changes.</p>

<p>Also a bonus would be to not do this to Markdown files (as trailing space has meaning in Markdown).</p>

<p>I am asking here as I fully intend to write this tool if it doesn't already exist.</p>",3,3,2009-11-17 20:39:50.357000 UTC,5.0,2010-03-29 04:20:33.273000 UTC,12,git|version-control|dvcs,2616,2008-09-15 10:08:20.597000 UTC,2022-02-15 14:59:41.387000 UTC,"Savannah, GA, United States",25222,1030,225,4928,,,,,,[]
Connect to Azure Data Lake Gen 2 from local Spark job,"<p>I'm trying to connect from a local Spark job to my ADLS Gen 2 data lake to read some Databricks delta tables, which I've previously stored through a Databricks Notebook, but I'm getting a very weird exception, which I can't sort out:</p>

<pre><code>Exception in thread ""main"" java.io.IOException: There is no primary group for UGI &lt;xxx&gt; (auth:SIMPLE)
    at org.apache.hadoop.security.UserGroupInformation.getPrimaryGroupName(UserGroupInformation.java:1455)
    at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.&lt;init&gt;(AzureBlobFileSystemStore.java:136)
    at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:108)
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)
    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
    at org.apache.spark.sql.delta.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:94)
</code></pre>

<p>Searching around, I've not found many hints on this. One, which I tried was to pass the config ""spark.hadoop.hive.server2.enable.doAs"", ""false"", but it didn't help out.</p>

<p>I'm using io.delta 0.3.0, Spark 2.4.2_2.12 and azure-hadoop 3.2.0. 
I can connect to my Gen 2 account without issues through an Azure Databricks Cluster/ Notebook.</p>

<p>I'm using code like the folling:</p>

<pre><code> try(final SparkSession spark = SparkSession.builder().appName(""DeltaLake"").master(""local[*]"").getOrCreate()) {
            //spark.conf().set(""spark.hadoop.hive.server2.enable.doAs"", ""false"");
            spark.conf().set(""fs.azure.account.key.stratify.dfs.core.windows.net"", ""my gen 2 key"");
            spark.read().format(""delta"").load(""abfss://myfs@myaccount.dfs.core.windows.net/Test"");
}
</code></pre>",1,0,2019-09-11 16:12:30.510000 UTC,2.0,2019-09-12 15:35:26.680000 UTC,2,java|azure|apache-spark|azure-databricks|delta-lake,963,2019-09-11 15:57:51.690000 UTC,2019-11-26 19:59:13.603000 UTC,"Zürich, Switzerland",21,0,0,2,,,,,,[]
How to perform insert overwrite dynamically on partitions of Delta file using PySpark?,"<p>I'm new to pyspark and looking for overwriting a delta partition dynamically. From the other resources available online I could see that spark supports dynamic partition by setting the below conf as ""dynamic""</p>

<p>spark.conf.set(""spark.sql.sources.partitionOverwriteMode"", ""dynamic"")</p>

<p>However, when I try overwriting the partitioned_table with a dataframe, the below line of code in pyspark (databricks) overwrites the entire table instead of a single partition on delta file.</p>

<p>data.write.insertInto(""partitioned_table"", overwrite = True)</p>

<p>I did come across the option of using Hive external table, but it is not straight forward in my case since the partitioned_table is based out of Delta file.</p>

<p>Please let me know what am I missing here. Thanks in advance!</p>",1,0,2020-06-08 08:53:51.463000 UTC,2.0,2020-06-08 10:00:17.963000 UTC,1,apache-spark|pyspark|azure-databricks|delta-lake|table-partitioning,2504,2020-06-08 03:18:20.897000 UTC,2021-01-13 02:08:55.373000 UTC,"Chennai, Tamil Nadu, India",11,0,0,1,,,,,,[]
Backing Out a backwards merge on Mercurial,"<p>How do you reverse the effect of a merge on polarised branches without dying of agony?</p>

<p>This problem has been plaguing me for <strong>months</strong> and I have finally given up. </p>

<p>You have 1 Repository, with 2  <strong>Named</strong> Branches.  A and B. </p>

<p>Changes that occur to A will inevitably occur on B. </p>

<p>Changes that occur directly on B MUST NEVER occur on A. </p>

<p>In such a configuration, merging ""B"" into ""A"" produces a dire problem in the repository, as all the changes to B appear in A as if they were made in A. </p>

<p>The only ""normal"" way to recover from this situation appears to be ""backing out"" the merge, ie: </p>

<pre><code> hg up -r A 
 hg backout -r BadMergeRev --parent BadMergerevBeforeOnA 
</code></pre>

<p>Which looks all fine and dandy, until you decide to merge later in the correct direction, and you end up with all sorts of nasty things happening and code that was erased / commented out on specifically branch B suddenly becomes unerased or uncommented. </p>

<p>There has not been a working viable solution to this so far other than ""let it do its thing, and then hand fix all the problems"" and that to be honest is a bit fubar.  </p>

<p>Here is an image clarifying the problem: </p>

<p><em>[Original image lost]</em></p>

<p>Files C &amp; E ( or changes C &amp; E ) must appear only on branch b, and not on branch a.
Revision A9 here ( branch a, revno 9 ) is the start of the problem. </p>

<p>Revisions A10 and A11 are the ""Backout merge"" and ""merge the backout"" phases. </p>

<p>And revision B12 is mercurial, erroneously repeatedly dropping a change that was intended  not to be dropped. </p>

<p>This Dilemma has caused much frustration and blue smoke and I would like to put an end to it. </p>

<h3>Note</h3> 

<p>It may be the obvious answer to try prohibiting the reverse merge from occurring, either with hooks or with policies, I have found the ability to muck this up is rather high and the chance of it happening so likely that even with countermeasures, you <em>must</em> still assume that inevitably, it <em>will</em> happen so that you can solve it when it does.</p>

<h3>To Elaborate</h3>

<p>In the model I have used Seperate files. These make the problem sound simple. These merely represent <em>arbitrary changes</em> which could be a separate line. </p>

<p>Also, to add insult to injury, there have been substantial changes on branch A which leaves the standing problem ""do the changes in branch A conflict with the changes in branch B which just turned up ( and got backed out ) which looks like a change on branch A instead "" </p>

<h3>On History Rewriting Tricks:</h3>

<p>The problem with all these retro-active solutions is as follows:</p>

<ol>
<li>We have 9000 commits. </li>
<li>Cloning freshly thus takes half an hour</li>
<li>If there exists <em>even one</em> bad clone of the repository <em>somewhere</em>, there is a liklihood of it comming back in contact with the original repository, and banging it up all over again. </li>
<li>Everyone has cloned this repository already, and now several days have passed with on-going commits.</li>
<li>One such clone, happens to be a live site, so ""wiping that one and starting from scratch"" = ""big nono"" </li>
</ol>

<p>( I admit, many of the above are a bit daft, but they are outside of my control ). </p>

<p>The only solutions that are viable are the ones that assume that people <em>can</em> and <em>will</em> do everything wrong, and that there is a way to 'undo' this wrongness. </p>",5,4,2008-11-05 17:25:11.050000 UTC,42.0,2012-02-23 22:18:48.367000 UTC,77,mercurial|merge|dvcs|branch,14273,2008-09-17 11:12:12.813000 UTC,2021-01-16 07:00:44.187000 UTC,New Zealand,54942,891,134,7100,,,,,,[]
Configuring the Azure AD Databricks SCIM application with Terraform,"<p>I am trying to create and configure the <a href=""https://azuremarketplace.microsoft.com/en-us/marketplace/apps/aad.azuredatabricks"" rel=""nofollow noreferrer"">Azure Databricks SCIM Provisioning Connector</a>, so I can provision users in my Databricks workspace from AAD.</p>
<p>Following <a href=""https://docs.microsoft.com/en-us/azure/databricks/administration-guide/users-groups/scim/aad"" rel=""nofollow noreferrer"">these instructions</a>, I can get it to work manually. That is, creating and setting up the application in Azure Portal works and my selected users synchronise in Databricks. (The process wasn't completely straightforward. A lot of fiddling, which I don't remember, with the provisioning setup was needed before it did anything.)</p>
<p>When I try to transpose this into Terraform, I'm not getting very far:</p>
<ul>
<li><p>I can create the application with Terraform, using the same Service Principal that created the Databricks Workspace resource:</p>
<pre><code>data &quot;azuread_application_template&quot; &quot;scim&quot; {
  display_name = &quot;Azure Databricks SCIM Provisioning Connector&quot;
}

resource &quot;azuread_application&quot; &quot;scim&quot; {
  display_name = &quot;${var.name}-scim&quot;
  template_id  = data.azuread_application_template.scim.template_id

  feature_tags {
    enterprise = true
    gallery    = true
  }
}
</code></pre>
<p>Similarly, I can create the Databricks access token for my Service Principal very easily:</p>
<pre><code>resource &quot;databricks_token&quot; &quot;scim&quot; {
  comment = &quot;SCIM Integration&quot;
}
</code></pre>
</li>
<li><p>Now I'm stuck:</p>
<ol>
<li>How do I define the users and groups for the enterprise application in Terraform? I don't see any <code>azuread</code> resource that looks appropriate.</li>
<li>Likewise, how do I configure the provisioning for the enterprise application in Terraform (i.e., with the SCIM endpoint URL and Databricks token, etc.)?</li>
</ol>
</li>
</ul>
<p>(Aside: I note that, in my Terraform-created application, if I proceed to manually set up the users and provisioning in Azure Portal, it doesn't seem to do anything. I may be being impatient: the &quot;Provision on Demand&quot; button does actually work, but the polled synchronisation is either not doing anything or being really slow.)</p>
<p>(Edit: An update on the aside: The polled provisioning -- set up manually on a Terraform-managed SCIM app -- has now run twice since I wrote this question. In which time, it has not synchronised the users I manually selected, but instead has decided to delete the &quot;Provision on Demand&quot; user in Databricks that I created earlier...)</p>",1,3,2022-01-31 12:52:38.443000 UTC,1.0,2022-01-31 16:11:13.743000 UTC,2,azure|azure-active-directory|terraform|azure-databricks|scim,131,2011-08-03 15:27:03.330000 UTC,2022-03-05 18:46:32.603000 UTC,United Kingdom,8085,178,29,552,,,,,,[]
How can I optimize a query in Neptune that needs to filter a few edges in a node with thousands,"<p>I'm having performace issues with a query in Gremlin using AWS Neptune graph database.</p>
<p>This is the scenario:</p>
<p><img src=""https://i.stack.imgur.com/hyhuM.png"" alt=""1"" /></p>
<p>Basically, there are 5000+ users connected by the same IP node.</p>
<p>I want all users that have a connection with the ip node at a date that matches one of the dates of the connections from user-1 with a window of 1 day. For example, starting from <strong>user-1</strong> I want to find only <strong>user-2</strong> and <strong>user-4</strong>.</p>
<p>I already have a query that works, thanks to the responses in <a href=""https://stackoverflow.com/questions/69711744/gremlin-with-neptune-filter-edges-using-math-result-with-previous-edge-value"">this question</a> I posted a while back, and looks something like this:</p>
<pre><code>g.V('user-1')
   .outE().as('ip_edges')
   .inV().inE('uses_ip').as('related')
   .where(P.lte('related')).by(math('ip_edges - 86400000').by('date_millis')).by('date_millis')
   .where(P.gte('related')).by(math('ip_edges + 86400000').by('date_millis')).by('date_millis')
   .outV()
</code></pre>
<p>But I'm experiencing performance issues in this scenario because the query is traversing through all of the 5000+ edges of the ip node.</p>
<p>I understand that <em>Neptune</em> has indexes that should allow me to filter edges by the property <code>date_millis</code> without having to go through all 5000+ edges. But I'm failing to write a query that actually uses those indexes.</p>
<p>This is how the profiling of the query looks like (the node ids are a bit different because i simplified it for the example here):</p>
<pre><code>*******************************************************
                Neptune Gremlin Profile
*******************************************************

Query String
==================
g.V('user-lt1001').outE().as('ip_edges').inV().inE('uses_ip').as('related').where(P.lte('related')).by(math('ip_edges - 86400000').by('at_millis')).by('at_millis').where(P.gte('related')).by(math('ip_edges + 86400000').by('at_millis')).by('at_millis').outV()

Original Traversal
==================
[GraphStep(vertex,
    [user-lt1001
    ]), VertexStep(OUT,edge)@[ip_edges
    ], EdgeVertexStep(IN), VertexStep(IN,
    [uses_ip
    ],edge)@[related
    ], WherePredicateStep(lte(related),
    [
        [MathStep(ip_edges - 86400000,
            [value(at_millis)
            ])
        ], value(at_millis)
    ]), WherePredicateStep(gte(related),
    [
        [MathStep(ip_edges + 86400000,
            [value(at_millis)
            ])
        ], value(at_millis)
    ]), EdgeVertexStep(OUT)
]

Optimized Traversal
===================
Neptune steps: [
    NeptuneGraphQueryStep(Edge) {
        JoinGroupNode {
            PatternNode[(?1=&lt;user-lt1001&gt;, ?5, ?3, ?6) . project ?1,?6,?3 . IsEdgeIdFilter(?6) .
            ],
            {estimatedCardinality=3, expectedTotalOutput=2, indexTime=0, joinTime=0, numSearches=1, actualTotalOutput=2
            }
            PatternNode[(?8, ?10=&lt;uses_ip&gt;, ?3, ?11) . project ?3,?11 . IsEdgeIdFilter(?11) .
            ],
            {estimatedCardinality=10022, indexTime=0, joinTime=13, numSearches=1
            }
        }, annotations={path=[Vertex(?1):GraphStep, Edge(?6):VertexStep@[ip_edges
                ], Vertex(?3):EdgeVertexStep, Edge(?11):VertexStep@[related
                ]
            ], joinStats=true, optimizationTime=1, maxVarId=12, executionTime=633
        }
    },
    NeptuneTraverserConverterStep
]
+ not converted into Neptune steps: [WherePredicateStep(lte(related),
    [
        [MathStep(ip_edges - 86400000,
            [value(at_millis)
            ]), ProfileStep
        ], value(at_millis)
    ]), NoOpBarrierStep(2500), WherePredicateStep(gte(related),
    [
        [MathStep(ip_edges + 86400000,
            [value(at_millis)
            ]), ProfileStep
        ], value(at_millis)
    ]), NoOpBarrierStep(2500), EdgeVertexStep(OUT)
]

WARNING: &gt;&gt; WherePredicateStep(lte(related),
[
    [MathStep(ip_edges - 86400000,
        [value(at_millis)
        ]), ProfileStep
    ], value(at_millis)
]) &lt;&lt; (or one of its children) is not supported natively yet

Physical Pipeline
=================
NeptuneGraphQueryStep
    |-- StartOp
    |-- JoinGroupOp
        |-- SpoolerOp(1000)
        |-- DynamicJoinOp(PatternNode[(?1=&lt;user-lt1001&gt;, ?5, ?3, ?6) . project ?1,?6,?3 . IsEdgeIdFilter(?6) .
],
{estimatedCardinality=3, expectedTotalOutput=2
})
        |-- SpoolerOp(1000)
        |-- DynamicJoinOp(PatternNode[(?8, ?10=&lt;uses_ip&gt;, ?3, ?11) . project ?3,?11 . IsEdgeIdFilter(?11) .
],
{estimatedCardinality=10022
})

Runtime (ms)
============
Query Execution: 633.282

Traversal Metrics
=================
Step                                                               Count  Traversers       Time (ms)    % Dur
-------------------------------------------------------------------------------------------------------------
NeptuneGraphQueryStep(Edge)                                         9358        9358          20.431     3.23
NeptuneTraverserConverterStep                                       9358        9358          23.427     3.70
WherePredicateStep(lte(related),
[
    [MathStep(ip_e...                     7           7         588.350    92.96
  MathStep(ip_edges - 86400000,
        [value(at_millis)
        ])                  9358        9358         293.918
NoOpBarrierStep(2500)                                                  7           7           0.036     0.01
WherePredicateStep(gte(related),
        [
            [MathStep(ip_e...                     5           5           0.542     0.09
  MathStep(ip_edges + 86400000,
                [value(at_millis)
                ])                     7           7           0.285
NoOpBarrierStep(2500)                                                  5           5           0.023     0.00
EdgeVertexStep(OUT)                                                    5           5           0.118     0.02
                                            &gt;TOTAL                     -           -         632.929        -

Predicates
==========
# of predicates: 38

Results
=======
Count: 5
Output: [v[user-lt1001
                    ], v[user-lt1004
                    ], v[user-lt1001
                    ], v[user-lt1003
                    ], v[user-lt1002
                    ]
                ]


Index Operations
================
Query execution:
    # of statement index ops: 18737
    # of unique statement index ops: 4686
    Duplication ratio: 4.00
    # of terms materialized: 0

</code></pre>
<p>To compare execution times, while this query takes 600+ ms, the same query without those 5000 extra edges takes 8ms.</p>
<p><strong>EDIT 1</strong></p>
<p>Here's a query that improves the execution times, but stills traverses all the edges.</p>
<pre><code>g.V('user-1')
   .outE().as('ip_edges')
   .values('at_millis').math('_ + 86400001').as('plus_one_day')
   .select('ip_edges').values('at_millis').math('_ - 86400001').as_('minus_one_day')
   .select('ip_edges')
   .inV().inE('uses_ip').as('result')
   .values('at_millis')
   .where(P.between('minus_one_day', 'plus_one_day'))
   .select('result')
   .outV()
</code></pre>
<p>And this is the profiling of this query:</p>
<pre><code>*******************************************************
                Neptune Gremlin Profile
*******************************************************

Query String
==================
g.V('user-lt1001').outE().as('ip_edges').values('at_millis').math('_ + 86400001').as('plus_one_day').select('ip_edges').values('at_millis').math('_ - 86400001').as_('minus_one_day').select('ip_edges').inV().inE('uses_ip').as('result').values('at_millis').where(P.between('minus_one_day', 'plus_one_day')).select('result').outV()

Original Traversal
==================
[GraphStep(vertex,
    [user-lt1001
    ]), VertexStep(OUT,edge)@[ip_edges
    ], PropertiesStep([at_millis
    ],value), MathStep(_ + 86400001)@[plus_one_day
    ], SelectOneStep(last,ip_edges), PropertiesStep([at_millis
    ],value), MathStep(_ - 86400001)@[minus_one_day
    ], SelectOneStep(last,ip_edges), EdgeVertexStep(IN), VertexStep(IN,
    [uses_ip
    ],edge)@[result
    ], PropertiesStep([at_millis
    ],value), WherePredicateStep(and(gte(minus_one_day), lt(plus_one_day))), SelectOneStep(last,result), EdgeVertexStep(OUT)
]

Optimized Traversal
===================
Neptune steps: [
    NeptuneGraphQueryStep(PropertyValue) {
        JoinGroupNode {
            PatternNode[(?1=&lt;user-lt1001&gt;, ?5, ?3, ?6) . project ?1,?6 . IsEdgeIdFilter(?6) .
            ],
            {estimatedCardinality=3, expectedTotalOutput=2, indexTime=0, joinTime=0, numSearches=1, actualTotalOutput=2
            }
            PatternNode[(?6, ?7=&lt;at_millis&gt;, ?8, &lt;~&gt;) . project ?6,?8 .
            ],
            {estimatedCardinality=8892, indexTime=0, joinTime=0, numSearches=1
            }
        }, annotations={path=[Vertex(?1):GraphStep, Edge(?6):VertexStep@[ip_edges
                ], PropertyValue(?8):PropertiesStep
            ], joinStats=true, optimizationTime=1, maxVarId=9, executionTime=271
        }
    },
    NeptuneTraverserConverterStep
]
+ not converted into Neptune steps: [MathStep(_ + 86400001)@[plus_one_day
    ], NoOpBarrierStep(2500), SelectOneStep(last,ip_edges), NoOpBarrierStep(2500), PropertiesStep([at_millis
    ],value), MathStep(_ - 86400001)@[minus_one_day
    ], NoOpBarrierStep(2500), SelectOneStep(last,ip_edges), NoOpBarrierStep(2500), EdgeVertexStep(IN), VertexStep(IN,
    [uses_ip
    ],edge)@[result
    ], PropertiesStep([at_millis
    ],value), WherePredicateStep(and(gte(minus_one_day), lt(plus_one_day))), NoOpBarrierStep(2500), SelectOneStep(last,result), NoOpBarrierStep(2500), EdgeVertexStep(OUT)
]

WARNING: &gt;&gt; MathStep(_ + 86400001)@[plus_one_day
] &lt;&lt; (or one of its children) is not supported natively yet

Physical Pipeline
=================
NeptuneGraphQueryStep
    |-- StartOp
    |-- JoinGroupOp
        |-- SpoolerOp(1000)
        |-- DynamicJoinOp(PatternNode[(?1=&lt;user-lt1001&gt;, ?5, ?3, ?6) . project ?1,?6 . IsEdgeIdFilter(?6) .
],
{estimatedCardinality=3, expectedTotalOutput=2
})
        |-- SpoolerOp(1000)
        |-- DynamicJoinOp(PatternNode[(?6, ?7=&lt;at_millis&gt;, ?8, &lt;~&gt;) . project ?6,?8 .
],
{estimatedCardinality=8892
})

Runtime (ms)
============
Query Execution: 271.410

Traversal Metrics
=================
Step                                                               Count  Traversers       Time (ms)    % Dur
-------------------------------------------------------------------------------------------------------------
NeptuneGraphQueryStep(PropertyValue)                                   2           2           0.338     0.12
NeptuneTraverserConverterStep                                          2           2           0.058     0.02
MathStep(_ + 86400001)@[plus_one_day
]                                  2           2           0.085     0.03
NoOpBarrierStep(2500)                                                  2           2           0.027     0.01
SelectOneStep(last,ip_edges)                                           2           2           0.015     0.01
NoOpBarrierStep(2500)                                                  2           2           0.012     0.00
PropertiesStep([at_millis
],value)                                      2           2           0.215     0.08
MathStep(_ - 86400001)@[minus_one_day
]                                 2           2           0.064     0.02
NoOpBarrierStep(2500)                                                  2           2           0.051     0.02
SelectOneStep(last,ip_edges)                                           2           2           0.014     0.01
NoOpBarrierStep(2500)                                                  2           2           0.012     0.00
EdgeVertexStep(IN)                                                     2           2           0.097     0.04
VertexStep(IN,
[uses_ip
],edge)@[result
]                              9358        9358          28.307    10.45
PropertiesStep([at_millis
],value)                                   9358        9358         233.549    86.18
WherePredicateStep(and(gte(minus_one_day), lt(p...                     5           5           8.080     2.98
NoOpBarrierStep(2500)                                                  5           5           0.042     0.02
SelectOneStep(last,result)                                             5           5           0.013     0.01
NoOpBarrierStep(2500)                                                  5           5           0.013     0.00
EdgeVertexStep(OUT)                                                    5           5           0.012     0.00
                                            &gt;TOTAL                     -           -         271.012        -

Predicates
==========
# of predicates: 38

Results
=======
Count: 5
Output: [v[user-lt1001
    ], v[user-lt1003
    ], v[user-lt1002
    ], v[user-lt1004
    ], v[user-lt1001
    ]
]


Index Operations
================
Query execution:
    # of statement index ops: 9366
    # of unique statement index ops: 4686
    Duplication ratio: 2.00
    # of terms materialized: 0
</code></pre>
<p>Any help will be really appreciated! Thanks!</p>",1,0,2021-11-10 23:26:05.057000 UTC,,2021-11-11 23:10:01.867000 UTC,1,gremlin|amazon-neptune,60,2016-01-08 02:23:54.773000 UTC,2022-02-25 23:08:12.203000 UTC,"Montevideo Departamento de Montevideo, Uruguay",71,6,0,8,,,,,,[]
ConcurrentModificationException in amazon neptune using gremlin javascript language variant,"<p>I am trying to check and insert 1000 vertices in chunk using <code>promise.all()</code>. The code is as follows:</p>
<pre><code>public async createManyByKey(label: string, key: string, properties: object[]): Promise&lt;T[]&gt; {
    const promises = [];
    const allVertices = __.addV(label);
    const propKeys: Array&lt;string&gt; = Object.keys(properties[0]);
     
    for(const propKey of propKeys){
      allVertices.property(propKey, __.select(propKey));
    }

    const chunkedProperties = chunk(properties, 5); // [[&quot;demo-1&quot;, &quot;demo-2&quot;, &quot;demo-3&quot;, &quot;demo-4&quot;, &quot;demo-5&quot;], [...], ...]
    
    for(const property of chunkedProperties){
        const singleQuery = this.g.withSideEffect('User', property)
       .inject(property)
       .unfold().as('data')
       .coalesce(__.V().hasLabel(label).where(eq('data')).by(key).by(__.select(key)), allVertices).iterate();

       promises.push(singleQuery);
     }

    const result = await Promise.all(promises);

    return result;
  }
</code></pre>
<p>This code throws ConcurrentModificationException. Need help to fix/improve this issue.</p>",1,0,2021-11-11 17:54:27.290000 UTC,,2021-11-12 08:40:36.097000 UTC,0,javascript|gremlin|amazon-neptune|gremlinjs|aws-neptune,90,2018-03-10 13:00:46.637000 UTC,2022-03-04 08:22:11.997000 UTC,"Mumbai, Maharashtra, India",17,4,0,12,,,,,,[]
How to resolve error getting while saving xgboost model in azure databricks?,"<p>I am trying to save model in azure databricks but getting error - &quot;It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.&quot;</p>
<p>Following is the code which I am using :-</p>
<pre><code>import dill as dill
with open('model.sav', &quot;wb&quot;) as f:
    dill.dump(model,f)
</code></pre>
<p>or is there any other way to download model to local without using mleap or mlflow?</p>
<p>I am using this code totrain model.</p>
<pre><code>pipe = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])

pipe_xgb = make_pipeline(pipe,DataFrameScaler(),xgb.XGBClassifier(objective='binary:logistic'))
param_grid_classifier = {
                        'xgbclassifier__n_estimators':[100,200], 
                         'xgbclassifier__learning_rate' :[0.1,0.01],
                         'xgbclassifier__colsample_bytree':[0.3,0.5],
                         'xgbclassifier__max_depth': [4],
                        'xgbclassifier__reg_lambda':[0.01,0.1],
                         'xgbclassifier__n_jobs':[4]
                        }
metric = 'average_precision'
grid_search1 = GridSearchCV(pipe_xgb, param_grid=param_grid_classifier,scoring=metric, cv=2)

gridmodel1 = grid_search1.fit(X_train, y_train_label)
model = gridmodel1.best_estimator_
</code></pre>
<p>Thanks in advance for help.</p>",0,2,2021-05-25 05:36:40.927000 UTC,,2021-06-23 06:54:56.897000 UTC,1,python-3.x|azure-databricks|dill,89,2020-11-01 14:11:43.950000 UTC,2021-11-23 09:01:13.227000 UTC,"Indore, Madhya Pradesh, India",106,5,0,11,,,,,,[]
Chopping image using polygons via pyspark udfs on azure databricks,"<p>I have a dataframe with an attribute which contains geometries defined via WKT.  I have a single image.  I want to segment an image into the boundaries defined by these polygons.</p>
<p>I had planned to use rasterio and to distribute the image (after reading) via the udf.  However, it appears the rasterio internal memory cannot be pickled and distributed.  Here is a code snippet describing what I was attempting to accomplish.</p>
<pre><code>img = rasterio.open('/dbfs/mnt/mymount/USDA_cropscape_exports/CDL_Iowa_2020.tif')

def cliplu ( wkt_in ) :
  poly = shwkt.loads( wkt_in )
  sub_image_nd = rasterio.mask.mask( img , [poly] , crop=True )[0].read(1)
  return sub_image_nd  

clip_landuseimg_udf = pyspkF.udf( lambda x : cliplu(x) , StringType() )

section_boundaries_df = section_boundaries_df.withColumn( 'clipped_img' , clip_landuseimg_udf( 'bbox_wkt' ) )
</code></pre>
<p>Is there any way to distribute segmenting using libraries compatible with databricks on Azure?</p>
<p>PS: I am unable to install rasterframes due to an incompatibility with databricks on Azure.</p>",0,0,2021-04-24 01:07:57.300000 UTC,0.0,,0,pyspark|azure-databricks|rasterio,22,2012-02-09 20:11:42.350000 UTC,2022-03-02 20:27:20.643000 UTC,"Sioux City, IA",325,242,4,50,,,,,,[]
AWS Neptune Gremlin C# GraphTraversalSource not working,"<p>I've seen multiple examples of this being done (various languages) which suggests this should work. Perhaps I'm missing a step? Commented out the lines that indicate other things I've tried.</p>
<p>Here's how I'm getting my gremlin client and also a graphTraversalSource to use directly.</p>
<pre><code>var gremlinServer = new GremlinServer(endpoint, 8182, enableSsl: true);
GremlinClient = new GremlinClient(gremlinServer);

//var remoteConnection = new DriverRemoteConnection(GremlinClient, &quot;g&quot;);
var remoteConnection = new DriverRemoteConnection(GremlinClient);
//g = AnonymousTraversalSource.Traversal().WithRemote(remoteConnection);
g = new Graph().Traversal().WithRemote(remoteConnection);
</code></pre>
<p>If I submit queries as strings like this:</p>
<pre><code>var gndrSetCnt = GremlinQueryCount(GremlinClient, &quot;g.V().count().next();&quot;);
var gndrResult = gndrSetCnt.Result;
</code></pre>
<p>and then....</p>
<pre><code>private async Task&lt;long&gt; GremlinQueryCount(GremlinClient gremlinClient, string query)
{
    return await gremlinClient.SubmitWithSingleResultAsync&lt;long&gt;(query);
}
</code></pre>
<p>that works fine, as clumsy as it is. However, if I try to use the &quot;g&quot; directly, like this:</p>
<pre><code>var example = g.V().Count().Next();
</code></pre>
<p>then I get an error like this:</p>
<pre><code>Gremlin.Net.Driver.Exceptions.ResponseException: 'InvalidRequestArguments: {&quot;detailedMessage&quot;:&quot;A message with [bytecode] op code requires the [aliases] argument to be a Map containing one alias assignment named 'g'.&quot;,&quot;requestId&quot;:&quot;ae024dd7-0fca-472b-acc6-7f717ca4bf2d&quot;,&quot;code&quot;:&quot;InvalidParameterException&quot;}'
</code></pre>
<p>Am I missing a step? I've seen this in multiple examples where nothing else seems to have been done, but I confess, only one in C# and that was only partial code, more of a tutorial. No aliases seem to have been injected, g just seems to be available by default? Again note I'm using g in the submitted groovy script, and that works.</p>
<p>For the record as per a suggestion, we added logging and this is what a sample statement produced:</p>
<p>&quot;RequestMessage{, requestId=709ba190-0ce9-4272-aadb-4b28c21accf6, op='bytecode', processor='traversal', args={gremlin={$type=System.Collections.Generic.Dictionary<code>2[[System.String, mscorlib],[System.Object, mscorlib]], mscorlib, @type=g:Bytecode, @value={$type=System.Collections.Generic.Dictionary</code>2[[System.String, mscorlib],[System.Collections.Generic.IEnumerable<code>1[[System.Collections.Generic.IEnumerable</code>1[[System.Object, mscorlib]], mscorlib]], mscorlib]], mscorlib, step={$type=System.Linq.Enumerable+WhereSelectListIterator<code>2[[Gremlin.Net.Process.Traversal.Instruction, Gremlin.Net],[System.Collections.Generic.IEnumerable</code>1[[System.Object, mscorlib]], mscorlib]], System.Core, $values=[[V], [hasLabel, article], [has, languageCode, fr-FR], [count]]}}}, aliases={$type=System.Collections.Generic.Dictionary<code>2[[System.String, mscorlib],[System.Object, mscorlib]], mscorlib, g=g}, $type=System.Collections.Generic.Dictionary</code>2[[System.String, mscorlib],[System.Object, mscorlib]], mscorlib}}&quot;</p>
<p>I'm not entirely sure if that's helpful. The original error message is suggesting that somehow the statement isn't starting with &quot;g&quot; but I don't see why it isn't, given what I'm doing - which is building a gts object from a drm which has &quot;g&quot; as the traveral source.</p>",1,1,2020-08-11 11:33:26.723000 UTC,,2020-08-13 09:07:16.670000 UTC,3,c#|gremlin|amazon-neptune,219,2020-08-11 11:28:12.290000 UTC,2021-09-28 10:35:47.487000 UTC,,31,0,0,2,,,,,,[]
using scala variable in shell script,"<p>I am running a databricks notebook from Azure devops pipeline using Execute Databricks notebook task. I am passing vertical name and branch name to my notebooks. Using dbutils am able to get the required values. I have Scala code and shell script code in the same notebook in different cells. I have to use the notebook parameters in the shell script which is in same notebook different cell.
Can someone please suggest me how i can use notebook parameters in the shell script</p>

<pre><code>%scala
val verticalName =  dbutils.widgets.get(""vertical"") 
val branchName =  dbutils.widgets.get(""branch"")
println(verticalName)
println(branchName)
%sh
echo $verticalName 
echo $branchName

</code></pre>",0,0,2020-01-30 13:59:07.283000 UTC,,,1,scala|shell|azure-databricks,228,2019-02-11 11:08:51.977000 UTC,2021-04-27 05:32:27.987000 UTC,,13,0,0,4,,,,,,[]
Embarringly Parallel Structured Streaming: Eventhub and Databricks,"<p><strong>Background</strong></p>
<p>I work for a company that produces online games (1000's of them) and are looking for some guidance from the greater community.  We have only been using Azure Databricks and Apache Spark for about a week, so chances are we doing some things wrong(This is why I'm reaching out)</p>
<p><strong>Business Requirement</strong></p>
<p>We are trying to use Structured Streaming and Azure Databricks to process around 17 Million Events using a sliding window.  The business requirement states that we need to figure out how many players are playing a specific game for the last 24 hours, hopping forward every one minute.  Essentially using the Sliding Window code <code>window(&quot;EventTimeUtc&quot;, &quot;24 hours&quot;, &quot;1 minute&quot;)</code></p>
<p><strong>EventHub Namespace Config</strong></p>
<ul>
<li>Eventhubs (Topic): 1</li>
<li>Incoming Events: 17 Million Events a Day</li>
<li>Eventhub: 32 Partitions</li>
<li>Throughput Units: 8</li>
<li>Partitioning: Events are partitioned properly, Ie there is no relation between events in partition 1 and 2, so that can be processed in parallel.</li>
</ul>
<p><strong>Test A</strong></p>
<p>With no aggregate query, ie just partitioning going into delta lake by MID, CID. Here is the stats.</p>
<ul>
<li>Input Rate 10K/sec</li>
<li>Processing Rate: 10K/sec</li>
<li>Time: 3-4 Seconds</li>
</ul>
<p>From this, I'm satisfied that Event Hub is sending records at an appropriate speed (Might be a false assumption)</p>
<p>From there, I applied an aggregate query with <code>approx_count_distinct</code> and the rate drops significantly. I'm obviously expecting the rate to drop, but not so drastically.</p>
<ul>
<li>Input Rate 200 - 300/sec</li>
<li>Processing Rate: 200 - 300/sec</li>
<li>Time: 2-3 Min</li>
</ul>
<p><strong>Questions</strong></p>
<ol>
<li><p>What can I do to process the micro-batches in parallel. Ie. Align the amount of micro-batches triggered with the amount of partitions from Eventhub? Is it possible, Is it recommended?</p>
</li>
<li><p>Is there anything in my code that contributes to slowing down the processing rate, perhaps I'm not decoding and serializing properly?</p>
</li>
</ol>
<p><strong>Full Code</strong></p>
<pre><code>#######################################
# Configure Streaming From Event Hub  #
#######################################
# https://tsmatz.github.io/azure-databricks-exercise/exercise08-streaming-eventhub.html
# https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md#user-configuration for Spark 3.1 (Encrypt Connection String Required)

# Library: com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.20
import json
from pyspark.sql.types import *
from pyspark.sql.functions import *
from datetime import datetime
from time import mktime

# Global Variables
conf = {}
startTime = &quot;2021-07-15T00:00:00.000000Z&quot;
endTime = &quot;2021-07-15T23:59:59.000000Z&quot;

# Set starting time eventhub stream
startingEventPosition = {
  &quot;offset&quot;: None,
  &quot;seqNo&quot;: -1, #not in use
  &quot;enqueuedTime&quot;: startTime,
  &quot;isInclusive&quot;: True
}

# Set end time eventhub stream
endingEventPosition = {
  &quot;offset&quot;: None, #not in use
  &quot;seqNo&quot;: -1, #not in use
  &quot;enqueuedTime&quot;: endTime,
  &quot;isInclusive&quot;: True
}

# Environment Variables
connectionString = &quot;xxxxxx&quot;
conf[&quot;eventhubs.connectionString&quot;] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString)
conf[&quot;eventhubs.consumerGroup&quot;] = &quot;xxxxxx&quot;
conf[&quot;eventhubs.startingPosition&quot;] = json.dumps(startingEventPosition)
conf[&quot;eventhubs.endingPosition&quot;] = json.dumps(endingEventPosition)

# Start Reading Event Hub Stream
streamingInputDF = (
  spark
    .readStream
    .format(&quot;eventhubs&quot;)
    .options(**conf)
    .load()
)
# Returns True for DataFrames that have streaming sources
streamingInputDF.isStreaming  

###############################
# Decode &amp; Clean Stream Data  #
###############################

# Define Structure
event_schema = StructType([
  StructField(&quot;MID&quot;, IntegerType(), False),
  StructField(&quot;CID&quot;, IntegerType(), False),
  StructField(&quot;PID&quot;, IntegerType(), False),
  StructField(&quot;EventTimeUtc&quot;, TimestampType(), False),
  StructField(&quot;UserID&quot;, StringType(), False)
])

# Decode 
decodedInputDF = ( 
  streamingInputDF
    .select(from_json(col(&quot;body&quot;)
    .cast(&quot;string&quot;), event_schema).alias(&quot;payload&quot;)
   )
)

# Clean and Flatten
cleanInputDF = (
  decodedInputDF
    .withColumn(&quot;EventTimeUtc&quot;,to_timestamp(col(&quot;payload.EventTimeUtc&quot;), &quot;yyyyMMdd HH:mm:ss&quot;))
    .withColumn(&quot;MID&quot;,col(&quot;payload.MID&quot;))
    .withColumn(&quot;CID&quot;,col(&quot;payload.CID&quot;))
    .withColumn(&quot;UserID&quot;,col(&quot;payload.UserID&quot;))
    .drop(&quot;payload&quot;)
)

# https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#reporting-metrics-using-dropwizard
spark.conf.set(&quot;spark.sql.streaming.metricsEnabled&quot;, &quot;true&quot;)

#######################
# Sandbox Tests      ##
#######################

# https://databricks.com/blog/2016/05/19/approximate-algorithms-in-apache-spark-hyperloglog-and-quantiles.html
# https://docs.delta.io/latest/best-practices.html#language-python

# Test A - approx_count_distinct with window - query
test_a_query = (
  cleanInputDF
    .withWatermark(&quot;EventTimeUtc&quot;, &quot;24 hours&quot;)
    .groupBy(
      &quot;MID&quot;,
      &quot;CID&quot;,
      window(&quot;EventTimeUtc&quot;, &quot;24 hours&quot;, &quot;1 minute&quot;).alias(&quot;HopWindow&quot;)
    ).agg(approx_count_distinct(&quot;UserID&quot;).alias(&quot;PlayerCount&quot;))
)

# Test A - approx_count_distinct with window - output sink delta tables
test_a_output = (
  test_a_query
    .writeStream
    .format(&quot;delta&quot;)
    .outputMode(&quot;complete&quot;) # https://docs.databricks.com/delta/delta-streaming.html#complete-mode
    .option(&quot;checkpointLocation&quot;, &quot;/mnt/delta/spinevents/_checkpoints/test_a&quot;)
    .queryName(&quot;test_a&quot;)
    .table(&quot;test_a&quot;) # https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-sink
)
</code></pre>
<p><strong>Cluster Utilization</strong></p>
<p><a href=""https://i.stack.imgur.com/jJFx0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jJFx0.png"" alt=""enter image description here"" /></a></p>
<p><strong>Monitoring Output</strong></p>
<p><a href=""https://i.stack.imgur.com/e07pk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e07pk.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/EJGiD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EJGiD.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/GXH4I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GXH4I.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/No1do.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/No1do.png"" alt=""enter image description here"" /></a></p>",0,2,2021-07-19 12:45:29.713000 UTC,1.0,2021-07-20 22:21:34.890000 UTC,2,azure|apache-spark|azure-databricks|azure-eventhub,283,2019-07-09 09:08:33.843000 UTC,2022-03-05 18:32:02.650000 UTC,,53,1,0,4,,,,,,[]
Connecting to Neptune using aiogremlin,"<p>I am trying to connect to AWS Neptune using <a href=""https://pypi.org/project/aiogremlin/"" rel=""nofollow noreferrer"">aiogremlin</a> but keep getting SSL certificate errors. I tried using 3 different certificates downloaded from <a href=""https://www.amazontrust.com/repository"" rel=""nofollow noreferrer"">Amazon Trust Repository</a> but none of them work. With <code>AmazonRootCA1.pem</code> and <code>SFSRootCAG2.pem</code> I keep getting <code>File Not Found error</code> and with <code>SFSRootCAG2.cer</code> I get <code>ssl_context.load_cert_chain(ssl.SSLError: [SSL] PEM lib (_ssl.c:4046)</code>.</p>
<p>Here is the snippet which I am using to interact with Neptune.</p>
<pre class=""lang-py prettyprint-override""><code>import asyncio
from aiogremlin import DriverRemoteConnection, Graph
from .constants import NEPTUNE_ENDPOINT, CERT_DIR


async def go():
    remote_connection = await DriverRemoteConnection.open(f'https://{NEPTUNE_ENDPOINT}:8182/gremlin', 'g',
                                                          ssl_certfile=CERT_DIR+'SFSRootCAG2.cer')
    g = Graph().traversal().withRemote(remote_connection)
    vertices = await g.V().toList()
    await remote_connection.close()
    return vertices

print(asyncio.get_event_loop().run_until_complete(go()))
</code></pre>
<p>Having trouble figuring out if I am using the wrong certificate file or doing something else that is wrong.</p>",1,6,2021-07-14 08:05:02.197000 UTC,,,1,python-3.x|ssl|gremlin|amazon-neptune|goblin,140,2014-03-15 03:14:17.787000 UTC,2022-01-16 18:03:34.097000 UTC,,97,8,0,24,,,,,,[]
Promote Row 1 as Column Heading - Spark DataFrame,"<p>I got below Spark Data Frame.</p>
<p><a href=""https://i.stack.imgur.com/cXntx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cXntx.png"" alt=""enter image description here"" /></a></p>
<p>I want to promote Row 1 as column Headings and the new spark DataFrame should be</p>
<p><a href=""https://i.stack.imgur.com/QC2Vd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QC2Vd.png"" alt=""enter image description here"" /></a></p>
<p>I know this can be done in pandas easily as:</p>
<pre><code>new_header =  pandaDF.iloc[0]
pandaDF = pandaDF[1:]
pandaDF.columns = new_header
</code></pre>
<p><strong>But</strong> doesn't want to convert into Pandas DF as have to persist this into to Database, wherein have to convert back pandas DF to Spark DF and then register as table and then write to db.</p>",1,0,2020-09-07 23:30:20.070000 UTC,,,0,python-3.x|scala|pyspark|apache-spark-sql|azure-databricks,92,2009-04-02 00:53:40.477000 UTC,2022-03-01 22:30:30.100000 UTC,"Brisbane QLD, Australia",27501,66,1,1101,,,,,,[]
Are there any restrictions on Amazon neptune when using gremlin syntax?,"<p>I am using a graph database for my project, Neptune by AWS. Neptune uses gremlin syntax for graph queries. I was trying to execute a scenario where I have to filter the outgoing edges from a vertex on the basis of property on the edge. Let's call that property 'x'. The value of this property 'x' is of the form 'abc::xyz::ref'. This is to store multiple values on the edge, as Neptune does not allow multi values on edges. I have to do a contains check with three combinations and an exact match :-</p>
<ul>
<li>'abc::'</li>
<li>'::abc'</li>
<li>'::abc::'</li>
<li>Exact match with 'abc'</li>
</ul>
<p>I was trying to use filter command in the gremlin in my java code. The below code works fine with TinkerGraph in-memory, but when I connect it with Neptune and run the same query it throws some parsing exception.</p>
<pre><code> String valueToCheck = &quot;abc&quot;;
 List&lt;String&gt; listOfValuesToCheck = new ArrayList&lt;&gt;();
 listOfValuesToCheck.add(&quot;::abc&quot;);
 listOfValuesToCheck.add(&quot;abc::&quot;);
 listOfValuesToCheck.add(&quot;::abc::&quot;);
 GraphTraversal&lt;Vertex, Map&lt;Object, Object&gt;&gt; gt24 = g.V().outE().has(&quot;x&quot;).filter(it -&gt; { 
      String value = String.valueOf(it.get().value(&quot;x&quot;));
      if(value.equals(valueToCheck)){
      return true;
      }else {
        for(String s: listOfValuesToCheck){
            if(value.contains(s){
            return true;
            }
        }
      }
    }).valueMap().with(WithOptions.tokens);
    
    while (gt24.hasNext()) {
      System.out.println(gt24.next());
    }
</code></pre>
<p>Does someone know, why this is happening with Neptune? And is there a better way to do it that works with Neptune.<br />
I have seen one more instance where Neptune did not throw an error but also gave back no results but the same work with TinkerGraph.</p>
<p>y - property on Edge<br />
z - property on Vertex</p>
<pre><code>GraphTraversal&lt;Vertex, Map&lt;String, Object&gt;&gt; gt13 = g.V(1, 2).project(&quot;id&quot;, &quot;summary&quot;).by(T.id)
        .by(__.outE().has(&quot;y&quot;, &quot;e&quot;).inV().group().by(&quot;z&quot;));
</code></pre>",2,0,2020-06-29 11:22:03.357000 UTC,,,3,java|gremlin|graph-databases|amazon-neptune,417,2020-06-29 10:47:57.243000 UTC,2022-03-03 11:15:41.173000 UTC,,31,14,0,7,,,,,,[]
Mercurial update to local revision or hash changeset,"<p>I use Mercurial and i have a weird problem, i have a very big history and the local revisions in Mercurial now has 5 characters.
In Mercurial you can execute ""hg up "" and it can choose between the local revision or the hash changeset ( i have no idea the policy it uses to choose between each other ), in my case the local revision coincide with the 5 first characters of another hash changeset. For example:</p>

<p>I want to update to the local revision: 80145
If i execute:</p>

<p>""hg up 80145""</p>

<p>Mercurial doesn't update to the revision i want, it updates to an old one because its hash changeset is:</p>

<p>801454d1cd5e</p>

<p>So, does anyone know if there is a way to specify to which type of revision you want to update to? local revision or hash changeset.</p>

<p>Thanks all!</p>

<p>====</p>

<p>Problem solved. After some investigation i realized that Mercurial always update to the local revision if it exists, and to the hash changeset otherwise.
In my case the local revision didn't exist, so it was updating to the hash changeset</p>",1,0,2012-09-14 11:01:54.737000 UTC,1.0,2012-09-14 11:10:09.660000 UTC,4,mercurial|dvcs,2669,2010-09-27 21:29:30.710000 UTC,2022-03-02 18:31:08.487000 UTC,San Francisco,882,17,2,80,,,,,,[]
AWS Neptune path() truncates if used after a repeat() loop,"<pre><code>g.addV('person').property('firstName','Bob').as('bob').
addV('decision').property('decision','REFER').as('brefer').select('bob').addE('hasDecision').to('brefer').
addV('phone').property('number','123').as('phone').select('bob').addE('hasPhone').to('phone').
addV('person').property('firstName','Jon').as('jon').
addV('decision').property('decision','ACCEPT').as('jaccept').select('jon').addE('hasDecision').to('jaccept').
addV('decision').property('decision','DECLINE').as('jdecline').select('jon').addE('hasDecision').to('jdecline').
addV('email').property('email','a@a.com').as('email').select('jon').addE('hasEmail').to('email').
select('jon').addE('hasPhone').to('phone').
addV('person').property('firstName','Phil').as('phil').
addV('decision').property('decision','DECLINE').as('pdecline').select('phil').addE('hasDecision').to('pdecline').
select('phil').addE('hasEmail').to('email')
</code></pre>
<p>In the above graph, Phil is linked to Jon by an email who in turn is linked to Bob by a phone. Each person node has decision nodes attached. I need to run a query that will return a path if Phil is linked to anyone within 4 hops who has a decision node attached with a decision of REFER. The query ignores decision nodes in its traversal.</p>
<p>The answer is Phil -&gt; email (hop 1) -&gt; Jon (hop 2) -&gt; phone (hop 3) -&gt; Bob (hop 4) (as Bob has a REFER decision node)</p>
<p>I am writing this in Gremlin on AWS Neptune. The query below should return Bob:</p>
<pre><code>g.V().has('firstName','Phil').repeat(bothE().not(has(label,'hasDecision')).bothV().simplePath())
.until(out().has('decision','REFER')).path().by(valueMap()).by(label())

==&gt;path[{firstName=[Phil]}, hasEmail, {email=[a@a.com]}, hasEmail, {firstName=[Jon]}]
</code></pre>
<p>It has found Bob - this can be proved by replacing REFER with X which returns nothing - but the path() step just gives up at Jon.
This is a repeat() step issue it seems, which can be shown by simplifying the query replacing until() with times()</p>
<pre><code>g.V().has('firstName','Phil').repeat(bothE().not(has(label,'hasDecision')).bothV().simplePath())
.times(2).path().by(valueMap()).by(label())
==&gt;path[{firstName=[Phil]}, hasEmail, {email=[a@a.com]}, hasEmail, {firstName=[Jon]}]

g.V().has('firstName','Phil').repeat(bothE().not(has(label,'hasDecision')).bothV().simplePath())
.times(4).path().by(valueMap()).by(label())
==&gt;path[{firstName=[Phil]}, hasEmail, {email=[a@a.com]}, hasEmail, {firstName=[Jon]}]

</code></pre>
<p>Note that the last query must have traversed to Bob at the end of the chain but path() has given up at Jon.</p>
<p>Recreating the query without the repeat gives the right path but this is no good to me as the target node is an unknown distance away</p>
<pre><code> g.V().has('firstName','Phil').
bothE().not(has(label,'hasDecision')).bothV().
bothE().not(has(label,'hasDecision')).bothV().
bothE().not(has(label,'hasDecision')).bothV().
bothE().not(has(label,'hasDecision')).bothV().
simplePath().path().by(valueMap()).by(label())
==&gt;path[{firstName=[Phil]}, hasEmail, {email=[a@a.com]}, hasEmail, {firstName=[Jon]}, hasPhone, {number=[123]}, hasPhone, {firstName=[Bob]}]
</code></pre>
<p>Has anyone seen this and has a workaround? Is there an alternative to path()? The query works fine on Tinkergraph, BTW (replacing has(label(...)) with hasLabel(...))</p>",1,1,2020-08-20 13:35:48.387000 UTC,,,0,gremlin|amazon-neptune,69,2020-03-12 08:50:10.913000 UTC,2022-02-09 15:43:16.053000 UTC,,11,0,0,1,,,,,,[]
How to drop an edge in Gremlin?,"<p>I currently have this state of my graph database. I've SSH'ed into my EC2 instance and from there connected to my Amazon Neptune database.</p>

<pre><code> g.V()
==&gt;v[26b648fe-b972-a1b4-1642-1ef06ae42a87]
==&gt;v[2]
==&gt;v[6]
==&gt;v[1]
==&gt;v[3]
==&gt;v[7cb648ff-2791-229a-50eb-2408938b42ba]
==&gt;v[4]
==&gt;v[5]
gremlin&gt;  g.E()
==&gt;e[62b648fe-fd52-d10f-71d6-f15edd014a13][1-knows-&gt;4]
==&gt;e[acb648ff-06b9-8526-846f-f03e5c08d6bc][6-created-&gt;3]
==&gt;e[2cb648ff-0663-6343-2d40-ea535087771b][1-created-&gt;3]
==&gt;e[4cb648fe-fd13-8a2b-049f-b65ddd7bd3d7][1-knows-&gt;2]
==&gt;e[0eb648ff-0678-90d1-d77f-69b1733d4c95][4-created-&gt;5]
==&gt;e[74b648ff-0686-3018-c0cf-2f4b15dcd4e2][4-knows-&gt;3]
gremlin&gt;  g.V().has(""name"", ""marko"").outE('knows')
==&gt;e[4cb648fe-fd13-8a2b-049f-b65ddd7bd3d7][1-knows-&gt;2]
==&gt;e[62b648fe-fd52-d10f-71d6-f15edd014a13][1-knows-&gt;4]
</code></pre>

<p>So what is <code>4cb648fe-fd13-8a2b-049f-b65ddd7bd3d7</code>? Is this some kind of ID?</p>

<p>1 knows 4, 3, 2. How do I make it so that 1 only knows 1 and 4?</p>",1,1,2019-08-15 17:41:55.410000 UTC,,,0,gremlin|amazon-neptune,473,2013-08-24 20:20:37.893000 UTC,2022-03-03 19:29:52.380000 UTC,"New York, United States",9768,858,18,1742,,,,,,[]
Where do I find properties for scripting an Enterprise Application from an application template,"<p>I'm attempting to automate deployment of an Azure Active Directory (AAD) Enterprise Application from an application template, specifically the Azure Databricks SCIM Provisioning Connector template. I can successfully create an application using the Azure Portal but I need to automate this using PowerShell or Microsoft Graph.</p>
<p>I found that using a REST client I can POST the following JSON body to change the name of the Enterprise Application. However, I'm coming up short on how to set the following: provisioning mode and scope, tenant url, and how to assign users and groups to the EA.</p>
<pre><code>{
  &quot;displayName&quot;: &quot;Created using Microsoft Graph&quot;
}
</code></pre>
<p>Any ideas where I can find this information? I looked at the JSON response from <a href=""https://graph.microsoft.com/beta/applicationTemplates"" rel=""nofollow noreferrer"">https://graph.microsoft.com/beta/applicationTemplates</a> and it doesn't list any such properties. Unfortunately, I don't recall where I found the sample payload to change the displayName property but I don't remember that webpage showing other properties anyway.</p>",1,0,2020-12-02 23:21:54.220000 UTC,,,0,powershell|microsoft-graph-api|azure-databricks,134,2009-04-30 03:15:21.947000 UTC,2021-10-22 14:50:31.917000 UTC,United States,25836,344,25,1060,,,,,,[]
Composite Baseline Concept applied to DVCS Mercurial/Git,"<p>I come from the IBM Rational ClearCase world (UCM and all that..) and as I remember the concept on Composite Baselines i wonder if there is something out there that has the same purpose for DVCS. </p>

<p>I understand that Mercurial/Git are ""Get All the Code Base or nothing"". But could be something that could manage the tags/baselines for different repositories and be able to manage those as composite baselines.</p>

<p>systemX/release-1  = subsystemA-repo/release-20 +  subsystemB-repo/release-18 + subsystemC-repo/release-2</p>

<p>This mechanism will abstract the was i can query baselines to systemX-releasex and get the appropriate tags from the dvcs repositories.</p>

<p>Any ideas.</p>",1,0,2012-08-07 19:07:12.483000 UTC,,,0,dvcs,301,2011-08-13 20:28:59.083000 UTC,2014-06-18 22:42:20.557000 UTC,"Arlington, VA",1263,15,1,144,,,,,,[]
Writing SQL table directly to file in Scala,"<p>Team,
I'm working on Azure databricks, I'm able to write a dataframe to CSV file using the following option:</p>

<pre><code>df2018JanAgg
.write.format(""com.databricks.spark.csv"")
.option(""header"", ""true"")
.save(""dbfs:/FileStore/output/df2018janAgg.csv"")
</code></pre>

<p>but I'm seeking an option to write data directly from SQL table to CSV in Scala.
Can someone please let me know if such options exist.</p>

<p>Thanks,
Srini</p>",1,0,2019-10-03 20:26:34.083000 UTC,1.0,2019-10-03 21:27:41.220000 UTC,1,scala|export-to-csv|azure-databricks,119,2018-06-18 02:04:37.287000 UTC,2022-01-20 20:06:06.133000 UTC,"San Diego, CA, USA",17,9,0,15,,,,,,[]
Azure Databricks connectivity from Azure VM,"<p>We are having following setup :-</p>
<ol>
<li>Azure Linux VM in subnet1 inside VNET01</li>
<li>Azure databricks hosted using custom connected VNET inside VNET01.</li>
</ol>
<p>While making a connection from Azure VM with ADB we are facing following issue :-</p>
<p>export DATABRICKS_TOKEN=sdgsdgsyd2382732</p>
<p><strong>curl -X GET --header &quot;Authorization: Bearer $DATABRICKS_TOKEN&quot; <a href=""https://adb-xyz.azuredatabricks.net/api/2.0/clusters/list"" rel=""nofollow noreferrer"">https://adb-xyz.azuredatabricks.net/api/2.0/clusters/list</a> -vvv</strong></p>
<ul>
<li>About to connect() to adb-xyz.azuredatabricks.net port 443 (#0)</li>
<li>Trying 40.74.30.80...</li>
<li>Connected to adb-xyz.azuredatabricks.net (30.85.20.80) port 443 (#0)</li>
<li>Initializing NSS with certpath: sql:/etc/pki/nssdb</li>
<li>CAfile: /etc/pki/tls/certs/ca-bundle.crt
CApath: none</li>
<li>NSS error -5938 (PR_END_OF_FILE_ERROR)</li>
<li>Encountered end of</li>
</ul>
<p>It is able to connect to ADB host however fails afterward, we suspect its related to certificates, if it is how can this be resolved, if not, can someone please explain how to handle this issue and make a connection?</p>",0,4,2021-11-03 10:58:04.693000 UTC,,,0,azure|azure-virtual-machine|azure-databricks,65,2012-04-05 18:55:14.100000 UTC,2022-02-24 09:34:25.503000 UTC,Bermuda Triangle,1191,171,1,136,,,,,,[]
Get my database under Version Control using a DVCS [Mercurial],"<p>What would be the best approach for versioning my whole database ?</p>

<p>Creating a file for each database object (table,view,procedsure..) or rather having one file for all DDL scripts and any new change will be put in a separate file ?</p>

<p>What about handling changes made in a Database manager tool ? </p>

<p>I'd like to have a generic solutions for any kind of RDBMS.</p>

<p>Are there any other options ?</p>",4,2,2010-07-29 11:22:45.213000 UTC,1.0,,7,sql|database|mercurial|dvcs,1371,2009-07-13 11:01:58.857000 UTC,2011-01-25 09:58:35.120000 UTC,,9838,923,7,271,,,,,,[]
Error while saving the dataframe as a table in spark-databricks,"<p>I get the below error while using the command</p>
<p><code> df.write.mode(&quot;overwrite&quot;).saveAsTable(&quot;testDB.someTable&quot;)</code></p>
<p>However
<code>df.write.mode(&quot;Overwrite&quot;).option(&quot;path&quot;, &quot;abfss://..../testDB.db&quot;).saveAsTable(&quot;testDB.someTable&quot;)</code>  works.
ie., if the path of the adls is passed, it works fine.</p>
<p>I am setting the <code>spark.sql.warehouse.dir</code> in my spark context, and in logs i can see <code>hive.metastore.warehouse.dir</code> is also rightly pointed to the dir mentioned in <code>spark.sql.warehouse.dir</code>.</p>
<p>What is missing here ?</p>
<p>Exception</p>
<pre><code>org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:
Please enter a valid location  )
    at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:862)...............
    Caused by: MetaException(message:
Please enter a valid location  )
</code></pre>",0,2,2020-07-18 15:41:09.190000 UTC,,2020-07-19 15:26:01.540000 UTC,0,scala|apache-spark|azure-databricks,147,2013-06-22 11:04:27.123000 UTC,2021-10-08 18:23:59.600000 UTC,,203,7,0,86,,,,,,[]
Assign Static IP address (Elastic IP) / Private link for VNet-injected Databricks workspaces in Azure,<p>We wanted to create a private link/static IP in order to access data lake storage from Databricks in Azure that would create a secure connection to data lake storage for data movement activities.</p>,1,0,2020-10-22 04:08:47.777000 UTC,,2020-10-24 13:15:58.177000 UTC,-1,azure|azure-databricks,193,2016-09-01 16:23:14.360000 UTC,2020-10-24 23:46:04.140000 UTC,,1,0,0,1,,,,,,[]
How to force Azure Data Factory Data Flows to use Databricks,"<p>I am working with Azure Data Factory and its new Data Flows feature. This is a GUI that is supposed to use Databricks to do data transformation, without writing any code. </p>

<p>All good so far. I have some examples working. My input data (from Azure Blob) is correctly transformed and joined to create the output (in Azure SQL). </p>

<p>The problem is that I have <em>no Databricks resource</em>. I deleted it. I also removed the Data Factory to Databricks connector. But I am still getting the right answers! </p>

<p>I suspect that my input sets are too small, or my transformations are too simple, so Data Factory is just handling them internally and knows it does not need the power of Databricks. But what do I have to do to force Data Factory to utilize Databricks? I want to test some things about that operation.</p>

<p>Another possibility is that Data Factory <em>is</em> using Databricks, but is doing so with its own Databricks resource rather than the users...??</p>",1,0,2019-05-10 21:55:38.760000 UTC,,2019-06-07 09:45:04.790000 UTC,2,azure|azure-data-factory|dataflow|azure-databricks,621,2018-11-28 16:01:16.037000 UTC,2019-11-13 17:03:43.733000 UTC,"Watertown, MA, USA",61,0,0,14,,,,,,[]
Vertex pattern analysis and matching using Gremlin with Amazon Neptune,"<p>I have following analysis data</p>
<p>Problem statement: Find out a session (From Neptune session) that contains least inbetween vertex. As you can see expected output for following example data is [11, 12, 1, 2, 3] as it contains continued 1,2,3 connected vertices and none inbetween. The target is to pick the array which contains lesser inbetween vertex 1 , 2 and 3.</p>
<pre><code>{
  inputGiven: [1, 2, 3],
  neptuneSessions: [[1, 2, 5, 6, 3], [11, 12, 1, 2, 3], [2, 4, 5]],
  expectedOutput: [11, 12, 1, 2, 3],
}
</code></pre>
<p>The solution I came up with is to query out all the related traversal which contain 1, 2 or 3 vertices and then apply logic to it. But I am looking out of solution through gremlin query</p>
<pre><code>  public async addNode(node: NeptuneNode): Promise&lt;void&gt; {
    const nodeValue = await this.traversal
      .V(A)
      .fold()
      .coalesce(this.__.unfold(), this.__.addV('action').property(id, A))
      .property('EVENT_ID', EVENT_ID)
      .property('EA', EA)
      .property('A', A)
      .next()
    console.log('--node--&gt; ', nodeValue)
  }

  public async addEdge(event: NeptuneEdge): Promise&lt;any&gt; {


    const edge = await this.traversal.addE('next')
      .from_(this.traversal.V(event.PH.toString()))
      .to(this.traversal.V(event.A.toString()))
      .property('ID', event.ID)
      .property('SESSION_ID', event.SESSION_ID)
      .property('DH', event.DH)
      .property('A', event.A)
      .property('PH', event.PH)
      .next()
    console.log('--edge--&gt; ', edge)
  }
</code></pre>
<p>sample graph</p>
<p><a href=""https://i.stack.imgur.com/p6B45.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p6B45.png"" alt=""Sample graph"" /></a></p>",0,3,2021-10-05 10:56:16.643000 UTC,,2021-10-29 17:36:45.480000 UTC,0,amazon-web-services|gremlin|tinkerpop|amazon-neptune,50,2017-09-03 03:07:52.660000 UTC,2021-11-04 09:41:10.563000 UTC,,56,5,0,15,,,,,,[]
Efficient lookup from pyspark in cosmosdb,"<p>In a spark job I need to retrieve data from cosmosdb on ~ 20000 documents whose ids and partitionkey I know.</p>

<p>My current code, which is awfully slow, is to prepare a query <code>SELECT * FROM c WHERE c.pkey = %{pkey}i AND c.id in (%{ids}s)</code> and I have a loop to sequentially query cosmosdb injecting in this query a batch of ~500 up to 1000 ids from the same partition. (Then using a union to assemble the dataframe)</p>

<p>Each of those queries is taking between 30s and a minute. It would be so much faster in SQLServer I don't understand what's going on. (I should add that those queries max out the 50k RUs capacity of the collection (or container as they are now called) which is quite surprising)</p>

<p>How could I do this more efficiently ?</p>

<p>edit : code sample</p>

<pre><code>for pil in partitionedIdsLists :
  for idsList in pil[1] :
    idsEtabsString = r'""'+r'"",""'.join(idsList)+r'""'
    part_df = spark\
                .read\
                .schema(schema_df)\
                .format(""com.microsoft.azure.cosmosdb.spark"")\
                .options(
                  **readConfigET, 
                  query_custom = r'SELECT * FROM c WHERE c[""pkey""] = %(pkey)i AND c[""id""] in (%(listeIds)s)'%{'pkey' : pil[0], 'listeIds' : idsEtabsString}
                )\
                .load()\
                .distinct()

    if full_df is None :
      full_df = spark\
                  .createDataFrame([], schema_df)

    full_df = full_df\
                .union(part_df)
</code></pre>

<p>partitionedIdsLists being a list of couples <code>(pkey,[[id1,..,id500],[id501,..,id1000]])</code></p>",1,2,2019-06-06 05:31:31.117000 UTC,,2019-06-06 10:29:10.327000 UTC,0,pyspark|azure-cosmosdb|azure-databricks,329,2015-01-02 07:51:23.380000 UTC,2021-10-07 16:06:34.913000 UTC,"Nantes, France",526,41,6,161,,,,,,[]
Cherry picking new features for stable branch,"<p>We have following requirement when it comes to bussiness process concerning introducing new features to a stable branch.<br>
We have a stable line which gets delivered to our clients. Also we have a development line where we develop new features. Sometimes, we decide that we need to introduce some developed features to a stable release line. Not all, but some of them. How to organize branches (we're using mercurial) so we can cherry pick features we want to apply to stable branch?<br>
On the other side we need to have a branch where we'll have all features integrated into one branch, call it dev branch (which was derived from stable branch).</p>

<p>One of the ideas is to have a stable branch, dev branch (which was derived at one time from stable) and a separate branches for each feature.<br>
Bugs are resolved on the stable branch and from time to time, changes are pulled to other branches (dev and feature ones). Once a decision, of integrating a particular feature to a stable branch, is made, then only a given feature branch is merged with the stable one. Also, feature branches are from time to time pulled on dev branch (which serves to integrate all features that are being developed).</p>",1,3,2014-10-02 23:23:24.570000 UTC,,2014-10-02 23:32:04.150000 UTC,-2,git|mercurial|continuous-integration|branch|dvcs,194,2008-10-04 17:05:49.270000 UTC,2021-10-14 18:22:08.293000 UTC,"Berlin, Germany",2867,790,8,354,,,,,,[]
Git Bare configuration for multiple projects,"<p>We are trying to maintain a centralised repo for all of our projects (We are using windows server)</p>

<ul>
<li><p>E:\Repo</p>

<ul>
<li>Project 1</li>
<li>Project 2</li>
<li>Project 3</li>
<li>Project 4</li>
</ul></li>
</ul>

<p>now I want to share the repo details with my fellow developers but I would like to do it with sort of credentials like team working on project 1 shouldn't able to pull/push into project 2.</p>

<p>How can we achieve this.</p>

<p>Thanks</p>",1,4,2013-04-15 06:40:54.050000 UTC,,,0,git|dvcs,57,2011-11-11 16:09:09.933000 UTC,2018-07-05 12:09:20.647000 UTC,"Nairobi, Kenya",3631,304,10,604,,,,,,[]
Getting TypeError: path can be only a single string when trying Stream Data in Apache Spark / Databricks,"<p>I am trying to test streaming data in Apache Spark on Databricks.</p>
<p>Streaming with Azure Event Hubs is relatively simple, however I'm trying stream some static data.</p>
<p>I first read in the static data stored in a folder called teststream using the following dataframe</p>
<pre><code>thestream = spark.read.parquet('/mnt/lake/RAW/teststream/')
</code></pre>
<p>I then attempt to read in the data in the 'teststream' folder by convert it to a streaming query that continuously updates as data arrives using the following code:</p>
<pre><code>streamingFlights = (spark
              .readStream
              .option(&quot;maxFilesPerTrigger&quot;, 1) #Treat a sequence of files as a stream by selecting one file at a time
              .csv(thestream)
            )
</code></pre>
<p>However, when I run the above I get the following error:</p>
<pre><code>TypeError: path can be only a single string
</code></pre>
<p>Any ideas on what is causing the error?</p>",1,4,2021-05-13 15:11:13.023000 UTC,,,0,apache-spark|pyspark|azure-databricks,98,2021-03-31 08:36:43.863000 UTC,2022-03-05 23:03:22.193000 UTC,"London, UK",651,58,0,93,,,,,,[]
Subtraction in a Gremlin Query (AWS Neptune),"<p>I am trying to create a <strong>gremlin query</strong> for <strong>AWS Neptune</strong> which checks for a particular property on a node (lastUpdated) and returns all nodes that have value smaller than a certain number. 
lastUpdated is epoch timestamp in this case, and I am trying to find all nodes which have lastUpdated 90 days less than current timestamp.</p>

<p>Here is the query that I wrote:</p>

<pre><code>g.V().hasLabel('nodelabel').hasNot('lastUpdated',P.gt(1544916150)).count()
</code></pre>

<p>To make this query dynamic, so that whenever it is fired, I get all nodes more than 90 days old, I changed it to following:</p>

<pre><code>g.V().hasLabel('nodelabel').has('lastUpdated',not(P.gt(1552798296-7776000))).count()
</code></pre>

<p>where 1552798296 is current_date and 7776000 is number of seconds in 90 days</p>

<p>Apparently, subtraction is not that straightforward in Gremlin. Any hints/suggestions on how I can write this gremlin query?</p>

<p>Thanks</p>",1,0,2019-03-17 05:50:18.933000 UTC,,,1,amazon-web-services|gremlin|subtraction|amazon-neptune,223,2014-08-17 15:12:16.263000 UTC,2021-12-23 05:37:02.047000 UTC,"Seattle, WA",23,1,0,14,,,,,,[]
Databricks and Delta cache setting,"<p>I am trying to follow the instructions on the MSFT website to use delta cache and hoping someone would help me understand it a little better:</p>
<p><a href=""https://docs.microsoft.com/en-us/azure/databricks/delta/optimizations/delta-cache"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/databricks/delta/optimizations/delta-cache</a></p>
<p>So In the guide it mentions that I should use Standard_E or L series of VMs. Our workload is now set to use the F series machines and when I tried to use only E or L it seemed that the job ran longer and would be using more DBUs.</p>
<p>I did however notice that the Dv3 series allow you to use delta caching (ex: Standard_D16s_v3 VMs). I tried to run some of our workloads using those types of machines and notices that under the storage tab it now shows a similar screen as in the MSFT docs:</p>
<p><a href=""https://i.stack.imgur.com/igbu8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/igbu8.png"" alt=""enter image description here"" /></a></p>
<p>Problem is that I am not sure if that is the right way to go about this. The reason I wanted to try to use Dv3 VMs was because it was relatively comparable to the F series, but also seem to allow the delta caching.</p>
<p>I am also wondering if the MSFT recommendation of using the following settings is correct or if they can be different:</p>
<pre><code>spark.databricks.io.cache.maxDiskUsage 50g
spark.databricks.io.cache.maxMetaDataCache 1g
spark.databricks.io.cache.compression.enabled false
</code></pre>
<p>Has any one else played with this and can recommend what they did it would be much appreciated.</p>
<p>As background we have the databricks clusters spin up using our Databricks Linked Service (from ADF) and in that linked service we put the following settings:</p>
<p><a href=""https://i.stack.imgur.com/P9365.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P9365.png"" alt=""enter image description here"" /></a></p>
<p>This is what sends the config settings to the automated clusters that are spun up when we execute Databricks notebooks though ADF.</p>
<p>Thank you</p>",0,0,2022-01-31 16:25:17.277000 UTC,,,0,azure-databricks,58,2019-05-07 19:21:08.243000 UTC,2022-03-03 19:51:26.787000 UTC,"Harrisburg, PA, USA",158,5,0,29,,,,,,[]
Load data to Salesforce using ADF SF connector,"<p>We are planning following to transfer data to Salesforce:
Data bricks to do logic transformation and put result in sql to use existing ADF salesforce connector to load.
Want to know if ADF supports latest bulk Salesforce API which is Bulk API 2.0 ??
While creating linked service to salesforce i dont see it in apiversionz</p>",1,1,2021-11-12 17:56:57.393000 UTC,,,0,api|salesforce|azure-data-factory|azure-databricks,36,2021-11-12 17:53:06.200000 UTC,2022-01-17 14:40:54.480000 UTC,,1,0,0,1,,,,,,[]
MS Dynamics CRM 2011 AND TFS 2010,"<p>For business purpose, I need to get in touch with CRM 2011 (cf. 5.0) and TFS 2010 then try to see whether they can be integrated together or not. I've read something about TFS 2008 and CRM 2009 but couldn't find input related to CRM 2011 and TFS 2010.</p>

<p>Any suggestion, tip, pointer, trick, highlight... is welcome :)</p>",2,2,2010-11-25 09:17:26.833000 UTC,1.0,2015-11-01 01:01:40.453000 UTC,0,tfs|dynamics-crm|dvcs,1083,2010-08-24 02:51:26.783000 UTC,2021-03-11 14:35:03.913000 UTC,"Brussels, Belgium",5180,296,14,632,,,,,,[]
ADF - Run VSTest/MSTest/NunitTest as an activity in between pipeline,"<p>This is how my ADF looks like</p>
<p>Activity 1 -&gt; Activity 2 -&gt; Activity 3</p>
<p>All the activities are various transformation and data movements</p>
<p>My scenario is - I want to run VSTest/MSTest/NunitTest as an activity and run the next activity only of tests pass.</p>
<p>Activity 1 -&gt; Activity 2 -&gt; run VSTest/MSTest/NunitTest -&gt; If Pass -&gt; Activity 3
-&gt; If Fail -&gt; Exit</p>",1,0,2022-01-27 07:52:53.337000 UTC,,,0,azure-devops|azure-data-factory|azure-databricks|azure-data-lake,17,2022-01-27 07:47:02.780000 UTC,2022-02-28 06:55:57.617000 UTC,,1,0,0,0,,,,,,[]
Read a zip file in databricks from Azure Storage Explorer,"<p>I want to read zip files that have csv files. I have tried many ways but I have not succeeded. In my case, the path where I should read the file is in Azure Storage Explorer.</p>
<p>For example, when I have to read a csv in databricks I use the following code:</p>
<pre><code>dfDemandaBilletesCmbinad = spark.read.csv(&quot;/mnt/data/myCSVfile.csv&quot;, header=True)
</code></pre>
<p>So, the Azure Storage path that I want is <code>&quot;/mnt/data/myZipFile.zip&quot;</code> , which inside I have some csv files.</p>
<p>Is it possible to read csv files coming <strong>from Azure storage via pySpark in databricks</strong>?</p>",2,0,2021-05-04 16:51:31.293000 UTC,,2021-06-12 21:34:21.230000 UTC,0,pyspark|azure-storage|unzip|zipfile|azure-databricks,826,2021-05-04 16:30:23.207000 UTC,2021-08-16 15:41:15.873000 UTC,"Madrid, España",1,0,0,1,,,,,,[]
Executing python scripts in azure data bricks and azure data factory,"<p>I'm trying to execute a python script in azure databricks cluster from azure data factory.</p>

<p>Python activity reads <code>main.py</code> from <code>dbfs:/scripts/main.py</code>
This main script is importing another class from <code>dbfs:/scripts/solutions.py</code></p>

<pre><code>#main.py
import solutions

print (""hello"")
</code></pre>

<p>While running in ADB, only main.py is copied from dbfs to execut and thowing error that solutions not found.</p>

<p>How can i execute this in ADF?</p>

<p>thanks</p>",0,0,2019-07-24 03:57:12.657000 UTC,2.0,,1,python|azure|azure-data-factory|azure-data-factory-2|azure-databricks,1117,2013-09-12 10:12:49.767000 UTC,2022-03-04 00:11:05.370000 UTC,"Hyderabad, India",393,27,0,137,,,,,,[]
Which Version Control System would you use for a 1000+ developer organization? Why?,"<p>There are many SCM systems out there. Some open, some closed, some free, some quite expensive. Which one <em>(please choose only one)</em> would you use for a 3000+ developer organization with several sites (some behind a very slow link)? Explain why you chose the one you chose. (Give some reasons, not just ""because"".)</p>",33,1,2008-09-15 18:32:49.607000 UTC,8.0,2010-08-16 20:17:44.430000 UTC,28,version-control|dvcs,4183,2008-09-15 18:32:49.543000 UTC,2014-03-07 09:03:26.480000 UTC,Finland,279,42,1,91,,,,,,[]
How can I get all new or changed files?,"<p>I want to make a script that uses all new/changed files in some folder.<br>
Intellij can tell changed (blue) and new (green) files from the unchanged ones. So, they must be foundable.  </p>

<p>How can I distinct them in a Gradle task?</p>",0,6,2017-07-11 16:05:39.853000 UTC,,,0,intellij-idea|gradle|groovy|dvcs,34,2011-04-19 13:19:28.397000 UTC,2022-03-02 17:02:02.810000 UTC,"Prague, Czech Republic",22932,705,51,3262,,,,,,[]
Jira DVCS Connector does not link commit to issues,"<p>Im trying to figure out how DVCS Connector plugin works. Im trying to link my commits to Jira issues. </p>

<p>As far as I can tell it seems like my setup is correct. At least it doesnt complain about anything.</p>

<p>I add my issue to the comment like ""MBL-10 Fixed some minor issues with.."" but they are not linked with the issue.</p>

<p>When I try to push the ""Force Sync"" button it say ""Last commit 7 hours ago"" and it's beein saying that the entire day.</p>

<p>Anyone bumped into similar problems before?</p>

<p>Btw, Im using Jira OnDemand.</p>",0,3,2012-07-05 21:17:54.953000 UTC,,,2,dvcs|jira|ondemand,267,2012-02-15 08:59:35.723000 UTC,2017-11-08 07:43:54.187000 UTC,Norway,137,7,0,20,,,,,,[]
I am unable to mount ADLS Gen2. Please assist,"<p>Code to mount ADLS Gen2:</p>
<p><a href=""https://i.stack.imgur.com/wAv3t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wAv3t.png"" alt=""enter image description here"" /></a></p>
<p>Error while mounting ADLS Gen2:</p>
<p><a href=""https://i.stack.imgur.com/5I51M.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5I51M.png"" alt=""enter image description here"" /></a></p>",1,2,2021-12-02 05:55:25.247000 UTC,,2021-12-02 07:09:12.617000 UTC,1,apache-spark|azure-databricks|azure-data-lake-gen2,32,2021-04-23 09:18:30.697000 UTC,2022-01-19 17:43:59.677000 UTC,"Bangalore, Karnataka, India",11,0,0,0,,,,,,[]
Multi-Statement query using Simba Spark JDBC Driver,"<p>I'm trying to call a Databricks cluster using the jdbc connection. Here, I want to send multiple statements. One would be a &quot;cache table select&quot; and the other would be a select the table created.</p>
<p>I've been unable to understand a series of things and because both statements are not of the same type, I tried to make a first test by passing two selects, but I've been unable to do so.</p>
<p>My Code</p>
<pre><code>Connection connection = null;
    
try {
    // Create the connection
    Class.forName(driver);
    connection = DriverManager.getConnection(url, username, password);
    if(connection != null){
        System.out.println(&quot;Connection Established&quot;);
    }
    else {
        System.out.println(&quot;Connection Failed&quot;);
    }
    // create the statement
    //Statement statement = connection.createStatement();
    StringBuffer sql = new StringBuffer( &quot;select 1;&quot; );
    sql.append( &quot; select 2&quot; );
    CallableStatement stmt = connection.prepareCall(sql.toString());
    boolean results = stmt.execute();
    while ( results ) {
        ResultSet rs = stmt.getResultSet();
        try {
            while (rs.next()) {
                // read the data
            }
        } finally {
            try { rs.close(); } catch (Throwable ignore) {}
        }
    }
}
catch (Exception e) {
    System.out.println(e.toString());
}
connection.close();
</code></pre>
<p>Error I get</p>
<p>[Simba]SparkJDBCDriver ERROR processing query/statement. Error Code: 0, SQL state: Error running query: org.apache.spark.sql.catalyst.parser.ParseException:
extraneous input 'select' expecting {, ';'}(line 1, pos 10)</p>
<p>I already tried using PreparedStatement class or just Statement instead of the CallableStatement and I get the same error.</p>
<p>Adding the option allowMultiQueries to the connection string also made no difference but I believe the problem is before that can act.</p>",0,0,2021-05-20 17:15:23.430000 UTC,,,0,java|azure-databricks|spark-jdbc,146,2012-07-31 15:35:28.560000 UTC,2022-01-28 11:51:46.850000 UTC,Portugal,699,29,0,119,,,,,,[]
XGBoost 4J spark giving XGBoostError: std::bad_alloc on databricks,"<p>I am using XGBoost 4J spark to create a distributed xgboost model on my data. I am developing my model in databricks.</p>
<p>The spark version is <strong>3.1.1</strong>, scala <strong>2.12</strong> and XGBoost 4J <strong>1.4.1</strong></p>
<p>My cluster setup looks as below,</p>
<ul>
<li>5 worker nodes with each worker of size 32GB and 16 cores</li>
<li>Driver node with 14GB of memory and 4 cores</li>
</ul>
<p>My cluster configuration looks as below,</p>
<ul>
<li>spark.executor.memory 9g</li>
<li>spark.executor.cores 5</li>
</ul>
<p>So basically I have 10 executors with 4.6GB memory and 1 driver with 3.3GB of memory.
<a href=""https://i.stack.imgur.com/4EAk4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4EAk4.png"" alt=""cluster setup"" /></a></p>
<p>I imported the package as below,</p>
<pre><code>import ml.dmlc.xgboost4j.scala.spark.{XGBoostRegressionModel,XGBoostRegressor}
</code></pre>
<p>In order to find the best parameters for my model, I created a parameter grid with train-validation split as shown below,</p>
<pre><code>//Parameter tuning
    import org.apache.spark.ml.tuning._
    import org.apache.spark.ml.PipelineModel
    import Array._

//Create parameter grid 
    val paramGrid = new ParamGridBuilder()
        .addGrid(xgbRegressor.maxDepth, range(6, 10, 2))
        .addGrid(xgbRegressor.eta, Array(0.01))
        .addGrid(xgbRegressor.minChildWeight, Array(8.0, 10.0, 12.0, 14.0))
        .addGrid(xgbRegressor.gamma, Array(0, 0.25, 0.5, 1))
        .build()
</code></pre>
<p>I then fit it to my data and saved it,</p>
<pre><code> val trainValidationSplit = new TrainValidationSplit()
      .setEstimator(pipeline)
      .setEvaluator(evaluator)
      .setEstimatorParamMaps(paramGrid)
      .setTrainRatio(0.75)

val tvmodel = trainValidationSplit.fit(train)

tvmodel.write.overwrite().save(&quot;spark-train-validation-split-28072021&quot;)
</code></pre>
<p>The error comes up when I try to load the model again,</p>
<pre><code>  import org.apache.spark.ml.tuning._
  val tvmodel = TrainValidationSplitModel.load(&quot;spark-train-validation-split-28072021&quot;)
</code></pre>
<p>The error message is
<strong>XGBoostError: std::bad_alloc</strong></p>
<p>I checked the executor and driver logs. The executor logs looked fine . I found the same error in driver logs (stderr and log4j files). Both the log files are avilable in this github link <a href=""https://github.com/jon-targaryen1995/xgboost4J"" rel=""nofollow noreferrer"">log_files_link</a></p>
<p>Since the error message was mainly found in the driver logs, I tried the following solutions,</p>
<ul>
<li>Increased the driver memory to 28GB</li>
<li>Increased the driver cores to 8</li>
<li>Made the driver same as worker</li>
<li>set spark.driver.maxResultSize to 0</li>
<li>set spark.driver.memory to 20g</li>
</ul>
<p>I also tried to increase the cluster size to 128GB and 32 cores. And got the same error.</p>
<p>The log4J has two different error messages as</p>
<ul>
<li>ERROR ScalaDriverLocal: User Code Stack Trace:
ml.dmlc.xgboost4j.java.XGBoostError: std::bad_alloc</li>
<li>ERROR Instrumentation: ml.dmlc.xgboost4j.java.XGBoostError:
std::bad_alloc</li>
</ul>
<p>The stderr has one error message that looks as below,</p>
<ul>
<li>ERROR Uncaught throwable from user code:
ml.dmlc.xgboost4j.java.XGBoostError: std::bad_alloc</li>
</ul>
<p>I continuously monitored the driver metrics and none of it was over loaded as you can see from the images below.</p>
<p><a href=""https://i.stack.imgur.com/fJFo2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fJFo2.png"" alt=""number of cpu used"" /></a></p>
<p><a href=""https://i.stack.imgur.com/GCWdF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GCWdF.png"" alt=""Memory usage"" /></a></p>
<p><a href=""https://i.stack.imgur.com/6l9ZL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6l9ZL.png"" alt=""CPU Usage"" /></a></p>
<p>I am not sure where the problem is. It would be great if one of you could help me.</p>",0,1,2021-08-05 12:09:18.847000 UTC,,,0,scala|apache-spark|xgboost|apache-spark-mllib|azure-databricks,170,2015-12-20 09:35:06.420000 UTC,2022-03-03 10:51:05.793000 UTC,nigeria,237,4,0,30,,,,,,[]
Pyspark: Dropping columns with no distinct values only using transformations,"<p>I have a huge dataframe with 1340 columns. Before diving into modelisation, I must get rid of columns with no distinct values. The few ways I found to do it require <strong>Actions</strong> on the dataframe i.e. it takes much time (approx: 75 hours). How to solve this only using <strong>Transformations</strong> in order to save a lot of time ?</p>

<p>I'm using Azure Databricks that runs Apache Spark 2.4.0 and Python 3.5.<br>
Cluster specs:<br>
- Worker: 56 GB memory, 16 cores<br>
- Driver: 56 GB memory, 16 cores<br>
2-8 nodes (autoscalling)</p>

<pre><code>from pyspark.sql.functions import *
# This shouldn't be run

cols_to_drop = []

for c in df.columns:
  # Extracting the value computed by countDistinct()
  # Here collect() is time-consuming because it's an action
  if df.agg(countDistinct(c)).collect()[0][0] &lt; 2:
    print(""{} has no distinct values."".format(c))
    cols_to_drop.append(c)

print(len(cols_to_drop))
df = df.drop(*cols_to_drop)
</code></pre>

<p>Also I tried using approx_count_distinct which is supposed to be faster with an estimation error > 0.01. But it didn't change much and was often longer.</p>

<p>I'd like something that does the same job - dropping columns with no distinct values - with no function that implies action such as collect().</p>

<p><strong>Edit</strong>:<br>
It's not recommended for large datasets, but I did it anyway. Converting my dataframe into a pandas dataframe with toPandas(). Took 10 minutes, which is pretty decent. Then this does the trick:</p>

<pre><code>cols_to_drop = [c for c in f.columns if len(df[c].unique()) &lt; 2]
</code></pre>",0,4,2019-04-11 12:14:45.683000 UTC,,2019-04-11 14:54:11.860000 UTC,1,python-3.x|apache-spark|pyspark|pyspark-sql|azure-databricks,48,2019-01-30 16:19:48.973000 UTC,2021-03-19 15:28:48.373000 UTC,,332,129,0,25,,,,,,[]
"rsync push - can rsync be made as easy to use as ""hg push""","<p>Q: is there a way to use rsync or some similar tool so that you can ""rsync++ pull"" or ""rysnc++ push"" as easily as you can ""git pull"" or ""hg push"" in a DVCS?</p>

<p>So that you can indicate that a directory tree is rsynced, e.g. via an rc file in RTREE_ROOT, to particular remote systems</p>

<p>and then, if you are deep in such a tree, you can just type in ""rsync++ push"" or ""rsync++ push remote-system-name""</p>

<p>and the hypothetical rsync++ tool would crawl up the filesystem tree until it finds RSYNC_ROOT, a directory marked, e.g. with an rc file.</p>

<p>Rather than having to type something like (which changes according to where you are_)</p>

<pre><code>rsync -r ../../../Rysnc_Root_Name remote-system:path/to/rsync/place/on/remote
</code></pre>

<p>Just be able to do</p>

<pre><code>rsync++ push remote-name
</code></pre>

<p>or even</p>

<pre><code>rsync++ push
</code></pre>

<p>when there is a default push target</p>

<p><strong>Background</strong>:</p>

<p>I often use a distributed version control tool such as git, mercurial (hg), or bazaar (bzr).</p>

<p>I find it far easier to keep DVCS trees in sync than to use rsync: once set up, one just says ""git push"" or ""hg pull"", possibly followed by ""hg update"".</p>

<p>I.e. the DVCS can find the root of the current tree, and can map it to remote trees or repos that need to be updated.</p>

<p>AFAIK, rsync does not do this.</p>

<p>But I can think of rysnc as a zeroth-level DVCS - okay, a distributed replication service. Unfortunately, the command line interface doesn't allow this pleasant fiction to be perceived.</p>

<p>Sometimes I use a DVCS when all I really need is rsync, just to make it easier to type.  But then I have to ensure that the remote user knows to use ""hg update"" in the remote target directory tree.</p>

<hr>

<p>I think there are some fairly generic aspects to this, that might be useful in multiople tools, not just rsync and DVCS tools like git/hg/bzr:</p>

<p>(1) in a directory tree, find some ""root""
e.g. by walking up .., ../.., to look for some special marking like a .hg file, etc.
e.g. by l</p>

<p>(Issue: nesting. E.g. I often have hg repos in bzr repos, or vice versa - which is basically a klugey way of trying to create nested subrepos, which so many DVCS tools do not handle. Might be nice to allow skipping of certain levels, or merging.)</p>

<p>(2) having found such a root, specify pleasant, human friendly, shorthands,
e.g. mapping a name like ""remote"" to ""ssh://host:port//path/to/corresponding""</p>

<p>or using default, or default-for-operation (like default=pull or default-push).</p>

<p>(typically merging stuff in the root with stuff in ~/.toolrc) </p>

<p>The monadic version is to find a root, and possibly, given the root, to find a config file, possibly merging.</p>

<p>The dyadic version observes that often commands involve a directiory tree under such a root, and another directory tree specified in the config file.</p>

<p>A multiadic version might allow you to push to multiple remotes.</p>

<p>We already have DVCS tools that follow this pattern: git/hg/mercurial.</p>

<p>My immediate wish is for an rsync-like rool.</p>

<p>If you think about it, non-distributed version control tools like CVS and SVN, and the umpteen RCS wrappers, solve the ""how do I find the remote"" with a link dir per xdir in the tree.  (As I pointed out in a paper a long time ago.)</p>

<p>What other tools might benefit from something like this?</p>

<p>?? how about a tool that converted relative symlinks to absolute and vice versa:  e.g. copy a tree, using relative symlinks for everything within the tree, and absolute for everything outside.</p>",0,3,2014-07-08 01:09:56.743000 UTC,,2014-07-08 18:57:26.447000 UTC,0,git|mercurial|rsync|dvcs,286,2011-11-17 05:59:52.473000 UTC,2022-03-06 03:58:37.633000 UTC,"Portland, OR",6708,493,39,2234,,,,,,[]
How is it that I can read from an Azure Blob Storage and fail to write onto it?,"<p>Banging my head up against the wall since I just can't write a parquet file into an Azure Blob Storage. On my Azure Databricks Notebook I basically:
1. read a CSV from the same blob storage as a dataframe and 
2. attempt to write the dataframe into the same storage. </p>

<p>I am able to read the CSV, but there is this error as I try to write the parquet file.</p>

<p>Here's the stack trace:</p>

<blockquote>
  <p>Job aborted due to stage failure: Task 0 in stage 8.0 failed 4 times, most recent failure: Lost task 0.3 in stage 8.0 (TID 20, 10.139.64.5, executor 0): shaded.databricks.org.apache.hadoop.fs.azure.AzureException: java.io.IOException
      at shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.storeEmptyFolder(AzureNativeFileSystemStore.java:1609)
  ...
  ...
  Caused by: com.microsoft.azure.storage.StorageException: The specified resource does not exist.</p>
</blockquote>

<p>Here's my python code:</p>

<pre><code>spark.conf.set(""fs.azure.sas.my_container.my_storage.blob.core.windows.net"", dbutils.secrets.get(scope = ""my_scope"", key = ""my_key""))
</code></pre>

<h1>read csv</h1>

<pre><code>df100 = spark.read.format(""csv"").option(""header"", ""true"").load(""wasbs://my_container@my_storage.blob.core.windows.net/folder/revenue.csv"") 
</code></pre>

<h1>write parquet</h1>

<pre class=""lang-py prettyprint-override""><code>df100.write.parquet('wasbs://my_container@my_storage.blob.core.windows.net/f1/deh.parquet')  

</code></pre>

<h1>end</h1>",2,0,2019-07-30 00:54:01.520000 UTC,,2019-07-30 01:06:58.750000 UTC,0,python|azure-blob-storage|azure-databricks,2660,2017-04-04 07:25:06.390000 UTC,2019-08-07 17:06:56.177000 UTC,,79,2,0,32,,,,,,[]
SPARQL SubQuery in Filter,"<pre><code>SELECT ?Name FROM &lt;http://.../biblio.rdf&gt;
WHERE { ?Aut name ?Name . ?Pub author ?Aut . ?Pub conf ?Conf
FILTER (?Conf IN ( SELECT ?ConfX FROM &lt;http://.../biblio.rdf&gt;
WHERE { ?ConfX series &quot;ISWC&quot; }))}
</code></pre>
<p>I have taken the query from <a href=""http://www.renzoangles.net/files/amw2011.pdf"" rel=""nofollow noreferrer"">http://www.renzoangles.net/files/amw2011.pdf</a>.</p>
<p>Getting the malformed query syntax error when I tried the above format in AWS Neptune.</p>
<p>Please help me fix the above query.</p>",1,5,2020-09-23 10:11:21.860000 UTC,,,0,sparql|amazon-neptune,103,2017-04-06 18:53:11.143000 UTC,2021-12-10 06:59:36.113000 UTC,"Gurgaon, Haryana, India",302,31,0,52,,,,,,[]
Push down DML commands to SQL using Pyspark on Databricks,"<p>I'm using Azure's Databricks and want to pushdown a query to a Azure SQL using PySpark. I've tried many ways and found a solution using Scala (code below), but doing this I need to convert part of my code to scala then bring back to PySpark again.</p>

<pre><code>%scala
import java.util.Properties
import java.sql.DriverManager

val jdbcUsername = username
val jdbcPassword = password
val driverClass = ""com.microsoft.sqlserver.jdbc.SQLServerDriver""

// Create the JDBC URL without passing in the user and password parameters.
val jdbcUrl = ""entire-string-connection-to-Azure-SQL""

// Create a Properties() object to hold the parameters.
val connectionProperties = new Properties()

connectionProperties.put(""user"", s""${jdbcUsername}"")
connectionProperties.put(""password"", s""${jdbcPassword}"")
connectionProperties.setProperty(""Driver"", driverClass)

val connection = DriverManager.getConnection(jdbcUrl, jdbcUsername, jdbcPassword)
val stmt = connection.createStatement()
val sql = ""TRUNCATE TABLE dbo.table""

stmt.execute(sql)
connection.close()
</code></pre>

<p>Is there a way to achieve the pushdown of a DML code using PySpark instead of Scala language?</p>

<p>Found something related but only works to read data and DDL commands:</p>

<pre><code>jdbcUrl = ""jdbc:mysql://{0}:{1}/{2}"".format(jdbcHostname, jdbcPort, jdbcDatabase)
connectionProperties = {
  ""user"" : jdbcUsername,
  ""password"" : jdbcPassword,
  ""driver"" : ""com.mysql.jdbc.Driver""
}

pushdown_query = ""(select * from employees where emp_no &lt; 10008) emp_alias""
df = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)
</code></pre>",0,5,2019-11-10 16:06:31.960000 UTC,1.0,,2,scala|pyspark|pyspark-sql|azure-databricks,720,2018-11-20 13:01:10.807000 UTC,2022-01-26 18:24:56.357000 UTC,"São Paulo, SP, Brasil",99,31,0,5,,,,,,[]
unable to import azure_sentinel_utilities module after pip install Azure-Sentinel-Utilities in Azure databricks notebook,"<p>I have pip installed Azure-Sentinel-Utilities in azure data bricks notebook and verified it through pip freeze command as shown below,</p>
<p>pip freeze command screen shot</p>
<p>I am getting error as No module named 'azure_sentinel_utilities' while trying to execute below line,</p>
<p>from azure_sentinel_utilities.azure_storage import storage_blob_manager</p>",0,0,2021-04-27 08:12:14.113000 UTC,,,0,python|azure-databricks,24,2019-01-19 09:32:02.037000 UTC,2021-08-27 11:47:30.907000 UTC,"Chennai, Tamil Nadu, India",1,0,0,4,,,,,,[]
Is Subversion better at cherry-picking then git (or any DAG VCS)?,"<p>I just read <a href=""http://www.draconianoverlord.com/2013/09/07/no-cherry-picking.html"" rel=""nofollow"">http://www.draconianoverlord.com/2013/09/07/no-cherry-picking.html</a> and seems that <code>svn:mergeinfo</code> can track info about single commit merge which happen at cherry-picking.</p>

<p>That avoid merge conflict when you merge back your feature-branch with cherry-picked bug-fixes to original branch (where you made bug-fixes before).</p>

<p>Here funny ASCII art (which people like at SO):</p>

<pre><code>      o--o--o--o--o    feature
     ^      ^      \
    /      /        v
-o--o--o--X--o---o---Y--o--&gt;  dev
</code></pre>

<p>Here <code>X</code> - only single changeset (essential/blocker fix that we move from <code>dev</code> to <code>feature</code> branch).</p>

<p>I try and found that <code>Git</code> have conflicts in <code>Y</code> merge if you have made changes at lines which effected by fix <code>X</code> already. While <code>SVN</code> just skip that changeset from merge.</p>

<p><strong>UPDATE</strong> That leads DAG VCS users to use <code>bisect</code> and аштв common ancestor from bisect and branches for fix propagation. So you can make clean history.</p>",1,0,2013-12-19 13:02:21.597000 UTC,,2013-12-19 13:12:10.070000 UTC,1,git|svn|mercurial|dvcs|directed-acyclic-graphs,149,2009-09-14 13:42:30.747000 UTC,2022-02-28 11:34:20.310000 UTC,"Dnipro, Ukraine",40399,19176,27,5468,,,,,,[]
"In Mercurial, how to ""merge"" with the commit's (single) parent?","<p>I've worked on some project. I'm able to see the difference between my project and its (sole) parent with:</p>

<pre><code>hg kdiff3
</code></pre>

<p>(After I've configured ""kdiff3"" as shown <a href=""https://mercurial.selenic.com/wiki/KDiff3"" rel=""nofollow"">here</a>). However, I want to be able to edit my files, perhaps remove some of my changes, edit some of them or make new changes. But that command only shows me the differences, rather than let me edit them. If I try:</p>

<pre><code>hg merge 5861231e8335
</code></pre>

<p>(When ""5861231e8335"" is the (sole) parent of the working directory) I get:</p>

<pre><code>abort: merging with a working directory ancestor has no effect
</code></pre>

<p>So how can I ""merge"" with the parent? (preferably using ""kdiff3"")</p>",2,3,2015-07-16 09:52:10.307000 UTC,0.0,,0,version-control|merge|mercurial|dvcs|kdiff3,450,2015-02-19 10:32:05.203000 UTC,2020-09-11 20:36:53.767000 UTC,,1987,90,0,85,,,,,,[]
How to close Gremlin session client without executing submitted queries,"<p>I'm trying to use <a href=""https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-sessions.html"" rel=""nofollow noreferrer"">Gremlin sessions</a> with Amazon Neptune and using <code>GroovyTranslator</code> to submit <code>String</code> queries, as shown in below snippet</p>
<pre><code> Client.SessionedClient sclient = cluster.connect(sessionId, false);
 GraphTraversalSource g = EmptyGraph.instance().traversal().withRemote(DriverRemoteConnection.using(sclient))
 
 Traversal t1 = g.addV(&quot;Person&quot;).property(T.id, 1).property(&quot;Name&quot;, &quot;Justin&quot;); ------ 1
 sclient.submit(GroovyTranslator.of(&quot;g&quot;).translate(t1.asAdmin().getBytecode()))
 
 Traversal t2 = g.addV(&quot;Person&quot;).property(T.id, 2).property(&quot;Name&quot;, &quot;Langer&quot;); ------ 2
 sclient.submit(GroovyTranslator.of(&quot;g&quot;).translate(t2.asAdmin().getBytecode()))
 
 
 // throw SomeException(&quot;some exeception....&quot;)      ------------------------------ 3
 
 Traversal t4 = g.addE(&quot;Edge Label&quot;).from(g.V(1)).to(g.V(2));    ------------------- 4
 sclient.submit(GroovyTranslator.of(&quot;g&quot;).translate(t2.asAdmin().getBytecode()))
 
 // all operations done now close sessioned client 
 sclient.close();      ------------------------------------------------------------- 
</code></pre>
<p>In above if an exception thrown from <code>3</code> then it won't close <code>SessionedClient</code> till it timeouts.</p>
<p>We can move <code>sclient.close()</code> to <code>finally</code> block using <code>try-catch</code>, this will execute <code>1</code> and <code>2</code> which actually should not be created as its executing as single transaction.</p>
<p>So I wanted to know if I can clear submitted queries (here in this case <code>1</code> and <code>2</code>) and close the <code>SessionedClient</code> without executing already submitted queries.</p>",1,0,2021-07-29 09:17:40.850000 UTC,,,0,transactions|gremlin|tinkerpop3|amazon-neptune|gremlin-server,87,2013-06-04 08:43:04.927000 UTC,2022-01-20 19:23:55.513000 UTC,"Pune, Maharashtra, India",11,0,0,4,,,,,,[]
How to Copy data from mount point databricks to ADLS gen2,"<p>I m trying to write the data from /mnt/Demo folder to adls gen2, Could you pls help the steps to do that. So far, i could able to execute the below lines of code and could able to copy data from adls to /mnt/demo folder and read data from it. How to write data to adls through databricks</p>
<pre><code>configs = {&quot;fs.azure.account.auth.type&quot;: &quot;OAuth&quot;,
       &quot;fs.azure.account.oauth.provider.type&quot;: &quot;org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider&quot;,
       &quot;fs.azure.account.oauth2.client.id&quot;: &quot;&quot;, #Enter &lt;appId&gt; = Application ID
       &quot;fs.azure.account.oauth2.client.secret&quot;: &quot;&quot;, #Enter &lt;password&gt; = Client Secret created in AAD
       &quot;fs.azure.account.oauth2.client.endpoint&quot;: &quot;https://login.microsoftonline.com/cccc/oauth2/token&quot;, #Enter &lt;tenant&gt; = Tenant ID
       &quot;fs.azure.createRemoteFileSystemDuringInitialization&quot;: &quot;true&quot;}

dbutils.fs.mount(
source = &quot;abfss://yy@xxx.dfs.core.windows.net/Test2&quot;, #Enter &lt;container-name&gt; = filesystem name &lt;storage-account-name&gt; = storage name
mount_point = &quot;/mnt/Demo17&quot;,
extra_configs = configs)
df = spark.read.csv(&quot;/mnt/Demo16/Contract.csv&quot;, header=&quot;true&quot;)
df_review = df[['AccountId', 'Id', 'Contract_End_Date_2__c', 'Contract_Type__c', 'StartDate', 'Contract_Term_Type__c', 'Status', 'Description', 'CreatedDate', 'LastModifiedDate']]
df_review.repartition(1).write.mode(&quot;append&quot;).csv(&quot;abfss://salesforcedata@storagedemovs.dfs.core.windows.net/Test2/trial&quot;)
display(df_review)
display(df)
</code></pre>",0,2,2021-04-14 23:19:01.030000 UTC,,,0,azure-databricks|azure-data-lake-gen2,50,2021-03-17 23:23:46.783000 UTC,2021-05-11 21:17:19.993000 UTC,,3,0,0,6,,,,,,[]
Faster approach to spark wholeTextFiles for multiple unformatted files. PySpark,"<p>I'm using spark to read multiple little files. Each file is a client specific format and contains multiple tables (each with a different structure). We have created a python parser which works and handles partitioning given the path. Let me explain via schema:</p>

<pre><code>folder
|- file_number=0001
   |- year=2019
      |- month=10
         |- day=21
            |- hour=17
               |- file.txt
|- file_number=0002
   |- year=2019
      |- month=10
         |- day=21
            |- hour=17
               |- file.txt
etc
.
.
.
</code></pre>

<p>So the naive approach is:</p>

<pre><code>sc.wholeTextFiles('/path/to/file_number=*/year=*/month=*/day=*/hour=*/*.txt')\ # This is a pair (path, file Content)
  .flatMap(lambda x: parser(x[1], x[0]))\ # This is the parser function. Is plain python and works fast. We use the path to pick up the partitioning. The parser returns a list of tuples that's why flatMap
  .foldByKey([], lambda x, y: x + y)      # The key is the table name and the value is the data as a list of dicts in a tabular-like style

</code></pre>

<p>The transformation <code>.wholeTextFiles('/path/to/file_number=*/year=*/month=*/day=*/hour=*/*.txt')</code> takes an insane amount of time, considerig than the rest takes not so much. </p>

<p>Up to <a href=""https://forums.databricks.com/questions/480/how-do-i-ingest-a-large-number-of-files-from-s3-my.html"" rel=""nofollow noreferrer"">this</a> blog, the problem might be some recursive call, so it is much better to list all files first and then read each file. I haven't be able to use Hadoop's <code>FileSystem.open(path)</code> as propose in the link because I am working on Azure Data Lake Gen2. But it is true that listing all files using <code>dbutlis.fs</code> is fast. </p>

<p>So the question is: <strong><em>How can I use such a list to read and parse files in parallel?</em></strong>. The problem is that <code>wholeTextFile</code> does not accept a list as an argument. I have try all of the following:</p>

<pre><code>list_of_all_files_paths = dbutils.someCode()

# Attempt 1: Type mismatch
rdd = sc.wholeTextFile(list_of_all_files_paths)

# Attempt 2: Works but all partitiong info is lost
rdd = spark.read.text(list_of_all_files_paths, wholetext=True)

# Attempt 3:  Takes a lot of time too
rdd = spark.read.text('path/to/')

# Attempt 3: This is the most succesfull approach... but looks sooo bad, and is not very fast neither...
rdd = sc.emptyRDD()
for path in list_of_all_files_paths:
  newRDD = sc.wholeTextFiles(path)
  rdd    = rdd.union(newRDD)
</code></pre>",1,4,2019-10-21 16:13:16.183000 UTC,,,0,apache-spark|pyspark|azure-databricks,482,2018-01-26 07:51:24.790000 UTC,2022-03-04 12:06:48.040000 UTC,"Sevilla, España",3552,249,35,227,,,,,,[]
AWS Neptune Node counts timing out,"<p>We're running a large bulk load into AWS neptune and can no longer query the graph to get node counts without the query timing out. What options do we have to ensure we can audit the total counts in the graph?</p>
<p>Fails on curl and sagemaker notebook.</p>",1,0,2021-09-22 18:53:46.690000 UTC,,,0,amazon-web-services|amazon-neptune,91,2020-08-20 21:22:57.010000 UTC,2022-03-05 23:21:22.593000 UTC,"Seattle, WA, USA",655,57,5,91,,,,,,[]
sparklyr :: Error reading parquet file using sparklyr library in R,"<p>I am trying to read <code>parquet</code> file from databricks <code>Filestore</code></p>
<pre><code>library(sparklyr)
</code></pre>
<h5>parquet_dir has been pre-defined</h5>
<pre><code>parquet_dir = /dbfs/FileStore/test/flc_next.parquet'
</code></pre>
<h5>List the files in the parquet dir</h5>
<pre><code>filenames &lt;- dir(parquet_dir, full.names = TRUE)
&quot;/dbfs/FileStore/test/flc_next.parquet/_committed_6244562942368589642&quot;                                                                   
[2] &quot;/dbfs/FileStore/test/flc_next.parquet/_started_6244562942368589642&quot;                                                                     
[3] &quot;/dbfs/FileStore/test/flc_next.parquet/_SUCCESS&quot;                                                                                         
[4] &quot;/dbfs/FileStore/test/flc_next.parquet/part-00000-tid-6244562942368589642-0edceedf-7157-4cce-a084-0f2a4a6769e6-925-1-c000.snappy.parquet&quot;
</code></pre>
<h5>Show the filenames and their sizes</h5>
<pre><code>data_frame(
  filename = basename(filenames),
  size_bytes = file.size(filenames)
)
rning: `data_frame()` was deprecated in tibble 1.1.0.
Please use `tibble()` instead.
This warning is displayed once every 8 hours.
Call `lifecycle::last_warnings()` to see where this warning was generated.
# A tibble: 4 × 2
  filename                                                            size_bytes
  &lt;chr&gt;                                                                    &lt;dbl&gt;
1 _committed_6244562942368589642                                             124
2 _started_6244562942368589642                                                 0
3 _SUCCESS                                                                     0
4 part-00000-tid-6244562942368589642-0edceedf-7157-4cce-a084-0f2a4a6…     248643
</code></pre>
<h4>Import the data into Spark</h4>
<pre><code>timbre_tbl &lt;- spark_read_parquet(&quot;flc_next.parquet&quot;, parquet_dir)
</code></pre>
<h4>Error : $ operator is invalid for atomic vectors</h4>
<pre><code>Some(&lt;code style = 'font-size:10p'&gt; Error: $ operator is invalid for atomic vectors &lt;/code&gt;)
</code></pre>
<p>I would appreciate any help/suggestion</p>
<p>Thanks in advance</p>",1,0,2022-02-01 16:02:31.030000 UTC,,2022-02-02 08:21:30.203000 UTC,-1,r|apache-spark|parquet|azure-databricks,24,2014-03-25 10:54:19.337000 UTC,2022-02-16 09:46:36.067000 UTC,,298,9,1,40,,,,,,[]
Release management with a distributed version control system,"<p>We're considering a switch from SVN to a distributed VCS at my workplace.</p>

<p>I'm familiar with all the reasons for wanting to use a DVCS for day-to-day development: local version control, easier branching and merging, etc., but I haven't seen that much that's compelling in terms of managing software releases.  Here's our release process:</p>

<ul>
<li>Discover what changes are available for merging.</li>
<li>Run a query to find the defects/tickets associated with these changes.</li>
<li>Filter out changes associated with ""open"" tickets. In our environment, tickets must be in a closed state in order to merged with a release branch.</li>
<li>Filter out changes we don't want in the release branch.  We are very conservative when it comes to merging changes.  If a change isn't absolutely necessary, it doesn't get merged.</li>
<li>Merge available changes, preferably in chronological order.  We group changes together if they're associated with the same ticket.</li>
<li>Block unwanted changes from the release branch (<code>svnmerge block</code>) so we don't have to deal with them again.</li>
</ul>

<p>Sometimes we can be juggling 3-5 different milestones at a time.  Some milestones have very different constraints, and the block list can get quite long.</p>

<p>I've been messing around with git, mercurial and plastic, and as far as I can tell none of them address this model very well.  It seems like they would work very well when you have only one product you're releasing, but I can't imagine using them for juggling multiple, very different products from the same codebase.</p>

<p>For example, cherry-picking seems to be an afterthought in mercurial.  (You have to use the 'transplant' extension, which is disabled by default).  After you cherry-pick a change into a branch it still shows up as an available integration.  Cherry-picking breaks the mercurial way of working.</p>

<p>DVCS seems to be better suited for feature branches.  There's no need for cherry-picking if you merge directly from a feature branch to trunk <em>and</em> the release branch.  But who wants to do all that merging all the time?  And how do you query for what's available to merge?  And how do you make sure all the changes in a feature branch belong together?  It sounds like total chaos.</p>

<p>I'm torn because the coder in me wants DVCS for day-to-day work.  I really want it.  But I fear the day when I have to put the release manager hat and sort out what needs to be merged and what doesn't.  I want to write code, I don't want to be a merge monkey.</p>",1,0,2010-05-15 04:57:53.170000 UTC,3.0,2010-05-15 06:15:51.407000 UTC,3,controls|version|distributed|dvcs,721,2010-05-15 04:57:53.153000 UTC,2010-05-16 02:56:04.420000 UTC,,31,0,0,9,,,,,,[]
What do people think of the fossil DVCS?,"<p>fossil <a href=""http://www.fossil-scm.org"" rel=""noreferrer"">http://www.fossil-scm.org</a><br>
I found this recently and have started using it for my home projects.  I want to hear what other people think of this VCS.  </p>

<p>What is missing in my mind, is IDE support.  Hopefully it will come, but I use the command line just fine.</p>

<p>My favorite things about fossil: single executable with built in web server wiki and bug tracking.  The repository is just one SQLite (<a href=""http://www.sqlite.org"" rel=""noreferrer"">http://www.sqlite.org</a>) database file, easy to do backups on.  I also like that I can run fossil from and keep the repository on my thumb drive.  This means my software development has become completely portable.  </p>

<p>Tell me what you think....</p>",10,5,2008-10-01 05:21:17.357000 UTC,46.0,2008-11-06 02:10:47.773000 UTC,124,dvcs|fossil,23088,2008-08-29 06:56:00.077000 UTC,2014-02-20 19:10:13.147000 UTC,"San Diego, CA",1891,28,3,141,,,,,,[]
Can I make an older revision the tip and push (using Mercurial)?,"<p>Say if I have a good revision: 3200. Then I want to test something, and since it has 10 lines of change, and I need to remove a few lines, even though I am still testing I commit first, and after some changes, commit again, and let's say, I did 6 commits.</p>

<p>Now I want to put this on hold, but I don't want to lose all the testing and code written, so I want to</p>

<pre><code>$ hg up -r 3200
</code></pre>

<p>which is the good, stable revision I want, and now can I commit and push as the tip? (if possible I want to avoid backing out <code>hg backout</code> because it looks bad somewhat, and I don't want to rollback because rollback has side effect of ""if someone pulled from me during this time, the change can somehow get back in the repo"")</p>",3,5,2011-03-22 01:40:03.837000 UTC,3.0,2015-03-26 00:38:20.957000 UTC,10,version-control|mercurial|dvcs|backout,4585,2009-05-09 15:50:29.477000 UTC,2022-03-04 09:41:10.460000 UTC,,137341,1445,39,12817,,,,,,[]
ERROR Shell: Failed to locate the winutils binary in the hadoop binary path,"<p>I know there are many questions like this but i tried all solutions trust me. And i keep getting the same error again and again.
I am trying to access spark of remote clusters and running localy by using data-bricks connect and conda env and the IDE i use is Pycharm.</p>
<p>I am running the env either in anaconda prompt or in the built in terminal of pycharm.  Both return this error:</p>
<pre><code> ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:382)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:397)
        at org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:390)
        at org.apache.hadoop.util.StringUtils.&lt;clinit&gt;(StringUtils.java:80)
        at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
        at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2693)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2693)
        at org.apache.spark.SecurityManager.&lt;init&gt;(SecurityManager.scala:79)
        at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
        at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
21/12/23 22:35:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to &quot;WARN&quot;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/12/23 22:35:59 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
View job details at https://&quot;databricks-name&quot;.cloud.databricks.com/?o=0#/setting/clusters/0-535-sh256/sparkUi
* Simple PySpark test passed
* Testing dbutils.fs
</code></pre>
<p>I saw all over the internt ppl say to download winutils and set hadoop_home var to point at it. I tried it plenty times with all variations i saw.  Nothing works i keep getting this error.</p>
<p>Also its strange because i thought winutils is only needed for local spark.  I dont need spark locally as i am trying to connect to it via db connect.   Any one can help me? I am stuck on it a few days already
thx</p>",0,5,2021-12-23 20:57:26.187000 UTC,,2021-12-31 08:10:59.133000 UTC,0,apache-spark|databricks-connect,63,2020-03-17 08:32:49.643000 UTC,2022-02-28 13:05:32.270000 UTC,,1,0,0,1,,,,,,[]
Accessing Azure DevOps Git file directly from Azure Databricks,"<p>We have a CSV file stored in a ADO (Azure DevOps) Git repository. I have Azure Databricks cluster running, and in the workspace I have a python code to read and transform this CSV file into a spark dataframe. But every time the file undergoes change,  I have to manually download it from ADO Git and upload to the Databricks workspace. I use the following command to verify that the file has been uploaded:-</p>
<pre><code>dbutils.fs.ls (&quot;/FileStore/tables&quot;)
</code></pre>
<p>It lists my file. I then use the following Python code to convert this CSV to Spark dataframe:</p>
<pre><code>file_location = &quot;/FileStore/tables/MyFile.csv&quot;
file_type = &quot;csv&quot;
# CSV options
infer_schema = &quot;true&quot;
first_row_is_header = &quot;true&quot;
delimiter = &quot;,&quot;
# The applied options are for CSV files. For other file types, these will be ignored.
df = spark.read.format(file_type) \
  .option(&quot;inferSchema&quot;, infer_schema) \
  .option(&quot;header&quot;, first_row_is_header) \
  .option(&quot;sep&quot;, delimiter) \
  .load(file_location)
</code></pre>
<p>So there is this manual step involved every time the file in the ADO Git repository changes. Is there any Python function using which I can directly point to the copy of the file in the master branch of the ADO Git ?</p>",1,0,2021-02-05 21:02:57.230000 UTC,,2021-02-05 21:24:52.417000 UTC,0,python|apache-spark|azure-devops|azure-databricks|azure-repos,653,2015-06-28 08:34:35.710000 UTC,2022-03-06 04:28:43.680000 UTC,,2814,233,7,444,,,,,,[]
Is it possible to access Databricks DBFS from Azure Data Factory?,"<p>I am trying to use the Copy Data Activity to copy data from Databricks DBFS to another place on the DBFS, but I am not sure if this is possible.</p>
<p>When I select Azure Delta Storage as a dataset source or sink, I am able to access the tables in the cluster and preview the data, but when validating it says that the tables are not delta tables (which they aren't, but I don't seem to acsess the persistent data on DBFS)</p>
<p>Furthermore, what I want to access is the DBFS, not the cluster tables. Is there an option for this?</p>",0,4,2020-11-09 12:48:55.830000 UTC,,,0,azure|etl|azure-data-factory|azure-databricks,226,2014-01-20 21:05:22.870000 UTC,2021-05-15 18:37:39.553000 UTC,,3,0,0,4,,,,,,[]
Pipeline disappeared from Azure Data Factory?,"<p>I have a streaming pipeline from Azure Data Factory that suddenly vanished from Monitoring tab from Azure Data Factory page. It is running on Databricks from quite a long time(May be more than 45 days without interruptions).</p>
<p>One day the pipeline is no longer shown in ADF Pipeline runs in Monitoring tab and the previous runs disappeared as well but none of the alerts set from Databricks side kicked off. Turns out the job is still running from Databricks side but the corresponding ADF pipeline details disappeared along with old runs for the same streaming pipeline.</p>
<p>How is this possible? Any reason for this to happen?</p>",1,0,2020-10-26 16:53:27.870000 UTC,,,0,spark-streaming|azure-data-factory|azure-databricks,141,2018-10-04 04:22:10.433000 UTC,2022-03-06 02:56:27.743000 UTC,"Bellevue, WA, USA",631,58,2,100,,,,,,[]
data.table fread in Databricks Spark,"<p>I've done a fair amount of searching and haven't come across any solid info regarding the use of the data.table package in the Databricks environment. Myself and other colleagues have carried out tests in Databricks trying to use the data.table fread function to read in a relatively large csv (about 15gb). The fread function takes a very long time (we've never actually run it to completion) but when running on our own laptops (16gb ram) it takes roughly 1-2 minutes.</p>
<p>Additionally to the example above, I've read in a relatively small 34mb csv with read.csv and fread. The run times are below:</p>
<ul>
<li>read.csv: 8 seconds</li>
<li>fread: 25 seconds</li>
</ul>
<p>As for cluster configuration, we're running the fread function on a single node cluster with 32 cores and 128gb of memory.</p>
<p>Does anyone have any suggestions for why data.table performs so poorly in the Databricks environment? I understand that this isn't really the best use of Databricks and that we should switch to SparkR for performance purposes but our agency has many users that would stand to benefit from being able to leverage the platform with their existing R code base and not having to tweak it too much.</p>",0,2,2022-01-11 16:22:06.077000 UTC,,,0,r|data.table|azure-databricks,49,2018-05-01 13:20:11.127000 UTC,2022-03-04 21:37:00.260000 UTC,,101,60,0,16,,,,,,[]
Spark Submit error when running a JAR from Azure Databricks,"<p>I'm trying to issue spark submit from Azure Databricks jobs scheduler, currently stuck with the below error. Error says: File file:/tmp/spark-events does not exist. I need some pointers to understand do we need to create this directory in Azure blob location(which is my storage Layer) or in Azure DBFS location.</p>
<p>As per the below link, not so clear where to create the directory when running the spark-submit from Azure Databricks jobs scheduler.</p>
<p><a href=""https://stackoverflow.com/questions/38350249/sparkcontext-error-file-not-found-tmp-spark-events-does-not-exist"">SparkContext Error - File not found /tmp/spark-events does not exist</a></p>
<p>Error:</p>
<pre><code>OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0
Warning: Ignoring non-Spark config property: eventLog.rolloverIntervalSeconds
Exception in thread &quot;main&quot; java.lang.ExceptionInInitializerError
    at com.dta.dl.ct.qm.hbase.reverse.pipeline.HBaseVehicleMasterLoad.main(HBaseVehicleMasterLoad.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.FileNotFoundException: File file:/tmp/spark-events does not exist
    at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
    at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
    at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
    at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
    at org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:97)
    at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:580)
    at com.dta.dl.ct.qm.hbase.reverse.pipeline.HBaseVehicleMasterLoad$.&lt;init&gt;(HBaseVehicleMasterLoad.scala:32)
    at com.dta.dl.ct.qm.hbase.reverse.pipeline.HBaseVehicleMasterLoad$.&lt;clinit&gt;(HBaseVehicleMasterLoad.scala)
    ... 13 more
</code></pre>",1,0,2020-07-15 02:03:42.223000 UTC,,,0,apache-spark|azure-databricks,243,2019-11-07 05:42:55.603000 UTC,2021-07-05 13:51:53.573000 UTC,"Bangalore, Karnataka, India",263,19,0,68,,,,,,[]
Filter edges with a list as the property value using gremlin-python,"<p>I'm storing a list as the property value of some edges in my graph, similar to the question <a href=""https://groups.google.com/forum/#!topic/gremlin-users/_-wCxCAEPh8"" rel=""nofollow noreferrer"">asked here</a>. The solution to that question was given in JavaScript, but I'm looking for a way to do the same thing in Python. </p>

<p>Also, note that Amazon Neptune doesn't support Lambda <a href=""https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-differences.html"" rel=""nofollow noreferrer"">steps</a>, so the solution can't use lambdas.</p>",1,0,2018-07-05 16:18:55.167000 UTC,,2018-07-06 00:21:08.090000 UTC,1,python|gremlin|tinkerpop3|amazon-neptune,695,2012-01-17 04:30:08.950000 UTC,2018-07-24 20:38:37.160000 UTC,"Troy, NY",181,0,0,22,,,,,,[]
Azure Databricks Python Job,"<p>I have a requirement to parse a lot of small unstructured files in near real-time inside Azure and load the parsed data into a SQL database. I chose Python <em>(because I don't think any Spark cluster or big data would suite considering the volume of source files and their size)</em> and the parsing logic has been already written. I am looking forward to schedule this python script in different ways using Azure PaaS</p>

<ol>
<li>Azure Data Factory</li>
<li>Azure Databricks</li>
<li>Both 1+2</li>
</ol>

<p>May I ask what's the implication of running a Python notebook activity from Azure Data Factory pointing to Azure Databricks? Would I be able to fully leverage the potential of the cluster (Driver &amp; Workers)?</p>

<p>Also, please suggest me if you think the script has to be converted to PySpark to meet my use case requirement to run in Azure Databricks? The only hesitation here is the files are in KB and they are unstructured. </p>",1,0,2019-12-26 07:27:31.213000 UTC,,,1,python|azure|azure-data-factory|azure-databricks,346,2017-03-08 07:09:46.707000 UTC,2022-03-06 04:36:04.880000 UTC,,839,26,0,163,,,,,,[]
How to load a AzureML model in an Azure Databricks compute?,"<p>I am trying to run a <code>DatabricksStep</code>. I have used <code>ServicePrincipalAuthentication</code> to authenticate the run:</p>
<pre><code>appId = dbutils.secrets.get(&lt;secret-scope-name&gt;, &lt;client-id&gt;)
tenant = dbutils.secrets.get(&lt;secret-scope-name&gt;, &lt;directory-id&gt;)
clientSecret = dbutils.secrets.get(&lt;secret-scope-name&gt;, &lt;client-secret&gt;)
subscription_id = dbutils.secrets.get(&lt;secret-scope-name&gt;, &lt;subscription-id&gt;)
resource_group = &lt;aml-rgp-name&gt;
workspace_name = &lt;aml-ws-name&gt;

svc_pr = ServicePrincipalAuthentication(
       tenant_id=tenant,
       service_principal_id=appId,
       service_principal_password=clientSecret)

ws = Workspace(
       subscription_id=subscription_id,
       resource_group=resource_group,
       workspace_name=workspace_name,
       auth=svc_pr
       )
</code></pre>
<p>The authentication is successful since running the following block of code gives the desired output:</p>
<pre><code>subscription_id = ws.subscription_id
resource_group = ws.resource_group
workspace_name = ws.name
workspace_region = ws.location
print(subscription_id, resource_group, workspace_name, workspace_region, sep='\n')
</code></pre>
<p>However, the following block of codes gives an error:</p>
<pre><code>model_name=&lt;registered-model-name&gt;
model_path = Model.get_model_path(model_name=model_name, _workspace=ws)
loaded_model = joblib.load(model_path)
print('model loaded!')
</code></pre>
<p>This is giving an error:</p>
<pre><code>UserErrorException:
    Message: 
Operation returned an invalid status code 'Forbidden'. The possible reason could be:
1. You are not authorized to access this resource, or directory listing denied.
2. you may not login your azure service, or use other subscription, you can check your
default account by running azure cli commend:
'az account list -o table'.
3. You have multiple objects/login session opened, please close all session and try again.
                
    InnerException None
    ErrorResponse 
{
    &quot;error&quot;: {
        &quot;message&quot;: &quot;\nOperation returned an invalid status code 'Forbidden'. The possible reason could be:\n1. You are not authorized to access this resource, or directory listing denied.\n2. you may not login your azure service, or use other subscription, you can check your\ndefault account by running azure cli commend:\n'az account list -o table'.\n3. You have multiple objects/login session opened, please close all session and try again.\n                &quot;,
        &quot;code&quot;: &quot;UserError&quot;
    }
}
</code></pre>
<p>The error is <code>Forbidden Error</code> even though I have authenticated using <code>ServicePrincipalAuthentication</code>.
How to resolve this error to run inference using an AML registered model in ADB?</p>",1,4,2020-12-17 14:59:00.290000 UTC,,,0,azure-databricks|azure-machine-learning-service,332,2020-10-03 12:46:02.437000 UTC,2022-02-28 17:58:49.850000 UTC,"Hyderabad, Telangana, India",704,173,32,103,,,,,,[]
How to use PipelineParameter of a published pipeline to specify databricks notebook widget with DatabricksStep?,"<p>i created a pipeline with several steps (azureml-defaults==1.23.0). When I run the published pipeline from the studio, the databricks steps always take the default value of the<code>PipelineParameter</code> no matter what value I choose when i submit the pipeline.</p>
<pre class=""lang-py prettyprint-override""><code>parser.add_argument('--import_date',         type=str, default = &quot;2021-04-23&quot;)   
....
....

import_date      = PipelineParameter(name=&quot;import_date&quot;       , default_value = params.import_date)
cluster_id       = PipelineParameter(name=&quot;cluster_id&quot;        , default_value = params.cluster_id)
step_type        = PipelineParameter(name=&quot;step_type&quot;         , default_value = params.step_type)
churn_months     = PipelineParameter(name=&quot;churnMonths&quot;       , default_value = params.churnMonths)



data_import_step = DatabricksStep(name=&quot;Databricks Data Import Step&quot;,
                                  existing_cluster_id=str(cluster_id.default_value),
                                  notebook_path=import_notebook_path,
                                  notebook_params={'ChurnMonthsWidget': churn_months, 
                                                   'startDateWidget'  : port_start_date,
                                                   'ImportDateWidget' : import_date,
                                                   'StepTypeWidget'   : step_type},
                                  run_name='Job_Data_Import',
                                  compute_target=databricks_compute,
                                  allow_reuse=False)
.....
.....
.....
pipeline_steps = StepSequence(steps=[data_import_step                  #Step 1
                                    ,data_manipulation_step            #Step 2
                                    ,data_extraction_step              #Step 3
                                 
                                    
                                    ,training_data_preparation_step    #Step 4
                                    ,model_training_step               #Step 5
                                 
                                
                                    ,prediction_data_preparation_step  #Step 6
                                    ,prediction_step                   #Step 7
                                    ])
                                    
pipeline = Pipeline(workspace = ws, steps=pipeline_steps)

published_pipeline = pipeline.publish(name        = params.pipeline_name,
                                         description = params.pipeline_description)
</code></pre>
<p>The default value of import_date is 2021-04-23.Even though I set
the import_date parameter to '2021-04-22' the databricks notebook still takes 2021-04-23 as import_date.</p>
<p>the databricks notebook has the following widgets</p>
<pre class=""lang-py prettyprint-override""><code>
today = str(date.today())
dbutils.widgets.text(&quot;ImportDateWidget&quot;, today, label = &quot;ImportDate&quot;)
import_date = dbutils.widgets.get(&quot;ImportDateWidget&quot;)


startDate = &quot;2019-01-01&quot;  
dbutils.widgets.text(&quot;startDateWidget&quot;, startDate, label = &quot;startDate&quot;)
start_date = dbutils.widgets.get(&quot;startDateWidget&quot;)

ChurnMonths = 3
dbutils.widgets.text(&quot;ChurnMonthsWidget&quot;, str(ChurnMonths), label = &quot;ChurnMonths&quot;)

step_type = &quot;Training&quot;
dbutils.widgets.text(&quot;StepTypeWidget&quot;,step_type,label =&quot;StepType&quot; )

N_MONTHS_CHURN = int(dbutils.widgets.get(&quot;ChurnMonthsWidget&quot;))
import_date    = dbutils.widgets.get(&quot;ImportDateWidget&quot;)
start_date     = dbutils.widgets.get(&quot;startDateWidget&quot;)
N_MONTHS_CHURN = int(dbutils.widgets.get(&quot;ChurnMonthsWidget&quot;))
step_type = dbutils.widgets.get(&quot;StepTypeWidget&quot;)
</code></pre>",0,0,2021-04-24 08:24:38.227000 UTC,,,0,azure-databricks|azure-machine-learning-studio|azure-machine-learning-service|azureml|azureml-python-sdk,112,2021-04-24 08:03:49.947000 UTC,2022-03-04 16:33:36.390000 UTC,,1,0,0,0,,,,,,[]
Azure databricks cluster don't have acces to mounted adls2,"<p>I followed the documentation <a href=""https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sp-access"" rel=""nofollow noreferrer"">azure-datalake-gen2-sp-access</a> and I mounted a ADLS2 storage in databricks, but when I try to see data from the GUI I get the next error:</p>
<p><strong>Cluster easy-matches-cluster-001 does not have the proper credentials to view the content. Please select another cluster.</strong></p>
<p><a href=""https://i.stack.imgur.com/NZBlm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NZBlm.png"" alt=""enter image description here"" /></a></p>
<p>I don't find any documentation, only something about premium databricks, so I can only access with a premium databricks resource?</p>
<p>Edit1: I can see the mounted storage with dbutils.</p>
<p><a href=""https://i.stack.imgur.com/QcmmU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QcmmU.png"" alt=""enter image description here"" /></a></p>",1,5,2021-10-06 00:51:29.807000 UTC,,2021-10-06 13:14:21.723000 UTC,0,azure|azure-databricks|azure-data-lake-gen2,458,2018-12-07 18:01:35.740000 UTC,2022-03-02 20:04:16.837000 UTC,"Santiago, Chile",380,20,1,43,,,,,,[]
"Neptune throwing ""Host did not respond in a timely fashion"" when trying to connect from private EC2","<p>I have created a neptune instance and per the <a href=""https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-console.html"" rel=""nofollow noreferrer"">documentation here</a>...</p>

<p>I create the following yaml...</p>

<pre><code>hosts: [xxx.xxx.us-east-2.neptune.amazonaws.com]
port: 8182
serializer: { className: org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV3d0, config: { serializeResultToString: true }}
</code></pre>

<p>And when I try to connect everything seems to work I see...</p>

<blockquote>
  <p>==>All scripts will now be sent to Gremlin Server - [xx.xx.us-east-2.neptune.amazonaws.com/172.xx.x.xxx:8182] - type
  ':remote console' to return to local mode</p>
</blockquote>

<p>What am I missing why is the query failing?</p>",2,2,2018-08-07 02:50:23.910000 UTC,,,0,amazon-web-services|amazon-neptune,334,2009-06-18 16:05:40.743000 UTC,2022-03-05 02:54:03.640000 UTC,"Columbus, OH",19055,690,51,2069,,,,,,[]
Azure Databrick: What configuration needs to add in cluster's configuration that helps to parallel execution and how to track applied logs,"<p>I created a message generator Tool that generates files in JSON format and directly writes into azure data lake storage.</p>
<p><strong>Scenario 1:</strong> To improve in performance,  I have applied java multithreading functionality within code</p>
<p>Use Java ExecutorService for multithreading :</p>
<pre><code>ExecutorService pool = Executors.newFixedThreadPool(5);
</code></pre>
<p>I tried to execute on my local machine via IDE  and identified that the process is executing in parallel (No of threads: 5) and there is good improvement in execution time.</p>
<p>While I am using azure Databricks Notebook to execute the same jar file with the below setting, identified that it's executing in serialization and there is no improvement in execution time:</p>
<p>Cluster Mode: Standard
Databrick runtime version: 7.0 (includes Apache Spark 3.0.0, Scala 2.12)
Driver and Worker Type: Standard_DS3_v2</p>
<p><strong>Scenario 2</strong>: Moreover, I have applied logger(<code>java.util.logging.Logger</code>) within code so to monitor activities while executing as a jar file.</p>
<p>Created this instance for logger:</p>
<pre><code>static Logger logger = Logger.getLogger(DataLakeConnection.class.getName());
</code></pre>
<p>a) I tried to access logs from the &quot;Driver logs&quot; section(track stdout, stderr, log-4j) in the Cluster page of Azure Databrick and unable to found any logs which I have applied as logger in jar file.</p>
<p>b) I tried to access more logs from the &quot;DBFS&quot; section in Azure Databrick using command via the notebook.</p>
<p><strong>My Question is</strong> :</p>
<ol>
<li><p>Do we have any configuration that needs to add for Spark cluster's configuration in Azure Databrick which allows parallel execution (in terms of multi-threads)</p>
</li>
<li><p>How can we track internal logs in details (the same internal logs applied within jar file)</p>
</li>
</ol>
<p>Thank you in advance :)</p>",0,0,2020-09-01 08:45:17.617000 UTC,,2020-09-01 12:19:18.973000 UTC,1,java|multithreading|scala|apache-spark|azure-databricks,66,2020-09-01 06:22:27.873000 UTC,2022-01-13 07:35:35.223000 UTC,,11,0,0,2,,,,,,[]
Check if a in-memory RDD table exists and unionize all those which exists using TeraData connection in Databricks?,"<p>I am checking for many tables as a part of weekly job and want to make the query dynamic.
Currently, I check for each table manually and then append those which exists using a series of nested &quot;try-except&quot; statements. Going forward, the business wants to add more tables</p>
<p>I want to create a query/connection from tera data which can scan if a table exists or not. If it does exists in Teradata, import it to databricks, merge all the imported tables together and then perform a series of calculations on the merged table.</p>
<pre><code>query = '''
(
SELECT 
*
FROM dleci_sa.eg_wk{}_kcg_result
) as df'''.format(last_week)

df_kcg = read_teradata_parallel(jdbcDatabase,td_user,td_pass,query)

#unionise the two disparate df together
try:
  df=df_kcg.union(df_ptvb).cache()
  df.count()
except:
  
  try:
    df=df_ptvb
  except:
    df=df_kcg
</code></pre>
<p>Looking it this way, I can very well check if a table exists or not. If it does not &quot;df_kcg&quot; (from the code) will not return any value. the problem arises when I need to unionize them all together dynamically. Help!!</p>",0,0,2021-05-05 17:18:13.877000 UTC,,,0,pyspark|azure-databricks,12,2015-12-10 08:21:59.907000 UTC,2022-03-04 19:08:03.980000 UTC,,95,34,0,27,,,,,,[]
AWS - Neptune restore from snapshot using SDK,"<p>I'm trying to test restoring Neptune instances from a snapshot using python (boto3). Long story short, we want to spin up and delete the Dev instance daily using automation.</p>

<p>When restoring, my restore seems to only create the cluster without creating the attached instance. I have also tried creating an instance once the cluster is up and add to the cluster, but that doesn't work either. (ref: client.create_db_instance)</p>

<p>My code does as follows, get the most current snapshot. Use that variable to create the cluster so the most recent data is there.</p>

<pre><code>import boto3

client = boto3.client('neptune')

response = client.describe_db_cluster_snapshots(
    DBClusterIdentifier='neptune',
    MaxRecords=100,
    IncludeShared=False,
    IncludePublic=False
)

snaps = response['DBClusterSnapshots']
snaps.sort(key=lambda c: c['SnapshotCreateTime'], reverse=True)

latest_snapshot = snaps[0]
snapshot_ID = latest_snapshot['DBClusterSnapshotIdentifier']

print(""Latest snapshot: "" + snapshot_ID)

db_response = client.restore_db_cluster_from_snapshot(
    AvailabilityZones=['us-east-1c'],
    DBClusterIdentifier='neptune-test',
    SnapshotIdentifier=snapshot_ID,
    Engine='neptune',
    Port=8182,
    VpcSecurityGroupIds=['sg-randomString'],
    DBSubnetGroupName='default-vpc-groupID'
)

time.sleep(60)

db_instance_response = client.create_db_instance(
    DBName='neptune',
    DBInstanceIdentifier='brillium-neptune',
    DBInstanceClass='db.r4.large',
    Engine='neptune',
    DBSecurityGroups=[
        'sg-string',
    ],
    AvailabilityZone='us-east-1c',
    DBSubnetGroupName='default-vpc-string',
    BackupRetentionPeriod=7,
    Port=8182,
    MultiAZ=False,
    AutoMinorVersionUpgrade=True,
    PubliclyAccessible=False,
    DBClusterIdentifier='neptune-test',
    StorageEncrypted=True
)
</code></pre>

<p>The documentation doesn't help much at all. It's very good at providing the variables needed for basic creation, but not the actual instance. If I attempt to create an instance using the same Cluster Name, it either errors out or creates a new cluster with the same name appended with '-1'.</p>",1,1,2018-10-18 15:51:38.270000 UTC,,2018-10-19 13:04:56.913000 UTC,3,python-3.x|amazon-web-services|boto3|amazon-neptune,382,2013-06-03 18:09:58.513000 UTC,2022-02-28 13:06:42.277000 UTC,,43,0,0,3,,,,,,[]
Options to write data from Azure Databricks to Azure Synapse analytics,"<p>In our current project we use Azure Databricks for all processing requirements and write the data finally to Azure Synapse. So I'm exploring on options of writing data from Azure Databricks to Azure Synapse analytics. For loading Dimension table with SCD2 logic having start date and end date where inserts and updates need to happen, what is the best way to write this data to Azure synapse. I can either create external table in Azure synapse and do truncate &amp; load to the target Azure synapse table. But in this case I need to truncate and load the table. What if I don't need to truncate and load the table and load only the daily inserts or updates to Azure Synapse? what are the possible options of doing this without truncation.</p>",0,0,2021-04-15 07:01:51.957000 UTC,,,0,azure|azure-databricks|azure-synapse,59,2016-04-28 14:47:00.813000 UTC,2021-06-08 12:35:47.530000 UTC,,1173,3,0,147,,,,,,[]
how to use a file date automatically in scala?,"<p>I am reading an avro file from Azure data lake using databricks and I am using this path to read current date file for daily run, the code to drive the file date looks like this and it gets the current date fine.</p>

<pre><code>    val pfdtm = ZonedDateTime.now(ZoneOffset.UTC)   
        val fileDate =DateTimeFormatter.ofPattern(""yyyy_MM_dd"").format(pfdtm)

fileDate=2020-02-02
</code></pre>

<p>but when I use the fileDate variable in the path, it does not work, it raises path does not exist error. you can see the below path</p>

<pre><code>val df=spark.read.format(""com.databricks.spark.avro"").load(""adl://power.azuredatalakestore.net/SD_Case/eventhubspace/venthub/0_${fileDate}_*_*_*.avro"")
</code></pre>

<p>but when I use the actual date instead of the variable, it works fine</p>

<pre><code>val df=spark.read.format(""com.databricks.spark.avro"").load(""adl://powerbiconnect.azuredatalakestore.net/SD_Case/sdeventhubspace/sdeventhub/0_2020_02_02_*_*_*.avro"")
</code></pre>

<p>the actual folder path looks like this, with a sample daily file form day 2.</p>

<pre><code>adl://power.azuredatalakestore.net/SD_Case/eventhubspace/venthub/0_2020_02_02_10_11_15.avro
</code></pre>

<p>I will appreciate any help on correcting my code. thanks in advance</p>",0,3,2020-02-05 03:53:39.767000 UTC,,2020-02-05 04:05:11.903000 UTC,0,date|azure-data-lake|azure-databricks|spark-avro,94,2019-02-18 20:39:58.007000 UTC,2021-11-10 05:52:10.410000 UTC,,125,5,0,58,,,,,,[]
"Mercurial ""default-push not found!""?","<p>I have Mercurial installed on my Windows 7 machine. I know this because when I type <code>hg --version</code> I get:</p>

<pre><code>Mercurial Distributed SCM (version 2.5.2)
(see http://mercurial.selenic.com for more information)

Copyright (C) 2005-2012 Matt Mackall and others
This is free software; see the source for copying conditions. There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
</code></pre>

<p>I just committed some code to a local repo via <code>hg commit -m ""Made some changes.""</code>.</p>

<p>I then type <code>hg push</code> and get:</p>

<pre><code>pushing to default-push
abort: repository default-push not found!
</code></pre>

<p>My <code>C:\Users\myuser\.hg\hgrc</code> file looks like:</p>

<pre><code>[trusted]
users = myuser
</code></pre>

<p><strong>What's going on here?</strong></p>",1,1,2014-10-22 13:54:47.140000 UTC,,,3,mercurial|dvcs,2175,2014-09-04 20:06:04.317000 UTC,2018-04-06 15:41:56.787000 UTC,,24747,1336,16,1797,,,,,,[]
SSL errors while installing python packages from PyPI in a databricks cluster,"<p><strong>I am trying to install azure-storage-file-datalake in a databricks cluster, but due to internal dependency I am facing the below SSL error. I am using python 3.7.3, pip 20 and was able to install python packages that don't have any dependency or if the dependencies are already installed by default.</strong></p>
<p>java.lang.RuntimeException: ManagedLibraryInstallFailed: org.apache.spark.SparkException: Process List(/databricks/python/bin/pip, install, azure-storage-file-datalake, --disable-pip-version-check) exited with code 1. Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(&quot;bad handshake: SysCallError(104, 'ECONNRESET')&quot;))': /simple/azure-storage-file-datalake/ Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(&quot;bad handshake: SysCallError(104, 'ECONNRESET')&quot;))': /simple/azure-storage-file-datalake/ Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(&quot;bad handshake: SysCallError(104, 'ECONNRESET')&quot;))': /simple/azure-storage-file-datalake/ Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(&quot;bad handshake: SysCallError(104, 'ECONNRESET')&quot;))': /simple/azure-storage-file-datalake/ Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(&quot;bad handshake: SysCallError(104, 'ECONNRESET')&quot;))': /simple/azure-storage-file-datalake/ Could not find a version that satisfies the requirement azure-storage-file-datalake (from versions: ) No matching distribution found for azure-storage-file-datalake for library:PythonPyPiPkgId(azure-storage-file-datalake,None,None,List()),isSharedLibrary=false</p>",2,0,2020-08-18 05:48:42.560000 UTC,,,1,azure-databricks,784,2018-03-07 13:10:25.403000 UTC,2022-03-04 14:13:24.873000 UTC,"Bangalore, Karnataka, India",135,6,0,38,,,,,,[]
How to convert json file to csv file using python or spark dataframe,"<p>I have to convert json file to csv file using spark dataframe in databricks. I have tried the below code to convert the json to csv but i'm getting the  CSV data source does not support array data type in spark dataframe . I'm unable to convert to csv file .can someone help me on this issue how to remove _corrupt_string?</p>

<pre><code> import json
    data=r'/dbfs/FileStore/tables/ABC.json'
    print (""This is json data "", data)
    def js_r(data):
       with open(data, encoding='utf-8') as f_in:
           return(json.load(f_in))

    if __name__ == ""__main__"":
        dic_data_first = js_r(data)
        print(""This is my dictionary"", dic_data_first)
    keys= dic_data_first.keys()
    print (""The original dict keys"",keys)
    dic_data_second={'my_items':dic_data_first['Data']for key in keys}
    with open('/dbfs/FileStore/tables/ABC_1.json', 'w') as f:   
         json.dump(dic_data_first, f)
    df = sqlContext.read.json('dbfs:/FileStore/tables/ABC_1.json')   # reading a json and writing a  parquet
    print(df)
df.write.mode(""overwrite"").format(""com.databricks.spark.csv"").option(""header"",""true"").csv(""/dbfs/FileStore/tables/ABC_1.csv"")
JSON data as follows:
{""Table"":""test1"",
  ""Data"":[
{""aa"":""1"",
 ""bb"":""2""},
{""aa"" :""ss"",
""bb"":""dc""}            
}]
}
</code></pre>",0,7,2019-04-09 15:00:44.190000 UTC,0.0,2019-04-10 13:43:11.703000 UTC,1,python-3.x|pyspark|azure-databricks,1741,2018-10-04 16:30:46.120000 UTC,2021-02-25 15:56:54.010000 UTC,,113,3,0,83,,,,,,[]
Test class helper library issue ( ImportError: cannot import name 'Test' ),"<p>I was working with a normal wordcount spark application on Databricks.
To use the helper library I used spark._mooc_meta library in my cluster.</p>

<p>After which I have expected to get output as 1 test passed for below code. But getting some different issue.</p>

<pre><code>    from databricks_test_helper import Test

    Test.assertEquals(wordCountsDF.collect(), [(tutorial, 1), (spark, 2),
    (look, 2), (python, 1)],
    'incorrect counts for wordCountsDF')
</code></pre>

<p>Output : ImportError: cannot import name 'Test'</p>

<p>Below is the python and Scala version used:
Python version : 3
Databricks Runtime version : 5.2 (Spark 2.4 and scala 2.11)</p>",0,2,2019-03-26 18:35:57.680000 UTC,1.0,,2,apache-spark|pyspark|jupyter-notebook|azure-databricks,739,2016-06-29 06:21:14.477000 UTC,2022-03-05 12:26:59.647000 UTC,,99,8,0,19,,,,,,[]
How to increase timeout setting in Gremlin Java Remote client?,"<p>We are getting below exception while performing a load test on our application which is using Gremlin Java.
how to solve this issue?</p>

<p>Exception:</p>

<pre><code>java.lang.IllegalStateException: org.apache.tinkerpop.gremlin.process.remote.RemoteConnectionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.TimeoutException: Timed out while waiting for an available host - check the client configuration and connectivity to the server if this message persists
        at org.apache.tinkerpop.gremlin.process.remote.traversal.step.map.RemoteStep.promise(RemoteStep.java:98 )
        at org.apache.tinkerpop.gremlin.process.remote.traversal.step.map.RemoteStep.processNextStart(RemoteStep.java:65 )
        at org.apache.tinkerpop.gremlin.process.traversal.step.ut
</code></pre>",2,6,2019-02-25 15:43:43.720000 UTC,1.0,2019-02-25 17:37:11.010000 UTC,1,java|gremlin|amazon-neptune,1899,2012-11-30 10:23:15.713000 UTC,2020-05-20 18:20:17.160000 UTC,,691,4,0,61,,,,,,[]
Add conf file to classpath in Microsoft Azure,"<p>This question is similar to <a href=""https://stackoverflow.com/questions/58238269/add-conf-file-to-classpath-in-google-dataproc"">my other question</a> but this time for Microsoft Azure.</p>

<p>We have a config file that needs to be in the classpath of the driver (and possibly the executors). </p>

<p>When defining a Databricks cluster in Microsoft Azure, I configured custom Spark configs:
spark.executor.extraClassPath and spark.driver.extraClassPath. I pointed both paths to a directory in dbfs that contains our config. But as half suspected, this doesn't work.</p>

<p>The question is, which type of paths are supported by these Spark Configs? If it's only local filesystem, how do I get my config file on the driver (and the executors)?</p>",1,0,2019-11-01 09:02:52.033000 UTC,,2019-11-06 10:47:36.233000 UTC,1,azure|apache-spark|azure-databricks,211,2009-06-04 10:29:58.440000 UTC,2022-03-02 20:49:56.740000 UTC,Switzerland,1142,86,8,93,,,,,,[]
Using git revert in this particular scenario,"<p>This is what I had:</p>

<pre><code>$ ls
file

$ cat file 
change 1
change 2
change 3

$ git log --oneline
8979b76 Add change 3 to file
ff1aead Add change 2 to file
53559fe Add change 1 to file
</code></pre>

<p>Q1 : This is what I did and what I got:</p>

<pre><code>$ git revert 53559fe
error: could not revert 53559fe... Add change 1 to file
hint: after resolving the conflicts, mark the corrected paths
hint: with 'git add &lt;paths&gt;' or 'git rm &lt;paths&gt;'
hint: and commit the result with 'git commit'
$ cat file 
change 1
change 2
change 3
</code></pre>

<p>Why so ?</p>

<p>Q2: Then I did:</p>

<pre><code>$ git revert --abort
$ git revert ff1aead
error: could not revert ff1aead... Add change 2 to file
hint: after resolving the conflicts, mark the corrected paths
hint: with 'git add &lt;paths&gt;' or 'git rm &lt;paths&gt;'
hint: and commit the result with 'git commit'
$ cat file 
change 1
&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD
change 2
change 3
=======
&gt;&gt;&gt;&gt;&gt;&gt;&gt; parent of ff1aead... Add change 2 to file
</code></pre>

<p>Why did I get an error?</p>

<p>How to interpret the markers in the file ?</p>",1,0,2014-12-12 05:32:14.690000 UTC,,2014-12-12 05:38:26.043000 UTC,0,git|github|version-control|versioning|dvcs,49,2010-05-22 23:11:39.357000 UTC,2022-03-05 01:46:45.317000 UTC,"San Francisco, CA, USA",21354,1330,29,1093,,,,,,[]
adding emailing Functionality in my Scala code in databricks,"<p>i want to add my email notification part in my scala code in databricks.</p>
<p>I am trying to import --- import org.apache.commons.mail._
but getting below error :
<strong>object mail is not a member of package org.apache.commons .</strong></p>
<p>Can you help me ?? i want</p>
<p>counts of the table to be printed in the email.</p>
<p>below is the code i found and trying Mail.scala:
<a href=""https://www.bing.com/search?q=how+to+send+emails+in+scala+language&amp;FORM=AWRE&amp;ntref=1"" rel=""nofollow noreferrer"">https://www.bing.com/search?q=how+to+send+emails+in+scala+language&amp;FORM=AWRE&amp;ntref=1</a></p>
<p>note : I am using Microsoft Azure with databricks</p>",0,4,2022-03-01 14:28:48.083000 UTC,,2022-03-01 14:53:07.320000 UTC,-1,scala|azure-databricks|mailing,28,2018-06-10 08:02:28.983000 UTC,2022-03-04 12:58:05.477000 UTC,,9,0,0,3,,,,,,[]
"Connect to DB2 using ibm_db , connection attempt starts but never finish","<p>trying to connect to Ibm DB2 with ibm_db in Python, Im running it on Azure Databricks.
I get through to the DB2 but the connection just hangs there for a while until it end with an error.</p>
<pre><code>from ibm_db import connect

connection = connect('DATABASE=MyDB;HOSTNAME=MyHost;PORT=MyPort;PROTOCOL=TCPIP;UID=MyUid;PWD=MyPwd', '', '')
</code></pre>
<p>I get after a while the error:</p>
<p>[IBM][CLI Driver] SQL30081N  A communication error has been detected. Communication protocol being used: &quot;TCP/IP&quot;.  Communication API being used: &quot;SOCKETS&quot;.  Location where the error was detected: &quot;MyIp&quot;.  Communication function detecting the error: &quot;connect&quot;.  Protocol specific error code(s): &quot;110&quot;, &quot;<em>&quot;, &quot;</em>&quot;.  SQLSTATE=08001 SQLCODE=-30081</p>
<p>On the DB2 side they can see the connection attempt as:</p>
<p>SYN-received
Waiting for a confirming connection request
acknowledgment.</p>
<p>Now the DB2 has a package collection that is perhaps needed, I cant seem to find the syntax to add this to the connection string however.</p>
<p>The other suspect is azure databricks but has no clue in what way.</p>
<p>Any help is greatly appreciated.</p>",0,8,2021-10-07 13:41:19.453000 UTC,,2021-10-07 14:46:44.883000 UTC,0,python|db2|azure-databricks|db2-400,127,2021-10-07 13:18:07.593000 UTC,2022-03-04 15:01:22.947000 UTC,,1,0,0,1,,,,,,[]
How to run a python script in databricks on Azure datalake delta data,<p>I have a python script written in azure databricks for doing ETL on the raw text files in &quot;.txt&quot; format and having no schema stored in Azure datalake V2. I migrated these text files from an on-premises virtual machine using data factory. My requirement is to run the python script only on new data (delta data) migrated into Azure datalake. How can I achieve it?</p>,1,0,2020-11-28 05:33:03.423000 UTC,,,0,azure|azure-functions|azure-data-factory|azure-data-lake|azure-databricks,231,2020-05-21 05:09:38.910000 UTC,2021-01-27 21:20:49.953000 UTC,,123,18,0,55,,,,,,[]
com.databricks.sql.io.FileReadException: Error while reading file wasbs:REDACTED_LOCAL_PART@****.blob.core.windows.net/,"<p>I am getting the following error message:</p>
<pre><code>com.databricks.sql.io.FileReadException: Error while reading file wasbs:REDACTED_LOCAL_PART@*******.blob.core.windows.net/cook/processYear=2021/processMonth=01/processDay=09/processHour=00/part-00003-tid-4640843606947508963-f47b3f23-a580-40bd-ad0d-29229-1.c000.avro.
at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:286)
at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:264)
at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException
at com.microsoft.azure.storage.core.Utility.initIOException(Utility.java:737)
at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:235)
... 23 more
Caused by: com.microsoft.azure.storage.StorageException: Blob hash mismatch (integrity check failed), Expected value is xmypzfnpTdq8eFLxZ49DhQ==, retrieved CY7+V9/JEfVroD5omBB2Uw==.
at com.microsoft.azure.storage.blob.CloudBlob$9.postProcessResponse(CloudBlob.java:1409)
at com.microsoft.azure.storage.blob.BlobInputStream.dispatchRead(BlobInputStream.java:255)
... 53 more
</code></pre>
<p>Any idea how to resolve this? Thanks.</p>",0,3,2021-01-29 20:41:40.977000 UTC,,2021-02-02 19:39:17.303000 UTC,0,apache-spark|azure-blob-storage|azure-databricks,654,2014-07-23 09:18:32.543000 UTC,2022-03-04 22:44:44.700000 UTC,,853,131,2,215,,,,,,[]
Is possible to read an Azure Databricks table from Azure Data Factory?,"<p>I have a table into an Azure Databricks Cluster, i would like to replicate this data into an Azure SQL Database, to let another users analyze this data from Metabase.</p>

<p>Is it possible to acess databricks tables through Azure Data factory?</p>",2,2,2019-01-14 16:38:12.110000 UTC,1.0,,0,azure|azure-data-factory|metabase|azure-databricks,1225,2013-12-19 03:32:00.203000 UTC,2022-03-04 22:14:52.737000 UTC,Brasil,779,96,5,92,,,,,,[]
Dataframe.write with table containing Always generate columns and auto generate columns is failing (SQL Server + sql-spark-connector),"<p>Dataframe write to SQL Server table containing Always autogenerate column fails. I am using Apache Spark Connector for SQL Server and Azure SQL.</p>
<p>When autogenerate field are not included in dataframe, I encountered - &quot;No key found &quot; error.</p>
<p>If auto-generate columns are included in dataframe, I encountered &quot;Cannot insert an explicit value into a GENERATED ALWAYS column in table error.&quot;</p>
<ul>
<li>Azure Databricks- 7.6 runtime</li>
<li>Azure SQL database</li>
<li>Language - PySpark</li>
</ul>
<p>Exception encountered:</p>
<blockquote>
<p>org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 25, 10.139.64.4, executor 1):   com.microsoft.sqlserver.jdbc.SQLServerException: Cannot insert an explicit value into a GENERATED ALWAYS column in table '&lt;</p>
<p>Use INSERT with a column list to exclude the GENERATED ALWAYS column, or insert a DEFAULT into GENERATED ALWAYS column.</p>
</blockquote>
<p><strong>PySpark code</strong></p>
<pre><code>df = read parquet file

df.write \
  .format(&quot;com.microsoft.sqlserver.jdbc.spark&quot;) \
  .mode(&quot;append&quot;) \
  .option(&quot;url&quot;, url) \
  .option(&quot;dbtable&quot;, &quot;TEMPORAL_TABLE&quot;) \
  .option(&quot;user&quot;, _username) \
  .option(&quot;password&quot;, _password) \
  .option(&quot;driver&quot;, &quot;com.microsoft.sqlserver.jdbc.SQLServerDriver&quot;)\
  .option(&quot;schemaCheckEnabled&quot;, False)\
  .save()
</code></pre>
<p>Azure SQL Temporal Table Definition:</p>
<pre><code>CREATE TABLE DBO.TEMPORAL_TABLE(
    [UUID] [varchar](255) NOT NULL,
    [SERVICE_ID] [bigint] NULL,
    [START_DATE] [datetime2](7) NULL,
    [END_DATE] [datetime2](7) NULL,
    [CHANGED_ON] [datetime2](7) NULL,
    [operation] [char](1) NULL,
    [SysStartTime] [datetime2](7) GENERATED ALWAYS AS ROW START NOT NULL,
    [SysEndTime] [datetime2](7) GENERATED ALWAYS AS ROW END NOT NULL,
 CONSTRAINT [PK_TEMPORAL] PRIMARY KEY CLUSTERED 
(
    [UUID] ASC
)WITH (STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, OPTIMIZE_FOR_SEQUENTIAL_KEY = OFF) ON [PRIMARY],
    PERIOD FOR SYSTEM_TIME ([SysStartTime], [SysEndTime])
) ON [PRIMARY]
WITH
(
SYSTEM_VERSIONING = ON ( HISTORY_TABLE = [history].[TEMPORAL_TABLE_HISTORY] )
)
GO

ALTER TABLE DBO.TEMPORAL_TABLE ADD  DEFAULT (newid()) FOR [UUID]
GO

ALTER TABLE DBO.TEMPORAL_TABLEADD  DEFAULT (getutcdate()) FOR [SysStartTime]
GO

ALTER TABLE DBO.TEMPORAL_TABLE ADD  DEFAULT (CONVERT([datetime2],'9999-12-31 23:59:59.9999999')) FOR [SysEndTime]
</code></pre>
<p>spark sql connector - <a href=""https://github.com/microsoft/sql-spark-connector"" rel=""nofollow noreferrer"">https://github.com/microsoft/sql-spark-connector</a></p>",0,0,2021-06-18 16:17:40.523000 UTC,1.0,2021-06-18 20:33:01.537000 UTC,0,pyspark|azure-sql-database|azure-databricks,140,2021-06-18 16:05:01.567000 UTC,2022-03-05 04:10:18.243000 UTC,,1,0,0,2,,,,,,[]
spark dataframe aggregation of column based on condition in scala,"<p>I have csv data a following in following format.</p>

<p>I need to find top 2 vendor whose turnover is greater than 100 in year 2017.</p>

<blockquote>
  <p>Turnover= Sum(Invoices whose status is Paid-in-Full ) - Sum(Invoices
  whose status is Exception or Rejected)</p>
</blockquote>

<p>I have loaded the data from csv in datebricks scala notebook as follow:</p>

<pre><code>val invoices_data = spark.read.format(file_type)
                  .option(""header"", ""true"")
                  .option(""dateFormat"", ""M/d/yy"")
                  .option(""inferSchema"", ""true"")
                 .load(""invoice.csv"")
</code></pre>

<p>Then I tried to make group by vendor name</p>

<pre><code>val avg_invoice_by_vendor = invoices_data.groupBy(""VendorName"")
</code></pre>

<p>But Now I don't know how to proceed further.</p>

<p>Here is sample csv data.</p>

<pre><code>Id     InvoiceDate      Status         Invoice   VendorName
    2   2/23/17         Exception       23        V1
    3   11/23/17        Paid-in-Full    56        V1
    1   12/20/17        Paid-in-Full    12        V1
    5   8/4/19          Paid-in-Full    123       V2
    6   2/6/17          Paid-in-Full    237       V2
    9   3/9/17          Rejected        234       V2
    7   4/23/17         Paid-in-Full    78        V3
    8   5/23/17         Exception       345       V4
</code></pre>",2,0,2020-02-03 09:44:15.380000 UTC,,,0,scala|dataframe|apache-spark-sql|azure-databricks,53,2015-07-11 18:37:47.507000 UTC,2022-03-05 16:17:49.200000 UTC,,318,10,1,129,,,,,,[]
Spark read/write to Azure blob storage - IOException: No FileSystem for scheme: wasbs,"<p>I am trying to read/write to Azure blob storage but am constantly getting the &quot;No FileSystem for scheme: wasbs&quot;. Here is what my gradle file looks like</p>
<pre><code>plugins {
    // Apply the scala plugin to add support for Scala
    id 'scala'
    id 'idea'
    id 'application'
}

repositories {
    mavenLocal()
    jcenter()

    maven {
        url &quot;https://repository.mulesoft.org/nexus/content/repositories/public&quot;
    }
}

dependencies {
    // Spark SQL subsumes Spark Core
    compileOnly 'org.apache.spark:spark-sql_2.12:3.0.3'
    implementation group: 'org.scala-lang', name: 'scala-library', version: '2.12.1'

    implementation group: 'com.typesafe', name: 'config', version: '1.4.1'
    implementation group: 'com.microsoft.azure', name: 'azure-storage', version: '8.6.6'
    implementation group: 'org.apache.hadoop', name: 'hadoop-azure', version: '3.3.1'
}


jar {
    manifest {
        attributes('Main-Class': 'AppRunner')
    }
    from {
        configurations.runtimeClasspath.collect { it.isDirectory() ? it : zipTree(it) }
    }
    exclude 'META-INF/*.RSA'
    exclude 'META-INF/*.SF'
    exclude 'META-INF/*.DSA'

    duplicatesStrategy(DuplicatesStrategy.EXCLUDE)
}
</code></pre>
<p>I am creating a jar file with all the required dependencies for hadoop-azure and azure-storage.</p>
<p>This is what my Scala file is primarily doing.</p>
<pre><code>spark.conf.set(&quot;fs.azure.account.key.&lt;blob-name&gt;.blob.core.windows.net&quot;, &quot;&lt;blob-key&gt;&quot;)
spark.sparkContext.hadoopConfiguration.set(&quot;fs.azure&quot;, &quot;org.apache.hadoop.fs.azure.NativeAzureFileSystem&quot;)

val df = spark.read.parquet(&quot;wasbs://&lt;container-name&gt;@&lt;blob-name&gt;.blob.core.windows.net/data/&quot;)
</code></pre>
<p>My Spark setup is currently on a VM in the Azure environment where I am running Spark 3.1.2 in standalone mode.</p>
<p>My spark-submit command looks like</p>
<p><code>./spark-3.1.2-bin-hadoop2.7/bin/spark-submit --master &quot;local[*]&quot; --jars jars/hadoop-azure-3.3.1.jar,jars/azure-storage-8.6.6.jar compiled-job.jar</code></p>
<p>I do not need to include the jars as a parameter but I included it for testing because it seems that the Spark job cannot primarily find the <code>wasbs</code> filesystem.</p>
<p>Here is the exception I receive when I run the jar file</p>
<pre><code>Exception in thread &quot;main&quot; java.io.IOException: No FileSystem for scheme: wasbs
    at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)
    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)
    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)
</code></pre>
<p>Any idea what I am doing wrong here?</p>",0,0,2022-03-01 08:49:13.513000 UTC,,,0,azure|apache-spark|hadoop|azure-blob-storage|azure-databricks,26,2012-05-10 03:51:40.140000 UTC,2022-03-05 10:41:36.517000 UTC,,1901,26,2,136,,,,,,[]
Spark Streaming on a REST endpoint,"<p>I am trying to develop a streaming application based on <strong>PySpark</strong> and I would like to have some suggestions.</p>
<p>The data source of my scenario would be a <strong>REST</strong> endpoint that serves API requests and returns a new JSON object for each call (like a sensor detecting temperature changes).</p>
<p>Considering that I am on Azure and PySpark is deployed on azure <strong>databricks</strong>, what is the best way to connect my spark streaming data frame to a REST endpoint?</p>
<p>I am reading about products like Web pub/sub, EventHub, and EventGrid, or even Kafka, but it looks like an overkill for my purpose.</p>",0,1,2021-12-16 15:15:20.257000 UTC,,,0,spark-streaming|publish-subscribe|azure-databricks,21,2016-04-01 15:47:35.593000 UTC,2022-03-05 10:32:03.447000 UTC,"Florence, Metropolitan City of Florence, Italy",747,319,7,259,,,,,,[]
Databricks Data Type Conversion error double to decimal,"<p>In databricks while writing data to curated layer, see error - Failed to execute user defined function (Double =&gt; decimal(38,18)). Does anyone know if faced such issue and how to resolve it.</p>
<p>When we multiply 2 numbers; the result size could go to the tune of 2x (num1 size + num2 size). Before we are explicitly casting multiplication of these 2, the result is bigger than to be casted data type. Its something like (double (x,y) can't be converted to decimal (x,y-1)). If we decrease the data type size of the values to me multiplied before multiplication we are getting incorrect value due to precision loss.</p>",0,2,2021-12-09 18:26:39.617000 UTC,,,0,types|azure-databricks|date-conversion,31,2021-10-28 17:27:09.780000 UTC,2022-03-02 12:41:26.803000 UTC,,9,0,0,3,,,,,,[]
Facing issue while configuring Confluent kafka with Azure databricks,"<p>I am newbie  to Azure databrciks and this forum. I am actually carrying out exercise for steaming Confluent Kafka on Azure databricks. I am able to stream the content on spark. However, there is slight problem as I am exposing username and password in the program. I would rather passed this through the variable(or Azure key-vault). I have tried passing username and password using variable but this approach is not working. I am getting error saying that 'unable to create Kafka consumer'. Can you please let me know how I can proceed? I am using scala for this.</p>
<pre><code>val streamingInputDF = spark
.readStream
.format(&quot;kafka&quot;)
.option(&quot;kafka.bootstrap.servers&quot;, host)
.option(&quot;kafka.security.protocol&quot;, &quot;SASL_SSL&quot;)
.option(&quot;kafka.sasl.jaas.config&quot;, &quot;kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\&quot;AAAAAA\&quot; password=\BBBBB\&quot;;&quot;)
.option(&quot;kafka.ssl.endpoint.identification.algorithm&quot;, &quot;https&quot;)
.option(&quot;kafka.sasl.mechanism&quot;, &quot;PLAIN&quot;)
.option(&quot;startingOffsets&quot;, &quot;earliest&quot;)
.option(&quot;failOnDataLoss&quot;, &quot;false&quot;)
.option(&quot;subscribe&quot;, &quot;Test&quot;)
.load()
</code></pre>
<p>This is how I want to pass but not working :-</p>
<pre><code>val streamingInputDF = spark
.readStream
.format(&quot;kafka&quot;)
.option(&quot;kafka.bootstrap.servers&quot;, host)
.option(&quot;kafka.security.protocol&quot;, &quot;SASL_SSL&quot;)
.option(&quot;kafka.sasl.jaas.config&quot;, f&quot;kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=$username password=$password&quot;;&quot;)
.option(&quot;kafka.ssl.endpoint.identification.algorithm&quot;, &quot;https&quot;)
.option(&quot;kafka.sasl.mechanism&quot;, &quot;PLAIN&quot;)
.option(&quot;startingOffsets&quot;, &quot;earliest&quot;)
.option(&quot;failOnDataLoss&quot;, &quot;false&quot;)
.option(&quot;subscribe&quot;, &quot;streaming_test_6&quot;)
.load()
</code></pre>",1,2,2022-02-21 15:08:36.440000 UTC,,,0,azure|azure-databricks,35,2022-02-21 14:53:05.897000 UTC,2022-03-05 14:44:36.870000 UTC,,1,0,0,0,,,,,,[]
Issue while writing data from databricks to Azure DW (synapse),"<p>Trying to write data into SQl DW through databricks stream data frame. process is trying to delete the temp folder in the BLOB storage and throwing below. In the documentation i see that process will not automatically cleanup tempdir. Is it true? if true, then why is this error? Using below query in python</p>

<pre><code>df1.writeStream
.format(""com.databricks.spark.sqldw"")
.option(""url"", sqlDwUrlSmall)
.option(""tempDir"", tempDir)
.option(""forwardSparkAzureStorageCredentials"", ""true"")
.option(""dbTable"", ""SampleTable"")
.option(""checkpointLocation"", ""/tmp_checkpoint_location1"")
.option(""numStreamingTempDirsToKeep"", -1)
.start()
</code></pre>

<p>ERROR AzureNativeFileSystemStore: Encountered Storage Exception for delete on Blob: <a href=""https://savupputest1.blob.core.windows.net/container1/tempDirs/2019-12-20/21-27-29-347/adca2ed6-a705-4274-8c24-0f0e3d7c64a7/batch0"" rel=""nofollow noreferrer"">https://savupputest1.blob.core.windows.net/container1/tempDirs/2019-12-20/21-27-29-347/adca2ed6-a705-4274-8c24-0f0e3d7c64a7/batch0</a>, Exception Details: This operation is not permitted on a non-empty directory. Error Code: DirectoryIsNotEmpty
19/12/20 21:27:32 ERROR AzureNativeFileSystemStore: Failed while attempting to delete key tempDirs/2019-12-20/21-27-29-347/adca2ed6-a705-4274-8c24-0f0e3d7c64a7/batch0</p>",1,0,2019-12-23 21:09:42.997000 UTC,,,0,azure-databricks|azure-sql-data-warehouse,691,2016-10-04 03:15:54.543000 UTC,2020-03-16 19:16:36.700000 UTC,,11,0,0,14,,,,,,[]
Writing a dataframe to a SQL database without replacing table format,"<p>I'm using Azure Databricks and pyspark to process data using dataframes and I use Azure SQL Database to store the data after it's been processed. I have created the output tables using ordinary CREATE TABLE scripts in SQL, but I realized that the dataframe write method overwrites the table format. E.g. all the string columns become nvarchar(max). Is there any way to keep the table format as specified in the CREATE TABLE script?</p>

<p>Example of my write statement in pyspark:</p>

<pre><code>df.write
  .mode(""overwrite"")
  .format(""jdbc"")
  .option(""url"", f""jdbc:sqlserver://myserver.database.windows.net;databaseName=mydatabase;"")
  .option(""dbtable"", ""mytable"")
  .option(""user"", jdbcUsername)
  .option(""password"", jdbcPassword)
  .option(""driver"", ""com.microsoft.sqlserver.jdbc.SQLServerDriver"")
  .save()
</code></pre>",1,2,2020-05-24 08:16:10.310000 UTC,,2020-05-25 05:47:18.603000 UTC,0,pyspark|apache-spark-sql|azure-databricks,463,2016-11-17 12:07:06.017000 UTC,2022-03-02 08:57:41.710000 UTC,,390,9,0,73,,,,,,[]
How do I efficiently migrate MongoDB to azure CosmosDB with the help of azure Databricks?,"<p>While searching for a service to migrate our on-premise MongoDB to Azure CosmosDB with Mongo API, We came across the service called, Azure Data Bricks. We have total of 186GB of data. which we need to migrate to CosmosDB with less downtime as possible. How can we improve the data transfer rate for that. If someone can give some insights to this spark based PaaS provided by Azure, It will be very much helpful.
Thank you</p>",1,0,2021-10-21 06:10:03.787000 UTC,1.0,2021-10-21 06:21:43.943000 UTC,1,mongodb|azure|azure-cosmosdb|database-migration|azure-databricks,43,2021-10-21 06:07:31.407000 UTC,2022-03-05 04:24:10.413000 UTC,,21,1,0,10,,,,,,[]
Migrate ADF - Datasets which are linked with Linked Services and Pipelines to Synapse Analytics,"<p>We Need to migrate Datasets from ADF which are linked with Linked Services and Pipelines only to Synapse Analytics.</p>
<p>The GITHUB solution (from previous posts <a href=""https://docs.microsoft.com/en-us/answers/questions/533505/import-bulk-pipelines-from-azure-data-factory-to-a.html"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/answers/questions/533505/import-bulk-pipelines-from-azure-data-factory-to-a.html</a>)
migrates entire all datasets, pipelines, linked services from ADF to Synapse Analytics.</p>
<p>But we need to migrate Datasets, linked services and pipelines which are linked each other and don't need to migrate which were not linked.</p>",1,0,2021-09-24 18:11:17.700000 UTC,,2021-09-24 20:54:19.077000 UTC,0,azure-data-factory-2|azure-databricks|azure-synapse,271,2021-09-24 14:56:55.063000 UTC,2022-01-18 23:58:32.627000 UTC,,1,0,0,1,,,,,,[]
Using databricks for twtter sentiment analysis - issue running the official tutorial,"<p>I am starting to use Databricks and tried to implement one of the official tutorials (<a href=""https://docs.microsoft.com/en-gb/azure/azure-databricks/databricks-sentiment-analysis-cognitive-services"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-gb/azure/azure-databricks/databricks-sentiment-analysis-cognitive-services</a>) from the website. However, I run into an issue - not even sure if I can call it an issue - when I run the second notebook (analysetweetsfromeventhub) then all commands (2nd, 3rd, 4th ...) are officially waiting to run, but never run. See the picture. Any idea what might be? Thanks.
<a href=""https://i.stack.imgur.com/JQTaB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JQTaB.png"" alt=""enter image description here""></a></p>",1,0,2020-04-20 17:24:45.873000 UTC,1.0,,-1,apache-spark|twitter|azure-eventhub|azure-databricks,156,2020-02-04 20:50:52.963000 UTC,2021-11-15 15:31:37.473000 UTC,,29,0,0,7,,,,,,[]
How to pass data generated by a Databricks notebook to a Python step?,"<p>I am building an Azure Data Factory v2, which comprises</p>

<ul>
<li>A Databricks step to query large tables from Azure Blob storage and generate a tabular result <code>intermediate_table</code>;</li>
<li>A Python step (which does several things and would be cumbersome to put in a single notebook) to read the <code>processed_table</code> and generate the final output.</li>
</ul>

<p>And looks like this</p>

<p><a href=""https://i.stack.imgur.com/g0CW8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g0CW8.png"" alt=""enter image description here""></a></p>

<p>The notebook generates a <code>pyspark.sql.dataframe.DataFrame</code> which I tried to save into parquet format with attempts like</p>

<pre><code>processed_table.write.format(""parquet"").saveAsTable(""intermediate_table"", mode='overwrite')
</code></pre>

<p>or </p>

<pre><code>processed_table.write.parquet(""intermediate_table"", mode='overwrite')
</code></pre>

<p>Now, I would like the Python step to re-read the intermediate result, ideally with a <code>postprocess.py</code> file with a syntax like</p>

<pre><code>import pandas as pd
intermediate = pd.read_parquet(""intermediate_table"")
</code></pre>

<p>after having installed <code>fastparquet</code> inside my Databricks cluster.<br>
This is (not surprisingly...) failing with errors like</p>

<blockquote>
  <p>FileNotFoundError: [Errno 2] No such file or directory:
  './my_processed_table'</p>
</blockquote>

<p>I assume the file is not found because the Python file is not accessing the data in the right context/path.</p>

<p>How should I amend the code above, and what would be the best/canonical ways to pass data across such steps in a pipeline? (any other advice on common/best practices to do this are welcome)</p>",1,3,2019-07-15 13:20:01.397000 UTC,,2019-07-15 17:22:16.893000 UTC,0,python|pyspark|azure-data-factory-2|azure-databricks|fastparquet,851,2014-11-11 16:17:30.717000 UTC,2022-03-05 21:49:17.573000 UTC,"Verona, VR, Italy",4399,347,66,641,,,,,,[]
"Python/AWS Neptune getting ""tornado.httpclient.HTTPError: HTTP 403: Forbidden""","<p>So I'm using a bastion host/SSH tunnel to connect from my local computer to AWS Neptune.</p>
<pre><code>ssh -N -i /Users/user1/.ssh/id_rsa -L 8182:my.xxx.us-east-1.neptune.amazonaws.com:8182 user1@transporter-int.mycloud.com
</code></pre>
<p>I did a simple Neptune connection test with gremlin.</p>
<pre><code>from gremlin_python.process.graph_traversal import __
from gremlin_python.structure.graph import Graph
from gremlin_python.process.strategies import *
from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection
from gremlin_python.process.traversal import T

graph = Graph()

wss = 'wss://{}:{}/gremlin'.format('localhost', 8182)
remoteConn = DriverRemoteConnection(wss, 'g')
g = graph.traversal().withRemote(remoteConn)

print(g.V().limit(2).toList())
remoteConn.close()
</code></pre>
<p>And getting this error:</p>
<pre><code>*aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host 
localhost:8182 ssl:True [SSLCertVerificationError: (1, &quot;[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'localhost'. (_ssl.c:1124)&quot;)]*
</code></pre>
<p>With @Taylor Riggan's suggestion, I update the /etc/hosts on my mac to the following:</p>
<p>Switched to use <strong>Python version 3.6.12</strong>, and <strong>gremlin-python version 3.4.10</strong></p>
<pre><code>127.0.0.1 localhost my.cluster-xxx.us-east-1.neptune.amazonaws.com
</code></pre>
<p>ran the following command to flush the hosts setting</p>
<pre><code>sudo dscacheutil -flushcache
</code></pre>
<p>updated this line in the source code</p>
<pre><code>wss = 'wss://{}:{}/gremlin'.format('my.cluster-xxx.us-east-1.neptune.amazonaws.com', 8182). 
</code></pre>
<p>and now getting the following error, and the <strong>tornado version 4.5.3</strong></p>
<pre><code>  File &quot;/Users/user1/myproj/tests/graph/venv/lib/python3.6/site-packages/gremlin_python/driver/client.py&quot;, line 148, in submitAsync
    return conn.write(message)
  File &quot;/Users/user1/myproj/tests/graph/venv/lib/python3.6/site-packages/gremlin_python/driver/connection.py&quot;, line 55, in write
    self.connect()
  File &quot;/Users/user1/myproj/tests/graph/venv/lib/python3.6/site-packages/gremlin_python/driver/connection.py&quot;, line 45, in connect
    self._transport.connect(self._url, self._headers)
  File &quot;/Users/user1/myproj/tests/graph/venv/lib/python3.6/site-packages/gremlin_python/driver/tornado/transport.py&quot;, line 41, in connect
    lambda: websocket.websocket_connect(url, compression_options=self._compression_options))
  File &quot;/Users/user1/myproj/tests/graph/venv/lib/python3.6/site-packages/tornado/ioloop.py&quot;, line 576, in run_sync
    return future_cell[0].result()
tornado.httpclient.HTTPClientError: HTTP 403: Forbidden
</code></pre>",2,0,2021-07-02 17:42:15.037000 UTC,,2021-07-02 20:31:26.680000 UTC,2,python|amazon-web-services|ssl|gremlin|amazon-neptune,261,2012-02-03 16:20:42.710000 UTC,2022-03-04 16:51:03.243000 UTC,,5628,119,6,508,,,,,,[]
i want to use simba.spark.jdbc driver in sprint boot to connect to databricks with token,"<p>want to use  simba spark jdbc driver in spring boot to connect to data bricks with token
so that i can leverage the code over the &quot;JDBC&quot; boiler plate code and using row mapper and can fetch data from data bricks database , what will be user and password in case of connecting to data bricks
database using token as there is no user and password .and reference or code is welcome</p>",1,0,2022-02-28 07:45:35.887000 UTC,,,0,spring|spring-boot|azure-databricks|spring-jdbc,23,2022-02-18 04:53:29.010000 UTC,2022-03-02 07:09:52.240000 UTC,,11,0,0,0,,,,,,[]
AWS Lambda (NodeJS) > AWS Neptune = 403,"<p>Currently attempting to connect to Neptune via NodeJS Lambda.
The code works to the point of <code>getUrlAndHeaders</code> in both libraries and I am getting response back and a connection is created, however, on attempt to insert/select, I get the 403.</p>
<ul>
<li>There is a policy attached to the execution role, either &quot;neptune-db:*&quot; or &quot;neptune-db:connect&quot;, but neither work.</li>
<li>All the same subnets are being used as a temporary measure</li>
<li>The docs mention Neptune lives in EC2 instances, but not seeing any reference to them</li>
<li>Confirmed that there are policies attached to said execution role for ec2: CreateNetworkInterface,DescribeNetworkInterface,DeleteNetworkInterface</li>
</ul>
<p>What am I missing? I am working on testing other things in the process, but not gaining any traction.</p>
<p>Documentation:<BR>
<a href=""https://docs.aws.amazon.com/neptune/latest/userguide/iam-auth-policy.html"" rel=""nofollow noreferrer"">AWS Neptune - IAM Auth Policy</a><BR>
<a href=""https://docs.aws.amazon.com/neptune/latest/userguide/iam-auth-temporary-credentials.html"" rel=""nofollow noreferrer"">AWS Neptune - Temp Credentials</a><BR></p>
<p>Code being used/modeled after:<BR>
<a href=""https://docs.aws.amazon.com/neptune/latest/userguide/lambda-functions-examples.html"" rel=""nofollow noreferrer"">AWS Lambda Examples</a><BR>
<a href=""https://www.npmjs.com/package/gremlin-aws-sigv4?activeTab=readme"" rel=""nofollow noreferrer"">gremlin-aws-sigv4</a><BR></p>
<p>In Progress:<BR>
<a href=""https://stackoverflow.com/questions/41177965/aws-lambdathe-provided-execution-role-does-not-have-permissions-to-call-describ"">AWSLambdaVPCAccessExecutionRole - SF</a><BR>
<a href=""https://www.npmjs.com/package/@aws-sdk/client-neptune"" rel=""nofollow noreferrer"">@aws-sdk/client-neptune - NPM</a><BR></p>",0,12,2021-07-23 14:40:07.847000 UTC,,2021-07-23 14:50:28.950000 UTC,0,node.js|aws-lambda|gremlin|amazon-neptune,110,2020-03-22 17:13:51.963000 UTC,2022-03-04 21:15:00.600000 UTC,Earth,73,4,0,19,,,,,,[]
Build AWS authorization header of http REST API requests for Neptune,"<p>I need to call Neptune REST endpoint for a query with http. How can I build http request authorization with aws access key, secret id, region WITHOUT SDK?</p>",1,0,2020-09-18 13:23:10.757000 UTC,,2020-09-18 13:46:01.507000 UTC,0,amazon-web-services|go|authorization|amazon-neptune,293,2020-09-18 13:18:06.600000 UTC,2021-08-10 02:14:14.183000 UTC,,19,5,0,25,,,,,,[]
Spark load Z compressed file using Scala on Databricks,"<p>Is there a way to read a <code>.Z</code> (capital) file extension with <code>Spark</code> directly?</p>
<p>I know <code>Scala</code> with <code>spark</code> can read <code>gzip</code> files <code>(.gz)</code> directly, but when I try to load a compressed <code>Z</code> file <code>(.Z)</code> into a <code>Dataframe</code> it doesn’t work.</p>",1,1,2020-05-06 09:18:18.733000 UTC,1.0,2020-07-16 10:03:49.973000 UTC,2,scala|apache-spark|compression|azure-databricks,351,2015-10-02 07:41:28.307000 UTC,2022-01-19 06:29:48.733000 UTC,,57,10,0,22,,,,,,[]
Can a group count query fail due to Big Data ? Amazon Neptune Graph Databases,"<p>Can a group count query in Amazon Neptune or any Graph Databases fail due to Big Data ?</p>

<p>I mean if the counts exceeds the limits of the count datatype can there be a n overflow?</p>",1,0,2018-07-28 15:56:00.070000 UTC,,,0,graph|amazon-neptune,60,2012-11-30 10:23:15.713000 UTC,2020-05-20 18:20:17.160000 UTC,,691,4,0,61,,,,,,[]
Getting more data on Gremlin traversal than just node id?,"<p>I'm looking to get more data back in my jupyter visualization on Neptune than just Node ID</p>
<pre><code>g.V(&quot;specific-id&quot;).emit().repeat(both().simplePath()).dedup().out().path().by(T.id)
</code></pre>
<p>In particular, it would be nice to know the label as well and maybe any other information. How can I modify this above query to achieve that?</p>",1,0,2021-09-29 19:14:45.103000 UTC,,,0,gremlin|amazon-neptune,28,2020-08-20 21:22:57.010000 UTC,2022-03-05 23:21:22.593000 UTC,"Seattle, WA, USA",655,57,5,91,,,,,,[]
Databricks API 2.0 - Can't create KEYVAULT secrets scopes using SPN credentials,"<p>I want to create a Secret Scope via the Databricks REST API 2.0.</p>
<p><strong>When I use SPN for az login I have next error</strong> when run request /api/2.0/secrets/scopes/create</p>
<pre><code>{&quot;error_code&quot;:&quot;CUSTOMER_UNAUTHORIZED&quot;,&quot;message&quot;:&quot;Unable to grant read/list permission to Databricks service principal to KeyVault 'https://dtbrcks-kvxxx.vault.azure.net/': key not found: https://management.core.windows.net/&quot;}% 
</code></pre>
<p><strong>But when I use User login same code worked fine!</strong></p>
<p>SPN and User have same permissions on Databricks(Owner/Admin) and Keyvault (Owner)resources.</p>
<p><strong>What necessary for make this operation using SPN?</strong></p>
<p>For get access token I use commands</p>
<pre><code>az login --service-principal

access_token=$(az account get-access-token \
                   --resource 2ff814a6-3304-4ab8-85cb-cd0e6f879c1d \
                   --query &quot;accessToken&quot; \
                   --output tsv)
</code></pre>
<p>And next code for create Secret Scope with Azure Keyvault:</p>
<pre><code>curl -X POST \-H &quot;Authorization: Bearer $access_token&quot; \
-H 'Content-Type: application/json' \
-d '{&quot;scope&quot;:&quot;keyvault-scope&quot;,&quot;scope_backend_type&quot;:&quot;AZURE_KEYVAULT&quot;,&quot;backend_azure_keyvault&quot;:{&quot;resource_id&quot;:&quot;/subscriptions/$subid/resourceGroups/$rg/providers/Microsoft.KeyVault/vaults/$kvname&quot;,&quot;dns_name&quot;:&quot;$kv_url&quot;}}' \
&quot;$dtbrcks_url/api/2.0/secrets/scopes/create&quot;
</code></pre>",1,0,2020-11-18 12:44:25.817000 UTC,,,2,azure-devops|azure-active-directory|azure-keyvault|azure-databricks,718,2020-11-18 11:00:55.943000 UTC,2021-07-07 11:09:06.550000 UTC,,21,0,0,3,,,,,,[]
Large nested XML parsing using pyspark,"<p>I have large nested XML and need to parse into multiple CSV files and also maintain relationship with parent and child sections.</p>
<p>I am looking for 6 csv files from above xml, as it have 6 nested xml elements.</p>
<ol>
<li><p>SOURCE csv
Policyno from PolicyXML
..
...</p>
</li>
<li><p>PolicyXML csv file
Policyno - same as above
Sourcesystem etc</p>
</li>
<li><p>PolicyHolder csv file - if multiple policyholder tag are there
Policyno - same as above
policyholderId - 1 if only one policyholder exist, else increment by 1 and create new row
PolicyHolderType etc</p>
</li>
<li><p>SubjectOfInsurance csv file
PolicyNo - same as above
SOIReinsuranceInd
etc</p>
</li>
<li><p>SectionPurchased csv file
PolicyNo - same as above
SectionPurchasedId  - 1 if only one policyholder exist, else increment by 1 and create new row</p>
</li>
<li><p>TermApplicable csv file
PolicyNo - same as above
SectionPurchasedId  - same as above
TermApplicable  - 1 if only one policyholder exist, else increment by 1 and create new row
TermCode etc</p>
</li>
</ol>
<p>I need to get this using pyspark, appreciate any help and guidance on this please.</p>
<p><strong>Sample xml</strong></p>
<p>         TESTSOURCE      756      V02 R00         TEST12342132   GEEK      VAP        I      MR      Bloggs      Jo      1950-01-01T00:00:00                  N               A2002         0         GBP         375         0000000200+00001                     CM018            0            GBP            GBP            0                             CM024            0            GBP            GBP            0                              A2003         0         GBP         375         GBP         GBP         0         2003         0000000200+00001                     CM018            0            0                              CM03            0            0                       </p>",0,1,2021-09-13 14:00:25.000000 UTC,,2021-09-13 14:08:42.720000 UTC,0,pyspark|azure-databricks,25,2021-09-02 09:59:52.440000 UTC,2022-03-03 11:46:32.813000 UTC,,21,0,0,5,,,,,,[]
Does mercurial transfer full files or only diffs?,"<p>We have a 200mb file. We currently use rsync to transfer it between developers when it changes. If we include it as part of our mercurial repository, will mercurial only transfer the diff like rsync or will it transfer the full file when changed?</p>",2,1,2012-02-08 14:51:22.833000 UTC,0.0,2012-03-30 12:32:23.160000 UTC,2,mercurial|dvcs|rsync|pull,537,2009-08-17 15:30:24.850000 UTC,2022-03-05 06:22:48.273000 UTC,"Karachi, Sindh, Pakistan",19073,571,56,1483,,,,,,[]
git: how do I merge between branches while keeping some changesets exclusive to one branch?,"<p>There's a special place in hell for people who hardcode absolute paths and database credentials into multiple random places in web applications. Sadly, before they go to hell they're wreaking havoc on Earth. And we  have to deal with their code.</p>

<p>I have to perform a few small changes to one of such web applications. I create a new branch <code>features</code>, and perform a global find &amp; replace to update the paths and credentials to my local environment. I commit that. I also tag this as <code>local</code>.</p>

<p>I merrily leap into perilous hacking penitence, and after a perplexing hundred patches, I want to merge my <code>features</code> changes into the <code>master</code> branch, but I do not want the one <code>local</code> commit to be merged.</p>

<p>Onwards, I'll be merging back and forth between <code>master</code> and <code>features</code>, and I'd like <code>local</code> to stay put in <code>features</code>, and never ever show up in <code>master</code>.</p>

<p>Ideally, I'd like all this to happen magically, with as little funny parameters and whatnot as possible.</p>

<p>Is there a simple obvious way to do it that I'm missing?</p>

<p>I can think of a couple, but they all require me to <strong>remember</strong> that I don't want that commit. And that's definitely not my forte. Especially with such poorly hacked programs.</p>

<p>Failing that, I'm interested in more convoluted, manual-ish ways to handle the situation.</p>",9,3,2009-08-17 15:03:39.697000 UTC,9.0,2009-08-17 16:17:27.803000 UTC,14,git|version-control|branch|dvcs|merge,8702,2008-09-16 21:37:38.873000 UTC,2022-02-07 12:33:27.313000 UTC,Berlin,73453,339,57,2304,,,,,,[]
Prevent pull/push from an obsolete Git repo and redirect to a new one,"<p>We are moving a Git repository to a new server. After migration we could obviously just delete the old repository, so when people try to push or pull they get an error and look up the new repository URL on the wiki, but is it possible to instead prevent pull and push and show the new URL in the error message? </p>",1,0,2014-08-27 10:06:08.470000 UTC,1.0,,3,git|dvcs|trac,385,2008-09-15 18:03:06.930000 UTC,2022-03-05 22:09:12.173000 UTC,"Moscow, Russia",159462,37203,14,10799,,,,,,[]
Is there a way to connect the neptune database graph visualization to a custom portal?,<p>I want to create a website and send queries to neptune db. How do I go about embedding the graph that's generated as a result of that query to my website?</p>,1,3,2022-01-26 20:11:31.473000 UTC,,2022-02-01 15:43:25.657000 UTC,0,database|amazon-web-services|graph|amazon-neptune|graph-notebook,18,2018-09-23 00:42:18.703000 UTC,2022-03-01 20:41:41.600000 UTC,,31,0,0,7,,,,,,[]
Retrieving data from Neptune DB using SPARQL queries,"<p>I am trying to retrieve the data from Neptune DB by using SPARQL queries. I connected to the EC2 instance which has same VPC as Neptune from local Jupyter Notebook. But the query is not retrieving any data and stdout is empty, I'm not sure where I am going wrong. Any help would be greatly appreciated. Please find the query below.</p>
<pre><code>stdin, stdout, stderr = ssh.exec_command(
' curl https://.cluster-.us-east1.neptune.amazonaws.com:8182/sparql \-d &quot;query=PREFIX mol: &lt;http://www.semanticweb.org/25#&gt;\
   SELECT * WHERE {?s ?p ?o } LIMIT 1&quot; \ -H &quot;Accept: text/csv&quot;')
</code></pre>
<p>Thank you in Advance.</p>",1,2,2021-06-17 20:12:56.910000 UTC,,2021-11-23 23:20:54.810000 UTC,0,sparql|rdf|amazon-neptune,109,2021-03-12 21:52:37.840000 UTC,2021-12-15 04:09:28.197000 UTC,,23,1,0,3,,,,,,[]
Attach Databricks Pool to a Databricks Job cluster in Azure,"<p>Is there a way we can attach a Databricks pool to a Databricks job cluster? The reason why I’m asking this question is, I’ve tested a Databricks job cluster configured as a new linked service and whenever an ADF pipeline triggers the job, I see a new Job cluster gets spined-up for each activity within the pipeline and every time when a new job cluster is spined-up it takes additional 2-3 minutes to spin-up the cluster, install the required libraries and to download the DBR version.</p>
<p>I've almost 30 ADF Pipelines to trigger on daily basis and each pipeline has an average of 3 activities within the pipeline, so in-total 30X3X(2.5)= 225 mints(3.75 hours). If we take on an average 2.5 mints to spin-up the cluster, then I would be wasting 3.75 hours to just spin-up the job clusters. Can we avoid cluster spin-up time.</p>
<p>In the high concurrency cluster this is not an issue at all, only the initial(very first) pipeline would take time post that subsequent pipelines will run faster by using the existing running nodes from the high concurrency cluster.</p>
<p>Any pointers would help!</p>",1,0,2022-01-25 08:59:49.680000 UTC,,2022-01-25 10:02:35.457000 UTC,1,apache-spark|azure-data-factory|azure-databricks,97,2016-01-22 09:29:22.083000 UTC,2022-03-05 17:22:10.320000 UTC,"Bengaluru, India",475,44,0,111,,,,,,[]
How to read data from Databricks DBFS using Rest API in csv or Excel format?,"<p>I am using Databricks Rest API to read datasets stored on DFBS. The output is coming in 64bit encoded format and in json format.
I need the output in tabular format which is easy to read.</p>
<p>Output of Rest Api:
{
&quot;bytes_read&quot;: 4601,
&quot;data&quot;: &quot;U2VwYWxMZW5ndGgsU2VwYWxXaWR0aCxQZXRhbE&quot;
}</p>
<p>output needed:</p>
<p><a href=""https://i.stack.imgur.com/vXh7g.png"" rel=""nofollow noreferrer"">enter image description here</a></p>",1,3,2020-07-25 06:11:30.150000 UTC,,2020-07-27 05:01:19.823000 UTC,0,pyspark|azure-databricks,287,2020-07-24 16:21:59.200000 UTC,2021-05-20 11:33:37.590000 UTC,,11,0,0,0,,,,,,[]
"Error in event loop with Flask, gremlin python and uWSGI","<p>I'm running into a problem when using Flask with a gremlin database (it's an Amazon Neptune database) and using uWSGI. Everything works fine in my unit tests which use the <code>test_client</code> provided by Flask. However, in production we use uWSGI and there I get the following error:</p>
<pre><code>There is no current event loop in thread 'uWSGIWorker4Core1'.
</code></pre>
<p>My app code is creating a connection to the database before a request and assigning it to the Flask <code>g</code> object. During teardown, the database connection is removed. The error happens when the app is trying to close the connection.</p>
<pre><code>from flask import Flask, g
from gremlin_python.structure.graph import Graph
from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection
from gremlin_python.process.anonymous_traversal import traversal

app = Flask(__name__, instance_relative_config=True)

@app.before_request
def _db_connect():
    if not hasattr(g, 'graph_conn'):
        g.graph_conn = DriverRemoteConnection(app.config['DATABASE_HOST'],'g')
        g.gg = traversal().withRemote(g.graph_conn)

# This hook ensures that the connection is closed when we've finished
# processing the request.
@app.teardown_appcontext
def _db_close(exc):
    if hasattr(g, 'graph_conn'):
        g.graph_conn.close(). # &lt;- ERROR THROWN AT THIS LINE
        del g.graph_conn
</code></pre>
<p>the uWSGI config does use multiple threads:</p>
<pre><code>[uwsgi]
http = 0.0.0.0:3031
manage-script-name = true
module = dogmaserver:app
processes = 4
threads = 2
offload-threads = 2
stats = 0.0.0.0:9191
</code></pre>
<p>But my understanding of how Flask's <code>g</code> object worked would be that it is all on the same thread. Can anyone let me know what I'm missing?</p>
<p>I'm using Flask 1.0.2, gremlinpython 3.4.11 and uWSGI 2.0.17.1.</p>",1,0,2021-08-27 22:44:35.610000 UTC,,,1,flask|uwsgi|amazon-neptune|gremlinpython,72,2013-02-07 02:21:18.593000 UTC,2022-03-02 17:59:43.957000 UTC,,1494,595,3,98,,,,,,[]
"If a Databricks job takes more than 1 hour, it stops writing to ADLS","<p>If a Databricks job takes more than 1 hour, it stops writing to ADLS. 
This is probably because the access token expires.</p>

<p>This means that some large datasets can not be written to disk, because just the writing already takes more than an hour. In other cases, it is possible to finish the writing in one hour, if some computations are done first and only then followed by writing the final output to disk.</p>

<p>Any possible workaround for this will be appriciated</p>",0,2,2020-01-23 12:10:09.457000 UTC,,,0,apache-spark|azure-databricks,76,2011-04-14 09:55:25.240000 UTC,2022-02-17 13:58:53.667000 UTC,,399,20,0,117,,,,,,[]
I am working on a databricks notebook running with ADLSV2 using service principle id but receive the following error after mounting my drive,"<p>I am working on a databricks notebook running with ADLSV2 using service 
    priciple id but receive the following error after mounting my drive.</p>

<pre><code>StatusCode=403

StatusDescription=This request is not authorized to perform this operation using this permission.


    configs = {""dfs.adls.oauth2.access.token.provider.type"": 
    ""ClientCredential"",
    ""dfs.adls.oauth2.client.id"": ""78jkj56-2ght-2345-3453-b497jhgj7587"",
    ""dfs.adls.oauth2.credential"": dbutils.secrets.get(scope = 
    ""DBRScope"", key = ""AKVsecret""),
    ""dfs.adls.oauth2.refresh.url"": 
    ""https://login.microsoftonline.com/bdef8a20-aaac-4f80-b3a0- 
    d9a32f99fd33/oauth2/token""}

    dbutils.fs.mount(source = 
    ""adl://&lt;accountname&gt;.azuredatalakestore.net/tempfile"",mount_point = 
    ""/mnt/tempfile"",extra_configs = configs)


    %fs ls mnt/tempfile
</code></pre>",1,1,2019-07-31 18:54:12.253000 UTC,1.0,,0,azure-data-lake|azure-databricks|service-principal,58,2017-05-30 03:43:03.657000 UTC,2020-10-21 16:24:10.000000 UTC,"Gurgaon, Haryana, India",273,0,0,93,,,,,,[]
Databricks release pipeline error:##[warning][FILENAME] has an unknown extension - skipping file,"<p>I'm getting this error</p>
<pre><code>##[warning]File TUTM Distribution Application has an unknown extension - skipping file
</code></pre>
<p>when running the Databricks Notebooks deployment task to deploy a notebook on databricks via a devops release pipeline.</p>
<p>according to this doc: <a href=""https://lucavallarelli.altervista.org/blog/azure-databricks-devops/"" rel=""nofollow noreferrer"">https://lucavallarelli.altervista.org/blog/azure-databricks-devops/</a></p>
<blockquote>
<p>A Databricks archive notebook has the .dbc format, but when syncing the notebook with DevOps it will be a .py file with “###command” lines that indicates the new cell you would see within the Databricks UI</p>
</blockquote>
<p>and this databricks notebook is in python so the file should be .py. But I'm not sure how to declare it or if it does it automatically? Can someone clarify?</p>
<p><a href=""https://i.stack.imgur.com/n48dQ.png"" rel=""nofollow noreferrer"">Detailed error message</a></p>",0,1,2021-06-15 18:09:33.467000 UTC,,,0,python|devops|azure-databricks|azure-pipelines-release-pipeline|cicd,26,2018-06-15 21:06:58.027000 UTC,2021-09-03 03:59:21.210000 UTC,,5,0,0,20,,,,,,[]
Azure databricks cluster error in a spark job : ExecutorLostFailure,"<p>I was trying to train an xgboost model in pysaprk on Azure Databricks, this code was executing completely fine till yesterday but now I am getting this error:</p>
<p>Job aborted due to stage failure: Task 107 in stage 29437.0 failed 4 times, most recent failure: Lost task 107.3 in stage 29437.0 (TID 7682534, 10.139.64.64, executor 145): ExecutorLostFailure (executor 145 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 163728 ms</p>",1,0,2021-12-28 06:30:21.543000 UTC,,,0,pyspark|azure-databricks,74,2017-04-17 05:53:49.107000 UTC,2022-02-24 09:16:07.550000 UTC,"Kolkata, West Bengal, India",167,13,0,42,,,,,,[]
How to loop over every row of streaming query dataframe in pyspark,"<p>I Have a Streaming query as below picture, now for every row i need to loop over dataframe do some tranformation and save the result to adls. Can anyone help me how to loop over streaming df. I m struck.</p>
<p><a href=""https://i.stack.imgur.com/0KvLn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0KvLn.png"" alt=""enter image description here"" /></a></p>",1,2,2022-02-15 12:13:25.907000 UTC,,,0,pyspark|azure-databricks|azure-eventhub|azure-data-lake-gen2,34,2021-05-26 06:28:38.243000 UTC,2022-03-03 06:39:00.907000 UTC,"Hyderabad, Telangana, India",71,0,0,10,,,,,,[]
How can I make Jenkins jobs report all the changes since a given Git merge-base commit?,"<p>In my Git repository, I have a staging branch where I merge a collection of topic branches onto the latest HEAD of the master branch.</p>

<p>I have a Jenkins job that resets the staging branch to master, merges each relevant topic branch into staging, and runs tests on the resulting staging HEAD.</p>

<p>In the job results, Jenkins intends to report the source code changes since the previous job. But it seems to get confused by this pattern where the commit IDs differ so significantly between jobs.</p>

<p>I think I'd like Jenkins to report all the changes since a given Git merge-base in each job.</p>

<p>Is there an existing Jenkins plugin I could use to do that? How?</p>",0,0,2013-10-30 14:02:21.283000 UTC,,,2,git|jenkins|continuous-integration|dvcs,362,2009-09-04 19:44:20.303000 UTC,2022-03-04 15:28:52.273000 UTC,,15089,217,6,322,,,,,,[]
Databricks -Can we variablize the file name for while loading(mounting a file name),"<p>can we variablize as below if not how do we do it
<a href=""https://i.stack.imgur.com/m10mS.png"" rel=""nofollow noreferrer"">enter image description here</a></p>",2,2,2020-07-08 07:53:32.650000 UTC,,,0,python|azure-databricks|azure-notebooks,46,2020-02-03 14:06:11.353000 UTC,2021-07-16 06:58:41.060000 UTC,,49,0,0,31,,,,,,[]
"How do I fork a git tool and edit it for use with another DVCS, without confusing git users?","<p>I'm not a git user (gasp). I use darcs. My question is: What steps should I take to fork a git repo, host it on github, yet make it apparent that it is to be used with a darcs repo? I'm trying to port an Atom package (git-time-machine which depends on git-log-utils) to make it work with darcs. I want to reuse most of the Javascript in both of those projects and just rewrite some of the bash commands and rewrite the parsing methods that handle what those commands return. Everything else should be reusable as far as I can see. I don't want my fork(s) to confuse people though. Is there a convention I should follow?</p>

<p>Loose example of something I want to replace (line: 1):</p>

<pre><code>    cmd = ""git log"" + flags + "" "" + fileName;
    if (process.env.DEBUG === '1') {
      console.log('$ ' + cmd);
    }
    return ChildProcess.execSync(cmd, {
      stdio: 'pipe',
      cwd: directory
    }).toString();
</code></pre>

<p>with (this code WILL NOT work, it's a loose example):</p>

<pre><code>    cmd = ""darcs annotate"" + flags + "" "" + fileName;
    if (process.env.DEBUG === '1') {
      console.log('$ ' + cmd);
    }
    return ChildProcess.execSync(cmd, {
      stdio: 'pipe',
      cwd: directory
    }).toString();
</code></pre>",0,0,2016-03-31 09:18:49.290000 UTC,,2017-01-19 15:15:52.737000 UTC,2,javascript|git|atom-editor|dvcs|darcs,40,2013-02-22 17:46:05.600000 UTC,2022-03-04 21:31:13.217000 UTC,,108,1,0,11,,,,,,[]
"When to branch, tag & merge in Mercurial?","<p>When should one branch/tag in Mercurial (<code>hg</code>), both at the local repo level and at the centralized/originating repo (that you <code>hg clone</code>)? When should you merge (again at the local level and in the central repo)?</p>

<p>I come from a SVN background where branches were used for new features (""<em>feature branches</em>""), as well as ""<em>release branches</em>"". In the case of feature branches, a developer would create a branch if he/she knew a project was going to span multiple sprints/releases. The feature branch would then be merged back in once the developer was certain they'd be releasing the new feature during the given sprint. The release branch would then be created and deployed to QA and a staging environemnt for QAT/UAT testing respectively. Any bugs that arose during testing would be committed directly to the release branch. When the release branch was finally ready to be released, it would be tagged (for archival/record-keeping purposes) and then finally, after the release branch was deployed live, it would be merged back with trunk.</p>

<p>How does this process change with a DVCS such as <code>hg</code>?</p>",1,1,2014-07-02 02:15:26.760000 UTC,,,0,version-control|mercurial|dvcs|branching-and-merging,209,2011-08-12 15:08:13.160000 UTC,2014-12-24 16:06:51.757000 UTC,Uranus,52373,783,37,4060,,,,,,[]
Data model for Graph database AWS neptune,"<p>We are planning to migrate Data from AuroraDB to AWS neptune (property Graph). we have table in AuroraDB with 3.5 million records, and which has relationship with other five tables. if I have to create all the records as Vertices result in 3.5 million vertices and it has relationship with other five table, each record will have 5 Edges for 5 other tables, then the edges will result in  3.5 million (records in one table ) * 5 (each records have relationship with other 5 tables) results in above 10 million.</p>
<p>How we can reduce this Edges and is there anything i am doing wrong?</p>",1,4,2021-09-01 11:18:20.873000 UTC,,2021-09-01 14:01:32.980000 UTC,-1,amazon-web-services|amazon-neptune|property-graph,56,2021-09-01 07:52:55.400000 UTC,2021-10-25 12:04:47.287000 UTC,"Hyderabad, Telangana, India",11,0,0,18,,,,,,[]
Does Spark SQL supports collation as it does in sql Server,"<p>In Sql server through this query <strong>select SERVERPROPERTY('COLLATION')</strong> we get to know collation property to be used.
Does the same supports in Spark SQl in databricks platform.
If it doesn't then what types can used to <strong>behave similar property</strong></p>",1,0,2020-06-30 07:31:41.883000 UTC,0.0,,1,apache-spark|azure-databricks,703,2019-07-30 14:49:54.870000 UTC,2021-10-21 10:16:48.650000 UTC,,51,0,0,1,,,,,,[]
"AWS Neptune access from Lambda - Error 429 ""Too Many Requests""","<p>I am working on an app that uses AWS Lambda which eventually updates Neptune.
I noticed, that in some cases I get a 429 Error from Neptune: Too Many Requests.
Well, as descriptive as it might sound, I would love to hear an advice on how to deal with it.
What would be the best way to handle that?</p>
<p>Although I am using a dead letter queue, I'd rather have it not going this road at the first place.</p>
<p>Btw the lambda is triggered by a SQS (standard) queue.</p>
<p>Any suggestions?</p>",0,2,2021-01-21 18:29:25.317000 UTC,,2021-03-07 19:22:44.063000 UTC,2,amazon-web-services|aws-lambda|gremlin|amazon-neptune,169,2015-03-19 13:01:49.903000 UTC,2022-03-05 10:58:03.623000 UTC,,665,20,0,45,,,,,,[]
Debugging ADF and Databrickswith display dataframes,"<p>I have ADF pipelines that calls Azure Databricks notebook. I want to call an ADF pipeline in normal mode(high performance) and then in debug mode.</p>
<p>When in debug mode, I want to display some DFs(Data frames) in databricks. But when run normally DFs should not displayed.</p>
<p>To achieve this I am thinking of sending parameters from ADF (debug=true) and let the display happen in an 'if' condition in databricks notebook. Is this the recommended approach or are there builtin functionlities in databricks or ADF?</p>",1,0,2021-04-06 14:41:29.257000 UTC,,,-1,azure-data-factory|azure-databricks,89,2011-09-20 13:28:23.627000 UTC,2022-02-23 16:04:51.277000 UTC,,5562,250,33,918,,,,,,[]
How to create log file (path sohuld be ADLS) in databricks pyspark,"<p>I have written a pyspark job in databricks. My objective is to save information, exception and errors into log file. This log file should be present in ADLS.</p>

<p>Can someone provide me guidance on this.</p>",1,1,2019-06-28 07:56:43.083000 UTC,,,0,azure|azure-data-lake|azure-databricks,1060,2014-09-21 11:19:48.290000 UTC,2021-12-28 03:32:53.317000 UTC,,283,19,0,39,,,,,,[]
spark.sql write to csv cause shifted column data issue when comma is there,"<p>I'm using scala as programming language in my azure databricks notebook, where my dataframe giving me accurate result, but when I'm trying to store the same in csv it shifting the cell where comma(,) is coming</p>
<pre><code>spark.sql(&quot;&quot;&quot;
  SELECT * FROM invalidData
  &quot;&quot;&quot;).coalesce(1)
      .write
      .option(&quot;header&quot;, &quot;true&quot;)
      .format(&quot;com.databricks.spark.csv&quot;)
      .mode(&quot;overwrite&quot;)
      .save(s&quot;$dbfsMountPoint/invalid/${fileName.replace(&quot;.xlsx&quot;, &quot;.csv&quot;)}&quot;)
</code></pre>
<p>Here one column having data like <strong>256GB SSD, Keyb.:</strong>, so while writing it using above function it show string after comma(,) in another cell.
Any spark inbuilt solution appriciated...</p>",1,1,2021-10-25 09:51:04.773000 UTC,,,0,scala|csv|azure-databricks|comma|spark-notebook,70,2019-01-29 09:12:34.020000 UTC,2022-02-25 09:41:27.740000 UTC,,177,17,0,48,,,,,,[]
Connect to Synapse from DataBricks using Service Principal,"<p>I am trying to connect from Databricks to Synapse using service principal.
I have configured the service principal in cluster configuration</p>
<pre><code>fs.azure.account.auth.type.&lt;datalake&gt;.dfs.core.windows.net OAuth
fs.azure.account.oauth.provider.type org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider
fs.azure.account.oauth2.client.id &lt;Service Principal ID/Application ID&gt;
fs.azure.account.oauth2.client.secret &lt;Client secret key/Service Principal Password&gt;
fs.azure.account.oauth2.client.endpoint https://login.microsoftonline.com/&lt;tenant-id&gt;/oauth2/token
fs.azure.createRemoteFileSystemDuringInitialization true
</code></pre>
<p>Whilst I can successfully connect to DataLake and work, i could not write to synapse, when I use the below command...</p>
<pre><code>DummyDF.write.format(&quot;com.databricks.spark.sqldw&quot;)\
.mode(&quot;append&quot;)\
.option(&quot;url&quot;, jdbcUrl)\
.option(&quot;useAzureMSI&quot;, &quot;true&quot;)\
.option(&quot;tempDir&quot;,tempdir)\
.option(&quot;dbTable&quot;, &quot;DummyTable&quot;).save()
</code></pre>
<p>I am getting the below error...</p>
<pre><code>Py4JJavaError: An error occurred while calling o831.save.
: com.databricks.spark.sqldw.SqlDWSideException: SQL DW failed to execute the JDBC query produced by the connector.
Underlying SQLException(s):
com.microsoft.sqlserver.jdbc.SQLServerException: External file access failed due to internal error: 'Error occurred while accessing HDFS: Java exception raised on call to HdfsBridge_IsDirExist. Java exception message:
HdfsBridge::isDirExist - Unexpected error encountered checking whether directory exists or not: AbfsRestOperationException: Operation failed: &quot;This request is not authorized to perform this operation using this permission.&quot;, 403, HEAD, https://datalakename.dfs.core.windows.net/temp/2020-06-24/14-21-57-819/88228292-9f00-4da0-b778-d3421ea4d2ec?upn=false&amp;timeout=90' [ErrorCode = 105019] [SQLState = S0001]
</code></pre>
<p>However i could write to Synapse using the below command...</p>
<pre><code>DummyDF.write.mode(&quot;append&quot;).jdbc(jdbcUrl,&quot;DummyTable&quot;)
</code></pre>
<p>I am not sure what is missing.</p>",1,0,2020-06-25 13:40:22.897000 UTC,,2020-06-28 23:55:41.760000 UTC,4,azure-databricks|azure-synapse|azure-service-principal,1877,2020-06-24 13:58:55.990000 UTC,2020-08-03 11:28:51.247000 UTC,,41,0,0,1,,,,,,[]
sql bulk insert never completes for 10 million records when using df.bulkCopyToSqlDB on databricks,"<p>I am reading 1 GB of CSV file ( record count : 10 million, columns : 13 ) and trying to dump it into the SQL server. Below are the infra details :</p>

<ul>
<li><p>CSV file location : azure blob storage </p></li>
<li><p>Code : Spark + Scala </p></li>
<li><p>Cluster : Databricks 
Size : 
<a href=""https://i.stack.imgur.com/usRFv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/usRFv.png"" alt=""enter image description here""></a></p></li>
<li><p>Code Used to read the file and dump it : </p>

<p>val df = spark.read.format(fileparser_config(""fileFormat"").as[String]).option(""header"", fileparser_config(""IsFirstRowHeader"").toString).load(fileparser_config(""FileName"").as[String]).withColumn(""_ID"", monotonically_increasing_id)</p>

<p>val bulkCopyConfig = Config(Map(
        ""url"" -> connConfig(""dataSource"").as[String],
        ""databaseName"" -> connConfig(""dbName"").as[String],
        ""user"" -> connConfig(""userName"").as[String],
        ""password"" -> connConfig(""password"").as[String],
        ""dbTable"" -> tableName,
        ""bulkCopyBatchSize"" -> ""500000"",
        ""bulkCopyTableLock"" -> ""true"",
        ""bulkCopyTimeout"" -> ""600""))</p>

<p>println(s"" ${LocalDateTime.now()} ************ sql bulk insert start ************"")</p>

<p>df.bulkCopyToSqlDB(bulkCopyConfig)</p>

<p>println(s"" ${LocalDateTime.now()} ************ sql bulk insert end ************"")</p></li>
<li><p>Problem : </p></li>
</ul>

<p><strong>the cluster goes into a limbo and my job never completes. One time when it ran long enough it threw an error</strong> :</p>

<pre><code>org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 38.0 failed 4 times, most recent failure: Lost task 13.3 in stage 38.0 (TID 1532, 10.0.6.6, executor 4): com.microsoft.sqlserver.jdbc.SQLServerException: The connection is closed.\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDriverError(SQLServerException.java:227)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.checkClosed(SQLServerConnection.java:796)\n\tat com.microsoft.sqlserver.jdbc.SQLServ
</code></pre>

<ul>
<li>Cluster Event logs :</li>
</ul>

<p><a href=""https://i.stack.imgur.com/YDYG4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YDYG4.png"" alt=""enter image description here""></a></p>

<ul>
<li><p>Other observations :</p>

<ol>
<li>While the job runs for a very long time, the cluster is not completely unresponsive. I tried this by submitting more jobs in that same window. The job ran but took comparatively more time than usual( around 10x time)</li>
<li>I tried increasing the worker nodes and the node type ( even chose 128 GB nodes ) but still the outcome was same.</li>
<li>While the job was running, I tried checking the SQL table row count with nolock query. I ran this after 3-4 minutes while the job was running, it gave me around 2 million records in the table. But when I ran it again after 10 minutes, the query kept running forever and never returned any records.</li>
<li>I have tried tweaking the bulkCopyBatchSize property but it hasnt helped much.</li>
<li>I have tried to remove the sqlinsertion code and used an aggregation operation on the dataframe that i create from 1 GB file and the entire thing takes only 40-50 seconds, so the problem is only with sql driver/sql server.</li>
</ol></li>
</ul>",1,0,2019-05-14 08:25:38.567000 UTC,1.0,2019-05-17 07:51:27.433000 UTC,1,scala|apache-spark|apache-spark-sql|bulkinsert|azure-databricks,1352,2013-04-02 15:42:09.283000 UTC,2022-02-25 14:09:08.733000 UTC,,516,19,1,146,,,,,,[]
Python ThreadPoolExecutor not raising errors,"<p>What I am trying to do, is to run some databricks notebooks in parallel. However, I do need a mechanism in my parallelnotebooks function (that I can switch off), to raise an error if any of the notebook fails(after all of the notebooks were executed). Hence, I have created this <code>raise_on_failure</code> decorator.</p>
<p>However, what I am getting instead is this futures object:
<a href=""https://i.stack.imgur.com/1Ti7D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1Ti7D.png"" alt=""enter image description here"" /></a></p>
<p>I fail to understand, why my decorator doesn't work in this case. Below the code:</p>
<pre><code>class NotebookData:
  def __init__(self, path, timeout = 0, parameters = None, retry = 0):
    self.path = path
    self.timeout = timeout
    self.parameters = parameters
    self.retry = retry

def submitNotebook(notebook, keyVault):
  print(&quot;Running notebook %s&quot; % notebook.path + &quot;\r&quot;)
  try:
    start = time.time()
    if (notebook.parameters):
      dbutils.notebook.run(notebook.path, notebook.timeout, notebook.parameters)
    else:
      dbutils.notebook.run(notebook.path, notebook.timeout)
    end = time.time()
    _PARALLEL_EVENTS.append([notebook.path, start, end])
    print(&quot;Passed: notebook %s&quot; % notebook.path)
    return (notebook.path, &quot;Passed&quot;)
  except Exception:
    if notebook.retry &lt; 1:
      print(&quot;Failed: notebook %s&quot; % notebook.path)
      return (notebook.path, &quot;Failed&quot;)
    print(&quot;Retrying: notebook %s&quot; % notebook.path)
    notebook.retry = notebook.retry - 1
    submitNotebook(notebook, keyVault)

def raise_on_failure(func):
    def wrapper(*args, **kwargs):
        _notebook, status = func(*args, **kwargs)
        if status == &quot;Failed&quot;:
            raise
        else:
            return func(*args, **kwargs)
    return wrapper

def parallelNotebooks(notebooks, numInParallel, keyVault, raise_errors=True):
  with ThreadPoolExecutor(max_workers = numInParallel) as ec:
     if raise_errors:
        return [ec.submit(raise_on_failure(submitNotebook), notebook, keyVault) for notebook in notebooks]
    else:
        return [ec.submit(submitNotebook, notebook, keyVault) for notebook in notebooks]
</code></pre>",0,0,2021-10-26 18:20:57.357000 UTC,,,0,python|azure-databricks,49,2017-11-20 18:20:15.443000 UTC,2022-03-05 13:03:45.133000 UTC,,473,29,0,106,,,,,,[]
Gremlin throwing an error while executing the lambda,"<pre><code>g.V().hasLabel(""OperatingSystem"")
            .filter(Lambda.predicate(""{it.get().property('name').startsWith('xyz')}"")).out(""dpend_on"")
            .as(""ast"").out(""depend_on"").hasLabel(""abc"")
</code></pre>

<p>results in an error:</p>

<blockquote>
  <p>java.util.concurrent.CompletionException:<br>
  org.apache.tinkerpop.gremlin.driver.exception.ResponseException: Query parsing failed at line 1, character position at 0, error message : mismatched input '[' expecting {'''', '""""', 'g'}  </p>
  
  <p>at java.util.concurrent.CompletableFuture.reportJoin(CompletableFuture.java:375)<br>
  at java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1934)<br>
  at org.apache.tinkerpop.gremlin.driver.ResultSet.one(ResultSet.java:107)<br>
  at org.apache.tinkerpop.gremlin.driver.ResultSet$1.hasNext(ResultSet.java:159)<br>
  at org.apache.tinkerpop.gremlin.driver.ResultSet$1.next(ResultSet.java:166)<br>
  at org.apache.tinkerpop.gremlin.driver.ResultSet$1.next(ResultSet.java:153)<br>
  at org.apache.tinkerpop.gremlin.driver.remote.DriverRemoteTraversal$TraverserIterator.next(DriverRemoteTraversal.java:142)<br>
  at org.apache.tinkerpop.gremlin.driver.remote.DriverRemoteTraversal$TraverserIterator.next(DriverRemoteTraversal.java:127)<br>
  at org.apache.tinkerpop.gremlin.driver.remote.DriverRemoteTraversal.nextTraverser(DriverRemoteTraversal.java:108)<br>
  at org.apache.tinkerpop.gremlin.process.remote.traversal.step.map.RemoteStep.processNextStart(RemoteStep.java:80)<br>
  at org.apache.tinkerpop.gremlin.process.traversal.step.util.AbstractStep.next(AbstractStep.java:128)<br>
  at org.apache.tinkerpop.gremlin.process.traversal.step.util.AbstractStep.next(AbstractStep.java:38)<br>
  at org.apache.tinkerpop.gremlin.process.traversal.util.DefaultTraversal.next(DefaultTraversal.java:200)  </p>
</blockquote>",1,5,2018-07-01 07:36:41.693000 UTC,,2018-07-01 07:58:22.920000 UTC,0,gremlin|amazon-neptune,783,2013-07-25 08:12:20.183000 UTC,2022-02-08 15:52:29.637000 UTC,"Pune, Maharashtra, India",279,1,0,57,,,,,,[]
Is there a way to recover deleted data in Azure Databricks?,"<p>With out realizing shift+enter runs a cell.
I was writing a delete from table and pressed shift enter which deleted all of the data in table.</p>",2,1,2020-08-01 00:30:04.217000 UTC,,,2,azure-databricks,1736,2019-01-29 19:11:36.057000 UTC,2020-08-04 19:45:22.317000 UTC,,21,0,0,6,,,,,,[]
Gremlin OLAP queries on AWS Neptune,"<p>In the AWS Neptune documentation it says it is Apache TinkerPop Gremlin compatible but it only refers to online transaction processing (OLTP) type of graph traversal queries. I haven't seen anything about long-running online analytical processing (OLAP) <a href=""http://tinkerpop.apache.org/docs/current/reference/#graphcomputer"" rel=""nofollow noreferrer"">GraphComputer</a> queries. </p>

<p>Is it possible to execute OLAP queries on  graphs stored in AWS Neptune graph database service? </p>",1,0,2017-12-20 21:58:46.937000 UTC,,2018-01-23 19:43:49.240000 UTC,6,amazon-web-services|graph|olap|gremlin|amazon-neptune,1023,2010-03-06 20:48:13.213000 UTC,2022-03-03 09:04:27.407000 UTC,,4695,164,5,215,,,,,,[]
How do I view list of commit messages between two branches,"<p>There is a repository located at:</p>

<p><a href=""http://hg.tryton.org/trytond"" rel=""nofollow"">http://hg.tryton.org/trytond</a></p>

<p>How do I view list of changes (as commits and messages) between branches 2.8 and 3.0?</p>

<p>I tried</p>

<pre><code>$ hg log -r ""2.8:3.0"" --template '{node|short} {desc|strip|firstline}\n' 
e1e5cf5700d0 Increase version number
faa26bc2e908 Added tag 2.8.12 for changeset a81a53304344
a81a53304344 Prepare release 2.8.12
b57e24462eb1 Increase version number
</code></pre>

<p>But it lists not all changes made between 2.8 and 3.0</p>",2,0,2014-11-11 08:18:02.103000 UTC,,2014-11-11 08:45:59.853000 UTC,0,version-control|mercurial|dvcs,121,2011-08-12 16:51:12.937000 UTC,2021-01-08 13:30:23.557000 UTC,,2460,39,0,173,,,,,,[]
Unable to connect to Azure Cosmos Db using mongo db api,"<p>I am trying to connect to azure cosmos db using mongo db api (spark mongo db connector )to export data to hdfs but I get the below exception:</p>

<p>Below is the complete stacktrace:</p>

<pre><code>{ ""_t"" : ""OKMongoResponse"", ""ok"" : 0, ""code"" : 115, ""errmsg"" : ""Command is not supported"", ""$err"" : ""Command is not supported"" }
at com.mongodb.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:115)
at com.mongodb.connection.CommandProtocol.execute(CommandProtocol.java:107)
at com.mongodb.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:159)
at com.mongodb.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:289)
at com.mongodb.connection.DefaultServerConnection.command(DefaultServerConnection.java:176)
at com.mongodb.operation.CommandOperationHelper.executeWrappedCommandProtocol(CommandOperationHelper.java:216)
at com.mongodb.operation.CommandOperationHelper.executeWrappedCommandProtocol(CommandOperationHelper.java:187)
at com.mongodb.operation.CommandOperationHelper.executeWrappedCommandProtocol(CommandOperationHelper.java:179)
at com.mongodb.operation.CommandOperationHelper.executeWrappedCommandProtocol(CommandOperationHelper.java:92)
at com.mongodb.operation.CommandOperationHelper.executeWrappedCommandProtocol(CommandOperationHelper.java:85)
at com.mongodb.operation.CommandReadOperation.execute(CommandReadOperation.java:55)
at com.mongodb.Mongo.execute(Mongo.java:810)
at com.mongodb.Mongo$2.execute(Mongo.java:797)
at com.mongodb.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:137)
at com.mongodb.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:131)
at com.mongodb.spark.rdd.partitioner.MongoSplitVectorPartitioner$$anonfun$partitions$2$$anonfun$4.apply(MongoSplitVectorPartitioner.scala:76)
at com.mongodb.spark.rdd.partitioner.MongoSplitVectorPartitioner$$anonfun$partitions$2$$anonfun$4.apply(MongoSplitVectorPartitioner.scala:76)
at scala.util.Try$.apply(Try.scala:192)
at com.mongodb.spark.rdd.partitioner.MongoSplitVectorPartitioner$$anonfun$partitions$2.apply(MongoSplitVectorPartitioner.scala:76)
at com.mongodb.spark.rdd.partitioner.MongoSplitVectorPartitioner$$anonfun$partitions$2.apply(MongoSplitVectorPartitioner.scala:75)
at com.mongodb.spark.MongoConnector$$anonfun$withDatabaseDo$1.apply(MongoConnector.scala:171)
at com.mongodb.spark.MongoConnector$$anonfun$withDatabaseDo$1.apply(MongoConnector.scala:171)
at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
at com.mongodb.spark.rdd.partitioner.MongoSplitVectorPartitioner.partitions(MongoSplitVectorPartitioner.scala:75)
at com.mongodb.spark.rdd.MongoRDD.getPartitions(MongoRDD.scala:137)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
at scala.Option.getOrElse(Option.scala:121)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
at scala.Option.getOrElse(Option.scala:121)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
at scala.Option.getOrElse(Option.scala:121)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
at scala.Option.getOrElse(Option.scala:121)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
at org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:182)
at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:636)
at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:691)
</code></pre>

<p>Maven dependency  added :</p>

<pre><code>&lt;dependency&gt;
        &lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt;
        &lt;artifactId&gt;mongo-spark-connector_2.11&lt;/artifactId&gt;
        &lt;version&gt;2.2.0&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p>Code :</p>

<pre><code>SparkSession spark = SparkSession.builder()
                .getOrCreate();

        jsc = new JavaSparkContext(spark.sparkContext());
        HiveContext hiveContext = new org.apache.spark.sql.hive.HiveContext(jsc);
        Dataset&lt;Row&gt; implicitDS = MongoSpark.load(jsc).toDF();
</code></pre>

<p>FYI : </p>

<p>implicitDS.count() gives 0</p>

<p>I am using MongoSplitVectorPartitioner. Updated with the complete stacktrace.</p>",0,12,2019-01-09 08:14:02.603000 UTC,1.0,2019-01-11 03:42:20.740000 UTC,2,apache-spark|azure-cosmosdb|azure-cosmosdb-mongoapi|azure-databricks,294,2018-11-15 12:28:27.787000 UTC,2019-09-23 14:42:37.007000 UTC,,41,1,0,28,,,,,,[]
Analyze Table Optimization in Databricks for Views,"<p>After doing Analyze Table Compute Statistics performance of my joins got better in Databricks Delta table.
As in Spark sql Analyze view is not supported. I would like to know if the query Optimizer will optimize the query if I have a view created on the same table on which I have used Analyze table compute statistics. </p>",1,1,2020-02-19 19:32:40.763000 UTC,,2020-02-19 19:40:09.193000 UTC,0,apache-spark|hive|apache-spark-sql|azure-databricks,357,2018-08-13 10:38:09.467000 UTC,2020-03-29 22:05:51.120000 UTC,"Bangalore, Karnataka, India",1,0,0,6,,,,,,[]
tipical way envoke azure databricks application via airfllow or user,"<p>if I have spark application (coded either in scala or python), and need to run on azure databricks, what is the typical way to envoke it either directly by useror via airflow?</p>
<p>When i worked on-premise hadoop mapR or cloudera cluster, for spark applications. We do:</p>
<ul>
<li>bash <strong>shell</strong> to envoke &quot;<strong>spark submit</strong>&quot; with the application <strong>JAR</strong>. User can <strong>ssh</strong> to an EDGE NODE to run the shell during manual testing</li>
<li>or oozie shell action to call the <strong>shell</strong> for production level scheduling.</li>
</ul>
<p>Currently, for cloude (azure databricks) environment, my new team uses</p>
<ul>
<li>airflow DAG (which specifies, 1. location of a databricks notebook and the notebook's input parameters, 2. the <strong>application JAR</strong> location, 3. a general purpose databricks cluster variable which was pre-created in databricks) and DatabricksSubmitRunOperator to envoke the databricks notebook.</li>
<li>or manual testing by a person: envoke the notebook on a general purpose cluster after attaching the applicaiton JAR to the cluster.</li>
</ul>
<p>My <strong>question</strong> is: is this the typical way to envoke the spark application in azure databricks?
is there any better way with examples?
also, at one stage, we tried to use <strong>job cluster</strong>, but when thare are multiple airflow tasks(spark jobs) in the DAG, each tasks creates a new job cluster, which takes more than 5 minutes to start. <strong>question for multiple job clusters for a DAG</strong> Is this a typical way? should the whole DAG share 1 job cluster somehow?</p>",0,0,2021-07-03 10:53:17.780000 UTC,,,0,airflow|azure-databricks,13,2015-02-19 04:10:53.350000 UTC,2022-02-21 22:58:16.290000 UTC,,868,922,19,379,,,,,,[]
How to get list of all git remotes present on my machine?,"<p>I have multiple apps configures on heroku and GitHub.</p>
<p>When I added heroku remotes, I used these aliases:</p>
<blockquote>
<p>app1</p>
<p>app1-staging</p>
<p>app2</p>
<p>app2-staging</p>
<p>... etc.</p>
</blockquote>
<p>Now, one of the remote staging name was somehow misspelled and I closed the terminal window.</p>
<p>Is there a way to list/find all heroku remotes I have configured on my system?</p>
<p><b>Note:</b> I tried <code>heroku apps</code> and it gives me apps name but not the remote names.</p>",1,0,2013-07-08 08:55:55.643000 UTC,2.0,2020-06-20 09:12:55.060000 UTC,7,git|heroku|dvcs,3772,2012-10-01 14:37:01.430000 UTC,2018-12-26 19:50:23.733000 UTC,,2950,127,6,550,,,,,,[]
"How to read a csv file from a ""File Share"" in an ADLS Gen2 Datalake inside Databricks using pyspark","<p>I have ADLS Gen2 Datalake with ""Blob Containers"" and ""File Shares"". I have mounted the Blob containers in my Databricks notebook, so I can read everything there inside my databricks notebooks.  </p>

<p>I also have some files in the ""File Share"", but I am not able to read these files into a dataframe thorugh Databricks using pyspark.  </p>

<p>I have created an Access Signature for the File share and I have got the url for one of the files inside the Share as well. That url works fine through Postman. I can download that file using the url.</p>

<p>The sample url is shown below:</p>

<pre><code>https://somedatalakename.file.core.windows.net/file_share_name/Data_20200330_1030.csv?sv=yyyy-mm-dd&amp;si=somename&amp;sr=s&amp;sig=somerandomsignature%3D
</code></pre>

<p>How to read the same csv, which is inside this file share, into a dataframe through databricks using pyspark? </p>

<p>I also tried </p>

<pre><code>from pyspark import SparkFiles
spark.sparkContext.addFile(uri)
call_df = spark.read.format(""csv"").option(""header"", ""true"").load(""file://"" + SparkFiles.get(""Data_"" + date_str + ""_1030.csv""))
</code></pre>

<p>And I get the below error:</p>

<pre><code>org.apache.spark.sql.AnalysisException: Path does not exist: file:/local_disk0/spark-ce42ed1b-5d82-4559-9000-d1bf3621539e/userFiles-eaf0fd36-68aa-409e-8610-a7909635b006/Data_20200330_1030.csv
</code></pre>

<p>Please give me some pointers on how to solve this problem. Thanks.</p>",1,1,2020-03-30 11:33:00.947000 UTC,,,0,pyspark|azure-databricks|azure-data-lake-gen2,999,2010-09-16 10:46:14.740000 UTC,2021-10-26 18:58:08.987000 UTC,South Africa,6053,95,5,794,,,,,,[]
Bulk insert PySpark Dataframe in Azure Synapse from Databricks Python Notebook,"<p>Usage-Batch</p>
<p>I have a PySpark Dataframe consisting of 750+ Columns and 2.5M records which is roughly around 6.5 GB. I am doing bulk insert (batch) from Databricks python notebook to Azure Synapse table.</p>
<p>Below is the sample code as per the Microsoft documentation (<a href=""https://docs.databricks.com/data/data-sources/azure/synapse-analytics.html"" rel=""nofollow noreferrer"">https://docs.databricks.com/data/data-sources/azure/synapse-analytics.html</a>)</p>
<pre><code>spark.conf.set(
  &quot;fs.azure.account.key.&lt;your-storage-account-name&gt;.blob.core.windows.net&quot;,
  &quot;&lt;your-storage-account-access-key&gt;&quot;)

df.write \
  .format(&quot;com.databricks.spark.sqldw&quot;) \
  .option(&quot;url&quot;, &quot;jdbc:sqlserver://&lt;the-rest-of-the-connection-string&gt;&quot;) \
  .option(&quot;forwardSparkAzureStorageCredentials&quot;, &quot;true&quot;) \
  .option(&quot;dbTable&quot;, &quot;my_table_in_dw_copy&quot;) \
  .option(&quot;tempDir&quot;, &quot;wasbs://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.blob.core.windows.net/&lt;your-directory-name&gt;&quot;) \
  .save()
</code></pre>
<p><strong>Problem Statement:
I need to implement the same for one more PySpark Dataframe which consists one column which contains more than 8000 characters as JSON. In the underlying Synapse table this column is of nvarhcar(max) type.
The above mentioned code doesn't work for columns having character length more than 4000 characters.</strong></p>
<p><em>Please help how to deal with this issue in the above code for this situation.</em></p>",0,3,2020-07-12 16:46:57.613000 UTC,,,1,python|pyspark|azure-databricks|pyspark-dataframes|azure-synapse,789,2013-04-10 10:06:52.820000 UTC,2022-03-02 11:17:54.347000 UTC,"New Delhi, India",291,14,0,75,,,,,,[]
Trying to iterate through files in a data lake and load them into a table in SQL Server,"<p>I'm using Azure Data Bricks and trying to think of a way to write PySpark code to iterate through patterns of letters and numbers.  I have a bunch of files that look like this:</p>
<pre><code>starts like this...
ABS0630.N.006134.gz
ABS0630.N.006135.gz

etc., etc., etc

ends like this...
ABS0630.J.000157.gz
ABS0630.J.000158.gz
</code></pre>
<p>The 'ABS' stays the same the the '0630' is June 30th.  Is there some brute force way to create a loop that iterates through patterns of letters and numbers and if a pattern matches the name of a file in our data lake, the file gets loaded into a table in SQL Server?  The table and related schema are all setup on the SQL Server side.  If I use ADF to load these one at a time, everything works fine.  I just can't figure out how to iterate through the files, and there are hundreds of files per day, sometimes over a thousand files per day.  The wildcard characters, like '*' and '?' don't work for some weird reason.  All I get is weird errors when I try to use the wildcard characters.</p>
<p>I am trying to used ADF to copy all these files into my SQL Server DB.</p>",0,8,2019-09-25 21:43:57.967000 UTC,,2020-11-24 00:26:46.090000 UTC,1,python-3.x|azure|azure-sql-database|azure-data-factory|azure-databricks,257,2015-08-10 20:17:19.013000 UTC,2022-03-04 03:19:47.247000 UTC,"New York City, NY, United States",17522,15535,12,2700,,,,,,[]
Databricks jdbc driver for Cubejs ( cube.dev),"<p>We are trying to connect cube olap engine to databricks sql using jdbc driver (<a href=""https://github.com/cube-js/cube.js/tree/master/packages/cubejs-databricks-jdbc-driver"" rel=""nofollow noreferrer"">https://github.com/cube-js/cube.js/tree/master/packages/cubejs-databricks-jdbc-driver</a>). We want to use Cube as olap layer for fast query and pre-aggregations.</p>
<p>However, it seems the jdbc connectoris not working. Has anyone seen this issue or tried connecting cube to databricks sql</p>",1,0,2022-02-01 15:27:55.797000 UTC,,,0,jdbc|cube|databricks-sql,21,2012-10-23 14:17:27.730000 UTC,2022-03-01 19:13:00.900000 UTC,"Bangalore, Karnataka, India",1550,77,1,223,,,,,,[]
"how to call function inside function and registering it in spark, Got error as PicklingError see SPARK-5063","<p>i'm trying to create a udf which contains other function, then i'm registering the function in spark to use that in select query. I need to pass the df(temp table) values as input to the function which is registered.
When tried executing the function getting error as follows</p>
<p>Error: <strong>PicklingError</strong>: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063</p>
<pre><code>def funcmain(a,b,c,lvl,x):
   lvl1=lvl
   x1=x
   while lvl1!=lv1:
      x1=x
   if x1!= something:
      x1= something
      func1(a,b,c)
   return x1
udf_fun = spark.register.udf(&quot;funcmain&quot;,funcmain)
</code></pre>",0,5,2021-02-24 14:57:00.630000 UTC,,,0,sql|pyspark|azure-databricks,150,2018-10-04 16:30:46.120000 UTC,2021-02-25 15:56:54.010000 UTC,,113,3,0,83,,,,,,[]
increase idle timeout for gremlin console AWS Neptune,"<p>I am on ubuntu 18, I have the gremlin console always opened, after I :remote console, I run some query, then if I keep idle for like 3 mins, the connection then got dropped, then I have to exit the current connection and reconnect, which is quite annoying.</p>

<p>Is there a way to increase the idle timeout</p>

<p>basically I need to type in these command again and again.....</p>

<pre><code>bin/gremlin.sh
:remote connect tinkerpop.server conf/neptune-remote.yaml
:remote console
</code></pre>",1,3,2020-01-22 10:47:41.610000 UTC,,,1,gremlin-server|amazon-neptune,352,2009-09-01 06:49:22.620000 UTC,2022-03-03 08:07:01.783000 UTC,"Shanghai, China",1317,18,2,72,,,,,,[]
Deserialize Avro Spark,"<p>I'm pushing a stream of data to Azure EventHub with the following code leveraging <code>Microsoft.Hadoop.Avro</code>.. this code runs every 5 seconds, and simply plops the same two Avro serialised items :</p>

<pre><code>  var strSchema = File.ReadAllText(""schema.json"");
  var avroSerializer = AvroSerializer.CreateGeneric(strSchema);
  var rootSchema = avroSerializer.WriterSchema as RecordSchema;

  var itemList = new List&lt;AvroRecord&gt;();

  dynamic record_one = new AvroRecord(rootSchema);
  record_one.FirstName = ""Some"";
  record_one.LastName = ""Guy"";
  itemList.Add(record_one);

  dynamic record_two = new AvroRecord(rootSchema);
  record_two.FirstName = ""A."";
  record_two.LastName = ""Person"";
  itemList.Add(record_two);

  using (var buffer = new MemoryStream())
  {
      using (var writer = AvroContainer.CreateGenericWriter(strSchema, buffer, Codec.Null))
      {
          using (var streamWriter = new SequentialWriter&lt;object&gt;(writer, itemList.Count))
          {
              foreach (var item in itemList)
              {
                  streamWriter.Write(item);
              }
          }
      }

      eventHubClient.SendAsync(new EventData(buffer.ToArray()));
  }
</code></pre>

<p>The schema used here is, again, v. simple:</p>

<pre><code>{
  ""type"": ""record"",
  ""name"": ""User"",
  ""namespace"": ""SerDes"",
  ""fields"": [
    {
      ""name"": ""FirstName"",
      ""type"": ""string""
    },
    {
      ""name"": ""LastName"",
      ""type"": ""string""
    }
  ]
}
</code></pre>

<p>I have validated this is all good, with a simple view in Azure Stream Analytics on the portal:</p>

<p><a href=""https://i.stack.imgur.com/ue2WD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ue2WD.png"" alt=""Stream Analytics Screenshot""></a></p>

<p>So far so good, but i cannot, for the life of me correctly deserialize this in Databricks leverage the <code>from_avro()</code> command under Scala..</p>

<p>Load (the exact same) schema as a string:</p>

<pre><code>val sampleJsonSchema = dbutils.fs.head(""/mnt/schemas/schema.json"")
</code></pre>

<p>Configure EventHub</p>

<pre><code>val connectionString = ConnectionStringBuilder(""&lt;CONNECTION_STRING&gt;"")
  .setEventHubName(""&lt;NAME_OF_EVENT_HUB&gt;"")
  .build

val eventHubsConf = EventHubsConf(connectionString).setStartingPosition(EventPosition.fromEndOfStream)
val eventhubs = spark.readStream.format(""eventhubs"").options(eventHubsConf.toMap).load()
</code></pre>

<p>Read the data..</p>

<pre><code>// this works, and i can see the serialised data
display(eventhubs.select($""body""))

// this fails, and with an exception: org.apache.spark.SparkException: Malformed records are detected in record parsing. Current parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.
display(eventhubs.select(from_avro($""body"", sampleJsonSchema)))
</code></pre>

<p>So essentially, what is going on here.. i am serialising the data with the same schema as deserializing, but something is malformed.. the documentation is incredibly sparse on this front (very very minimal on the Microsoft website).</p>",1,0,2019-11-07 16:36:22.250000 UTC,0.0,2020-02-03 13:35:09.110000 UTC,5,c#|apache-spark|azure-databricks|azure-stream-analytics|spark-avro,1749,2013-07-03 11:16:58.847000 UTC,2022-02-11 09:11:36.863000 UTC,"Amsterdam, NL",1257,115,2,190,,,,,,[]
Structured Streaming in Databricks Azure throwing exception - java.lang.IllegalStateException: Error reading delta file dbfs:/raw_zone/1.delta,"<p>We are using Structured Streaming in Databricks environment, Every time while we run this program - kAFKA - Structured Streaming (DBR6.6, Spark 2.4.5) - Writing to CosmosDB, we are getting the same exception as below just before we do the final joins to save the data to Cosmos DB. We haven't modified any spark specific settings and leveraging the default spark /DBR configurations.</p>
<pre><code>Caused by: org.apache.spark.SparkException:
           Job aborted due to stage failure:
           Task 174 in stage 9353.0 failed 4 times, most recent failure:
           Lost task 174.3 in stage 9353.0 (TID 60863, 10.139.64.9, executor 1): 
           java.lang.IllegalStateException:
           Error reading delta file dbfs:/raw_zone/uffRetail_jointbl_dev_cp1/state/8/174/left-keyToNumValues/1.delta of HDFSStateStoreProvider[id = (op=8,part=174),dir = dbfs:/raw_zone/uffRetail_jointbl_dev_cp1/state/8/174/left-keyToNumValues]: 
           dbfs:/raw_zone/uffRetail_jointbl_dev_cp1/state/8/174/left-keyToNumValues/1.delta does not exist
Caused by: java.io.FileNotFoundException:
           /6455647419774311/raw_zone/uffRetail_jointbl_dev_cp1/state/8/174/left-keyToNumValues/1.delta
</code></pre>",0,2,2020-11-24 09:27:58.527000 UTC,,2020-11-24 12:49:24.267000 UTC,0,apache-spark-sql|spark-streaming|spark-structured-streaming|azure-databricks|spark-checkpoint,81,2012-11-07 15:52:01.823000 UTC,2022-03-02 11:58:04.127000 UTC,India,41,0,0,16,,,,,,[]
How to ignore comments while reading an XML file in Pyspark Databricks?,"<p>I am trying to read an xml file in Azure Databricks Notebook in PySpark.
The problem is that my persons.xml has some comments in the beginning.
I just want to ignore them while reading the file.</p>
<pre><code>df = spark.read
      .format(&quot;com.databricks.spark.xml&quot;)
      .option(&quot;rowTag&quot;, &quot;person&quot;)
      .xml(&quot;src/main/resources/persons.xml&quot;)
</code></pre>
<p>My XML looks like this:</p>
<pre><code>        &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
    &lt;!-- 
&lt;top&gt;
       &lt;t1 attr1=&quot;a1&quot;&gt;
          &lt;!-- t1 comment --&gt;
          &lt;t2&gt;Something 1&lt;/t2&gt;
       &lt;/t1&gt;
       &lt;!-- between rows comment --&gt;
       &lt;t1 attr1=&quot;a2&quot;&gt;
          &lt;t2&gt;Something 2&lt;/t2&gt;
       &lt;/t1&gt;
    &lt;/top&gt; 
    --&gt; 
        &lt;naman&gt;
           &lt;t1 attr1=&quot;a1&quot;&gt;
              &lt;t2&gt;Something 1&lt;/t2&gt;
           &lt;/t1&gt;
           &lt;t1 attr1=&quot;a2&quot;&gt;
              &lt;t2&gt;Something 2&lt;/t2&gt;
           &lt;/t1&gt;
        &lt;/naman&gt;
</code></pre>",1,0,2021-11-26 10:26:56.770000 UTC,0.0,2021-11-29 08:10:46.933000 UTC,2,xml|apache-spark|pyspark|azure-databricks|apache-spark-xml,54,2020-09-15 13:20:23.047000 UTC,2022-02-21 12:46:36.677000 UTC,"Bengaluru, Karnataka, India",62,3,0,67,,,,,,[]
How to strip and rebuild an existing branch and push it to the main repository? (aka replace a branch with a new one),"<p>THE CONTEXT:</p>

<p>There are three branches: ""mine"" and ""yours"" and ""main"". I started with a clean clone and the only requirement is to make the ""mine"" branch like the ""main"" branch with only a few added changes. I do not want to alter the other branches. The current ""mine"" branch has only one changeset that I would prefer to delete and replace with a new changeset. </p>

<p>MY ATTEMPT:</p>

<p>""mine"" contains only one changeset, which I want to discard. So, I stripped this changeset(effectively deleting ""mine""), updated directory to ""main"" branch, recreated ""mine"" (""hg branch mine""), added my changes, committed, and now I want to push. The output of ""hg push -b mine"" is ""abort: push creates new remote head..."" and references my new changeset. However, I just want to replace my new changeset with my old (locally deleted) changeset. ""hg outgoing"" lists my new changeset, ""hg incoming"" lists my old changeset.</p>

<p>I think I am very close, any suggestions? Would a forced push work here? Alternative solutions and helpful references are welcome.</p>",1,0,2013-06-25 01:26:56.550000 UTC,,2013-06-25 21:43:08.350000 UTC,1,mercurial|branch|push|dvcs,455,2012-12-07 01:34:27.190000 UTC,2022-03-04 17:42:23.667000 UTC,Seattle,12447,1574,14,867,,,,,,[]
Spark cannot resolve column,"<p>While running below code getting the error..it is azure Data bricks hands on EDA.   </p>

<pre><code>df_typed = spark.sql(""SELECT cast(Price as int), 
   cast(Age as int), cast(KM as int), FuelType, 
   cast(HP as int), cast(MetColor as int), 
   cast(Automatic as int), cast(CC as int), 
   cast(Doors as int), cast(Weight as int) FROM usedcars_CSV"")

df_typed
</code></pre>

<p>Error:</p>

<pre><code>Py4JJavaError                             Traceback (most recent call last)
/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
     62         try:
---&gt; 63             return f(*a, **kw)
     64         except py4j.protocol.Py4JJavaError as e:

/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    327                     ""An error occurred while calling {0}{1}{2}.\n"".
--&gt; 328                     format(target_id, ""."", name), value)
    329             else:

Py4JJavaError: An error occurred while calling o204.sql.
: org.apache.spark.sql.AnalysisException: cannot resolve '`Price`' given input columns: [default.usedcars_csv._c3, default.usedcars_csv._c6, default.usedcars_csv._c5, default.usedcars_csv._c1, default.usedcars_csv._c7, default.usedcars_csv._c0, default.usedcars_csv._c9, default.usedcars_csv._c2, default.usedcars_csv._c4, default.usedcars_csv._c8]; line 1 pos 12;
'Project [unresolvedalias(cast('Price as int), None), unresolvedalias(cast('Age as int), None), unresolvedalias(cast('KM as int), None), 'FuelType, unresolvedalias(cast('HP as int), None), unresolvedalias(cast('MetColor as int), None), unresolvedalias(cast('Automatic as int), None), unresolvedalias(cast('CC as int), None), unresolvedalias(cast('Doors as int), None), unresolvedalias(cast('Weight as int), None)]
+- SubqueryAlias `default`.`usedcars_csv`
   +- Relation[_c0#347,_c1#348,_c2#349,_c3#350,_c4#351,_c5#352,_c6#353,_c7#354,_c8#355,_c9#356] csv

    at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:120)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)
    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:302)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)
    at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)
    at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)
    at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)
    at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)
</code></pre>",1,2,2020-01-20 14:47:35.860000 UTC,,2020-01-23 15:09:32.317000 UTC,-2,apache-spark|azure-databricks|py4j,1054,2020-01-20 14:40:32.170000 UTC,2020-01-23 13:16:40.040000 UTC,,1,0,0,0,,,,,,[]
Advantages of using container vs no container in sparql,"<p><strong>Normal Data Insertion</strong></p>

<pre><code>{
  :sub :pred 'o1'. 
  :sub :pred 'o2'.
  :sub :pred 'o3'. 
}
</code></pre>

<p><strong>Data insertion using container</strong></p>

<pre><code>{
  :sub :pred :_b. 
  :_b rdf:type rdf:Seq.
  :_b rdf:_1 'o1'. 
  :_b rdf:_2 'o2'.
  :_b rdf:_3 'o3'. 
}
</code></pre>

<p>When I used the basic select query, both the above insertion models were returning results in same order(o1, o2, o3). </p>

<p>What are the benefits we get by container approach(<code>rdf:Seq</code>)?</p>

<p>Is <code>rdf:Seq</code> just for representational purpose or does it perform anything under the hood(i.e, preserves insertion ordering) when queried upon?</p>

<p><strong>My Understanding:</strong> Even the container insertion model just works the same as the basic normal model when retrieved upon. So even with the container model, insertion order won't be guaranteed while retrieval. I really don't understand the significance of <code>rdf:Seq</code> (<a href=""https://www.w3.org/TR/rdf-schema/#ch_seq"" rel=""nofollow noreferrer"">docs link</a>)</p>",0,3,2020-04-08 11:06:55.810000 UTC,,,0,sparql|amazon-neptune|rdf4j,79,2017-04-06 18:53:11.143000 UTC,2021-12-10 06:59:36.113000 UTC,"Gurgaon, Haryana, India",302,31,0,52,,,,,,[]
"A DRCS (git, bzr, merc) that has ""pending changesets"" like perforce?","<p>We have perforce at work and one thing about it really seems to work quite well with the workflow and that is pending changesets.</p>

<p>I can make a new pending changeset, assign changes to it, assign jira tickets to it, etc...  I can shelve files there and make other changes in other changesets.  Then when it's checkin time I can push just those changes by simply providing the changeset ID.</p>

<p>Do any of the main distributed revision control systems offer anything like this?  I see that most offer a variant of shelving, but pending changesets?</p>

<p>Edit - an example to clarify:</p>

<p>I have a project with files A-Z.  I do an update right before making changes for a source change I'm making.  I notice that a change I'm not ready to adopt has been made so I create a hack around it by modifying A and B.  I then make changes to files C, D, and E.</p>

<p>I only want to check in the latter three and I want to attach the jira ticket I'm responding to.  In p4v this is as simple as creating a new ""pending changeset"" and checking off the files I want in it along with the ticket number.  Then I commit my pending changeset.  I can then keep my hack around for the next run or revert it.</p>",3,2,2012-12-08 21:09:17.170000 UTC,,2012-12-08 21:51:26.357000 UTC,2,git|mercurial|dvcs|bazaar,148,2010-03-25 16:45:10.317000 UTC,2022-03-05 21:10:42.833000 UTC,Gliese 581G,39539,570,636,5866,,,,,,[]
issue with Full-Text-Search Query Execution in Neptune with iam DB authorization enabled,"<p>I am trying to integrate ES with neptune for Full-Text-Search Query Execution. With IAM DB authorization enabled in neptune, it is not able to execute query and giving below error</p>
<p><strong>{&quot;detailedMessage&quot;:&quot;The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.&quot;,&quot;code&quot;:&quot;AccessDeniedException&quot;,&quot;requestId&quot;:&quot;b4bbe6b7-8361-5c9b-1584-ba7a6a64a03c&quot;}</strong></p>
<p>However with IAM DB authorization disabled, same query execution is working fine.</p>
<p>I am generating AWS SIG4 signature and utilising in the header as part of the query. With IAM enabled , i can able to connect(status check) to neptune DB with same sig4 signature.</p>",0,3,2021-02-23 06:34:26.080000 UTC,,2021-03-07 19:21:52.260000 UTC,0,amazon-web-services|elasticsearch|amazon-neptune,77,2021-02-11 11:52:26.593000 UTC,2021-07-16 10:45:48.563000 UTC,,23,0,0,3,,,,,,[]
Crucible and multiple mercurial clones handling,"<p>I was wondering if Crucible can handle the following scenario with Mercurial.
How do you use DVCSs with Crucible in such a scenario?</p>

<p>There are several issues in a project, for each issue a developer makes a clone of the project from repo ""stable-build"", to repo ""dev-0001"" (on a local sharing server). 
Clone is named according to the issue : ""dev-0001"" for example.</p>

<p>Now from there a developer clones on his local machine into clone ""local-dev-0001"", makes the changes and then pushes to ""dev-0001"".</p>

<p>Some other developer wants to review the changes in repo ""dev-0001"" before the dev that implemented 0001 can push to ""stable-build"".</p>

<p>What I tried is to set up Crucible for a repo (a separate test clone ""test-crucible"" directly from ""stable-build"". It took a loong time on a very power full machine, about 5 days.</p>

<p>My question is : how can Crucible and Mercurial be set up so that one can create reviews for the ""dev-0001"" clone befor eit is pushed to a somewhat central server, withouth waiting 5 days for Crucible to parse the ""dev-0001"" repo from the start, and maybe use the information of it's parent ? Is this already done does it need some sort of plugin?</p>

<p>I can offer more clarity for the scenario if that was a bit hazzy,
Thanks</p>",2,0,2012-02-07 10:58:00.877000 UTC,,2013-02-09 06:25:56.520000 UTC,0,mercurial|dvcs|atlassian-crucible,229,2009-06-06 16:05:54.757000 UTC,2022-01-14 20:27:17.303000 UTC,"Cluj-Napoca, Romania",1795,135,2,207,,,,,,[]
Databricks-connect: sparkContext.wholeTextFiles,"<p>I have setup databricks-connect version 5.5.0. This runtime includes Scala 2.11 and Spark 2.4.3. All the Spark code I have written has been correctly executed and without any issues until I tried calling <code>sparkContext.wholeTextFiles</code>. The error that I get is the following:</p>

<pre><code>Exception in thread ""main"" java.lang.NoClassDefFoundError: shaded/databricks/v20180920_b33d810/com/google/common/base/Preconditions
    at shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.ensureAuthority(AzureBlobFileSystem.java:775)
    at shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:94)
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)
    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)
    at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:500)
    at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469)
    at org.apache.spark.SparkContext$$anonfun$wholeTextFiles$1.apply(SparkContext.scala:997)
    at org.apache.spark.SparkContext$$anonfun$wholeTextFiles$1.apply(SparkContext.scala:992)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
    at org.apache.spark.SparkContext.withScope(SparkContext.scala:820)
    at org.apache.spark.SparkContext.wholeTextFiles(SparkContext.scala:992)
    ...
Caused by: java.lang.ClassNotFoundException: shaded.databricks.v20180920_b33d810.com.google.common.base.Preconditions
    at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
    ... 20 more
</code></pre>

<p>One attempt at solving the problem was to move to the latest Databricks runtime - which at the time of this writing is 6.5. That didn't help. I have proceeded going back in versions - 6.4 and 6.3 - since they use different Spark versions but to no avail.</p>

<p>Another thing that I tried was adding <code>""com.google.guava"" % ""guava"" % ""23.0""</code> as a dependency to my <code>build.sbt</code>. That in itself results in errors like:</p>

<pre><code>Exception in thread ""main"" java.lang.IllegalArgumentException: Wrong FS: abfss://abc-fs@cloud.dfs.core.windows.net/paths/something, expected: file:///
</code></pre>

<p>I feel that going down the road of satisfying in and every dependency that somehow is not included in the jar may not be the best option.</p>

<p>I wonder if someone has had a similar experience and if so - how did they solve this.</p>

<p>I am willing to give more context if that is necessary.</p>

<p>Thank you!</p>",0,2,2020-04-22 15:09:41.303000 UTC,,,0,apache-spark|databricks-connect,170,2012-06-12 09:21:26.693000 UTC,2022-03-04 21:33:32.087000 UTC,,927,44,4,74,,,,,,[]
Changing the magic tags in same cell - Azure Databricks,"<p>I am working on Azure Databricks and have fetched a Spark data frame, and need to convert it to R data.frame. I am getting a syntax error when I am using as.data.frame in the same cell for the same.</p>
<p>When tried in different cells, after the initiation of magic tag (%r), and using the same command- it is throwing different errors that object is not found.</p>",1,0,2021-09-17 05:27:18.233000 UTC,,,0,azure|dataframe|apache-spark-sql|azure-databricks,21,2021-09-13 13:47:41.813000 UTC,2021-12-03 11:09:36.593000 UTC,,3,0,0,3,,,,,,[]
Who propagate bugfixes across branches (corporate development)?,"<p>We have many release and custom branches. When bugfix occur it MUST propagated across many branches.</p>

<p>I have several related questions:</p>

<ul>
<li>who decide (which role) where propagate bugfix</li>
<li>who control (which role) bugfix propagation</li>
<li>where fix bug - in trunk/default or in selected branch?</li>
<li>is it need test bugfix on all branches by bugfix developer or only on original and pass another branch testing to QA team?</li>
<li>who propagate bugfix - original bugfix creator or assigned separate person (which role)?</li>
</ul>",1,0,2011-09-07 07:29:48.733000 UTC,,,2,version-control|workflow|dvcs,167,2009-09-14 13:42:30.747000 UTC,2022-02-28 11:34:20.310000 UTC,"Dnipro, Ukraine",40399,19176,27,5468,,,,,,[]
Airflow Cluster Logs with Azure Data Bricks,"<p>I have an Airflow pipeline that sends task requests to Azure Databricks. For each task in the pipeline, a new cluster is created in Databricks which is very time-consuming for pipelines with a high number of tasks.</p>
<p>I can create a cluster in Databricks and give its id to tasks in the pipeline to use it as an existing cluster which solves the problem of creating a cluster for each task. However, this solution comes with a price that is all tasks dump their logs to the same file. My question is that is there a way to separate the logs of each task to track the process or the error of different tasks easily?</p>",0,0,2021-06-09 13:57:58.370000 UTC,,,0,azure|airflow|azure-databricks|cluster-logging,39,2012-10-16 13:51:48.757000 UTC,2022-02-23 17:20:57.523000 UTC,"Toronto, Ontario, Kanada",7,0,0,10,,,,,,[]
ErrorCode=DelimitedTextMoreColumnsThanDefined,"<p>I'm actually working with the Comma(,) delimited csv file, trying to perform copy activity.</p>
<p>One column contains json data like below</p>
<pre><code>&quot;{&quot;&quot;allow&quot;&quot;:&quot;&quot;&quot;&quot;,&quot;&quot;deny&quot;&quot;:&quot;&quot;&quot;&quot;,&quot;&quot;description&quot;&quot;:&quot;&quot;Public&quot;&quot;,&quot;&quot;friends&quot;&quot;:&quot;&quot;&quot;&quot;,&quot;&quot;value&quot;&quot;:&quot;&quot;EVERYONE&quot;&quot;}&quot;
</code></pre>
<p>So when we process the data, the columns are shifting and we are getting the below error.</p>
<p><a href=""https://i.stack.imgur.com/cHEqS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cHEqS.png"" alt=""enter image description here"" /></a></p>
<p>What should be on the source and sink escape and quote characters?</p>
<p>Thanks in advance.</p>",1,0,2021-12-01 12:35:59.800000 UTC,,,0,csv|delimiter|azure-data-factory-2|azure-databricks,30,2019-04-04 07:53:20.343000 UTC,2022-02-21 11:16:47.800000 UTC,"Chennai, Tamil Nadu, India",133,3,0,11,,,,,,[]
How to add trailer/footer in csv dataframe azure blob pyspark,"<hr />
<p>i have as solution which goes like</p>
<ul>
<li>df1 --&gt;dataframe 1 with having 50 columns of data</li>
<li>df2 ---&gt;datarame 2 having footer/trailer 3 columns of data like Trailer,count of rows,date</li>
</ul>
<p>so i added the remaining 47 columns as <code>&quot;&quot;,&quot;&quot;,&quot;&quot;.....</code>  so on
so that i can union 2 dataframe:</p>
<pre><code>df3=df1.union(df2) 
</code></pre>
<p>now if i want to save</p>
<pre><code>df3.coalesce(1).write.format(&quot;com.databricks.spark.csv&quot;)\
  .option(&quot;header&quot;,&quot;true&quot;).mode(&quot;overwrite&quot;)\
  .save(output_blob_path);
</code></pre>
<p>so now i am getting the footer as well
like this <code>Trailer,400,20210805,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;,&quot;&quot;..</code> and so on</p>
<p>if any one can suggest how to remove ,&quot;&quot;,&quot;&quot;,&quot;&quot;,.. these double quotes from the last row
where i want to save this file  in blob container.
it would be very helpful</p>",1,0,2021-08-06 06:10:56.037000 UTC,1.0,2021-08-06 09:15:34.680000 UTC,1,pyspark|azure-blob-storage|azure-databricks,113,2014-07-26 09:26:53.540000 UTC,2021-09-12 14:59:49.410000 UTC,"Hyderabad, Telangana, India",11,0,0,8,,,,,,[]
Databricks: dbutils.fs.mv() throws java.io.FileNotFoundException although file exists,"<p>I have a function to replace parquet files in ADLS Gen2:</p>
<pre class=""lang-py prettyprint-override""><code>def replace_parquet_file(df: DataFrame, path: str):
  path_new = path + '_new'
  path_old = path + '_old'
  
  if not file_exists(path):
    df.write.mode('overwrite').parquet(path)
  else:  
    df.write.parquet(path_new)

    dbutils.fs.mv(path, path_old, recurse = True)
    dbutils.fs.mv(path_new, path, recurse = True)
    dbutils.fs.rm(path_old, recurse = True)
</code></pre>
<p>This function keeps failing at the second mv() operation with the following error:</p>
<pre><code> 135 
    136     dbutils.fs.mv(path, path_old, recurse = True)
--&gt; 137     dbutils.fs.mv(path_new, path, recurse = True)
    138     dbutils.fs.rm(path_old, recurse = True)
    139 

java.io.FileNotFoundException: Failed with java.io.FileNotFoundException while processing file/directory :[/tmp/myfile.parquet/_committed_6614329046368392922] in method:[Operation failed: &amp;#34;The specified path does not exist.&amp;#34;, 404, PUT, https://XXX.dfs.core.windows.net/XXX/tmp/myfile.parquet/_committed_6614329046368392922?action=append&amp;amp;position=0&amp;amp;timeout=90, PathNotFound, &amp;#34;The specified path does not exist. ...] 
</code></pre>
<p>This is obviously a partition, written by spark. As this is the second mv() operation, my assumption would be spark has already finished writing all partitions. When I look at the ADLS myself, the file clearly exists.</p>
<p>I do not understand, what happens here? Is the write process by spark not finished at the moment the mv() operation ist started or is this more of an error with the ADLS API?</p>
<p>Best,
Jan</p>",0,7,2021-08-18 11:23:11.083000 UTC,,,1,apache-spark|pyspark|azure-databricks|azure-data-lake-gen2,373,2016-08-17 08:34:30.690000 UTC,2022-03-01 13:06:35.763000 UTC,,91,2,0,8,,,,,,[]
Neptune InternalFailureException: Can not get the attachable from the host vertex,"<p>I am using neptune's graph database with gremlin queries through python, to store addresses in a database. Most of the queries execute fine, but once i try the following query neptune returns a internal failure exception:</p>

<pre><code>g.V(address).outE('isPartOf').inV().
  dedup().as_('groupNode').
  inE('isPartOf').outV().dedup().as_('children').
  addE('isPartOf').to(group).
  select('groupNode').drop().
  fold().
  coalesce(__.unfold(), 
           g.V(address).addE('isPartOf').to(group)).next()
</code></pre>

<p>Every address has the possibility to belong to a group. when the address is already assigned to a group, i try to take all addresses assigned to that group and assign them to a new group, while deleting the old group. If the address is not yet assigned to a group i simply want to assign the address to the new group immediately.</p>

<p>If i try this query on it's own everything executes perfectly (although it is a bit of a slow query). However once i try to execute this query in parallel on more addresse this query fails with the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""/usr/lib64/python2.7/threading.py"", line 804, in __bootstrap_inner
    self.run()
  File ""gremlinExample.py"", line 30, in run
    processTx(self.tx, self.g, self.parentBlock)
  File ""gremlinExample.py"", line 152, in processTx
    g.V(address).outE('isPartOf').inV().dedup().as_('groupNode').inE('isPartOf').outV().dedup().as_('children').select('children').addE('isPartOf').to(group).select('groupNode').drop().fold().coalesce(__.unfold(), g.V(address).addE('isPartOf').to(group)).next()
  File ""/home/ec2-user/.local/lib/python2.7/site-packages/gremlin_python/process/traversal.py"", line 70, in next
    return self.__next__()
  File ""/home/ec2-user/.local/lib/python2.7/site-packages/gremlin_python/process/traversal.py"", line 43, in __next__
    self.traversal_strategies.apply_strategies(self)
  File ""/home/ec2-user/.local/lib/python2.7/site-packages/gremlin_python/process/traversal.py"", line 346, in apply_strategies
    traversal_strategy.apply(traversal)
  File ""/home/ec2-user/.local/lib/python2.7/site-packages/gremlin_python/driver/remote_connection.py"", line 143, in apply
    remote_traversal = self.remote_connection.submit(traversal.bytecode)
  File ""/home/ec2-user/.local/lib/python2.7/site-packages/gremlin_python/driver/driver_remote_connection.py"", line 54, in submit
    results = result_set.all().result()
  File ""/usr/lib/python2.7/site-packages/concurrent/futures/_base.py"", line 405, in result
    return self.__get_result()
  File ""/usr/lib/python2.7/site-packages/concurrent/futures/_base.py"", line 357, in __get_result
    raise type(self._exception), self._exception, self._traceback
GremlinServerError: 500: {""requestId"":""a42015b7-6b22-4bd1-9e7d-e3252e8f3ab6"",""code"":""InternalFailureException"",""detailedMessage"":""Can not get the attachable from the host vertex: v[64b32957-ef71-be47-c8d7-0109cfc4d9fd]-/-&gt;neptunegraph[org.apache.commons.configuration.PropertiesConfiguration@6db0f02]""}
</code></pre>

<p>To my knowledge execution in parallel shouldn't be the problem, since every query simply get's queued at the database (exactly for this reason i tried to create a query which executes the whole task at once).</p>

<p>Excuses for any bad English, it is not my native language</p>",1,5,2018-10-08 08:20:19.480000 UTC,,2018-10-09 10:24:13.960000 UTC,1,python|gremlin|amazon-neptune,638,2016-01-20 10:01:45.427000 UTC,2022-03-02 12:26:49.133000 UTC,,11,0,0,2,,,,,,[]
Databricks cannot connect to Azure Synapse Analytics: Unexpected version returned: Microsoft SQL Azure (RTM) - 12.0.2000.8,"<p>I am trying to write some data to Azure Synapse Analytics from Databricks notebook with following code:</p>
<pre class=""lang-py prettyprint-override""><code>write_to_synapse = (
  spark.readStream.format('delta').option('ignoreChanges',True).table('turbine_agg') # Read in Gold turbine readings from Delta as a stream
    .writeStream.format(&quot;com.databricks.spark.sqldw&quot;)                                     # Write to Synapse (SQL DW connector)
    .option(&quot;url&quot;,dbutils.secrets.get(&lt;scope&gt;, &lt;secret&gt;))                                # SQL Pool JDBC connection (with SQL Auth) string 
    .option(&quot;tempDir&quot;, SYNAPSE_PATH)                                                      # Temporary ADLS path to stage the data (with forwarded permissions)
    .option(&quot;forwardSparkAzureStorageCredentials&quot;, &quot;true&quot;)
    .option(&quot;dbTable&quot;, &quot;turbine_agg&quot;)                                                # Table in Synapse to write to
    .option(&quot;checkpointLocation&quot;, CHECKPOINT_PATH+&quot;synapse&quot;)                              # Checkpoint for resilient streaming
    .start()
)
</code></pre>
<p>Where secret is in JDBC connection string format:</p>
<pre><code>jdbc:sqlserver://&lt;on-demand-sql-pool&gt;.sql.azuresynapse.net:1433;database=master;user=&lt;user&gt;;password=&lt;password&gt;;encrypt=true;trustServerCertificate=true;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;
</code></pre>
<p>I am getting the following error:</p>
<pre><code>IllegalArgumentException: Unexpected version returned: Microsoft SQL Azure (RTM) - 12.0.2000.8 
    Aug 17 2020 07:52:53 
    Copyright (C) 2019 Microsoft Corporation

Make sure your JDBC url includes a &quot;database=&lt;DataWareHouseName&gt;&quot; option and that
it points to a valid Azure Synapse SQL Analytics (Azure SQL Data Warehouse) name.
This connector cannot be used for interacting with any other systems (e.g. Azure
SQL Databases).
</code></pre>
<p>I have a database option included, and database master exists.</p>
<p>What is wrong and how I fix this issue?</p>
<p>Thank you!</p>",0,2,2020-08-31 10:40:32.767000 UTC,,2020-09-25 06:18:52.213000 UTC,0,azure|jdbc|azure-databricks|azure-synapse,440,2015-01-28 17:41:09.370000 UTC,2021-10-07 11:01:59.850000 UTC,,1,0,0,6,,,,,,[]
Apache Spark on Databricks Caused by: scala.reflect.internal.MissingRequirementError: object java.lang.Object in compiler mirror not found,"<p>Whenever I attempt to excute any command (for example %fs rm -r /mnt/driver-daemon/jars/), on Apache Spark on Databricks Community Edition I get the following error:</p>
<p>java.lang.Exception: An error occurred while initializing the REPL. Please check whether there are conflicting Scala libraries or JARs attached to the cluster, such as Scala 2.11 libraries attached to Scala 2.10 cluster (or vice-versa).</p>
<p>When I look into the error logs I see the problem is caused by:</p>
<p><strong>Caused by: scala.reflect.internal.MissingRequirementError: object java.lang.Object in compiler mirror not found.</strong></p>
<p>The full error is as follows:</p>
<pre><code>    at com.databricks.backend.daemon.driver.DriverILoop.initSpark(DriverILoop.scala:60)
    at com.databricks.backend.daemon.driver.DriverILoop.initializeSpark(DriverILoop.scala:185)
    at com.databricks.backend.daemon.driver.DriverILoop.createInterpreterForWeb(DriverILoop.scala:165)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal.createInterpreter(ScalaDriverLocal.scala:417)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal.interp(ScalaDriverLocal.scala:434)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:202)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:202)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:202)
    at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:714)
    at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:667)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:202)
    at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:396)
    at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:373)
    at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
    at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
    at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)
    at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)
    at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)
    at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:373)
    at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)
    at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)
    at scala.util.Try$.apply(Try.scala:192)
    at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)
    at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)
    at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)
    at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)
    at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)
    at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)
    at java.lang.Thread.run(Thread.java:748)
Caused by: scala.reflect.internal.MissingRequirementError: object java.lang.Object in compiler mirror not found.
    at scala.reflect.internal.MissingRequirementError$.signal(MissingRequirementError.scala:17)
    at scala.reflect.internal.MissingRequirementError$.notFound(MissingRequirementError.scala:18)
    at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:53)
    at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:45)
    at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:45)
    at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:66)
    at scala.reflect.internal.Mirrors$RootsBase.getClassByName(Mirrors.scala:102)
    at scala.reflect.internal.Mirrors$RootsBase.getRequiredClass(Mirrors.scala:105)
    at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute(Definitions.scala:257)
    at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass(Definitions.scala:257)
    at scala.reflect.internal.Definitions$DefinitionsClass.init(Definitions.scala:1390)
    at scala.tools.nsc.Global$Run.&lt;init&gt;(Global.scala:1242)
    at scala.tools.nsc.interpreter.IMain.compileSourcesKeepingRun(IMain.scala:439)
    at scala.tools.nsc.interpreter.DriverIMain.compileSourcesKeepingRun(DriverIMain.scala:305)
    at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.compileAndSaveRun(IMain.scala:862)
    at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.compile(IMain.scala:820)
    at scala.tools.nsc.interpreter.DriverIMain.bind(DriverIMain.scala:84)
    at com.databricks.backend.daemon.driver.DriverILoop.bind(DriverILoop.scala:191)
    at com.databricks.backend.daemon.driver.DatabricksILoop$class.initSpark(DatabricksILoop.scala:87)
    at com.databricks.backend.daemon.driver.DriverILoop.initSpark(DriverILoop.scala:60)
    at com.databricks.backend.daemon.driver.DriverILoop.initializeSpark(DriverILoop.scala:185)
    at com.databricks.backend.daemon.driver.DriverILoop.createInterpreterForWeb(DriverILoop.scala:165)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal.createInterpreter(ScalaDriverLocal.scala:417)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal.interp(ScalaDriverLocal.scala:434)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:202)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:202)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:202)
    at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:714)
    at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:667)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:202)
    at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:396)
    at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:373)
    at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
    at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
    at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)
    at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)
    at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)
    at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:373)
    at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)
    at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)
    at scala.util.Try$.apply(Try.scala:192)
    at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)
    at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)
    at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)
    at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)
    at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)
    at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)
    at java.lang.Thread.run(Thread.java:748)```

Any thoughts on how to go about resolving this issue?
</code></pre>",1,10,2020-07-04 11:27:41.000000 UTC,1.0,2020-07-05 20:45:45.513000 UTC,0,apache-spark|azure-databricks,525,2016-06-25 22:07:19.907000 UTC,2021-11-21 00:16:09.640000 UTC,,716,49,1,223,,,,,,[]
Django db driver for AWS Neptune,<p>Is there a Django driver for AWS Neptune. As I would be using Neptune Graph DB with Django and was wondering would there be a driver available for it.</p>,1,1,2019-01-15 08:53:30.813000 UTC,0.0,,2,amazon-web-services|amazon-neptune,374,2014-11-07 05:23:33.780000 UTC,2022-02-20 08:22:50.773000 UTC,SG,350,15,1,35,,,,,,[]
Spark Job stuck writing dataframe to partitioned Delta table,"<p>Running databricks to read csv files and then saving as a partitioned delta table.</p>
<p>Total records in file are 179619219 .  It is being split on COL A (8419 unique values) and Year ( 10 Years) and Month.</p>
<p>df.write.partitionBy(&quot;A&quot;,&quot;year&quot;,&quot;month&quot;).format(&quot;delta&quot;).mode(&quot;append&quot;).save(path)</p>
<p>Job gets stuck on the write step and aborts after running for 5-6 hours</p>",1,1,2021-11-10 18:24:56.000000 UTC,,2021-11-10 18:44:50.887000 UTC,1,python|apache-spark|pyspark|azure-databricks|delta-lake,448,2021-10-28 22:53:17.650000 UTC,2021-11-29 22:42:23.627000 UTC,,21,0,0,1,,,,,,[]
How to get distances in pyspark?,"<p>I have a table like the following:</p>

<pre><code>+--------------------+--------------------+-------------------+
|                  ID|               point|          timestamp|
+--------------------+--------------------+-------------------+
|679ac975acc4bdec9...|POINT (-73.267631...|2020-01-01 17:10:49|
|679ac975acc4bdec9...|POINT (-73.271446...|2020-01-01 02:12:31|
|679ac975acc4bdec9...|POINT (-73.265991...|2020-01-01 17:10:40|
|679ac975acc4bdec9...|POINT (-73.271446...|2020-01-01 02:54:15|
|679ac975acc4bdec9...|POINT (-73.265609...|2020-01-01 17:10:24|
+--------------------+--------------------+-------------------+
</code></pre>

<p>I want to compute the distance between all the points but I am not able to do it.</p>

<p>However I can compute the distance from each point in the comlumn <code>point</code> to a specific point in the following way</p>

<pre><code>distances = spark.sql(
    """"""
        SELECT ID, timestamp, point,
        ST_Distance(point, ST_PointFromText('-74.00672149658203, 40.73177719116211', ',')) as distance
        FROM myTable
    """""").show(5)



+--------------------+-------------------+--------------------+------------------+
|                  ID|          timestamp|               point|          distance|
+--------------------+-------------------+--------------------+------------------+
|679ac975acc4bdec9...|2020-01-01 17:10:49|POINT (-73.267631...|0.7485722629444987|
|679ac975acc4bdec9...|2020-01-01 02:12:31|POINT (-73.271446...|0.7452303978930688|
|679ac975acc4bdec9...|2020-01-01 17:10:40|POINT (-73.265991...|0.7503403834426271|
|679ac975acc4bdec9...|2020-01-01 02:54:15|POINT (-73.271446...|0.7452310193408604|
|679ac975acc4bdec9...|2020-01-01 17:10:24|POINT (-73.265609...|0.7511492495935203|
+--------------------+-------------------+--------------------+------------------+
</code></pre>

<p>How can I compute the distance from one point in a row to the following one?</p>",1,0,2020-03-28 12:45:39.480000 UTC,,,0,python|sql|pyspark|geospatial|azure-databricks,173,2014-04-30 16:00:53.307000 UTC,2022-03-03 14:37:36.293000 UTC,"Boston, MA",5561,98,7,794,,,,,,[]
How to create a dynamic dataframe,"<p>I was trying to create a data frame and the reason why I gave the create a data frame in the below manner is to make it dynamic but the expression is passed as a string and the exec command is not able to create the data frame and assign to a variable.</p>

<p>Here is my code:</p>

<pre><code>def fileReader(inputFileType,sourceFilePath,inputFileType):
 value ='true'
 header='header'


 a= ""spark.read.option('""+header+""','""+value+""').""+inputFileType+""('""+sourceFilePath+""')""
 print(a)
 print(type(a))
 ds = exec(a)
 return 'True'
</code></pre>",3,1,2020-02-08 15:36:06.853000 UTC,,2020-02-18 20:16:23.917000 UTC,0,python|dataframe|apache-spark|pyspark|azure-databricks,3236,2018-09-06 04:18:09.453000 UTC,2022-03-04 07:54:38.533000 UTC,"Hyderabad, Telangana, India",300,12,0,56,,,,,,[]
"Read and write fixed length flat file using spark(SparkSQL) in java(not scala, not python)",<p>I have to implement a use case in which I have to read fixed length flat file using Spark SQL(DataFrame) in java and write to another fixed length file . My boundation is I can use only java. Can anyone suggest me a good way to implement this functinality in Java spark</p>,0,6,2020-11-17 12:56:54.033000 UTC,,,0,java|apache-spark-sql|azure-databricks|flat-file,75,2016-01-25 11:11:11.683000 UTC,2022-03-03 04:13:39.687000 UTC,,41,0,0,9,,,,,,[]
TypeError: first() missing 1 required positional argument: 'offset' in DecisionTree.trainClassifier in Pyspark,"<p>I have written simple code in pyspark on Azure databricks( followed this link
<a href=""http://spark.apache.org/docs/latest/mllib-decision-tree.html#classification"" rel=""nofollow noreferrer"">decision tree in pyspark</a>-)</p>

<pre><code>%python
x='x'
z='y'
data = pd.DataFrame({'a':[1,2,3,41,2,6,2,3,56,1,2,5,1,2,45,1,3,2], 'b':[x,z,x,x,z,x,z,x,x,x,z,z,x,z,z,x,x,x]})

# Train a DecisionTree model.
model = DecisionTree.trainClassifier(data, numClasses=2, categoricalFeaturesInfo={},impurity='gini', maxDepth=5, maxBins=32)

</code></pre>

<p>I have kept parameters as default. While running I get error-</p>

<p><strong>TypeError: first() missing 1 required positional argument: 'offset'</strong></p>

<p>I am not sure which argument this error is referring to and also where do i need to specify my dependent variable in classifier?</p>

<p><a href=""https://i.stack.imgur.com/0ggVp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0ggVp.png"" alt=""enter image description here""></a></p>",1,0,2019-04-02 16:55:22.803000 UTC,,,1,pyspark|decision-tree|azure-databricks,2976,2015-08-11 08:00:53.903000 UTC,2022-01-24 15:17:15.283000 UTC,"Bangalore, Karnataka, India",523,450,6,254,,,,,,[]
Detecting conflicts between parent repo and fork without actually pulling,"<p>I'd like to solve the following problem:
I have a repository (call it A) and a clone of it (call it B). I made some changes in B and I want to ask the owner of A to pull it. I'd like to implement a function that can figure out if the changes in B can be pulled to A without causing any conflicts. I want to implement this in such a way that doesn't corrupt the information in A (so anyone who clones A sees exactly the same than before this confict detection).</p>

<p>Please note that I'd like to solve it the more efficient way. I saw that bitbucket performs this check for pull requests very fast. Do you have any idea?</p>

<p>Thanks in advance</p>",2,0,2011-06-20 07:39:11.237000 UTC,,,2,version-control|mercurial|dvcs,71,2011-06-08 18:51:31.643000 UTC,2022-03-04 08:46:56.747000 UTC,"Debrecen, Magyarország",2857,11,0,62,,,,,,[]
Get Azure Databricks lineage components,"<p>Usually In data bricks we will have workspace and then notebooks. Inside notebooks we will have commands. I will get these commands one by one and based on this each command I have to prepare lineage. For building lineage we need source and destination so how can we get source and destination using this command
For Example:
<strong>%python
display(dbutils.fs.ls(&quot;/databricks-datasets&quot;))</strong>
Above is one command so how can we figure source and destination. I know using Spline tool we can get the solution but we need to work on commands.
So can anyone help me on this</p>",0,0,2021-04-20 12:06:20.300000 UTC,,,1,apache-spark|pyspark|apache-spark-sql|azure-databricks|azure-notebooks,36,2018-01-11 09:15:22.790000 UTC,2022-01-02 17:27:18.567000 UTC,,63,1,0,58,,,,,,[]
Databricks-connect Java connects to local instead of remote,"<p>I have a Java application that connects to an Apache Spark cluster and performs some operations. I'm trying to connect to a Databricks cluster on Azure, using databricks-connect 7.3. If I run from the terminal <code>databricks-connect test</code>, everything works perfectly. I'm following their <a href=""https://docs.databricks.com/dev-tools/databricks-connect.html"" rel=""nofollow noreferrer"">documentation</a>, I included the jars in IntelliJ, added <code>spark.databricks.service.server.enabled true</code> to the cluster in Databricks and used the following to create the SparkSession:</p>
<pre><code>SparkSession spark = SparkSession
                .builder()
                .master(&quot;local&quot;)
                .getOrCreate();
</code></pre>
<p>The problem is that this command connects to a local cluster that is instantiated at runtime, and does not connect to the remote Databricks cluster. Am I missing something?</p>",0,2,2021-02-04 15:26:17.607000 UTC,,,0,apache-spark|azure-databricks|databricks-connect,329,2012-04-16 18:18:23.363000 UTC,2022-03-04 14:49:08.170000 UTC,"Amsterdam, Netherlands",1637,2516,2,141,,,,,,[]
How to connect Azure Data Lake Store gen 2 File Share with Azure Databricks?,"<p>I have an Azure data lake storage gen 2 account, with hierarchical namespace enabled. I generated a SAS-token to the account, and I recieve data to a folder in the File Share (File Service). Now I want to access these files through Azure Databricks and python. However, it seems like Azure Databricks can only access the File System (called Blob Container in gen1), and not the File Share. I also failed to generate a SAS-token to the File System. </p>

<p>I wish to have a storage instance to which can generate a SAS-token and give to my client, and access the same from azure databricks using python. It is not important if it is File System, File Share, ADLS gen2 or gen1 as long as it somehow works. </p>

<p>I use the following to access the File System from databricks: </p>

<pre class=""lang-javascript prettyprint-override""><code>configs = {""fs.azure.account.auth.type"": ""OAuth"",
           ""fs.azure.account.oauth.provider.type"": ""org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"",
           ""fs.azure.account.oauth2.client.id"": ""my_client_id"",
           ""fs.azure.account.oauth2.client.secret"": ""my_client_secret"",
           ""fs.azure.account.oauth2.client.endpoint"": ""https://login.microsoftonline.com/""+""My_tenant_id"" +""/oauth2/token"",
           ""fs.azure.createRemoteFileSystemDuringInitialization"": ""true""}

dbutils.fs.mount(source = ""abfss://""+""my_file_system""+""@""+""my_storage_account""+"".dfs.core.windows.net/MyFolder"",
                 mount_point = ""/mnt/my_mount"",
                 extra_configs = configs) 
</code></pre>

<p>Works fine but I cannot make it access the File Share. And I have a SAS-token with a connection string like this: </p>

<pre class=""lang-javascript prettyprint-override""><code>connection_string = (
    'BlobEndpoint=https://&lt;my_storage&gt;.blob.core.windows.net/;'+
    'QueueEndpoint=https://&lt;my_storage&gt;.queue.core.windows.net/;'+
    'FileEndpoint=https://&lt;my_storage&gt;.file.core.windows.net/;'+
    'TableEndpoint=https://&lt;my_storage&gt;.table.core.windows.net/;'+
    'SharedAccessSignature=sv=2018-03-28&amp;ss=bfqt&amp;srt=sco&amp;sp=rwdlacup&amp;se=2019-09-26T17:12:38Z&amp;st=2019-08-26T09:12:38Z&amp;spr=https&amp;sig=&lt;my_sig&gt;'
)
</code></pre>

<p>Which I manage to use to upload stuff to the file share, but not to the file system. Is there any kind of Azure storage that can be accessed by both a SAS-token and azure databricks?</p>",1,0,2019-08-26 14:21:47.723000 UTC,1.0,2019-08-26 14:41:01.903000 UTC,1,azure|azure-storage|azure-databricks|sas-token,1169,2019-08-26 12:53:57.823000 UTC,2019-10-14 14:33:51.593000 UTC,,11,0,0,0,,,,,,[]
Connect to AWS Postgres using Databricks,"<p>having issues connecting to AWS Postgres from Azure Databricks, I am new to Azure and below is the code I am using to connect to Postgres but somehow its throwing an error
error: org.postgresql.util.PSQLException: The connection attempt failed.</p>

<p>Code: </p>

<pre><code>jdbc_url=""jdbc:postgresql://postgreshost:5432/db?user={}&amp;password={}&amp;ssl=true.format(username,password)""

pushdown_query = ""(select * from test limit 10) emp_alias""
df = spark.read.jdbc(url=jdbc_url, table=""test"")
display(df)
</code></pre>

<p>2nd method:</p>

<pre><code>df = spark.read \
.format(""jdbc"") \
.option(""url"", ""jdbc:postgresql://postgreshost:5432/db?user=user&amp;password=password"") \
.option(""dbtable"", ""test"") \
.load()
</code></pre>

<p>Am I missing anything? or should I follow any steps prior to execution?</p>

<p>Log using Scala:</p>

<pre><code>at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:275)
    at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)
    at org.postgresql.jdbc.PgConnection.&lt;init&gt;(PgConnection.java:194)
    at org.postgresql.Driver.makeConnection(Driver.java:450)
    at org.postgresql.Driver.connect(Driver.java:252)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:64)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:55)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:56)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:210)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)
    at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
    at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:298)
    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:279)
    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:202)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3334328075204474:8)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3334328075204474:51)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3334328075204474:53)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$read$$iw$$iw$$iw.&lt;init&gt;(command-3334328075204474:55)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$read$$iw$$iw.&lt;init&gt;(command-3334328075204474:57)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$read$$iw.&lt;init&gt;(command-3334328075204474:59)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$read.&lt;init&gt;(command-3334328075204474:61)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$read$.&lt;init&gt;(command-3334328075204474:65)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$read$.&lt;clinit&gt;(command-3334328075204474)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$eval$.$print$lzycompute(&lt;notebook&gt;:7)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$eval$.$print(&lt;notebook&gt;:6)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$eval.$print(&lt;notebook&gt;)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:786)
    at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1047)
    at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:638)
    at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:637)
    at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
    at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)
    at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:637)
    at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:569)
    at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:565)
    at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:199)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:189)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:189)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:189)
    at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:587)
    at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:542)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:189)
    at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:324)
    at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:304)
    at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:235)
    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
    at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:230)
    at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:45)
    at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:268)
    at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:45)
    at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:304)
    at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)
    at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)
    at scala.util.Try$.apply(Try.scala:192)
    at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:584)
    at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:475)
    at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:542)
    at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:381)
    at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:328)
    at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:215)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.SocketTimeoutException: connect timed out
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
    at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
    at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    at java.net.Socket.connect(Socket.java:589)
    at org.postgresql.core.PGStream.&lt;init&gt;(PGStream.java:68)
    at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:144)
    at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)
    at org.postgresql.jdbc.PgConnection.&lt;init&gt;(PgConnection.java:194)
    at org.postgresql.Driver.makeConnection(Driver.java:450)
    at org.postgresql.Driver.connect(Driver.java:252)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:64)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:55)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:56)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:210)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)
    at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)
    at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:298)
    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:279)
    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:202)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3334328075204474:8)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3334328075204474:51)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3334328075204474:53)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$read$$iw$$iw$$iw.&lt;init&gt;(command-3334328075204474:55)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$read$$iw$$iw.&lt;init&gt;(command-3334328075204474:57)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$read$$iw.&lt;init&gt;(command-3334328075204474:59)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$read.&lt;init&gt;(command-3334328075204474:61)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$read$.&lt;init&gt;(command-3334328075204474:65)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$read$.&lt;clinit&gt;(command-3334328075204474)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$eval$.$print$lzycompute(&lt;notebook&gt;:7)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$eval$.$print(&lt;notebook&gt;:6)
    at lined9bdaa60f31e4f44a370d2ec7ae9793627.$eval.$print(&lt;notebook&gt;)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:786)
    at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1047)
    at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:638)
    at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:637)
    at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
    at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)
    at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:637)
    at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:569)
    at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:565)
    at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:199)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:189)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:189)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:189)
    at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:587)
    at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:542)
    at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:189)
    at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:324)
    at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:304)
    at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:235)
    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
    at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:230)
    at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:45)
    at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:268)
    at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:45)
    at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:304)
    at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)
    at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)
    at scala.util.Try$.apply(Try.scala:192)
    at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:584)
    at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:475)
    at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:542)
    at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:381)
    at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:328)
    at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:215)
    at java.lang.Thread.run(Thread.java:748)
</code></pre>",2,0,2019-05-02 15:35:00.020000 UTC,,2019-05-02 16:33:21.037000 UTC,0,postgresql|amazon-web-services|pyspark|azure-databricks,1130,2014-09-03 16:15:15.787000 UTC,2021-11-18 17:25:23.640000 UTC,,87,9,0,111,,,,,,[]
Databricks Spark cluster MLlib linear regression same performance as Spark MLlib on local laptop?,"<p>Using the same code, I'm running a toy linear regression on 10,000 rows on my laptop (16 GB memory, 8 cores) and on Azure Databricks 7.3 LTS cluster with 8 workers (16 GB memory, 8 cores each) and both finish the regression in the same amount of time, ~240 seconds. Is this a configuration problem or am I wrong to expect that this should be faster on the cluster?</p>
<h1>Code</h1>
<h2>Setup</h2>
<pre><code>sc &lt;- sparklyr::spark_connect(master = &quot;local&quot;)

library(magrittr)

n = 10000

df &lt;-
  data.frame(
    num1 = runif(n, 0, 10),
    num2 = rnorm(n, 100, 10),
    fac1num = sample(1:50, n, replace = TRUE),
    fac2num1 = sample(1:50, n, replace = TRUE),
    fac2num2 = sample(1:50, n, replace = TRUE),
    noise = rnorm(n, 0, 10)
  )

##### my real problem uses factors with ~50 and ~2,000 levels each
df$fac1 = state.name[df$fac1num]
df$fac2 = paste0(state.name[df$fac2num1], state.name[df$fac2num2])

df$y = df$num1 + df$num2 * 12 - log(df$fac1num) + df$fac2num1*df$fac2num2/1000 + df$noise


##### copy from R to Spark memory
df_tbl &lt;- sparklyr::copy_to(sc, df, &quot;df_spark&quot;)
</code></pre>
<h2>Benchmarks</h2>
<pre><code>### ~240 seconds (same as local laptop!)
system.time(
mdl_df &lt;- sparklyr::ml_linear_regression(df_tbl, formula = y ~ num1 * fac1 * fac2 + num2)
)
</code></pre>
<h2>Playing around with alternatives</h2>
<pre><code>##### copy explicitly to Spark memory (I don't think this does anything here since it is already in memory, 
##### but I do this in my real problem after SQL server has done some work)
system.time(
df_tbl_cached &lt;- df_tbl %&gt;% dplyr::compute()
)

##### ~230 seconds (I think same as above?)
system.time(
mdl_df_cached &lt;- sparklyr::ml_linear_regression(df_tbl_cached, formula = y ~ num1 * fac1 * fac2 + num2)
)

##### just playing around wondering if I needed to do something different
system.time(
df_tbl_persist &lt;- sparklyr::sdf_persist(df_tbl, storage.level = &quot;MEMORY_ONLY&quot;, name = &quot;df_tbl_persist_spark&quot;)
)

##### ~230 seconds
system.time(
mdl_df_persist &lt;- sparklyr::ml_linear_regression(df_tbl_persist, formula = y ~ num1 * fac1 * fac2 + num2)
)

##### playing around again with Spark memory
system.time({
sparklyr::sdf_register(df_tbl, &quot;df_tbl_register&quot;)

sparklyr::tbl_cache(sc, &quot;df_tbl_register&quot;)
})

##### ~230 seconds
system.time(
mdl_df_register &lt;- sparklyr::ml_linear_regression(df_tbl, formula = y ~ num1 * fac1 * fac2 + num2)
)
</code></pre>",0,0,2021-07-07 12:45:32.753000 UTC,,,0,r|apache-spark|linear-regression|apache-spark-mllib|azure-databricks,27,2021-04-21 16:43:43.857000 UTC,2022-03-01 17:11:05.657000 UTC,,33,0,0,2,,,,,,[]
Databricks: Provide schema in dataframe column as a parameter for from_avro,"<p>I'm trying to use the function from_avro in a dataframe.</p>
<p>This dataframe has its origin from a streamRead from kafka and at some point I create a column with the schemaId (related to schema registry) and the message.</p>
<p>I then have an UDF that grabs the schema Id and goes to my SchemaRegistry API to fetch the schema, so at the end I have a column with it.</p>
<p>After this, I'm trying to create a new column with the decoded message by calling from_avro(&quot;message&quot;, &quot;schema&quot;, options) but my &quot;schema&quot; is a column and the from_avro expects a string. I tried everything to turn the column to string but I get other errors like &quot;Column is not  iterable&quot;.</p>
<p>I also tried to move the from_avro into a UDF but then I get issues related to the fact that the from_avro needs to be executed in driver context (even though I'm using a single node cluster).</p>",0,4,2021-08-23 15:25:49.727000 UTC,,,2,apache-kafka|apache-spark-sql|azure-databricks|spark-avro,105,2012-07-31 15:35:28.560000 UTC,2022-01-28 11:51:46.850000 UTC,Portugal,699,29,0,119,,,,,,[]
How can I use GIT-TFS and GIT-SVN in the same source tree?,"<p>I have a project at work that I'd like to make <del>needlessly complex</del>easier by using git for my local source control.  We use TFS at work, but I also have to push the source to the client's SVN server for production releases.  TFS needs to be the official source of record [until I can make a convincing case to switch :-)], SVN is just milestones, and I'd love for git to be the day-to-day working environment.</p>

<p>I've been using <a href=""http://git-scm.com/docs/git-svn"" rel=""nofollow"">git-svn</a> for the second part, basically just committing to git at the milestones, then pushing to svn.  </p>

<p>I'd like to move to using git instead of TFS for the more active development, too, but I'm not sure how/if I can have both mappings setup at the same time.  All of the examples that I see for <a href=""https://github.com/spraints/git-tfs"" rel=""nofollow"">git-tfs</a> and git-svn start with cloning the repository, which requires the repository to be empty.  Is there some sort of metadata that I can add instead of cloning to setup the TFS mapping?</p>

<p>The mappings are also a little weird (it's a CMS site, so we don't store the core files in source control, only our customizations):  </p>

<ul>
<li>tags (only used by SVN)</li>
<li>branches (from SVN, not used)</li>
<li>trunk (from SVN, symlinked to Website)</li>
<li>Website
<ul>
<li>DesktopModules (mapped in TFS)</li>
<li>Portals (mapped in TFS)</li>
<li>Providers (mapped in TFS)</li>
</ul></li>
</ul>

<p>Will git be able to handle the SVN repository mapped at the root and the TFS repository mapped further in?</p>

<p>Would it work to delete the folders mapped in TFS, then clone them from git-tfs?</p>",2,1,2011-04-04 13:57:18.130000 UTC,,2011-04-04 19:48:54.900000 UTC,2,git|git-svn|dvcs|git-tfs,368,2008-08-24 14:48:05.757000 UTC,2022-03-04 16:42:44.020000 UTC,"St Louis, MO",143191,3831,541,6089,,,,,,[]
Impact of Azure databricks and ADF deployment to running jobs/pipelines,"<p>For Azure DevOps to deploy azure databricks, if there is running jobs on databricks (for example it is a streaming job), would the current running job still be running or be stopped? And will it still use the old lib or new? When will the new code be picked up?</p>
<p>Similar for ADF deployment, how would deployment impact running pipelines?</p>",1,1,2022-02-21 07:24:55.863000 UTC,,,0,azure-devops|azure-data-factory-2|azure-databricks,25,2015-08-26 01:49:59.573000 UTC,2022-03-04 11:15:46.160000 UTC,,61,1,0,8,,,,,,[]
"How do I get all the edges, associated vertices, and the respective id, label and properties via gremlin-python?","<p>I want to fetch all the edges and associated vertices from my <a href=""https://aws.amazon.com/neptune/"" rel=""nofollow noreferrer"">AWS Neptune DB</a>. In addition, I want the id, labels and properties for the nodes as well as edges.<br>
My data is a stored as a property graph and I'm using <a href=""https://pypi.org/project/gremlinpython/"" rel=""nofollow noreferrer"">gremlin-python</a> to query it.</p>

<p>Below is a query which provides the required data when executed in the gremlin shell. However, trying to execute the same using gremlin-python throws an error. </p>

<pre><code>g.V().bothE().otherV().limit(2).path().by(__.valueMap(true))
</code></pre>

<p><em>Python variant</em></p>

<pre><code>from gremlin_python import statics
from gremlin_python.structure.graph import Graph
from gremlin_python.process.traversal import T
from gremlin_python.process.graph_traversal import __
from gremlin_python.process.strategies import *
from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection

# Define a graph instance
graph = Graph()

# Connect to the neptune instance
try:
    remoteConn = DriverRemoteConnection('wss://&lt;YOUR_NEPTUNE_IP&gt;:8182/gremlin', 'g')
    g = graph.traversal().withRemote(remoteConn)
except:
    print(""Couldn't connect successfully with Neptune instance"")
    exit()

g.V().bothE().otherV().limit(2).path().by(__.valueMap(True))
</code></pre>

<p><em>Error</em></p>

<pre><code>GremlinServerError: 498: {""requestId"":""..."",""code"":""UnsupportedOperationException"",""detailedMessage"":""org.apache.tinkerpop.gremlin.process.traversal.step.util.ImmutablePath cannot be cast to org.apache.tinkerpop.gremlin.structure.Element""}
</code></pre>

<p>Can someone provide me a way to get the required information? Be it by successfully converting the above query to Python or via some other query.</p>",1,0,2019-07-18 08:34:41.303000 UTC,,2019-07-20 19:49:07.607000 UTC,2,python|gremlin|tinkerpop|amazon-neptune,2651,2013-07-07 09:42:52.263000 UTC,2022-03-05 12:10:17.987000 UTC,India,349,998,1,454,,,,,,[]
Why does Mercurial sometimes allow merging with an ancestor?,"<p>I'm puzzled by <a href=""https://bitbucket.org/lorien/grab/changesets/a57326ec3e4a"" rel=""nofollow"">this repo history</a>. On that page, you'll see a small branch <code>file-content</code> which is merged back to the branch <code>default</code> even though one head was an ancestor of the other.</p>

<p><strike>
When I try doing something similar I get an abort message unless</p>

<ol>
<li>the side branch is closed and</li>
<li>I merge from the ancestor to the closed branch head (but not the other way around). (In this grab repo, the <code>file-content</code> branch is marked inactive, not closed.)
</strike></li>
</ol>

<p>Edit: the real conditions under which you can merge with an ancestor are described in my answer.</p>

<p>So what's going on here?</p>",2,0,2012-03-22 11:11:01.393000 UTC,,2012-03-23 01:52:32.137000 UTC,2,mercurial|merge|branch|dvcs,1037,2008-08-16 21:07:34.560000 UTC,2014-12-21 23:01:17.973000 UTC,,11084,50,1,264,,,,,,[]
Azure Databricks to Azure SQL DW: Long text columns,"<p>I would like to populate an Azure SQL DW from an Azure Databricks notebook environment. I am using the built-in connector with pyspark:</p>

<pre class=""lang-py prettyprint-override""><code>sdf.write \
  .format(""com.databricks.spark.sqldw"") \
  .option(""forwardSparkAzureStorageCredentials"", ""true"") \
  .option(""dbTable"", ""test_table"") \
  .option(""url"", url) \
  .option(""tempDir"", temp_dir) \
  .save()
</code></pre>

<p>This works fine, but I get an error when I include a string column with a sufficiently long content. I get the following error:</p>

<blockquote>
  <p>Py4JJavaError: An error occurred while calling o1252.save.
  : com.databricks.spark.sqldw.SqlDWSideException: SQL DW failed to execute the JDBC query produced by the connector.</p>
  
  <p>Underlying SQLException(s):
    - com.microsoft.sqlserver.jdbc.SQLServerException: HdfsBridge::recordReaderFillBuffer - Unexpected error encountered filling record reader buffer: HadoopSqlException: String or binary data would be truncated. [ErrorCode = 107090] [SQLState = S0001]</p>
</blockquote>

<p>As I understand it, this is because the default string type is NVARCHAR(256). It is possible to configure (<a href=""https://docs.databricks.com/data/data-sources/azure/sql-data-warehouse.html"" rel=""noreferrer"">reference</a>), but the maximum NVARCHAR length is 4k characters. My strings occasionally reach 10k characters. <strong>Therefore, I am curious as to how I can export certain columns as text/longtext instead.</strong></p>

<p>I would guess that the following would work, if only the <code>preActions</code> were executed after table was created. It's not, and therefore it fails.</p>

<pre class=""lang-py prettyprint-override""><code>sdf.write \
  .format(""com.databricks.spark.sqldw"") \
  .option(""forwardSparkAzureStorageCredentials"", ""true"") \
  .option(""dbTable"", ""test_table"") \
  .option(""url"", url) \
  .option(""tempDir"", temp_dir) \
  .option(""preActions"", ""ALTER TABLE test_table ALTER COLUMN value NVARCHAR(MAX);"") \
  .save()
</code></pre>

<p>Also, <code>postActions</code> are executed after data is inserted, and therefore this will also fail.</p>

<p>Any ideas?</p>",1,0,2020-03-04 08:37:58.523000 UTC,,,5,pyspark|azure-databricks|azure-sqldw|azure-sql-data-warehouse|azure-synapse,1831,2011-12-07 09:26:25.137000 UTC,2022-01-05 15:10:28.630000 UTC,"Copenhagen, Denmark",2862,131,11,414,,,,,,[]
"Loop through Spark Dataframe, save results and use results on the previous iteration","<p>How can I loop through a spark dataframe, apply business logic and use the results in the next iteration. I'm moving a script from pandas/numpy to spark because of the amount of data we have to process in this job. The business logic we have is very complicated and I've been able to move it to spark. The issue I'm having is how can I carry the results from Group 1 below to group 2 to be used. Also, the problem isn't that simple, there are about 10 variables that depend on the past group that will be used in the current group's calculations. I've been thinking about maybe streaming in the groups and saving the results to a temp table of some sorts and then using the results on the next stream? Not sure how that would work yet. Any ideas?</p>

<p><a href=""https://i.stack.imgur.com/qyNMo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qyNMo.png"" alt=""enter image description here""></a></p>

<p>For added Context:</p>

<p>I have a dataframe with a ton of logic implemented into it.There's a column from 1 - 20. I have defined a ton of logic for Group 1. I need to pass in those same tranformations with the calculations in place to the next group 2 and so on and so on. Is it possible to pass the dataframe to a function with outputs?</p>",0,2,2019-11-05 20:30:56.367000 UTC,,2019-11-08 18:14:51.163000 UTC,0,apache-spark|pyspark-sql|azure-databricks,137,2013-07-17 14:05:21.790000 UTC,2021-05-17 04:29:11.983000 UTC,,49,3,0,9,,,,,,[]
How to update a Databricks Delta table with inner join in Databricks using Spark sql,"<p>I have to update a table column with inner join with other table.I have tried using the below sql.But i'm getting error in Databricks as (Error in SQL statement: ParseException: 
mismatched input '' expecting 'WHEN').I tried different ways of updating the table.Can someone help me on this issue how to fix this?</p>

<pre><code>%sql
merge into test a using test_1 b
on (a.id_num=b.id_num)
when matched then
update set a.name=b.name;
</code></pre>",4,3,2019-02-14 15:12:30.907000 UTC,,2019-02-15 11:53:55.520000 UTC,0,apache-spark-sql|azure-databricks,5676,2018-10-04 16:30:46.120000 UTC,2021-02-25 15:56:54.010000 UTC,,113,3,0,83,,,,,,[]
Breadth-first limiting on each node within a result in Gremlin?,"<h2>Background</h2>
<p>Looking at the below image, we're facing an issue with how we want to do our limiting at a breadth-level.</p>
<p>The goal is to ensure that off each neighbor, we never read more than <code>X</code> edges off the current node to avoid timeouts on nodes with a large amount of edges.</p>
<h2>Example</h2>
<p>We have a max-breadth limit of <code>X</code> where <code>X</code> is the number of neighbors we aggregate off a single node. We begin a BFS traversal from <code>0</code> and aggregate <code>3</code>, <code>1</code> and <code>2</code>.</p>
<p>Assuming our max-breath limit is <code>3</code>, the problem that can occur is that we first pull 1 and immediately begin reading <em>all</em> of 1's neighbors. As a result, we completely disregard the neighbors that could exist off of nodes <code>3</code> and <code>2</code> because <code>1</code> would fulfill the max-breadth.</p>
<h2>Question</h2>
<p>How can we, in Gremlin (in a single query), say that we want an edge limit for <em>each</em> node in my list of neighbors?</p>
<p>In other words, I want <code>X</code> neighbors from node 3, <code>X</code> neighbors from node 1 and <code>X</code> neighbors from node 2. This idea should hold true recursively up until some depth <code>D</code>.</p>
<h2>Attempt</h2>
<p><code>g.V(idsList).outE().limit(50).inV().dedup.by(T.id).fold()</code></p>
<p>The issue with the above is that we blindly limit all edges from all neighbors to <code>X</code> which can favor a single subgraph.</p>
<p><a href=""https://i.stack.imgur.com/GfbsP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GfbsP.png"" alt=""enter image description here"" /></a></p>",0,3,2021-10-11 16:58:29.413000 UTC,,,1,amazon-web-services|gremlin|amazon-neptune,38,2020-08-20 21:22:57.010000 UTC,2022-03-05 23:21:22.593000 UTC,"Seattle, WA, USA",655,57,5,91,,,,,,[]
Pyspark dataframe append to another csv file in StorageBlob with Azure Databricks,"<p>I have an xls file that I load from an Azure storage container into my Azure databricks. I've mounted the blob as a mount point to read this file. After some transformations into csv file it needs  to be appended to a master file.
I've tried the below codes</p>
<pre><code>final_df.select(cols).toPandas().to_csv(outfile,mode='a', header=False,index=False)
</code></pre>
<p>and also using spark.sql.dataframe.writer</p>
<pre><code>final_df.select(cols).coalesce(1).write.csv(outfile, mode='append',header=False)
</code></pre>
<p>In both the cases, there was not error or operation unsupported error but neither the file was appended to the master data file also. I've referred to some posts but they all mention about using python file open functions. Tried a similar way as below</p>
<pre><code>p_df = final_df.toPandas()
with open(outfile, 'a') as fd:
  p_df.to_csv(fd)
</code></pre>
<p>However this gave me an operation error as unsupported operation</p>
<pre><code>OSError                                   Traceback (most recent call last)
OSError: [Errno 95] Operation not supported

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
&lt;command-3424188746178105&gt; in &lt;module&gt;
      1 p_df = final_df.toPandas()
      2 with open(outfile, 'a') as fd:
----&gt; 3   p_df.to_csv(fd)

OSError: [Errno 95] Operation not supported
</code></pre>
<p>Is there a way to append the new data to the master csv file in Azure databricks notebook?</p>",0,0,2021-04-08 14:14:45.360000 UTC,1.0,,0,python|azure|append|azure-blob-storage|azure-databricks,200,2020-01-03 06:40:20.503000 UTC,2021-09-15 19:32:05.737000 UTC,"Melbourne VIC, Australia",81,8,0,26,,,,,,[]
How to maintain order while retrieving the data in sparql,"<p>Is there a way to ensure the order during the retrieval in sparql?</p>

<p>Say I want to store <code>List&lt;Literals&gt;</code>(order important) as an object</p>

<p><code>{:subj :pred 'o1'. :subj :pred 'o2'. :subj :pred 'o3'}</code></p>

<p>How can I insert the above data such that when I perform <code>select ?s ?p ?o where {?s ?p ?o}</code> the order shouldn't change?</p>

<p>Do sparql always preserve the insertion data order when retrieved?</p>

<p>In the above example, I used literals as an example. Is there a way of preserving the order of IRIs as well?</p>

<p>I have seen sparql documentation on <code>RDF.Seq</code>, But I didn't understand exactly how to use that. Please provide some simple insert and select queries for ensuring order or share any relevant resources.  </p>",2,2,2020-04-07 13:37:14.090000 UTC,,,2,sparql|amazon-neptune|rdf4j,375,2017-04-06 18:53:11.143000 UTC,2021-12-10 06:59:36.113000 UTC,"Gurgaon, Haryana, India",302,31,0,52,,,,,,[]
Alternative to combining where & and steps in gremlin query,"<p>I'm struggling to write a fast query that makes use of multiple predicates using an <code>and</code> step
in amazon neptune. The basic graph structure is below and is used for modelling biological data. The setup is that there are &quot;pathways&quot; which connect to &quot;enzymes&quot; which connect to &quot;reactions&quot; which connect to &quot;compounds&quot;. I'm trying to filter the pathways so that only those that connect to multiple compounds get returned e.g. find the pathways that are connected to both compound 1 and compound 2.</p>
<pre><code>g.addV('pathway').property('name', 'pathway 1').as('p1').
  addV('pathway').property('name', 'pathway 2').as('p2').
  addV('pathway').property('name', 'pathway 3').as('p3').
  addV('enzyme').property('name', 'enzyme 1').as('e1').
  addV('enzyme').property('name', 'enzyme 2').as('e2').
  addV('enzyme').property('name', 'enzyme 3').as('e3').
  addV('reaction').property('name', 'reaction 1').as('r1').
  addV('reaction').property('name', 'reaction 2').as('r2').
  addV('reaction').property('name', 'reaction 3').as('r3').
  addV('compound').property('name', 'compound 1').as('c1').
  addV('compound').property('name', 'compound 2').as('c2').
  addV('compound').property('name', 'compound 3').as('c3').
  addV('compound').property('name', 'compound 4').as('c4').
  addV('compound').property('name', 'compound 5').as('c5').
  addV('compound').property('name', 'compound 6').as('c6').
  addE('contains').from('p1').to('e1').
  addE('contains').from('p1').to('e2').
  addE('contains').from('p1').to('e3').
  addE('contains').from('p2').to('e1').
  addE('contains').from('p3').to('e2').
  addE('partof').from('e1').to('p1').
  addE('partof').from('e2').to('e1').
  addE('partof').from('e3').to('p1').
  addE('partof').from('e1').to('p2').
  addE('partof').from('e2').to('p3').
  addE('catalyzes').from('e1').to('r1').
  addE('catalyzes').from('e2').to('r2').
  addE('catalyzes').from('e3').to('r3').
  addE('substrate').from('c1').to('r1').
  addE('product').from('r1').to('c2').
  addE('substrate').from('c3').to('r2').
  addE('product').from('r2').to('c4').
  addE('substrate').from('c5').to('r3').
  addE('product').from('r3').to('c6')
</code></pre>
<p>My current solution is to start at the pathway nodes and use a combination of <code>where</code> and <code>and</code> steps to do the filtering:</p>
<pre><code>g.V().hasLabel('pathway').where(and(
  out('contains').hasLabel('enzyme').
    out('catalyzes').hasLabel('reaction').both().has('compound', 'name', 'compound 6'),
  out('contains').hasLabel('enzyme').
    out('catalyzes').hasLabel('reaction').both().has('compound', 'name', 'compound 4')
  )
).valueMap().toList()
</code></pre>
<p>This works fine and allows me to search for any number of compounds but is slow, taking multiple seconds to run the query.</p>
<p>In comparison if I start at the compound node and traverse to the pathway, it's almost instantaneous, but I don't know how to replicate the multiple predicates like above:</p>
<pre><code>g.V().has('compound', 'name', 'compound 6').both().
  in('catalyzes').out('partof').hasLabel('pathway').dedup().valueMap().toList()
</code></pre>
<p>For this toy dataset both queries are fast but in my production DB with 1000 pathways, 6000 enzymes, 10000 reactions and 50000 compounds the query can take 3-5 seconds to run.</p>
<p>Is there an alternative in amazon neptune to the <code>where</code>-<code>and</code> pattern I'm using for filtering based on multiple predicates that might get better performance?</p>",1,0,2020-07-08 01:03:32.993000 UTC,,,3,gremlin|amazon-neptune,55,2013-02-07 02:21:18.593000 UTC,2022-03-02 17:59:43.957000 UTC,,1494,595,3,98,,,,,,[]
PySpark UDF does not return expected result,"<p>I have a Databricks dataframe with multiple columns and a UDF that generates the contents of a new column, based on values from other columns.</p>
<p>A sample of the original dataset is:</p>
<pre><code>interval_group_id     control_value     pulse_value     device_timestamp

2797895314            5                 5               2020-09-12 09:08:44
0                     5                 5               2020-09-12 09:08:45
0                     6                 5               2020-09-12 09:08:46
0                     0                 5               2020-09-12 09:08:47
</code></pre>
<p>Now I am trying to add a new column, called <code>group_id</code>, based on some logic with the columns above. My UDF code is:</p>
<pre><code>@udf('integer')
def udf_calculate_group_id_new (interval_group_id, prev_interval_group_id, control_val, pulse_val):
  
  if interval_group_id != 0:
    return interval_group_id
  elif control_val &gt;= pulse_val and prev_interval_group_id != 0:
    return prev_interval_group_id
  else:
    return -1
</code></pre>
<p>And the new column being added to my dataframe is done with:</p>
<pre><code>df = df.withColumn('group_id'
                   , udf_calculate_group_id_new(
                          df.interval_group_id                                                   
                          , lag(col('interval_group_id')).over(Window.orderBy('device_timestamp'))            
                          , df.control_value
                          , df.pulse_value)
                  )
</code></pre>
<hr />
<p>My expected results are:</p>
<pre><code>interval_group_id    control_value    pulse_value   device_timestamp       group_id

2797895314           5                5             2020-09-12 09:08:44    2797895314
0                    5                5             2020-09-12 09:08:45    2797895314
0                    6                5             2020-09-12 09:08:46    2797895314
0                    0                5             2020-09-12 09:08:47    -1
</code></pre>
<p>However, the results of adding the new <code>group_id</code> column are:</p>
<pre><code>interval_group_id    control_value    pulse_value   device_timestamp       group_id

2797895314           5                5             2020-09-12 09:08:44    null
0                    5                5             2020-09-12 09:08:45    null
0                    6                5             2020-09-12 09:08:46    -1
0                    0                5             2020-09-12 09:08:47    -1
</code></pre>
<p>My <strong>goal</strong> is to propagate the value <code>2797895314</code> down the <code>group_id</code> column, based on the conditions mentioned above, but somehow this doesn't happen and the results are populated with <code>null</code> and <code>-1</code> incorrectly.</p>
<p>Is this a bug with UDF's or is my expectation of the way of working for the UDF incorrect? Or am I just bad at coding?</p>",0,5,2020-09-19 00:56:21.130000 UTC,,2020-09-26 20:03:02.190000 UTC,0,pyspark|azure-databricks,71,2012-03-13 14:33:17.553000 UTC,2022-03-05 23:26:56.340000 UTC,"Cluj-Napoca, Romania",18921,1730,16,2293,,,,,,[]
"I have try to connect python jupyter notebook with amazon neptune DB Instance, but I got an error like this, what should I do?","<p><strong>This code, I got from the Amazon Neptune Tutorial <a href=""https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-python.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-python.html</a></strong>
But, I got an error like this when I try to run the code in Jupyter Notebook (internal in my laptop).
<a href=""https://i.stack.imgur.com/EKIkm.png"" rel=""nofollow noreferrer"">This is my code</a></p>
<pre><code>---------------------------------------------------------------------------
HTTPError                                 Traceback (most recent call last)
&lt;ipython-input-1-fae80b27d2c6&gt; in &lt;module&gt;
     12 g = graph.traversal().withRemote(remoteConn)
     13 
---&gt; 14 print(g.V().limit(2).toList())
     15 
     16 remoteConn.close()

C:\ProgramData\Anaconda3\lib\site-packages\gremlin_python\process\traversal.py in toList(self)
     56 
     57     def toList(self):
---&gt; 58         return list(iter(self))
     59 
     60     def toSet(self):

C:\ProgramData\Anaconda3\lib\site-packages\gremlin_python\process\traversal.py in __next__(self)
     46     def __next__(self):
     47         if self.traversers is None:
---&gt; 48             self.traversal_strategies.apply_strategies(self)
     49         if self.last_traverser is None:
     50             self.last_traverser = next(self.traversers)

C:\ProgramData\Anaconda3\lib\site-packages\gremlin_python\process\traversal.py in apply_strategies(self, traversal)
    571     def apply_strategies(self, traversal):
    572         for traversal_strategy in self.traversal_strategies:
--&gt; 573             traversal_strategy.apply(traversal)
    574 
    575     def apply_async_strategies(self, traversal):

C:\ProgramData\Anaconda3\lib\site-packages\gremlin_python\driver\remote_connection.py in apply(self, traversal)
    147     def apply(self, traversal):
    148         if traversal.traversers is None:
--&gt; 149             remote_traversal = self.remote_connection.submit(traversal.bytecode)
    150             traversal.remote_results = remote_traversal
    151             traversal.side_effects = remote_traversal.side_effects

C:\ProgramData\Anaconda3\lib\site-packages\gremlin_python\driver\driver_remote_connection.py in submit(self, bytecode)
     54 
     55     def submit(self, bytecode):
---&gt; 56         result_set = self._client.submit(bytecode, request_options=self._extract_request_options(bytecode))
     57         results = result_set.all().result()
     58         side_effects = RemoteTraversalSideEffects(result_set.request_id, self._client,

C:\ProgramData\Anaconda3\lib\site-packages\gremlin_python\driver\client.py in submit(self, message, bindings, request_options)
    125 
    126     def submit(self, message, bindings=None, request_options=None):
--&gt; 127         return self.submitAsync(message, bindings=bindings, request_options=request_options).result()
    128 
    129     def submitAsync(self, message, bindings=None, request_options=None):

C:\ProgramData\Anaconda3\lib\site-packages\gremlin_python\driver\client.py in submitAsync(self, message, bindings, request_options)
    146         if request_options:
    147             message.args.update(request_options)
--&gt; 148         return conn.write(message)

C:\ProgramData\Anaconda3\lib\site-packages\gremlin_python\driver\connection.py in write(self, request_message)
     53     def write(self, request_message):
     54         if not self._inited:
---&gt; 55             self.connect()
     56         request_id = str(uuid.uuid4())
     57         result_set = resultset.ResultSet(queue.Queue(), request_id)

C:\ProgramData\Anaconda3\lib\site-packages\gremlin_python\driver\connection.py in connect(self)
     43             self._transport.close()
     44         self._transport = self._transport_factory()
---&gt; 45         self._transport.connect(self._url, self._headers)
     46         self._protocol.connection_made(self._transport)
     47         self._inited = True

C:\ProgramData\Anaconda3\lib\site-packages\gremlin_python\driver\tornado\transport.py in connect(self, url, headers)
     38         if headers:
     39             url = httpclient.HTTPRequest(url, headers=headers)
---&gt; 40         self._ws = self._loop.run_sync(
     41             lambda: websocket.websocket_connect(url, compression_options=self._compression_options))
     42 

~\AppData\Roaming\Python\Python38\site-packages\tornado\ioloop.py in run_sync(self, func, timeout)
    456         if not future_cell[0].done():
    457             raise TimeoutError('Operation timed out after %s seconds' % timeout)
--&gt; 458         return future_cell[0].result()
    459 
    460     def time(self):

~\AppData\Roaming\Python\Python38\site-packages\tornado\concurrent.py in result(self, timeout)
    236         if self._exc_info is not None:
    237             try:
--&gt; 238                 raise_exc_info(self._exc_info)
    239             finally:
    240                 self = None

~\AppData\Roaming\Python\Python38\site-packages\tornado\util.py in raise_exc_info(exc_info)

~\AppData\Roaming\Python\Python38\site-packages\tornado\stack_context.py in wrapped(*args, **kwargs)
    314             if top is None:
    315                 try:
--&gt; 316                     ret = fn(*args, **kwargs)
    317                 except:
    318                     exc = sys.exc_info()

~\AppData\Roaming\Python\Python38\site-packages\tornado\simple_httpclient.py in _on_timeout(self, info)
    305         error_message = &quot;Timeout {0}&quot;.format(info) if info else &quot;Timeout&quot;
    306         if self.final_callback is not None:
--&gt; 307             raise HTTPError(599, error_message)
    308 
    309     def _remove_timeout(self):
</code></pre>
<p>HTTPError: HTTP 599: Timeout while connecting</p>
<p>This is the error that I got.
What should I do?</p>",1,0,2021-01-27 04:45:47.347000 UTC,,2021-11-23 23:02:53.367000 UTC,0,jupyter-notebook|gremlin|amazon-neptune|gremlinpython|graph-notebook,354,2021-01-27 03:43:16.013000 UTC,2021-09-13 09:33:59.450000 UTC,"Jakarta, Indonesia",1,0,0,2,,,,,,[]
Pyspark: how to improve spatial intersection?,"<p>I am working with <code>pyspark</code> on Databriks where I have a table of data points that looks like the following</p>

<pre><code>pingsGeo.show(5)
+--------------------+--------------------+----------+--------------------+
|                  ID|               point|      date|            distance|
+--------------------+--------------------+----------+--------------------+
|00007436cf7f96cb1...|POINT (-82.640937...|2020-03-19|0.022844737780675896|
|00007436cf7f96cb1...|POINT (-82.641281...|2020-03-19|3.946137920280456...|
|00007436cf7f96cb1...|POINT (-82.650238...|2020-03-19| 0.00951798692682881|
|00007436cf7f96cb1...|POINT (-82.650947...|2020-03-19|7.503617154519347E-4|
|00007436cf7f96cb1...|POINT (-82.655853...|2020-03-19|0.007148426134394903|
+--------------------+--------------------+----------+--------------------+

root
 |-- ID: string (nullable = true)
 |-- point: geometry (nullable = false)
 |-- date: date (nullable = true)
 |-- distance: double (nullable = false)
</code></pre>

<p>And another table of polygons (from shapefile)</p>

<pre><code>zoneShapes.show(5)
+--------+--------------------+
|COUNTYNS|            geometry|
+--------+--------------------+
|01026336|POLYGON ((-78.901...|
|01025844|POLYGON ((-80.497...|
|01074088|POLYGON ((-81.686...|
|01213687|POLYGON ((-76.813...|
|01384015|POLYGON ((-95.152...|
</code></pre>

<p>I would like to assign to each point a <code>COUNTYNS</code></p>

<p>I am doing it with <code>geospark</code> functions. I am doing the following:</p>

<pre><code>queryOverlap = """"""
        SELECT p.ID, z.COUNTYNS as zone,  p.date, p.point, p.distance
        FROM pingsGeo as p, zoneShapes as z
        WHERE ST_Intersects(p.point, z.geometry))
    """"""

spark.sql(queryOverlap).show(5)
</code></pre>

<p>This query works in a small dataset but it fails for a larger one.</p>

<pre><code>org.apache.spark.SparkException: Job aborted due to stage failure: Task 117 in stage 51.0 failed 4 times, most recent failure: Lost task 117.3 in stage 51.0 (TID 4879, 10.17.21.12, executor 13): org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 16384 bytes of memory, got 0
</code></pre>

<p>I am wondering if there is a way to optimize the process.</p>",1,0,2020-03-29 06:19:19.450000 UTC,1.0,,1,apache-spark|pyspark|geospatial|azure-databricks|geospark,492,2014-04-30 16:00:53.307000 UTC,2022-03-03 14:37:36.293000 UTC,"Boston, MA",5561,98,7,794,,,,,,[]
How do I tell Git it got moves/renames wrong (false positives),"<p>I'm merging two branches in git, having a week worth of work each and some of the files were moved or renamed and modified.</p>

<p>Git seems to get the stuff completely wrong in some cases and it says file a was moved to file b when in reality they are totally unrelated.</p>

<p>I'm having quite a few such false positives (roughly 25%). I would like to influence the algorithm Git uses to figure out whether a file was moved, renamed or is new.</p>

<ul>
<li>I'd like to assign higher priority to names. (file a was moved from folder foo to folder b) yet git insists that it was renamed to be file b in folder foo - totally unrelated)</li>
<li>I'd like to bump up the similarity index above which Git considers a file to be rename/move</li>
</ul>

<p>I know I read somewhere I can do the latter and I hope I can do the former too, but my googling skills are failing me today.</p>",1,2,2011-04-14 09:43:52.897000 UTC,5.0,,12,git|version-control|merge|dvcs,3852,2008-09-16 17:26:24.700000 UTC,2021-12-10 01:19:08.007000 UTC,Australia,26924,973,176,6571,,,,,,[]
Amazon Neptune full text search query not working as expected,"<p>I am trying to implement a full-text search for Neptune DB using elasticsearch manually but getting this error :</p>
<pre><code>{&quot;requestId&quot;:&quot;bcb16f6b-7e60-4e71-b0d8-a6a4a9b38b00&quot;,&quot;code&quot;:&quot;MalformedQueryException&quot;,&quot;detailedMessage&quot;:&quot;Failed to interpret Gremlin query: null&quot;}
</code></pre>
<p>Here is my document:</p>
<pre><code>{
    &quot;entity_id&quot;: &quot;f8b9726f-74f9-a0e0-5fbd-b609bbb14f89&quot;,
    &quot;entity_type&quot;: [
        &quot;suggestions&quot;
    ],
    &quot;document_type&quot;: &quot;vertex&quot;,
    &quot;predicates&quot;: {
        &quot;title&quot;: {
            &quot;value&quot;: &quot;samsung mobile&quot;
        }
    }
}
</code></pre>
<p>query:</p>
<pre><code>g.withSideEffect('Neptune#fts.endpoint','elasticsearch cluster end point').withSideEffect('Neptune#fts.queryType', 'match').V().has('title','Neptune#fts samsung').local(values('title').fold()).limit(5).valueMap().toList()
</code></pre>
<p>it is giving error only if I am putting an existing word in search i.e Samsung but if I am searching for an unavailable word it worked fine not throwing any error.
Not sure what is wrong here, can anyone help me with this?</p>",1,4,2020-09-26 09:27:18.003000 UTC,,2020-10-01 08:22:53.287000 UTC,2,elasticsearch|gremlin|amazon-neptune,217,2017-05-30 16:03:29.947000 UTC,2022-03-03 10:51:32.050000 UTC,"Agra, Uttar Pradesh, India",1134,66,9,79,,,,,,[]
How to get output parameter from Executed Pipeline in ADF?,"<p>I have a databricks pipeline that will give an output, but at the moment, I need run the databricks from the Executed Pipelines, when I tried to run it, my databricks output didn't show up on Executed Pipelines ? Is this pipeline can't show the output ?</p>
<p>So this is my Databricks output result.</p>
<p>[![enter image description here][1]][1]</p>
<p>And this is my Executed Pipeline.</p>
<p><a href=""https://i.stack.imgur.com/esvEr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/esvEr.png"" alt=""enter image description here"" /></a></p>
<p>How can I get the <code>runOutput</code> result from Executed Pipeline ?</p>",2,0,2021-07-05 07:43:57.263000 UTC,1.0,2021-12-08 06:09:05.513000 UTC,3,azure|azure-data-factory|azure-databricks,1459,2018-11-16 03:30:26.920000 UTC,2022-03-02 02:49:02.267000 UTC,,517,26,5,135,,,,,,[]
AWS Neptune reader endpoint fails to work,"<p>I have a Kubernetes pod communicating to a neptune cluster over a transit gateway. Querying and storing data over the primary cluster endpoint works without any issues at all, however, swapping to a reader endpoint just times out. This has been happening for over 24 hours now.</p>
<p>What could be the root cause of this? This is a java-based application running gremlin 3.4.10 on the latest Neptune engine.</p>
<p>I'm getting same results locally with credentials and on remote server. Again, the primary endpoint works in both environments, but the read-only does not work at all.</p>",1,0,2021-06-15 21:19:47.760000 UTC,,,0,amazon-web-services|gremlin|amazon-neptune,31,2020-08-20 21:22:57.010000 UTC,2022-03-05 23:21:22.593000 UTC,"Seattle, WA, USA",655,57,5,91,,,,,,[]
Git checkout to external work tree and remove deleted files,"<p>We want to use Git to deploy code on our webserver. Therefore, we have a initialized a bare repository on our production server. Whenever we release a new version, we perform a git checkout into the DocumentRoot of the website:</p>

<pre><code>git --work-tree=/path/to/webroot/ checkout -f master
</code></pre>

<p>In the subdirectories of <code>webroot</code>, there are several files which are not tracked by Git (Cache files, user-uploaded files etc.). These must of course not be deleted by Git when performing the checkout (and this part works fine so far).</p>

<p>However, Git also does not delete files which were previously tracked, but have been removed in the meantime (e.g. they were deleted in the development process because they are no longer needed). Such files currently survive the <code>checkout</code> process, leading to a steadily increasing number of ""dead"" files. Is there a way to make Git delete such files when performing the <code>checkout</code>?</p>

<p><strong>EDIT - steps to reproduce:</strong></p>

<pre><code># create dirs and repos
cd /base/path
mkdir repo.git
mkdir target
cd repo.git &amp;&amp; git init

# create, add and commit two files
touch test1.txt
touch test2.txt
git add test1.txt test2.txt
git commit -m testcommit

# checkout to target directory
git --work-tree=../target checkout master -f
# target directory now contains both files

# remove one file from file system and git repo, commit
rm test2.txt
git rm test2.txt
git commit -m deletecommit

# checkout to target again
git --work-tree=../target checkout master -f
# target still contains both files
</code></pre>",3,5,2015-06-26 15:08:50.220000 UTC,1.0,2015-06-26 22:52:58.063000 UTC,4,git|dvcs|git-checkout,1151,2013-08-13 16:14:13.843000 UTC,2022-02-01 19:35:54.957000 UTC,,1365,23,16,106,,,,,,[]
Azure Databricks to Azure SQL DW connection,"<p>Based on Databicks documentation at this link <a href=""https://docs.databricks.com/data/data-sources/azure/sql-data-warehouse.html"" rel=""nofollow noreferrer"">https://docs.databricks.com/data/data-sources/azure/sql-data-warehouse.html</a>  , to be able to connect from Azure Databricks to Azure SQL DW , Database user should have ""control"" permissions. This is practically not possible because ""control"" permissions on entire DB can't be given to individual user connecting to SQL DW from Databicks. Is there an alternative solution for this?</p>",1,0,2019-12-15 16:48:54.793000 UTC,,2019-12-25 06:59:36.287000 UTC,1,azure-databricks|azure-sqldw,181,2019-12-13 15:14:40.590000 UTC,2020-10-05 00:39:29.827000 UTC,,11,0,0,1,,,,,,[]
How to write a JSON to Azure queue from Azure Databricks,"<p>I'm trying to read a JSON file from BLOB and write that file in Azure queue. The reading part works fine but while writing it throws an error.</p>

<p>I've already tried the URL of the queue folder in which I'm trying to write, as parameter for .save()</p>

<p>Here's my code:</p>

<pre><code>storage_account_name=""mrktmabcdestaaue""
storage_account_access_key=""myurl==""
file_location=""wasbs://myfolder@mrktmabcdestaaue.blob.core.windows.net/input.json""
file_type=""json""
spark.conf.set(
        ""fs.azure.account.key.""+storage_account_name+"".blob.core.windows.net"",
  storage_account_access_key)
df = spark.read.option(""multiline"", ""true"").format(file_type).load(file_location)

df.write.mode(""overwrite"").format(""com.databricks.spark.json"").save(""wasbs://myqueue@mrktmabcdestaaue.queue.core.windows.net"")
</code></pre>

<p>My Input Json:</p>

<pre><code>{
""Name"": ""Abc"",
""Age"": 18,
""City"": ""def""
}
</code></pre>

<p>The error message I'm getting is:</p>

<p>""shaded.databricks.org.apache.hadoop.fs.azure.AzureException: shaded.databricks.org.apache.hadoop.fs.azure.AzureException: Unable to access container myqueue in account mrktmabcdestaaue.queue.core.windows.net using anonymous credentials, and no credentials found for them in the configuration.""</p>",3,0,2019-10-30 04:53:34.290000 UTC,,,2,azure|azure-servicebus-queues|azure-databricks,1159,2019-10-30 04:01:01.680000 UTC,2021-09-17 16:02:28.480000 UTC,"Kolkata, West Bengal, India",43,1,0,8,,,,,,[]
How to handle mergeschema option for differing datatypes in Databricks?,"<pre><code>import spark.implicits._

val data = Seq((&quot;James&quot;,&quot;Sales&quot;,34))
val df1 = data.toDF(&quot;name&quot;,&quot;dept&quot;,&quot;age&quot;)
df1.printSchema()
df1.write.option(&quot;mergeSchema&quot;, &quot;true&quot;).format(&quot;delta&quot;).save(&quot;/location&quot;)

val data2 = Seq((&quot;Tiger&quot;,&quot;Sales&quot;,&quot;34&quot;) )
var df2 = data2.toDF(&quot;name&quot;,&quot;dept&quot;,&quot;age&quot;)
df2.printSchema()
df2.write.option(&quot;mergeSchema&quot;, &quot;true&quot;).format(&quot;delta&quot;).save(&quot;/location&quot;)
df2.show(false)
</code></pre>
<p>When we write the df2 dataframe, it fails because in the delta table age is of IntergerType and the second df2 age is of StringType. How do we handle such sitaution so that the code handles this case smoothly.</p>",1,2,2021-12-17 04:04:22.657000 UTC,,,0,dataframe|scala|azure-databricks|delta-lake,32,2017-10-08 06:43:25.163000 UTC,2022-03-04 05:08:47.690000 UTC,,89,9,0,9,,,,,,[]
Subtraction of multiple date ranges in graphdb,"<p>I am trying to learn graphdb and compare its capabilities with relational databases. Consider the following problem:</p>
<p>I have two lists of date ranges: date-in and date-out:</p>
<p>Date-in date ranges:</p>
<ul>
<li>1/1/2000-12/31/2025</li>
<li>1/1/2026-12/31/2030</li>
<li>2/1/2030-12/31/2033</li>
</ul>
<p>Date-out date ranges:</p>
<ul>
<li>2/1/2005-12/31/2020</li>
<li>1/1/2024-12/31/2026</li>
</ul>
<p><strong>The calculation that I want to do is to subtract all the Date-out date ranges from the Date-in date ranges, meaning that I want to know all the date ranges described in the Date-in ranges that are not described in the Date-out ranges.</strong> Note the date ranges may overlap.</p>
<p>The correct answer is:</p>
<ul>
<li>1/1/2000-1/31/2005</li>
<li>1/1/2021-12/31/2023</li>
<li>1/1/2027-12/31/2030</li>
<li>2/1/2030-12/31/2023</li>
</ul>
<p><strong>I know how to solve this problem using a relational db such as Postgres. The relational db solution would be:</strong></p>
<ol>
<li><p>Use generate_series() to list all the days described in the Date-in ranges and Date-out date. There are ~33 years here, so about 12,000 days total.</p>
</li>
<li><p>Select all the days from the Date-in days list that are not present in the Date-out days list. This should be fast, because again, it's only ~33 years, so about 12,000 days total.</p>
</li>
<li><p>Use the &quot;island&quot; detection SQL query <a href=""https://www.red-gate.com/simple-talk/sql/t-sql-programming/the-sql-of-gaps-and-islands-in-sequences/"" rel=""nofollow noreferrer"">https://www.red-gate.com/simple-talk/sql/t-sql-programming/the-sql-of-gaps-and-islands-in-sequences/</a> to find contiguous date ranges from the resulting list of days from Step 2.</p>
</li>
</ol>
<p><strong>I don't have an approach to solve this problem using a graphdb gremlin traversal. I tried the same approach in graphdb as in the relational db, but there is no generate_series() in the AWS Neptune implementation of graphdb that I'm aware of. Furthermore I didn't want to add nodes to a graphdb just to run a read type query like this one.</strong></p>
<p><strong>Is there a graphdb solution for this problem?</strong></p>",1,1,2020-09-02 00:34:58.193000 UTC,,2020-09-04 11:53:02.967000 UTC,3,gremlin|graph-databases|amazon-neptune,70,2014-06-18 16:52:17.130000 UTC,2020-09-06 05:32:54.920000 UTC,,719,18,0,45,,,,,,[]
Preventing version merges between VCS branches,"<p>there's an ""issue"" (actually a nuisance, but a major one) with our development process.</p>

<p>We've setup a good continuous integration-deployment-delivery pipeline; basically, our process is like that:</p>

<p>New development happens on the <strong>default</strong> branch (we use Mercurial, but git would have the same issue); usually we develop just one small improvement there;</p>

<p>Once the improvement is ready and tested (something that usually happens twice or three times a week, our cycle is pretty short), it's merged into the <strong>release</strong> branch (which actually stands for <em>release candidate</em>), it's tested again in a production-like environment and if it passes all our testing it's pushed to the production environment.</p>

<p>If we need to make a bugfix, we do it on the release branch, we test it and then push it to the production env.</p>

<p>An artifact (usually an RPM package) is built from each branch; when there's a release in production, the very artifact from the release branch is promoted to our public repository. There's no branch for the production environment, because we usually don't need it (there's a very short timespan when something it's on the release branch but it's not in production; code doesn't linger there unattended).</p>

<p>In this context, there's a small issue for us.</p>

<p>In order to distinguish packages, we usually set version to 1 for packages on the release branch, and 2 to packages on the default branch. Our CI system then adds its build number, so we've got packages like <em>oursoftware-1.0-build37</em> for release and <em>oursoftware-1.0-build902</em> for default. Such version is written either in our .spec file or in our pom.xml files (or other files for different softwares, like .gemspec or simila)</p>

<p>Whenever we want to put something in release candidate, we merge the default branch to the release branch.</p>

<p>What happens there? <strong>the version gets merged as well</strong>. So after each merge we need to get back at the file containing the version and put the old version in them. We should need to do it just once (Mercurial does not try to re-merge things that were already merged) but whenever we need to do a merge from release from default (e.g. in order to merge a fix done on release to the dev branch) the problem rises again.</p>

<p>I'd like some advice on how to prevent this problem completely; <strong>how do I set versions outside of my branches?</strong> I sometimes actually need to change them (e.g. I want to change from 1 to 2 on release and from 2 to 3 on default) and I wouldn't like to hardcode them in our CI system, I'd like some system which is ""outside"" the scope of branches and it's kind of ""global"" for a repository. Or I would need something like ""ignore modification for some files"".</p>

<p>In many situations I actually need versions in my files as well; for example I cannot omit version info from a Maven pom.xml file or from a .gemspec, otherwise it won't work when developing.</p>

<p>Thanks,</p>",1,0,2014-06-06 12:56:29.257000 UTC,,,0,version-control|mercurial|continuous-integration|dvcs|continuous-deployment,34,2009-11-26 08:57:22.500000 UTC,2022-02-25 21:07:00.580000 UTC,"Trieste, Italy",2878,53,12,315,,,,,,[]
Databricks JSON column,"<p>In Databricks I've a Json data in a column. I want to read JSON data in each record, find if JSON column is exists and if exists I want to change the value and update it in the same record column. Can some please help?</p>
<p>check if number column is exists
if exists update the number &quot;XXXXXX&quot;</p>
<p>JSON:
<a href=""https://i.stack.imgur.com/MfXEx.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://i.stack.imgur.com/xG41U.png"" rel=""nofollow noreferrer"">enter image description here</a></p>",1,0,2022-02-16 18:02:07.830000 UTC,,,1,python|azure-databricks,25,2018-10-08 19:36:25.717000 UTC,2022-02-21 20:57:17.253000 UTC,"London, UK",11,0,0,1,,,,,,[]
Using PlasticSCM (or any DVCS client) to connect to TFS,"<p>Has anyone used the PlasticSCM client tools to work against a TFS repository?</p>

<p>Basically my current client is using the new hosted TFS for version control (not TFS-Git, just standard TFS), but I have seen the light of distributed version control systems on previous projects and now can't go back!</p>

<p>Is there a recommended approach to getting a DVCS style experience with a standard TFS backend?</p>

<p>I know I am asking a lot, but I have done some research and there seems to be hints that it maybe possible (to sync PlasticSCM with TFS) all over the PlasticSCM forums, but I have found nothing concrete.</p>

<p>This youtube video looks very promising. With checkins made to Plastic appearing in TFS and vice versa (via a command line sync operation). <a href=""http://www.youtube.com/watch?v=AJKF3cjg7jA"" rel=""nofollow"">http://www.youtube.com/watch?v=AJKF3cjg7jA</a></p>",3,0,2013-05-15 22:54:06.600000 UTC,2.0,2013-05-15 23:00:14.677000 UTC,3,git|tfs|mercurial|dvcs|plasticscm,750,2008-08-10 17:13:08.703000 UTC,2015-06-28 09:12:55.607000 UTC,"London, United Kingdom",1655,54,2,174,,,,,,[]
Gremlin search for vertexes related to 2 or more specific nodes,"<p>I'm trying to produce a Gremlin query whereby I need to find vertexes which have edges from specific other vertexes. The less abstract version of this query is I have user vertexes, and those are related to group vertexes (i.e subjects in a school, so students who are in ""Year 6 Maths"" and ""Year 6 English"" etc). An extra difficulty is the ability for subgroups to exist in this query.</p>

<p>The query I need to find those users who are in 2 or more groups specified by the user.</p>

<p>Currently I have a brief solution, but in production usage using Amazon Netpune this query performs way too poorly, even with a small amount of data. I'm sure there's a simpler way of achieving this :/</p>

<pre><code>g.V()
.has('id', 'group_1')
.repeat(out(""STUDENT"", ""SUBGROUP""))
.until(hasLabel(""USER""))
.aggregate(""q-1"")
.V()
.has('id', 'group_2')
.repeat(out(""STUDENT"", ""SUBGROUP""))
.until(hasLabel(""USER""))
.where(within(""q-1""))
.aggregate(""q-2"")
.V()
.hasLabel(USER)
.where(within(""q-2""))
# We add some more filtering here, such as search terms
.dedup()
.range(0, 10)
.values(""id"")
.toList()
</code></pre>",1,0,2019-01-28 09:34:26.440000 UTC,,,0,graph|gremlin|amazon-neptune,66,2009-12-12 20:21:19.030000 UTC,2022-03-05 22:49:23.643000 UTC,"Seaham, UK",2501,218,22,411,,,,,,[]
Azure databricks connect to on-prem databases through ADF,"<p>Is possible to connect an Azure Databricks notebook with Azure Data Factory linkedservices (connections to on prem DBs)?</p>
<p>On ADF, I have connections to on prem gateways through linked services to connect to local DBs. I need to connect my Databricks notebook with that linked services on ADF. It is possible?</p>
<p>Regards.</p>",1,1,2020-09-17 03:19:23.487000 UTC,1.0,,1,azure-data-factory|azure-databricks,66,2020-06-12 16:47:01.187000 UTC,2021-12-27 18:04:25.073000 UTC,Argentina,19,0,0,2,,,,,,[]
pyspark join fails when (Azure) databricks runtime version changes,"<p>I have a databricks notebook and in one of the cells, there are many joins.</p>
<p>pseudo code:</p>
<pre><code>df_final = df_final.join(dfTest1, on ['ID], how = 'left').drop(dfTest1.ID) \
                   .join(dfTest2, on ['ID], how = 'left').drop(dfTest2.ID) \
                   .join(dfTest3, on ['ID], how = 'left').drop(dfTest3.ID) 
</code></pre>
<p>The same command runs successfully with 6.6ML (includes Spark 2.4.5, scala 2.11) but fails with run time version 7.3 LTS ML (includes Apache Spark 3.0.1, scala 2.12)</p>
<p>Any one faced this issue? How to overcome this issue? Thanks.</p>",0,5,2020-11-26 09:52:39.937000 UTC,,,0,azure|pyspark|azure-databricks,62,2019-08-15 06:38:39.327000 UTC,2022-03-03 11:02:35.683000 UTC,,769,82,0,142,,,,,,[]
Databricks and SQL server issue with token,"<p>I need your help to create a &quot;permanently&quot; connection from databricks to sql server database in Azure.<br />
I have a code in pyspark to connect to database, using driver &quot;com.microsoft.sqlserver.jdbc.spark&quot; and JAR spark_mssql_connector_2_12_3_0_1_0_0_alpha.jar.<br />
I have created a class to connect to DB is via token</p>
<pre class=""lang-py prettyprint-override""><code>
class SQLSpark():
    database_name: str = &quot;&quot;
    sql_service_name: str = &quot;&quot;
    service_principal_id: str = &quot;&quot;
    service_principal_secret: str = &quot;&quot;
    tenant_id: str = &quot;&quot;
    authority: str = &quot;&quot;
    state = None
    except_error = None

    def __init__(self, database_name, service_principal_id, service_principal_secret, tenant_id,
                 authority, spark, sql_service_name=None):

        self.database_name = database_name
        self.sql_service_name = sql_service_name
        self.service_principal_id = service_principal_id
        self.service_principal_secret = service_principal_secret
        self.tenant_id = tenant_id
        self.authority = authority
        self.state = True
        self.except_error = &quot;&quot;       
        self._spark_session = spark

        context = adal.AuthenticationContext(self.authority)
        token = context.acquire_token_with_client_credentials(&quot;https://database.windows.net&quot;, self.service_principal_id,
                                                              self.service_principal_secret)
        self._access_token = token[&quot;accessToken&quot;]

        server_name = &quot;jdbc:sqlserver://&quot; + self.sql_service_name + &quot;.database.windows.net&quot;
        self._url = server_name + &quot;;&quot; + &quot;databaseName=&quot; + self.database_name + &quot;;&quot;


    def select_table(self, table, sql_query):
        try:
            logger.info(f&quot;Reading table {table} in DB {self.database_name} &quot;)
            df = self._spark_session.read.format(&quot;com.microsoft.sqlserver.jdbc.spark&quot;) \
                    .options(
                    url=self._url,
                    databaseName=self.database_name,
                    accessToken=self._access_token,
                    hostNameInCertificate=&quot;*.database.windows.net&quot;,
                    query=sql_query) \
                    .load()

            self.custom_logger.info(f&quot;Table {table} in database {self.database_name} has been read&quot;)
            return df
        except Exception as ex:
            logger.error(f&quot;Failed to read table  {table}&quot;)
            logger.error(ex)
</code></pre>
<p>The problem is that I have to process huge data and processes took more that 1h to process and database token expired. Is there a way to refresh the token when I call to <code>select_table</code> method?<br />
Error given is:<br />
<code>Caused by: com.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user '&lt;token-identified principal&gt;'. Token is expired.</code></p>
<p>Full error:</p>
<pre><code>Py4JJavaError: An error occurred while calling o9092.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 59.0 failed 4 times, most recent failure: Lost task 0.3 in stage 59.0 (TID 2611, 10.139.64.5, executor 0): com.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user '&lt;token-identified principal&gt;'. Token is expired. ClientConnectionId:009909b8-d779-4df2-b077-59cf4c4b3c73
    at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:262)
    at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:283)
    at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:129)
    at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection.sendLogon(SQLServerConnection.java:5173)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:3810)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection.access$000(SQLServerConnection.java:94)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection$LogonCommand.doExecute(SQLServerConnection.java:3754)
    at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7225)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3053)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:2562)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:2216)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:2067)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1204)
    at com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:825)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:64)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:272)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:356)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:320)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:356)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:320)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:356)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:320)
    at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)
    at org.apache.spark.scheduler.Task.run(Task.scala:117)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:655)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:658)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)
    at scala.Option.foreach(Option.scala:407)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
Caused by: com.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user '&lt;token-identified principal&gt;'. Token is expired. ClientConnectionId:009909b8-d779-4df2-b077-59cf4c4b3c73
    at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:262)
    at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:283)
    at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:129)
    at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection.sendLogon(SQLServerConnection.java:5173)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:3810)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection.access$000(SQLServerConnection.java:94)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection$LogonCommand.doExecute(SQLServerConnection.java:3754)
    at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7225)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3053)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:2562)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:2216)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:2067)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1204)
    at com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:825)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:64)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:272)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:356)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:320)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:356)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:320)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:356)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:320)
    at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)
    at org.apache.spark.scheduler.Task.run(Task.scala:117)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:655)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:658)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
</code></pre>",1,2,2022-01-13 11:19:06.937000 UTC,,,0,sql-server|apache-spark|azure-sql-database|azure-databricks,84,2021-01-11 09:14:54.407000 UTC,2022-03-04 13:24:05.293000 UTC,,99,19,0,21,,,,,,[]
Malloc Failed in Git Clone,"<p>Using Git version 2.19.1.windows.1 on Windows 7 64 bit. </p>

<p>I am trying to run in <em>D:/Users/Della/Documents/Python_Scripts/</em></p>

<pre><code>git clone https://gitlab.com/forkingpin/dui-dashboard.git
</code></pre>

<p>on the git bash. The repository contains only a 59 bytes ReadMe so far. The error message I am getting reads </p>

<pre><code>fatal: Out of memory, malloc failed (tried to allocate 1744830464 bytes)
fatal: not a git repository: 'D:/Users/Della/Documents/Python_Scripts/
</code></pre>

<p>What is the way out?</p>

<p>Points to note. </p>

<ul>
<li>The same command is working like a charm on my Linux Desktop</li>
<li><p>I had a similar issue while pushing the committed changes (on windows) in another repository. I corrected that by appending .git/config with </p>

<pre><code>[http]                                                         
      postbuffer = 5m
</code></pre></li>
</ul>

<p>But that was for an existing repository. I cannot have the config file before I clone it. </p>",1,0,2018-12-10 04:29:15.827000 UTC,1.0,2018-12-10 05:46:21.233000 UTC,1,git|version-control|gitlab|git-bash|dvcs,104,2013-07-21 13:20:15.283000 UTC,2022-03-03 07:18:29.467000 UTC,"İstanbul, Turkey",1009,41,2,193,,,,,,[]
Databricks REST API returns HTTP 400 error (with a AAD Access Token),"<p>I am trying to access to the REST API of Databricks with an Active Directoy Access Token.</p>

<p>To do so, from the <strong>Active Directory -> App Registration -> AAD App:</strong></p>

<p>In API Permissions I have added the AzureDatabricks API</p>

<p><a href=""https://i.stack.imgur.com/L2fTU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L2fTU.png"" alt=""enter image description here""></a></p>

<p>Then I am trying to access to any data from the Databricks REST API, but always I am getting the same error:</p>

<p><strong>""io.jsonwebtoken.security.SignatureException: JWT signature does not match locally computed signature. JWT validity cannot be asserted and should not be trusted.""</strong></p>

<p>One of the endpoint I am trying to request is: </p>

<pre><code>https://adb-XXXXXXXXXXXXXXX.azuredatabricks.net/api/2.0/token/list
</code></pre>

<p>This Access Token let me do requests to my Graph API, so I guess the token is OK. If my Access Token expires, returns that the token has expired... </p>

<p>To do all tries I am using Insomnia (similar to Postman). </p>

<p>What am I doing wrong? Do I need to do something more? </p>

<p>Thanks beforehand </p>",1,1,2020-06-17 10:11:15.990000 UTC,,,1,azure|azure-active-directory|azure-api-management|azure-databricks,1046,2016-06-23 15:40:19.077000 UTC,2022-02-28 11:52:12.390000 UTC,,367,33,0,60,,,,,,[]
Installing Janitor library in Azure Databricks,"<p>I have Python 3.7 installed.
Trying to install <code>janitor</code> library in <code>Azure DataBricks</code>. It works properly in my local machine, but have difficulty to be installed in <code>Azure DataBricks</code>.</p>
<p>I run <code>dbutils.library.installPyPI('janitor')</code>, but got the below error:
<code>ModuleNotFoundError: No module named 'ConfigParser'</code>. I tried <code>butils.library.installPyPI('mysqlclient')</code>, as mentioned in 'https://stackoverflow.com/questions/14087598/python-3-importerror-no-module-named-configparser', but didn't work.</p>",2,2,2021-10-07 02:43:04.740000 UTC,,,1,python|azure-databricks,124,2015-04-03 01:24:14.297000 UTC,2022-03-05 04:10:47.057000 UTC,,371,12,0,52,,,,,,[]
GremlinPython add connection and request timeout to DriverRemoteConnection with aiohttp,"<p>I am upgrading the gremlinpython package from 3.4 to 3.5</p>
<p>As part of the upgrade, tornado has been removed and only aiohttp is supported.</p>
<p>Before, to create a DriverRemoteConnection with a connection and request timeout, I used the following code</p>
<pre><code>from tornado import httpclient

req = httpclient.HTTPRequest(
    connection_str,
    connect_timeout=gremlin_connect_timeout_secs, 
    request_timeout=gremlin_request_timeout_secs
)
driver_remote_connection = DriverRemoteConnection(
    req, &quot;g&quot;, pool_size=pool_size, max_workers=max_workers
)
g = traversal().withRemote(driver_remote_connection)
</code></pre>
<p>Now that tornado isn't supported anymore, what would be the equivalent in 3.5+?</p>
<p>My connection is to AWS Neptune.</p>
<p>I've looked into the aiogremlin package and the aiohttp library but it requires me to create asynchronous client while I don't need for the connection to be asynchronous.</p>
<p>I can create the DriverRemoteConnection with simply this</p>
<pre><code>driver_remote_connection = DriverRemoteConnection(
    self.conn, pool_size=self.pool_size, max_workers=self.max_workers
)
</code></pre>
<p>But then I'm not able to pass a connection/request timeout.</p>",1,0,2022-03-03 13:16:24.420000 UTC,,,0,aiohttp|amazon-neptune|gremlinpython,13,2013-10-17 08:08:48.680000 UTC,2022-03-04 14:00:50.413000 UTC,"London, United Kingdom",225,11,0,24,,,,,,[]
Get DataBricks Notebook Run id,"<p>I have a Databricks NoteBook task in Azure datafactory, while it executes I want to fetch its run id and save it as a new field in the destination table. Is there a way you can suggest, Thanks.</p>",0,2,2021-10-27 05:52:07.400000 UTC,,,0,azure-databricks|azure-data-factory-pipeline,115,2021-03-04 16:43:21.617000 UTC,2022-02-04 06:59:35.870000 UTC,"Henrico, VA, USA",31,0,0,1,,,,,,[]
Understanding git - why even commit locally?,"<p>I know the difference between commit vs push (local vs remote repo)</p>

<p>I come from a SubVersion background: Why even have the option to commit ""locally""? What is the point of that? Wouldn't it make sense to just always commit to the remote repository? </p>

<p>I don't understand why we have this intermediate local repository?</p>",3,5,2015-06-12 13:11:35.677000 UTC,,2015-06-12 13:30:57.130000 UTC,1,git|version-control|dvcs,84,2013-04-24 21:10:45.317000 UTC,2022-03-06 04:04:20.800000 UTC,,2748,225,11,117,,,,,,[]
The best practice to be followed when reading data from azure datalake gen1 through azure databricks,"<p>I am new to azure databricks. I was trying to read data from datalake into databricks. I found that there are mainly two methods </p>

<ol>
<li>Mounting the file present in datalake into dbfs (Advantage being Authentication required just once)</li>
<li>Using Service Principal and OAuth (Authentication required for each request)</li>
</ol>

<p>I am interested to know if there is some significant memory consumption when we choose to mount folders in dbfs. I learnt that the data mounted is persisted . So I guessing that might lead to some memory consumption. I'll like if somebody can explain me what's going on the backend when we mount a file in dbfs</p>",1,0,2020-02-27 18:30:28.530000 UTC,,,0,azure|azure-data-lake|azure-databricks,87,2018-05-09 05:34:31.210000 UTC,2022-03-03 19:37:02.097000 UTC,"Kolkata, West Bengal, India",1,0,0,5,,,,,,[]
aws neptune times out for large graph drop (),"<p>There are some threads on this subject already..
particularly <a href=""https://stackoverflow.com/questions/49877928/drop-all-edges-on-aws-neptune-using-pythongremlin"">this one</a></p>

<p>but is there any recommended solution to drop large graph other than batching..?
I tried increasing timeout and it doesn't work</p>

<p>Below is the example..</p>

<hr>

<p>gremlin> g.V().count()</p>

<p>==>5230885</p>

<p>gremlin> g.V().drop().iterate()</p>

<p>{""requestId"":""77c64369-45fa-462f-91d7-5712e3308497"",""detailedMessage"":""A timeout occurred within the script during evaluation of [RequestMessage{, requestId=77c64369-45fa-462f-91d7-5712e3308497, op='eval', processor='', args={gremlin=g.V().drop().iterate(), bindings={}, batchSize=64}}] - consider increasing the timeout"",""code"":""TimeLimitExceededException""}
Type ':help' or ':h' for help.
Display stack trace? [yN]N</p>

<p>gremlin> g.E().count()</p>

<p>==>83330550</p>

<p>gremlin> :remote config timeout none</p>

<p>==>Remote timeout is disabled</p>

<p>gremlin> g.E().drop().iterate()</p>

<p>{""requestId"":""d418fa03-72ce-4154-86d8-42225e4b9eca"",""detailedMessage"":""A timeout occurred within the script during evaluation of [RequestMessage{, requestId=d418fa03-72ce-4154-86d8-42225e4b9eca, op='eval', processor='', args={gremlin=g.E().drop().iterate(), bindings={}, batchSize=64}}] - consider increasing the timeout"",""code"":""TimeLimitExceededException""}
Type ':help' or ':h' for help.
Display stack trace? [yN]N</p>",3,0,2019-08-01 13:58:57.073000 UTC,1.0,,5,gremlin|amazon-neptune,1518,2012-04-17 17:50:37.913000 UTC,2022-01-31 01:56:27.010000 UTC,"Chicago, IL, USA",2726,68,4,210,,,,,,[]
"errorMessage“: ”'Neptune' object has no attribute 'stop_db_cluster""","<p>Even which is not running on boto3 version 1.12.19 updated version</p>

<p>depend on this ticket, <a href=""https://stackoverflow.com/questions/60528110/errormessage-neptune-object-has-no-attribute-stop-db-cluster"">errorMessage&quot;: &quot;&#39;Neptune&#39; object has no attribute &#39;stop_db_cluster&quot;</a></p>",1,1,2020-03-12 13:34:58.537000 UTC,,2020-04-08 14:08:48.933000 UTC,0,boto3|amazon-neptune,26,2014-05-14 04:50:20.920000 UTC,2021-03-28 02:17:10.330000 UTC,Chennai,23,0,0,2,,,,,,[]
Need for Azure Blob Storage in Azure Databricks running Spark clusters,"<p>I'm working with Spark clusters on Azure Databricks ecosystem having Azure Blob Storage associated with it. Also, there is Databricks File System (DBFS) associated with Databricks. I wanted to know is there a need to have an Azure Blob Storage for storing data? Is DBFS not enough to store the files/data?</p>",1,0,2019-03-05 10:29:55.823000 UTC,,,0,python|apache-spark|cluster-computing|azure-blob-storage|azure-databricks,56,2016-01-27 15:07:06.643000 UTC,2020-11-09 18:39:49.130000 UTC,,309,8,0,25,,,,,,[]
How to project on multiple different node values with Gremlin,"<p>I have an performance issue when I try to get result from a projection with gremlin. My approach is for sure false. But I understand well the issue.</p>
<p>I have a data models like :</p>
<p><a href=""https://i.stack.imgur.com/tz77f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tz77f.png"" alt=""enter image description here"" /></a></p>
<p>I want to get a table for all D :</p>
<p>V1 | D.id | V2 | V3 | V4 | V5 | V6</p>
<p>To do it I have try a request like (every circles is a node):</p>
<pre><code>g.V().hasLabel(A)
.out().hasLabel(B).as('B_node')
.out().hasLabel(C)
.out().hasLabel(V1).values('value1').as('value1')
.select('B_node')
.out().hasLabel(D)
.project('V1', 'D.id', 'V2', 'V3', 'V4', 'V5', 'V6')
.by(select('value1'))
.by(id())
.by(out().hasLabel(E).out().hasLabel(V2).values('value2'))
.by(out().hasLabel(F).out().hasLabel(V3).values('value3'))
.by(out().hasLabel(F).out().hasLabel(V4).values('value4'))
.by(out().hasLabel(G).out().hasLabel(V5).values('value5'))
.by(out().hasLabel(G).out().hasLabel(V6).values('value6'))
</code></pre>
<p>The problem is the number of node D and the number of node out of D which is large. I understand for each D I will execute multiple times the loop to find F and to find G. How can I avoid this ? and create an alias to do the loop only one times ?</p>
<p>If I'm not clear do not hesitate to ask me questions.</p>",1,4,2020-12-18 15:29:24.843000 UTC,,,0,optimization|graph|gremlin|tinkerpop|amazon-neptune,145,2015-08-05 10:57:40.773000 UTC,2022-03-04 11:30:43.810000 UTC,,402,18,0,38,,,,,,[]
To delete the vertex by looping the dataframe in glue timeout,"<ol>
<li>I want to delete the vertex to loop on one dataframe.</li>
<li>Suppose I will delete the vertex based on some cols of dataframe
my function is written in this way: and it is timeout </li>
</ol>

<pre><code>    def delete_vertices_for_label(rows):
        conn = self.remote_connection()
        g = self.traversal_source(conn)
        for row in rows:
            entries = row.asDict()
            create_traversal = __.hasLabel(str(entries[""~label""]))
            for key, value in entries.iteritems():
                if key=='~id':
                    pass
                elif key == '~label':
                    pass
                else:
                    create_traversal.has(key), value)
            g.V().coalesce(create_traversal).drop().iterate()
</code></pre>

<p>I have succeed in using this function locally on tinkerGraph, however ,when I try to run above function in glue which manipulate data in aws Neptune ; it failed.
I also create one lambda function in below: still meet the issue like timeout.</p>

<pre><code>     def run_sample_gremlin_basedon_property():
        remoteConn = DriverRemoteConnection('ws://' + CLUSTER_ENDPOINT + "":"" + 
        CLUSTER_PORT + '/gremlin', 'g')
        graph = Graph()
        g = graph.traversal().withRemote(remoteConn)
        create_traversal = __.hasLabel(""Media"")
        create_traversal.has(""Media_ID"", ""99999"")
        create_traversal.has(""src_name"", ""NET"")
        print (""create_traversal:"",create_traversal)
        g.V().coalesce(create_traversal).drop().iterate()


</code></pre>",1,5,2019-04-23 13:04:10.840000 UTC,,2019-11-04 04:00:43.853000 UTC,0,amazon-web-services|amazon-neptune,46,2017-08-05 01:23:36.733000 UTC,2022-03-03 21:07:44.963000 UTC,,505,404,1,103,,,,,,[]
"Looking at Projectlocker, Cloudforge, and Assembla Portfolio for Git Hosting - Any opinions?","<p>We're a small webdesign shop here and we want to get all our clients on a DVCS system that deploys our commits via sftp.</p>

<p>I've narrowed it down to Projectlocker, CloudForge and Assembla Portfolio.</p>

<p>There's also Beanstalk and Springloops but they'd end up being 2-4x the price of the other options for us. (300 projects is a lot to these hosts even though we're only 10-15gb)</p>

<p>Does anybody have any experience with Projectlocker, Cloudforge and/or Assembla Portfolio?</p>

<p>Is there a better solution that I'm missing out on? Thanks.</p>",3,0,2012-10-25 23:59:10.267000 UTC,,,2,git|dvcs|assembla|projectlocker,1452,2012-02-09 17:49:03.413000 UTC,2012-12-21 17:39:53.267000 UTC,,19,0,0,6,,,,,,[]
Is it possible to delete rows using JDBC?,"<p>Using spark jdbc connection, I am able to either read or write data. For example:</p>
<pre><code>app_project_model_df = (
  sqlContext.read.format(&quot;jdbc&quot;)
  .option(&quot;url&quot;,sqlURL)
  .option(&quot;driver&quot;,&quot;com.microsoft.sqlserver.jdbc.SQLServerDriver&quot;)
  .option(&quot;dbtable&quot;,&quot;app.projectmodel&quot;)
  .option(&quot;user&quot;, dbuser)
  .option(&quot;password&quot;, dbpassword)
  .load()
)
</code></pre>
<p>However, is it possible to perform other SQL queries the very same way? Let's say <code>delete from app.projectmodel where key = 6</code>. Or do I need to use pure python for that?</p>
<p>I wasn't able to find any documentation on that. Thank you in advance.</p>",2,1,2021-08-08 12:37:20.037000 UTC,1.0,2021-08-08 15:37:04.383000 UTC,0,python|apache-spark|pyspark|azure-databricks,114,2017-11-20 18:20:15.443000 UTC,2022-03-05 13:03:45.133000 UTC,,473,29,0,106,,,,,,[]
How to write large Spark DataFrame(1.2 GB 14M rows) to an MSSQL Server tbl? My current solution takes around 10 hours,"<p><strong>Problem:</strong>
<em>1.2 GB (14 Millions  records) stored in an apache spark dataframe. The computation took less than 1 min but the writing to an MSSQL SERVER table (non-indexed) takes more than 10 hours.</em> <strong>Hardware:</strong> (1-VM 8-vcpus, 64 GB memory, SSD). Question:The following have been tried without success, would you have a brilliant ideas, suggestion or even a simple one to help? Thanks</p>
<pre><code>def FUNC_A1(df):  
    start = time.time()
    jdbc_url = f&quot;jdbc:sqlserver://{config.get('mydb',
    'host')}:1434;database={config.get('mydb', 'database')}&quot;
    df.select(&quot;F1&quot;, &quot;F2&quot;, &quot;F3&quot;, &quot;F4&quot;, &quot;F5&quot;, &quot;F6&quot;, &quot;F7&quot;, &quot;F8&quot;, &quot;F9&quot;)\ 
      .write.format(&quot;jdbc&quot;).mode(&quot;overwrite&quot;) \
      .option(&quot;driver&quot;, &quot;com.microsoft.sqlserver.jdbc.SQLServerDriver&quot;)\
      .option(&quot;url&quot;, jdbc_url) \
      .option(&quot;dbtable&quot;, &quot;dbo.tblAFS&quot;) \
      .option(&quot;user&quot;, config.get('mydb', 'username'))\
      .option(&quot;password&quot;, config.get('mydb', 'password'))\
      .save()
</code></pre>
<p>The preceeding code creates the table, but when trying to insert the records it fails and  has generated the following error message;</p>
<p><a href=""https://i.stack.imgur.com/02bDa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/02bDa.png"" alt=""generated the following error message"" /></a></p>
<p><strong>Option #1</strong> <em>(this code take 9 to 10 hours to complete, it’s writing the actual results(dataframe) into the table.</em></p>
<hr />
<pre><code>     # Execute insert into tblAFS for each row in dataframe
     # Using fast execute

     start = time.time()
     df = df.select(&quot;F1&quot;, &quot;F2&quot;, &quot;F3&quot;, &quot;F4&quot;, &quot;F5&quot;,
          &quot;F6&quot;, &quot;F7&quot;, &quot;F8&quot;, &quot;F9&quot;).na.fill(0)
     conn = pyodbc.connect(shared.get_odbcconn(config))
     cursor = conn.cursor()
     cursor.fast_executemany = True
     collected = df.rdd.toLocalIterator()
     counter = 1
     for row in collected:
        cursor.execute(&quot;&quot;&quot;
               INSERT INTO dbo.tblAFS 
               ([F1],[F2],[F3],[F4],[F5],[F6],[F7],[F8],[F9])
               VALUES (?,?,?,?,?,?,?,?,?)&quot;&quot;&quot;,
                 row[&quot;F1&quot;], row[&quot;F2&quot;], row[&quot;F3&quot;], row[&quot;F4&quot;], row[&quot;F5&quot;],
                 row[&quot;F6&quot;], row[&quot;F7&quot;], row[&quot;F8&quot;], row[&quot;F9&quot;])
        counter = counter + 1
    conn.commit()
    conn.close()
</code></pre>
<p><strong>Option #2</strong> _ takes as much time as Option #1_</p>
<pre><code>def FUNC_C1(df):
    # 1. Create csv file for each F2 partition
    # 2. Bulk insert all files generated in table tblAFS

    start = time.time()
    file_path = config.get('action', 'stats_results')
    df = df.select(&quot;F1&quot;, &quot;F2&quot;, &quot;F3&quot;, &quot;F4&quot;, &quot;F5&quot;,
                   &quot;F6&quot;, &quot;F7&quot;, &quot;F8&quot;, &quot;F70&quot;).na.fill(0)

    df.write.mode(&quot;overwrite&quot;).options(header=True).csv(file_path)
    conn = pyodbc.connect(shared.get_odbcconn(config))
    cursor = conn.cursor()
    cursor.fast_executemany = True
    files = glob(f&quot;{file_path}/*.csv&quot;)
    counter = 1
    for file in files:
        cursor.execute(f&quot;BULK INSERT dbo.tblAFS FROM '{getcwd()}/{file}' 
             WITH (FORMAT = 'CSV', FIRSTROW = 2)&quot;)
        counter = counter+1
    conn.commit()
    conn.close()
</code></pre>
<h1>The following ideas have been considered but have not been implemented with a significant time reduction in processing time.</h1>
<hr />
<ul>
<li>rdd.collect() should not be used in this case as it will collect all
data as an Array in the driver, which is the easiest way to get out
of memory.</li>
<li>rdd.coalesce(1).saveAsTextFile() should also not be used as the
parallelism of upstream stages will be lost to be performed on a
single node, where data will be stored from.</li>
<li>rdd.coalesce(1, shuffle = true).saveAsTextFile() is the best simple
option as it will keep the processing of upstream tasks parallel and
then only perform the shuffle to one node
(rdd.repartition(1).saveAsTextFile() is an exact synonym).</li>
<li>rdd.saveAsSingleTextFile() as provided bellow additionally allows one
to store the rdd in a single file with a specific name while keeping
the parallelism properties of rdd.coalesce(1, shuffle =
true).saveAsTextFile().</li>
</ul>",1,1,2020-12-23 02:17:55.610000 UTC,,2020-12-23 06:23:23.417000 UTC,1,sql-server|apache-spark|apache-spark-sql|azure-databricks|sql-server-2019,234,2020-12-22 19:15:35.133000 UTC,2021-02-20 20:36:52.720000 UTC,,19,0,0,9,,,,,,[]
nested json to tsv in databricks pyspark,"<p>Want to convert a nested json to tsv in databricks notebook using pysoark.</p>

<p>Below is json structure where columns can be changed.</p>

<pre><code>{""tables"":[{""name"":""Result"",""columns"":[{""name"":""JobTime"",""type"":""datetime""},{""name"":""Status"",""type"":""string""}]
,""rows"":[
[""2020-04-19T13:45:12.528Z"",""Failed""]
,[""2020-04-19T14:05:40.098Z"",""Failed""]
,[""2020-04-19T13:46:31.655Z"",""Failed""]
,[""2020-04-19T14:01:16.275Z"",""Failed""],
[""2020-04-19T14:03:16.073Z"",""Failed""],
[""2020-04-19T14:01:16.672Z"",""Failed""],
[""2020-04-19T14:02:13.958Z"",""Failed""],
[""2020-04-19T14:04:41.099Z"",""Failed""],
[""2020-04-19T14:04:41.16Z"",""Failed""],
[""2020-04-19T14:05:14.462Z"",""Failed""]
]}
]}
</code></pre>

<p>I am new in databricks Please help</p>",1,1,2020-05-14 06:04:31.693000 UTC,,,0,python|pyspark|apache-spark-sql|azure-databricks|pyspark-dataframes,110,2017-09-21 08:49:05.220000 UTC,2021-10-27 09:32:25.250000 UTC,,193,3,0,72,,,,,,[]
Is there a limit to how many requests can be sent to Azure Data Lake (gen 1)?,"<p>I have a pipeline in Azure Data Factory that moves data from Google BigQuery(GBQ) to Azure Data Lake (gen 1) and in between does some cleaning in Azure Databricks.</p>
<p><a href=""https://i.stack.imgur.com/8luKC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8luKC.png"" alt=""pipeline image"" /></a></p>
<p>The first copy activity copies data from GBQ to Data lake, then the data goes through Databricks, and finally, the last activity copies the data to a blob container.</p>
<p>Out of the 4 initial copy activities, one randomly fails with the following error</p>
<blockquote>
<p>Failure happened on 'Sink' side.
ErrorCode=UserErrorAdlsFileWriteFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Writing
to 'AzureDataLakeStore' failed. Path:
/.../.../PageTracking_06072021.csv. Message: The remote server
returned an error: (403) Forbidden.. Response details:
{&quot;RemoteException&quot;:{&quot;exception&quot;:&quot;AccessControlException&quot;,&quot;message&quot;:&quot;
[......] Access Denied :
/../../PageTracking_06072021.csv[.....]&quot;,&quot;javaClassName&quot;:&quot;org.apache.hadoop.security.AccessControlException&quot;}},Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Net.WebException,Message=The
remote server returned an error: (403) Forbidden.,Source=System,'</p>
</blockquote>
<p>When I run the pipeline again, the failed one succeeds and others fail with the same error.</p>
<p><strong>What I have tried so far.</strong></p>
<p>Tried deleting the files and running fresh, but the first time everything succeeds and the cycle repeats itself.
Tried changing the sequence of activities (like you see in the image). I still get the same error randomly.</p>
<p>Access cannot be the issue because the same IR and configuration are being used in all the activities.</p>
<p><strong>Update:</strong></p>
<p>I have set up a trigger (once daily) for the pipeline and the pipeline runs fine. The problem happens only when I try to run the pipeline manually.</p>",1,0,2021-07-09 16:57:10.360000 UTC,,2021-07-12 13:26:29.297000 UTC,0,azure|azure-data-factory|azure-databricks|azure-data-lake,90,2020-04-02 23:03:26.933000 UTC,2022-03-05 08:52:48.190000 UTC,"Gurgaon, Haryana, India",1,0,0,0,,,,,,[]
Connecting to AWS Neptune,"<p>In the official aws neptune documentation they have mentioned that we can connect to neptune outside vpc but a security group has to be defined for that. Though somewhere it is mentioned that you cannot connect to it from outside vpc and if want to connect then it is only possible through EC2. 
So now I am a bit confused and want to know can we connect to AWS Neptune from outside VPC but not through EC2 via a Java application running on any server or local? </p>",1,3,2018-07-31 11:18:42.623000 UTC,1.0,,0,java|amazon-web-services|graph-databases|amazon-vpc|amazon-neptune,727,2018-07-31 11:10:43.343000 UTC,2018-09-12 11:58:55.753000 UTC,,21,0,0,7,,,,,,[]
Spark:join condition with Array (nullable ones),"<p>I have 2 Dataframes and would like to join them and would like to filter the data, i want to filter the<br>
data where OrgTypeToExclude is matching with respect to each transactionid.</p>

<p>in single word my transactionId is join contiions and  OrgTypeToExclude is exclude condition,sharing a simple example here </p>

<pre><code>import org.apache.spark.sql.functions.expr
import spark.implicits._
val jsonstr =""""""{

  ""id"": ""3b4219f8-0579-4933-ba5e-c0fc532eeb2a"",
  ""Transactions"": [
    {
      ""TransactionId"": ""USAL"",
      ""OrgTypeToExclude"": [""A"",""B""]
    },
    {
      ""TransactionId"": ""USMD"",
      ""OrgTypeToExclude"": [""E""]
    },
    {
      ""TransactionId"": ""USGA"",
      ""OrgTypeToExclude"": []
    }
    ]   
}""""""
val df = Seq((1, ""USAL"",""A""),(4, ""USAL"",""C""), (2, ""USMD"",""B""),(5, ""USMD"",""E""), (3, ""USGA"",""C"")).toDF(""id"", ""code"",""Alp"")
val json = spark.read.json(Seq(jsonstr).toDS).select(""Transactions.TransactionId"",""Transactions.OrgTypeToExclude"")

df.printSchema()
json.printSchema()
df.join(json,$""code""&lt;=&gt; $""TransactionId"".cast(""string"") &amp;&amp; !exp(""array_contains(OrgTypeToExclude, Alp)"") ,""inner"" ).show()

  --Expecting output
 id  Code    Alp 
 4   ""USAL""  ""C""
 2   ""USMD""  ""B""
 3   ""USGA""  ""C""
</code></pre>

<p>Thanks,
Manoj.</p>",2,1,2020-05-07 23:45:47.850000 UTC,0.0,2020-05-08 02:51:17.543000 UTC,0,scala|apache-spark|join|apache-spark-sql|azure-databricks,57,2013-03-22 19:12:27.517000 UTC,2022-03-03 19:58:16.283000 UTC,"Hyderabad, Telangana, India",51,2,0,24,,,,,,[]
Mercurial Release Management,"<p>I am using mercurial for source control. I want to have a main dev branch, and then have points in time that align with say ""v1.0"" ""v1.01"" and ""v2.0"", so that at any time I can pull down say ""v2.0"" and crush some bugs on it. I have heard some people say I need tags, some say I need bookmarks, others say I need named branches, and still others say I just need to maintain multiple cloned repositories. </p>

<p>From my point of view, multiple cloned repos seems like a poor choice because one thing I like about DVCS is that once you clone ""the"" repo, you have all past history and can totally restore from someones laptop if your central server burns down. If your repo is split up all over the place, I feel like you lose this benefit, unless you expect people to clone 5 repos and maintain them on their computers locally. This concerns me because the majority of people say that this is a good way to do it, yet it logically doesn't make sense to me. (I understand this is not a proper way to do backups, but not having full access to a part of the repo without going back to the server seems odd to me)</p>

<p>So to me, the way to keep everything together must be either tags, named branches, or bookmarks. However, I can't seem to differentiate these. People tend to explain bookmarks as ""kinda like tags, with some caveats"" and named branches as some kind of a moving tag that is probably better done with clones. </p>

<p>I really like git style branching (single repo, multiple branches off of it), however, I do not want to resort to weird plugins or hacks to make it look like git. I want to understand the proper mercurial way. </p>

<p>Bonus: how do ""small-scale"" branches fit into the mix, i.e. you want to work on a small feature in its own branch?</p>",5,0,2010-11-09 00:38:18.797000 UTC,6.0,,10,mercurial|dvcs,749,2010-11-09 00:38:18.813000 UTC,2010-12-15 03:28:57.153000 UTC,,173,0,0,4,,,,,,[]
Missing rows while processing records using foreachBatch in Spark Structured Streaming in databricks,"<p>I am new to real time scenarios and I need to create a spark structured streaming jobs in databricks. I am trying to apply some rule based validations from backend configurations on each incoming JSON message. I need to do the following actions on the incoming JSON</p>
<ol>
<li>I need to flatten the incoming JSON</li>
<li>Based on the source property in JSON, I need to fetch the validations configured for the source and apply it on the message</li>
<li>I need to log the validation errors to a synapse table</li>
<li>If there are no validation errors the data needs to be saved to a separate table based on the backend config.</li>
</ol>
<p>I tried implementing this using the foreachBatch in databricks notebook. My code snippets looks as below:</p>
<ol>
<li>Reading from EventHub</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>streamingDF = spark.readStream.format(&quot;eventhubs&quot;).options(**ehConf).load() 
</code></pre>
<ol start=""2"">
<li>Starting the foreachbatch query</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>streamingDF.writeStream.outputMode(&quot;append&quot;) \
  .option(&quot;checkpointLocation&quot;,&quot;dbfs:/FileStore/checkpoint/rtdqchekpoint&quot;) \
  .foreachBatch(processStream).start().awaitTermination()
</code></pre>
<p>3.processStream function</p>
<pre class=""lang-py prettyprint-override""><code>msgDF = df.select(df.body.cast(&quot;string&quot;).alias(&quot;msgBody&quot;))
msgDfVal = msgDF.collect()
messages = msgDfVal[0][0]
properties = df.select(df.properties).collect()
sourceFeedName = properties[0][0]['sourceFeedName']
ruleConfigDF = getBusinessRulesforSource(sourceFeedName)
dfJson = spark.read.json(sc.parallelize([messages]))
srcDqDataDf = flatten(dfJson)
errorDetails = applyBusinessRule(srcDqDataDf, ruleConfigDF)
if(errorDetails != None and len(errorDetails) &gt;0):
    errCnt = len(errorDetails)
    saveErrorDetailsToSynapse(errorDetails)
saveDatatoSynapse(srcDqDataDf)
</code></pre>
<p>However while processing i found that only a few incoming messages are coming into the processStream  function. We found that msgDF.collect() was causing the issue. Can someone help on how to fix this.</p>",0,0,2021-12-20 08:58:15.500000 UTC,1.0,2022-01-12 03:40:13.553000 UTC,1,spark-streaming|azure-databricks|azure-eventhub,107,2020-04-02 08:52:31.047000 UTC,2022-03-02 05:09:54.260000 UTC,,83,0,0,9,,,,,,[]
Spark fails to merge parquet files (INTEGER -> DECIMAL),"<p>I've got 2 parquets files.</p>
<p>The first one contains the following column: <strong>DECIMAL: decimal(38,18) (nullable = true)</strong></p>
<p>The second one has the same column, but with a different type: <strong>DECIMAL: integer (nullable = true)</strong></p>
<p>I want to merge them, but I can't simply read them separatedly and throw a cast into the specific column, because this is part of an app that receives lots of distinct parquet schemas. I need something that would cover every scenario.</p>
<p>I am reading both like this:</p>
<pre><code>df = spark.read.format(&quot;parquet&quot;).load(['path_to_file_one', 'path_to_file_2'])
</code></pre>
<p>It fails with the error below when I try to display the data</p>
<blockquote>
<p>Parquet column cannot be converted. Column: [DECIMAL], Expected:
DecimalType(38,18), Found: INT32</p>
</blockquote>
<p>I am using Azure Databricks with the following configs:</p>
<ul>
<li>DBR: 7.1</li>
<li>Spark 3.0.0</li>
</ul>
<p>I have uploaded the parquet files here: <a href=""https://easyupload.io/m/su37e8"" rel=""nofollow noreferrer"">https://easyupload.io/m/su37e8</a></p>
<p>Is there anyway I can force spark to autocast null columns into the type of the same column in the other dataframe?</p>
<p>It should be easy, all the columns are nullable...</p>",1,2,2020-11-05 19:30:48.513000 UTC,,2020-11-06 14:47:24.957000 UTC,1,apache-spark|pyspark|azure-databricks,436,2015-09-15 08:29:24.193000 UTC,2021-08-26 20:22:38.807000 UTC,"São Paulo, State of São Paulo, Brazil",328,306,3,79,,,,,,[]
AWS Neptune Typecasting,"<p>How to typecast an vertex property value in AWS Neptune. Taking into account the initial type of the vertex is string, I want to convert that to Integer.</p>

<p>I tried some of the available resources in Stackoverflow for the typecasting as below.</p>

<pre><code>g.V().values('code').map{(''+it).toInteger()}
</code></pre>

<p>But this is throwing error mentioning</p>

<blockquote>
  <p>error message : token recognition error at: 'it)'""}</p>
</blockquote>

<p>It seems it is unable to parse the ""it"" after ""+"".</p>

<p>Is there a direct way that this could be achieved in Neptune using Gremlin.</p>",1,0,2020-01-29 14:28:14.253000 UTC,,2020-01-31 21:18:47.213000 UTC,1,gremlin|amazon-neptune,160,2019-01-10 13:46:21.157000 UTC,2020-12-02 16:58:02.900000 UTC,,63,2,0,10,,,,,,[]
How to read CSV file without caring delimiter and create data frame with Azure Databricks(Python)?,"<p>I would have CSV file. I would like to read entire row (as single string) and parse string and create data frame with columns and then save CSV file. Reason is that there is encoding issues in CSV file and cannot read it properly. How to read CSV as single column? How to parse based on pipes and colons and form data frame?</p>
<p>Shape123|&quot;MULTIPOLYGON (((496000 6908000, 495000 6908000, 495000 6909000, 496000 6909000, 496000 6908000)))&quot;|&quot;Red&quot;|&quot;Long&quot;|&quot;208336&quot;|&quot;5&quot;|&quot;-1&quot;</p>",2,0,2021-01-23 12:17:05.617000 UTC,,,0,azure-databricks,166,2016-11-04 09:17:30.693000 UTC,2022-03-04 08:19:44.720000 UTC,Finland,1157,106,0,333,,,,,,[]
How to write to HBase in Azure Databricks?,"<p>I'm trying a build a lambda architecture using 'kafka-spark-hbase'. I'm using azure cloud and the components are on following platforms
1. Kafka (0.10) -HDinsights 
2. Spark (2.4.3)- Databricks
3. Hbase (1.2)- HDinsights</p>

<p>All the 3 components are under the same V-net so there is no issue in connectivity.
I'm using spark structured streaming, and successfully able to connect to Kafka as a source.</p>

<p>Now as spark does not provide native support to connect to Hbase, I'm using 'Spark Hortonworks Connector' to write data to Hbase, and I have implemented the code to write a batch to hbase in ""foreachbatch"" api provided in spark 2.4 onward.</p>

<p>The code is as below:</p>

<pre><code>import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.expr
import org.apache.spark.sql.functions._
import org.apache.spark.sql.streaming.OutputMode
import org.apache.spark.sql.streaming.Trigger
import org.apache.spark.sql.Dataset
import org.apache.spark.SparkConf
import org.apache.spark.sql.DataFrame

//code dependencies
import com.JEM.conf.HbaseTableConf
import com.JEM.constant.HbaseConstant

//'Spark hortonworks connector' dependency
import org.apache.spark.sql.execution.datasources.hbase._



    //---------Variables--------------//
    val kafkaBroker = ""valid kafka broker""
    val topic = ""valid kafka topic""
    val kafkaCheckpointLocation = ""/checkpointDir""

 //---------code--------------//
    import spark.sqlContext.implicits._

    val kafkaIpStream = spark.readStream.format(""kafka"")
      .option(""kafka.bootstrap.servers"", kafkaBroker)
      .option(""subscribe"", topic)
      .option(""checkpointLocation"", kafkaCheckpointLocation)
      .option(""startingOffsets"", ""earliest"")
      .load()

    val streamToBeSentToHbase = kafkaIpStream.selectExpr(""cast (key as String)"", ""cast (value as String)"")
      .withColumn(""ts"", split($""key"", ""/"")(1))
      .selectExpr(""key as rowkey"", ""ts"", ""value as val"")
      .writeStream
      .option(""failOnDataLoss"", false)
      .outputMode(OutputMode.Update())
      .trigger(Trigger.ProcessingTime(""30 seconds""))
      .foreachBatch { (batchDF: DataFrame, batchId: Long) =&gt;
        batchDF
          .write
          .options(Map(HBaseTableCatalog.tableCatalog -&gt; HbaseTableConf.getRawtableConf(HbaseConstant.hbaseRawTable), HBaseTableCatalog.newTable -&gt; ""5""))
          .format(""org.apache.spark.sql.execution.datasources.hbase"").save
      }.start()

</code></pre>

<p>Form the logs I can see that code is successfully able to get the data but when it tries to write to Hbase I get the following exception.</p>

<pre><code>19/10/09 12:42:48 ERROR MicroBatchExecution: Query [id = 1a54283d-ab8a-4bf4-af65-63becc166328, runId = 670f90de-8ca5-41d7-91a9-e8d36dfeef66] terminated with error
java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/NamespaceNotFoundException
    at org.apache.spark.sql.execution.datasources.hbase.DefaultSource.createRelation(HBaseRelation.scala:59)
    at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:72)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:70)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:88)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:146)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:134)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:187)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:183)
    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:134)
    at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:116)
    at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:116)
    at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:710)
    at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:710)
    at org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:111)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:240)
    at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:97)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:170)
    at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:710)
    at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:306)
    at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:292)
    at com.JEM.Importer.StreamingHbaseImporter$.$anonfun$main$1(StreamingHbaseImporter.scala:57)
    at com.JEM.Importer.StreamingHbaseImporter$.$anonfun$main$1$adapted(StreamingHbaseImporter.scala:53)
    at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:36)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5$$anonfun$apply$17.apply(MicroBatchExecution.scala:568)
    at org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:111)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:240)
    at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:97)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:170)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5.apply(MicroBatchExecution.scala:566)
    at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:251)
    at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:61)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch(MicroBatchExecution.scala:565)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(MicroBatchExecution.scala:207)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:175)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:175)
    at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:251)
    at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:61)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:175)
    at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:169)
    at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:296)
    at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.NamespaceNotFoundException
    at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 44 more
19/10/09 12:42:48 INFO SparkContext: Invoking stop() from shutdown hook
19/10/09 12:42:48 INFO AbstractConnector: Stopped Spark@42f85fa4{HTTP/1.1,[http/1.1]}{172.20.170.72:47611}
19/10/09 12:42:48 INFO SparkUI: Stopped Spark web UI at http://172.20.170.72:47611
19/10/09 12:42:48 INFO StandaloneSchedulerBackend: Shutting down all executors
19/10/09 12:42:48 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
19/10/09 12:42:48 INFO SQLAppStatusListener: Execution ID: 1 Total Executor Run Time: 0
19/10/09 12:42:49 INFO SQLAppStatusListener: Execution ID: 0 Total Executor Run Time: 0
19/10/09 12:42:49 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/10/09 12:42:49 INFO MemoryStore: MemoryStore cleared
19/10/09 12:42:49 INFO BlockManager: BlockManager stopped
19/10/09 12:42:49 INFO BlockManagerMaster: BlockManagerMaster stopped
19/10/09 12:42:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/10/09 12:42:49 INFO SparkContext: Successfully stopped SparkContext
19/10/09 12:42:49 INFO ShutdownHookManager: Shutdown hook called
19/10/09 12:42:49 INFO ShutdownHookManager: Deleting directory /local_disk0/tmp/temporaryReader-79d0f9b8-c380-4141-9ac2-46c257c6c854
19/10/09 12:42:49 INFO ShutdownHookManager: Deleting directory /local_disk0/tmp/temporary-d00d2f73-96e3-4a18-9d5c-a9ff76a871bb
19/10/09 12:42:49 INFO ShutdownHookManager: Deleting directory /local_disk0/tmp/spark-b92c0171-286b-4863-9fac-16f4ac379da8
19/10/09 12:42:49 INFO ShutdownHookManager: Deleting directory /local_disk0/tmp/spark-ef92ca6b-2e7d-4917-b407-4426ad088cee
19/10/09 12:42:49 INFO ShutdownHookManager: Deleting directory /local_disk0/spark-9b138379-fa3a-49db-95dc-436cd7040a95
19/10/09 12:42:49 INFO ShutdownHookManager: Deleting directory /local_disk0/tmp/spark-2666c4ff-30a2-4161-868e-137af5fa3787

</code></pre>

<p>What all have I tried:</p>

<p>There are 3 ways to run spark jobs in databricks:</p>

<ol>
<li><p>Notebook: I have Installed SHC jar as library on the cluster and placed 'hbase-site.xml' into my job jar and installed it onto the cluster as well, not I have pasted the main class code onto the notebook, When I run it I'm able to load dependencies from SHC but get above error.</p></li>
<li><p>Jar: This is almost similar to notebook, with that difference that instead of notebook, I give the main class and the jar to the job and run it. Gives me the same error.</p></li>
<li><p>Spark submit: I created an uber-jar with all the dependencies including SHC, I uploaded the hbase-site file to dbfs path and provided above in the submit command as below.</p></li>
</ol>

<pre><code>[
""--class"",""com.JEM.Importer.StreamingHbaseImporter"",""dbfs:/FileStore/JarPath/jarfile.jar"",
""--packages"",""com.hortonworks:shc-core:1.1.1-2.1-s_2.11"",
""--repositories"",""http://repo.hortonworks.com/content/groups/public/"",
""--files"",""dbfs:/PathToSiteFile/hbase_site.xml""
]

</code></pre>

<p>Still I get the same error. Can anyone please help?
Thanks</p>",0,10,2019-10-09 14:43:01.003000 UTC,1.0,2019-10-10 13:38:16.170000 UTC,1,apache-spark|hbase|azure-hdinsight|azure-databricks,824,2017-06-23 08:00:37.393000 UTC,2022-02-15 17:14:08.050000 UTC,"Pune, Maharashtra, India",86,13,0,25,,,,,,[]
Azure Databricks Throwing Error while talking to an App Registration in another Tenant,"<p>I have an API web app and Azure AD App Registration in one particular Azure Tenant. We have Azure Databricks in another tenant. The Azure Databricks calls this REST API app and inserts data into Cosmos DB.
We are using Terraform to provision resources. The app is consented to in Azure AD and the API Permissions in App Registration is :</p>
<pre><code>Microsoft.Graph
  User.Read
Type-&gt;Delegated
</code></pre>
<p>It was working it seems before but now Databricks throws this error while trying to talk to the API App :</p>
<blockquote>
<p>AADSTS500011: The resource principal named
https://app-iops-coalsa-api-prod was not found in the tenant named
Contoso Limited. This can happen if the application has not been
installed by the administrator of the tenant or consented to by any
user in the tenant. You might have sent your authentication request to
the wrong tenant. Trace ID: b0d5374d-0066-4a79-a9c5-44d27a502b00
Correlation ID: f80890c0-cdfa-4e06-bf36-00083200db8d Timestamp:
2021-11-10 15:07:02Z f80890c0-cdfa-4e06-bf36-00083200db8d</p>
</blockquote>
<p>Can anyone help here to understand what can be going wrong here, it was working fine it seems and stopped working for the last 2 days. No changes were done to the App Registration</p>",0,1,2021-11-10 18:06:26.453000 UTC,,,0,azure-active-directory|azure-databricks|azure-ad-graph-api|azure-app-configuration,56,2017-06-07 05:52:59.323000 UTC,2022-03-05 16:38:10.803000 UTC,,1033,0,0,123,,,,,,[]
Unable to read from reader endpoint on AWS neptune,"<p>My application as well as usage from <code>awscurl</code> fails to properly hit the <code>reader endpoint</code> of my neptune cluster. I have spawned a single read replica in addition to the primary. I try to hit the status endpoint with it and it fails (whereas the primary works)</p>
<pre><code>awscurl https://endpoint:8182/status --service neptune-db -v
</code></pre>
<p>I use the above between primary (works) reader (doesn't work). Why would this be?</p>",1,9,2021-06-14 21:53:43.513000 UTC,,,0,amazon-web-services|gremlin|amazon-neptune,172,2020-08-20 21:22:57.010000 UTC,2022-03-05 23:21:22.593000 UTC,"Seattle, WA, USA",655,57,5,91,,,,,,[]
Gremlin: limit by vertex label,"<p>Hello dear gremlin jedi,</p>
<p>I have a bunch of nodes with different labels in my graph:</p>
<pre class=""lang-rb prettyprint-override""><code>  g.addV('book')
   .addV('book')
   .addV('book')
   .addV('movie')
   .addV('movie')
   .addV('movie')
   .addV('album')
   .addV('album')
   .addV('album').iterate()
</code></pre>
<p>There also may be vertices with other labels.</p>
<p>and a hash map describing what labels and how many vertices of each label I want to get:</p>
<pre class=""lang-rb prettyprint-override""><code>LIMITS = {
  &quot;book&quot;: 2,
  &quot;movie&quot;: 2,
  &quot;album&quot;: 2,
}
</code></pre>
<p>I'd like to write a query that returns a list of vertices consisting of vertices with specified labels whete amount of vertices with each label is limited in according to the LIMITS hash map. In this case there should be 2 books, 2 movies and 2 albums in the result.</p>
<p>The limits and requested labels are calculated independently for every query so they cannot be hardcoded.</p>
<p>As far as I can see the limit step does not support passing traversals as an argument.</p>
<p>What trick can I use to write such query? The only option I see is to build the query using capabilities of the client side programming language (Ruby with <a href=""https://github.com/babbel/grumlin"" rel=""nofollow noreferrer"">grumlin</a> as a gremlin client in my case):</p>
<pre class=""lang-rb prettyprint-override""><code>  nodes = LIMITS.map do |label, limit|
    __.hasLabel(label).limit(limit)
  end

   g.V().union(*nodes).toList
</code></pre>
<p>But I believe there is a better solution.</p>
<p>Thank you!</p>",1,0,2021-12-13 14:54:46.997000 UTC,,2021-12-13 16:43:50.117000 UTC,1,gremlin|tinkerpop|tinkerpop3|amazon-neptune|gremlin-server,48,2012-07-10 03:29:26.730000 UTC,2022-03-05 01:29:01.787000 UTC,Berlin,123,3,0,8,,,,,,[]
404 Error when I try to integrate JIRA with GitHub Enterprise,"<p>I am trying to integrate my GitHub Enterprise account with JIRA via the steps documented here: <a href=""https://confluence.atlassian.com/display/AOD/Linking+a+bitbucket+or+GitHub+repository+with+JIRA+OnDemand"" rel=""nofollow"">https://confluence.atlassian.com/display/AOD/Linking+a+bitbucket+or+GitHub+repository+with+JIRA+OnDemand</a></p>

<p>However, after I complete the process and hit ""Add"" and ""Continue,"" I receive a 404 error from GitHub.</p>

<p>I've looked on JIRA's support site as well as here--can't find a solution. Does anyone know of one or a workaround? I'm thinking this is a bug.</p>

<p>Thanks!</p>",3,0,2014-02-10 17:49:36.587000 UTC,,,4,github|oauth|http-status-code-404|jira|dvcs,1757,2014-02-10 17:11:23.970000 UTC,2014-04-03 20:35:02.593000 UTC,,41,0,0,1,,,,,,[]
How to upgrade the Hive version in Azure Databricks,"<blockquote>
<p>org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.UnsupportedOperationException: Parquet does not support timestamp. See HIVE-6384;</p>
</blockquote>
<p>Getting above error while executing following code in Azure Databricks.</p>
<pre><code>spark_session.sql(&quot;&quot;&quot;
    CREATE EXTERNAL TABLE IF NOT EXISTS dev_db.processing_table
    (
      campaign STRING,
      status STRING,
      file_name STRING,
      arrival_time TIMESTAMP
    )
    PARTITIONED BY ( 
      Date DATE)
    ROW FORMAT SERDE
      'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' 
    STORED AS INPUTFORMAT
      'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' 
    OUTPUTFORMAT
      'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
    LOCATION &quot;/mnt/data_analysis/pre-processed/&quot;
&quot;&quot;&quot;)
</code></pre>
<p>I am using hive in Azure Data bricks, and when I run a command <code>spark_session.conf.get(&quot;spark.sql.hive.metastore.version&quot;)</code> it is showing as <strong>Hive 0.13</strong> version.</p>
<p>Hive 0.13 won't have a support for <strong>Timestamp</strong> datatype for parquet file.</p>
<p>In my current dataset I have multiple columns with Timestamp datatype.
As per Hive-6384 Jira, Starting from Hive-1.2 you can use Timestamp,date types in parquet tables.</p>
<p>How can I upgrade the Hive/Hive metastore version?</p>",1,0,2020-09-13 08:31:41.160000 UTC,,2020-10-08 16:18:16.390000 UTC,1,azure|apache-spark|hadoop|hive|azure-databricks,623,2016-04-08 10:36:36.687000 UTC,2022-03-02 07:18:21.530000 UTC,,145,66,0,36,,,,,,[]
Spark streaming from eventhub: how to stop stream once there is no more data?,"<p>What I am trying to do, is to read some data from my event hub, and save it in azure data lake. However, the issue is, that the stream doesn't stop, and the <code>writeStream</code> step is not triggered. I am not able to find any setting to identify when the input rate reaches 0 in order to stop the stream then.</p>
<p><a href=""https://i.stack.imgur.com/cEzyV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cEzyV.png"" alt=""enter image description here"" /></a></p>",1,0,2021-07-09 16:30:22.760000 UTC,,,2,python|apache-spark|pyspark|azure-databricks,433,2017-11-20 18:20:15.443000 UTC,2022-03-05 13:03:45.133000 UTC,,473,29,0,106,,,,,,[]
Azure Data Factory connection to Databricks doesn't work when using Key Vault to retrieve token,"<p>I have a Databricks instance which does some work. Jobs are triggered from Azure Data Factory. There is several environments and each one has its own Key Vault to store secrets.</p>
<p>As long as I kept access token - let's say &quot;hardcoded&quot; - within a Databricks linked service configuration everything worked fine. But I need to comply with security standards, so keeping it in JSON which lays somewhere isn't an option - it was fine for the time being.</p>
<p>Key Vault to the rescue - access token to the Databricks is created via API and stored in a Key Vault, now I wanted to use the Key Vault as linked service in Databricks linked service to populate access token, and the surprise comes here - it doesn't work.</p>
<p><strong>I can't debug pipeline, I can't trigger it, I can't even test a connection, it always fails with 403 Invalid access token:</strong></p>
<p><a href=""https://i.stack.imgur.com/qfRcD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qfRcD.png"" alt=""enter image description here"" /></a></p>
<p>The JSON for this linked service:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;name&quot;: &quot;ls_databricks&quot;,
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/linkedservices&quot;,
    &quot;properties&quot;: {
        &quot;annotations&quot;: [],
        &quot;type&quot;: &quot;AzureDatabricks&quot;,
        &quot;typeProperties&quot;: {
            &quot;domain&quot;: &quot;https://**************.azuredatabricks.net&quot;,
            &quot;accessToken&quot;: {
                &quot;type&quot;: &quot;AzureKeyVaultSecret&quot;,
                &quot;store&quot;: {
                    &quot;referenceName&quot;: &quot;ls_keyVault&quot;,
                    &quot;type&quot;: &quot;LinkedServiceReference&quot;
                },
                &quot;secretName&quot;: &quot;DatabricksAccessToken&quot;
            },
            &quot;existingClusterId&quot;: &quot;*********&quot;
        }
    }
}
</code></pre>
<hr />
<p>While, using Postman I can easily access Databricks API using the same access token:
<a href=""https://i.stack.imgur.com/ViNN3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ViNN3.png"" alt=""enter image description here"" /></a></p>
<hr />
<p>Key Vault linked service itself works fine and connection test passes:
<a href=""https://i.stack.imgur.com/RYEec.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RYEec.png"" alt=""enter image description here"" /></a></p>
<hr />
<p>I have configured different linked service to connect to ADLS using Key Vault and it works as expected:
<a href=""https://i.stack.imgur.com/Nmxo1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Nmxo1.png"" alt=""enter image description here"" /></a></p>
<p>Does anybody have any ideas what's wrong here?
It is just broken or I'm doing something wrong?</p>
<p>p.s. Apologies for flooding you with all of these screenshots :)</p>
<p>I'm using <a href=""https://docs.databricks.com/dev-tools/api/latest/scim/scim-sp.html"" rel=""nofollow noreferrer"">https://docs.databricks.com/dev-tools/api/latest/scim/scim-sp.html</a> SCIM API to entitle my service principal to a proper group on Databricks instance.</p>",2,0,2020-10-06 13:40:15.630000 UTC,,2020-12-24 10:32:16.343000 UTC,0,azure|azure-data-factory|http-status-code-403|azure-keyvault|azure-databricks,968,2011-08-25 09:54:59.743000 UTC,2022-03-04 12:57:49.900000 UTC,Poland,11,0,0,10,,,,,,[]
Pandas UDF for pyspark - Package not found error,"<p>I am using the pandas UDF approach to scale my models. However, I am getting an error with the pmdarima package not found. The code works fine till I run it on my notebook on the pandas dataframe itself. So the package is available for use in the notebook. From few answers online, the error seems in package not being available on the worker nodes where the code is trying to parallelize. Can someone help on how to resolve this? How can I also install the package on my worker nodes, if that's the case.</p>
<p>FYI - I am working on Azure Databricks.</p>
<pre><code>def funct1(grp_keys, df):
     other statements

     model = pm.auto_arima(train_data['sum_hlqty'],X=x,
                               test='adf',trace=False,
                               maxiter = 12,max_p=5,max_q=5,
                                njobs=-1)



forecast_df = sales.groupby('Col1','Col2').applyInPandas(funct1,schema=&quot;C1 string, C2 string, C3 date, C4 float, C5 float&quot;)








   Py4JJavaError: An error occurred while calling o256.sql.
: org.apache.spark.SparkException: Job aborted.
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:230)
    at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$5(TransactionalWriteEdge.scala:183)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:116)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:249)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:101)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)
    at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:199)
    at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$1(TransactionalWriteEdge.scala:135)
    at com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:431)
    at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)
    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
    at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)
    at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)
    at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:19)
    at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)
    at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)
    at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:19)
    at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:412)
    at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:338)
    at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:19)
    at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:56)
    at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:129)
    at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:71)
    at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:58)
    at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:85)
    at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:401)
    at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:380)
    at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:84)
    at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:108)
    at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:94)
    at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:84)
    at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:92)
    at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:88)
    at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:84)
    at com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:112)
    at com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:111)
    at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:84)
    at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:112)
    at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.$anonfun$run$2(WriteIntoDelta.scala:71)
    at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.$anonfun$run$2$adapted(WriteIntoDelta.scala:70)
    at com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:203)
    at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:70)
    at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:1128)
    at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.run(WriteIntoDelta.scala:69)
    at com.databricks.sql.transaction.tahoe.catalog.WriteIntoDeltaBuilder$$anon$1.insert(DeltaTableV2.scala:193)
    at org.apache.spark.sql.execution.datasources.v2.SupportsV1Write.writeWithV1(V1FallbackWriters.scala:118)
    at org.apache.spark.sql.execution.datasources.v2.SupportsV1Write.writeWithV1$(V1FallbackWriters.scala:116)
    at org.apache.spark.sql.execution.datasources.v2.AppendDataExecV1.writeWithV1(V1FallbackWriters.scala:38)
    at org.apache.spark.sql.execution.datasources.v2.AppendDataExecV1.run(V1FallbackWriters.scala:44)
    at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:39)
    at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:39)
    at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:45)
    at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:234)
    at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3709)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:116)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:249)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:101)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)
    at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:199)
    at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3707)
    at org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:234)
    at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:104)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)
    at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)
    at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:680)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)
    at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:675)
    at sun.reflect.GeneratedMethodAccessor655.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
    at py4j.Gateway.invoke(Gateway.java:295)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:251)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 98 in stage 7774.0 failed 4 times, most recent failure: Lost task 98.3 in stage 7774.0 (TID 177293, 10.240.138.10, executor 133): org.apache.spark.api.python.PythonException: 'pyspark.serializers.SerializationError: Caused by Traceback (most recent call last):
  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 177, in _read_with_length
    return self.loads(obj)
  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 466, in loads
    return pickle.loads(obj, encoding=encoding)
  File &quot;/databricks/spark/python/pyspark/cloudpickle.py&quot;, line 1110, in subimport
    __import__(name)
**ModuleNotFoundError: No module named 'pmdarima''** Full traceback below:
Traceback (most recent call last):
  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 177, in _read_with_length
    return self.loads(obj)
  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 466, in loads
    return pickle.loads(obj, encoding=encoding)
  File &quot;/databricks/spark/python/pyspark/cloudpickle.py&quot;, line 1110, in subimport
    __import__(name)
ModuleNotFoundError: No module named 'pmdarima'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 638, in main
    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 438, in read_udfs
    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 255, in read_single_udf
    f, return_type = read_command(pickleSer, infile)
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 75, in read_command
    command = serializer._read_with_length(file)
  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 180, in _read_with_length
    raise SerializationError(&quot;Caused by &quot; + traceback.format_exc())
pyspark.serializers.SerializationError: Caused by Traceback (most recent call last):
  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 177, in _read_with_length
    return self.loads(obj)
  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 466, in loads
    return pickle.loads(obj, encoding=encoding)
  File &quot;/databricks/spark/python/pyspark/cloudpickle.py&quot;, line 1110, in subimport
    __import__(name)
**ModuleNotFoundError: No module named 'pmdarima'****strong text**
</code></pre>",0,0,2022-03-03 15:23:16.583000 UTC,,,0,apache-spark|azure-databricks|pmdarima|pyspark-pandas|pandas-udf,12,2020-02-25 09:24:41.127000 UTC,2022-03-04 12:24:08.480000 UTC,,75,8,0,21,,,,,,[]
How to save Excel to Azure blob storage with openpyxl in databricks,"<p>I'm trying to load an Excel report template in Azure Blob Storage via Python's library openpyxl in Databricks,
edit this template and save to a new Excel file to another path in Azure Blob Storage.
However, I found that error &quot;Operation not supported&quot;.</p>
<pre><code>import openpyxl
from openpyxl.utils.dataframe import dataframe_to_rows
from openpyxl.styles import Border, Side, PatternFill, Font, Alignment
from openpyxl.styles import numbers
import pandas as pd
import numpy as np

ENV = dbutils.widgets.get(&quot;ENV&quot;)
path = '/dbfs/mnt/EXCEL/TEMP/'
export = '/dbfs/mnt//EXCEL/EXPORT/'
file = 'Template.xlsx'

wb = openpyxl.load_workbook(path + file)
sheets = wb.sheetnames
ws = wb[sheets[0]]
old_max_row = ws.max_row
cell =ws.cell(row=old_max_row+1, column=1)
cell.value = -1401230.12

exp_path = export+'Test.xlsx'
wb.save(exp_path)
</code></pre>
<p>Error meassage:</p>
<pre><code>OSError: [Errno 95] Operation not supported
</code></pre>",0,3,2021-07-21 12:25:24.953000 UTC,,2021-07-22 14:41:33.620000 UTC,0,python|azure|azure-blob-storage|openpyxl|azure-databricks,224,2020-05-18 03:46:23.897000 UTC,2022-02-24 11:08:58.877000 UTC,"Bangkok, Thailand",1,0,0,7,,,,,,[]
How to make first row header with Apache Spark in SQL,"<p>Can someone show me how to make the first row header=True with Apache Spark on Databricks using magic SQL.</p>
<p>The code that I'm using is</p>
<pre><code>%sql
CREATE OR REPLACE VIEW enrraces.race_circuits_df
AS SELECT *
FROM csv.`/FileStore/tables/results.csv`
</code></pre>
<p>I just want to make the first row the header.</p>
<p>Thanks</p>",1,0,2021-11-20 22:34:25.747000 UTC,,,0,apache-spark|pyspark|azure-databricks,83,2021-03-31 08:36:43.863000 UTC,2022-03-05 23:03:22.193000 UTC,"London, UK",651,58,0,93,,,,,,[]
Error installing requests_kerberos==0.12.0 in Azure DataBricks,"<p>I am trying to install <code>requests_kerberos == 0.12.0</code> in Azure DataBricks, but trying to install it generates an error and does not install.</p>
<p>this is the cluster configuration where I am trying to install it: <code>Databricks Runtime Version7.0 (includes Apache Spark 3.0.0, Scala 2.12)</code></p>
<p>also try installing the following,<code>% sh sudo apt-get install gcc python-dev libkrb5-dev</code>, but the installation never finishes.</p>
<p><strong>This is the error installing in the databricks notebook</strong></p>
<pre><code>ERROR: Command errored out with exit status 1: /databricks/python3/bin/python3.7 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-k1b9yec8/pykerberos/setup.py'&quot;'&quot;'; file='&quot;'&quot;'/tmp/pip-install-k1b9yec8/pykerberos/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(file);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, file, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record /tmp/pip-record-tfttk2g6/install-record.txt --single-version-externally-managed --compile --install-headers /databricks/python3/include/site/python3.7/pykerberos Check the logs for full command output.
</code></pre>
<p>I appreciate any help</p>",1,1,2020-08-26 12:57:08.577000 UTC,,2020-08-27 07:29:25.653000 UTC,0,azure-databricks,214,2020-08-26 12:55:03.573000 UTC,2022-03-03 22:03:33.013000 UTC,,49,2,0,82,,,,,,[]
Jenkins dynamic use of token,"<p>When I use a hook to trigger a build from a DVCS like Bitbucket, I have to add a Jenkins API token to authenticate that the triggered user has the right to build this job.
But if I want to use the user token, specific to the user who checked in, I want Jenkins to look up if the token belongs to a permitted user and run the job on his behalf.
Any idea if there is a plug-in that makes the used token dynamic in Jenkins?</p>",0,0,2017-08-22 12:54:25.053000 UTC,,,1,authentication|jenkins|token|dvcs,117,2015-07-30 08:54:43.863000 UTC,2022-03-04 09:25:06.637000 UTC,"Cologne, Germany",226,9,0,42,,,,,,[]
"""ambiguous merge - picked m action"" - what does it mean?","<p>I try to merge:</p>

<pre><code>hg me -r &lt;some-revision&gt;
</code></pre>

<p>and get:</p>

<pre><code>&lt;some-file&gt;: ambiguous merge - picked m action
</code></pre>

<p>What does it mean?</p>",1,0,2016-08-23 13:55:49.127000 UTC,1.0,,7,version-control|mercurial|dvcs,1345,2015-02-19 10:32:05.203000 UTC,2020-09-11 20:36:53.767000 UTC,,1987,90,0,85,,,,,,[]
How do I debug a failed commit to github?,"<p>I'm new to git and github.  My first commit of my project has failed.  I'm probably doing something stupid.</p>

<p>I'd welcome anyone pointing out the stupid thing that I'm probably doing, but the worthier question is this: how can I find out what's going on here?</p>

<pre><code>$ git remote add origin git@github.com:andy-twosticks/flungle.git

$ git push origin master
Enter passphrase for key '/home/andy/.ssh/id_rsa': 
ERROR: andy-twosticks/flungle.git doesn't exist. Did you enter it correctly?
fatal: The remote end hung up unexpectedly

$ git remote add origin git@github.com:andy-twosticks/Flungle.git
fatal: remote origin already exists.

$ git push origin master
Enter passphrase for key '/home/andy/.ssh/id_rsa': 
ERROR: andy-twosticks/flungle.git doesn't exist. Did you enter it correctly?
fatal: The remote end hung up unexpectedly
</code></pre>",0,5,2011-06-29 08:47:28.510000 UTC,,2011-06-29 08:50:17.807000 UTC,1,git|github|dvcs,1222,2011-06-13 14:04:49.703000 UTC,2011-07-30 08:48:07.110000 UTC,"Manchester, United Kingdom",1440,67,2,86,,,,,,[]
AWS Neptune Performance,"<p>I'm working on transferring data from our database which is a rdf store DB to AWS Neptune, and I'm facing some performance issues.</p>

<p>I have a <code>db.r4.large</code> Neptune instance &amp; ec2 instance on the same vpc as Neptune.</p>

<p>Basically, I'm trying to ingest data to Neptune using the following http request: <code>&lt;myinstance&gt;:8182/sparql</code>.</p>

<p>Actually, I send the http request from my ec2 instance, and it seems that Neptune processing time is slow. In addition, it seems that Neptune's processing is not parallel.</p>

<p>Below are my tests &amp; results:</p>

<ol>
<li><p>I sent the following request to Neptune:</p>

<p><code>time curl -X POST -d @/tmp/my_file_32m.txt http://myneptune-poc.c0zm6uyrnnwp.us-east-1.neptune.amazonaws.com:8182/sparql</code></p>

<p><code>/tmp/my_file_32m.txt</code> contains sparql insert commands and the time for this request is <code>34.037s</code> while Neptune claims that it took <code>21.846 s</code>:</p>

<blockquote>
<pre>
{
""type"" : ""Commit"",
""totalElapsedMillis"" : 21846
}
</pre>
  
  <p><strong><code>real    0m34.037s</code></strong> <br />
  <code>user    0m0.044s</code> <br />
  <code>sys    0m0.062s</code> <br /></p>
</blockquote>

<p>A <code>tcpdump</code> can clearly proves that the response from Neptune was received in a delay of 34 seconds.</p></li>
<li><p>When I sent a data of 100m it took more than 1 min.</p></li>
<li><p>When I sent the same file of 32m in parallel, time was multiple in 2 :</p>

<p><code>time xargs -I % -P 8 curl -vX POST -d @/tmp/my_file_32m.txt ""http://myneptune-poc.c0zm6uyrnnwp.us-east-1.neptune.amazonaws.com:8182/sparql"" &lt; &lt;(printf '%s\n' {1..2})&lt;</code></p>

<blockquote>
<pre>
{
""type"" : ""Commit"",
""totalElapsedMillis"" : 29797
}
{
""type"" : ""Commit"",
""totalElapsedMillis"" : 30362
}
</pre>
  
  <p><strong><code>real    0m57.752s</code></strong> <br />
  <code>user    0m0.137s</code> <br />
  <code>sys    0m0.101s</code> <br /></p>
</blockquote>

<p>I took a <code>tcpdump</code> and clearly see from the <code>wireshark</code> that the request was sent in parallel, but there is a delay of ~1 min till Neptune returned <code>200 OK</code> for both requests.</p>

<p>Actually, it seems that Neptune's processing is not concurrent.</p>

<p>request was sent in time 12 and <code>200 ok</code> for both requests was sent in time 69 which is exactly 57 seconds of delay.</p></li>
<li><p>I tried to increase my Neptune instance size to <code>db.r4.xlarge</code> and also to <code>db.r4.2xlarge</code>, db, but I got the same performance.</p></li>
<li>I tried to send a compressed data in a <code>gzip</code> format in order to improve times, but it seems that Neptune doesn't support it (checking in <code>wireshark</code> the request was sent correctly).</li>
</ol>

<p>I would like to hear your opinion about my tests and the results:</p>

<ol>
<li>why performance is slow for a single http request?</li>
<li>why Neptune's processing is not parallel?</li>
</ol>",1,6,2019-01-08 10:25:03.827000 UTC,,2019-01-08 14:31:30.470000 UTC,2,performance|http|sparql|amazon-neptune,1531,2013-07-23 09:26:17.320000 UTC,2022-02-07 19:49:18.220000 UTC,,91,0,0,25,,,,,,[]
Spark dataframe changes column values when writing on SQL server,"<p>I'm facing a very specific problem. I'm working on a pyspark notebook on Databricks. I run the following command:</p>
<pre><code>my_df.select(&quot;insert_date&quot;).distinct().show()
</code></pre>
<p>and get:</p>
<pre><code>+--------------------+
|         insert_date|
+--------------------+
|2021-12-22 09:52:...|
|2021-12-20 16:36:...|
+--------------------+
</code></pre>
<p>then, I run:</p>
<pre><code>my_df.persist()
my_df.write.option(&quot;truncate&quot;, &quot;true&quot;).mode(&quot;overwrite&quot;).jdbc(sqlDbUrl, &quot;[dbo].[my_table]&quot;,properties = connectionProperties)
my_df.unpersist()
my_df.select(&quot;insert_date&quot;).distinct().show()
</code></pre>
<p>and get:</p>
<pre><code>+--------------------+
|         insert_date|
+--------------------+
|2021-12-22 09:52:...|
+--------------------+
</code></pre>
<p>if I interrogate the SQL database, the result is consistent with the second show:</p>
<pre><code>select distinct insert_date from [dbo].[my_table]

**result**
insert_date
2021-12-22 09:52:31.000

</code></pre>
<p>and this is undesired, I would like to preserve the two distinct values for column insert_date within the SQL table, and I can't get why this is not happening.</p>
<p>Further information:<br />
Databricks column type: string<br />
python version: 3<br />
SQL column type: NVARCHAR(MAX)<br />
There are no costraints such as default value for the SQL column</p>
<p>For any further information, please ask in the comments.<br />
Thanks in advance!</p>",0,0,2021-12-22 11:53:06.957000 UTC,,,1,sql-server|jdbc|azure-databricks,37,2018-06-24 16:56:31.857000 UTC,2022-03-04 15:40:50.420000 UTC,"Haifa, Israele",19,0,0,2,,,,,,[]
"How to save my Pandas DataFrame to Azure Data Lake Gen2 account in ""XLSX"" excel format?","<p>Currently i am importing data from azure data lake gen2  using pandas in Azure Data Bricks which is working fine. But after i am done with data processing, i want to export pandas data frame to azure data lake gen2 account which is still working fine but when i tried to open file from azure data storage, its showing that file is corrupted.</p>

<p>I have tried to install xlsxwriter to save the file but azure data bricks library is not supporting pip install xlsxwriter command</p>

<p>Below code shows how i import data from Azure data lake storage account and how i am trying to export data from azure data bricks to azure data lake storage account.</p>

<pre><code>import pandas as pd

getpath = r'/dbfs/mnt/native/internal/sales/wholesalesellthru/123/current/123.xlsx' getweekdata = pd.read_excel(getpath, sheetnames = 0, skiprows=2)

getdata = pd.read_excel(getpath, sheetnames = 0, skiprows=3)

getdata = getdata.iloc[:, [0,1,2,3,4]]
</code></pre>

<p>Here i am trying to save pandas dataframe ""getdata"" to Azure Data Lake Gen2 Account.</p>

<pre><code>outputpath = r'/dbfs/mnt/native/internal/sales/wholesalesellthru/123/current/output.xlsx'

getalldata.to_excel(outputpath, header=True)
</code></pre>

<p>File is getting corrupted in azure data lake gen2 account.</p>",0,1,2019-07-12 07:16:04.517000 UTC,,2019-07-12 07:27:05.097000 UTC,2,python|pandas|azure-data-lake|azure-databricks,1009,2013-02-13 03:21:03.673000 UTC,2021-09-26 17:42:02.467000 UTC,,47,2,0,17,,,,,,[]
Gremlin query edge history,"<p>I am using Gremlin, TinkerPop, AWS Neptune.</p>
<p>I have the following use case:
I need to query edge history, e.g. I have a connection between two points and I want to check the changes over time. If I am doing edge update I am losing the history, if I am adding edges for each update(like append-only) I have a lot of edges per vertex, if I am adding property per update e.g. {time_stamp: updated_value} I have complex queries because I have a lot of properties per edge.</p>
<p>What is the right way to handle this?
Thanks.</p>",0,2,2020-09-10 08:51:22.613000 UTC,,,1,gremlin|tinkerpop|amazon-neptune,104,2020-09-10 08:41:36.587000 UTC,2021-06-27 11:33:21.967000 UTC,,11,0,0,3,,,,,,[]
How should I organize my Git repositories and branches to create multiple versions of a website?,"<p>I'm an SVN user hoping to move to Git. I've been reading documentation and tutorials all day, and I still have unanswered questions. I don't know if this workflow will make sense, but here's my situation, and what I would like to get out of my workflow:</p>

<ul>
<li>Multiple developers, all developing locally on their work stations</li>
<li>3 versions of the website: Dev, Staging, Production</li>
</ul>

<p>Here's my dream:</p>

<p>A developer works locally on his own branch, say ""developer1"", tests on his local machine, and commits his changes.</p>

<p>Another developer can pull down those changes into his own branch. Merge developer1 -> developer2.</p>

<p>When the work is ready to be seen by the public, I'd like to be able to ""push"" to Dev, Staging, or Production.</p>

<pre><code>git push origin staging 
</code></pre>

<p>or maybe</p>

<pre><code>git merge developer1 staging
</code></pre>

<p>I'm not sure. Like I said, I'm still new to Git.</p>

<p>Here are my main questions:</p>

<ul>
<li><p>Do my websites (Dev, Staging, Production) have to be repositories? And do they have to be ""bare"" in order to be the recipients of new changes?</p></li>
<li><p>Do I want one repository or many, with several branches?</p></li>
<li><p>Does this even make sense, or am I on the wrong path?</p></li>
</ul>

<p>I've read a lot of tutorials, so I'm really hoping someone can just help me out with my specific situation. Thanks so much!</p>",3,1,2011-02-03 22:54:55.993000 UTC,,2011-08-30 20:40:24.290000 UTC,5,git|dvcs|git-branch|git-workflow,3062,2011-02-03 16:14:35.700000 UTC,2011-07-14 18:30:49.433000 UTC,,303,0,0,11,,,,,,[]
Error while connecting Azure databricks to Cosmos DB Mongo API,"<p>I have installed the Spark mongodb connector in databricks and trying to execute the sample code as below:</p>
<pre><code>from pyspark.sql import SparkSession
 

my_spark = SparkSession \
    .builder \
    .appName(&quot;myApp&quot;) \
    .getOrCreate()


df = my_spark.read.format(&quot;com.mongodb.spark.sql.DefaultSource&quot;) \
  .option(&quot;uri&quot;, CONNECTION_STRING) \
  .load()
</code></pre>
<p>where CONNECTION_STRING is in below format:</p>
<pre><code>mongodb://USERNAME:PASSWORD@testgp.documents.azure.com:10255/DATABASE_NAME.COLLECTION_NAME?ssl=true&amp;replicaSet=globaldb
</code></pre>
<p>But facing the below error:</p>
<pre><code>org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 15) (10.25.238.198 executor 0): java.io.InvalidClassException: com.mongodb.spark.rdd.partitioner.MongoPartition; local class incompatible: stream classdesc serialVersionUID = -2855217470084313385, local class serialVersionUID = -3413909316915051241
</code></pre>
<p>Does anyone has faced this error and a possible solution?</p>",0,0,2021-05-18 17:53:03.737000 UTC,,2021-05-18 18:34:52.320000 UTC,0,apache-spark|azure-cosmosdb|azure-databricks|azure-cosmosdb-mongoapi|databricks-connect,58,2017-09-11 12:38:42.273000 UTC,2021-08-16 05:51:45.390000 UTC,,3,3,0,6,,,,,,[]
"Need to decompress an archive from Azure Databricks, using Python","<p>i'm using a code to decompress an archive coming from a blob storage and this code is already functional for another archive that has 300mb, but while trying to decompress another one bigger than this, i've got this error:</p>
<pre><code>&quot;NotImplementedError: That compression method is not supported&quot;
The last lines of error console show this :
/usr/local/lib/python3.8/zipfile.py in _get_decompressor(compress_type)
    718 
    719 def _get_decompressor(compress_type):
--&gt; 720     _check_compression(compress_type)
    721     if compress_type == ZIP_STORED:
    722         return None

/usr/local/lib/python3.8/zipfile.py in _check_compression(compression)
    698                 &quot;Compression requires the (missing) lzma module&quot;)
    699     else:
--&gt; 700         raise NotImplementedError(&quot;That compression method is not supported&quot;)
</code></pre>
<p>And i'm using this code to this:</p>
<pre><code># mother folder
files = dbutils.fs.ls(dl_path)

for fi in sorted(files, reverse=True):
  zip_files = zipfile.ZipFile(f'/dbfs{dl_path}{fi.name}')
  print(zip_files.namelist())
  for f in zip_files.namelist():
    zip_files.extract(f, str(extract_path).replace('dbfs:', '/dbfs'))
</code></pre>
<p>I dont know why in one of archives this one works but the other one dont. I assume it might be about size? So i'm thinking about do a try: first code and except a second one? Idk, some one has tips?</p>",1,2,2021-08-20 14:14:53.397000 UTC,,,1,python|compression|azure-databricks|unzip|zipfile,118,2021-04-03 12:40:24.837000 UTC,2021-12-07 12:20:03.277000 UTC,,11,0,0,0,,,,,,[]
Python Code syntax not working in databricks,"<p>I am migrating my python code from Jupiter notebook to databricks notebook. But lots of things don't work as it is.</p>
<p>The below code works properly in Jupyter notebook but does not work in databricks:</p>
<pre><code># Replace Periods with Month Name:    
replacements = {
   'FISCAL_PERIOD_NBR': {
      r'(Per01)': 'Feb',
      r'(Per02)': 'Mar',
      r'(Per03)': 'Apr',
      r'(Per04)': 'May',
      r'(Per05)': 'Jun',
      }
}

df_input.replace(replacements, regex=True, inplace=True)

</code></pre>
<p>I don't see any errors either but the column &quot;FISCAL_PERIOD_NBR&quot; gets deleted post this statement. I am not sure what changes are required while migrating the code from jupyter notebook to databricks but there is a 1500 line code which I have to debug line by line. Any support is appreciated.</p>",0,2,2020-09-10 09:27:54.160000 UTC,,,1,python|pandas|jupyter-notebook|azure-databricks,52,2016-05-27 04:26:41.343000 UTC,2022-02-27 23:35:56.343000 UTC,India,539,87,0,85,,,,,,[]
How to provide login credentials with hg pull?,"<p>Is there any way of providing login credentials (Mercurial username and its password) with <code>hg pull</code> command? Or there is anyway of starting a session with particular user by providing username and password? I want to automate the mercurial pulling process using Java code and command line API.</p>

<p>I know there is a way of constructing the repository URL accordingly, e.g., <code>http://user:password@host/MercurialRepo</code> but this is not suitable to my case.</p>",1,2,2012-01-31 13:16:55.120000 UTC,,2012-03-30 12:39:51.650000 UTC,3,java|authentication|mercurial|dvcs,5321,2011-12-15 06:57:01.747000 UTC,2012-09-10 09:35:51.213000 UTC,,71,1,0,8,,,,,,[]
Add Double Quotes for String Column Value Separated With Comma And Write to CSV File With Comma Delimited,"<p>While writing data frame to file csv with comma delimited to some location, aim is to get string column value enclosed within double quotes and int column value without double quotes.</p>

<p>While writing data frame to csv file with comma delimited to some location, aim is to get string column value enclosed within double quotes and int column value without double quotes. 
Using below code,able to get expected results until got one column value separated with comma(e.g. ""SOUTHERN GLAZES W&amp;S OF CA, LOCAL"").
In this case column Value is written to csv like :NUL""SOUTHERN GLAZERS W&amp;S OF CA, NOCAL""NUL.
When tried with pipe(""|"") delimited getting proper result but need output with comma delimited.</p>

<h2>Table structure:</h2>

<pre><code>%sql
CREATE TABLE A.dummy_table
(
col1 string,
col2 string,
col3 int
)
</code></pre>

<h2>Table Data:</h2>

<pre><code>%sql
insert into A.dummy_table values('A','B',1),('C','D',2),('SOUTHERN GLAZERS W&amp;S OF CA, NOCAL','DOWE AVE',1234)

%sql
select* from A.dummy_table

col 1   col 2   col 3
SOUTHERN GLAZES W&amp;S OF CA, LOCAL    DOE AVE 1234
A   B   1
C   D   2
</code></pre>

<h2>Transforming table data Into Data frame:</h2>

<pre><code>var df = spark.sql(""select * from A.dummy_table"")
</code></pre>

<h2>Code to add quotes on String column Value and remove quotes for Int column Value:</h2>

<pre><code>var df_new = df.select(df.columns.map(x =&gt;concat(lit(""\""""),col(x),lit(""\"""")) as x):_*)
val int_columns = df.dtypes.filter(_._2 !=""StringType"").map(_._1)
for(colName&lt;-int_columns)
{
df_new = df_new.withColumn(colName,regexp_replace(col(colName),""\"""",""""))
}

df_new.coalesce(1).write.format(""com.databricks.spark.csv"").mode(""overwrite"").option(""ignoreLeadingWhiteSpace"",false).option(""ignoreTrailingWhiteSpace"",false).option(""header"", ""true"").option(""quote"",""\u0000"").option(""delimiter"","","").save(""somePath"")
val fileNm = dbutils.fs.ls(""somePath"").map(_.name).filter(r =&gt; r.startsWith(""part-00000""))(0)
dbutils.fs.mv(""somePath""+""/""+fileNm,""someFinalPath"")
dbutils.fs.rm(""somePath"",true)
</code></pre>

<h2>Expected Result :</h2>

<pre><code>col1,col2,col3
""SOUTHERN GLAZERS W&amp;S OF CA, NOCAL"",""DOWE AVE"",1234
""A"",""B"",1
""C"",""D"",2
</code></pre>

<h2>Actual Result</h2>

<pre><code>col1,col2,col3
NUL""SOUTHERN GLAZERS W&amp;S OF CA, NOCAL""NUL,""DOWE AVE"",1234
""A"",""B"",1
""C"",""D"",2
</code></pre>",0,2,2019-10-02 10:47:46.193000 UTC,,,0,scala|apache-spark|azure-databricks,609,2014-05-07 08:16:25.047000 UTC,2019-10-24 07:31:15.510000 UTC,,25,0,0,4,,,,,,[]
Unable to read spark temp table data on databricks notebook,"<p>I have created a dataframe &quot;df&quot; and a temp table <code>df.createOrReplaceTempView(&quot;temp_table&quot;)</code> in azure databricks notebook. Then I'm writing a select query on that temp table using</p>
<pre><code>dbutils.notebook.exit(&quot;&quot;&quot;select col1, col4 from temp_table&quot;&quot;&quot;). 
</code></pre>
<p>Now, when I try to run this notebook from another notebook, I'm getting the error</p>
<blockquote>
<p>org.apache.spark.sql.AnalysisException: Table or view not found: temp_table.</p>
</blockquote>
<p>How this can be resolved?</p>",0,4,2021-03-16 05:34:30.200000 UTC,,2021-03-16 05:50:48.340000 UTC,0,azure|apache-spark-sql|azure-databricks,204,2020-02-07 08:18:13.437000 UTC,2021-03-24 04:04:30.467000 UTC,,85,7,0,31,,,,,,[]
using like operator in spark sql databricks,"<p>I'm using spark sql, and I created some Vues to join some data. but I have to join these Vues based on a string column. thats whu I had to use the like operator.</p>
<pre><code>  select table.perfume,table2.perfume
  from global_temp.gv_table1 table1
  join global_temp.gv_table2 table2
   on(lower(table1.perfume) like CONCAT('%', lower(table2.perfume), '%') )
</code></pre>
<p>but the problem with this query it does not not give all the result, example.
there'es a perfume on the table1 called &quot;FlowerBomb&quot; and a perfume on the table2 called &quot;Flowerbomb Eau du parfum&quot;, after the join this perfume was not displayed.
is there a problem with the like operator ?</p>",2,0,2022-01-03 16:29:12.850000 UTC,,,0,sql|apache-spark|azure-databricks,102,2019-12-07 15:00:02.513000 UTC,2022-03-04 14:58:41.847000 UTC,France,201,0,0,39,,,,,,[]
Skip first 2 lines and remove quotes from row values in pyspark dataframe,"<p>I have a csv with around 15 columns</p>
<ol>
<li>I would like to skip first 2 lines and use a custom schema</li>
<li>Remove double quotes from row values</li>
</ol>
<p>csv is as below.</p>
<pre><code>Header1 blah blah
Header2 blah blah
Name1;&quot;1,456&quot;;&quot;City1&quot;;&quot;3&quot;;&quot;pet&quot;
Name2;&quot;3,450&quot;;&quot;City2&quot;;&quot;4&quot;;&quot;not pet&quot;


delimiter = &quot;;&quot;
salesDF =  spark.read.format(&quot;csv&quot;) \
     .option(&quot;quote&quot;, &quot;&quot;) \
     .option(&quot;sep&quot;, delimiter) \     
     .load(&quot;sales_2018.csv&quot;) 
salesDF = salesDF.replace(&quot;\&quot;&quot;,&quot;&quot;)
</code></pre>
<p>I tried as above to remove quotes from csv. Delimiter works but quotes are not getting removed.</p>
<p>Results are as below: It has added only quotes but didn't remove.</p>
<pre><code>Header1 blah blah
Header2 blah blah
&quot;Name1;&quot;&quot;1,456&quot;&quot;;&quot;&quot;City1&quot;&quot;;&quot;&quot;3&quot;&quot;;&quot;&quot;pet&quot;&quot;
&quot;Name2;&quot;&quot;3,450&quot;&quot;;&quot;&quot;City2&quot;&quot;;&quot;&quot;4&quot;&quot;;&quot;&quot;not pet&quot;&quot;
</code></pre>
<p>My idea is to remove quotes and the remove the first 2 lines of the dataframe to add my custom schema. Thanks.</p>",0,1,2021-04-05 09:35:45.930000 UTC,,,1,pyspark|azure-databricks,53,2019-08-15 06:38:39.327000 UTC,2022-03-03 11:02:35.683000 UTC,,769,82,0,142,,,,,,[]
"how can you configure tortoisehg to see in the web servers page the ""graph"" link?","<p>In the mercurial repo i can see it.
I think that mercurial already have this feature with the revision graph.</p>

<p>How i've to configure tortoisehg to achieve that?</p>

<p>I want to have something like <a href=""http://selenic.com/repo/index.cgi/hg-stable/graph/b965605dfb2e"" rel=""nofollow noreferrer"">this</a> for the hgweb.</p>",1,0,2008-11-19 01:27:00.800000 UTC,1.0,2008-11-27 17:17:12.440000 UTC,0,mercurial|webserver|dvcs|tortoisehg,1195,2008-10-26 22:40:48.363000 UTC,2021-08-24 17:36:05.680000 UTC,,7660,136,8,501,,,,,,[]
Spark Cloudfiles Autoloader BlobStorage Azure java.io.IOException: Attempted read from closed stream,"<p>I learn to use the new autoloader streaming method on SPARK 3 and I have this issue.
Here i'm trying to listen simple json files but my stream never start.</p>
<p>My code (creds removed) :</p>
<pre class=""lang-py prettyprint-override""><code>from pyspark.sql.types import StructType, StringType, IntegerType

azdls_connection_string = &quot;My connection string&quot;
queue_name = &quot;queu-name&quot;

stream_schema = StructType() \
  .add(&quot;timestamp&quot;, StringType(), False) \
  .add(&quot;temperature&quot;, IntegerType(), False)

ressource_group = &quot;&quot;
cloudfiles_subid_telem = &quot;&quot;
cloudfiles_clientid_telem = &quot;&quot;
cloudfiles_clientsecret_telem = &quot;&quot;
tenantid = &quot;&quot;
conainer_name = &quot;mydb&quot;
abs_fs = &quot;abfss://&quot; + conainer_name + &quot;@&quot; + dls_name + &quot;.dfs.core.windows.net&quot;

read_stream = (
  spark.readStream.format(&quot;cloudFiles&quot;)
  .option(&quot;cloudFiles.useNotifications&quot;, True)
  .option(&quot;cloudFiles.format&quot;, &quot;json&quot;)
  .option(&quot;cloudFiles.connectionString&quot;, azdls_connection_string)
  .option(&quot;cloudFiles.resourceGroup&quot;, ressource_group)
  .option(&quot;cloudFiles.subscriptionId&quot;, cloudfiles_subid_telem)
  .option(&quot;cloudFiles.tenantId&quot;, tenantid)
  .option(&quot;cloudFiles.clientId&quot;, cloudfiles_clientid_telem)
  .option(&quot;cloudFiles.clientSecret&quot;, cloudfiles_clientsecret_telem)
  .option(&quot;cloudFiles.region&quot;, &quot;francecentral&quot;)
  .schema(stream_schema)
  .option(&quot;cloudFiles.includeExistingFiles&quot;, False)
  .load(abs_fs + &quot;/input&quot;)
)

checkpoint_path = abs_fs + &quot;/checkpoints&quot;
out_path = abs_fs + &quot;/out&quot;

df = read_stream.writeStream.format(&quot;delta&quot;) \
  .option(&quot;checkpointLocation&quot;, checkpoint_path) \
  .start(out_path)
</code></pre>
<p>And when i try to start my streaming i got a error. My permissions are correctly set because my Azure Queue is created. I don't find any informations in the documentation of autloader on the databricks website about this error.</p>
<p>And here is my error :</p>
<pre><code>java.io.IOException: Attempted read from closed stream.
at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:165)
    at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135)
    at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
    at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
    at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
    at java.io.InputStreamReader.read(InputStreamReader.java:184)
    at java.io.Reader.read(Reader.java:140)
    at org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:2001)
    at org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:1980)
    at org.apache.commons.io.IOUtils.copy(IOUtils.java:1957)
    at org.apache.commons.io.IOUtils.copy(IOUtils.java:1907)
    at org.apache.commons.io.IOUtils.toString(IOUtils.java:778)
    at org.apache.commons.io.IOUtils.toString(IOUtils.java:759)
    at com.databricks.sql.aqs.EventGridClient.prettyResponse(EventGridClient.scala:428)
    at com.databricks.sql.aqs.EventGridClient.com$databricks$sql$aqs$EventGridClient$$errorResponse(EventGridClient.scala:424)
    at com.databricks.sql.aqs.EventGridClient$$anonfun$createEventSubscription$3.applyOrElse(EventGridClient.scala:235)
    at com.databricks.sql.aqs.EventGridClient$$anonfun$createEventSubscription$3.applyOrElse(EventGridClient.scala:229)
    at com.databricks.sql.aqs.EventGridClient.executeRequest(EventGridClient.scala:387)
    at com.databricks.sql.aqs.EventGridClient.createEventSubscription(EventGridClient.scala:226)
    at com.databricks.sql.aqs.autoIngest.AzureEventNotificationSetup.$anonfun$setupEventGridSubscription$1(AzureEventNotificationSetup.scala:135)
    at scala.Option.getOrElse(Option.scala:189)
    at com.databricks.sql.aqs.autoIngest.AzureEventNotificationSetup.setupEventGridSubscription(AzureEventNotificationSetup.scala:121)
    at com.databricks.sql.aqs.autoIngest.AzureEventNotificationSetup.&lt;init&gt;(AzureEventNotificationSetup.scala:75)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at com.databricks.sql.fileNotification.autoIngest.EventNotificationSetup$.$anonfun$create$1(EventNotificationSetup.scala:66)
    at com.databricks.sql.fileNotification.autoIngest.ResourceManagementUtils$.unwrapInvocationTargetException(ResourceManagementUtils.scala:42)
    at com.databricks.sql.fileNotification.autoIngest.EventNotificationSetup$.create(EventNotificationSetup.scala:50)
    at com.databricks.sql.fileNotification.autoIngest.CloudFilesSourceProvider.$anonfun$createSource$2(CloudFilesSourceProvider.scala:162)
    at scala.Option.getOrElse(Option.scala:189)
    at com.databricks.sql.fileNotification.autoIngest.CloudFilesSourceProvider.createSource(CloudFilesSourceProvider.scala:154)
    at org.apache.spark.sql.execution.datasources.DataSource.createSource(DataSource.scala:306)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$1(MicroBatchExecution.scala:93)
    at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:90)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:88)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:322)
    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:80)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:322)
    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:166)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:164)
    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:311)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:88)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:68)
    at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:346)
    at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:269)
</code></pre>
<p>Thank you in advance</p>",1,0,2021-01-06 10:35:31.180000 UTC,,,0,python|apache-spark|autoload|azure-databricks,178,2020-04-20 13:30:58.657000 UTC,2022-02-14 23:35:23.750000 UTC,France,13,0,0,2,,,,,,[]
How do I connect to Neptune using Java,"<p>I have the following code based on the docs...</p>

<pre><code>@Controller
@RequestMapping(""neptune"")
public class NeptuneEndpoint {
    @GetMapping("""")
    @ResponseBody
    public String test(){
        Cluster.Builder builder = Cluster.build();
        builder.addContactPoint(""...endpoint..."");
        builder.port(8182);

        Cluster cluster = builder.create();

        GraphTraversalSource g = EmptyGraph.instance()
                                           .traversal()
                                           .withRemote(
                                               DriverRemoteConnection.using(cluster)
                                           );

        GraphTraversal t = g.V().limit(2).valueMap();

        t.forEachRemaining(
                e -&gt;  System.out.println(e)
        );

        cluster.close();
        return ""Neptune Up"";
    }
}
</code></pre>

<p>But when I try to run I get ...</p>

<blockquote>
  <p>java.util.concurrent.TimeoutException: Timed out while waiting for an available host - check the client configuration and connectivity to the server if this message persists</p>
</blockquote>

<p>Also how would I add Secret key from AWS IAM account?</p>",5,1,2018-06-28 03:41:06.703000 UTC,1.0,,2,amazon-neptune,1809,2009-06-18 16:05:40.743000 UTC,2022-03-05 02:54:03.640000 UTC,"Columbus, OH",19055,690,51,2069,,,,,,[]
Passing composite variables to Azure DevOps Release Pipelines,"<p>I'm pretty new to Azure DevOps and in the last few days I've been building a pipeline to deploy artifacts to Databricks clusters. I have a few Python libraries saved as Azure DevOps artifacts and I want to deploy them using an Azure DevOps Release (CD) pipeline. We don't have YAML for CD pipelines, so that's not on the table.</p>
<p>I've been able to do this using a python script which is run by the CD pipeline and calls Databricks API, and to specify the libraries and clusters I use pipeline variables like <code>libs=&quot;[lib1, lib2, ...]&quot;</code> and <code>clusters=&quot;[cluster1, cluster2, ...]&quot;</code>, these are just list-like strings which are converted, inside the python script, into python lists.</p>
<p>This solution works fine for me, but it installs all the libraries in <code>libs</code> into all the <code>clusters</code>. I was thinking about passing a json variable that specifies which libraries to install on which clusters, something like:</p>
<pre><code>{
    installation1: {
        cluster: &quot;id1&quot;
        libraries: [{lib: &quot;lib1&quot;}, {lib: &quot;lib2&quot;}, ...]
    }
    installation2: {
        cluster: &quot;id2&quot;
        libraries: [{lib: &quot;lib3&quot;}, {lib: &quot;lib4&quot;}, ...]
    }
    ...
}
</code></pre>
<p>Of course one could pass this json as a string the same way as I already pass a list as a string in <code>libs</code>, but this would be a pain to modify in case a new installation is required and I was wondering whether there's a better solution to this requirement. Is there a better way to pass complex variables to DevOps CD pipelines rather than putting them into strings?</p>",0,0,2021-08-03 06:59:49.870000 UTC,0.0,,0,azure|azure-devops|azure-pipelines|azure-databricks,42,2016-02-13 11:36:15.973000 UTC,2022-03-05 18:02:18.037000 UTC,"Milano, MI, Italia",164,30,10,13,,,,,,[]
Difference between CREATE TEMPORARY VIEW vs Createorreplacetempview in spark databricks,"<p>In databricks, what's the difference between the two methods.</p>
<pre><code>%sql

CREATE TEMPORARY VIEW diamonds
USING CSV
OPTIONS (path &quot;/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv&quot;, 
         header &quot;true&quot;)
OK
</code></pre>
<p>==================================================================</p>
<pre><code>dataFrame = &quot;/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv&quot;
val df = spark.read.format(&quot;csv&quot;).option(&quot;header&quot;,&quot;true&quot;)\
  .option(&quot;inferSchema&quot;, &quot;true&quot;).load(dataFrame)

df.createOrReplaceTempView(&quot;diamonds&quot;)
</code></pre>",1,0,2021-07-18 06:36:16.930000 UTC,,2021-07-18 07:06:44.737000 UTC,1,sql|azure|apache-spark|azure-databricks,1850,2018-12-12 05:26:09.463000 UTC,2021-07-22 03:17:06.243000 UTC,India,11,0,0,2,,,,,,[]
Is it possible to store 2 different struct types in the same column of a data bricks delta table?,"<p>I'm receiving multiple XML files that need to be loaded into one table. Those XML files have different struct types for a particular column. I'm wondering if somehow this column could be stored in the same column of a data bricks table. please refer below for the different struct types I'm getting for the same column col1. In file1 col1 is struct and col1a is struct and col1a1, col1a2..are string types
for file2 same col1 is a struct type but underlying col1b and col1c are string types.</p>
<p>file1 :
col1
col1a
col1a1
col1a2
.
.
col1b</p>
<p>file2:
col1
col1b
col1c</p>",1,0,2020-12-24 08:11:17.973000 UTC,,2021-04-06 08:47:10.540000 UTC,0,azure-databricks|delta|apache-spark-xml,26,2019-05-12 02:08:18.950000 UTC,2021-08-19 15:00:10.033000 UTC,,1,0,0,2,,,,,,[]
Does Git treat add/remove as a rename?,"<p>This may be more appropriate as an issue in whatever issue tracker/forum Git uses, but I thought I'd get an SO confirmation/explanation first:</p>

<p>I have a repo tracking a bunch of installer executables.</p>

<p>Let's say foo-1.0.exe is already in the repo.</p>

<p>I now add foo-2.0.exe in the same directory (git add foo-2.0.exe).  Next, I remove foo-1.0.exe (git rm foo-1.0.exe).</p>

<p>I expect Git status to show me one added file and one removed file.  Instead, I get this:</p>

<p><br>
On branch master
<br>
Changes to be committed:
<br>
(use ""git reset HEAD ..."" to unstage)
<br>
renamed:    foo-1.0.exe -> foo2.0.exe
<br></p>

<p>That's a WTF for me... is Git using some sort of heuristic to guess that 2.0 is an update to 1.0... I can see how that might make sense, but I don't think I want it to do that in this case.</p>",1,2,2009-07-29 11:16:45.770000 UTC,,,8,git|version-control|dvcs,2125,2008-10-10 13:35:04.687000 UTC,2022-02-22 17:18:34.010000 UTC,,2792,481,17,220,,,,,,[]
Mercurial marks unmodified files as modified in working directory and fails to revert,"<p>Our team uses TortoisHg 2.0.5 on Windows and after refreshing file list in working directory it sometimes (at least once a day :(( ) shows a list of unmodified files as modified. Manual comparison doesn't show any changes in code, line breaks are also equal. Reverting of these ""fantom"" files causes no result. There a two way how we deal with this problem:</p>

<ol>
<li>Turning off eol extension and reverting the files,</li>
<li>Manually removing the files and update them from the head revision.</li>
</ol>

<p>It's really annoying to do this every day (twice ...three times... per day), especially on large changeset! Please help to find a reason of the problem. </p>",1,1,2011-06-24 07:43:22.913000 UTC,,2011-06-24 07:54:10.987000 UTC,7,mercurial|dvcs|tortoisehg,1018,2011-06-24 07:17:02.660000 UTC,2011-09-26 08:01:24.150000 UTC,,71,0,0,8,,,,,,[]
Databricks considering files as directory,"<p>We are facing an issue on the Databrick filesyste that considers files as directory and we are unable to read files with Pandas. The files exist in the Azure Storage Explorer, and are considered as files as seen here :</p>

<p><a href=""https://i.stack.imgur.com/NLq2C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NLq2C.png"" alt=""enter image description here""></a></p>

<p>We have mounted the storage with <a href=""https://docs.databricks.com/data/data-sources/azure/azure-datalake-gen2.html#mount-an-azure-data-lake-storage-gen2-account-using-a-service-principal-and-oauth-20"" rel=""nofollow noreferrer"">oAuth 2.0.</a></p>

<p>On Databricks, </p>

<pre><code>%sh  ls -al '&lt;path_to_files&gt;'
</code></pre>

<p>returns the following :</p>

<pre><code>total 1127
drwxrwxrwx 2 root root   4096 Jan 29 09:26 .
drwxrwxrwx 2 root root   4096 Jan  9 13:47 ..
drwxrwxrwx 1 root root 136705 Jan 28 16:35 AAAA_2019-10-01_2019-12-27.csv
drwxrwxrwx 1 root root 183098 Jan 28 16:35 BBBB_2019-10-01_2019-12-27.csv
-rwxrwxrwx 1 root root 313120 Jan 28 16:35 CCCC_2019-10-01_2019-12-27.csv
-rwxrwxrwx 1 root root 212935 Jan 29 09:26 df_cube.csv
-rwxrwxrwx 1 root root 298228 Jan 29 09:26 df_other_cube.csv
</code></pre>

<p>​The thing is, the two first csv files are not directories at all. We can download them and read them as csv, but we cannot load them into a Pandas dataframe.</p>

<pre class=""lang-py prettyprint-override""><code>df = pd.read_csv(rootname_source_test + r'AAAA_2019-10-01_2019-12-27.csv',header=0,sep=""|"",engine='python')
&gt;&gt;&gt; IsADirectoryError: [Errno 21] Is a directory: '/dbfs/mnt/&lt;path&gt;/AAA_2019-10-01_2019-12-27.csv'
</code></pre>

<p>They are generated the same way the 3rd csv is generated, and the 3rd on is loadable in pandas. Sometimes they appear as files, sometimes as directories and we are having trouble recreating and solving this consistently.</p>

<p>Cluster config : Runtime 6.2 ML (includes Apache Spark 2.4.4, Scala 2.11)</p>

<p>Any help will be very appreciated.</p>",0,3,2020-02-03 10:57:21.420000 UTC,,2020-02-03 14:21:55.450000 UTC,0,pandas|csv|filesystems|azure-databricks,112,2013-12-25 22:40:35.990000 UTC,2022-03-04 18:31:13.387000 UTC,"Paris, France",657,76,7,96,,,,,,[]
Duplicate commits after hg rebase,"<p>I know that rebasing, already public repo, is not a good idea since we can end up with duplicated changesets (they have different ids, but the changes are the same as for the source files).
Here's the snippet from <a href=""http://mercurial.selenic.com/wiki/RebaseExtension"" rel=""nofollow"">Mercurial Rebase Extension documentation</a>:</p>

<blockquote>
  <p>You should not rebase changesets that have already been shared with
      others. Doing so will force everybody else to perform the same rebase or
      they will end up with duplicated changesets after pulling in your rebased
      changesets.</p>
</blockquote>

<p>But I'm interested what exactly could be a problem with duplicated commits? Can you give me some example of the possible problems that might occur? I've tried merging two branches with duplicated commits and had no problems, just two changeset with the same name exist on the merged branch.</p>",1,0,2014-08-21 10:13:00.957000 UTC,,2014-08-21 10:28:06.447000 UTC,1,mercurial|dvcs,1128,2008-10-04 17:05:49.270000 UTC,2021-10-14 18:22:08.293000 UTC,"Berlin, Germany",2867,790,8,354,,,,,,[]
PySpark unzip files: Which is a good approach for unzipping files and storing the csv files into a Delta Table?,"<p>I have zip files stored in Amazon s3 then I have a Python list as <code>[""s3://mybucket/file1.zip"", ..., ""s3://mybucket/fileN.zip""]</code>, I need to unzip all these files using a Spark Cluster, and stored all the CSV files into a delta format table. I would like to know a faster processing approach than my current approach: </p>

<p>1) I have a <strong>bucle for</strong> for iterating in my Python list.</p>

<p>2) I'm obtaining the zip files from s3 using Python Boto3 <code>s3.bucket.Object(file)</code></p>

<p>3) I'm unzipping the files using the next code</p>

<pre><code>import io
import boto3
import shutil
import zipfile
for file in [""s3://mybucket/file1.zip"", ..., ""s3://mybucket/fileN.zip""]:
    obj = s3.bucket.Object(file)
    with io.BytesIO(obj.get()[""Body""].read()) as tf:
        tf.seek(0)
        with zipfile.ZipFile(tf, mode='r') as zipf:
            for subfile in zipf.namelist():
                zipf.extract(subfile, outputZip)
    dbutils.fs.cp(""file:///databricks/driver/{0}"".format(outputZip), ""dbfs:"" + outputZip, True)
    shutil.rmtree(outputZip)
    dbutils.fs.rm(""dbfs:"" + outputZip, True)
</code></pre>

<p>4) My files are unzipped in the Driver Node, then the executors can't reach these files (I don't find a way to do it) so I move all these csv files to DBFS using <code>dbutils.fs.cp()</code></p>

<p>5) I read all the csv files from DBFS using a Pyspark Dataframe and I write that into a Delta table</p>

<pre><code>df = self.spark.read.option(""header"", ""true"").csv(""dbfs:"" + file) 
df.write.format(""delta"").save(path)
</code></pre>

<p>6) I delete the data from DBFS and the Driver Node</p>

<p>So, my current goal is to ingest zip files from S3 into a Delta table in less time than my previous process. I suppose that I can parallelize some of these processes as the 1) step, I would like to avoid the copy step to DBFS because I don't need to have the data there, also I need to remove the CSV files after each ingests into a Delta Table to avoid a memory error in the Driver Node disk. Any advice? </p>",2,1,2019-10-30 15:20:51.890000 UTC,1.0,2019-10-30 15:28:45.110000 UTC,5,python|amazon-s3|zip|azure-databricks|delta-lake,9758,2018-04-17 10:44:54.633000 UTC,2022-03-04 11:51:13.393000 UTC,"Barcelona, Spain",1337,95,3,169,,,,,,[]
How to clear state of one notebook without affecting other notebook using command in Azure Databricks?,"<p>I have 3 notebook pipelines running in parallel on same cluster, I want to know if there is any way to clear all the scala variables and spark DFs of one notebook without affecting other notebook? I tried spark.close() but it clears state of other two notebooks. I know that detaching notebook will clear state but how to do it using commands?And I also want to know if there are any ways to call gc and clear the garbage.</p>
<p>Thank you in advance.</p>",1,0,2020-07-10 08:08:40.257000 UTC,,,0,scala|apache-spark|azure-databricks,824,2020-07-10 08:05:36.580000 UTC,2020-09-11 12:32:30.223000 UTC,,1,0,0,0,,,,,,[]
Is this the right way to build & run target data-validator.jar?,"<p>I'm trying to run target data-validator.jar file on Azure Databricks.
<a href=""https://i.stack.imgur.com/eHAT5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eHAT5.png"" alt=""spark-submit-on-databricks"" /></a></p>
<p>I was able to use the config.yaml file for validating data in the parquet file.</p>
<p>The error can be seen like:</p>
<pre><code>OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0
Warning: Ignoring non-Spark config property: libraryDownload.sleepIntervalSeconds
Warning: Ignoring non-Spark config property: libraryDownload.timeoutSeconds
Warning: Ignoring non-Spark config property: eventLog.rolloverIntervalSeconds
21/12/30 17:11:09 INFO Main$: Logging configured!
21/12/30 17:11:09 INFO Main$: Data Validator
21/12/30 17:11:09 INFO ConfigParser$: Parsing `/dbfs/FileStore/shared_uploads/jyoti/config_2.yaml`
21/12/30 17:11:09 INFO ConfigParser$: Attempting to load `/dbfs/FileStore/shared_uploads/jyoti/config_2.yaml` from file system
Exception in thread &quot;main&quot; java.lang.ExceptionInInitializerError
    at com.target.data_validator.validator.RowBased.&lt;init&gt;(RowBased.scala:11)
    at com.target.data_validator.validator.NullCheck.&lt;init&gt;(NullCheck.scala:12)
    at com.target.data_validator.validator.NullCheck$.fromJson(NullCheck.scala:37)
    at com.target.data_validator.validator.JsonDecoders$$anon$7$$anonfun$decoders$2.apply(JsonDecoders.scala:16)
    at com.target.data_validator.validator.JsonDecoders$$anon$7$$anonfun$decoders$2.apply(JsonDecoders.scala:16)
    at com.target.data_validator.validator.JsonDecoders$$anon$7$$anonfun$2.apply(JsonDecoders.scala:32)
    at com.target.data_validator.validator.JsonDecoders$$anon$7$$anonfun$2.apply(JsonDecoders.scala:32)
    at scala.Option.map(Option.scala:230)
    at com.target.data_validator.validator.JsonDecoders$$anon$7.com$target$data_validator$validator$JsonDecoders$$anon$$getDecoder(JsonDecoders.scala:32)
    at com.target.data_validator.validator.JsonDecoders$$anon$7$$anonfun$apply$3.apply(JsonDecoders.scala:27)
    at com.target.data_validator.validator.JsonDecoders$$anon$7$$anonfun$apply$3.apply(JsonDecoders.scala:27)
    at cats.syntax.EitherOps$.flatMap$extension(either.scala:149)
    at com.target.data_validator.validator.JsonDecoders$$anon$7.apply(JsonDecoders.scala:27)
    at io.circe.SeqDecoder.apply(SeqDecoder.scala:17)
    at io.circe.Decoder$class.tryDecode(Decoder.scala:36)
    at io.circe.SeqDecoder.tryDecode(SeqDecoder.scala:6)
    at com.target.data_validator.ConfigParser$anon$importedDecoder$macro$15$1$$anon$6.apply(ConfigParser.scala:21)
    at io.circe.generic.decoding.DerivedDecoder$$anon$1.apply(DerivedDecoder.scala:13)
    at io.circe.Decoder$$anon$28.apply(Decoder.scala:178)
    at io.circe.Decoder$$anon$28.apply(Decoder.scala:178)
    at io.circe.SeqDecoder.apply(SeqDecoder.scala:17)
    at io.circe.Decoder$class.tryDecode(Decoder.scala:36)
    at io.circe.SeqDecoder.tryDecode(SeqDecoder.scala:6)
    at com.target.data_validator.ConfigParser$anon$importedDecoder$macro$81$1$$anon$10.apply(ConfigParser.scala:28)
    at io.circe.generic.decoding.DerivedDecoder$$anon$1.apply(DerivedDecoder.scala:13)
    at io.circe.Json.as(Json.scala:106)
    at com.target.data_validator.ConfigParser$.configFromJson(ConfigParser.scala:28)
    at com.target.data_validator.ConfigParser$$anonfun$parse$1.apply(ConfigParser.scala:65)
    at com.target.data_validator.ConfigParser$$anonfun$parse$1.apply(ConfigParser.scala:65)
    at cats.syntax.EitherOps$.flatMap$extension(either.scala:149)
    at com.target.data_validator.ConfigParser$.parse(ConfigParser.scala:65)
    at com.target.data_validator.ConfigParser$.parseFile(ConfigParser.scala:60)
    at com.target.data_validator.Main$.loadConfigRun(Main.scala:23)
    at com.target.data_validator.Main$.main(Main.scala:171)
    at com.target.data_validator.Main.main(Main.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:951)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.IllegalArgumentException: requirement failed: Literal must have a corresponding value to bigint, but class Integer found.
    at scala.Predef$.require(Predef.scala:281)
    at org.apache.spark.sql.catalyst.expressions.Literal$.validateLiteralValue(literals.scala:249)
    at org.apache.spark.sql.catalyst.expressions.Literal.&lt;init&gt;(literals.scala:341)
    at org.apache.spark.sql.catalyst.expressions.Literal$.create(literals.scala:174)
    at com.target.data_validator.validator.ValidatorBase$.&lt;init&gt;(ValidatorBase.scala:139)
    at com.target.data_validator.validator.ValidatorBase$.&lt;clinit&gt;(ValidatorBase.scala)
    ... 47 more

</code></pre>
<p>So the salary column has a null in the 5th row.
<a href=""https://i.stack.imgur.com/vCWHA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vCWHA.png"" alt=""salary-null"" /></a></p>
<p>And I have kept a nullCheck on the salary column.</p>
<p><strong>config_2.yaml</strong></p>
<pre><code>numKeyCols: 2
numErrorsToReport: 742

tables:
  - parquetFile: /dbfs/FileStore/shared_uploads/jyoti/userdata1.parquet
    checks:
      - type: nullCheck
        column: salary
</code></pre>
<p>While building the fat jar file, I had removed the test folder from the project data-validator/src/ as I was getting errors during build.</p>
<p><a href=""https://i.stack.imgur.com/DYRRv.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DYRRv.jpg"" alt=""removed test folder"" /></a></p>
<p>Am I missing any step(s) while building it? or
Is it just that datavalidator is expecting a bigint and my parquet file doesn't have bigint?</p>",0,0,2021-12-28 12:47:52.677000 UTC,,2021-12-30 18:20:51.597000 UTC,0,apache-spark|validation|azure-databricks,57,2020-09-27 19:00:14.083000 UTC,2022-03-05 16:15:56.760000 UTC,,11,0,0,5,,,,,,[]
Connection of Event hubs to Azure Databricks,"<p>I want to add libraries in Azure Databricks for connecting to Event Hubs. I will be writing notebooks in python. So which library should I add for connecting to Event Hubs?</p>

<p>As per my search till now I got a spark connecting library in Maven coordinates. But I don't think I will be able to import it in python.</p>",1,0,2019-07-10 14:43:00.313000 UTC,1.0,2019-07-11 10:44:13.283000 UTC,0,python|azure|azure-databricks,195,2019-06-06 12:01:42.137000 UTC,2021-07-09 14:14:02.727000 UTC,,31,6,0,16,,,,,,[]
Pyspark Job aborted due to stage error.: kmeans implementation via map reduce,"<p>As part of my distributed databases course we have a task to implement a kmeans algorithm via the usage of map reduce functions, however i cant seem to access or display the results of my map reduce code.
Here is my implementation:</p>

<pre><code>def Find_dist(x,y):
  sum = 0
  vec1= list(x[0])
  vec2 = list(y)
  for i in range(len(vec1)):
    sum = sum +(vec1[i]-vec2[i])*(vec1[i]-vec2[i])
  return sum

def mapper(centers, datapoint):
  min = Find_dist(datapoint,cent[0])
  closest = cent[0]
  for i in range(1,len(cent)):
    curr = Find_dist(datapoint,cent[i])
    if curr &lt; min:
      min = curr
      closest = cent[i]
  yield (closest,datapoint)

def combiner(Key,Values):
  sum = [0]*len(Key)
  counter = 0
  for datapoint in Values:
    vec = list(datapoint[0])
    counter = counter+1
    sum = sum+vec
  point = Row(vec)
  result = (counter,point)
  yield (Key, result)

def Reducer(Key,Values):
  sum = [0]*len(Key)
  total_counter = 0
  for value in Values:
    counter = value[0]
    data = list(value[1][0])
    sum = sum+data
    total_counter = total_counter+counter
  avg = [0]*len(Key)
  for i in range(len(Key)):
    avg[i] = sum[i]/total_counted
  centroid = Row(avg)
  yield (Key, centroid)

def kmeans_fit(data,k,max_iter):
  centers = data.rdd.takeSample(False,k,seed=42)
  for i in range(max_iter):
    mapped = data.rdd.map(lambda x: mapper(centers,x))
    combined = mapped.combineByKey(map, combiner,combiner)
    reduced = combined.reduceByKey(lambda x: Reducer(x)).collect()
    flag = True
    for i in range(k):
      if(reduced[i][1] != reduced[i][0] ):
        for j in range(k):
          centers[i] = reduced[i][1]
        flag = False
        break
    if (flag):
      break
  return centers
data = spark.read.parquet(""/mnt/ddscoursedatabricksstg/ddscoursedatabricksdata/random_data.parquet"")
kmeans_fit(data,5,10)
</code></pre>

<p>However I am getting this error when i try to display the data in each of the map combine or reduce stages:</p>

<pre><code>org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1077.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1077.0 (TID 29254, 10.139.64.7, executor 20): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
&lt;command-2268222378870327&gt; in &lt;module&gt;
     59   return centers
     60 data = spark.read.parquet(""/mnt/ddscoursedatabricksstg/ddscoursedatabricksdata/random_data.parquet"")
---&gt; 61 kmeans_fit(data,5,10)

&lt;command-2268222378870327&gt; in kmeans_fit(data, k, max_iter)
     47     mapped = data.rdd.map(lambda x: mapper(centers,x))
     48     combined = mapped.combineByKey(map, combiner,combiner)
---&gt; 49     reduced = combined.reduceByKey(lambda x: Reducer(x)).collect()
     50     flag = True
     51     for i in range(k):

/databricks/spark/python/pyspark/rdd.py in collect(self)
    829         # Default path used in OSS Spark / for non-credential passthrough clusters:
    830         with SCCallSiteSync(self.context) as css:
--&gt; 831             sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
    832         return list(_load_from_socket(sock_info, self._jrdd_deserializer))
    833 

/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1255         answer = self.gateway_client.send_command(command)
   1256         return_value = get_return_value(
-&gt; 1257             answer, self.gateway_client, self.target_id, self.name)
   1258 
   1259         for temp_arg in temp_args:

/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
     61     def deco(*a, **kw):
     62         try:
---&gt; 63             return f(*a, **kw)
     64         except py4j.protocol.Py4JJavaError as e:
     65             s = e.java_exception.toString()

/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    326                 raise Py4JJavaError(
    327                     ""An error occurred while calling {0}{1}{2}.\n"".
--&gt; 328                     format(target_id, ""."", name), value)
    329             else:
    330                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1077.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1077.0 (TID 29254, 10.139.64.7, executor 20): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/databricks/spark/python/pyspark/worker.py"", line 480, in main
    process()
  File ""/databricks/spark/python/pyspark/worker.py"", line 470, in process
    out_iter = func(split_index, iterator)
  File ""/databricks/spark/python/pyspark/rdd.py"", line 2542, in pipeline_func
    return func(split, prev_func(split, iterator))
  File ""/databricks/spark/python/pyspark/rdd.py"", line 2542, in pipeline_func
    return func(split, prev_func(split, iterator))
  File ""/databricks/spark/python/pyspark/rdd.py"", line 353, in func
    return f(iterator)
  File ""/databricks/spark/python/pyspark/rdd.py"", line 1904, in combineLocally
    merger.mergeValues(iterator)
  File ""/databricks/spark/python/pyspark/shuffle.py"", line 238, in mergeValues
    for k, v in iterator:
  File ""&lt;command-2268222378870327&gt;"", line 10, in mapper
  File ""&lt;command-2268222378870327&gt;"", line 3, in Find_dist
TypeError: 'float' object is not iterable

    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)
    at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:676)
    at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:659)
    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)
    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
    at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)
    at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
    at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)
    at org.apache.spark.scheduler.Task.run(Task.scala:113)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:537)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:543)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)
    at scala.Option.foreach(Option.scala:257)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2582)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2280)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2302)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2321)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2346)
    at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:997)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
    at org.apache.spark.rdd.RDD.withScope(RDD.scala:392)
    at org.apache.spark.rdd.RDD.collect(RDD.scala:996)
    at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:247)
    at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
    at sun.reflect.GeneratedMethodAccessor220.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
    at py4j.Gateway.invoke(Gateway.java:295)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:251)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/databricks/spark/python/pyspark/worker.py"", line 480, in main
    process()
  File ""/databricks/spark/python/pyspark/worker.py"", line 470, in process
    out_iter = func(split_index, iterator)
  File ""/databricks/spark/python/pyspark/rdd.py"", line 2542, in pipeline_func
    return func(split, prev_func(split, iterator))
  File ""/databricks/spark/python/pyspark/rdd.py"", line 2542, in pipeline_func
    return func(split, prev_func(split, iterator))
  File ""/databricks/spark/python/pyspark/rdd.py"", line 353, in func
    return f(iterator)
  File ""/databricks/spark/python/pyspark/rdd.py"", line 1904, in combineLocally
    merger.mergeValues(iterator)
  File ""/databricks/spark/python/pyspark/shuffle.py"", line 238, in mergeValues
    for k, v in iterator:
  File ""&lt;command-2268222378870327&gt;"", line 10, in mapper
  File ""&lt;command-2268222378870327&gt;"", line 3, in Find_dist
TypeError: 'float' object is not iterable

    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)
    at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:676)
    at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:659)
    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)
    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
    at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)
    at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
    at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)
    at org.apache.spark.scheduler.Task.run(Task.scala:113)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:537)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:543)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
</code></pre>

<p>Thanks in advance for anyone willing to assist.</p>",0,0,2020-05-26 14:32:17.507000 UTC,,2020-05-26 16:28:36.033000 UTC,1,python|pyspark|mapreduce|k-means|azure-databricks,202,2018-11-06 08:45:26.337000 UTC,2022-02-27 16:44:55.723000 UTC,,39,0,0,12,,,,,,[]
Is Feature Branching still (or ever) considered a bad practice?,"<p>Coming from the TFS world and having just gotten comfortable enough with Git, I am about to propose to my team that we should incorporate the <a href=""https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow"">Gitflow workflow</a> as pointed out by the <a href=""http://nvie.com/posts/a-successful-git-branching-model/"">famous article by Vincent Dressen</a> going forward.</p>

<p>Almost all modern-day literature surrounding branching strategies voice the effectiveness of the Gitflow workflow, which is an extended version of feature branching, but dated articles from influential engineers, such as <a href=""http://martinfowler.com/bliki/FeatureBranch.html"">Martin Fowler's Feature Branch article (2009)</a>, discredit feature branching in general in favor of continuous integration.</p>

<p><a href=""http://arialdomartini.wordpress.com/2011/11/02/help-me-because-i-think-martin-fowler-has-a-merge-paranoia/"">Some of his critics</a> stated that Fowler's opposition to feature branching was in part because he was using SVN as his VCS, which was an ineffective tool for merging and therefore led Fowler to recommend a branching anti-pattern ""merge paranoia"".</p>

<p>Fowler then responded in 2011 <a href=""http://martinfowler.com/bliki/SemanticConflict.html"">by saying DVCS systems may make merging easier, but they still don't solve semantic conflicts</a>. Now in 2014, we have language aware merge tools such as <a href=""http://www.semanticmerge.com/"">Semantic Merge</a>, which might solve this problem altogether.</p>

<p>My questions are</p>

<ol>
<li><p><strong>Is feature branching and continuous integration mutually exclusive?</strong></p></li>
<li><p><strong>How relevant is Fowler's article in modern day development, especially with our accessibility to tools like SourceTree, Git, Jenkins, and other code review software that make feature branching and the like much easier?</strong></p></li>
</ol>",4,8,2014-09-29 21:19:41.457000 UTC,1.0,2014-09-29 22:02:26.363000 UTC,8,git|continuous-integration|branch|dvcs,2489,2010-05-26 13:32:38.323000 UTC,2022-01-29 19:45:29.217000 UTC,"San Francisco, CA, United States",1220,823,19,211,,,,,,[]
Can I run gremlin queries on cosmos-db (graph) from azure databricks notebook?,"<p>Is there a direct integration of Gremlin into azure databricks notebook ?</p>

<p>I have a graph into cosmosDb and I want to run some gremlin queries. For e.g</p>

<p><code>g.V().hasLabel('x').out('y').out('z')</code></p>

<p>I run the queries from azure portal but for a large set of data it will throw </p>

<blockquote>
  <p>[""Request rate is large""]</p>
</blockquote>

<p>Switching to Azure databricks, I've created a cluster, add ""azure-cosmosdb-spark"" library and from a python notebook a was able to run only sql queries like ""SELECT * FROM c"" </p>

<p>I've tried using spark-gremlin and hadoop-gremlin libraries, but the only way I can see is right now is to load all my nodes and edges into a dataFrame (label by label) and then change it into graphFrame and only then, after I rebuild the graph here I can make some traversal queries, but not Gremlin (yet). (and having millions of nodes and edges I don't know how much this can help me).</p>

<p>I will like to know if a direct gremlin query from notebook on cosmosDb is supported, or at least a direct migration of the graph into dataFrame ? </p>",1,0,2019-01-28 11:33:31.100000 UTC,,2019-12-10 00:22:39.897000 UTC,0,apache-spark|graph|azure-cosmosdb|gremlin|azure-databricks,451,2018-11-14 15:15:53.957000 UTC,2019-02-26 12:46:47.927000 UTC,Romania,1,0,0,2,,,,,,[]
displayHTML in Azure Databricks shows only a small slice of a leaflet map,"<p>I'm attempting to display an HTML file (a leaflet map) using <code>displayHTML()</code> inside a notebook. The file is created with the R mapview package, after following <a href=""https://docs.databricks.com/notebooks/visualizations/htmlwidgets.html"" rel=""nofollow noreferrer"">https://docs.databricks.com/notebooks/visualizations/htmlwidgets.html</a> to make it all work.  The HTML file is valid and I can open it in a browser window with full functionality.</p>
<p>What's happening in the notebook is that in the cell output, the map is only rendering as a very short slice of the map:</p>
<p><a href=""https://i.stack.imgur.com/xC3xN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xC3xN.png"" alt=""enter image description here"" /></a></p>
<p>The visual appears to be fully functional, but I cannot figure out how to increase the size of the render/display area. I imagine it's either something within the HTML itself (like it's fitting to the size of the pane of the Databricks cell output, which it's thinking is a single line, perhaps), or there's a Databricks setting I'm not seeing.  Resizing the cell output with a click-drag on the bottom-right corner has no effect.</p>
<p>Is there a way to make the map take up more space in a cell output?</p>
<p>DBR 9.1 LTS</p>",0,0,2022-03-01 16:13:23.337000 UTC,,,0,leaflet|azure-databricks|mapview,16,2014-07-19 07:05:27.467000 UTC,2022-03-03 17:19:33.510000 UTC,"Seattle, WA, United States",95,5,0,34,,,,,,[]
Databrick pyspark Error While getting Excel data from my Azure Blob Storage,"<p>I want to read an excel file with multiple sheets in my Blob storage Azure Gen2 using Databrick pyspark. I already install the maven package.
Below my code :</p>
<pre><code>df = spark.read.format('com.crealytics.spark.excel') \
.option(&quot;header&quot;, &quot;true&quot;) \
.option(&quot;useHeader&quot;, &quot;true&quot;) \
.option(&quot;treatEmptyValuesAsNulls&quot;, &quot;true&quot;) \
.option(&quot;inferSchema&quot;, &quot;true&quot;) \
.option(&quot;sheetName&quot;, &quot;sheet1&quot;) \
.option(&quot;maxRowsInMemory&quot;, 10) \
.load(file_path)    
</code></pre>
<p>Running this code I get this error:</p>
<blockquote>
<p>Py4JJavaError: An error occurred while calling o323.load.
: java.lang.NoClassDefFoundError: Could not initialize class com.crealytics.spark.excel.WorkbookReader$
at com.crealytics.spark.excel.DefaultSource.createRelation(DefaultSource.scala:22)
at com.crealytics.spark.excel.DefaultSource.createRelation(DefaultSource.scala:13)
at com.crealytics.spark.excel.DefaultSource.createRelation(DefaultSource.scala:8)
at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:390)
at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:444)
at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:400)
at scala.Option.getOrElse(Option.scala:189)
at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:400)
at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:287)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
at py4j.Gateway.invoke(Gateway.java:295)
at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
at py4j.commands.CallCommand.execute(CallCommand.java:79)
at py4j.GatewayConnection.run(GatewayConnection.java:251)
at java.lang.Thread.run(Thread.java:748)</p>
</blockquote>
<p>Any help is appreciated. Thanks</p>",1,7,2021-12-08 15:48:37.327000 UTC,,,1,pyspark|azure-databricks,83,2019-07-12 08:12:41.823000 UTC,2022-03-04 15:54:16.923000 UTC,,23,0,0,1,,,,,,[]
Not able to configure databricks with external hive metastore,"<p>I am following this document <a href=""https://docs.databricks.com/data/metastores/external-hive-metastore.html#spark-configuration-options"" rel=""nofollow noreferrer"">https://docs.databricks.com/data/metastores/external-hive-metastore.html#spark-configuration-options</a>
to connect to my external hive metastore. My metastore version is 3.1.0 and followed the document.</p>

<p>docs.databricks.comdocs.databricks.com
External Apache Hive metastore — Databricks Documentation
Learn how to connect to external Apache Hive metastores in Databricks.
10:51
I have getting this error when trying to connect to external hive metastore</p>

<pre><code>org/apache/hadoop/hive/conf/HiveConf when creating Hive client using classpath: 
Please make sure that jars for your version of hive and hadoop are included in the paths passed to spark.sql.hive.metastore.jars
</code></pre>

<p>spark.sql.hive.metastore.jars=/databricks/hive_metastore_jars/*</p>

<p>When I do an ls on /databricks/hive_metastore_jars/, I can see all copied files
10:52
Do I need to copy any hive specific files and upload it in this folder?</p>

<p>I did exactly what was mentioned in the site</p>

<p>These are the contents of my hive_metastore_jars</p>

<pre><code>total 56K
drwxr-xr-x 3 root root 4.0K Mar 24 05:06 1585025573715-0
drwxr-xr-x 2 root root 4.0K Mar 24 05:06 d596a6ec-e105-4a6e-af95-df3feffc263d_resources
drwxr-xr-x 3 root root 4.0K Mar 24 05:06 repl
drwxr-xr-x 2 root root 4.0K Mar 24 05:06 spark-2959157d-2018-441a-a7d3-d7cecb8a645f
drwxr-xr-x 4 root root 4.0K Mar 24 05:06 root
drwxr-xr-x 2 root root 4.0K Mar 24 05:06 spark-30a72ee5-304c-432b-9c13-0439511fb0cd
drwxr-xr-x 2 root root 4.0K Mar 24 05:06 spark-a19d167b-d571-4e58-a961-d7f6ced3d52f
-rwxr-xr-x 1 root root 5.5K Mar 24 05:06 _CleanRShell.r3763856699176668909resource.r
-rwxr-xr-x 1 root root 9.7K Mar 24 05:06 _dbutils.r9057087446822479911resource.r
-rwxr-xr-x 1 root root  301 Mar 24 05:06 _rServeScript.r1949348184439973964resource.r
-rwxr-xr-x 1 root root 1.5K Mar 24 05:06 _startR.sh5660449951005543051resource.r
</code></pre>

<p>Am I missing anything?</p>

<p>Strangely If I look into the cluster boot logs here is what I get</p>

<pre><code>20/03/24 07:29:05 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionDriverName unknown - will be ignored
20/03/24 07:29:05 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionURL unknown - will be ignored
20/03/24 07:29:05 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionUserName unknown - will be ignored
20/03/24 07:29:05 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
20/03/24 07:29:05 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionPassword unknown - will be ignored
20/03/24 07:29:05 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
20/03/24 07:29:05 INFO Persistence: Property datanucleus.schema.autoCreateAll unknown - will be ignored

20/03/24 07:29:09 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
20/03/24 07:29:09 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
</code></pre>

<p>I have already set the above configurations and it shows in the logs as well</p>

<pre><code>20/03/24 07:28:59 INFO SparkContext: Spark configuration:
spark.hadoop.javax.jdo.option.ConnectionDriverName=org.mariadb.jdbc.Driver
spark.hadoop.javax.jdo.option.ConnectionPassword=*********(redacted)
spark.hadoop.javax.jdo.option.ConnectionURL=*********(redacted)
spark.hadoop.javax.jdo.option.ConnectionUserName=*********(redacted)
</code></pre>

<p>Also version information is available in my hive metastore, I can connect to mysql and see it it shows
SCHEMA_VERSION : 3.1.0
VER_ID = 1</p>",2,0,2020-03-24 05:26:39.090000 UTC,,2020-03-24 07:40:25.120000 UTC,0,azure-databricks,1015,2009-05-21 11:08:27.700000 UTC,2022-03-03 09:20:28.020000 UTC,"Pune, India",3733,108,1,410,,,,,,[]
How to streamout or extract only inserts/adds from a Databricks delta file?,"<p>I have a scenario, where I want to run a Spark Structured Streaming job to read a Databricks Delta Source File and extract only the inserts to the Source file. I want to filter-out any Updates/Deletes.</p>

<p>I was trying following on a smaller file but the code does not seem to do what I expect.</p>

<pre><code>spark
.readStream
.format(""delta"")
.option(""latestFirst"",""true"")
.option(""ignoreDeletes"", ""true"")
.option(""ignoreChanges"",""true"")
.load(""/mnt/data-lake/data/bronze/accounts"")
.writeStream
.format(""delta"")
.outputMode(""append"")
.option(""checkpointLocation"",""/mnt/data-lake/tmp/chkpnt_accounts_inserts"")
.option(""path"",""/mnt/data-lake/tmp/accounts_inserts"")
.start()
</code></pre>",0,0,2020-01-11 02:34:38.487000 UTC,1.0,2020-01-14 13:29:41.660000 UTC,1,apache-spark|spark-structured-streaming|azure-databricks|delta-lake,96,2016-01-21 02:29:13.593000 UTC,2022-03-04 22:01:26.687000 UTC,"Phoenix, AZ, USA",425,43,0,65,,,,,,[]
Accessing Azure Data Lake Storage gen2 from Scala,"<p>I am able to connect to ADLS gen2 from a notebook running on Azure Databricks but am unable to connect from a job using a jar.  I used the same settings as I did in the notebook, save for the use of dbutils.</p>

<p>I used the same setting for Spark conf from the notebook in the Scala code.</p>

<p>Notebook:<br></p>

<pre><code>spark.conf.set(
""fs.azure.account.key.xxxx.dfs.core.windows.net"",
dbutils.secrets.get(scope = ""kv-secrets"", key = ""xxxxxx""))

spark.conf.set
(""fs.azure.createRemoteFileSystemDuringInitialization"", ""true"")

spark.conf.set
(""fs.azure.createRemoteFileSystemDuringInitialization"", ""false"")

val rdd = sqlContext.read.format
(""csv"").option(""header"", 
""true"").load(
""abfss://catalogs@xxxx.dfs.core.windows.net/test/sample.csv"")
// Convert rdd to data frame using toDF; the following import is 
//required to use toDF function.
val df: DataFrame = rdd.toDF()
// Write file to parquet
df.write.parquet
(""abfss://catalogs@xxxx.dfs.core.windows.net/test/Sales.parquet"")
</code></pre>

<p>Scala code:<br></p>

<pre><code>val sc = SparkContext.getOrCreate()
val spark = SparkSession.builder().getOrCreate()
sc.getConf.setAppName(""Test"")

sc.getConf.set(""fs.azure.account.key.xxxx.dfs.core.windows.net"",
""&lt;actual key&gt;"")

sc.getConf.set(""fs.azure.account.auth.type"", ""OAuth"")

sc.getConf.set(""fs.azure.account.oauth.provider.type"",
""org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"")

sc.getConf.set(""fs.azure.account.oauth2.client.id"", ""&lt;app id&gt;"")

sc.getConf.set(""fs.azure.account.oauth2.client.secret"", ""&lt;app password&gt;"")

sc.getConf.set(""fs.azure.account.oauth2.client.endpoint"",
  ""https://login.microsoftonline.com/&lt;tenant id&gt;/oauth2/token"")

sc.getConf.set
(""fs.azure.createRemoteFileSystemDuringInitialization"", ""false"")

val sqlContext = spark.sqlContext
val rdd = sqlContext.read.format
(""csv"").option(""header"", 
""true"").load
(""abfss://catalogs@xxxx.dfs.core.windows.net/test/sample.csv"")
// Convert rdd to data frame using toDF; the following import is 
//required to use toDF function.
val df: DataFrame = rdd.toDF()
println(df.count())
// Write file to parquet

df.write.parquet
(""abfss://catalogs@xxxx.dfs.core.windows.net/test/Sales.parquet"")
</code></pre>

<p>I expected the parquet file to get written.  Instead I get the following error:
19/04/20 13:58:40 ERROR Uncaught throwable from user code: Configuration property xxxx.dfs.core.windows.net not found.
at shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:385)
at shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:802)
at shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.(AzureBlobFileSystemStore.java:133)
at shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:103)
at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)</p>",1,0,2019-04-20 15:38:05.453000 UTC,1.0,,1,scala|apache-spark|azure-data-lake|azure-databricks,2399,2013-03-04 03:21:57.910000 UTC,2020-07-06 17:24:20.270000 UTC,"Atlanta, GA, USA",113,2,0,11,,,,,,[]
Unable to write to Azure Cosmos DB from Databricks,"<p>I'm attempting to follow write to Azure Cosmos DB, but failing with the following error:</p>

<blockquote>
  <p>flights.write.format(""com.microsoft.azure.cosmosdb.spark"").options(writeConfig).save()</p>
  
  <p>AttributeError: 'NoneType' object has no attribute 'write'</p>
</blockquote>

<p>I'm using the following document as a guide:<br>
<a href=""https://docs.microsoft.com/en-us/azure/cosmos-db/spark-connector"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/cosmos-db/spark-connector</a></p>

<p>Can someone let me know where I'm going wrong?</p>",1,3,2019-12-23 13:25:11.637000 UTC,,2019-12-23 16:12:35.093000 UTC,-1,azure|azure-cosmosdb|azure-databricks,757,2016-06-25 22:07:19.907000 UTC,2021-11-21 00:16:09.640000 UTC,,716,49,1,223,,,,,,[]
Amazon Neptune query performance for simple label based query,"<p>I have a simple query based on a rare vertex label on a large Graph (several 100 Million vertices):</p>
<pre><code>g.V().hasLabel(&quot;mylabel&quot;).count()
g.V().hasLabel(&quot;mylabel&quot;).toList()
g.V().hasLabel(&quot;mylabel&quot;).order().by(&quot;time&quot;).toList()
</code></pre>
<p>The first query that counts the vertices runs fairly quick and returns a count of 40. The next two queries, however, run for a long time, the last one close to 10 minutes. To my understanding, Neptune has an index that allows to lookup the vertices by label, thus I would expect each of the queries to be very fast. Is there an explanation for this behaviour?</p>",0,1,2022-01-11 09:54:37.647000 UTC,,2022-01-11 13:13:38.507000 UTC,0,performance|gremlin|amazon-neptune|tinkerpop3,71,2012-08-09 12:31:35.957000 UTC,2022-03-04 09:47:05.630000 UTC,,2129,19,1,44,,,,,,[]
"AzureDatabrick:Error in SQL statement: package.TreeNodeException: execute, tree: Exchange hashpartitioning","<p>currently working with 2 temporary views A &amp; B . while selecting records from individual views it gives results. But when creating  3rd view C with join of A &amp; B it works but when we run any select query on 3rd view C it gives error  ""Error in SQL statement: package.TreeNodeException: execute, tree: Exchange hashpartitioning""
Please help whats going wrong here.</p>",1,0,2020-05-29 07:45:09.750000 UTC,,,0,azure-databricks,917,2020-05-29 07:40:35.907000 UTC,2021-12-09 19:37:47.727000 UTC,,1,0,0,1,,,,,,[]
Create Table in Azure Table Storage using REST API from R,"<p>I am working on Azure Databricks in an R notebook and in order to write meta data generated by my R code, I try to connect/read/write to/from/to Azure Table Storage Service. </p>

<p><a href=""https://stackoverflow.com/questions/44519413/azure-put-blob-authentication-fails-in-r/"">Here</a> I learned how to access and read a table (named ""myTable), which with some modifications works fine. Here is what I did:</p>

<pre><code> key &lt;- &lt;myKey&gt;
 account &lt;- &lt;myAccount&gt;

 url &lt;- paste0(""https://"", account, "".table.core.windows.net/myTable()"")
 requestdate &lt;- format(Sys.time(),""%a, %d %b %Y %H:%M:%S %Z"", tz=""GMT"")
 #content_length &lt;- 0

 signature_string &lt;- paste0(""GET"", ""\n"",            # HTTP Verb
                       ""\n"",                   # Content-MD5
                       ""text/plain"", ""\n"",     # Content-Type
                       requestdate, ""\n"",                   # Date
                       # Here comes the Canonicalized Resource
                       ""/"",account, ""/"",""myTable()"" )

 header &lt;- add_headers(Authorization=paste0(""SharedKey "",account,"":"", 
                                            RCurl::base64(digest::hmac(key = 
                                                                         RCurl::base64Decode(key, mode = ""raw""),
                                                                       object = enc2utf8(signature_string),
                                                                       algo = ""sha256"", raw = TRUE))),
                       `x-ms-date`= requestdate,
                       `x-ms-version`= ""2015-02-21"",
                       `Content-Type`=""text/plain"")

 GET(url, config = header, verbose()) -&gt; res
</code></pre>

<p>Now, I would like to create another table as well as to add new entities to existing tables. I adopted above approach and modified it to </p>

<pre><code> url &lt;- paste0(""https://"", account, "".table.core.windows.net/TABLES"")
 requestdate &lt;- format(Sys.time(),""%a, %d %b %Y %H:%M:%S %Z"", tz=""GMT"")

 signature_string &lt;- paste0(""POST"", ""\n"",            # HTTP Verb
                       ""\n"",                   # Content-Encoding  
                       ""\n"",                   # Content-Language
                       content_length, ""\n"",   # Content-Length
                       ""\n"",                   # Content-MD5
                       ""application/json"", ""\n"",     # Content-Type
                       ""\n"",                   # Date
                       ""\n"",                   # If-Modified-Since
                       ""\n"",                   # If-Match  
                       ""\n"",                   # If-None-Match
                       ""\n"",                   # If-Unmodified-Since
                       ""\n"",                   # Range
                       # Here comes the Canonicalized Headers
                       # ""x-ms-blob-type:BlockBlob"",""\n"",
                       ""x-ms-date:"",requestdate,""\n"",
                       #""x-ms-version:2015-02-21"",""\n"",
                       # Here comes the Canonicalized Resource
                       ""/"",account, ""/"",""TABLES"")

 header &lt;- add_headers(Authorization=paste0(""SharedKey "",account,"":"", 
                                            RCurl::base64(digest::hmac(key = 
                                                                         RCurl::base64Decode(key, mode = ""raw""),
                                                                       object = enc2utf8(signature_string),
                                                                       algo = ""sha256"", raw = TRUE))),
                       `x-ms-date`= requestdate,
                      # `x-ms-version`= ""2015-02-21"",
                       `Content-Type`=""application/json"",
                       `Content-Length`=100)

 body &lt;- paste0(""{"", ""\n"",
                       ""'TableName':'newTable'"",  ""\n"",                          
           ""}"")

 POST(url = url, config = header, body = body, encode = ""json"", handle = NULL) -&gt; res
</code></pre>

<p>Running that gives me status 403. The exact message is:</p>

<pre><code>  Response [https://myAccount.table.core.windows.net/TABLES]
  Date: 2020-01-31 18:47
  Status: 403
  Content-Type: application/json
  Size: 437 B
</code></pre>

<p>﻿      
      
      <code>AuthenticationFailed</code>
      Server failed to authenticate the request. Make s...
      RequestId:9bc35a5f-c002-1666-d88043000000
      Time:2020-01-31T18:47:36.6598228Z</p>

<p>I played around with the inputs to the signature_string for hours without success. Also I tried different body definitions as e.g. </p>

<pre><code> request_body &lt;- data.frame(TableName = ""access"")
 body &lt;- jsonlite::toJSON(request_body, auto_unbox = T)
</code></pre>

<p>I am not sure about the following inputs: Content-Length, Content-Type, x-ms-version, request-body and whether I miss anything else.</p>

<p>Could someone give me advise how to write a correct header to create new tables and to add new entities. If there is a more elegant way, this would be also highly appreciated.</p>",0,3,2020-01-31 18:54:42.460000 UTC,1.0,,0,r|azure|rest|azure-table-storage|azure-databricks,199,2017-10-09 15:56:03.437000 UTC,2022-02-17 09:42:19.327000 UTC,,61,8,0,16,,,,,,[]
"After transition from CVCS to DVCS , what problems were solved? what still remain unsolved?","<p>After transition from Centralized Version Control System(CVCS) to Distruted Version Control System(DVCS), what problems were solved? What still remain unsolved?</p>",1,0,2015-11-08 15:01:47.597000 UTC,1.0,,0,version-control|dvcs|cvcs,35,2015-01-26 05:55:35.330000 UTC,2019-03-28 13:33:34.003000 UTC,,1,0,0,1,,,,,,[]
How do I best manage SQL Server insertions from Databricks spark session and identity columns?,"<p>I have a batch-processing transaction data transformation/validation pipeline written in a Scala Databricks notebook, and when the pipeline is finished, it dumps my validated data into a SQL server for later use.</p>
<p>Ongoing requirements are beginning to focus on transaction metadata, facts about transactions rather than in-situ data transformations. These will live on a separate table in the SQL server, but will retain a 1:1 relationship with transactions.</p>
<p>Currently, in the spark session, a unique index ID is assigned to each transaction in the batch with this function</p>
<pre><code>  def addColumnIndex(df: DataFrame) = {
    spark.sqlContext.createDataFrame(
      df.rdd.zipWithIndex.map {
        case (row, index) =&gt; Row.fromSeq(row.toSeq :+ index)
      },
      StructType(df.schema.fields :+ StructField(&quot;index&quot;, LongType, false)))
  }
</code></pre>
<p>This helps some of our transformations but, because we're batch-processing, IDs are not unique across batches. Our strategy is to also have a batchID on each transaction, and then use a composite key in the sql server, a combination batch and index. This ignores the native identity column on the table in SQL.</p>
<p>When writing to SQL, we use a simple</p>
<p>.write
.mode(SaveMode.Append)
.jdbc(jdbcUrl, &quot;Transactions&quot;, connectionProperties)</p>
<p>On insertion, transactions are given a unique identity ID by the sql server. Is there a way to reliably use this ID for the relationship between transactions and fact tables? Is the composite key approach overcomplicating this?</p>
<p>Basically, can we make Spark respect the unique IDs it creates?</p>",0,0,2021-11-12 17:17:20.893000 UTC,,,1,sql-server|scala|apache-spark|azure-databricks,43,2019-07-08 16:24:36.580000 UTC,2022-03-02 15:40:21.380000 UTC,"Northport, MI, USA",155,26,1,21,,,,,,[]
"Netbeans/Mercurial - Define multiple push/pull repositories under the ""Team -> Share"" menu?","<p>In Netbeans w/Mercurial you have a menu that lists your default push/pull repositories along with an option for ""other"" to manually specify a repository to push/pull:</p>

<p><img src=""https://i.imgur.com/3DcQw.png"" alt=""push/pull menu""></p>

<p>My ""default push/pull"" is set to our central repository, but my working repositories are pulled into a staging repository first. Needless to say, I could use more items in the ""Share"" menu than just ""default push/pull"". Is there a way to define more, so it would look something like:</p>

<pre>
- Share
--- Push to default
--- Push to my-other-repo-1
--- Push to my-other-repo-2
--- Push to other
--- Pull from default
--- Pull from my-other-repo-1
--- Pull from my-other-repo-2
--- Pull from other
</pre>

<p>Currently I just select ""pull from other"" but then have to manually type in the other repositories' information each time.</p>",1,0,2012-02-09 00:53:19.963000 UTC,1.0,2012-03-30 12:18:22.983000 UTC,2,netbeans|mercurial|push|dvcs,632,2012-02-01 16:48:40.820000 UTC,2013-06-16 06:04:35.633000 UTC,,2175,6,0,53,,,,,,[]
Write Spark dataframe into delta lake,"<p>I am trying to convert Spark data frame into delta format using the example code provided by documentation but always getting this strange error. Can you please help or guide?</p>

<pre class=""lang-py prettyprint-override""><code>df_sdf.write.format(""delta"").save(""/mnt/.../delta/"")
</code></pre>

<p>Error looks like:</p>

<pre class=""lang-py prettyprint-override""><code>org.apache.spark.SparkException: Job aborted.

--------------------------------------------------------------------------- Py4JJavaError Traceback (most recent call last) &lt;command-3011941952225495&gt; in &lt;module&gt; ----&gt; 1 df_sdf.write.format(""delta"").save(""/mnt/.../delta/"") /databricks/spark/python/pyspark/sql/readwriter.py in save(self, path, format, mode, partitionBy, **options) 737 self._jwrite.save() 738 else: --&gt; 739 self._jwrite.save(path) 740 741 @since(1.4)
/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in call(self, *args) 1255 answer = self.gateway_client.send_command(command) 1256 return_value = get_return_value( -&gt; 1257 answer, self.gateway_client, self.target_id, self.name) 1258 1259 for temp_arg in temp_args:

/databricks/spark/python/pyspark/sql/utils.py in deco(a, *kw)
</code></pre>",3,0,2020-02-04 16:40:22.757000 UTC,,2020-02-04 17:04:36.280000 UTC,0,apache-spark|pyspark|azure-databricks|delta|delta-lake,3276,2016-01-19 03:14:42.883000 UTC,2021-02-26 21:21:27.440000 UTC,,335,0,0,48,,,,,,[]
Vertex properties order in valuemap,"<pre><code>addV('src').
property(id,'sales__src').
property(&quot;index1&quot;,&quot;brand&quot;).
property(&quot;index2&quot;,&quot;time&quot;).
property(&quot;index3&quot;,&quot;city1&quot;).
property(&quot;index4&quot;,&quot;city2&quot;).
property(&quot;index5&quot;,&quot;city3&quot;).   
property(&quot;index6&quot;,&quot;city4&quot;).
property(&quot;index7&quot;,&quot;city5&quot;).
property(&quot;index8&quot;,&quot;city6&quot;).
property(&quot;index9&quot;,&quot;city7&quot;).
property(&quot;index10&quot;,&quot;city8&quot;).
property(&quot;index11&quot;,&quot;city9&quot;).
property(&quot;index12&quot;,&quot;city10&quot;).
as(&quot;sales_src&quot;).
</code></pre>
<p>When I plot in a graph</p>
<pre><code>    %%gremlin -p v,inE,outV,inE,outV
g.V('city_brand').inE().outV().inE().outV().path().
by(valueMap().with(WithOptions.tokens))
</code></pre>
<p>The order of the properties index became</p>
<p>index1</p>
<p>index11</p>
<p>index12</p>
<p>How to force the order to be in 1,2,3,4 order?</p>",1,1,2021-11-25 03:01:43.770000 UTC,,,0,gremlin|amazon-neptune,24,2021-04-05 13:22:39.740000 UTC,2021-12-24 08:46:30.397000 UTC,,5,0,0,1,,,,,,[]
How to list the files in azure data lake using spark from pycharm(local IDE) which is connected using databricks-connect,"<p>I am working on some code on my local machine on pycharm. 
The execution is done on a databricks cluster, while the data is stored on azure datalake.</p>

<p>basaically, I need to list down the files in azure datalake directory and then apply some reading logic on the files, for this I am using the below code</p>

<pre><code>sc = spark.sparkContext
hadoop = sc._jvm.org.apache.hadoop

fs = hadoop.fs.FileSystem
conf = hadoop.conf.Configuration()

path = hadoop.fs.Path('adl://&lt;Account&gt;.azuredatalakestore.net/&lt;path&gt;')
for f in fs.get(conf).listStatus(path):
    print(f.getPath(), f.getLen())
</code></pre>

<p>the above code runs fine on the databricks notebooks, but when i try to run the same code through pycharm using databricks-connect i get the following error.</p>

<pre><code>""Wrong FS expected: file:///.....""
</code></pre>

<p>on some digging it turns out, that the code is looking in my local drive to find the ""path"".
I had a similar issue with python libraries (os, pathlib)</p>

<p>I have no issue in running other code on the cluster.</p>

<p>Need help in figuring out how to run this so as to search the datalake and not my local machine. </p>

<p>Also, azure-datalake-store client is not an option due to certain restrictions.</p>",1,1,2020-02-08 21:03:24.003000 UTC,,,0,azure|pycharm|azure-databricks|databricks-connect,983,2015-08-28 19:41:28.867000 UTC,2022-03-05 10:13:23.073000 UTC,,15,0,0,26,,,,,,[]
Best practice for inferring schema from a CSV file in a raw ingestion layer of a data lake?,"<p>Is there a best practice for inferring schema in a raw ingestion layer of a data lake (not schema validation, just infer data types and column names)?</p>
<p>I am using Azure and want to design a way to validate the schema downstream from the ingestion layer, so therefore want a way to infer it from a CSV in order to do the validation.</p>
<p>So far I have tried to read a csv with integers using Azure Data Factory and write to AVRO because of the schema in the header and it stored all as strings. I also tried to scan the files (CSV and AVRO) with Purview but still returned all strings.</p>
<p>File Format: NAICS Company Number, NAICS Company Name, Column for each state (w a value of 1 or 0)</p>
<p>I think the obvious answer may be to use Spark (Databricks) but I want to make sure I go with a simple / necessary rationale for needing to suggest this.</p>
<p>Edit: We need to do this dynamically as we will be running it daily and for a pipeline that ingests many csvs (not just one file).</p>",1,0,2021-02-10 18:55:15.757000 UTC,,2021-02-10 19:07:00.943000 UTC,1,azure|csv|schema|azure-databricks|azure-purview,180,2020-09-17 15:24:36.140000 UTC,2021-03-18 15:22:53.273000 UTC,"Kansas City, MO, USA",11,0,0,7,,,,,,[]
How can we connect Azure SQL database using Active Directory authentication in Azure databricks,"<p>How to put AAD session value instead on password and user name in the code shown here:</p>

<pre><code>    import com.microsoft.azure.sqldb.spark.config.Config
    import com.microsoft.azure.sqldb.spark.connect._

    val config = Config(Map(
      ""url""            -&gt; ""kkk-server.database.windows.net:1433"",
      ""databaseName""   -&gt; ""MyDatabase"",
      ""dbTable""        -&gt; ""dbo.Clients"",
      ""user""           -&gt; ""AD-account"",
      ""password""       -&gt; ""xxxxxxxx"",
      ""connectTimeout"" -&gt; ""5"", //seconds
      ""queryTimeout""   -&gt; ""5""  //seconds
    ))

    val collection = spark.read.sqlDB(config)
    collection.show()
</code></pre>",2,0,2019-07-29 20:01:01.390000 UTC,,2019-07-29 20:50:08.693000 UTC,1,active-directory|connection|azure-sql-database|azure-databricks,1123,2017-05-30 03:43:03.657000 UTC,2020-10-21 16:24:10.000000 UTC,"Gurgaon, Haryana, India",273,0,0,93,,,,,,[]
"How to solve ""Exception in thread ""main"" java.lang.Error:Unresolved compilation problems:SparkSession cannot be resolved to a type"" in java spark","<p>I installed <strong>&quot;VScode, jdk 8, python 3.8 and databricks-connect==8.1.*&quot;</strong> in Azure Windows Virtual Machine. After that I created a databricks cluster and <strong>configured Databricks-connect</strong> by using cmd. After setting all the path variables I executed following Java code:</p>
<p>'''</p>
<pre><code>import java.util.ArrayList;
import java.util.List;
import java.sql.Date;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.*;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.Dataset;

public class test_{
    public static void main(String[] args) throws Exception {
        SparkSession spark = SparkSession.builder().appName(&quot;Temps Demo&quot;).config(&quot;spark.master&quot;, &quot;local&quot;).getOrCreate();


        // Create a Spark DataFrame consisting of high and low temperatures
        // by airport code and date.
        StructType schema = new StructType(new StructField[] {
            new StructField(&quot;AirportCode&quot;, DataTypes.StringType, false, Metadata.empty()),
            new StructField(&quot;Date&quot;, DataTypes.DateType, false, Metadata.empty()),
            new StructField(&quot;TempHighF&quot;, DataTypes.IntegerType, false, Metadata.empty()),
            new StructField(&quot;TempLowF&quot;, DataTypes.IntegerType, false, Metadata.empty()),
        });
}}'''
</code></pre>
<p>=========================================================
Inside VScode terminal I am getting below output
'''</p>
<pre><code>PS C:\Users\gaurav&gt;  &amp; 'c:\Users\gaurav\.vscode\extensions\vscjava.vscode-java-debug-0.35.0\scripts\launcher.bat' 'C:\Program Files\AdoptOpenJDK\jdk-11.0.11.9-hotspot\bin\java.exe' '-Dfile.encoding=UTF-8' '-cp' 'C:\Users\gaurav\AppData\Local\Temp\vscodesws_8933a\jdt_ws\jdt.ls-java-project\bin' 'App' 
    Exception in thread &quot;main&quot; java.lang.Error: Unresolved compilation problems: 
            SparkSession cannot be resolved to a type
            SparkSession cannot be resolved
            StructType cannot be resolved to a type
            StructType cannot be resolved to a type
            StructField cannot be resolved to a type
            StructField cannot be resolved to a type
            StructField cannot be resolved to a type
            DataTypes cannot be resolved to a variable
            Metadata cannot be resolved
            StructField cannot be resolved to a type
            StructField cannot be resolved to a type
            DataTypes cannot be resolved to a variable
            Metadata cannot be resolved
    PS C:\Users\gaurav&gt;  &amp; 'c:\Users\gaurav\.vscode\extensions\vscjava.vscode-java-debug-0.35.0\scripts\launcher.bat' 'C:\Program Files\AdoptOpenJDK\jdk-11.0.11.9-hotspot\bin\java.exe' '-Dfile.encoding=UTF-8' '-cp' 'C:\Users\gaurav\AppData\Local\Temp\vscodesws_8933a\jdt_ws\jdt.ls-java-project\bin' 'test_'
    Exception in thread &quot;main&quot; java.lang.Error: Unresolved compilation problems: 
            SparkSession cannot be resolved to a type
            SparkSession cannot be resolved
            StructType cannot be resolved to a type
            StructType cannot be resolved to a type
            StructField cannot be resolved to a type
            StructField cannot be resolved to a type
            StructField cannot be resolved to a type
            DataTypes cannot be resolved to a variable
            Metadata cannot be resolved
            StructField cannot be resolved to a type
            StructField cannot be resolved to a type
            DataTypes cannot be resolved to a variable
            Metadata cannot be resolved
            StructField cannot be resolved to a type
            StructField cannot be resolved to a type
            DataTypes cannot be resolved to a variable
            Metadata cannot be resolved
            StructField cannot be resolved to a type
            StructField cannot be resolved to a type
            DataTypes cannot be resolved to a variable
            Metadata cannot be resolved
    
            at test_.main(testjava.java:22)
</code></pre>
<p>'''</p>
<p>When I run &quot;databricks-connect test&quot; in cmd after configuring databricks connect. I get following output...
[1]: <a href=""https://i.stack.imgur.com/p5WG6.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/p5WG6.png</a></p>",1,0,2021-08-02 16:48:13.637000 UTC,,,-2,java|apache-spark|visual-studio-code|azure-databricks|databricks-connect,398,2021-08-02 16:14:25.163000 UTC,2021-09-28 09:36:33.377000 UTC,"Kanpur, Uttar Pradesh, India",1,0,0,2,,,,,,[]
How to implement lookup/reference data in apache tinkerpop gremlin or graph database?,"<p>How to implement <code>lookup/reference</code> data in a <code>gremlin or graph database</code>?</p>
<p>I need to collect all types of <code>identification</code>,</p>
<pre><code>1. Diving license,
2. Social security number,
3....
</code></pre>
<p>For UI, I need to send the <code>&quot;identification_type&quot;</code> list (above list).</p>
<p>&amp;</p>
<p>What is the best way to implement this? Create an <code>edge</code> b/w type and actual value? or type as a <code>property</code>?</p>",1,0,2022-02-09 11:29:14.167000 UTC,,,0,neo4j|cypher|gremlin|graph-databases|amazon-neptune,18,2014-03-07 07:02:31.267000 UTC,2022-03-04 04:37:05.967000 UTC,"Bangalore, India",4909,538,406,442,,,,,,[]
Databricks with python 3 for Azure SQl Databas and python,"<p>I am trying to use Azure Databricks in order to :</p>
<p>1- insert rows into table of Azure SQL Databse with python 3. I cannot see a documentation about insert rows. (I have use this link to connect to the database <a href=""https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/sql-databases"" rel=""nofollow noreferrer"">Doc</a> and it is working).</p>
<p>2- Save  Csv file in my datalake</p>
<p>3- Create Table from Dataframe if possible</p>
<p>Thanks for your help and sorry for my novice questions</p>",1,1,2020-12-06 14:00:29.877000 UTC,,,0,python-3.x|azure|azure-sql-database|azure-databricks|databricks-connect,647,2016-11-23 12:18:08.873000 UTC,2022-02-04 15:41:52.470000 UTC,USA,339,21,0,98,,,,,,[]
How to check if a phrase is in english at scale,"<p>I need to enrich my dataframe in PySpark-Sql with a language attribute, that basically tells the language of a paper title for each row. I need to filter out English papers only. I've tens of millions of papers, so I need to do it in parallel.</p>

<p>I have registered an UDF using a Python library called <code>langdetect</code> (<a href=""https://pypi.org/project/langdetect/"" rel=""nofollow noreferrer"">https://pypi.org/project/langdetect/</a>), after having installed the library on the cluster. I'm using the following code:</p>

<pre><code>from langdetect import detect

def lang_detector(_s):
  try:
    lan = detect(_s)
  except:
    lan = 'null'
  return lan

detect2 = udf(lang_detector, StringType())

papers_abs_fos_en = papers_abs \
.join(papersFos_L1, ""PaperId"") \
.withColumn(""Lang"", detect2(col(""PaperTitle""))) \
.filter(""Lang =='en'"") \
.select(""PaperId"", ""Rank"", ""PaperTitle"", ""RefCount"", ""CitCount"", ""FoSList"")
</code></pre>

<p>It works, but it takes forever even on ca 10M titles. I am not sure if this is due to <code>langdetect</code>, to UDFs or if I'm just doing something wrong, but I'd be grateful for any suggestion!</p>

<p>Thanks a lot!
Paolo</p>",1,1,2019-04-12 16:50:55.917000 UTC,,,0,pyspark|azure-databricks,119,2018-12-04 18:51:29.360000 UTC,2022-03-05 22:33:20.040000 UTC,"Milano, Metropolitan City of Milan, Italy",3,0,0,5,,,,,,[]
Installing matplotlib / basemap on Azure Databricks,"<p>Working on POC with netCDF(.nc) files.  Would like do some visualisation and while trying to install <code>Basemap</code> having some issues.</p>

<p>As per the pre-requisites, got <code>numpy</code> and <code>matplotlib</code>  installed.</p>

<p><code>geos</code> is already installed</p>

<p>When installing <code>basemap</code> from git <code>%sh pip install pip install --user git+https://github.com/matplotlib/basemap.git</code> getting below error.</p>

<pre><code>Collecting git+https://github.com/matplotlib/basemap.git
  Cloning https://github.com/matplotlib/basemap.git to /tmp/pip-req-build-w20pcpms
  Running command git clone -q https://github.com/matplotlib/basemap.git /tmp/pip-req-build-w20pcpms
    ERROR: Command errored out with exit status 1:
     command: /databricks/python3/bin/python3.7 -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-req-build-w20pcpms/setup.py'""'""'; __file__='""'""'/tmp/pip-req-build-w20pcpms/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /tmp/pip-req-build-w20pcpms/pip-egg-info
         cwd: /tmp/pip-req-build-w20pcpms/
    Complete output (18 lines):
    checking for GEOS lib in /root ....
    checking for GEOS lib in /root/local ....
    checking for GEOS lib in /usr ....
    checking for GEOS lib in /usr/local ....
    checking for GEOS lib in /sw ....
    checking for GEOS lib in /opt ....
    checking for GEOS lib in /opt/local ....

    Can't find geos library in standard locations ('/root', '/root/local', '/usr', '/usr/local', '/sw', '/opt', '/opt/local').
    Please install the corresponding packages using your
    systems software management system (e.g. for Debian Linux do:
    'apt-get install libgeos-3.3.3 libgeos-c1 libgeos-dev' and/or
    set the environment variable GEOS_DIR to point to the location
    where geos is installed (for example, if geos_c.h
    is in /usr/local/include, and libgeos_c is in /usr/local/lib,
    set GEOS_DIR to /usr/local), or edit the setup.py script
    manually and set the variable GEOS_dir (right after the line
    that says ""set GEOS_dir manually here"".
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.
</code></pre>",1,0,2019-12-04 00:22:53.800000 UTC,,2019-12-12 14:01:52.810000 UTC,0,matplotlib-basemap|azure-databricks,493,2009-04-02 00:53:40.477000 UTC,2022-03-01 22:30:30.100000 UTC,"Brisbane QLD, Australia",27501,66,1,1101,,,,,,[]
get most recent file from azure blob storage,"<p>My azure blob storage has several files , like</p>
<p>name        last modified
data-GUID1   jan 1,20
data_guid2   jan 2, 20</p>
<p>How would I grab the file   most recent 'last modified' ,like data_guid2 ?</p>
<p>Currently I hard-code the name :</p>
<pre><code> file_location=  /dbfs/mnt/blob/container/data_Guid1
</code></pre>
<p>Thanks in advance.</p>",1,0,2020-08-26 22:36:42.803000 UTC,,,0,azure-databricks,381,2018-03-07 15:50:03.570000 UTC,2020-10-02 22:52:19.897000 UTC,"Redmond, WA, USA",1,0,0,2,,,,,,[]
PowerBI consuming Data Bricks consuming Azure Data Lake GEN1,"<p>I am trying to consume Azure Data Bricks from Power BI. 
Bricks has been set up to connect to Azure DataLake GEN1.</p>

<p>I have no issue in getting data out of GEN1 from Bricks.
And I have no issue in consuming a Bricks instance from Power BI.</p>

<p>However, when trying the whole chain, Power BI consuming Bricks and Bricks calling into GEN1, I get a security related error saying that is not possible to find ADLS Gen1 Token in com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider.</p>

<p>Do you have any idea of what the problem that I am facing could be? </p>

<p>Cheers,
G.</p>",0,0,2020-06-16 13:13:20.443000 UTC,,,1,azure|powerbi|azure-data-lake|azure-databricks,25,2010-05-04 12:02:53.877000 UTC,2021-06-09 05:00:20.557000 UTC,"London, United Kingdom",1021,34,6,140,,,,,,[]
Search all vertexes for a partial property match in gremlin,"<p>I have some gremlin vertexes that have the following properties (plus a few others):
label, ID, parent_id, notes, type.</p>
<p>I want to be able to search a value (or partial value) and return all vertexes that contain that partial property. I am also currently sorting them by label.
I am using typescript, gremlin and Neptune.</p>
<pre><code>g.V().hasLabel('sample_label')
.or(
__.has('ID', TextP.containing(value)
__.has('parent_id', TextP.containing(value)
__.has('notes', TextP.containing(value)
__.has('type', TextP.containing(value)
)
</code></pre>
<p>However, this code either doesn't filter enough and I get data that shouldn't be there, or it doesn't return all the data I am expecting.
What is the correct gremlin query to do this?</p>",0,2,2021-06-21 17:54:23.837000 UTC,,,0,graph|gremlin|amazon-neptune,51,2018-12-17 14:36:56.593000 UTC,2022-02-22 19:31:46.843000 UTC,Georgia,271,11,1,36,,,,,,[]
Stop Execution of Databricks notebook after specific cell,"<p>I Tried sys.exit(0)(Python code) and dbutils.notebook.exit() on Databricks notebook. But both the option didn't work.
Please suggest any other way to stop the execution of code after a specific cell in Databricks notebook.</p>",2,6,2021-02-19 10:00:30.993000 UTC,1.0,,4,azure-databricks,4641,2020-01-30 05:08:22.337000 UTC,2022-03-04 08:17:44.810000 UTC,,121,17,0,28,,,,,,[]
Access Named Graph in Neptune with Gremlin,"<p>I need to save several Graphs in Neptune DB cluster.</p>
<p><strong>Named Graph</strong> is smths that is supported for SPARQL format in Neptune <a href=""https://docs.aws.amazon.com/neptune/latest/userguide/best-practices-sparql-graph.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/neptune/latest/userguide/best-practices-sparql-graph.html</a></p>
<p>Is that possible to query that Graph<strong>s</strong> with GREMLIN?</p>
<p>I found only this approach <a href=""https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-rest.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-rest.html</a> but no any references to ability to access Named graph. Maybe I missed some?</p>",2,0,2020-11-05 17:39:25.150000 UTC,1.0,2020-11-05 17:52:10.113000 UTC,0,amazon-web-services|sparql|gremlin|amazon-neptune,610,2011-01-17 14:56:27.957000 UTC,2022-03-04 10:51:10.870000 UTC,,3495,471,1,253,,,,,,[]
Azure Devops and Azure Databricks authentication tokens,"<p>Recently I've been developing a python package <code>install_databricks_packages</code> which contacts the Databricks APIs (using <code>requests</code>, not the CLI) in order to install packages on Databricks Clusters. This package is used in release pipelines, where one can add a bash script which uses <code>install_databricks_packages</code> as a cli to install the needed packages on  one or more clusters.</p>
<p>The problem is that during development I realized I need two different tokens in order to make <code>install_databricks_packages</code> work with packages hosted on our private Azure Artifacts repository, where we host some internally developed packages. The first token is a Databricks PAT which is needed to authorize the API call, the second one is a DevOps PAT needed when calling the <code>/api/2.0/libraries/install</code> API in order to install a package. Basically, I need to call the API like this</p>
<pre><code>import requests

data = {&quot;cluster_id&quot;: 123,
        &quot;libraries&quot;: [
            {&quot;pypi&quot;: {
                 &quot;package&quot;: &quot;private_package==1.0.0&quot;,
                 &quot;repo&quot;: &quot;https://&lt;devops-token&gt;@pkgs.dev.azure.com/&lt;company&gt;/pypi/simple/&quot;
            }}
        ]}

requests.post(https://&lt;host&gt;/api/2.0/libraries/install, auth=('token', &lt;databricks-token&gt;))
</code></pre>
<p>I generated the two tokens with my user and saved them on Azure KeyVault as two different secrets, which can then be fetched in any release pipeline using the Azure KeyVault task.</p>
<p>I was wondering whether this is the only course of action. Having two PAT which are connected to a specific user and have expiration dates, thus have to be manually managed, is cumbersome. I couldn't find a better solution looking online, so any advice is welcome!</p>",1,1,2021-08-26 10:33:03.097000 UTC,,,1,azure-devops|azure-databricks|azure-authentication,162,2016-02-13 11:36:15.973000 UTC,2022-03-05 18:02:18.037000 UTC,"Milano, MI, Italia",164,30,10,13,,,,,,[]
Skip first row when loading csv file into Apache Spark data frame in Azure Databricks,"<p>In my <a href=""https://docs.microsoft.com/en-us/azure/databricks/scenarios/what-is-azure-databricks#:%7E:text=Azure%20Databricks%20is%20a%20data,Microsoft%20Azure%20cloud%20services%20platform.&amp;text=For%20a%20big%20data%20pipeline,Event%20Hub%2C%20or%20IoT%20Hub."" rel=""nofollow noreferrer"">Azure Databricks</a> notebook, following code correctly loads the following data to an <a href=""https://docs.microsoft.com/en-us/azure/databricks/getting-started/spark/"" rel=""nofollow noreferrer"">Azure Apache Spark</a> <code>DataFrame</code>.</p>
<p><strong>Question</strong>: How can I skip the first row from the <code>DataFrame</code> - either during the data load process or after the data has been loaded to the data frame?</p>
<p><strong>Goal</strong>: File has millions of rows that need to be loaded to a SQL database except for the first row. File is too big to be opened in notepad or Excel. The process, eventually, needs to be automated.</p>
<p><strong>.CSV file</strong>:</p>
<pre><code>HD|20211210
DT|D-|12/22/2017|12/22/2017 09:41:45.828000|11/01/2017|01/29/2018 14:46:10.666000|1.2|1.2|ABC|ABC|123|123|4554|023|11/01/2017|ACDF|First|0012345||f|ABCD|ABCDEFGH|ABCDEFGH||||
DT|D-|12/25/2017|12/25/2017 09:24:20.202000|12/13/2017|01/29/2018 07:52:23.607000|6.4|6.4|ABC|ABC|123|123|4540|002|12/13/2017|ACDF|First|0012345||f|ABC|ABCDEF|ABCDEFGH||||
</code></pre>
<p><strong><a href=""https://docs.microsoft.com/en-us/azure/databricks/data/data#python"" rel=""nofollow noreferrer"">Code</a></strong>:</p>
<pre><code>sparkDF = spark.read.csv(&quot;/FileStore/tables/MyDataFile.csv&quot;, header=&quot;true&quot;, inferSchema=&quot;true&quot;)
display(sparkDF)
</code></pre>",1,0,2022-01-14 01:48:31.163000 UTC,,,0,azure|apache-spark|azure-databricks,77,2012-02-25 04:28:19.340000 UTC,2022-03-06 03:21:32.830000 UTC,,18057,2500,22,2093,,,,,,[]
Databricks list all blobs in Azure Blob Storage,"<p>I have mounted a Blob Storage Account in to Databricks, and can access it fine, so i know that it works.</p>

<p>What i want to do though, is list out the names all of the files at a given path.. currently i'm doing this with:</p>

<pre><code>list = dbutils.fs.ls('dbfs:/mnt/myName/Path/To/Files/2019/03/01')
df = spark.createDataFrame(list).select('name')
</code></pre>

<p>The issue i have though, is that it's exceptionally slow.. due to there being around 160,000 blobs at that location (storage explorer shows this as ~1016106592 bytes which is 1Gb!)</p>

<p>This surely can't be pulling down all this data, all i need/want is the filename..</p>

<p>Is blob storage my bottle neck, or can i (somehow) get Databricks to execute the command in parallel or something?</p>

<p>Thanks.</p>",1,0,2019-03-06 23:53:40.153000 UTC,1.0,,2,python|azure|azure-blob-storage|azure-databricks,3234,2013-07-03 11:16:58.847000 UTC,2022-02-11 09:11:36.863000 UTC,"Amsterdam, NL",1257,115,2,190,,,,,,[]
In Windows and Linux how to find installed mercurial is 32 or 64 bit?,"<p>In both windows and linux how to find installed mercurial is 32 or 64 bit using cmd line ?</p>

<p>hg version doesnt show.</p>

<pre><code>C:\Users\dkanagaraj&gt;hg --version
Mercurial Distributed SCM (version 3.4.2)
(see http://mercurial.selenic.com for more information)

Copyright (C) 2005-2015 Matt Mackall and others
This is free software; see the source for copying conditions. There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
</code></pre>",1,0,2016-03-02 12:13:45.520000 UTC,,2016-03-02 12:26:15.270000 UTC,0,mercurial|dvcs,257,2013-04-10 06:53:57.493000 UTC,2020-09-07 06:25:50.297000 UTC,India,1144,18,0,91,,,,,,[]
Error: Invalid configuration value detected for fs.azure.account.key,"<p>I am using Azure Databricks to make a delta table in Azure Blob Storage using ADLS Gen2 but i am getting the error &quot;Failure to initialize configurationInvalid configuration value detected for fs.azure.account.key&quot; on last line</p>
<pre class=""lang-scala prettyprint-override""><code>%scala
spark.conf.set(
    &quot;fs.azure.account.oauth2.client.secret&quot;,
    &quot;&lt;storage-account-access-key&gt;&quot;)
friends = spark.read.csv('myfile/fakefriends-header.csv',
   inferSchema = True, header = True)
friends.write.format(&quot;delta&quot;).mode('overwrite')\
   .save(&quot;abfss://tempfile@tempaccount.dfs.core.windows.net/myfile/friends_new&quot;)
</code></pre>
<p>Please help me out how can i avoid this error</p>",1,0,2021-11-03 13:13:21.840000 UTC,,2021-11-03 15:08:48.493000 UTC,1,azure|azure-blob-storage|azure-databricks,1163,2019-08-19 16:38:54.260000 UTC,2022-03-02 11:49:22.207000 UTC,Pakistan,394,143,7,147,,,,,,[]
How to writeStream Azure Service Bus Queues in Apache Spark with Databricks,"<p>I have used the the following link to receive messages from Azure Service Bus queues.
<a href=""https://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-python-how-to-use-queues"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-python-how-to-use-queues</a></p>
<p>I only focusing on the following code (written in Databricks) to receive queues:</p>
<pre><code>with servicebus_client:
    # get the Queue Receiver object for the queue
    receiver = servicebus_client.get_queue_receiver(queue_name=QUEUE_NAME, max_wait_time=5)
    with receiver:
        for msg in receiver:
            print(&quot;Received: &quot; + str(msg))
            # complete the message so that the message is removed from the queue
            receiver.complete_message(msg)
</code></pre>
<p>When I manually send a message to the queue <strong>(This Is a a Message)</strong> it is successully received by Databricks</p>
<p>When I collect the output of the msg using str(msg) I get:</p>
<pre><code>Out[77]: 'This Is a Message'
</code></pre>
<p>When I collect the whole output of the <strong>ServiceBusReceivedMessage</strong> I get following output</p>
<p>Out[76]: ServiceBusReceivedMessage(body=<strong>This Is a Message</strong>, application_properties={b'MachineName': b'DESKTOP-GV10S60', b'UserName': b'Carlton'}, session_id=None, message_id=d7ff2f86-dc6d-483c-85d2-bb6eebd5de51, content_type=None, correlation_id=None, to=None, reply_to=None, reply_to_session_id=None, subject=Service Bus Explorer, time_to_live=2 days, 2:00:10, partition_key=None, scheduled_enqueue_time_utc=0001-01-01 00:00:00+00:00, auto_renew_error=None, dead_letter_error_description=None, dead_letter_reason=None, dead_letter_source=None, delivery_count=0, enqueued_sequence_number=None, enqueued_time_utc=2022-02-05 12:06:46.154000+00:00, expires_at_utc=2022-02-07 14:06:56.154000+00:00, sequence_number=97, lock_token=None, locked_until_utc=None)</p>
<p>You will notice the body=This Is a Message.</p>
<p>I am trying to writeStream the body of the of message to a location either on DBFS or Azure Data Lake.</p>
<p>I have tried the following:</p>
<pre><code>streamingQuery = (
   receiver
  .writeStream
  .format(&quot;delta&quot;)
  .outputMode(&quot;append&quot;)
  .option(&quot;checkpointLocation&quot;, &quot;...&quot;) 
  .start()
</code></pre>
<p>But I get the error <strong>AttributeError: 'ServiceBusReceiver' object has no attribute 'writeStream'</strong></p>
<p>I have also tried</p>
<pre><code>streamingQuery = (
   str(msg)
  .writeStream
  .format(&quot;delta&quot;)
  .outputMode(&quot;append&quot;)
  .option(&quot;checkpointLocation&quot;, &quot;...&quot;) 
  .start()
</code></pre>
<p>But I get the error <strong>AttributeError: 'str' object has no attribute 'writeStream'</strong></p>
<p>I have been working on this for quite sometime.</p>
<p>If someone can let me know where I might be going wrong.</p>
<p>If someone could even just let me know if what I'm doing is even possible? That would at least allow me to work on another solution - instead of wasting my time trying to get this work.</p>
<p>Thanks in advance</p>",0,0,2022-02-05 12:26:19.960000 UTC,1.0,,0,apache-spark|azureservicebus|azure-databricks,42,2021-03-31 08:36:43.863000 UTC,2022-03-05 23:03:22.193000 UTC,"London, UK",651,58,0,93,,,,,,[]
Gremlin/AWS Neptune: Adding Edge w/ Properties,"<p>I currently have a series of two vertices in a parent/child relationship, two edges between them that I have no issue with.<BR></p>
<p>The issue happens when I start attempting to add properties to the edge and I get an error message that gives only some help:</p>
<pre><code>{
    &quot;requestId&quot;: &quot;...&quot;,
    &quot;code&quot;: &quot;InternalFailureException&quot;,
    &quot;detailedMessage&quot;: &quot;null:to([[SelectOneStep(last,child)]])&quot;
}
</code></pre>
<p>I can run the same pattern below on Gremlify, but I understand that is 3.5 as opposed to Neptune on 3.4.<BR></p>
<p>Do I need to point back to the edge itself after adding in all the properties or am I supposed to specify <code>to(select('child'))</code> and then add all the properties while in scope (and then return back to <code>child</code>)?</p>
<p>Traversal Query:</p>
<pre><code>g.V(13695)
    .out(&quot;latest_parent_to&quot;).as(&quot;child&quot;)
    .in(&quot;parent_to&quot;)
    .addE(&quot;role&quot;)
        .property(single,'name','some_role')
    .to(
        select(&quot;child&quot;)    
    ).select(&quot;child&quot;)
</code></pre>
<p>Update: I made a few changes to try the other thought: addE &gt; to(select(...)) &gt; set properties. Different error this time:<BR></p>
<pre><code>{
    &quot;requestId&quot;: &quot;77ee2b5b-8309-4226-b163-8b253450c721&quot;,
    &quot;code&quot;: &quot;UnsupportedOperationException&quot;,
    &quot;detailedMessage&quot;: &quot;Cardinality specification may not be used with Edge properties.&quot;
}
</code></pre>
<p>The reason for using the single cardinality is every property stored/returned as a set. This was to prevent accidentally appending when one should update.</p>",0,7,2022-01-05 15:50:41.480000 UTC,,2022-01-05 17:07:56.593000 UTC,0,gremlin|amazon-neptune|aws-neptune,78,2020-03-22 17:13:51.963000 UTC,2022-03-04 21:15:00.600000 UTC,Earth,73,4,0,19,,,,,,[]
Find Last modified timestamp of a files/folders in Azure Datalake through python script in Azure databricks that uses Credential passthrough,"<p>I have an Azure DataLake Storage Gen2 which contains a few Parquet files. My Organization has enabled credential passthrough and so I am able to create a python script in Azure Databricks and access the files available in ADLS using dbutils.fs.ls. All these work fine. </p>

<p>Now, I need to access the last modified timestamp of these files too. I found a <a href=""https://stackoverflow.com/questions/58237338/how-to-get-the-last-modification-time-of-each-files-present-in-azure-datalake-st"">link</a> that does this. However, it uses BlockBlobService and requires an account_key. </p>

<p>I do not have an account key and can't get one due to security policies of the organization. I am unsure of how to do the same using Credential passthrough. Any ideas here?</p>",2,1,2020-06-05 13:30:01.920000 UTC,,,2,pyspark|azure-data-lake|azure-blob-storage|azure-databricks,1489,2012-10-03 09:11:58.107000 UTC,2022-03-04 16:50:42.890000 UTC,"Mumbai, Maharashtra, India",81,0,0,16,,,,,,[]
