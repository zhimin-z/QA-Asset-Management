Question_title,Question_body,Question_answer_count,Question_comment_count,Question_creation_date,Question_favorite_count,Question_last_edit_date,Question_score,Question_tags,Question_view_count,Owner_creation_date,Owner_last_access_date,Owner_location,Owner_reputation,Owner_up_votes,Owner_down_votes,Owner_views,Answer_body,Answer_comment_count,Answer_creation_date,Answer_last_edit_date,Answer_score,Question_valid_tags
Difference in usecases for AWS Sagemaker vs Databricks?,"<p>I was looking at Databricks because it integrates with AWS services like Kinesis, but it looks to me like SageMaker is a direct competitor to Databricks? We are heavily using AWS, is there any reason to add DataBricks into the stack or odes SageMaker fill the same role?</p>",2,0,2019-03-13 00:23:26.600000 UTC,3.0,,7,apache-spark|pyspark|databricks|amazon-sagemaker,8897,2015-01-15 17:43:03.700000 UTC,2022-03-05 18:43:20.680000 UTC,,1277,47,1,141,"<p>SageMaker is a great tool for deployment, it simplifies a lot of processes configuring containers, you only need to write 2-3 lines to deploy the model as an endpoint and use it.  SageMaker also provides the dev platform (Jupyter Notebook) which supports Python and Scala (sparkmagic kernal) developing, and i managed installing external scala kernel in jupyter notebook. Overall, SageMaker provides end-to-end ML services. Databricks has unbeatable Notebook environment for Spark development. </p>

<p>Conclusion</p>

<ol>
<li><p>Databricks is a better platform for Big data(scala, pyspark) Developing.(unbeatable notebook environment)</p></li>
<li><p>SageMaker is better for Deployment. and if you are not working on big data, SageMaker is a perfect choice working with (Jupyter notebook + Sklearn + Mature containers + Super easy deployment). </p></li>
<li><p>SageMaker provides ""real time inference"", very easy to build and deploy, very impressive. you can check the official SageMaker Github.
<a href=""https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/scikit_learn_inference_pipeline"" rel=""noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/scikit_learn_inference_pipeline</a></p></li>
</ol>",3.0,2019-03-20 21:40:34.270000 UTC,,10.0,"['databricks', 'amazon-sagemaker']"
sagemaker endpoint invocation randomly throws error,"<p>Env:</p>

<ul>
<li>XGBoost model trained on static data (excel).</li>
<li>Model Saved and deployed to Sagemaker with MLFlow.</li>
<li>Sagemaker Model and Sagemaker Endpoint are running.</li>
</ul>

<p>Invocation:
 - I invoke the model with new data via REST Request (POSTMAN) and via boto3.sagemaker</p>

<pre><code>client.invoke_endpoint(
    EndpointName=""xxxxxxx"",
    Body=data,
    ContentType=""application/json"",
    Accept=""string"",
)
</code></pre>

<p>Problem:
It seems that Sagemaker randomly (~50% of the time) fails with following Exception:</p>

<pre><code>{
""ErrorCode"": ""CLIENT_ERROR_FROM_MODEL"",
""LogStreamArn"": ""arn:aws:logs:eu-central-1:xxxxxxxxxx:log-group:/aws/sagemaker/Endpoints/xxxxxxxx"",
""Message"": ""Received client error (400) from mfs-xxxxxxxxx-model-dztwabjotscyoc-zx7lzyfg with message \""{\""error_code\"": \""BAD_REQUEST\"", \""message\"": \""Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.\"", \""stack_trace\"": \""Traceback (most recent call last):\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/mlflow/pyfunc/scoring_server/__init__.py\\\"", line 196, in transformation\\n    raw_predictions = model.predict(data)\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/mlflow/xgboost.py\\\"", line 198, in predict\\n    return self.xgb_model.predict(xgb.DMatrix(dataframe))\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/xgboost/core.py\\\"", line 1443, in predict\\n    self._validate_features(data)\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/xgboost/core.py\\\"", line 1862, in _validate_features\\n    data.feature_names))\\nValueError: feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81'] ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85']\\ntraining data did not have the following fields: 83, 85, 84, 82\\n\""}\"". See https://eu-central-1.console.aws.amazon.com/cloudwatch/home?region=eu-central-1#logEventViewer:group=/aws/sagemaker/Endpoints/xxxxxxxxx in account xxxxxxxfor more information."",
""OriginalMessage"": ""{\""error_code\"": \""BAD_REQUEST\"", \""message\"": \""Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.\"", \""stack_trace\"": \""Traceback (most recent call last):\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/mlflow/pyfunc/scoring_server/__init__.py\\\"", line 196, in transformation\\n    raw_predictions = model.predict(data)\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/mlflow/xgboost.py\\\"", line 198, in predict\\n    return self.xgb_model.predict(xgb.DMatrix(dataframe))\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/xgboost/core.py\\\"", line 1443, in predict\\n    self._validate_features(data)\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/xgboost/core.py\\\"", line 1862, in _validate_features\\n    data.feature_names))\\nValueError: feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81'] ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85']\\ntraining data did not have the following fields: 83, 85, 84, 82\\n\""}"",
""OriginalStatusCode"": 400
</code></pre>

<p>}</p>

<p>After this Exception, i hit again ""Send Request"" in Postman (with the exact same Body Data) and get a successful response:</p>

<pre><code>[
0.9989840388298035
]
</code></pre>

<p>The problem is exactly the same (randomly) when using boto3.sagemaker either in AWS Lambda or locally when testing.</p>

<p><strong>UPDATE 1:</strong>
When i load the exported function with mlflow.xgboost.load_model() and run the prediction, it works everytime. Here is the code:</p>

<pre class=""lang-py prettyprint-override""><code>    import mlflow
    from mlflow import xgboost
    from xgboost import DMatrix
    import numpy as np
    import pandas as pd
    base_path = ""src/models/""


    model_path_name_a = f""{base_path}/model_a""
    model_path_name_b = f""{base_path}/model_b""
    # xgboost.log_model(xgb_model_a, artifact_path='https://mlflow-server-tracking.s3.eu-central-1.amazonaws.com')
    xgb_model_a = xgboost.load_model(model_path_name_a)
    xgb_model_b = xgboost.load_model(model_path_name_b)

    X_test = DMatrix(
        np.array(
            [
                [7.60000e+01, 2.57000e+02, 9.25200e+03, 2.00400e+03, 0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00]
            ]
        )
    )


    ############# Prediction Model A ################
    y_predA = xgb_model_a.predict(X_test)
    print(f""y_predA: {y_predA}"")

    ############# Prediction Model B ################
    y_predB = xgb_model_b.predict(X_test)
    print(f""y_predA: {y_predB}"")
</code></pre>

<p>Does that mean that the problem is with Sagemaker Inference/Endpoint Service?</p>",0,5,2020-04-15 09:13:53.927000 UTC,,2020-04-15 11:20:45.103000 UTC,0,python|amazon-web-services|xgboost|amazon-sagemaker|mlflow,375,2017-02-19 13:12:04.900000 UTC,2021-12-23 16:14:44.843000 UTC,"Nuremberg, Germany",177,227,5,71,,,,,,"['mlflow', 'amazon-sagemaker']"
How to run the mlflow web-based user interface from Amazon SageMaker?,"<p>I want to use the mlflow web-based user interface from a notebook on Amazon SageMaker. But the given address <a href=""http://127.0.0.1:5000"" rel=""noreferrer"">http://127.0.0.1:5000</a> doesn't seeem to work.</p>

<p>I have installed mlflow on a SageMaker notebook.</p>

<p>This code runs nicely:</p>

<pre><code>import mlflow
mlflow.start_run()
mlflow.log_param(""my"", ""param"")
mlflow.log_metric(""score"", 100)
mlflow.end_run()
</code></pre>

<p>And then if I run </p>

<pre><code>! mlflow ui
</code></pre>

<p>I get the expected result: </p>

<pre><code>[2019-04-09 11:15:52 +0000] [17980] [INFO] Starting gunicorn 19.9.0
[2019-04-09 11:15:52 +0000] [17980] [INFO] Listening at: http://127.0.0.1:5000 (17980)
[2019-04-09 11:15:52 +0000] [17980] [INFO] Using worker: sync
[2019-04-09 11:15:52 +0000] [17983] [INFO] Booting worker with pid: 17983
</code></pre>

<p>However, after that when going to <code>http://127.0.0.1:5000</code> in my browser, nothing loads. </p>

<p>My guess it that <code>127.0.0.1</code> is not the right address, but how can I know which address to use instead?</p>",1,1,2019-04-09 12:40:21.920000 UTC,1.0,,7,amazon-sagemaker|mlflow,1113,2019-04-09 11:30:39.983000 UTC,2021-01-26 11:37:19.970000 UTC,,71,1,0,1,,,,,,"['mlflow', 'amazon-sagemaker']"
Hyperopt failed to execute mlflow.end_run() with tracking URI: databricks,"<p>I'm using Azure Databricks + Hyperopt + MLflow for some hyperparameter tuning on a small dataset.  Seem like the job is running, and I get output in MLflow, but the job ends with the following error message:</p>

<pre><code>Hyperopt failed to execute mlflow.end_run() with tracking URI: databricks
</code></pre>

<p>Here is my code code with some information redacted:</p>

<pre><code>from pyspark.sql import SparkSession

# spark session initialization
spark = (SparkSession.builder.getOrCreate())
sc = spark.sparkContext

# Data Processing
import pandas as pd
import numpy as np
# Hyperparameter Tuning
from hyperopt import fmin, tpe, hp, anneal, Trials, space_eval, SparkTrials, STATUS_OK
from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score
# Modeling
from sklearn.ensemble import RandomForestClassifier
# cleaning
import gc
# tracking
import mlflow
# track runtime
from datetime import date, datetime

mlflow.set_experiment('/user/myname/myexp')
# notebook settings \ variable settings
n_splits = #
n_repeats = #
max_evals = #

dfL = pd.read_csv(""/my/data/loc/mydata.csv"")

x_train = dfL[['f1','f2','f3']]
y_train = dfL['target']

def define_model(params):
    model = RandomForestClassifier(n_estimators=int(params['n_estimators']),
                                   criterion=params['criterion'], 
                                   max_depth=int(params['max_depth']), 
                                   min_samples_split=params['min_samples_split'], 
                                   min_samples_leaf=params['min_samples_leaf'], 
                                   min_weight_fraction_leaf=params['min_weight_fraction_leaf'], 
                                   max_features=params['max_features'], 
                                   max_leaf_nodes=None, 
                                   min_impurity_decrease=params['min_impurity_decrease'], 
                                   min_impurity_split=None, 
                                   bootstrap=params['bootstrap'], 
                                   oob_score=False, 
                                   n_jobs=-1, 
                                   random_state=int(params['random_state']), 
                                   verbose=0, 
                                   warm_start=False, 
                                   class_weight={0:params['class_0_weight'], 1:params['class_1_weight']})
        return model


space = {'n_estimators': hp.quniform('n_estimators', #, #, #),
         'criterion': hp.choice('#', ['#','#']),
         'max_depth': hp.quniform('max_depth', #, #, #),
         'min_samples_split': hp.quniform('min_samples_split', #, #, #),
         'min_samples_leaf': hp.quniform('min_samples_leaf', #, #, #),
         'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', #, #, #),
         'max_features': hp.quniform('max_features', #, #, #),
         'min_impurity_decrease': hp.quniform('min_impurity_decrease', #, #, #),
         'bootstrap': hp.choice('bootstrap', [#,#]),
         'random_state': hp.quniform('random_state', #, #, #),
         'class_0_weight': hp.choice('class_0_weight', [#,#,#]),
         'class_1_weight': hp.choice('class_1_weight', [#,#,#])}

# define hyperopt objective
def objective(params, n_splits=n_splits, n_repeats=n_repeats):

    # define model
    model = define_model(params)
    # get cv splits
    kfold = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=1331)
    # define and run sklearn cv scorer
    scores = cross_val_score(model, x_train, y_train, cv=kfold, scoring='roc_auc')
    score = scores.mean()

    return {'loss': score*(-1), 'status': STATUS_OK}

spark_trials = SparkTrials(parallelism=36, spark_session=spark)
with mlflow.start_run():
  best = fmin(objective, space, algo=tpe.suggest, trials=spark_trials, max_evals=max_evals)
</code></pre>

<p>and then at the end I get..</p>

<pre><code>100%|██████████| 200/200 [1:35:28&lt;00:00, 100.49s/trial, best loss: -0.9584565527065526]

Hyperopt failed to execute mlflow.end_run() with tracking URI: databricks

Exception: 'MLFLOW_RUN_ID'

Total Trials: 200: 200 succeeded, 0 failed, 0 cancelled.
</code></pre>

<p>My Azure Databricks cluster is..</p>

<pre><code>6.6 ML (includes Apache Spark 2.4.5, Scala 2.11)
Standard_DS3_v2
min 9 max 18 nodes
</code></pre>

<p>Am I doing something wrong or is this a bug?</p>",1,4,2020-06-02 20:19:55.507000 UTC,,,2,pyspark|databricks|azure-databricks|mlflow|hyperopt,494,2018-06-10 03:57:32.363000 UTC,2022-03-04 22:36:21.683000 UTC,,217,570,0,34,,,,,,"['databricks', 'mlflow']"
How to add more metrics to a finished MLflow run?,"<p>Once an MLflow run is finished, external scripts can access its parameters and metrics using python <code>mlflow</code> client and <code>mlflow.get_run(run_id)</code> method, but the <code>Run</code> object returned by <code>get_run</code> seems to be read-only.</p>
<p>Specifically, <code>.log_param</code> <code>.log_metric</code>, or <code>.log_artifact</code> cannot be used on the object returned by <code>get_run</code>, raising errors like these:</p>
<pre><code>AttributeError: 'Run' object has no attribute 'log_param'
</code></pre>
<p>If we attempt to run any of the <code>.log_*</code> methods on <code>mlflow</code>, it would log them into to a new run  with auto-generated run ID in the <code>Default</code> experiment.</p>
<p>Example:</p>
<pre><code>final_model_mlflow_run = mlflow.get_run(final_model_mlflow_run_id)

with mlflow.ActiveRun(run=final_model_mlflow_run) as myrun:    
    
    # this read operation uses correct run
    run_id = myrun.info.run_id
    print(run_id)
    
    # this write operation writes to a new run 
    # (with auto-generated random run ID) 
    # in the &quot;Default&quot; experiment (with exp. ID of 0)
    mlflow.log_param(&quot;test3&quot;, &quot;This is a test&quot;)
   
</code></pre>
<p>Note that the above problem exists regardless of the <code>Run</code> status (<code>.info.status</code> can be both &quot;FINISHED&quot; or &quot;RUNNING&quot;, without making any difference).</p>
<p>I wonder if this read-only behavior is by design (given that immutable modeling runs improve experiments reproducibility)? I can appreciate that, but it also goes against code modularity if everything has to be done within a single monolith like the <code>with mlflow.start_run()</code> context...</p>",1,1,2021-03-29 14:12:06.043000 UTC,,2021-03-29 17:50:32.537000 UTC,2,python|databricks|mlflow,632,2018-06-19 12:32:08.930000 UTC,2022-03-05 08:58:09.607000 UTC,EU,2506,728,2,328,,,,,,"['databricks', 'mlflow']"
display the metric values from mlflow for each run in pycharm,"<p>I have written a small code snippet for hyperparameter tuning using Grid search method. I am passing the parameters value using python code in pycharm. This is running successfully</p>
<p>'''</p>
<pre><code>import mlflow
lr = [1e-3, 1e-4,1e-5]
batch_size = [16, 32, 48]

for x in lr:
    for y in batch_size:
       local_env_run = mlflow.projects.run(uri=&quot;.&quot;,
                                           entry_point='train_centernet',
                                           parameters={&quot;ngpus&quot;:8, &quot;epochs&quot;:100, &quot;lr&quot;:x, &quot;batch_size&quot;:y},
                                           experiment_id='10000000000000',
                                           use_conda=True,
                                           backend=&quot;databricks&quot;,
                                           backend_config =&quot;cluster-spec-p2-8xlarge.json&quot;
                                           )
</code></pre>
<p>'''</p>
<p>Now I would like to read the metrics value (MeanAP, CentLoss, Sum losses, Heatmap focul losses ) for each run above so as to have a history of metric values to implement bayesian hyperparameter tuning. But I am getting an error that uri is not defined.
'''</p>
<pre><code>import mlflow
lr = 1e-3
batch_size = 16
metrics = {&quot;MeanAP&quot;: 2500.00}

with mlflow.projects.run(uri=&quot;.&quot;,
                         entry_point='train_centernet',
                         parameters={&quot;ngpus&quot;:8, &quot;epochs&quot;:100, &quot;lr&quot;:lr, 
               &quot;batch_size&quot;:batch_size},
                        experiment_id='611148175186456',
                        use_conda=True,
                        backend=&quot;databricks&quot;,
                        backend_config =&quot;cluster-spec-p2-8xlarge.json&quot;
                         ):
     results = mlflow.log_metrics(metrics)
     print(results)`
</code></pre>
<p>'''</p>
<p>Can you please help me with the right implementation of mlflow.log_metrics() or is there any other function with which I can read the metrics value for each run in pycharm</p>",0,0,2021-05-27 14:12:03.437000 UTC,,,0,python-3.x|pycharm|databricks|hyperparameters|mlflow,99,2018-06-19 15:23:06.943000 UTC,2021-11-17 14:19:00.127000 UTC,,25,3,0,21,,,,,,"['databricks', 'mlflow']"
How to send response from Azure Databricks the UI in real-time?,"<p>I have registered my ML model using Mlflow in Azure Databricks, and have a model URL.
Now, in another notebook of Databricks, I am preparing data(retrieved from the SQL) and filtering it based on the input parameters fetched as a RestAPI from Postman.
Next, I am using this prepared data frame(max. 20 records) to get the prediction for each row. And, converting this data frame into JSON serializable format to send it as a response to Postman.
The notebook runtime is 2 secs.</p>
<p>Every time a request is got, a databricks job is run, and the response is sent. I am not sure why each job's runtime is 12 secs or more(despite the notebook runtime being 2 secs. only).
What I actually I am looking into is to send a curated response, that'll have some additional parameters along with the prediction, in form of a RestAPI in milliseconds(as it is realtime).</p>
<p>Everything is done in Databricks, itself.</p>
<p>I believe I am somewhere lacking in understanding which architecture shall help us achieve my requirement. Kindly help me understand the same. Thanks a lot, in advance.</p>",0,4,2020-12-24 08:11:32.573000 UTC,,2020-12-31 04:14:47.950000 UTC,0,azure|postman|databricks|job-scheduling|mlflow,175,2017-03-29 18:50:29.467000 UTC,2021-02-23 05:52:42.913000 UTC,"Hyderabad, Telangana, India",25,1,0,60,,,,,,"['databricks', 'mlflow']"
Assume IAM Role to store MLFlow Artifact on S3 bucket in another account,"<p>I have to save my MLFlow artifacts (using Databricks Unified Analytics) to a S3 bucket, with service-side encrpytion using a KMS key.</p>

<p>My instances are into an AWS account A, my S3 bucket and my KMS key into an account B. I can't have my KMS Key into my account A.</p>

<p>I don't want to use DBFS to mount S3 buckets, for security reasons (buckets can contains sensitive data and I don't want to share this between users).</p>

<p>I have to assume an IAM role in order to access the bucket, as I did to access it through s3a (with <code>spark.hadoop.fs.s3a.credentialsType</code> and <code>spark.hadoop.fs.s3a.stsAssumeRole.arn</code> parameters).</p>

<p>When I create an experiment with s3 and try to log a model like this : </p>

<pre class=""lang-py prettyprint-override""><code>import mlflow
import mlflow.sklearn
id_exp = mlflow.create_experiment(""/Users/first.last@company.org/Experiment"",'s3://s3-bucket-name/')
with mlflow.start_run(experiment_id=id_exp):
  clf_mlf = tree.DecisionTreeClassifier()
  clf_mlf = clf_mlf.fit(X_train, y_train)
  y_pred = clf_mlf.predict(X_test)
  mlflow.sklearn.log_model(clf_mlf, ""model"", serialization_format='pickle')
</code></pre>

<p>I have this error : </p>

<pre><code>S3UploadFailedError: Failed to upload /tmp/tmp2yl2olhi/model/conda.yaml to s3-bucket-name//05c17a33a33d46a5ad3cc811a9faf35a/artifacts/model/conda.yaml: An error occurred (KMS.NotFoundException) when calling the PutObject operation: Key 'arn:aws:kms:eu-central-1:account_a_id:key/key_id' does not exist
</code></pre>

<p>How can I told MLFlow to assume a role before accessing to S3 ?</p>",0,0,2019-07-24 09:27:00.500000 UTC,,,3,amazon-s3|amazon-iam|databricks|mlflow,586,2018-03-13 17:58:54.917000 UTC,2020-11-03 18:13:09.453000 UTC,,31,0,0,4,,,,,,"['databricks', 'mlflow']"
Serving models from mlflow registry to sagemaker,"<p>I have an mlflow server running locally and being exposed at port 80. I also have a model in the mlflow registry and I want to deploy it using the <code>mlflow sagemaker run-local</code> because after testing this locally, I am going to deploy everything to AWS and Sagemaker. My problem is that when I run:</p>
<pre><code>export MODEL_PATH=models:/churn-lgb-test/2
export LOCAL_PORT=8000
mlflow sagemaker run-local -m $MODEL_PATH -p $LOCAL_PORT -f python_function -i splicemachine/mlflow-pyfunc:1.6.0
</code></pre>
<p>it starts the container and I immediately get this error:</p>
<pre><code>2020-07-27 13:02:13 +0000] [827] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:13 +0000] [828] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:13 +0000] [828] [INFO] Worker exiting (pid: 828)
[2020-07-27 13:02:13 +0000] [827] [INFO] Worker exiting (pid: 827)
[2020-07-27 13:02:13 +0000] [829] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:13 +0000] [829] [INFO] Worker exiting (pid: 829)
[2020-07-27 13:02:13 +0000] [830] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:13 +0000] [830] [INFO] Worker exiting (pid: 830)
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 209, in run
    self.sleep()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 357, in sleep
    ready = select.select([self.PIPE[0]], [], [], 1.0)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 242, in handle_chld
    self.reap_workers()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 525, in reap_workers
    raise HaltServer(reason, self.WORKER_BOOT_ERROR)
gunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/bin/gunicorn&quot;, line 8, in &lt;module&gt;
    sys.exit(run())
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 58, in run
    WSGIApplication(&quot;%(prog)s [OPTIONS] [APP_MODULE]&quot;).run()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 228, in run
    super().run()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 72, in run
    Arbiter(self).run()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 229, in run
    self.halt(reason=inst.reason, exit_status=inst.exit_status)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 342, in halt
    self.stop()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 393, in stop
    time.sleep(0.1)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 242, in handle_chld
    self.reap_workers()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 525, in reap_workers
    raise HaltServer(reason, self.WORKER_BOOT_ERROR)
gunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;
creating and activating custom environment
Got sigterm signal, exiting.
[2020-07-27 13:02:13 +0000] [831] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:13 +0000] [831] [INFO] Worker exiting (pid: 831)
[2020-07-27 13:02:14 +0000] [833] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:14 +0000] [833] [INFO] Worker exiting (pid: 833)
[2020-07-27 13:02:14 +0000] [832] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:14 +0000] [832] [INFO] Worker exiting (pid: 832)
</code></pre>",1,1,2020-07-27 13:26:19.060000 UTC,,,2,amazon-sagemaker|mlflow,274,2020-06-05 19:27:43.857000 UTC,2020-11-19 23:45:52.310000 UTC,,21,0,0,1,,,,,,"['mlflow', 'amazon-sagemaker']"
NameError: name 'dbutils' is not defined,"<p>I've .py file with following code line and it lives in git.</p>
<pre><code>dbutils.widgets.text(name='CORPORATION_ID', defaultValue='1234') 
</code></pre>
<p>I am using mlflow to run it in remote databricks job cluster. I've conda.yml and MLProject file to pick it up from git and run it in databricks job cluster but I am getting following error.</p>
<pre><code>  File &quot;tea/src/cltv_xgb_tea.py&quot;, line 40, in &lt;module&gt;
    dbutils.widgets.text(name='CORPORATION_ID', defaultValue='1234')
NameError: name 'dbutils' is not defined
</code></pre>
<p>Any help/solution is much appreciated.</p>
<hr />
<p>My current files in git</p>
<p>Conda.yml has</p>
<pre><code>name: cicd-environment
channels:
  - defaults
dependencies:
  - python=3.7
  - pip=19.0.3
  - pip:
    - mlflow==1.7.2
    - DBUtils==1.3
    - ipython==7.14.0
    - databricks-connect==6.5.1
    - invoke==1.4.1
    - awscli==1.18.87
</code></pre>",0,2,2020-06-25 16:03:46.770000 UTC,,2020-06-29 04:11:36.033000 UTC,0,pyspark|conda|databricks|mlflow,549,2019-04-15 16:50:36.127000 UTC,2022-02-28 22:02:10.127000 UTC,"Minnesota, USA",332,26,1,83,,,,,,"['databricks', 'mlflow']"
what are the events (ex MODEL_VERSION_CREATED) associated with ML FLow Databricks CI/CD,<p>ML Flow has multiple events to subscribe like MODEL_VERSION_CREATED when a model version is created. what are the other events available to subscribe.</p>,1,1,2021-09-28 18:27:45.547000 UTC,,,1,databricks|azure-databricks|mlflow,20,2013-06-12 10:44:50.553000 UTC,2021-11-16 16:58:46.133000 UTC,,11,0,0,1,,,,,,"['databricks', 'mlflow']"
Cannot find '/dbfs/databricks-datasets' in my notebook,"<p>I am using the Databricks community edition and working through the <a href=""https://databricks.com/discover/introduction-to-data-analysis-workshop-series/ml-scikit-learn"" rel=""nofollow noreferrer"">ML intro</a> tutorial.</p>
<p>I am able to <code>%fs ls databricks-datasets/COVID/covid-19-data/us-states.csv</code>, but not able to read it through pandas</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; df = pd.read_csv(&quot;/dbfs/databricks-datasets/COVID/covid-19-data/us-states.csv&quot;)

FileNotFoundError: [Errno 2] File /dbfs/databricks-datasets/COVID/covid-19-data/us-states.csv does not exist: '/dbfs/databricks-datasets/COVID/covid-19-data/us-states.csv'

</code></pre>
<p>Directly <code>open</code> the <code>README.md</code> file in <a href=""https://docs.databricks.com/data/databricks-datasets.html"" rel=""nofollow noreferrer"">databricks dataset</a> also failed</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; f = open(&quot;/dbfs/databricks-datasets/README.md&quot;, &quot;r&quot;)

FileNotFoundError: [Errno 2] No such file or directory: '/dbfs/databricks-datasets/README.md'
</code></pre>
<p>Any thoughts or suggestions?</p>
<p><a href=""https://i.stack.imgur.com/YoMVX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YoMVX.png"" alt=""enter image description here"" /></a></p>",0,4,2021-09-04 05:01:13.607000 UTC,,2021-09-04 15:16:08.450000 UTC,0,databricks|databricks-community-edition|databricks-ml,19,2014-10-29 19:07:27.193000 UTC,2022-03-05 19:16:09.623000 UTC,"Bay Area, CA, USA",4048,573,3,331,,,,,,"['databricks', 'databricks-ml']"
Databricks: Migrate a registered model from one workspace to another?,"<p>We have multiple Databricks Workspaces on Azure. On one of them we trained multiple models and registered them in the MLflow registry. Our goal is to move those model from one databricks workspace to another and so far, i could not find a straight forwared way to do this except running the training script again on the new databricks workspace.</p>
<p>Downloading the model an registering them in the new workspace didn't work so far. Should I create a &quot;dummy&quot; training script, that just loads the model, does nothing with it and then logs it away in the new workspace?</p>
<p>Seems to me like databricks never anticipated, that someone might want to migrate ML models?</p>",2,0,2021-08-25 06:59:47.863000 UTC,,2021-08-25 07:42:09.853000 UTC,1,azure|machine-learning|migration|databricks|mlflow,287,2017-01-22 21:52:54.840000 UTC,2022-03-04 14:25:49.287000 UTC,,178,0,1,19,,,,,,"['databricks', 'mlflow']"
botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not access model data,"<p>I want to deploy an MLflow image to an AWS Sagemaker endpoint that contains a machine learning model. I executed the following code, which I found in <a href=""https://towardsdatascience.com/deploying-models-to-production-with-mlflow-and-amazon-sagemaker-d21f67909198"" rel=""nofollow noreferrer"">this blog post</a>.</p>
<pre><code>import mlflow.sagemaker as mfs

run_id = run_id # the model you want to deploy - this run_id was saved when we trained our model
region = &quot;us-east-1&quot; # region of your account
aws_id = &quot;XXXXXXXXXXX&quot; # from the aws-cli output
arn = &quot;arn:aws:iam::XXXXXXXXXXX:role/your-role&quot;
app_name = &quot;iris-rf-1&quot;
model_uri = &quot;mlruns/%s/%s/artifacts/random-forest-model&quot; % (experiment_id,run_id) # edit this path based on your working directory
image_url = aws_id + &quot;.dkr.ecr.&quot; + region + &quot;.amazonaws.com/mlflow-pyfunc:1.2.0&quot; # change to your mlflow version

mfs.deploy(app_name=app_name, 
           model_uri=model_uri, 
           region_name=region, 
           mode=&quot;create&quot;,
           execution_role_arn=arn,
           image_url=image_url)
</code></pre>
<p>But I got the following error. I checked all policies and permissions attached to the IAM role. They all comply with what the error message complains about. I don't know what to do next. I'd appreciate your help. Thanks.</p>
<p>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not access model data at <a href=""https://s3.amazonaws.com/mlflow-sagemaker-us-east-1-xxx/mlflow-xgb-demo-model-eqktjeoit5mxhmjn-abpanw/model.tar.gz"" rel=""nofollow noreferrer"">https://s3.amazonaws.com/mlflow-sagemaker-us-east-1-xxx/mlflow-xgb-demo-model-eqktjeoit5mxhmjn-abpanw/model.tar.gz</a>. Please ensure that the role &quot;arn:aws:iam::xxx:role/mlflow-sagemaker-dev&quot; exists and that its trust relationship policy allows the action &quot;sts:AssumeRole&quot; for the service principal &quot;sagemaker.amazonaws.com&quot;. Also ensure that the role has &quot;s3:GetObject&quot; permissions and that the object is located in us-east-1.</p>",1,0,2021-01-28 15:47:29.710000 UTC,,,0,amazon-web-services|amazon-sagemaker|mlflow,927,2016-12-22 14:48:27.070000 UTC,2021-05-11 00:09:27.510000 UTC,"Raleigh, NC, United States",39,0,0,9,,,,,,"['mlflow', 'amazon-sagemaker']"
MLFlow model not logging to Azure Blob Storage,"<p>I am trying to use MLFlow to log artifacts to Azure Blob Storage. Though the logging to dbfs works fine, when I try to log it to Azure Blob Storage, I only see a folder with the corresponding runid but inside it there are no contents.</p>

<p>Here is what I do-</p>

<ol>
<li><p>Create a experiment from Azure Databricks, give it a name and the artifacts location as wasbs://mlartifacts@myazurestorageaccount.blob.core.windows.net/ .</p></li>
<li><p>In the spark cluster, in the environemtn Variables section pass on the AZURE_STORAGE_ACCESS_KEY=""ValueoftheKey"" </p></li>
<li>In the notebook, use mlflow to log metrics, param and finally the model using a snippet like below</li>
</ol>

<pre><code>
with mlflow.start_run():
      lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)
      lr.fit(train_x, train_y)

      predicted_qualities = lr.predict(test_x)

      (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)

      print(""Elasticnet model (alpha=%f, l1_ratio=%f):"" % (alpha, l1_ratio))
      print(""  RMSE: %s"" % rmse)
      print(""  MAE: %s"" % mae)
      print(""  R2: %s"" % r2)

      mlflow.log_param(""alpha"", alpha)
      mlflow.log_param(""l1_ratio"", l1_ratio)
      mlflow.log_metric(""rmse"", rmse)
      mlflow.log_metric(""r2"", r2)
      mlflow.log_metric(""mae"", mae)

      mlflow.sklearn.log_model(lr, ""model"")
</code></pre>

<p>Of course before using it , I set the experiment to the one where I have defined the artifacts store to be azure blob storage</p>

<pre><code>experiment_name = ""/Users/user@domain.com/mltestazureblob""
mlflow.set_experiment(experiment_name)
</code></pre>

<p>The metrices and params I can from the MLFlow  UI within Databricks but as since my artifacts location is Azure Blob Storage , I expect the model, the .pkl and conda.yaml file to be in the container in the Azure Blob Storage but when I go to check it, I only see a folder corresponding to the run id of the experiment but with nothing inside.</p>

<p>I do not know what I am missing. In case, someone needs additional details I will be happy to provide.</p>

<p>Point to note everything works fine when I use the default location i.e. dbfs.</p>",1,1,2019-10-23 09:12:45.343000 UTC,,,0,databricks|azure-databricks|mlflow,636,2015-04-10 08:31:54.763000 UTC,2022-03-04 10:48:45.643000 UTC,,410,31,1,51,,,,,,"['databricks', 'mlflow']"
Setting array of tags to MLFlow registered model,"<p>I have a model registered in ML Flow and would like associate a list of tags to that model.
But when i looked at the reference APIs, it looks like we can add only one tag at a time with a single http request.</p>
<pre><code>https://www.mlflow.org/docs/latest/rest-api.html#set-registered-model-tag
</code></pre>
<p>Is it possible to create an array of tags and associate that with model in a single http call ?
Like how we do during model creation API ?</p>
<pre><code>https://www.mlflow.org/docs/latest/rest-api.html#create-registeredmodel
</code></pre>",0,2,2021-12-07 11:48:04.190000 UTC,,2021-12-07 11:55:14.697000 UTC,0,databricks|azure-databricks|mlflow,49,2019-09-20 08:55:45.383000 UTC,2022-03-04 16:27:31.517000 UTC,,237,25,0,58,,,,,,"['databricks', 'mlflow']"
Databricks MLFlow AutoML XGBoost can't predict_proba(),"<p>I used AutoML in Databricks Notebooks for a binary classification problem and the winning model flavor was XGBoost (big surprise).</p>
<p>The outputted model is of this variety:</p>
<pre><code>mlflow.pyfunc.loaded_model:
      artifact_path: model
      flavor: mlflow.sklearn
      run_id: 123456789
</code></pre>
<p>Any idea why when I use <code>model.predict_proba(X)</code>, I get this response?</p>
<p><code>AttributeError: 'PyFuncModel' object has no attribute 'predict_proba'</code></p>
<p>I know it is possible to get the probabilities because ROC/AUC is a metric used for tuning the model. Any help would be amazing!</p>",0,0,2021-12-31 01:02:03.883000 UTC,,,1,pandas|scikit-learn|databricks|xgboost|mlflow,69,2019-07-11 07:02:37.217000 UTC,2022-03-01 00:38:33.770000 UTC,"San Francisco, CA, USA",65,27,0,15,,,,,,"['databricks', 'mlflow']"
Accessing Delta Lake Table in Databricks via Spark in MLflow project,"<p>I am currently accessing deltalake table from databricks notebook using spark. However now I need to access delta tables from MLflow project. MLflow spark api only allows logging and loading of SparkML models. Any idea on how can I accomplish this?</p>
<p>Currently I am trying to access spark via this code in MLflow project:</p>
<pre class=""lang-py prettyprint-override""><code>
spark = pyspark.sql.SparkSession._instantiatedSession
if spark is None:
  # NB: If there is no existing Spark context, create a new local one.
  # NB: We're disabling caching on the new context since we do not need it and we want to
  # avoid overwriting cache of underlying Spark cluster when executed on a Spark Worker
  # (e.g. as part of spark_udf).
  spark = ( pyspark.sql.SparkSession.builder \
   .config(&quot;spark.python.worker.reuse&quot;, True)
   .config(&quot;spark.databricks.io.cache.enabled&quot;, False)
   # In Spark 3.1 and above, we need to set this conf explicitly to enable creating
   # a SparkSession on the workers
   .config(&quot;spark.executor.allowSparkContext&quot;, &quot;true&quot;)
   .master(&quot;local[*]&quot;)
   .appName(&quot;MLflow Project&quot;)
   .getOrCreate()
  )
</code></pre>
<p>But I am getting this error:</p>
<pre><code>py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
</code></pre>",1,2,2022-02-05 20:21:43.000000 UTC,,2022-02-06 17:51:53.797000 UTC,1,apache-spark|pyspark|databricks|delta-lake|mlflow,51,2016-10-23 15:09:44.120000 UTC,2022-02-18 10:03:07.710000 UTC,,61,0,0,7,,,,,,"['databricks', 'mlflow']"
How can I run Tensorboard with MLFlow's logs?,"<p>I use MLFlow with autolog to keep track of my Tensorflow models:</p>
<pre><code>mlflow.tensorflow.autolog(every_n_iter=1)
with mlflow.start_run():
  model = ...
  model.compile(...)
  model.fit(...)
</code></pre>
<p>and then I want to use my tensorboard logs located in the artifacts.
But when I run:</p>
<pre><code>%tensorboard --logdir=&lt;logs_path&gt;
</code></pre>
<p>I have the error message:
&quot;No dashboards are active for the current data set.
Probable causes:</p>
<p>You haven’t written any data to your event files.
TensorBoard can’t find your event files.&quot;</p>
<p>I work on Databricks, so log_path is something like:</p>
<pre><code>/dbfs/databricks/mlflow-tracking/..
</code></pre>
<p>Any ideas?</p>",0,0,2021-12-16 18:15:45.067000 UTC,,,0,tensorflow|databricks|tensorboard|mlflow,62,2021-12-16 18:03:10.923000 UTC,2022-03-04 14:46:11.230000 UTC,,1,0,0,1,,,,,,"['databricks', 'mlflow']"
Sagemaker Train Job can't connect to ec2 instance,"<p>I have MLFlow server running on ec2 instance, port 5000.</p>
<p>This ec2 instance has security group with opened TCP connection on port 5000 to another security group designated for SageMaker.</p>
<p>ec2 instance inbound rules:
<a href=""https://i.stack.imgur.com/VXwid.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VXwid.png"" alt=""enter image description here"" /></a></p>
<p>SageMaker outbound rules:
<a href=""https://i.stack.imgur.com/ZUzek.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZUzek.png"" alt=""enter image description here"" /></a></p>
<p>These 2 security groups are in the same VPC</p>
<p>Now, I try to run SageMaker training job with designated security group, so that the training script will log metrics to ec2 server via internal IP address. (As answered <a href=""https://stackoverflow.com/questions/45416882/aws-security-group-include-another-security-group"">here</a>), but connection fails</p>
<p>SageMaker job init:</p>
<pre><code>   role = &quot;ml_sagemaker&quot;
   security_group_ids = ['sg-04868acca16e81183']
   bucket = sagemaker_session.default_bucket()  
   out_path = f&quot;s3://{bucket}/{project_name}&quot;

   estimator = PyTorch(entry_point='run_train.py',
                       source_dir='.',
                       sagemaker_session=sagemaker_session,
                       instance_type=instance_type,
                       instance_count=1,
                       framework_version='1.5.0',
                       py_version='py3',
                       role=role,
                       security_group_ids=security_group_ids,
                       hyperparameters={},
                       )
   ....

</code></pre>
<p>Inside <code>run_train.py</code>:</p>
<pre><code>import mlflow
tracking_uri = &quot;http://172.31.77.137:5000&quot;  # &lt;- this is internal ec2 IP
mlflow.set_tracking_uri(tracking_uri)
mlflow.log_param(&quot;test_param&quot;, 3)
</code></pre>
<p>Error:</p>
<pre><code>File &quot;/opt/conda/lib/python3.6/site-packages/urllib3/util/connection.py&quot;, line 74, in create_connection
    sock.connect(sa)
TimeoutError: [Errno 110] Connection timed out
</code></pre>
<p><strong>However</strong>, when when I create SageMaker Notebook instance with the same security group and the same IAM role, I am able to successfully connect to ec2 and log metrics from within the Notebook.</p>
<p><a href=""https://i.stack.imgur.com/YYHlO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YYHlO.png"" alt=""enter image description here"" /></a></p>
<p>Here is SageMaker Notebook configurations:</p>
<img src=""https://i.stack.imgur.com/bslu8.png"" width=""300"" />
<p>How can I connect to ec2 instance from SageMaker Training Job?</p>",1,0,2021-02-19 17:18:49.957000 UTC,1.0,,1,amazon-ec2|amazon-vpc|amazon-sagemaker|aws-security-group|mlflow,391,2015-07-28 15:46:05.740000 UTC,2022-02-14 09:45:54.667000 UTC,,633,213,1,72,,,,,,"['mlflow', 'amazon-sagemaker']"
Access databricks secrets in pyspark/python job,"<p>Databricks secrets can be accessed within notebooks using dbutils, however since dbutils is not available outside notebooks how can one access secrets in pyspark/python jobs, especially if they are run using mlflow.</p>

<p>I have already tried <a href=""https://stackoverflow.com/questions/51885332/how-to-load-databricks-package-dbutils-in-pyspark?rq=1"">How to load databricks package dbutils in pyspark</a></p>

<p>which does not work for remote jobs or mlflow project runs.</p>",1,0,2020-06-08 21:40:46.997000 UTC,,,0,databricks|azure-databricks|mlflow,717,2017-06-15 11:22:56.727000 UTC,2021-03-07 17:34:22.217000 UTC,"Mumbai, Maharashtra, India",471,16,0,48,,,,,,"['databricks', 'mlflow']"
How to download an artifact from MLFlow using REST?,"<p>I see the Python API:
<code>download_artifacts(run_id: str, path: str, dst_path: Optional[str] = None) → str</code> (<a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.download_artifacts"" rel=""nofollow noreferrer"">here</a>), but I can't find the equivalent in REST.</p>",0,0,2021-11-08 11:13:52.513000 UTC,,2021-11-09 10:44:51.217000 UTC,1,rest|databricks|mlflow,75,2009-06-14 12:54:00.077000 UTC,2022-03-04 12:04:16.940000 UTC,"New York, NY",13180,299,12,678,,,,,,"['databricks', 'mlflow']"
Is it possible to specify MLflow project Environment through a Dockerfile (instead of an image)?,"<p>To my understanding, currently (May 2019) mlflow support running project in docker environment; however, it needs the docker image already been built. This leaves the docker image building to be a separate workflow. What is the suggested way to run a mlflow project from Dockerfile? </p>

<p>Is there plans to support targeting Dockerfile natively in mlflow? What are the considerations about using image vs Dockerfile? Thanks!</p>",2,0,2019-05-13 21:20:34.943000 UTC,,2019-05-13 23:47:12.507000 UTC,2,docker|machine-learning|artificial-intelligence|databricks|mlflow,501,2013-12-11 04:18:22.123000 UTC,2022-03-04 09:04:18.810000 UTC,,3039,621,8,986,,,,,,"['databricks', 'mlflow']"
MLflow 1.2.0 define MLproject file,"<p>Trying to run mlflow run by specifying MLproject and code which lives in a different location as MLproject file.</p>

<p>I have the following directory structure:</p>

<pre><code>/root/mflow_test
.
├── conda
│   ├── conda.yaml
│   └── MLproject
├── docker
│   ├── Dockerfile
│   └── MLproject
├── README.md
├── requirements.txt
└── trainer
    ├── __init__.py
    ├── task.py
    └── utils.py
</code></pre>

<p>When I'm run from: <code>/root/</code></p>

<pre><code>mlflow run mlflow_test/docker
</code></pre>

<p>I get:</p>

<pre><code>/root/miniconda3/bin/python: Error while finding module specification for 'trainer.task' (ImportError: No module named 'trainer')
</code></pre>

<p>Since my <code>MLproject</code> file can't find the Python code.
I moved MLproject to <code>mflow_test</code> and this works fine.</p>

<p>This is my MLproject entry point:</p>

<pre><code>name: mlflow_sample
docker_env:
  image: mlflow-docker-sample
entry_points:
  main:
    parameters:
      job_dir:
        type: string
        default: '/tmp/'
    command: |
        python -m trainer.task --job-dir {job_dir}
</code></pre>

<p>How can I run <code>mlflow run</code> and pass the MLproject and ask it to look in a different folder?</p>

<p>I tried:</p>

<pre><code>""cd .. &amp;&amp; python -m trainer.task --job-dir {job_dir}"" 
</code></pre>

<p>and I get:</p>

<p><code>/entrypoint.sh: line 5: exec: cd: not found</code></p>

<p><strong>Dockerfile</strong></p>

<pre><code># docker build -t mlflow-gcp-example -f Dockerfile .
FROM gcr.io/deeplearning-platform-release/tf-cpu 
RUN git clone github.com/GoogleCloudPlatform/ml-on-gcp.git 
WORKDIR ml-on-gcp/tutorials/tensorflow/mlflow_gcp 
RUN pip install -r requirements.txt 
</code></pre>",0,7,2019-08-19 04:02:46.530000 UTC,,2019-08-19 05:51:16.957000 UTC,0,docker|databricks|mlflow,565,2010-01-28 09:42:15.677000 UTC,2022-03-06 03:10:49.463000 UTC,"San Francisco, CA",8214,1868,97,1201,,,,,,"['databricks', 'mlflow']"
Pyspark: How to save and apply IndexToString to convert labels back to original values in a new predicted dataset,"<p>I am using pyspark.ml.RandomForestClassifier and one of the steps here involves <strong>StringIndexer</strong> on the training data target variable to convert it into labels.</p>
<pre><code>indexer = StringIndexer(inputCol = target_variable_name, outputCol = 'label').fit(df)
df = indexer.transform(df)
</code></pre>
<p>After fitting the final model I am saving it using mlflow.spark.log_model(). So, when applying the model on a new dataset in future, I just load the model again and apply to the new data:</p>
<pre><code>model = mlflow.sklearn.load_model(&quot;models:/RandomForest_model/None&quot;)
predictions = rfModel.transform(new_data)
</code></pre>
<p>In the new_data the prediction will come as <strong>labels</strong> and not in original value. So, if I have to get the original values I have to use <strong>IndexToString</strong></p>
<pre><code>labelConverter = IndexToString(inputCol=&quot;prediction&quot;, outputCol=&quot;predictedLabel&quot;,labels=indexer.labels)
predictions = labelConverter.transform(predictions)
</code></pre>
<p>So, the question is, my model doesn't save the <strong>indexer.labels</strong> as only the model gets saved. How do, I save and use the indexer.labels from my training dataset on any new dataset. Can this be saved and retrived in mlflow ?</p>
<p>Apologies, if Iam sounding naïve here . But, getting back the original values in the new dataset is really getting me confused.</p>",0,0,2021-10-07 16:29:57.133000 UTC,,2021-10-08 07:06:50.887000 UTC,1,pyspark|databricks|random-forest|apache-spark-mllib|mlflow,51,2017-07-27 12:59:26.927000 UTC,2022-03-01 10:23:05.930000 UTC,,405,93,0,57,,,,,,"['databricks', 'mlflow']"
Serving multiple ML models using mlflow in a single VM,"<p>I have setup an mlflow service in a VM and I am able to serve the model using mlflow serve command.
Wanted to know if we can host multiple models in a single VM ?</p>
<p>I am using the below command to serve a model using mlflow in a vm.</p>
<p>command:</p>
<pre><code>/mlflow models serve -m models:/$Model-Name/$Version --no-conda -p 443 -h 0.0.0.0
</code></pre>
<p>Above command creates a model serving and runs it on 443 port.
Is it possible to have an endpoint like below being created with model name in it ?</p>
<p>Current URL:
https://localhost:443/invocations</p>
<p>Expected URL:
https://localhost:443/model-name/invocations ?</p>",2,1,2022-01-07 10:48:20.160000 UTC,,2022-01-07 12:33:40.040000 UTC,0,apache-spark|machine-learning|databricks|mlflow,84,2019-09-20 08:55:45.383000 UTC,2022-03-04 16:27:31.517000 UTC,,237,25,0,58,,,,,,"['databricks', 'mlflow']"
What are the pros and cons of using DVC and Pachyderm?,"<p>What are the pros and cons of using either of these?</p>

<p><a href=""https://github.com/iterative/dvc"" rel=""nofollow noreferrer"">https://github.com/iterative/dvc</a></p>

<p><a href=""https://github.com/pachyderm/pachyderm"" rel=""nofollow noreferrer"">https://github.com/pachyderm/pachyderm</a></p>",1,1,2019-07-04 06:12:59.043000 UTC,,,1,machine-learning|version-control|data-science|dvc|pachyderm,1515,2019-07-04 06:06:43.123000 UTC,2019-07-18 00:21:59.090000 UTC,,41,0,0,3,,,,,,"['dvc', 'pachyderm']"
How to run non-spark model training task (using fasttext) efficiently on a databricks cluster?,"<p>I want to train some models using <code>fasttext</code> and since it doesn't use spark, it will be running on my driver. The number of training jobs that will be running simultaneously is very large and so is the size of the data. Is there a way to make it run it on different workers or distribute it across workers?
Is this the best approach or am I better off using a large single node cluster?</p>
<p>FYI, I am using Databricks. So solutions specific to that are also okay.</p>",1,0,2021-11-12 12:03:47.093000 UTC,,2021-11-20 12:06:30.140000 UTC,2,apache-spark|pyspark|databricks|fasttext|mlflow,47,2018-11-28 07:36:30.310000 UTC,2022-03-03 09:57:34.407000 UTC,"Chennai, Tamil Nadu, India",125,6,1,34,,,,,,"['databricks', 'mlflow']"
How to use MLfLow with private git repositories?,"<p>I tested <code>MLflow</code> experiment when the source code is stored in public a git repository. Example command looks like this</p>

<pre><code>mlflow run  https://github.com/amesar/mlflow-fun.git#examples/hello_world \
 --experiment-id=2019 \
 -Palpha=100 -Prun_origin=GitRun -Plog_artifact=True
</code></pre>

<p>However, when I provide an internal (private) git repository link instead of public- MLflow redirects to login url, and then execution fails like this.</p>

<pre><code>git.exc.GitCommandError: Cmd('git') failed due to: exit code(128)
cmdline: git fetch -v origin
stderr: 'fatal: unable to update url base from redirection:
asked for: https://gitlab-master.companyname.com/myusername/project_name
/tree/master/models/myclassifier/info/refs?service=git-upload-pack
redirect: https://gitlab-master.company.com/users/sign_in'
</code></pre>

<p>Is there any way to commmunicate credentials of git account to MLflow?</p>",1,0,2019-07-15 05:56:44.117000 UTC,,,2,git|gitlab|databricks|mlflow,961,2014-05-26 11:37:08.227000 UTC,2021-08-12 20:41:51.087000 UTC,"Santa Clara, CA, USA",3571,78,6,231,,,,,,"['databricks', 'mlflow']"
Does MLflow allow to log artifacts from remote locations like S3?,"<h2>My setting</h2>
<p>I have developed an environment for ML experiments that looks like the following: training happens in the AWS cloud with SageMaker Training Jobs. The trained model is stored in the <code>/opt/ml/model</code> directory, <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-output.html"" rel=""nofollow noreferrer"">which is reserved by SageMaker to pack models</a> as a <code>.tar.gz</code> in SageMaker's own S3 bucket. Several evaluation metrics are computed during training and testing, and recorded to an MLflow infrastructure consisting of an S3-based artifact store (see <a href=""https://www.mlflow.org/docs/latest/tracking.html#scenario-4-mlflow-with-remote-tracking-server-backend-and-artifact-stores"" rel=""nofollow noreferrer"">Scenario 4</a>). Note that this is a different S3 bucket than SageMaker's.</p>
<p>A very useful feature from MLflow is that any model artifacts can be logged to a training run, so data scientists have access to both metrics and more complex outputs through the UI. These outputs include (but are not limited to) the trained model itself.</p>
<p>A limitation is that, as I understand it, the <a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact"" rel=""nofollow noreferrer"">MLflow API for logging artifacts</a> only accepts as input a local path to the artifact itself, and will always upload it to its artifact store. This is suboptimal when the artifacts are stored somewhere outside MLflow, as you have to store them twice. A transformer model may weigh more than 1GB.</p>
<h2>My questions</h2>
<ul>
<li>Is there a way to pass an S3 path to MLflow and make it count as an artifact, without having to download it locally first?</li>
<li>Is there a way to avoid pushing a copy of an artifact to the artifact store? If my artifacts already reside in another remote location, it would be ideal to just have a link to such location in MLflow and not a copy in MLflow storage.</li>
</ul>",0,0,2022-01-12 10:49:12.913000 UTC,,,0,python|amazon-s3|amazon-sagemaker|mlflow|mlops,73,2016-01-08 20:55:48.080000 UTC,2022-03-03 17:19:05.740000 UTC,"Madrid, Spain",88,27,0,10,,,,,,"['mlflow', 'amazon-sagemaker']"
Register SageMaker model in MLflow,"<p>MLflow can be used to track (hyper)parameters and metrics when training machine learning models. It stores the trained model as an artifact for every experiment. These models then can be directly deployed as <a href=""https://mlflow.org/docs/latest/models.html#deploy-a-python-function-model-on-amazon-sagemaker"" rel=""nofollow noreferrer"">SageMaker endpoints</a>.</p>
<p>Is it possible to do it the other way around, too, i.e. to register models trained in SageMaker into MLflow?</p>",1,0,2022-02-23 13:34:36.023000 UTC,,,0,amazon-sagemaker|mlflow,14,2013-10-28 11:07:46.783000 UTC,2022-03-05 01:57:07.347000 UTC,"Vienna, Austria",670,685,1,168,,,,,,"['mlflow', 'amazon-sagemaker']"
mlflow.pyfunc.spark_udf and vector struct type,"<p>My <em>PySpark</em> dataset contains categorical data.</p>
<p>To train a model on this data, I followed this <a href=""https://docs.databricks.com/_static/notebooks/binary-classification.html"" rel=""nofollow noreferrer"">example notebook</a>. Especially, see the <em>Preprocess Data</em> section for the encoding part.</p>
<p>I now need to use this model somewhere else; hence, I followed <em>Databricks</em> recommendation to save and load this model.</p>
<p>It's working fine with <em>Pandas</em> (cf. code below).</p>
<pre><code>logged_model = 'runs:/e905f5759d434a1391bbe1e54a2b/best-model'

# Load model as a PyFuncModel.
loaded_model = mlflow.pyfunc.load_model(logged_model)

# Predict on a Pandas DataFrame.
import pandas as pd
loaded_model.predict(pd.DataFrame(data))
</code></pre>
<p>However the dataframe is to big to be converted to <em>Pandas</em>. Hence I need to make it work in <em>Spark</em>:</p>
<pre><code>import mlflow
logged_model = 'runs:/e905f5759d434a131bbe1e54a2b/best-model'

# Load model as a Spark UDF.
loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model)

# Predict on a Spark DataFrame.
df.withColumn('predictions', loaded_model(*columns)).collect()
</code></pre>
<p>But this snippet is producing:</p>
<pre><code>java.lang.UnsupportedOperationException: Unsupported data type: struct&amp;lt;type:tinyint,size:int,indices:array&amp;lt;int&amp;gt;,values:array&amp;lt;double&amp;gt;&amp;gt;
</code></pre>
<p>My feeling is that the udf doesn't accept this type of data as input.
Is there a way to fix it ?
Another solution ?</p>",1,0,2021-07-26 09:23:08.767000 UTC,2.0,,1,pyspark|databricks|mlflow,321,2016-11-29 05:56:02.230000 UTC,2021-09-06 10:25:08.607000 UTC,,410,18,4,49,,,,,,"['databricks', 'mlflow']"
How can I save an Onnx model that takes a array as an input type?,"<p>I have a model pipeline that includes a word2vec step before a logistic regression. I want to save the model using ONNX for model portability. There are three datasets: two properties columns where the data starts as long format and is aggregated into an array before being inputted into the model and one with information about each IDs.</p>
<p>The spark Pipeline I'm using has a few transformation stages like a string indexer and one hot encoder on the string column <code>Value 3</code> and uses a word2vec model on the property columns.</p>
<pre><code>pipeline = Pipeline(stages=[string_indexer, one_hot_encoder, word2vec1, word2vec2, vector_assembler, logistic_regression])
</code></pre>
<p>When I run the model and try to save it in Onnx format, it tells me that it cannot map the array input for the word2vecs to Onnx types; Onnx only accepts certain types (see <a href=""https://github.com/onnx/onnxmltools/blob/master/onnxmltools/convert/sparkml/utils.py"" rel=""nofollow noreferrer"">here</a>) and this doesn't include arrays.</p>
<p>I tried moving the data transformation to a Transformer pipeline stage <a href=""https://stackoverflow.com/questions/51415784/how-to-add-my-own-function-as-a-custom-stage-in-a-ml-pyspark-pipeline"">like this</a> but I can't input three separate dataframes as input to the Transformer and if I make it so the two property dataframes are inputs to the class initialization like:</p>
<pre><code>data_transformer = DataTransformer(properties1, properties2)
pipeline = Pipeline(stages=[data_transformer, the_other_stages])
results = pipeline.transform(id_table)
</code></pre>
<p>I can't log/save the model with MLFlow because the stage is not MLWritable and it doesn't seem like best practice to have two potentially giant dataframes as an input to the class initialization.</p>
<p>Is there a way to use an array type as an input to an Onnx model?</p>
<p>If not, how would you approach the data transformation so that all the inputs to the pipeline can be of a type that Onnx will accept but that will also work as a loggable model in MLFlow?</p>
<p>I am working in Azure Databricks.</p>",0,0,2021-12-20 14:48:12.073000 UTC,,,0,pyspark|databricks|onnx|mlflow,50,2021-07-06 20:01:11.153000 UTC,2022-03-04 14:56:55.440000 UTC,,31,0,0,7,,,,,,"['databricks', 'mlflow']"
UI does display data in MLflow,"<p>This is in reference to rather comment (not answer), I added here: <a href=""https://stackoverflow.com/questions/63255631/mlflow-invalid-parameter-value-unsupported-uri-mlruns-for-model-registry-s/66371465#66371465"">MLflow: INVALID_PARAMETER_VALUE: Unsupported URI './mlruns' for model registry store</a></p>
<p>I extracted files from <a href=""https://github.com/mlflow/mlflow/tree/master/examples/sklearn_elasticnet_wine"" rel=""nofollow noreferrer"">here</a></p>
<pre><code>train.py     MLproject     wine-quality.csv 
</code></pre>
<p>These are in directory:<code>feb24MLFLOW</code></p>
<p>I am in directory <code>feb24MLFLOW</code> with following contents</p>
<pre><code>:memory          mlruns           train.py         wine-quality.csv
</code></pre>
<p>When I run following command</p>
<pre><code>mlflow server --backend-store-uri sqlite:///:memory --default-artifact-root ./mlruns
</code></pre>
<p><strong>The UI loads but does not show any data in it neigther does database as below. see screenshot.</strong>
<a href=""https://i.stack.imgur.com/j3XSe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j3XSe.png"" alt=""enter image description here"" /></a>
I am using <code>--default-artifact-root ./mlruns</code> flag because, when I print  print(mlflow.get_tracking_uri()), I get the current directory</p>
<pre><code>file:///&lt;mydirectorylocations&gt;/feb24MLFLOW/mlruns
</code></pre>
<p>For some reason I see my database is not updating (or inserting). I checked that with in terminal.</p>
<pre><code>$  sqlite3
sqlite&gt; .open :memory 
sqlite&gt; .tables
alembic_version        metrics                registered_model_tags
experiment_tags        model_version_tags     registered_models    
experiments            model_versions         runs                 
latest_metrics         params                 tags                 
sqlite&gt; select * from runs;
sqlite&gt; 
</code></pre>
<p>As you can see there is no data after running <code>select * from runs</code> above.</p>
<p>Please note that I have following contents in</p>
<pre><code>./mlruns

d6db5cf1443d49c19971a1b8b606d692 meta.yaml

Can somebody suggest I show results in the UI? or insert in databse? or what am I doing wrong? 
</code></pre>
<p>Please note that when I run <code>mlflow ui</code>, I see data in the UI but I get:</p>
<pre><code>error_code: &quot;INVALID_PARAMETER_VALUE&quot;
message: &quot; Model registry functionality is unavailable; got unsupported URI './mlruns' for model registry data storage. Supported URI schemes are: ['postgresql', 'mysql', 'sqlite', 'mssql']. See https://www.mlflow.org/docs/latest/tracking.html#storage for how to run an MLflow server against one of the supported backend storage locations.&quot; 
</code></pre>",0,5,2021-02-25 15:35:45.170000 UTC,,2021-02-26 18:02:39.173000 UTC,2,databricks|mlflow,634,2015-07-15 19:03:26.677000 UTC,2022-03-06 02:19:52.877000 UTC,"New York, NY, United States",793,6,2,173,,,,,,"['databricks', 'mlflow']"
Data bricks:- Cannot display the predicted output by using ml flow registered model,"<p>I have created a model using diabetes dataset for prediction. I have trained, evaluated, logged and registered it as a new model in ML flow. Now I am trying to load the registered model and trying to predict on new data. All though I was able to predict the results. I am not able to display it. When I try to display using command <code>.show()</code> or <code>display()</code> it is throwing an error. What is the cause of the error? and How do I display the results?</p>
<p>Note: I have programmed using pure pyspark and all the ML flow operation was done on Data bricks</p>
<p>Code:-</p>
<pre><code>model_details = mlflow.tracking.MlflowClient().get_latest_versions('model1',stages=['staging'])[0]
model = mlflow.pyfunc.spark_udf(spark,model_details.source)
input_df = sdf.drop('progression')
columns = list(map(lambda c: f&quot;{c}&quot;, input_df.columns))
df = input_df.withColumn(&quot;progression&quot;, model(*columns))
df.show(truncate=False)
</code></pre>
<p>Error :-</p>
<pre><code>PythonException: An exception was thrown from a UDF: 'Exception: Java gateway process exited before sending its port number'. Full traceback below:
PythonException                           Traceback (most recent call last)
&lt;command-1343735193245452&gt; in &lt;module&gt;
     34 df = input_df.withColumn(&quot;progression&quot;, model(*columns))
     35 
---&gt; 36 df.show(truncate=False)

/databricks/spark/python/pyspark/sql/dataframe.py in show(self, n, truncate, vertical)
    441             print(self._jdf.showString(n, 20, vertical))
    442         else:
--&gt; 443             print(self._jdf.showString(n, int(truncate), vertical))
    444 
    445     def __repr__(self):

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-&gt; 1305             answer, self.gateway_client, self.target_id, self.name)
   1306 
   1307         for temp_arg in temp_args:

/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
    131                 # Hide where the exception came from that shows a non-Pythonic
    132                 # JVM exception message.
--&gt; 133                 raise_from(converted)
    134             else:
    135                 raise

/databricks/spark/python/pyspark/sql/utils.py in raise_from(e)

PythonException: An exception was thrown from a UDF: 'Exception: Java gateway process exited before sending its port number'. Full traceback below:
Traceback (most recent call last):
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 654, in main
    process()
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 646, in process
    serializer.dump_stream(out_iter, outfile)
  File &quot;/databricks/spark/python/pyspark/sql/pandas/serializers.py&quot;, line 281, in dump_stream
    timely_flush_timeout_ms=self.timely_flush_timeout_ms)
  File &quot;/databricks/spark/python/pyspark/sql/pandas/serializers.py&quot;, line 97, in dump_stream
    for batch in iterator:
  File &quot;/databricks/spark/python/pyspark/sql/pandas/serializers.py&quot;, line 271, in init_stream_yield_batches
    for series in iterator:
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 467, in mapper
    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 467, in &lt;genexpr&gt;
    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 111, in &lt;lambda&gt;
    verify_result_type(f(*a)), len(a[0])), arrow_return_type)
  File &quot;/databricks/spark/python/pyspark/util.py&quot;, line 109, in wrapper
    return f(*args, **kwargs)
  File &quot;/databricks/python/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 827, in predict
    model = SparkModelCache.get_or_load(archive_path)
  File &quot;/databricks/python/lib/python3.7/site-packages/mlflow/pyfunc/spark_model_cache.py&quot;, line 64, in get_or_load
    SparkModelCache._models[archive_path] = load_pyfunc(temp_dir)
  File &quot;/databricks/python/lib/python3.7/site-packages/mlflow/utils/annotations.py&quot;, line 43, in deprecated_func
    return func(*args, **kwargs)
  File &quot;/databricks/python/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 693, in load_pyfunc
    return load_model(model_uri, suppress_warnings)
  File &quot;/databricks/python/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 667, in load_model
    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/databricks/python/lib/python3.7/site-packages/mlflow/spark.py&quot;, line 707, in _load_pyfunc
    .master(&quot;local[1]&quot;)
  File &quot;/databricks/spark/python/pyspark/sql/session.py&quot;, line 189, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
  File &quot;/databricks/spark/python/pyspark/context.py&quot;, line 384, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File &quot;/databricks/spark/python/pyspark/context.py&quot;, line 134, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File &quot;/databricks/spark/python/pyspark/context.py&quot;, line 333, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File &quot;/databricks/spark/python/pyspark/java_gateway.py&quot;, line 105, in launch_gateway
    raise Exception(&quot;Java gateway process exited before sending its port number&quot;)
Exception: Java gateway process exited before sending its port number
</code></pre>",0,0,2021-09-14 10:18:11.113000 UTC,,,1,pyspark|apache-spark-sql|user-defined-functions|databricks|mlflow,105,2021-08-10 12:49:57.537000 UTC,2021-11-22 17:00:30.000000 UTC,,101,12,0,17,,,,,,"['databricks', 'mlflow']"
How to save objects in MLFlow Model Class cache in Databricks?,"<p>I have defined a custom MLFlow Model using <a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html"" rel=""nofollow noreferrer""><code>mlflow.pyfunc</code></a> class. It contains the custom <code>load_context()</code> and <code>predict()</code> functions as required.</p>
<p>It also contains some other helper functions which I need inside the <code>predict()</code> functions. One of these functions includes a function that saves a Pandas DataFrame in the cache, which is used to create some custom features which need historic data (just 20 rows).</p>
<pre class=""lang-py prettyprint-override""><code>artifacts = {
         &quot;preprocessor&quot;: preprocessor,
         &quot;keras_model&quot;: kerasmodel
      }

class CustomModelClass(mlflow.pyfunc.PythonModel):
  def load_context(self, context):
    from keras.models import load_model
    import pickle

    #load in and deserialize the preprocessor from the artifact
    with open(str(context.artifacts[&quot;preprocessor&quot;]), 'rb') as handle:
      self.clf = pickle.load(handle)
    
    #load in and deserialize the keras model from the artifact
    self.keras_model = load_model(context.artifacts[&quot;keras_model&quot;])
    
  global test_df

  def prepend_window(self, in_df):
    global test_df
    if test_df is None:
        out_df = in_df.copy()
        test_df = in_df.tail(20).copy()
    else:  
        out_df = pd.concat([test_df, in_df])
        test_df = out_df.tail(20).copy()

    return out_df

  def predict(self, context, model_input):
    in_df = model_input
    in_df['_time'] = pd.to_datetime(in_df['_time'], utc=True)
        
    hist_df = self.prepend_window(in_df)
    hist_df.fillna(method='ffill', inplace=True)

    # Some Custom Feature Engineering Code

    #Transforming the dataset and making predictions.

    X_train = self.clf.transform(model_input)
    X_pred = self.keras_model.predict(X_train)
    
    #Some More Post-Processing to create a Dataframe - results.
    
    return results

# Create a Conda environment for the new MLflow Model that contains all necessary dependencies.
PYTHON_VERSION = &quot;{major}.{minor}.{micro}&quot;.format(major=version_info.major,minor=version_info.minor,micro=version_info.micro)

conda_env = {
    'channels': ['defaults'],
    'dependencies': [
      'python={}'.format(PYTHON_VERSION),
      'pip',
      {
        'pip': [
             'mlflow',
             'tensorflow==2.4.0',
             'scipy==1.5.2',
             'numpy==1.19.5',
             'pandas==1.1.5',
             'scikit-learn==0.24.0',
             'sklearn-pandas==2.0.4',
             'keras==2.4.3'
        ],
      },
    ],
    'name': 'custom_env'
}

# Log the model
mlflow.pyfunc.log_model(artifact_path=&quot;model&quot;, python_model=CustomModelClass(), artifacts=artifacts, conda_env=conda_env)
</code></pre>
<p>I know that it is working properly since when I initiate the class and call the predict function with more than 20 rows (for the first 20 rows - we do not need feature engineering), we get the expected results (features get populated from the 21st row). However, when I call the predict function again, those 20 rows no longer stay in cache (and we do no get the features).</p>
<p>In production, we will be getting one row at a time and it should keep getting added to the cache:
If I denote a record by an alphabet it will be like this:</p>
<pre><code>test_df = a

test_df = ab

test_df = abc

test_df = abcd

......

test_df = abcde.........pqrst

test_df = bcde.........pqrstu

test_df = cde.........pqrstuv
</code></pre>
<p>and so on</p>
<p>And the Feature Engineering step calculates some windowing features.</p>
<p>How do we ensure that test_df stays in memory?</p>
<pre class=""lang-py prettyprint-override""><code>loaded_model = mlflow.pyfunc.load_model(model_uri=f&quot;models:/{model_name}/{model_version}&quot;)
test1 = input_df[0:30]
pred = loaded_model.predict(test1)
</code></pre>
<p>works perfectly</p>
<pre class=""lang-py prettyprint-override""><code>loaded_model = mlflow.pyfunc.load_model(model_uri=f&quot;models:/{model_name}/{model_version}&quot;)
test1 = input_df[0:10]
pred = loaded_model.predict(test1)
test2 = input_df[10:20]
pred = loaded_model.predict(test2)
test3 = input_df[20:30]
pred = loaded_model.predict(test3)
</code></pre>
<p>Should work similarly (the first 20 rows) should stay in cache and we get custom features for the last 10 rows, but that is not working.</p>",0,0,2021-08-17 06:07:43.013000 UTC,,2021-08-18 13:49:39.470000 UTC,0,python|azure|databricks|azure-databricks|mlflow,286,2020-10-03 12:46:02.437000 UTC,2022-02-28 17:58:49.850000 UTC,"Hyderabad, Telangana, India",704,173,32,103,,,,,,"['databricks', 'mlflow']"
How to run data bricck notebook with mlflow in azure data factory pipeline?,"<p>My colleagues and I are facing an issue when trying to run my databricks notebook in Azure Data Factory and the error is coming from MLFlow.</p>
<p>The command that is failing is the following:</p>
<pre><code># Take the parent notebook path to use as path for the experiment
context = json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())
nb_base_path = context['extraContext']['notebook_path'][:-len(&quot;00_training_and_validation&quot;)]

experiment_path = nb_base_path + 'trainings'
mlflow.set_experiment(experiment_path)
experiment = mlflow.get_experiment_by_name(experiment_path)
experiment_id = experiment.experiment_id

run = mlflow.start_run(experiment_id=experiment_id, run_name=f&quot;run_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}&quot;)
</code></pre>
<p>And the error that is throwing is:</p>
<p>An exception was thrown from a UDF: 'mlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: No experiment ID was specified. An experiment ID must be specified in Databricks Jobs and when logging to the MLflow server from outside the Databricks workspace. If using the Python fluent API, you can set an active experiment under which to create runs by calling mlflow.set_experiment(&quot;/path/to/experiment/in/workspace&quot;) at the start of your program.', from , line 32.</p>
<p>The pipeline just runs the notebook from ADF, it does not have any other step and the cluster we are using is type 7.3 ML.</p>
<p>Could you please help us?</p>
<p>Thank you in advance!</p>",1,0,2022-02-03 09:23:02.260000 UTC,,,0,azure-data-factory|databricks|mlflow,34,2020-04-08 07:25:08.360000 UTC,2022-02-03 09:43:32.287000 UTC,"Madrid, Spain",35,1,0,10,,,,,,"['databricks', 'mlflow']"
How do I invoke a data enrichment function before model.predict while serving the model in Databricks,"<p>In Databricks, I have used mlflow and got my model served through REST API. It works fine when all model features are provided. But my use case is that only a single feature (the primary key) will be provided by the consumer application, and my code has to lookup the other features from a database based on that key and then use the model.predict to return the prediction. I tried researching but understood that the REST endpoints will simply invoke the model.predict function. How can I make it invoke a data massaging function before predicting?</p>",1,0,2022-02-14 08:12:02.870000 UTC,,2022-02-14 10:52:46.403000 UTC,1,model|databricks|mlflow|serving,12,2018-07-09 13:16:02.080000 UTC,2022-03-04 10:37:27.640000 UTC,,11,0,0,0,,,,,,"['databricks', 'mlflow']"
ModuleNotFoundError: No module named 'pyspark.dbutils' while running multiple.py file/notebook on job clusters in databricks,"<p>I am working in TravisCI, MlFlow and Databricks environment where .tavis.yml sits at git master branch and detects any change in <code>.py</code> file and whenever it gets updated, It will run mlflow command to run .py file in databricks environment. 
my MLProject file looks as following:</p>

<pre><code>name: mercury_cltv_lib
conda_env: conda-env.yml


entry_points:    
  main:
    command: ""python3 run-multiple-notebooks.py""
</code></pre>

<p>Workflow is as following:
TravisCI detects change in master branch-->triggers build which will run MLFlow command and it'll spin up a job cluster in databricks to run .py file from repo.</p>

<p>It worked fine with one .py file but when I tried to run multiple notebook using dbutils, it is throwing </p>

<pre><code>  File ""run-multiple-notebooks.py"", line 3, in &lt;module&gt;
    from pyspark.dbutils import DBUtils
ModuleNotFoundError: No module named 'pyspark.dbutils'
</code></pre>

<p>Please find below the relevant code section from run-multiple-notebooks.py</p>

<pre><code>  def get_spark_session():
    from pyspark.sql import SparkSession
    return SparkSession.builder.getOrCreate()

  def get_dbutils(self, spark = None):
    try:
        if spark == None:
            spark = spark

        from pyspark.dbutils import DBUtils #error line
        dbutils = DBUtils(spark) #error line
    except ImportError:
        import IPython
        dbutils = IPython.get_ipython().user_ns[""dbutils""]
    return dbutils

  def submitNotebook(notebook):
    print(""Running notebook %s"" % notebook.path)
    spark = get_spark_session()
    dbutils = get_dbutils(spark)
</code></pre>

<p>I tried all the options and tried </p>

<pre><code>https://stackoverflow.com/questions/61546680/modulenotfounderror-no-module-named-pyspark-dbutils
</code></pre>

<p>as well. It is not working :(</p>

<p>Can someone please suggest if there is fix for the above-mentioned error while running .py in job cluster. My code works fine inside databricks local notebook but running from outside using TravisCI and MLFlow isn't working which is must requirement for pipeline automation.</p>",0,0,2020-05-18 22:15:02.237000 UTC,,2020-05-20 13:47:56.477000 UTC,2,pyspark|travis-ci|databricks|mlflow|dbutils,325,2019-04-15 16:50:36.127000 UTC,2022-02-28 22:02:10.127000 UTC,"Minnesota, USA",332,26,1,83,,,,,,"['databricks', 'mlflow']"
