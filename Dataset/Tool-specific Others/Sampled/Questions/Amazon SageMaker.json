[
    {
        "Question_title":"Does Amazon SageMaker RL support heterogenous clusters?",
        "Question_creation_time":1593179712000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE59_Oro0SGKaOIdZNmySiw\/does-amazon-sage-maker-rl-support-heterogenous-clusters",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":27.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Does SageMaker RL support heterogenous clusters? I'd like to have our training to run on GPU and and SageMaker RL, and our inferences to run on CPUs.",
        "Answers":[
            {
                "Answer_creation_time":"2020-06-28T20:35:03.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yes, Amazon SageMaker RL allows you to define training workers separately from inference workers.\n\nFor more information, see Amazon SageMaker RL \u2013 Managed reinforcement learning with Amazon SageMaker on the AWS News Blog. Also, the Build and Train Reinforcement Models with Amazon SageMaker RL AWS online tech talk (see minute 26).",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SQL Server driver issue on notebook instance running on AWS SageMaker",
        "Question_creation_time":1667244851129,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeC8hq97nSzKHHmMKbCPxww\/sql-server-driver-issue-on-notebook-instance-running-on-aws-sage-maker",
        "Question_topic":[
            "Database",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Microsoft SQL Server",
            "Amazon SageMaker",
            "Database"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":49.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am using aws sagemaker notebook instance to run jupyter notebook. I have a MS SQL Server DB 2019 db that I am trying to connect to from the notebook. Notebook instance is running on Amazon Linux 2, Jupyter Lab 1 platform.\n\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport json\nimport os\n\n\n# sql database\nimport pyodbc\nconnection = pyodbc.connect(\n                              'Driver={SQL Server};'\n                              'Server=sname;'\n                              'Database=dbname;'\n                              'Trusted_Connection=True;'\n                           )\n\ncursor = connection.cursor()\n\n\nI get an error, likely because the driver is not installed on the instance.\n\nError                                     Traceback (most recent call last)\n\/tmp\/ipykernel_20407\/3026941781.py in <cell line: 12>()\n     10 # sql database\n     11 import pyodbc\n---> 12 connection = pyodbc.connect(\n     13                               'Driver={SQL Server};'\n     14                               'Server=sname;'\n\nError: ('01000', \"[01000] [unixODBC][Driver Manager]Can't open lib 'SQL Server' : file not found (0) (SQLDriverConnect)\")\n\n\nhow do I install the driver on sagemaker instance and resolve this issue?",
        "Answers":[
            {
                "Answer_creation_time":"2022-11-01T10:44:32.142Z",
                "Answer_upvote_count":0,
                "Answer_body":"You're correct in assuming that the problem is due to the driver not being installed. Microsoft have recently published a useful guide to installing various versions of the SQL Server drivers on different editions of Linux which should help you to get the driver installed: https:\/\/learn.microsoft.com\/en-us\/sql\/connect\/odbc\/linux-mac\/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-ver16\n\nOnce you've done that, you also need to check that you have connectivity to your SQL Server instance from your EC2 instance. You can check basic network connectivity with a ping or traceroute, but you also need to check that you can establish a connection between the two. Personally, I'd start by installing the command line tools - again, Microsoft publish a good tutorial for installing them at this link: https:\/\/learn.microsoft.com\/en-us\/sql\/linux\/sql-server-linux-setup-tools?view=sql-server-ver16\n\nIf you can't connect to your SQL Server by name, try instead to use the IP address as the server name. This will work if it's a \"base\" instance of SQL Server, I think you can use it with a secondary instance too, by using something like 192.168.0.1\/instancename (subsitute the IP address of the SQL Server).\n\nOnce you've done that, you just need to sort out authentication. You don't say if you've got SQL Server installed on Linux or Windows, but these two articles should help you to fix any LDAP\/Kerberos configuration issues that you might have in getting the two to talk to each other:\n\nAD Authentication with SQL Server & Linux: https:\/\/learn.microsoft.com\/en-us\/sql\/linux\/sql-server-linux-active-directory-authentication?view=sql-server-ver15\n\nThis may also be of use, which describes how to join a SQL Server that's running on a Linux box to AD. Worth a read to understand the mechanism, but I think the previous link is what you need: https:\/\/learn.microsoft.com\/en-us\/sql\/linux\/sql-server-linux-active-directory-join-domain?view=sql-server-ver15\n\nAlternatively, you could (temporarily) bypass that step, which can be tricky to the uninitiated, and simply use SQL Server authentication instead, by creating a SQL Server login\/password and corresponding user in the database you're trying to access. This isn't as secure as using Kerberos authentication, but it may suffice for your needs, or be useful to get you to connect your notebook to SageMaker temporarily to prove basic connectivity.\n\nPlease vote up my answer if it's useful to you, hope that helps. Jon.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker training with FSx: what is \"directory_path\"",
        "Question_creation_time":1604678225000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCaemzfoDRIy9AgLRW8suqw\/sage-maker-training-with-f-sx-what-is-directory-path",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon FSx for Lustre",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":105.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"SageMaker can train on FSx data. One SageMaker SDK parameter for FSx training is directory_path. Where do we find that?",
        "Answers":[
            {
                "Answer_creation_time":"2020-11-06T16:13:42.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"FSx for Lustre is a file system that you can use to provide high performance for ML training workloads. The directory_path should point to the location on your file system where your dataset is stored.\n\nIn the example in the docs: directory_path='\/fsx\/tensorflow',\n\n\/fsx is the directory you define on your compute instances where you are mounting the file system \/tensorflow would represent a folder within the fsx directory\n\nIf you are using an S3-linked FSx for Lustre file system \/tensorflow would be a prefix within your S3-linked bucket.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to pass values for a \"shap_baseline\" if we have categorical values (string values) as features in classsagemaker.clarify.SHAPConfig method.",
        "Question_creation_time":1663938111276,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVdzi-7h5RAapCYJUehzuZw\/how-to-pass-values-for-a-shap-baseline-if-we-have-categorical-values-string-values-as-features-in-classsagemaker-clarify-shap-config-method",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Ground Truth",
            "Amazon SageMaker Clarify",
            "Monitoring"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":41.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"using this documentation i passing a single row as to shap_baseline parameter to implement explainability monitoring , a similar implementation of what is done in in this github repo implementation. if I am passing a single row as input to shap_baseline parameter, the schedule is failing by concatenating 2 rows. If i ignore the shap_baseline (as it is optional), the schedule is taking forever to run. Help of any kind is really appreciated.\n\nthanks for your time and effort :)",
        "Answers":[
            {
                "Answer_creation_time":"2022-10-04T05:28:35.427Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks for your question\n\nThere is some guidance and information about selecting a proper baseline from here that may help: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/clarify-feature-attribute-shap-baselines.html\n\nif I am passing a single row as input to shap_baseline parameter, the schedule is failing by concatenating 2 rows\n\nFor this, you may want to open a ticket via AWS Support in the AWS console so we can take a better look at this internally, and include your processing job details if possible (such as your analysis configurations, Job ARN, etc.)\n\nIf i ignore the shap_baseline (as it is optional), the schedule is taking forever to run\n\nThere is a num_clusters parameter in the SHAPConfig which you can explicitly set to reduce the size of the baseline dataset that is generated. A lower number (low single digits) will generally provide faster runtime",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Train machine learning model using reserved instance",
        "Question_creation_time":1641871148701,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsy3vkTMkSA2ojA1bmafDSA\/train-machine-learning-model-using-reserved-instance",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":151.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi.\n\nIs it possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance every time which is somewhat time consuming? I'm familiar with local mode, but I understand this is not supported when using AWS SageMaker machine learning estimators.\n\nAppreciate any suggestions for how to make the model training process in SageMaker go faster when using AWS SageMaker machine learning estimators.\n\nThanks, Stefan",
        "Answers":[
            {
                "Answer_creation_time":"2022-01-14T08:13:53.228Z",
                "Answer_upvote_count":0,
                "Answer_body":"As of today, it's not possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance. The service team is currently working on it, unfortunately I don't have an ETA as to when the feature will be released.\n\nLocal Mode is supported for frameworks images (TensorFlow, MXNet, Chainer, PyTorch, and Scikit-Learn) and images you supply yourself.\n\nUsing the SageMaker Python SDK \u2014 sagemaker 2.72.3 documentation\n\nIf you want to train Built-in algorithm models simply faster, you should check the recommendation in the SageMaker document.\n\nExample Blazingtext-instances, Deepar-instances\n\nIf the algorithm supports it, one can also try using Pipe mode or FastFile mode. These offer some fast training job startup time. Accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cant generate XGBoost training report in sagemaker, only profiler_report.",
        "Question_creation_time":1657222531986,
        "Question_link":"https:\/\/repost.aws\/questions\/QUskI-0YIvQ_2GRSkzyGiD2A\/cant-generate-xg-boost-training-report-in-sagemaker-only-profiler-report",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon S3 Glacier",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Model Training"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":57.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to generate the XGBoost training report to see feature importances however the following code only generates the profiler report.\n\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np\nimport pandas as pd\nfrom sagemaker.predictor import csv_serializer\nfrom sagemaker.debugger import Rule, rule_configs\n\n# Define IAM role\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\nrole = get_execution_role()\nprefix = 'sagemaker\/models'\nmy_region = boto3.session.Session().region_name \n\n# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\nxgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", my_region, \"latest\")\n\n\n\nbucket_name = 'binary-base' \ns3 = boto3.resource('s3')\ntry:\n    if  my_region == 'us-east-1':\n      s3.create_bucket(Bucket=bucket_name)\n    else: \n      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n    print('S3 bucket created successfully')\nexcept Exception as e:\n    print('S3 error: ',e)\n\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train\/train.csv')).upload_file('..\/Data\/Base_Model_Data_No_Labels\/train.csv')\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'validation\/val.csv')).upload_file('..\/Data\/Base_Model_Data_No_Labels\/val.csv')\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test\/test.csv')).upload_file('..\/Data\/Base_Model_Data\/test.csv'\n\n\nsess = sagemaker.Session()\nxgb = sagemaker.estimator.Estimator(xgboost_container,\n                                    role, \n                                    volume_size =5,\n                                    instance_count=1, \n                                    instance_type='ml.m4.xlarge',\n                                    output_path='s3:\/\/{}\/{}\/output'.format(bucket_name, prefix, 'xgboost_model'),\n                                    sagemaker_session=sess, \n                                    rules=rules)\n\nxgb.set_hyperparameters(objective='binary:logistic',\n                        num_round=100, \n                        scale_pos_weight=8.5)\n\nxgb.fit({'train': s3_input_train, \"validation\": s3_input_val}, wait=True)\n\n\nWhen Checking the output path via:\n\nrule_output_path = xgb.output_path + \"\/\" + xgb.latest_training_job.job_name + \"\/rule-output\"\n! aws s3 ls {rule_output_path} --recursive\n\n\nWhich Outputs:\n\n2022-07-07 18:40:27     329715 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-report.html\n2022-07-07 18:40:26     171087 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-report.ipynb\n2022-07-07 18:40:23        191 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/BatchSize.json\n2022-07-07 18:40:23        199 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/CPUBottleneck.json\n2022-07-07 18:40:23        126 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/Dataloader.json\n2022-07-07 18:40:23        127 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/GPUMemoryIncrease.json\n2022-07-07 18:40:23        198 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/IOBottleneck.json\n2022-07-07 18:40:23        119 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/LoadBalancing.json\n2022-07-07 18:40:23        151 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/LowGPUUtilization.json\n2022-07-07 18:40:23        179 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/MaxInitializationTime.json\n2022-07-07 18:40:23        133 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/OverallFrameworkMetrics.json\n2022-07-07 18:40:23        465 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/OverallSystemUsage.json\n2022-07-07 18:40:23        156 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/StepOutlier.json\n\n\nAs you can see only the profiler report in created which does not interest me. Why isn't there a CreateXGBoostReport folder generated with the training report? How do I generate this\/what am I missing?",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"how to increase the storage of host instance",
        "Question_creation_time":1576741179000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeVh4VvD6R-eIhRYY6K8dsw\/how-to-increase-the-storage-of-host-instance",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":308.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"The parameters of a big neural network model can be huge. But the largest storage size of a host instance is only 30G, according to https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/host-instance-storage.html. Is there a way to increase the storage volume? I have a model (embeddings) that is very close to 30G and caused a no space error when deploying.\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_time":"2020-01-24T05:55:47.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"The disk size is currently not configurable for SageMaker Endpoints with EBS backed volumes. As a workaround, please use instances with ephemeral storage for your SageMaker endpoint.\n\nExample instance types with ephemeral storage:\n\nm5d instances: https:\/\/aws.amazon.com\/ec2\/instance-types\/m5\/\nc5d instances: https:\/\/aws.amazon.com\/ec2\/instance-types\/c5\/\nr5d instances: https:\/\/aws.amazon.com\/ec2\/instance-types\/r5\/\n\nThe full list of Amazon SageMaker instance types can be accessed here: https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2019-12-30T23:03:17.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks! Using a x5d instance solves the issue.\n\nAnd a quick note: even though I could download the big model to endpoint now, I got timeout error when loading the model in the endpoint. After some trial and error, I solved it by increasing the timeout value and reducing the number of worker by setting the environment variables. To do it, pass this dict\n\nenv={\"SAGEMAKER_MODEL_SERVER_WORKERS\":\"1\",\n\"SAGEMAKER_MODEL_SERVER_TIMEOUT\":\"1800\"}\n\nwhen creating the model object.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Ground Truth - Progress Not Updating?",
        "Question_creation_time":1550529412000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUf9EnD7gzRlu86yy1Zlry7g\/ground-truth-progress-not-updating",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":1.0,
        "Question_view_count":117.0,
        "Question_answer_count":9,
        "Question_has_accepted_answer":false,
        "Question_body":"I've just configured my first Ground Truth labelling job, 1737 images assigned to a private workforce.\n\nWhen I first logged in as a worker I was able to label 10 images as a test run, and the Ground Truth \"Labeling job summary\" in the AWS Console shows 10\/1737 images labelled. So far so good.\n\nI then returned to the labelling job and worked through approx 50 images and then used the 'Stop working' button to finish my labelling session.\n\nDespite that work, the progress shown in \"Labelling job summary\" has not been updated with those additional labeled images, even 48+ hours after the work was done.\n\nHow can I confirm whether the image labelling is happening correctly? I don't see any error messages that might help debug the situation, and I certainly don't want to get other workers to label 1700+ images without being sure that the labelling data is being saved.",
        "Answers":[
            {
                "Answer_creation_time":"2019-02-20T21:01:35.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"To update my own question:\n\nI concluded that the job had somehow become corrupted, so I stopped it with the intention of starting a new one and seeing if the issue went away.\n\nDuring the \"stopping\" process the progress counter updated to show that the images I'd labelled had actually been saved, but by then it was too late and the job was queued for termination.\n\nI started a new job with the same settings (rebuilt the image manifest from scratch), and had the same issue with the initial 10 images being labelled and updated in the summary progress counter, but the images labelled in subsequent launches do not update the progress counter so there's no way of knowing how far long we are in the process and\/or checking the labelled areas of any images already labelled.\n\nIs this expected behaviour, or is this a bug?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-02-19T04:12:21.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Bump\n\nCan anyone using Ground Truth for labelling shed any light on whether this lack of status update is normal?\n\nHow do we keep track of progress on a job?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-06-28T16:02:52.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"BUMP AGAIN - I'll answer my own question incase anyone has the same issue.\n\nIt seems that Ground Truth just takes a very long time to update the progress of any labelling tasks. A job we created on Feb 23 has just shown progress on the Ground Truth console, so it seems to take about 5 days for any labelling work to be reflected in the console.\n\nI have no idea why this is the case, and I've had no response from AWS to my query, but at least we can be confident that our work is being saved and the labels are being generated.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-08T13:38:56.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi jamesatfish,\n\nI'm an AWS engineer working on SageMaker Ground Truth. I appreciate that you've been stuck with this for over a week, and I want to personally help make it better. It shouldn't take 5 days for a labeling task to be reflected in the console. We'd very much like to understand how you set up your labeling job so we can investigate it on our end. I sent you a private message on February 21st, but I'm replying here in the main thread in the hopes that you'll see this. Please feel free to send me a private message at your earliest convenience so we can discuss the particulars of your labeling job.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-08T22:42:06.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks Jonathan!\n\nApologies for not seeing your reply or PM earlier, I don't seem to have received a notification email to let me know of the reply.\n\nI have replied to your PM this morning with the ARN details, let me know if you need further background.\n\nAs an update to the thread, we spent a week labelling images in our most recently created task on the assumption that the updates to the task progress were just delayed. Unfortunately that was not the case, and we discovered this morning that Ground Truth has automatically cancelled our labelling job because \"no progress had been made\", causing us to lose most of the labelled images we had processed..",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-03-01T00:12:48.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi jamesatfish,\n\nI'm currently setting up a labeling job in Ground Truth and was hoping to avoid any pitfalls you came across through this process. Do you mind sharing what ended up being the resolutions to the issues you faced?\n\nThank you!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-03-01T21:28:59.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Naquent,\n\nHere's a quick summary of what we learned:\n\nGround Truth jobs are split into image batches. The first batch size in any job is 10 images, but after that the default for each batch is 1000 images. You need to get through a batch in entirety before Ground Truth will prepare the next batch.\n\nBy default Ground Truth gives you 4 days to complete each batch. If you fail to complete the batch within 4 days the entire Ground Truth job will be failed and any progress lost.\n\nYou cannot control those 2 variables via the AWS Admin Console, but if you create the Ground Truth job via the CLI you can control both, up to a maximum of 10 days for each batch to be valid. You can only set these at the creation of the job, not once it has been created.\n\nUsing the above, you'll need to find a suitable batch size that's a compromise between ensuring you get the batch completed and not being delayed at the end of each batch. If you're working on the labelling tasks as a full time endeavour or with a dedicated team then set a large batch size so the team can label as many images as possible before waiting for the next batch to generate. If you're labelling on an ad-hoc basis then set a smaller batch size to ensure you get through the batch before the timeout.\n\nAlways set the maximum 10 day timeout for each batch, there appears to be no penalty for doing so.\n\nAWS have changed the Ground Truth console output to more frequently update the progress of your labelling so you can see how many images have been labelled, but there's no way to see where you are up to in a batch or how long you have left to complete the current batch. If you've got a large labelling job you'll want to keep track of that yourself somehow.\n\nI hope that helps!\n\nJames\n\nEdited by: jamesatfish on Apr 9, 2019 8:42 AM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-03-07T21:47:02.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks so much for sharing, James! This is really good information to have.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-09T12:55:52.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I am facing the same issue. I created the labeling job and after submitting almost 24+ hours of tagged images the status did not change. then when I stooped the labeling job it updated the progress. How do I resolve this?",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS Ground Truth Plus Available Medical Expertise",
        "Question_creation_time":1668530819224,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1vINii7DRkGK-4klJOq7wA\/aws-ground-truth-plus-available-medical-expertise",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth Plus"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":28.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"On Dec 1, 2021, AWS put out a press release regarding SageMaker Ground Truth Plus that contained the statement:\n\nTo get started, customers simply point Amazon SageMaker Ground Truth Plus to their data source in Amazon Simple Storage Service (Amazon S3) and provide their specific labeling requirements (e.g. instructions for how medical experts should label anomalies in radiology images of lungs).\n\nCan AWS provide medical experts for labeling medical data? Or am I misinterpreting this statement and the services included in this \"turnkey\" solution. (BTW, I've already built and tested a custom segmentation task for SageMaker Ground Truth and am looking for \"expert\" labeling.)",
        "Answers":[
            {
                "Answer_creation_time":"2022-11-15T19:01:33.076Z",
                "Answer_upvote_count":0,
                "Answer_body":"That's a poor choice of example on their part; the FAQ for the service explicitly states: \"Currently, Amazon SageMaker Ground Truth Plus is not a HIPAA eligible service.\"",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Received server error (0) from model when hosting",
        "Question_creation_time":1562557558000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUut1Vw-EPQUWVOy6cxmOmXw\/received-server-error-0-from-model-when-hosting",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":440.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\n\nI was trying to deploy my trained model to the endpoint and I was given a ModelError.\n\"ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from model with message \"Your invocation timed out while waiting for a response from container model. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See Link:https:\/\/ap-southeast-2.console.aws.amazon.com\/cloudwatch\/home?region=ap-southeast-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/ss-notebook-treemapping-2019-07-08-00-56-39-825 in account 125017970330 for more information.\"\n\nI'm not sure what caused this issue and couldn't figure out how latency metrics in the CloudWatch would be useful in this case. Does anyone know what the approach is to solve this issue? It would also be great to know why this happens. Thanks in advance for any help!",
        "Answers":[
            {
                "Answer_creation_time":"2019-07-10T20:17:00.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"ModelLatency is helpful because Sagemaker requires the container to respond within 60 seconds [1]: if you see the ModelLatency at or above 60 seconds that confirms the container isn't responding fast enough. At that point you'll need to debug why your container isn't fast enough: if it's a custom container you wrote you'll need to debug it; if it's a built-in container you should reach out to AWS support for assistance.\n\n[1] Timeout documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-code-container-response",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to use an Augmented Manifest File for AWS SageMaker Ground Truth?",
        "Question_creation_time":1546562747000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1LLbT-AYQDO-XXrjPUFl9w\/how-to-use-an-augmented-manifest-file-for-aws-sage-maker-ground-truth",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":98.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hey,\n\nI'm trying to use Ground Truth to do image classification but with a different set of label options for each image. I have the custom labeling task template and pre-\/post-labeling Lambda functions set up and I figured I could pass in the labels through the manifest file.\n\nMy issue is that the Ground Truth job ignores the attributes in the manifest file that are not \"source-ref\" (or \"source\"). This causes the pre-processing Lambda function to fail because the request it is passed only contains the \"source-ref\" attribute, but the Lambda function also references a different attribute. Are augmented manifest files supported for Ground Truth and if they are, how can I make use of the extra attributes?\n\nReferences:\nGround Truth Input Data: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html\nSageMaker Augmented Manifest Files: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/augmented-manifest.html\n\nExample:\n\nA normal Ground Truth manifest file:\n\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img1.png\"}\r\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img2.png\"}\r\n...\n\n\nWhat I want to be able to use:\n\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img1.png\",\"labels\":[\"pen\",\"pencil\",\"stick\"]}\r\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img2.png\",\"labels\":[\"tv\",\"laptop\",\"phone\"]}\r\n...",
        "Answers":[
            {
                "Answer_creation_time":"2019-01-07T19:20:40.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi sageuser, I'm an engineer at AWS. Augmented manifests are not supported for custom workflows, and so it is not possible to pass through additional parameters, e.g., \"labels\" in your example. We appreciate that you are using the service and welcome customer feedback. We can always be reached at https:\/\/aws.amazon.com\/contact-us\/.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using AWS services to perform pet face recognition",
        "Question_creation_time":1601645649000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2S-z85hhSr-XVecDxE3ihw\/using-aws-services-to-perform-pet-face-recognition",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS DeepLens",
            "Amazon SageMaker",
            "Amazon Rekognition"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":132.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I want to build a camera that automatically recognizes a specific pet with image or video recognition. What AWS service can I use to identify an individual pet (not just the pet type). I've tried to use AWS Rekognition, but it can only differentiate between animal types, race, or color. Amazon SageMaker could be another option to create a completely new mode, but is very costly. What other AWS services can I use to identify specific pets?",
        "Answers":[
            {
                "Answer_creation_time":"2020-10-02T14:29:04.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can use Amazon Rekognition Custom Labels to use single class object detection to identify or classify a specific animal. However, note that Amazon Rekognition Custom Labels do not perform animal face recognition. It only classifies the image or object.\n\nFor example, you can train your detection model to identify an animal based on the images you provide for that label. For more information about using Amazon Rekognition Custom Labels, see this blog: https:\/\/aws.amazon.com\/blogs\/machine-learning\/training-a-custom-single-class-object-detection-model-with-amazon-rekognition-custom-labels\/.\n\nTo use frames from a video with Custom Labels, see Video analysis .",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to connect a Sagemaker Notebook to Glue Catalog",
        "Question_creation_time":1594231696000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUoiI3L85FT6OmPewooCH4lQ\/how-to-connect-a-sagemaker-notebook-to-glue-catalog",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Glue"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":519.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"A customer wants to connect a Sagemaker notebook to Glue Catalog, but is not allowed to use developer endpoints because of security constraints.\n\nI can't seem to find documentation on the Glue Catalog API that would allow this, or examples of how this might be done. Any links or pointers would be greatly appreciated.",
        "Answers":[
            {
                "Answer_creation_time":"2020-07-08T18:19:07.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"So there is the catalog API which allows you to describe databases, tables, etc. Documentation regarding the calls and data structures can be found here:\n\nhttps:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/aws-glue-api-catalog-tables.html\n\nBoto3 for get_table\n\nhttps:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/glue.html#Glue.Client.get_table\n\nIf they have a restrictive security posture (as suggested by the avoidance of Dev Endpoints) you may also suggest a Glue VPC-E: https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/vpce-interface.html\n\nI would ask what are they accessing the catalog for, as the Dev Endpoint isn't entirely about the Glue Catalog, but about the compute resources andSparkMagic.\n\nAlso, think about steering them towards AWS Data Wrangler for interacting with Glue Catalog if they are using Pandas. Helpful snippets can be found here:\n\nhttps:\/\/github.com\/awslabs\/aws-data-wrangler\/blob\/master\/tutorials\/005%20-%20Glue%20Catalog.ipynb",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is any component of Sagemaker still involved in production inferencing",
        "Question_creation_time":1668489528537,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVpIbYCjMTzOPTHJFfh9rnQ\/is-any-component-of-sagemaker-still-involved-in-production-inferencing",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Test & Optimize ML Models"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":36.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Are any components of the SageMaker Engine involved still during production ( inference) , I had a discussion with someone saying Sagemaker has nothing to do with production anymore because everything is deployed , containerized and executed somewhere in a VM or cluster., I think there are some instances the main engine is working in production and please correct me if i am wrong 1. Billing records - have to be generated -so something is connected to the engine ? does the model help here or is it just pure EC2 etc compute measured ? Who manages the endpoints ( server less, and asynch) ? if we need to use Sage maker monitor this is also alive during production. Question is What main components are active in production outside model or does mode communicate to engine still? As an addendum someone mentioned to me if it is deployed to EC2 or K8s yes it is disconnected from the main services in production you CANNOT use Model monitor in this scenario? --- but if you deploy to SAGEMAKER INFERENCE? you can use model monitor in prod. You deploy to an endpoint for real time... What is SageMAker Inference - as a concept or as some physical thing because there are 71 options in Sagemaker inference ?",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Notebook - SSL failed validation when Boto3 session.client(verify=False)",
        "Question_creation_time":1651530941407,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmNuNjDSJSSyOUJ8dxVP-hg\/sagemaker-notebook-ssl-failed-validation-when-boto-3-session-client-verify-false",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":456.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"In a sagemaker notebook with an associated git repository, when I try to create a boto3 session client using verify = False, I get the following : SSLError: SSL validation failed for {service_name } [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)\n\nThe error resolves if I allow the default value of verify = None (meaning SSL certs are verified). My problem is that this session is created as part of a function call on the associated git repository and I don't want to change the behavior in the repo. I don't understand why this error occurs only when I specify not to validate SSL certificates. Any ideas of what explains this behavior?",
        "Answers":[
            {
                "Answer_creation_time":"2022-05-05T18:39:40.758Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nThank you for using AWS SageMaker.\n\nWhen running the below command, it got executed without any error. More over this issue seems to look like a configuration issue, I'd recommend you to reach out to boto3 team via Github or opening a case with Premium Support team to identify if calling the session from Github is possible or not.\n\nimport boto3\nimport boto3.session\n\n# Create your own session\nmy_session = boto3.session.Session()\n\n# Now we can create low-level clients or resource clients from our custom session\ns3 = my_session.client('s3',verify=False)\n\n\n\nOpen a support case with AWS using the link:\n\nhttps:\/\/console.aws.amazon.com\/support\/home?#\/case\/create",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker - All metrics in statistics.json by Model Quality Monitor are \"0.0 +\/- 0.0\", but confusion matrix is built correctly for multi-class classification!!",
        "Question_creation_time":1645965956086,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOOz6SJnzR7-VDglJ1rAW8Q\/sage-maker-all-metrics-in-statistics-json-by-model-quality-monitor-are-0-0-0-0-but-confusion-matrix-is-built-correctly-for-multi-class-classification",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Monitoring"
        ],
        "Question_upvote_count":1.0,
        "Question_view_count":27.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I have scheduled an hourly model-quality-monitoring job in AWS SageMaker. both the jobs, ground-truth-merge and model-quality-monitoring completes successfully without any errors. but, all the metrics calculated by the job are \"0.0 +\/- 0.0\" while the confustion matrix gets calculated as expected.\n\nI have done everything as mentioned in this notebook for model-quality-monitoring from sagemaker-examples with very few changes and they are:\n\nI have changed the model from xgboost churn to model trained on my data.\nmy input to the endpoint was csv like in the example-notebook, but output was json.\ni have changed the problem-type from BinaryClassfication to MulticlassClassification wherever necessary.\n\nconfustion matrix was built successfully, but all metrics are 0 for some reason. So, I would like the monitoring job to calculate the multi-classification metrics on data properly.\n\nAll Logs\n\nHere's the statistics.json file that model-quality-monitor saved to S3 with confustion matrix built, but with 0s in all the metrics:\n\n{\n  \"version\" : 0.0,\n  \"dataset\" : {\n    \"item_count\" : 4432,\n    \"start_time\" : \"2022-02-23T03:00:00Z\",\n    \"end_time\" : \"2022-02-23T04:00:00Z\",\n    \"evaluation_time\" : \"2022-02-23T04:13:20.193Z\"\n  },\n  \"multiclass_classification_metrics\" : {\n    \"confusion_matrix\" : {\n      \"0\" : {\n        \"0\" : 709,\n        \"2\" : 530,\n        \"1\" : 247\n      },\n      \"2\" : {\n        \"0\" : 718,\n        \"2\" : 497,\n        \"1\" : 265\n      },\n      \"1\" : {\n        \"0\" : 700,\n        \"2\" : 509,\n        \"1\" : 257\n      }\n    },\n    \"accuracy\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"weighted_recall\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"weighted_precision\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"weighted_f0_5\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"weighted_f1\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"weighted_f2\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"accuracy_best_constant_classifier\" : {\n      \"value\" : 0.3352888086642599,\n      \"standard_deviation\" : 0.003252410977346705\n    },\n    \"weighted_recall_best_constant_classifier\" : {\n      \"value\" : 0.3352888086642599,\n      \"standard_deviation\" : 0.003252410977346705\n    },\n    \"weighted_precision_best_constant_classifier\" : {\n      \"value\" : 0.1124185852154987,\n      \"standard_deviation\" : 0.0021869336610830254\n    },\n    \"weighted_f0_5_best_constant_classifier\" : {\n      \"value\" : 0.12965524348784485,\n      \"standard_deviation\" : 0.0024239410000317335\n    },\n    \"weighted_f1_best_constant_classifier\" : {\n      \"value\" : 0.16838092925822584,\n      \"standard_deviation\" : 0.0028615098045768348\n    },\n    \"weighted_f2_best_constant_classifier\" : {\n      \"value\" : 0.24009212108475822,\n      \"standard_deviation\" : 0.003326031863819311\n    }\n  }\n}\n\n\nHere's how couple of lines of captured data looks like(prettified for readability, but each line has no tab spaces as shown below) :\n\n{\n    \"captureData\": {\n        \"endpointInput\": {\n            \"observedContentType\": \"text\/csv\",\n            \"mode\": \"INPUT\",\n            \"data\": \"0,1,628,210,30\",\n            \"encoding\": \"CSV\"\n        },\n        \"endpointOutput\": {\n            \"observedContentType\": \"application\/json\",\n            \"mode\": \"OUTPUT\",\n            \"data\": \"{\\\"label\\\":\\\"Transfer\\\",\\\"prediction\\\":2,\\\"probabilities\\\":[0.228256680901919,0.0,0.7717433190980809]}\\n\",\n            \"encoding\": \"JSON\"\n        }\n    },\n    \"eventMetadata\": {\n        \"eventId\": \"a7cfba60-39ee-4796-bd85-343dcadef024\",\n        \"inferenceId\": \"5875\",\n        \"inferenceTime\": \"2022-02-23T04:12:51Z\"\n    },\n    \"eventVersion\": \"0\"\n}\n{\n    \"captureData\": {\n        \"endpointInput\": {\n            \"observedContentType\": \"text\/csv\",\n            \"mode\": \"INPUT\",\n            \"data\": \"0,3,628,286,240\",\n            \"encoding\": \"CSV\"\n        },\n        \"endpointOutput\": {\n            \"observedContentType\": \"application\/json\",\n            \"mode\": \"OUTPUT\",\n            \"data\": \"{\\\"label\\\":\\\"Adoption\\\",\\\"prediction\\\":0,\\\"probabilities\\\":[0.99,0.005,0.005]}\\n\",\n            \"encoding\": \"JSON\"\n        }\n    },\n    \"eventMetadata\": {\n        \"eventId\": \"7391ac1e-6d27-4f84-a9ad-9fbd6130498a\",\n        \"inferenceId\": \"5876\",\n        \"inferenceTime\": \"2022-02-23T04:12:51Z\"\n    },\n    \"eventVersion\": \"0\"\n}\n\n\nHere's couple of lines from my ground-truths that I have uploaded to S3 look like(prettified for readability, but each line has no tab spaces as shown below):\n\n{\n  \"groundTruthData\": {\n    \"data\": \"0\",\n    \"encoding\": \"CSV\"\n  },\n  \"eventMetadata\": {\n    \"eventId\": \"1\"\n  },\n  \"eventVersion\": \"0\"\n}\n{\n  \"groundTruthData\": {\n    \"data\": \"1\",\n    \"encoding\": \"CSV\"\n  },\n  \"eventMetadata\": {\n    \"eventId\": \"2\"\n  },\n  \"eventVersion\": \"0\"\n},\n\n\nHere's couple of lines from the ground-truth-merged file look like(prettified for readability, but each line has no tab spaces as shown below). this file is created by the ground-truth-merge job, which is one of the two jobs that model-quality-monitoring schedule runs:\n\n{\n  \"eventVersion\": \"0\",\n  \"groundTruthData\": {\n    \"data\": \"2\",\n    \"encoding\": \"CSV\"\n  },\n  \"captureData\": {\n    \"endpointInput\": {\n      \"data\": \"1,2,1050,37,1095\",\n      \"encoding\": \"CSV\",\n      \"mode\": \"INPUT\",\n      \"observedContentType\": \"text\/csv\"\n    },\n    \"endpointOutput\": {\n      \"data\": \"{\\\"label\\\":\\\"Return_to_owner\\\",\\\"prediction\\\":1,\\\"probabilities\\\":[0.14512373737373732,0.6597074314574313,0.1951688311688311]}\\n\",\n      \"encoding\": \"JSON\",\n      \"mode\": \"OUTPUT\",\n      \"observedContentType\": \"application\/json\"\n    }\n  },\n  \"eventMetadata\": {\n    \"eventId\": \"c9e21f63-05f0-4dec-8f95-b8a1fa3483c1\",\n    \"inferenceId\": \"4432\",\n    \"inferenceTime\": \"2022-02-23T04:00:00Z\"\n  }\n}\n{\n    \"eventVersion\": \"0\",\n    \"groundTruthData\": {\n        \"data\": \"1\",\n        \"encoding\": \"CSV\"\n    },\n    \"captureData\": {\n        \"endpointInput\": {\n            \"data\": \"0,2,628,5,90\",\n            \"encoding\": \"CSV\",\n            \"mode\": \"INPUT\",\n            \"observedContentType\": \"text\/csv\"\n        },\n        \"endpointOutput\": {\n            \"data\": \"{\\\"label\\\":\\\"Adoption\\\",\\\"prediction\\\":0,\\\"probabilities\\\":[0.7029623691085284,0.0,0.29703763089147156]}\\n\",\n            \"encoding\": \"JSON\",\n            \"mode\": \"OUTPUT\",\n            \"observedContentType\": \"application\/json\"\n        }\n    },\n    \"eventMetadata\": {\n        \"eventId\": \"5f1afc30-2ffd-42cf-8f4b-df97f1c86cb1\",\n        \"inferenceId\": \"4433\",\n        \"inferenceTime\": \"2022-02-23T04:00:01Z\"\n    }\n}\n\n\nSince, the confusion matrix was constructed properly, I presume that I fed the data to sagemaker-model-monitor the right-way. But, why are all the metrics 0.0, while confustion-matrix looks as expected?\n\nEDIT 1:\nLogs for the job are available here.",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"how to configure ideal value for MaxConcurrentTransforms in setting up a sagemaker batch transform ?",
        "Question_creation_time":1649205572690,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKcUBF0wPQSyerTP2IK53hQ\/how-to-configure-ideal-value-for-max-concurrent-transforms-in-setting-up-a-sagemaker-batch-transform",
        "Question_topic":[
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "High Performance Compute",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":152.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"based on the documentation , https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html, it states that \" The ideal value for MaxConcurrentTransforms is equal to the number of compute workers in the batch transform job.\" how to figure out what the number of compute workers is , i assume this depends on the instance type. also what about the instance count parameter we can set , do we have to take that into account as well?",
        "Answers":[
            {
                "Answer_creation_time":"2022-04-06T12:08:37.948Z",
                "Answer_upvote_count":0,
                "Answer_body":"The ideal value for MaxConcurrentTransforms varies based on instance type as well as based on your specific model.\n\nit could make sense to increase MaxConcurrentTransforms up to the core count of the instance you are using (for cpu based transform), however, you should also take into account the memory utilisation by your model.\n\nThe ultimate answer is it \"depends\" and I would recommend that you experiment with increasing this number gradually from 1 up to instance core count, while monitoring RAM\/cpu utilisation to find the optimal.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Custom packages in Sagemaker studio",
        "Question_creation_time":1592469976000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZZFjMw_gS5Cz8sh-TK4J3w\/custom-packages-in-sagemaker-studio",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":266.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi everyone,\n\nhow can i install custom OS libraries on Sagemaker studio? When I open a terminal it states:\n\nroot@0f04278e59cf:~\/# yum install unzip\n\nbash: yum: command not found\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_time":"2020-06-18T09:18:20.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Short answer: [Studio UI] > File > New > Terminal > sudo yum install unzip\nThen unzip away...\n\nLong answer:\nYou can open a terminal in two different types of compute environment:\n\nOn a specific kernel you're running: [Studio UI] > kernal tab > Terminial icon.\nOn the compute studio (jupyter) itself: [Studio UI] > File > New > Terminal\n\nIn both options your personal files folder is accessible. In a kernel terminal: \/root. In a Jupyter terminal: \/home\/sagemaker-user.\nWhen opening a kernel terminal you'll have access to the software that is part of the kernel's container (say tensorflow container). Which in your case is missing yum. You can of course try apt-get, and such to install more tools.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"tensorboard with custom docker image without notebook",
        "Question_creation_time":1588924703000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdIJGIVDEQSm-9eX3jo0ubA\/tensorboard-with-custom-docker-image-without-notebook",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":61.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\nIs it possible to use tensorboard with a custom docker image without using a notebook ? Is there any other method to monitor the training process ? I'm using the tensorflow object detection API and currently exposing metrics (only loss) from cloudwatch using a regex but I'd like a more detailed way like tensorboar.. is that possible ??\nThanks",
        "Answers":[
            {
                "Answer_creation_time":"2020-05-11T21:19:16.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yes, it is possible to use tensorboard outside of the SageMaker notebooks.\nHere is an example https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/keras_script_mode_pipe_mode_horovod\/tensorflow_keras_CIFAR10.ipynb that uses TensorBoard to compare the training jobs.\nAll of it can be run locally.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-25T10:26:28.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"For anyone interested I made a tutorial for this:\n\nhttps:\/\/github.com\/roccopietrini\/TFSagemakerDetection\n\nEdited by: rokk07 on Jan 25, 2021 2:28 AM",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deploy SageMaker model to IoT Greengrass in different account?",
        "Question_creation_time":1556295446000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJha7KbxOTXuhMRGbMYGC0g\/deploy-sage-maker-model-to-io-t-greengrass-in-different-account",
        "Question_topic":[
            "Internet of Things (IoT)",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS IoT Greengrass",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":38.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Is it possible to deploy a model created by SageMaker in one account to an IoT Greengrass device in a different account?",
        "Answers":[
            {
                "Answer_creation_time":"2019-04-29T09:20:54.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"For IoT Greengrass 1.x, this is possible but not trivial. From the console this is not possible, as you can only select buckets or SageMaker jobs from the same account, but you can refer to resources in other accounts if you use the CLI or the API.\n\nYou have to create a new Resource Definition Version with the correct data specifying the model resource and then add it to your group definition. For permissions in the source account, you must set up the S3 bucket policy to allow access from the destination account. For permissions in the destination account, you must update the IoT Greengrass service role policy to access the model resource in the source account.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"is it possible to create steps within the sagemaker pipeline via cloudformation?",
        "Question_creation_time":1663606095949,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3aphi0KgRNyk6AcB9c9Spg\/is-it-possible-to-create-steps-within-the-sagemaker-pipeline-via-cloudformation",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS CloudFormation",
            "Machine Learning & AI",
            "Amazon SageMaker Studio Lab",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":38.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am experimenting with sagemaker studio, while i was able to create the sagemaker studio domain and user profiles via cloudformation. I was wondering if, it was possible to create sagemaker projects and other resources like model registry group and steps for preprocessing , training ... via cloudformation? if is it possible , are there any samples, examples around this, if it is not supported or is not possible via cfn , may be help me on how can create link this to existing user and domain in studio .",
        "Answers":[
            {
                "Answer_creation_time":"2022-09-20T08:32:00.559Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi there,\n\nAll the resources for the CloudFormation Sagemaker crossover can be found here\n\nHope this helps to inform you more\n\nRegards NN",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"can a sagemaker endpoint be made public?",
        "Question_creation_time":1664239929738,
        "Question_link":"https:\/\/repost.aws\/questions\/QU96EIw32SSxmx3plPtUUcYA\/can-a-sagemaker-endpoint-be-made-public",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":1.0,
        "Question_view_count":40.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"is there a way to make a sagemaker endpoint be accessible publicly ?",
        "Answers":[
            {
                "Answer_creation_time":"2022-09-27T09:46:53.193Z",
                "Answer_upvote_count":3,
                "Answer_body":"I believe the way to make a sagemaker inference endpoint public is to use api-gw infront of it. Check out this solution https:\/\/docs.aws.amazon.com\/solutions\/latest\/constructs\/aws-apigateway-sagemakerendpoint.html",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-27T02:32:12.501Z",
                "Answer_upvote_count":2,
                "Answer_body":"Hello. In order to make an Amazon SageMaker Real-Time Endpoint public, you can create and manage APIs through an API Gateway. This is an official blog that is showing you a possible solution:\n\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What value should I set for directory_path for the Amazon SageMaker SDK with FSx as data source?",
        "Question_creation_time":1605283057000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHaScKqcfRu-aZ1Cwza63NQ\/what-value-should-i-set-for-directory-path-for-the-amazon-sage-maker-sdk-with-f-sx-as-data-source",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon FSx for Lustre",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":1.0,
        "Question_view_count":120.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"What value should I set for the directory_path parameter in FileSystemInput for the Amazon SageMaker SDK?\n\nHere is some information about my Amazon FSx for Lustre file system:\n\nMy FSx ID is fs-0684xxxxxxxxxxx.\nMy FSx has the mount name lhskdbmv.\nThe FSx maps to an Amazon S3 bucket with files (without extra prefixes in their keys)\n\nMy attempts to describe the job and the results are the following:\n\nAttempt 1:\n\nfs = FileSystemInput(\n    file_system_id='fs-0684xxxxxxxxxxx',\n    file_system_type='FSxLustre',\n    directory_path='lhskdbmv',\n    file_system_access_mode='ro')\n\n\nResult:\n\nestimator.fit(fs) returns ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: FileSystem DirectoryPath 'lhskdbmv' for channel 'training' is not absolute or normalized. Please ensure you don't have a trailing \"\/\", and\/or \"..\", \".\", \"\/\/\" in the path.\n\nAttempt 2:\n\nfs = FileSystemInput(\n    file_system_id='fs-0684xxxxxxxxxxx',\n    file_system_type='FSxLustre',\n    directory_path='\/',\n    file_system_access_mode='ro')\n\n\nResult:\n\nClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: The directory path for FSx Lustre file system fs-068406952bf758bac is invalid. The directory path must begin with mount name of the file system.\n\nAttempt 3:\n\nfs = FileSystemInput(\n    file_system_id='fs-0684xxxxxxxxxxx',\n    file_system_type='FSxLustre',\n    directory_path='fsx',\n    file_system_access_mode='ro')\n\n\nResult:\n\nClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: FileSystem DirectoryPath 'fsx' for channel 'training' is not absolute or normalized. Please ensure you don't have a trailing \"\/\", and\/or \"..\", \".\", \"\/\/\" in the path.",
        "Answers":[
            {
                "Answer_creation_time":"2020-11-13T19:41:48.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"The directory_path parameter must point to \/mountname\/path\/to\/specific\/folder\/in-file-system. The value of mountname is returned in the CreateFileSystem API operation response. It is also returned in the response of the describe-file-systems AWS Command Line Interface (AWS CLI) command and the DescribeFileSystems API operation.\n\nFor your use case, the response might look similar to the following: mountName = lhskdbmv",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"No such file or directory: '\/opt\/ml\/input\/data\/test\/revenue_train.csv' Sagemaker [SM_CHANNEL_TRAIN]",
        "Question_creation_time":1661681019239,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBY-fIUMuRDqwBmObCn8GqQ\/no-such-file-or-directory-opt-ml-input-data-test-revenue-train-csv-sagemaker-sm-channel-train",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Model Training",
            "Amazon SageMaker Model Building",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":54.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to deploy my RandomForestClassifier on Amazon Sagemaker using Python SDK. I have been following this example https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-script-mode\/sagemaker-script-mode.ipynb but keep getting an error that the train file was not found. I think the file were not uploaded to the correct channel. When I run the script as follows it works fine.\n\n! python script_rf.py --model-dir .\/ \\\n                   --train .\/ \\\n                   --test .\/ \\\n\n\nThis is my script code:\n\n# inference functions ---------------\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n    return clf\n\nif __name__ =='__main__':\n\n    print('extracting arguments')\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--max_depth', type=int, default=2)\n    parser.add_argument('--n_estimators', type=int, default=100)\n    parser.add_argument('--random_state', type=int, default=0)\n    \n\n    # Data, model, and output directories\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--train-file', type=str, default='revenue_train.csv')\n    parser.add_argument('--test-file', type=str, default='revenue_test.csv')\n    \n    args, _ = parser.parse_known_args()\n    \n    print('reading data')\n    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n    \n    if len(train_df) == 0:\n        raise ValueError(('There are no files in {}.\\n').format(args.train, \"train\"))\n\n    print('building training and testing datasets')\n    attributes = ['available_minutes_100','ampido_slots_amount','ampido_slots_amount_100','ampido_slots_amount_200','ampido_slots_amount_300','min_dist_loc','count_event','min_dist_phouses','count_phouses','min_dist_stops','count_stops','min_dist_tickets','count_tickets','min_dist_google','min_dist_psa','count_psa']\n    X_train = train_df[attributes]\n    X_test = test_df[attributes]\n    y_train = train_df['target']\n    y_test = test_df['target']\n    \n    # train\n    print('training model')\n    model = RandomForestClassifier(\n        max_depth =args.max_depth, n_estimators = args.n_estimators)\n    \n    model.fit(X_train, y_train)\n     \n    # persist model\n    path = os.path.join(args.model_dir, \"model_rf.joblib\")\n    joblib.dump(model, path)\n    print('model persisted at ' + path)\n    \n    # print accuracy and confusion matrix \n    print('validating model')\n    y_pred=model.predict(X_test) \n    print('Confusion Matrix:')\n    result = confusion_matrix(y_test, y_pred)\n    print(result)\n    print('Accuracy:')\n    result2 = accuracy_score(y_test, y_pred)\n    print(result2)\n\n\nthe error is raised in the train_df line of the script (FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/input\/data\/test\/revenue_train.csv').\n\nI tried specifying the input parameters:\n\n# change channel input dirs \ninputs = {\n    \"train\": \"ampido-exports\/production\/revenue_train\",\n    \"test\": \"ampido-exports\/production\/revenue_test\",\n}\nfrom sagemaker.sklearn.estimator import SKLearn\nenable_local_mode_training = False\n\n\nhyperparameters = {\"max_depth\": 2, 'random_state':0, \"n_estimators\": 100}\n\nif enable_local_mode_training:\n    train_instance_type = \"local\"\n    inputs = {\"train\": trainpath, \"test\": testpath}\n\nelse:\n    train_instance_type = \"ml.c5.xlarge\"\n    inputs = {\"train\": trainpath, \"test\": testpath}\n\nestimator_parameters = {\n    \"entry_point\": \"script_rf.py\",\n    \"framework_version\": \"1.0-1\",\n    \"py_version\": \"py3\",\n    \"instance_type\": train_instance_type,\n    \"instance_count\": 1,\n    \"hyperparameters\": hyperparameters,\n    \"role\": role,\n    \"base_job_name\": \"randomforestclassifier-model\",\n    'channel_input_dirs' : inputs\n}\n\nestimator = SKLearn(**estimator_parameters)\nestimator.fit(inputs)\n\n\nbut i still get the error FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/input\/data\/test\/revenue_train.csv",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-29T07:18:21.655Z",
                "Answer_upvote_count":1,
                "Answer_body":"Your code has a typo that results in the issue, maybe caused by copy-pasting. Note that instead of a training path you're passing a test path location as the default to args.train:\n\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n                           ^^^^^                                                ^^^^\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n\nLater, when you try to access the training file from the test location, the file is obviously not there:\n\nNo such file or directory: '\/opt\/ml\/input\/data\/test\/revenue_train.csv\n                                               ^^^^         ^^^^^\n\n\nChanging the default argument for the args.train to SM_CHANNEL_TRAIN should resolve the issue,",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ClientError: Data download failed:Unable to create download dir",
        "Question_creation_time":1633390841000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhODEUiiMT2y0cBJixQ3ofA\/client-error-data-download-failed-unable-to-create-download-dir",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":194.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello,\nI searched for this error in the forum read through the first 6 or 7 forum pages with no luck. Im able to build models in Sagemaker Studio, but when I try in Sagemaker I get this error:\n\nClientError: Data download failed:Unable to create download dir \/opt\/ml\/checkpoints\/tc19\/preprocessed-data\/header\n\nAnybody know how to clear this ?\n\nThank in advance.",
        "Answers":[
            {
                "Answer_creation_time":"2021-10-05T21:41:00.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"The problem is that Sagemaker was trying to use a sub-directory that already contained a download. Not sure how that happened. I cleared the error by creating a sub-dir in S3 and referencing that location for the data download.\n\nHope this helps some one.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker endpoint creation fails for Multi Model",
        "Question_creation_time":1649256875061,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2bUNsPi3Rgautb0ZycrziA\/sage-maker-endpoint-creation-fails-for-multi-model",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":119.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"When using scikit to create multi model, it throws an exception, but when in single model it works.\n\nComplains about model_fn implementation or ping issues, any tips on how to fix this?\n\ne.g container={\\n\", \" 'Image' : image_uri,\", \" 'Mode': 'MultiModel',\", \" 'ModelDataUrl': 's3:\/\/somepatch\/with\/all\/models\/,\", \" 'Environment': {'SAGEMAKER_SUBMIT_DIRECTORY': mme_artifacts_path,\", \" 'SAGEMAKER_PROGRAM': 'inference.py'} \"\n\nFile \"\/miniconda3\/bin\/serve\", line 8, in <module> sys.exit(serving_entrypoint()) File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/serving.py\", line 144, in serving_entrypoint start_model_server() File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/serving_mms.py\", line 124, in start_model_server modules.import_module(serving_env.module_dir, serving_env.module_name) File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_modules.py\", line 263, in import_module six.reraise(_errors.ImportModuleError, _errors.ImportModuleError(e), sys.exc_info()[2]) File \"\/miniconda3\/lib\/python3.7\/site-packages\/six.py\", line 702, in reraise raise value.with_traceback(tb) File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_modules.py\", line 258, in import_module module = importlib.import_module(name) File \"\/miniconda3\/lib\/python3.7\/importlib\/init.py\", line 118, in import_module if name.startswith('.'):",
        "Answers":[
            {
                "Answer_creation_time":"2022-04-08T09:24:37.439Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nWe would need a bit more information to help you, including a bit more detail on how you create the model (code wise) as well as more details on the errors you receive.\n\nI would also suggest first trying out one of the available examples for multi model endpoints, like this one: Amazon SageMaker Multi-Model Endpoints using Scikit Learn and from there modify to your own needs.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Trigger and\/or monitor Automated Data Labeling Job",
        "Question_creation_time":1659628069730,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5eZvKOO5R4y3FAXlbpUqiw\/trigger-and-or-monitor-automated-data-labeling-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":61.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am following the documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html#sms-auto-labeling-ec2 and I have created a Ground Truth labeling job for object detection. I also enabled automated data labeling when creating the job. I have roughly 5000 images in my dataset. I have manually labelled ( by creating myself as a worker ) 129 of these images. How many do I need to label before the automated labeling job triggers? How do I know if a job was triggered\/succeeded\/failed etc?\n\nThanks",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-16T13:46:06.220Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nAs per this link to the documentation: \"The minimum number of objects allowed for automated data labeling is 1,250, but we strongly suggest providing a minimum of 5,000 objects\" when using active learning in Ground Truth to automate labeling.\n\nYou should also be able to see the status of your labeling job in the Labeling Jobs section of the console if that is how you created the job in the first place.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Registered for aws sagemaker studio but not able to create account",
        "Question_creation_time":1652237658633,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSB0wO0OiQ_irUdCx9ppjbg\/registered-for-aws-sagemaker-studio-but-not-able-to-create-account",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":70.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi I received my aws sagemaker studio approval to create an account 1 hr ago When I go to the link to create an account it says my email has not been approved Even though I have an email to the contrary\n\nHow do I contact amazon to sort this out?",
        "Answers":[
            {
                "Answer_creation_time":"2022-05-11T15:42:07.722Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, If you requested a SageMaker Studio Lab free account, then please create an issue on this link.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Multi Model EndPoint and Inference Data Capture feature",
        "Question_creation_time":1649794559479,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlAvpGSsISyqu0MyebgRJDA\/sage-maker-multi-model-end-point-and-inference-data-capture-feature",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":175.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Does Data Capture feature used for model monitor and analytics work with the multi model endpoint (one container).. we ran into an error. See error \" An error occurred (ValidationException) when calling the CreateEndPointConfig operation: Data Capture Feature is not supported with MultiModel mode\" Theoretically, it should work because it is calling the DataCaptureConfig:\n\nfrom sagemaker.model_monitor import DataCaptureConfig\n\nendpoint_name = 'your-pred-model-monitor-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()) print(\"EndpointName={}\".format(endpoint_name))\n\ndata_capture_config=DataCaptureConfig( enable_capture = True, sampling_percentage=100, destination_s3_uri=s3_capture_upload_path)",
        "Answers":[
            {
                "Answer_creation_time":"2022-04-13T13:39:17.628Z",
                "Answer_upvote_count":0,
                "Answer_body":"SageMaker multi-model endpoints do not have support for SageMaker Model monitor as of writing this answer. So the error is pointing to exactly that.\n\nHowever, if you are looking to implement data drift using sagemaker model monitor then you can do that my mimicking data capture config functionality by capturing inference input and prediction output and storing it in the format supported by Model Monitor. And then setup a customer monitoring container using the instructions listed https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-byoc-containers.html",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Optimal notebook instance type for DeepAR in AWS Sagemaker",
        "Question_creation_time":1644862112974,
        "Question_link":"https:\/\/repost.aws\/questions\/QUnYV-WoO2R3KY4sNEq-Dshw\/optimal-notebook-instance-type-for-deep-ar-in-aws-sagemaker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Model Training"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":96.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I am currently utilizing an ml.c4.2xlarge instance type for a DeepAR use case to run an Automated Model Tuning job. The data consists of 7157 time series with 152 timesteps in the training set and 52 timesteps in the test set respectively. I estimate the run time for the tuning job on this specific instance type to take about 4-5 days. Looking to find out if DeepAR is engineered to take advantage of GPU computing for training and if it would be advisable to use a 'p' or 'g' compute instance instead for faster results. Also would be great for recommendations as to which Accelerated Computing instance would be optimal for this scenario.",
        "Answers":[
            {
                "Answer_creation_time":"2022-02-15T02:29:47.263Z",
                "Answer_upvote_count":1,
                "Answer_body":"(As detailed further on the algorithm details page), yes, the SageMaker DeepAR algorithm implementation is able to train on GPU-accelerated instances to speed up more challenging jobs. There's also a handy reference table here listing all the SageMaker built-in algorithms and whether they're likely to be accelerated with GPU.\n\nHowever, to be clear, it shouldn't be the notebook instance type that affects this... Typically when training models on SageMaker, the notebook would provide your interactive compute environment but you'd run training in training jobs - for example using the SageMaker Python SDK Estimator class as shown in the sample notebooks for DeepAR electricity and synthetic. The instance type you select for training is independent of the instance type you use for your notebook - for example in the electricity notebook it's set as follows:\n\nestimator = sagemaker.estimator.Estimator(\n    image_uri=image_name,\n    sagemaker_session=sagemaker_session,\n    role=role,\n    train_instance_count=1,  # <-- Setting training instance count\n    train_instance_type=\"ml.c4.2xlarge\",  # <-- Setting training instance type\n    base_job_name=\"deepar-electricity-demo\",\n    output_path=s3_output_path,\n)\n\nSo normally I wouldn't expect you to need to change your notebook instance type to speed up training - just edit the configuration of your training job from within the notebook.\n\nSuggesting a particular type is tricky because DeepAR hyperparameters like context_length, embedding_dimension, and mini_batch_size will affect how much GPU capacity is needed for a particular run. Since you're coming from CPU-only baseline, I'd maybe suggest to start small with trying out single-GPU g4dn.xlarge, g5.xlarge or p3.2xlarge instances, perhaps starting with the lowest cost-per-hour? You can keep an eye on your jobs' GPUUtilization and GPUMemoryUtilization metrics to check whether utilization is low on instances like p3 with \"bigger\" GPUs. Increasing mini_batch_size should help fill extra capacity on these and complete your job faster, but it will probably affect model convergence - so may need to tune other parameters like learning_rate to try and compensate. So considering all of this, you may find trade-offs between speed and total cost, or speed and accuracy, for good hyperparameter combinations on your dataset. Of course you could also scale up to multi-GPU instance types if you'd like to accelerate further.\n\nIf I understood right you're also using SageMaker Automatic Hyperparameter Tuning to search these parameters, something like this XGBoost notebook with the HyperparameterTuner class?\n\nIn that case would also mention:\n\nIncreasing the max_parallel_jobs parameter may accelerate the overall run time (by running more of the individual training jobs in parallel) - with a trade-off on how much information is available when each training job in the budget is kicked off.\nIf you're planning to run this training regularly on a dataset which evolves over time, you probably don't need to run HPO each time: Will likely see good results using your previously-optimized hyperparameters, unless something materially changes in the nature of the data and patterns.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Where should I report to when encounter a trouble at SageMaker Canvas?",
        "Question_creation_time":1644763229468,
        "Question_link":"https:\/\/repost.aws\/questions\/QUpsGPPvV7SbuofE1fnwC5AA\/where-should-i-report-to-when-encounter-a-trouble-at-sage-maker-canvas",
        "Question_topic":[
            "Machine Learning & AI",
            "AWS Well-Architected Framework",
            "Microservices"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Well-Architected Framework",
            "Microservices",
            "Amazon SageMaker Canvas"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":63.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"When I was building model for analyzing in SageMaker Canvas, it just run for 1h 2m and then I got this notification:\n\nModel building failed: Failed to run Neo compilation or generate explainability report. client_request_id is f76d5bf7-6780-4257-9631-500101632b1e\n\nWhere should I contact the admin for this issue? Thank you so much!",
        "Answers":[
            {
                "Answer_creation_time":"2022-02-14T18:28:44.414Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Ailee, you can submit a ticket here and engineering will get back to you: https:\/\/t.corp.amazon.com\/create\/templates\/20b56bae-3fca-4281-9b94-69b6e50128cd. Thanks!",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"how to choose ml.g4dn.* instances in sagemaker processing jobs",
        "Question_creation_time":1643215786791,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXqikCZktSFywwXL14PWcYg\/how-to-choose-ml-g-4-dn-instances-in-sagemaker-processing-jobs",
        "Question_topic":[
            "Machine Learning & AI",
            "Compute"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon EC2"
        ],
        "Question_upvote_count":1.0,
        "Question_view_count":302.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have to perform some data manipulation for which the sagemaker \"processing job\" would fit perfectly. Such jobs would benefit from GPU and thus I was looking to use instances from the ml.g4dn family for cost efficiency. Unfortunately, I cant see them available in the dropdown when creating a processing job from the aws dashboard, only when creating training jobs. I previously requested the limit increase to the aws support, and i was told it was not necessary and up to 20 instances could be run in the chosen region.\n\nAm I missing anything? do i have to enable the instance family somewhere else?\n\nthanks",
        "Answers":[
            {
                "Answer_creation_time":"2022-01-26T22:19:03.162Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi there, thanks for reaching out about your issue. Are you able to use the AWS CLI or SDKs to start Processing jobs with the ml.g4dn instance types? Also, could you clarify whether AWS support increased your ml.g4dn quota for both Processing and Training or not?",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"invoke_endpoint error in Lambda: StreamingBody is not JSON serializable",
        "Question_creation_time":1550644604000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6wwr5n1XTuWtQjkbCGbCOA\/invoke-endpoint-error-in-lambda-streaming-body-is-not-json-serializable",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":763.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm writing a Lambda function that invokes an endpoint:\n\nruntime= boto3.Session().client('runtime.sagemaker')\r\npayload = {\"data\": [\"McDonalds\"]}\r\nresponse = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\r\n                                       ContentType='application\/json',\r\n                                       Body=json.dumps(payload))\n\n\nIt returns this error\n\nAn error occurred during JSON serialization of response: <botocore.response.StreamingBody object at 0x7f59e40acc50> is not JSON serializable\n\n\nI tried this exact function in SageMaker notebook and it works but it doesn't work in Lambda. Can someone please help me?\n\nEdited by: aurelius on Feb 19, 2019 10:38 PM",
        "Answers":[
            {
                "Answer_creation_time":"2019-12-20T09:29:36.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Was this because I didn't attach the necessary SageMaker policies to the IAM role? I only added this policy:\n\n{\r\n    \"Version\": \"2012-10-17\",\r\n    \"Statement\": [\r\n        {\r\n            \"Sid\": \"VisualEditor0\",\r\n            \"Effect\": \"Allow\",\r\n            \"Action\": \"sagemaker:InvokeEndpoint\",\r\n            \"Resource\": \"*\"\r\n        }\r\n    ]\r\n}",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-12-20T11:03:09.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi aurelius,\nDo you have further code after the response = runtime.invoke_endpoint(..) line? the error message says problem with serialization of the response object.\n\nYour role permission looks fine.\n\nThank you,\nArun",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-12-18T12:10:27.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I am facing similar issue, in sagemaker jupyter notebook instance the endpoint is invoked successfully and I am able to get back the inference results.\n\nBelow is my Lambda function that i used to invoke the endpoint, but I am facing the following error,\n\nimport json \r\nimport io\r\nimport boto3 \r\n\r\nclient = boto3.client('runtime.sagemaker')\r\n\r\ndef lambda_handler(event, context):\r\n    print(\"Received event: \" + json.dumps(event, indent=2))\r\n    \r\n    #data = json.loads(json.dumps(event))\r\n    #payload = data['data']\r\n    print(json.dumps(event))\r\n    \r\n    response = client.invoke_endpoint(EndpointName='linear-learner-2019-12-12-16-20-56-788',\r\n                                  ContentType='application\/json',\r\n                                  Body=(json.dumps(event)))\r\n    return response\n\n\nOutput:\n\nFunction Logs:\r\nSTART RequestId: 7f4c7589-b70f-4af8-834c-89a1a1fbe5e5 Version: $LATEST\r\nReceived event: {\r\n  \"instances\": [\r\n    {\r\n      \"features\": [\r\n        0.1,\r\n        0.2\r\n      ]\r\n    }\r\n  ]\r\n}\r\n{\"instances\": [{\"features\": [0.1, 0.2]}]}\r\nAn error occurred during JSON serialization of response: <botocore.response.StreamingBody object at 0x7f53918e2828> is not JSON serializable\n\n\nPlease help me resolve the issue, you can see the JSON input passed to the function. Not sure what is going wrong here. I even checked the cloudwatch logs not able to identify the origin of the issue.\n\nThanks in advance,\nArun\n\nEdited by: NMAK on Dec 18, 2019 5:54 AM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-02-22T01:07:19.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I call Sagemaker from Lambda using a slightly different approach in terms of data structures:\n\nfinal_data = ','.join(ordered_data.iloc[0].astype(str).values.tolist())\r\nruntime = boto3.client('runtime.sagemaker')\r\nresponse = runtime.invoke_endpoint(EndpointName='whatever-endpoint', \r\n                                           ContentType='text\/csv',\r\n                                           Body=final_data)\r\nresult = json.loads(response['Body'].read().decode())\n\n\nI start with a dataframe of one row containing all the data.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-02-20T22:36:48.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"hello Javierlopez,\n\nThanks for your response, I really appreciate it.\n\nI tried the below code and still it gives me same error\n\n    payload = ','.join(str(item) for item in data['instances'][0]['data']['features'])\r\n    #payload=bytearray(payload)\r\n    print(payload)\r\n    \r\n    response = client.invoke_endpoint(EndpointName='linear-learner-2019-12-12-16-20-56-788',\r\n                                  ContentType='text\/csv',\r\n                                  Body=payload)\r\n    return response\n\n\nbelow is output\/error for the above code.\n\nFunction Logs:\r\nSTART RequestId: 73daa03e-dec2-4d37-b779-72c1f70a7142 Version: $LATEST\r\nReceived event: {\r\n  \"instances\": [\r\n    {\r\n      \"data\": {\r\n        \"features\": [\r\n          0.1,\r\n          0.2\r\n        ]\r\n      }\r\n    }\r\n  ]\r\n}\r\n0.1,0.2 # this is the payload sent for inference.\r\nAn error occurred during JSON serialization of response: <botocore.response.StreamingBody object at 0x7f77555e59e8> is not JSON serializable\n\n\nI really don't understand the logic behind this on what format the function expects the input, my colleague ran a different model with same JSON format I used and it works fine. I believe you used this is inference format for XGBoost. I am not able to find any documentation on linear-learner sample inference requests using lambda.\n\nCould you please point me in the right direction.\n\nRegards,\nArun\n\nEdited by: NMAK on Dec 20, 2019 3:04 AM",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Jupyter notebook",
        "Question_creation_time":1606707121000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIzWlfNVTSIWIqkVsIaNv2A\/sagemaker-jupyter-notebook",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":62.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"What are the advantages of using SageMaker jupyter instance instead of running it locally? Is there a special integration with SageMaker that we lose it if we do not use Sagemaker jupyer instance?",
        "Answers":[
            {
                "Answer_creation_time":"2020-11-30T04:05:24.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Some useful points:\n\nThe typical arguments of cloud vs local will apply (as with e.g. Cloud9, Workspaces, etc): Can de-couple your work from the lifetime of your laptop, keep things running when your local machine is shut down, right-size the environment for what workloads you need to do on a given day, etc.\nSageMaker notebooks already run in an explicit IAM context (via assigned execution role) - so you don't need to log in e.g. as you would through the CLI on local machine... Can just run sagemaker.get_execution_role()\nPre-built environments for a range of use-cases (e.g. generic data science, TensorFlow, PyTorch, MXNet, etc) with libraries already installed, and easy wiping\/reset of the environment by stopping & starting the instance - no more \"environment soup\" on your local laptop.\nLinux-based environments, which typically makes for a shorter path to production code than Mac\/Windows.\nIf you started using SageMaker Studio, then yes there are some native integrations such as the UIs for experiment tracking and endpoint management\/monitoring; easy sharing of notebook snapshots; and whatever else might be announced over the next couple of weeks.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Serverless Inference - Limit number of workers",
        "Question_creation_time":1642602434394,
        "Question_link":"https:\/\/repost.aws\/questions\/QUWYP78UdYQseoErcj4kjiug\/serverless-inference-limit-number-of-workers",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":161.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"We've deployed a HuggingFace model to Sagemaker as a serverless endpoint. We set memory to be 6GB and max concurrency to be 1. With these settings, we keep getting errors when we call invoke_endpoint. Not all the time, but about 60% of the time...\n\nWhen we check the logs and metrics, we see that the memory has gone up to almost 100%. We also see that, since the machine has 6 CPUs, if starts 6 workers. We believe this could be the cause of the problem. How can se set the number of workers?\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_time":"2022-01-19T17:44:50.854Z",
                "Answer_upvote_count":0,
                "Answer_body":"From \u201csagemaker.pytorch.model.PyTorchModel\u201d documentation:\n\nmodel_server_workers (int) \u2013 Optional. The number of worker processes used by the inference server. If None, server will use one worker per vCPU.\n\nYou can see this example on how to set \u201cMODEL_SERVER_WORKERS\u201d environment variable to set number of workers.\n\nenv={\n    \"MODEL_SERVER_WORKERS\":\"2\"\n    }\n\nlocal_regressor = Estimator(\n    image,\n    role,\n    instance_count=1,\n    instance_type=\"local\")\n\ntrain_location = 'file:\/\/'+local_train\nvalidation_location = 'file:\/\/'+local_validation\nlocal_regressor.fit({'train':train_location, 'validation': validation_location}, logs=True)\n\npredictor = local_regressor.deploy(1, 'local', serializer=csv_serializer, env=env)\n\n\nHope it helps.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-19T16:04:33.089Z",
                "Answer_upvote_count":0,
                "Answer_body":"Eitan, thanks for replying.\n\nI'm not sure if this worked or not, as not the cloudwatch logs are not showing the number of workers anymore! The performance seems to be the same, however. It's failing more often than it's responding. And still reaching almost 100% memory.\n\nInstead of your code, I used the following, as I'm deploying a Hugging Face model:\n\nhuggingface_model = HuggingFaceModel(\n    name=model_name,\n    model_data=os.path.join(\"s3:\/\/\" + tar_bucket_name, tarfile_name),\n    env={\n        'HF_TASK': 'text-classification',\n        'MODEL_SERVER_WORKERS': '1',\n        'MODEL_SERVER_TIMEOUT': '300'\n    },\n    role=sagemaker.get_execution_role(),\n    entry_point='inference.py',\n    transformers_version='4.12.3',\n    pytorch_version='1.9.1',\n    py_version='py38'\n)\n\n\nTwo follow up questions then, if you don't mind:\n\nHow can I see if the serverless function actually created only one worker per instance?\nWhere can I find all the different environment variables accepted by SageMaker?\n\nMany thanks!\n\nRogerio",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can we run a python script in Sagemaker using boto3 from a local machine?",
        "Question_creation_time":1646819726632,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjnuGv6KCRaS9BCxzVgCYyA\/can-we-run-a-python-script-in-sagemaker-using-boto-3-from-a-local-machine",
        "Question_topic":[
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "DevOps"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":808.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Here's what I am trying to do: In my application that resides outside aws, I take some user inputs, and trigger scripts that reside inside Sagemaker notebook instance. I am able to start or create a new instance using boto3, and also use lifecycle configuration to run some starter script while the instance turns on. But I want to run multiple scripts in short intervals based on user inputs, so I don't want to restart my instance each time with a new lifecycle configuration script. I am trying to find if there is a way to execute shell commands in sagemaker using boto3 (or any other way).",
        "Answers":[
            {
                "Answer_creation_time":"2022-03-09T10:41:19.588Z",
                "Answer_upvote_count":1,
                "Answer_body":"It should be possible, but it's probably not a great idea...\n\nThis is not really an intended pattern for SageMaker notebooks today, and it's more likely that you should be using SageMaker Processing Jobs to schedule your regular tasks - taking input and output data direct from S3 rather than relying on local notebook storage.\n\nWith that warning out of the way, a hacky solution:\n\nSageMaker notebooks (both Notebook Instances and Studio) are based on Jupyter and thus today more-or-less conform (with some customizations) to Jupyter's client\/server API model, which has both REST and WebSocket\/ZeroMQ aspects. This means as long as you're able to handle authentication, it's possible to interact with the notebooks from a script using the same interfaces your browser would.\n\nThis automation-style solution would proceed as (assuming Python):\n\nUse boto3 and the SageMaker CreatePresignedNotebookInstanceUrl API to create a presigned notebook instance URL (Granting this IAM permission is what allows a User\/Role\/principal to open the notebook)\nUse a stateful HTTP library like requests to request this URL in a session and and save the cookie data set by the response. Fetching the URL logs your client in to Jupyter, and \"your client\" is the session - need to keep it persistent.\nUse the JupyterServer REST APIs for things like opening terminal or notebook sessions, listing available kernels, listing open sessions, etc.\nWhen you have a session open (terminal or notebook), use a WebSocket client library like websocket-client to interact with it (sending commands, receiving results, etc). Remember you'll need to use your same session for authentication.\n\nI think I only have end-to-end examples of this for SMStudio: The deprecated auto-installer of the official SageMaker Studio Auto-Shutdown Extension used to use this method before SMStudio Lifecycle Configuration Scripts became available, and some rough draft PoCs on GitHub explore the notebook side too but always with ref to Studio. However it should be possible for NBIs too with almost the same process: Just need to use the above mentioned API in place of CreatePresignedDomainUrl, and may need to check whether the REST api_base_url needs to be adjusted.\n\nIt might even be possible to use a higher-level solution like the nbclient library if you can get the authentication to work with it - would be interested to hear if anyone does!",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Studio default server failing",
        "Question_creation_time":1654597460713,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJhSBFYdSQ8aGHNA3DjMk-A\/sagemaker-studio-default-server-failing",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":54.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello, I am trying to start Amazon SageMaker Studio today but without success. The default server is failing to start. On CloudWatch I only see SIGKILL messages as errors:\n\n2022-06-07T10:55:01.523+02:00\t2022-06-07 08:55:00,466 WARN killing 'jupyterlabserver' (11) with SIGKILL\n2022-06-07T10:55:04.524+02:00\t2022-06-07 08:55:01,470 INFO waiting for jupyterlabserver-listener, jupyterlabserver to die\n\n\nOn the Apps section I see the default server in status \"Failed\", and I cannot do anything, not even delete it.\n\nI cannot use the service right now. Is it maybe related to the new release of Jupyter Lab v3.0?\n\nThanks",
        "Answers":[
            {
                "Answer_creation_time":"2022-06-08T07:33:16.014Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, Could you try the steps from this documentation? https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tasks-update-studio.html You mention that you cannot delete it. How does the UI look like? Is the Detele button greyed out? Could you try shutting down all the services in the Studio and create a new one?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-14T09:59:37.829Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello. Just as an update for this question, the final solution was to delete the entire SageMaker domain suing the AWS cli and recreate it from scratch. Having done that the problem is no more there, so I encountered by chance some unexpected AWS internal error that stuck my entire domain. Best",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"HELP!!!! Amazon SageMaker not writing best optimal route based on Genetic Algorithm to 2nd Output Dynamo Database(am stucked here&incurring dollar charges with no progress -Error Screenshot available)",
        "Question_creation_time":1666611225424,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDsciSjamQ5WZR_FHC1u0vQ\/help-amazon-sage-maker-not-writing-best-optimal-route-based-on-genetic-algorithm-to-2nd-output-dynamo-database-am-stucked-here-incurring-dollar-charges-with-no-progress-error-screenshot-available",
        "Question_topic":[
            "Machine Learning & AI",
            "Database",
            "Containers",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon DynamoDB",
            "Containers",
            "DevOps"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":26.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"My Challenges is this: I used CloudFormation template to deploy 2 Dynamo DB (Input and Output) and 1 IAM role to use AWS-managed Lamba Function for Genetic Algorithm, so Amazon SageMaker (using Jupyter Notebook on AWS) is meant to write the locations (X and Y coordinates into the input Dynamo DB (Successful), while the Docker file to Docker Image to Docker Container is also to be run by Amazon SageMaker to write the best Optimal Route based on Genetic Algorithm (Mutation, genomes and generation transfer mode of operation) to the 2nd Dynamo DB (Unsuccessful) and this is where I am stucked, have read a lot of materials and research a lot and even reached out to some Amazon AWS Community but they could not resolve it, Please will be glad if repost.aws can help please (Error Screenshot Available)",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to pass credentials in Glue Notebooks - Interactive Session using Magic Commands to override the 1 hour temporary token expiration",
        "Question_creation_time":1666635956843,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0wxONrToTd2vvUzwsmt3cw\/how-to-pass-credentials-in-glue-notebooks-interactive-session-using-magic-commands-to-override-the-1-hour-temporary-token-expiration",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Glue",
            "Session Management"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":28.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi, If I start the notebook via the console the token\/credentials expire after one hour, Gives the following error \"Exception encountered while retrieving session: An error occurred (ExpiredTokenException) when calling the GetSession operation: The security token included in the request is expired \" .\n\nI am guessing this is happening since its using temporary credentials by default. How does one pass the credentials using the magic commands such that credentials do not expire or workaround?\n\nI can run notebooks locally using the profile in local .aws folder, but can't use TAGS for the sessions to account for costs.",
        "Answers":[
            {
                "Answer_creation_time":"2022-11-01T17:00:31.794Z",
                "Answer_upvote_count":0,
                "Answer_body":"From the console Glue notebook, I tried running %iam_role my-glue-role magic to reset the IAM role and that seemed to do the trick.\n\nYou are already connected to session axxxxxxx-2xxxx-4xxx-bxxx-0xxxxxxx. Your change will not reflect in the current session, but it will affect future new sessions. \n\nCurrent iam_role is arn:aws:iam::xxxxxxxxx:role\/my-glue-role\niam_role has been set to my-glue-role.\n\n\nIf you are doing it using an API in your computer, you can increase the maximum session duration expiration for temporary credentials for IAM roles using the DurationSeconds parameter for your use case.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker - S3 bucket access when logged into us-east-1",
        "Question_creation_time":1652377507550,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVgOCxivDSgiiHh-HZ9aYYA\/sagemaker-s-3-bucket-access-when-logged-into-us-east-1",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":76.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"My IAM works, but when I login, it logs me into us-east-1. My project is on the S3 instance (sagemaker tool). How can I access buckets in S3 if I am logged into us-east-1 by default.",
        "Answers":[
            {
                "Answer_creation_time":"2022-05-13T13:01:21.591Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can change the region when you are inside a SageMaker notebook or python script. Here you can find many examples of how to access S3 buckets from SageMaker. Please elaborate more on the issue you are facing so we can help you.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can't Import Modules In Sagemaker Jupyter Notebook",
        "Question_creation_time":1660764982506,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBSiq8Lx5St-1D8CT3k328g\/cant-import-modules-in-sagemaker-jupyter-notebook",
        "Question_topic":[
            "Developer Tools",
            "Machine Learning & AI",
            "Analytics",
            "Compute"
        ],
        "Question_tag":[
            "Developer Tools",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Elastic MapReduce",
            "FPGA Development"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":117.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I've tried to use the pip install librosa command within a jupyter notebook, yet I get a modulenotfounderror. Is there a place where I can use pip install librosa?\n\nThanks.",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-17T19:57:53.668Z",
                "Answer_upvote_count":2,
                "Answer_body":"Tested in one of the notebook in my account and it works with !pip install librosa\n\nPlease review the configuration of your notebook instances, under the Network section, you should see the result as below:\n\n*No custom VPC settings applied.\n\nDirect internet access *",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"how to log error\/messages in while running a sagemaker batch transform job?",
        "Question_creation_time":1653699481978,
        "Question_link":"https:\/\/repost.aws\/questions\/QUNirOT1cMSfig9ANtgmZMpg\/how-to-log-error-messages-in-while-running-a-sagemaker-batch-transform-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":58.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"i'm using a hugging face model and a container to create a batch transform job in sagemaker. i have a custom inference code and in the output_fn function i'm returning json_dumps(prediction). I'm using print(prediction) to see, if i can see it in the cloudwatch logs to find out type and what prediction is. how can log these messages . Also, the inference output i get is in the form below., i'm not sure why is it not a json object in each line instead it has square brackets. I want to use the filter to match the input and output in the batch job. I'm not sure how the output should look like , because i'm trying to associate input with output by using dataprocessing config as below. but i get an error. the documenation has example of csv not json. what should the output look like so that i can associate the input with output when they both are in json format.\n\n  \"DataProcessing\": {\n        \"JoinSource\": \"Input\"\n    },\n\n[ output text 1 ]\n[output text 2 ]\n\n# Serialize the prediction result into the desired response content type\ndef output_fn(prediction, accept=JSON_CONTENT_TYPE):\n    logger.info(\"Serializing the generated output.\")\n    if accept == JSON_CONTENT_TYPE:\n        output = json.dumps(prediction)\n        return output, accept\n    raise Exception(\"Requested unsupported ContentType in Accept: {}\".format(accept))",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cloudformation Deployment for Serverless Sagemaker Model Endpoint",
        "Question_creation_time":1655512915811,
        "Question_link":"https:\/\/repost.aws\/questions\/QUo7p2DuabQaapi9C4_nPVZg\/cloudformation-deployment-for-serverless-sagemaker-model-endpoint",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI",
            "Management & Governance",
            "DevOps"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "AWS CloudFormation",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":66.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi, i want to deploy a serverless model using CloudFormation.\n\nI've created the model, the endpoint configuration, and when I try to create the endpoint the script times out, because it can't find a saved model to attach to the endpoint (because I never trained one for this instance).\n\nI've tried to look around for a trainingjob cloudformation API, but there doesn't seem to be one.\n\nHow do I solve this issue?",
        "Answers":[
            {
                "Answer_creation_time":"2022-06-20T23:10:52.687Z",
                "Answer_upvote_count":0,
                "Answer_body":"In general I would like to confirm if you are bringing your own model which is trained outside the SageMaker ?\n\nIf so SageMaker expects to Upload the pre-trained model to S3 and specify the ModelDataUrl i.e s3 location along with the Container details with the CreateModel API call. For more details on bring your own model, please refer to our public examples [1][2][4][5]\n\nAlso as an alternative please refer to the following example to Bring your own model with Amazon SageMaker script mode which can be used for training and hosting ML models using the script mode[2]\n\nIf you have other questions or require any further clarifications please don't hesitate to reply or open a premium support case with the CloudFormation team for further investigation of the issue along with the error message and the resource details.\n\n[1] https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/advanced_functionality\/xgboost_bring_your_own_model\/xgboost_bring_your_own_model.html#Upload-the-pre-trained-model-to-S3\n\n[2] https:\/\/aws.amazon.com\/blogs\/machine-learning\/bring-your-own-model-with-amazon-sagemaker-script-mode\/\n\n[3] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\n\n[4] https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-model.html\n\n[5] https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-properties-sagemaker-model-containerdefinition.html",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-21T20:37:40.461Z",
                "Answer_upvote_count":0,
                "Answer_body":"thank you these docs were very helpful. It appears that when I use cloudformation for provisioning resources for my model, I need to provision the ECR, then do the training step, then create the model, enpoint config, then endpoint. I can't really do it all in 1 script, i guess.\n\neverything works now for me, so thank you for pointing me to this resource!",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"An error occurred (ModelError) when calling the InvokeEndpoint operation",
        "Question_creation_time":1623820480000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU68gR5B3JRdqOEGlwE2-pnA\/an-error-occurred-model-error-when-calling-the-invoke-endpoint-operation",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":831.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello,\nI received the following error message when I tried to send an array to my model:\n\nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from container-1 with message \"<!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\">\n\n<title>500 Internal Server Error<\/title> <h1>Internal Server Error<\/h1> <p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.<\/p> \". See https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group\n\nI have created inference pipeline containing preprocessing and autoencoder model and deployed it to a single endpoint. Am trying to send raw data in text\/csv format. EX: \"39, 4, 9, 8, contact\"\n\nPlease help me out in this.\n\nMuch appreciated,\nKarthik",
        "Answers":[
            {
                "Answer_creation_time":"2021-07-22T16:37:09.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nSo the issue here is most likely with your inference code and how you are parsing\/transforming the data coming in. Your endpoint is up and running but the format in which you are feeding it data is confusing it. The endpoint is expecting encoded data thus you need to convert your payload into the appropriate data format, there are two manners in which you can approach this.\n\nUse a serializer, when creating your endpoint with the predictor class you want to use the SageMaker Serializer to automatically encode\/decode your data, this is configured while creating your endpoint. Look at the following code snippet below.\n\nfrom sagemaker.predictor import csv_serializer\nrf_pred = rf.deploy(1, \"ml.m4.xlarge\", serializer=csv_serializer)\n\n#for prediction, decode the data properly\nprint(rf_pred.predict(payload).decode('utf-8'))\n\nIf you choose not to use the serializer you want to encode the data on your own using something such as json.dumps(payload) to encode your data properly before sending the data to the endpoint.\n\nExtra Resources:\nSageMaker Serializers: https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\n\nHope this helps!",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Studio JupyterServer App does not load",
        "Question_creation_time":1656519218822,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5Da9xot8TwST0MP_8uRV2A\/sagemaker-studio-jupyter-server-app-does-not-load",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":85.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"After months of seamless work in SageMaker Studio, the JupyterServer App won't load for the last 4 days. The Control Panel shows that the JupyterServer is in \"Pending\" or \"Failed\" state after I try to launch the app. When clicking \"Launch app\", the screen shows that:\n\n\"The JupyterServer app default encountered a problem and was stopped.\"\nThe \"Restart Now\" button is visible, but pressing this results in the same behaviour. I created a new JupyterServer App and it experiences the same problem under that account. I use a different account for another project and the JupyterServer under that account works perfectly. I even mounted the EFS associated with the App on an EC2 instance and deleted some files to reduce the EFS volume but it did not help (it was 995 MB and as far as I know, 5GB is the default limit).\n\nI found a post stating the same problem from 2 years ago, but could not follow the advice to delete the app and create a new one, since the Delete option is not available in the Action dropdown (https:\/\/repost.aws\/questions\/QUxoSA7eTzQbK-T4OWjJvSmQ\/sage-maker-studio-will-not-load). All apps that I create is \"default\".\n\nPlease help, how could I overcome this and access Jupyter Lab again? Thank you.",
        "Answers":[
            {
                "Answer_creation_time":"2022-06-30T03:09:11.417Z",
                "Answer_upvote_count":0,
                "Answer_body":"Even if you're not seeing the \"Delete app\" option in console (because the app Failed), the good news is that the \"Restart\" button should be doing the same thing for you: So how can you find out more about what's breaking here and hopefully fix it?\n\nTo see logs, you can open the \/aws\/sagemaker\/studio log group in Amazon CloudWatch: Here you should find log streams like {DomainID}\/{UserProfileName}\/JupyterServer\/default and (if you have a lifecycle configuration script set up?) {DomainID}\/{UserProfileName}\/JupyterServer\/default\/LifecycleConfigOnStart.\n\nIf you do have a custom lifecycle configuration script set up, this can often be a point of failure: I'd suggest trying to detach it and\/or adding some extra flags like set -ux to help debug what might be going wrong in it. Also since SageMaker Studio recently launched JupyterLab v3 support in parallel to JLv1. If you've been experimenting with both versions, it's worth checking which version your user is currently configured to use, and remembering that some setup scripts which might work on one version could break on another.\n\nYour user's EFS home folder (and any additional setup the LCC script does) will be the only data persisted between JupyterServer launches, so content could be another point of failure. It sounds like you started to explore this already.\n\nYou could try to delete (or otherwise edit) the user's ~\/.jupyter folder from EFS to clear any customized Jupyter configuration settings that might be causing problems during start-up. Again, this may be useful if you're using any features or extensions for which the Jupyter configuration API changed between JLv1 and v3.\nI haven't found overall data volume to cause these kind of start-up problems myself so far. I have seen some cases where having a git repository with many active changes (e.g. thousands) causes a UI slowdown when working in the repository's folder, but I haven't seen it prevent the actual start-up I think?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-01T19:57:55.897Z",
                "Answer_upvote_count":0,
                "Answer_body":"@Alex_T thank you for the reply. The CloudWatch logs don't show any sign of failing notebooks, and I did not have any LCC either. The final solution was the following:\n\nI mounted the EFS associated to the SageMaker domain on an EC2 instance and made a backup of all notebooks and other files (saved them on my local computer too).\nThen, I deleted the SageMaker domain by following these steps: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-delete-domain.html (via CLI)\nCreated a new SageMaker domain. The JupyterServer App starts now.\nFinally, I mounted the new EFS to the EC2 instance and uploaded the notebooks and files. They are all visible and working in Jupyter.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"XGBoost Reports Not Generated",
        "Question_creation_time":1638475570231,
        "Question_link":"https:\/\/repost.aws\/questions\/QUx_M71_2nQJSDp-I1mgbjDg\/xg-boost-reports-not-generated",
        "Question_topic":[
            "Machine Learning & AI",
            "AWS Well-Architected Framework"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Performance Efficiency"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":169.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi!\n\nI have been trying to create a model using XGBoost, and was able to successfully run\/train the model. However, I have not been able to generate the training reports. I have included the rules parameter as follows: \"rules=[Rule.sagemaker(rule_configs.create_xgboost_report())]\".\n\nI am following this tutorial, but I am using objective: \"multi:softmax\" instead of the \"binary:logistic\" used in the example.\n\nWhen I run the model everything is fine but only the Profiler Report gets generated and I do not see the XGBoostReport under the rule-output folder. According to the tutorial it should be under the same file path.\n\nHere is my code for the model if it helps any:\n\ns3_output_location='s3:\/\/{}\/{}\/{}'.format(bucket, prefix, 'xgboost_model')\ncontainer = sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"latest\")\n\ntrain_input = TrainingInput(\n    \"s3:\/\/{}\/{}\/{}\".format(bucket, prefix, \"data\/train.csv\"), content_type=\"csv\"\n)\nvalidation_input = TrainingInput(\n    \"s3:\/\/{}\/{}\/{}\".format(bucket, prefix, \"data\/validation.csv\"), content_type=\"csv\"\n)\n\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\n\nxgb = sagemaker.estimator.Estimator(\n    image_uri=container,\n    role=sagemaker.get_execution_role(),\n    instance_count=1,\n    instance_type=\"ml.c5.2xlarge\",\n    volume_size=5,\n    output_path=s3_output_location,\n    sagemaker_session=sagemaker.Session(),\n    rules=rules\n)\n\nxgb.set_hyperparameters(\n    max_depth=6,\n    objective='multi:softmax',\n    num_class=num_classes,\n    gamma=800,\n    num_round=250\n)\n\n\nAny help is appreciated! Thanks!",
        "Answers":[
            {
                "Answer_creation_time":"2021-12-03T14:44:54.672Z",
                "Answer_upvote_count":0,
                "Answer_body":"I believe the issue here is the rules parameter is receiving a URL, not an array of RuleBase objects, as required by the Estimator documentation. Try re-writing your estimator accordingly, as suggested by the SageMaker Developer Guide:\n\nxgb = sagemaker.estimator.Estimator(\n   image_uri=container, \n   role=sagemaker.get_execution_role(), \n   instance_count=1, \n   instance_type=\"ml.c5.2xlarge\", \n   volume_size=5, \n   output_path=s3_output_location, \n   sagemaker_session=sagemaker.Session(), \n   rules=[\n      Rule.sagemaker(\n         rule_configs.create_xgboost_report()\n      )  \n   ]\n)\n\nFor more information on using Rules, check out the SageMaker Debugger documentation.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-14T11:17:17.194Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello jughead, Has your problem of your code been resolve? Let us know. Also, remember to click on the \"Accept\" button when an answer provided in the community helped you. This allows other community members to also benefit from it. Thank you for your participation.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-13T22:12:23.421Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nI had the same problem. To be able to generate the xgboost report, make sure that you use xgboost version 1.2-1 in your image_uri and sklearn version 1.0-1 as your estimator_cls in FrameworkProcessor. Furthermore, set header to False in train, validation and test files in your preprocessing script so that you exclude them before you run estimator.fit(). This worked for me.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Input and Output interface for the CatBoost algorithm",
        "Question_creation_time":1658463993278,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-0PVSBTSR4GvFO3E5FusCQ\/input-and-output-interface-for-the-cat-boost-algorithm",
        "Question_topic":[
            "Developer Tools",
            "Security, Identity, & Compliance",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS CodeBuild",
            "AWS CodeDeploy",
            "AWS Artifact",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":66.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"to set up CatBoost Classifier as a built-in algorithm, aws in this [https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/catboost.html] suggested this notebook [https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/lightgbm_catboost_tabular\/Amazon_Tabular_Classification_LightGBM_CatBoost.ipynb] , my question is should I prepare inference file on top of the train.csv? if yes what is that and how it should be prepared?",
        "Answers":[
            {
                "Answer_creation_time":"2022-07-22T15:29:47.312Z",
                "Answer_upvote_count":1,
                "Answer_body":"According to the documentation,[https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/catboost.html] 'The CatBoost built-in algorithm runs in script mode, but the training script is provided for you and there is no need to replace it. If you have extensive experience using script mode to create a SageMaker training job, then you can incorporate your own CatBoost training scripts.' Is the same with the Inference script, all provided artifacts.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-07-22T15:34:44.534Z",
                "Answer_upvote_count":1,
                "Answer_body":"For the built-in algorithms, you can simply specify estimator.deploy(), or tuner.deploy() and the trained model will be deployed to an endpoint for inference.\n\nYou can also bring your own code\/model, in which case, you'll need an inference.py file. See Use your own Inference Code for details.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Canvas failed to import the Redshift Data",
        "Question_creation_time":1641655668805,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCUdYWY0gSV6W60g7ArukXw\/sage-maker-canvas-failed-to-import-the-redshift-data",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon Redshift",
            "Amazon SageMaker Canvas"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":103.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Actions:\n\nThe Redshift connection has been setup on SageMaker Canvas.\nThe Redshift already load the sample data (users, sales, etc)\nDrag and drop table 'users' to import pane.\nCheck the import preview can show the data of 'users'\nClick Import\n\nExpected result:\n\nThe dataset can be imported successfully\n\nActual result: Import failed with below details:\n\n{'message': \"Variable '$input' got invalid value None at 'input.uri'; Expected non-nullable type 'String!' not to be None.\", 'locations': [{'line': 1, 'column': 8}], 'path': None}\n\nPlease contact your admin. Request ID: 8b849887-b067-46fc-9be9-dd122e9c8874",
        "Answers":[
            {
                "Answer_creation_time":"2022-02-16T15:57:31.443Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi there @AWS-User-8556114,\n\nThis could be a mishap in the configuration of the Redshift connector on the Canvas service side. Can you try to forcefully close the Canvas app by deleting the connector first, then logging out of Canvas and\/or by deleting the app (by going into the AWS Management Console, SageMaker, your domain, your profile, and deleting the app with type Canvas), then creating it again?\n\nAlternatively, I'd have to ask you to reach out to Support so that they can help you troubleshoot this by looking into your configuration.\n\nThanks!",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Uploading a Dataframe to AWS S3 Bucket from SageMaker",
        "Question_creation_time":1562042043000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfoMiB7A8SFOpr5uklZZuNg\/uploading-a-dataframe-to-aws-s-3-bucket-from-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":807.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"After successfully uploading CSV files from S3 to SageMaker notebook instance, I am stuck on doing the reverse.\n\nI have a dataframe and want to upload that to S3 Bucket as CSV or JSON. The code that I have is below:\n\nbucket='bucketname'\ndata_key = 'test.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\ndf.to_csv(data_location)\nI assumed since I successfully used pd.read_csv() while loading, using df.to_csv() would also work but it didn't. Probably it is generating error because this way I cannot pick the privacy options while uploading a file manually to S3. Is there a way to upload the data to S3 from SageMaker?",
        "Answers":[
            {
                "Answer_creation_time":"2019-07-02T04:34:23.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"One way to solve this would be to save the CSV to the local storage on the SageMaker notebook instance, and then use the S3 API's via boto3 to upload the file as an s3 object. S3 docs for upload_file() available here.\n\nNote, you'll need to ensure that your SageMaker hosted notebook instance has proper ReadWrite permissions in its IAM role, otherwise you'll receive a permissions error.\n\ncode you already have, saving the file locally to whatever directory you wish\n\nfile_name = \"mydata.csv\"\ndf.to_csv(file_name)\n\ninstantiate S3 client and upload to s3\n\nimport boto3\n\ns3 = boto3.resource('s3')\ns3.meta.client.upload_file(file_name, 'YOUR_S3_BUCKET_NAME', 'DESIRED_S3_OBJECT_NAME')\nAlternatively, upload_fileobj() may help for parallelizing as a multi-part upload.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do I achieve the least-access secure networking for SageMaker Training on Amazon FSx for Lustre?",
        "Question_creation_time":1605279993000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrTkxH_kIT-a_LJSGYS5SXA\/how-do-i-achieve-the-least-access-secure-networking-for-sage-maker-training-on-amazon-f-sx-for-lustre",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI",
            "Networking & Content Delivery"
        ],
        "Question_tag":[
            "Amazon FSx for Lustre",
            "Amazon SageMaker",
            "Networking & Content Delivery",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":56.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I'm trying to figure out a minimally permissive yet operational network configuration for Amazon SageMaker training to train on data from Amazon FSx for Lustre. My understanding is that both the file system and the SageMaker instance can have their own security groups and that FSx uses TCP on ports 988 and 1021-1023. Therefore, I think a good network configuration for using SageMaker with FSx is the following:\n\nSageMaker EC2 equipped with the security group SM-SG that allows Inbound only with TCP on 988 and 1021-1023 from FSX-SG only.\nAmazon FSx equipped with the security group FSX-SG that allows outbound only with TCP on 988 and 1021-1023 towards SM-SG only. Is this configuration enough for the training to work? Do FSx and SageMaker need other ports and sources to be opened to operate normally?",
        "Answers":[
            {
                "Answer_creation_time":"2020-11-13T15:26:19.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"For the security group for Amazon FSx (Example: FSx-SG), you need to add the following additional rules:\n\nFSx-SG needs inbound access from the security group for SageMaker (Example: SM-SG). The SageMaker instance needs to initiate a connection to the Amazon FSx file system, which is an inbound TCP packet to FSx.\nFSx-SG needs inbound and outbound access to itself. This is because, Amazon FSx for Lustre is a clustered file system, where each file system is typically powered by multiple file servers, and the file servers need to communicate with one another.\n\nFor more information on the minimum set of rules required for FSx-SG, see [File system access control with Amazon VPC][1]. [1]: https:\/\/docs.aws.amazon.com\/fsx\/latest\/LustreGuide\/limit-access-security-groups.html",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Ground truth pdf annotation tool not rendering anything",
        "Question_creation_time":1649874349265,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzYkN4V8kRsWhT3ZSNJy9mQ\/sagemaker-ground-truth-pdf-annotation-tool-not-rendering-anything",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth",
            "Amazon Comprehend"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":169.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello, I have followed these docs https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/cer-annotation-pdf.html and have gotten to the point in which I have created the annotation task and I have uploaded several pdf's to a s3 bucket to be used for an annotation task so I can create a comprehend model. I put myself and a co-worker as annotators just so I can verify that I can set up the task properly and I only uploaded 37 pdf's. However when both of us log in and start the task, the webpage loads as the instructions tell us to however there is no pdf rendered (though I think I see it briefly flash on the screen before it goes blank) and there are also no entities to be selected as a part of the ui unlike how the documentation pictures the tool. I am trying to do named entity recognition and created this task with the full 25 entities I want to be able to label and Also another time with only 5 entities to label. However there seems to be something wrong with this native pdf annotation feature.",
        "Answers":[
            {
                "Answer_creation_time":"2022-04-15T22:07:07.140Z",
                "Answer_upvote_count":0,
                "Answer_body":"Dear Customer,\u2028\u2028\u2028\n\nThank you so much for reaching to us. I understand that you followed our AWS Comprehend documentation for annotating PDF\u2019s, in-order to annotate your training PDFs in SageMaker Ground Truth. Further yourself and your team is working as private annotators, in-order to facilitate the task appropriately. However when you and your co-workers login to the page for annotation in Sagemaker Ground truth, the webpage does not list any pdfs and the page is blank.\u2028Hence you were looking for guidance in resolving this issue.\u2028\u2028\n\nThank you so much for providing the details.\u2028\u2028\n\nTo further better assist you on this issue, can you please create a Support Ticket to AWS. Below link will assist you to create the Support Ticket. [+]https:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html\u2028\u2028 [+]https:\/\/console.aws.amazon.com\/support\/home#\/case\/create\n\n\u2014While creating the support ticket, we kindly request you to provide the below information\n\nUse-case description.\nGround Truth Job ARN details\nScreen-shots of the issue you are facing.\nLog Files for the Ground truth Job(This logs from your labeling jobs appear in Amazon CloudWatch under the \/aws\/sagemaker\/LabelingJobs group.).\n\nThe reason behind this ask is this would help us to understand your use-case in a better way, further if we might need to deep dive and access the job created from our internal tools, we will have more visibility through the support ticket.\n\nRest assured we will do everything best in our abilities to assist you on this issue. \u2028\u2028 Thanks.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[Help\/ideas wanted] Serverless Inference: Optimize cold start time",
        "Question_creation_time":1643639465200,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlakvrCXORXyNh7KZehiXKQ\/help-ideas-wanted-serverless-inference-optimize-cold-start-time",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":1.0,
        "Question_view_count":359.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"We are using Sagemaker Serverless Inference, where the endpoint is wrapped with a Lambda that has a 30sec timeout (this timeout is not adjustable). Our cold start time of the model is quite above that (around 43sec). We load a model using Huggingface transformers and have a FLASK API for serving the model. The model size is around 1.75GB.\n\nAre there any guides on how to improve cold start and model loading time? Could we compile the weights differently beforehand for faster loading?",
        "Answers":[
            {
                "Answer_creation_time":"2022-01-31T15:26:17.484Z",
                "Answer_upvote_count":0,
                "Answer_body":"instead of loading model object from a zip file in lambda session. you can load the model object to elastic-cache upfront and load it in lambda instance from elastic-cache. you might need to serialize and deserialize but I think it would still be faster.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-31T15:11:04.633Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi! Thanks for your answer. In theory, that'd be a good idea and could work. However, my other question in this forum then comes into play :D\n\nhttps:\/\/repost.aws\/questions\/QU0JnCsfMHRrSUosWjOiOM9g\/feature-request-serverless-inference-with-vpc-config\n\nServerless Inference currently does not support a VPC configuration. Redis clusters, however, need to be in a VPC.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Ground Truth label - a word at the end of the sentence getting split into 2 parts",
        "Question_creation_time":1668086625248,
        "Question_link":"https:\/\/repost.aws\/questions\/QU50Od2mJZTjyEcGuGds1-qQ\/ground-truth-label-a-word-at-the-end-of-the-sentence-getting-split-into-2-parts",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":15.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"We are annotating the pdf for extrating NER, The targetted entity word at the end of the sentence getting split into 2 parts. First part stay at the end of the first line and second part coming in second line. While annotating the tool doesn't allow the dragging to next line.\n\nAre there any work arounds available ?",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker debugger built in rule CreateXgboostRule not generating report as expected",
        "Question_creation_time":1662562294513,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuZEDvbaeRfqOT_g2Sxlw7w\/sage-maker-debugger-built-in-rule-create-xgboost-rule-not-generating-report-as-expected",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Build & Train ML Models",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":48.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm currently working with a SageMaker hosted XGBoost model; I've added the built in rule \"CreateXgboostRule\" to generate a training report, however, only the ProfilerReport is generated in the S3 rule-output folder - the expected result based on the dev doc is for a CreateXGBoostRule folder as well within this same folder.\n\nThe code I'm using is based directly on the example provided in: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/debugger-training-xgboost-report.html\n\nimport boto3\nimport sagemaker\nfrom sagemaker.estimator import Estimator\nfrom sagemaker import image_uris\nfrom sagemaker.debugger import Rule, rule_configs\n\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\n\nregion = boto3.Session().region_name\nxgboost_container=sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n\nestimator=Estimator(\n    role=sagemaker.get_execution_role()\n    image_uri=xgboost_container,\n    base_job_name=\"debugger-xgboost-report-demo\",\n    instance_count=1,\n    instance_type=\"ml.m5.2xlarge\",\n    \n    # Add the Debugger XGBoost report rule\n    rules=rules\n)\n\nestimator.fit(wait=False)\n\n\nI've tried rewriting the estimator a number of ways, verified \"rules\" is receiving an array of objects, tried different versions of XGBoost within the region, but everything still results in the built in rule only creating the ProfilerReport with no CreateXGBoostRule directory under rule-output.\n\nAny ideas would be greatly appreciated! Thanks.",
        "Answers":[
            {
                "Answer_creation_time":"2022-09-08T13:53:26.107Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Dennis, I tried a code above in AWS SageMaker notebook and didn't get any ProfilerReport as well.\n\nThe possible issue is that there is no data to train hence nothing to report. To prove this hypothesis I decided to take a sample XGBoost notebook and to add report functionality in it. This link tells how to access sample notebooks. I used \"xgboost_customer_churn.ipynb\".\n\nHere some changes that I made:\n\nIn order to be able to run it smoothly, instead of \"1.6-1\" XGBoost image I set the \"1.2-1\":\ncontainer = sagemaker.image_uris.retrieve(\"xgboost\", sess.boto_region_name, \"1.2-1\")\n\nI added reporting to the training cell:\nfrom sagemaker.debugger import Rule, rule_configs\n\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\nsess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(\n    container,\n    role,\n    instance_count=1,\n    instance_type=\"ml.m4.xlarge\",\n    output_path=\"s3:\/\/{}\/{}\/output\".format(bucket, prefix),\n    sagemaker_session=sess,\n    base_job_name=\"debugger-xgboost-report-demo\",\n    rules=rules\n)\n\nxgb.set_hyperparameters(\n    max_depth=5,\n    eta=0.2,\n    gamma=4,\n    min_child_weight=6,\n    subsample=0.8,\n    verbosity=0,\n    objective=\"binary:logistic\",\n    num_round=100,\n)\n\nxgb.fit({\"train\": s3_input_train, \"validation\": s3_input_validation})\n\n\nAll the rest I run without changes.\n\nHowever, it was a little confusing to fing a ProfilerReport. In this particular example it had the path: sagemaker-region-11122233\/sagemaker\/DEMO-xgboost-churn\/output\/debugger-xgboost-report-demo-2022-00-000\/rule-output\/CreateXgboostReport\/\n\nIn order to get this path you can use (run this code in the SageMaker notebook):\n\nxgb.output_path - gives the first part of the path (including the bucket name)\n\nxgb.latest_training_job.job_name - gives the second part of the path (training job name)\n\nIf you combine these two, you will get the full path towards the rule-output directory.\n\nI hope this helps.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Endpoint is not created when deploying HuggingFace Model using it.",
        "Question_creation_time":1657903012949,
        "Question_link":"https:\/\/repost.aws\/questions\/QUT4ywRDmOTO-8YSR4MBrVKg\/sagemaker-endpoint-is-not-created-when-deploying-hugging-face-model-using-it",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Lambda",
            "Amazon SageMaker",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":54.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to deploy the HuggingFace model onto sagemaker. Here is the link for the model: https:\/\/huggingface.co\/dalle-mini\/dalle-mini\n\nI am testing in my personal account and here is the code for the same:\n\nfrom sagemaker.huggingface import HuggingFaceModel\nimport sagemaker\n\nsess = sagemaker.Session()\n# sagemaker session bucket -> used for uploading data, models and logs\n# sagemaker will automatically create this bucket if it not exists\nsagemaker_session_bucket='sagemaker-hugging-face-model-demo'\nif sagemaker_session_bucket == 'sagemaker-hugging-face-model-demo' and sess is not None:\n    # set to default bucket if a bucket name is not given\n    sagemaker_session_bucket = sess.default_bucket()\n\nrole = sagemaker.get_execution_role()\nsess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n\nprint(f\"sagemaker role arn: {role}\")\nprint(f\"sagemaker bucket: {sess.default_bucket()}\")\nprint(f\"sagemaker session region: {sess.boto_region_name}\")\n\n\nhub = {\n    'HF_MODEL_ID':'dalle-mini\/dalle-mini',\n    'HF_TASK':'Text-to-image'\n}\n\nhuggingface_model = HuggingFaceModel(\n  env=hub,\n  role=role,\n  #image_uri=\"428136181372.dkr.ecr.ca-central-1.amazonaws.com\/sagemaker-hugging-face\",\n  transformers_version=\"4.6.1\",     # transformers version used\n  pytorch_version=\"1.7\",          # pytorch version used\n  py_version='py36'\n)\n\n# deploy model to Sagemaker Inference\npredictor = huggingface_model.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m5.xlarge'\n)\n\n\n\nWhen I am trying to create the sagemaker endpoint I am experiencing the error: ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Requested image 428136181372.dkr.ecr.ca-central-1.amazonaws.com\/sagemaker-hugging-face not found.\n\nAlso I need to create a lambda function that will invoke the SageMaker endpoint that will send a text description for which it will return a generated image. E.g. --> The text Sun is shining should be transformed to image after the lambda function invokes the sagemaker endpoint.\n\nAlso need to know what should be the ContentType for image.",
        "Answers":[
            {
                "Answer_creation_time":"2022-07-18T03:49:36.138Z",
                "Answer_upvote_count":0,
                "Answer_body":"I see you have an incorrect-looking image_uri commented-out there...\n\nOne aspect of the SageMaker Python SDK that can be a little confusing at first is there is no direct correspondence between a \"model\" in the SDK (e.g. HuggingFaceModel) and a \"Model\" in the SageMaker APIs (as shown in Inference > Models page of the AWS Console for SageMaker).\n\nThe reason for this is that SDK \"Model\" constructors don't collect quite all the information needed to define API Models: If image_uri is not specified, you don't know until you .deploy() or .transformer() to a particular instance_type, whether you're using a CPU or GPU instance and therefore whether you should be using the CPU or GPU container image... And a specific container image is needed before it can create the API Model. Because of this:\n\nWhen you first ran the code (I guess with image_uri included), the Model was not actually created in SageMaker API\/Console until it reached the .deploy() step\nIn some situations, the SDK might re-use the initially created API Model rather than re-creating it with the new parameters (e.g. are you specifying a specific name that you just removed for publishing the code snippet?)\n\nSo if you removed the explicit image_uri and are still seeing the error about incorrect image URI, I would go in to SageMaker Console and explicitly delete the previous Model to force your code to create it from scratch using the updated params. (Of course there are also API\/SDK ways to do this e.g. huggingface_model.delete_model()). When you just use the HuggingFaceModel class provide the framework version parameters, it should be able to look up the correct URI itself.\n\nSince AWS Lambda runtimes don't have the high-level SageMaker Python SDK installed by default, I'd probably suggest to use plain boto3 SageMakerRuntime invoke_endpoint there (rather than e.g. predictor.predict() as you'll usually see used in notebooks).\n\nI'm not sure yet what format the default pipeline will expect for your image inputs, or even if the default model serving stack is already set up to return images nicely (since Hugging Face has historically mainly been used for text). Possibly you'll need to customize the output processing, which you can do by defining your own output_fn (and even predict_fn, model_fn, input_fn if needed) as documented here. I'd first try sending in your input as application\/json similar to { \"instances\": [\"Sun is shining\"] } with an application\/json Accept header as well, and see what type of response that gets you.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using SageMaker SDK to deploy a open source xgboost model locally",
        "Question_creation_time":1638503327094,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEH97qD5dSjS93XkXTdel8w\/using-sage-maker-sdk-to-deploy-a-open-source-xgboost-model-locally",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":141.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a locally trained model that I am trying to debug locally on docker container before deploying \/ creating endpoint on SageMaker. I am following the documentation that AWS customer service provided, however, I am running into issue with Creating Endpoint Config.\n\nHere's the code snippet:\n\nfrom sagemaker.xgboost import XGBoost, XGBoostModel\nfrom sagemaker.session import Session\n\nsm_client = boto3.client(\n                         \"sagemaker\",\n                         aws_access_key_id='xxxxxx',\n                         aws_secret_access_key='xxxxxx'\n                        )\n\nsagemaker_session = Session(sagemaker_client = sm_client)\n\nxgb_inference_model = XGBoostModel(\n                                   model_data=model_url,\n                                   role=role,\n                                   entry_point=\"inference.py\",\n                                   framework_version=\"0.90-2\",\n                                   sagemaker_session = sagemaker_session     \n)\n\nprint('Deploying endpoint in local mode')\npredictor = xgb_inference_model.deploy(\n                                       initial_instance_count = 1,\n                                       instance_type = \"local\"\n)\n\n\nTraceback:\n\n20 print('Deploying endpoint in local mode')\n21 predictor = xgb_inference_model.deploy(\n22                                        initial_instance_count = 1,\n\nClientError: An error occurred (ValidationException) when calling the CreateEndpointConfig operation: 1 validation error detected: Value 'local' at 'productionVariants.1.member.instanceType' failed to satisfy constraint: Member must satisfy enum value set: [ml.r5d.12xlarge, ml.r5.12xlarge, ml.p2.xlarge, ml.m5.4xlarge, ml.m4.16xlarge, ml.r5d.24xlarge,\n\n\nHere's the documentation link: https:\/\/github.com\/aws-samples\/amazon-sagemaker-local-mode\/blob\/main\/xgboost_script_mode_local_training_and_serving\/xgboost_script_mode_local_training_and_serving.py",
        "Answers":[
            {
                "Answer_creation_time":"2021-12-03T04:54:22.627Z",
                "Answer_upvote_count":4,
                "Answer_body":"Can you try using from sagemaker.local import LocalSession instead of sagemaker.session import Session as specified in the documentation?\n\nhttps:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is it possible to use Lambda functions along with other services to scale up and scale down(probably to 0 instances) Ec2 Deployed apps",
        "Question_creation_time":1649176043559,
        "Question_link":"https:\/\/repost.aws\/questions\/QUl2PMA3JZRU2QmN7_knI7fQ\/is-it-possible-to-use-lambda-functions-along-with-other-services-to-scale-up-and-scale-down-probably-to-0-instances-ec-2-deployed-apps",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Developer Tools",
            "Machine Learning & AI",
            "Application Integration"
        ],
        "Question_tag":[
            "AWS Lambda",
            "AWS CodeDeploy",
            "Amazon SageMaker",
            "Amazon EC2",
            "Amazon EventBridge"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":125.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi there, hope you are fine. Recently I came across Sagemaker Async inference API, there we can scale down even to 0 instances. What I want is that I deploy my solution to EC2 instances using FastAPI, uvicorn and Celery or Rabbit-MQ(as message broker, for queuing). Then I can scale up and scale down instances based on traffic. Also, if that's not the case, then I keep a minimal CPU instance on always and based on that I scale up and scale down GPU instances for handling requests.\n\nThanks , for any help.\n\nBest Regards Muhammad Ali",
        "Answers":[
            {
                "Answer_creation_time":"2022-04-05T17:51:22.995Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, I guess everything is possible in computers world and your case too. But it will take a lot of effort to do it and will not be a best practice. I have not seen somebody scale ec2's with lambda.\n\nI guess all you need is to scale ec2s based on messages in queue (very easy with sqs) or just created a load balancer or use autoscaling group https:\/\/docs.aws.amazon.com\/autoscaling\/ec2\/userguide\/what-is-amazon-ec2-auto-scaling.html\n\nRegards Denys",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-21T16:50:07.121Z",
                "Answer_upvote_count":0,
                "Answer_body":"Scaling with EC2 instances with Lambdas is a common pattern. There are a few options available to assist with this. 1\/ Leverage Systems Manager to Start\/Stop EC2 instances on a schedule. If you want to minimize the cost of EC2 during off hours, say at night and weekends, this is a simple solution. 2\/ Cloudwatch events. You can set up CloudWatch events that trigger based upon a cron or usage of EC2. If using cron, that would trigger at a certain interval, say once a day, to run a Lambda that will shut down the EC2 instance. There are several examples you can search online that shows the code for that Lambda. If you want to have more flexibility, using CloudWatch telemetry, such as CPU utilization, to trigger a shutdown, or even start new instances. 3\/ An auto-scaling group is a great way to do the same. You will set up the parameters for scale up\/scale down, and the auto-scaling group will manage that for you. 4\/ If you want to manage the size of all the services as an entire platform, then using containers managed by Fargate can be a solution. --You can combine these solutions to reduce consumption based upon your needs. Let's say you configure an auto-scaling group that will run during your work hours that has a minimum size of 1. At night and weekends, you have a cron that shuts down the auto-scaling group during off hours. An hour or two before work hours, a cron runs to start the auto-scaling group so you have warm instances ready to go.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"S3 Dataset versioning with SageMaker?",
        "Question_creation_time":1549396058000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhYC1EJQuSWqpwTByAtB_fg\/s-3-dataset-versioning-with-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":386.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"Is there any standard for ML S3 dataset tracking or versioning? Basically, what setup allows to track a given model training execution to a given dataset? Interested to hear about proven or state-of-the-art ideas",
        "Answers":[
            {
                "Answer_creation_time":"2019-02-06T07:18:29.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Unfortunately, managing versions of datasets and which models used them is not embedded in SageMaker. But, you can use SageMaker search to manage the differences in data location between experiments. In that case, if your dataset isn't too big, my recommendation will be to create a standard for data structure in S3. i.e. for each new dataset, create a new prefix in S3 with your logic. Using SageMaker search you'll be able to find all your jobs and compare between datasets.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-09-28T12:51:12.007Z",
                "Answer_upvote_count":0,
                "Answer_body":"Nowadays, there are 3rd party tool that can be used alongside SageMaker. One example is Data Version Control (DVC), and we have discussed it how to integrate within SageMaker Processing jobs and SageMaker Training Jobs in this blogpost. As an alternative, you can leverage SageMaker Pipelines when your data preparation step is executed as a processing step within a pipeline execution. Pipelines allows you to achieve data versioning in a programmatic way by using execution-specific variables like ExecutionVariables.PIPELINE_EXECUTION_ID, which is the unique ID of a pipeline run. We can, for example, create a unique key for storing the output datasets in S3 that ties them to a specific pipeline run. We have also discussed this possibility as part of this blogpost.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS StepFunctions - SageMaker's InvokeEndpoint block throws \"validation error\" when fetching parameters for itself inside iterator of Map block",
        "Question_creation_time":1647503861594,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDc1foN9TQhe3OYkkGzCKhQ\/aws-step-functions-sage-makers-invoke-endpoint-block-throws-validation-error-when-fetching-parameters-for-itself-inside-iterator-of-map-block",
        "Question_topic":[
            "Serverless",
            "Application Integration",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "AWS Step Functions",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":110.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I have a state-machine workflow with 3 following states:\n\nscreenshot-of-my-workflow\n\nA 'Pass' block that adds a list of strings(SageMaker endpoint names) to the original input. (this 'Pass' will be replaced by a call to DynamoDB to fetch list in future.)\nUse map to call SageMaker endpoints dictated by the array(or list) from above result.\nSend the result of above 'Map' to a Lambda function and exit the workflow.\n\nHere's the entire workflow in .asl.json, inspired from this aws blog.\n\n{\n  \"Comment\": \"A description of my state machine\",\n  \"StartAt\": \"Pass\",\n  \"States\": {\n    \"Pass\": {\n      \"Type\": \"Pass\",\n      \"Next\": \"InvokeEndpoints\",\n      \"Result\": {\n        \"Endpoints\": [\n          \"sagemaker-endpoint-1\",\n          \"sagemaker-endpoint-2\",\n          \"sagemaker-endpoint-3\"\n        ]\n      },\n      \"ResultPath\": \"$.EndpointList\"\n    },\n    \"InvokeEndpoints\": {\n      \"Type\": \"Map\",\n      \"Next\": \"Post-Processor Lambda\",\n      \"Iterator\": {\n        \"StartAt\": \"InvokeEndpoint\",\n        \"States\": {\n          \"InvokeEndpoint\": {\n            \"Type\": \"Task\",\n            \"End\": true,\n            \"Parameters\": {\n              \"Body\": \"$.InvocationBody\",\n              \"EndpointName\": \"$.EndpointName\"\n            },\n            \"Resource\": \"arn:aws:states:::aws-sdk:sagemakerruntime:invokeEndpoint\",\n            \"ResultPath\": \"$.InvocationResult\"\n          }\n        }\n      },\n      \"ItemsPath\": \"$.EndpointList.Endpoints\",\n      \"MaxConcurrency\": 300,\n      \"Parameters\": {\n        \"InvocationBody.$\": \"$.body.InputData\",\n        \"EndpointName.$\": \"$$.Map.Item.Value\"\n      },\n      \"ResultPath\": \"$.InvocationResults\"\n    },\n    \"Post-Processor Lambda\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::lambda:invoke\",\n      \"Parameters\": {\n        \"Payload.$\": \"$\",\n        \"FunctionName\": \"arn:aws:lambda:<my-region>:<my-account-id>:function:<my-lambda-function-name>:$LATEST\"\n      },\n      \"Retry\": [\n        {\n          \"ErrorEquals\": [\n            \"Lambda.ServiceException\",\n            \"Lambda.AWSLambdaException\",\n            \"Lambda.SdkClientException\"\n          ],\n          \"IntervalSeconds\": 2,\n          \"MaxAttempts\": 6,\n          \"BackoffRate\": 2\n        }\n      ],\n      \"End\": true\n    }\n  }\n}\n\n\nAs can be seen in the workflow, I am iterating over the list from the previous 'Pass' block and mapping those to iterate inside 'Map' block and trying to access the Parameters of 'Map' block inside each iteration. Iteration works fine with number of iterators, but I can't access the Parameters inside the iteration. I get this error:\n\n{\n  \"resourceType\": \"aws-sdk:sagemakerruntime\",\n  \"resource\": \"invokeEndpoint\",\n  \"error\": \"SageMakerRuntime.ValidationErrorException\",\n  \"cause\": \"1 validation error detected: Value '$.EndpointName' at 'endpointName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9])* (Service: SageMakerRuntime, Status Code: 400, Request ID: ed5cad0c-28d9-4913-853b-e5f9ac924444)\"\n}\n\n\nSo, I presume the error is because \"$.EndpointName\" is not being filled with the relevant value. How do I avoid this.\n\nBut, when I open the failed execution and check the InvokeEndpoint block from graph-inspector, input to that is what I expected and above JSON-Paths to fetch the parameters should work, but they don't.\nscreenshot-of-graph-inspector\n\nWhat's causing the error and How do I fix this?",
        "Answers":[
            {
                "Answer_creation_time":"2022-03-17T10:46:07.263Z",
                "Answer_upvote_count":1,
                "Answer_body":"In general (as mentioned here in the parameters doc), you also need to end the parameter name with .$ when using a JSON Path.\n\nIt looks like you're doing that some places in your sample JSON (e.g. \"InvocationBody.$\": \"$.body.InputData\"), but not in others (\"EndpointName\": \"$.EndpointName\"), so I think the reason you're seeing the validation error here is that Step Functions is trying to interpret $.EndpointName as literally the name of the endpoint (which doesn't satisfy ^[a-zA-Z0-9](-*[a-zA-Z0-9])*!)\n\nSo suggest you change to EndpointName.$ and Body.$ in your InvokeEndpoint parameters",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Line magic error in SageMaker Studio",
        "Question_creation_time":1658670880505,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZPg9WRBtSwGvYhI9-3tscA\/line-magic-error-in-sage-maker-studio",
        "Question_topic":[
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":60.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi AWS, I am trying to create virtual environment in SageMaker Studio but while doing so I am experiencing a line magic function error. Also I am not able to import the libraries. I am attaching the error screenshot for the same.\n\nThanks",
        "Answers":[
            {
                "Answer_creation_time":"2022-07-25T08:28:37.568Z",
                "Answer_upvote_count":0,
                "Answer_body":"As far as I can tell, %virtualenv and %import are not standard line magics in IPython - and from a quick search around, I couldn't see what package implements them?\n\nBut I think you might want to consider more fundamental changes to your workflow and will try to make a case for it...\n\nIn SageMaker Studio, as the other answer noted already, kernel environments are containerized - rather than conda-based. This is nicely consistent with how SageMaker job environments (training, deployment, processing, etc) are also container-based.\n\nAs a result, my usual environment management suggestions would be:\n\nFor quickly and interactively experimenting with different library installations in the notebook, simply use !pip shell commands to install\/edit packages directly in the running container. (e.g. !pip install abc==x.y.z, !pip show abc, and so on). You could even save these dependencies in a requirements.txt and !pip install from that file (which would be nice because SageMaker script-mode jobs can accept a requirements.txt).\nFor formalizing an environment to be standard, repeatable, and shareable between users without them having to re-run the install commands: set up a custom kernel container image.\n\nAs outlined here, all your open notebooks with the same kernel and instance type selected will share a running container. When you delete\/restart the \"app\" (container), these changes will be lost. Keeping running\/customized kernel environments as disposable as possible, and trying to use SageMaker jobs (training, processing, etc) early in the workflow instead of sticking everything in notebook, helps to prevent reproducibility problems.\n\nBecause they're containerized, I find I don't really need to worry about environment management\/virtualization technologies: In notebook experimentation, my different kernels are fully separated anyway and I can just restart the container (\"app\") to reset. In SageMaker jobs there's only one task being run in the container, so it doesn't need to play nice with other workloads. It's different, and actually quite nice, compared to working on my local laptop where I have to carefully manage separate project environments and fix things if they break!\n\nEdit to add: The following works for me... Perhaps your problem was that the code folder didn't exist yet? os.makedirs() or !mkdir or UI actions can fix that:\n\n%%writefile src\/main.py\n\nimport os",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-28T04:44:00.831Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nWhen running a notebook on SageMaker Studio you can select the default environment you want by clicking on the top right corner on the kernel you currently have enabled (Default should be Data Science) and select the one you want from the dropdown.\n\nIn the case you want to install more\/custom packages, then you can install those straight from your notebook using one of the valid magic cell commands. These are %conda install and %pip install (see doc: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-notebooks-add-external.html)\n\nAlternatively you can open a terminal and use your tool of choice to manage the python packages installed.\n\nIn terms of importing packages, the correct syntax for that is without the %. So please instead of %import <package_name> try import <package_name>.\n\nLet us know if this solved your issue,\n\nhave a nice day",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to create Keras's encoder-decoder model's endpoint?",
        "Question_creation_time":1557413472000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUa1eV25XxRjKhLPHVXe9TKQ\/how-to-create-kerass-encoder-decoder-models-endpoint",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":31.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\nI'm just started to use sagemaker.\nNow, I'm testing encoder-decoder model for regression.\nThe model is coded in Sagemaker's script mode, and finished learning on a Jupyter notebook.\n\nThe learning code is written by keras + tensorflow, and the model is based on the encoder decoder model. It is similar to the Keras's seq2seq example linked below.\nBelow, I will take this as an example.\nhttps:\/\/keras.io\/examples\/lstm_seq2seq\/\n\nIn the above model, there are \"encoder_model\" and \"decoder_model\" apart from \"model\" to be trained, and in the inference, \"encoder_model\" and \"decoder_model\" are used to generate the prediction by the function \"decode_sequence (input_seq)\".\nI would like to deploy this function \"decode_sequence (input_seq)\" as an endpoint, but it doesn't work as usual with estimator.deploy () and I don't know how to implement it.\n\nIs there any sample code or resources to solve this?\nThanks in advance.",
        "Answers":[
            {
                "Answer_creation_time":"2019-05-20T00:36:29.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks, the problem was self resolved.\nWhat should I do was to code \"decode sequence\" withint the \"keras_model_fn\".",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Input Manifest Errors in Sagemaker Ground Truth for Custom Labeling Job",
        "Question_creation_time":1655745668932,
        "Question_link":"https:\/\/repost.aws\/questions\/QUn7gIM_MkSHmd9IzuV4_pmw\/input-manifest-errors-in-sagemaker-ground-truth-for-custom-labeling-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth",
            "Amazon Comprehend"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":77.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am attempting to create a native PDF annotation labeling job for use with Comprehend to identify entities within similar documents. I have around 20 pdf files, all of them around 100-300 pages long.\n\nI used the tools and followed the directions from this blog post. I've struggled a little with the tools but ultimately got everything working.\n\nMy problem comes from the labeling job itself. When I open the labeling job in a private workforce that I've created, I find only a blank page. I did some research and found that there is something wrong with the input manifest as it seems AWS isn't able to parse it for some reason.\n\nI checked my manifest and found that it was generated as multiple objects. Each object was a single page from my PDFs. This seems normal, however the objects were not put into a list or 'top level' object, which does not fit JSON Lines guidelines. I attempted a quick fix of placing these objects all within a list (which satisfies JSON Lines) but it does not seem to help.\n\nAny suggestions or advice would be greatly appreciated.",
        "Answers":[
            {
                "Answer_creation_time":"2022-07-19T06:55:04.062Z",
                "Answer_upvote_count":0,
                "Answer_body":"Dear Customer,\u2028\u2028\u2028\n\nThank you so much for reaching to us. I understand that you followed our AWS Comprehend documentation for annotating PDF\u2019s, in-order to annotate your training PDFs in SageMaker Ground Truth. However, there were some issues with the tool showing a blank page in the UI, and you are assuming it may be an issue with the input manifest. Hence you were looking for guidance in resolving this issue.\u2028\u2028\n\nThank you so much for providing the details.\u2028\u2028\n\nTo further better assist you on this issue, can you please create a Support Ticket to AWS. Below link will assist you to create the Support Ticket. [+]https:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html\u2028\u2028 [+]https:\/\/console.aws.amazon.com\/support\/home#\/case\/create\n\n\u2014While creating the support ticket, we kindly request you to provide the below information\n\nUse-case description.\nGround Truth Job ARN details\nScreen-shots of the issue you are facing.\nLog Files for the Ground truth Job(This logs from your labeling jobs appear in Amazon CloudWatch under the \/aws\/sagemaker\/LabelingJobs group.).\n\n\nThe reason behind this ask is this would help us to understand your use-case in a better way, further if we might need to deep dive and access the job created from our internal tools, we will have more visibility through the support ticket.\n\nRest assured we will do everything best in our abilities to assist you on this issue. \u2028\u2028 Thanks.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"mxnet error encountered in Lambda Function",
        "Question_creation_time":1638553590411,
        "Question_link":"https:\/\/repost.aws\/questions\/QUW1vSlOVxRC2pGlTEXs0Z2w\/mxnet-error-encountered-in-lambda-function",
        "Question_topic":[
            "Storage",
            "Serverless",
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon Simple Storage Service",
            "AWS Lambda",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":69.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I trained and deployed a semantic segmentation network (mlp2.xlarge) using SageMaker. I wanted to use an AWS Lambda function to send an image to this endpoint and get a mask in return however when I use invoke_endpoint it gives an mxnet error in the logs. Funnily when I use the deployed model from a transformer object from inside the SageMaker notebook the mask is returned properly. Here is my Lambda function code:\n\nimport json\nimport boto3\n\ns3r = boto3.resource('s3')\n\ndef lambda_handler(event, context):\n    # TODO implement\n    \n    bucket = event[\"body\"]\n    key = 'image.jpg'\n    local_file_name = '\/tmp\/'+key\n    s3r.Bucket(bucket).download_file(key, local_file_name)\n\n    runtime = boto3.Session().client('sagemaker-runtime')\n\n    with open('\/tmp\/image.jpg', 'rb') as imfile:\n        imbytes = imfile.read()\n\n    # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given\n    response = runtime.invoke_endpoint(\n    EndpointName='semseg-2021-12-03-10-05-58-495', \n    ContentType='application\/x-image', \n    Body=bytearray(imbytes))                       # The actual image\n\n    # The response is an HTTP response whose body contains the result of our inference\n    result = response['Body'].read()\n    \n    return {\n        'statusCode': 200,\n        'body': json.dumps(result)\n    }\n\n\nHere are the errors I see in the logs: mxnet.base.MXNetError: [10:26:14] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.4.x.4276.0\/AL2_x86_64\/generic-flavor\/src\/3rdparty\/dmlc-core\/src\/recordio.cc:12: Check failed: size < (1 << 29U) RecordIO only accept record less than 2^29 bytes",
        "Answers":[
            {
                "Answer_creation_time":"2021-12-03T20:37:40.178Z",
                "Answer_upvote_count":0,
                "Answer_body":"If you are using the built-in Sagemaker algorithm for Semantic Segmentation , the content type must be \"image\/jpeg\" for inference to accept images. For more details, see https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/semantic-segmentation.html",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"how to choose an instance type for a sagemaker testing\/inference?",
        "Question_creation_time":1650126925753,
        "Question_link":"https:\/\/repost.aws\/questions\/QULxw59aBCRfmso_f7-VCjRQ\/how-to-choose-an-instance-type-for-a-sagemaker-testing-inference",
        "Question_topic":[
            "Machine Learning & AI",
            "Compute"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Spot Instances"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":225.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"looking at few examples, for training in sagemaker . are there some guidelines based on the model size, data to be trained , what type of instance cpu\/gpu to use? also, can one use spot instances ( may be with multiple gpu cores)?",
        "Answers":[
            {
                "Answer_creation_time":"2022-04-16T20:54:39.046Z",
                "Answer_upvote_count":1,
                "Answer_body":"Yes, you can use spot instances. I recommend it, and always run training on spot instances. If you are using the Python SDK, add the following parameters to your Estimator:\n\n       use_spot_instances=True,\n       max_run={maximum runtime here},\n       max_wait={maximum wait time},\n       checkpoint_s3_uri={URI of your bucket and folder },\n\n\nSee the documentation for more details here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-managed-spot-training.html\n\nAs far as instance types are concerned, the individual algorithms contain some initial recommendations for instances types: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\n\nFor example, see the EC2 Instance Recommendation for the Image Classification Algorithm: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\n\nThere was a presentation at re:Invent 2020 - How to choose the right instance type for ML inference: https:\/\/www.youtube.com\/watch?v=0DSgXTN7ehg\n\nHope this helps",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-06T20:32:34.069Z",
                "Answer_upvote_count":1,
                "Answer_body":"And for the selection of instance type for inference, you might want to look at Amazon SageMaker Inference Recommender:\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-recommender.html",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker instances keep awakening and charge the credit",
        "Question_creation_time":1653535822137,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjCMOSHaPR4WwWP1SoFzzng\/sagemaker-instances-keep-awakening-and-charge-the-credit",
        "Question_topic":[
            "Cloud Financial Management",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Cost and Usage Report",
            "Amazon SageMaker",
            "Amazon SageMaker Data Wrangler"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":306.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have tried Data Wrangler in Sagemaker last month and close the service. A few weeks later I have noticed the credit was charge $1 every hour and just realized that the Data Wranger auto-save the flow every minute. So, I deleted the unsaved flow and shut down all the services and instances according to advice on these two links :\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-lab-use-shutdown.html\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-cleanup.html\n\nThen, I left the Sagemaker untouched for the whole month of May, and just got back to the console yesterday. This is what I found out for May's bill:\n\nAmazon SageMaker RunInstance $531.74\nDetail\tUsage\tTotal\n$0.00 for Host:ml.m5.xlarge per hour under monthly free tier\t125.000 Hrs\t$0.00\n$0.00 for Notebk:ml.t2.medium per hour under monthly free tier\t107.056 Hrs\t$0.00\n$0.00 per Data Wrangler Interactive ml.m5.4xlarge hour under monthly free tier\t25.000 Hrs\t$0.00\n$0.23 per Hosting ml.m5.xlarge hour in US East (N. Virginia)\t88.997 Hrs\t$20.47\n$0.922 per Data Wrangler Interactive ml.m5.4xlarge hour in US East (N. Virginia)\t554.521 Hrs\t$511.27\n\nSo, with another attempt, I installed an extension to automatically shut down idle kernels and set the limit to 10 min from advice here: https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-costs-by-automatically-shutting-down-idle-resources-within-amazon-sagemaker-studio\/ Checked the cost in usage report, it turns out that the service was shut down after installing the extension but then it revoked itself after 5 hours later (during my sleep time). There's still cost from Studio although with less charge than previous one.\n\nService\tOperation\tUsageType\tStartTime\tEndTime\tUsageValue\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/24\/2022 23:00\t5\/25\/2022 0:00\t1\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/25\/2022 0:00\t5\/25\/2022 1:00\t1\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/25\/2022 1:00\t5\/25\/2022 2:00\t1\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/25\/2022 2:00\t5\/25\/2022 3:00\t0.76484417\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/25\/2022 8:00\t5\/25\/2022 9:00\t0.36636722\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/25\/2022 9:00\t5\/25\/2022 10:00\t0.38959556\n\nDuring this time, I'm sure that there're no running instances, running apps, kernel sessions or terminal sessions. I even deleted the user profile. Last thing I haven't tried is to set up scheduled shutdown coz I think the services should not cause difficulty to our life that much. Any advice for any effective action to completely shutdown the Sagemaker instance? Thanks.",
        "Answers":[
            {
                "Answer_creation_time":"2022-05-26T16:33:36.084Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, you can shut down SageMaker Studio resources per the documentation here - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks-run-and-manage-shut-down.html (you've linked to Studio Lab documentation, so making sure). In addition, I see you've been charged for hosting and you've since deleted the endpoints, and don't see the hosting charges after deleting them.\n\nFor data wrangler, once you have the flow saved, you need to shut down the app (closing the window does not automatically shut down the app). Note that if you open the DW flow later, it does start a compute instance, which you'll then have to shut down.\n\nIf you've deleted the user profile (and associated apps), you shouldn't be seeing any more Studio charges for that user profile. If you still see the DW charges (and there's no other user profile), please cut a ticket to support for further investigation.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How long does it take for AWS tech support team to respond to a \"system impaired\" issue?",
        "Question_creation_time":1656580942341,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFgnjt9J3T0iXhE0axG10vQ\/how-long-does-it-take-for-aws-tech-support-team-to-respond-to-a-system-impaired-issue",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Support Case"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":76.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi all,\n\nI have raised a ticket for multiple issues we've been having with SageMaker lately, the ticket was created more than 36 hours ago, and I have not had any response, in fact the ticket hasn't even been assigned yet.\n\nThe case ID is 10300240931.\n\nI thought AWS guarantee a response under 12 hours for \"system impaired\" issues, does anyone know what I can do to accelerate this?\n\nthank you! Ruoy",
        "Answers":[
            {
                "Answer_creation_time":"2022-06-30T10:34:31.689Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Ruoy! My advice here is to scale this issue via your account team, they will have the mechanisms to scale this concern. If you are on basic or developer support, you could look into upgrading to business support for a day and open a live chat with support! Hope this helps",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Extending Docker image for SageMaker Inference",
        "Question_creation_time":1655198456555,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxESyB86cSMCN3dOUMyi4cw\/extending-docker-image-for-sage-maker-inference",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":217.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm trying to create my own Docker image for use with SageMaker Batch Transform by extending an existing one. Following the documentation at https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/prebuilt-containers-extend.html, I have created the following to run Detectron 2:\n\nFROM 763104351884.dkr.ecr.eu-west-2.amazonaws.com\/pytorch-inference:1.10.2-gpu-py38-cu113-ubuntu20.04-sagemaker\n\n############# Installing latest builds ############\nRUN pip install --upgrade torch==1.10.2+cu113 torchvision==0.11.3+cu113 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\n\nENV FORCE_CUDA=\"1\"\n# Build D2 only for Turing (G4) and Volta (P3) architectures. Use P3 for batch transforms and G4 for inference on endpoints\nENV TORCH_CUDA_ARCH_LIST=\"Turing;Volta\"\n\n# Install Detectron2\nRUN pip install \\\n   --no-cache-dir pycocotools~=2.0.0 \\\n   --no-cache-dir https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu113\/torch1.10\/detectron2-0.6%2Bcu113-cp38-cp38-linux_x86_64.whl\n   \n# Set a fixed model cache directory. Detectron2 requirement\nENV FVCORE_CACHE=\"\/tmp\"\n\n############# SageMaker section ##############\n\nENV PATH=\"\/opt\/ml\/code:${PATH}\"\n\nCOPY inference.py \/opt\/ml\/code\/inference.py\n\nENV SAGEMAKER_SUBMIT_DIRECTORY \/opt\/ml\/code\nENV SAGEMAKER_PROGRAM inference.py\n\n\nI then create a model (create-model) with this image using the following configuration:\n\n{\n\"ExecutionRoleArn\": \"arn:aws:iam::[redacted]:role\/model-role\",\n\"ModelName\": \"model-test\",\n\"PrimaryContainer\": { \n  \"Environment\": {\n    \"SAGEMAKER_PROGRAM\": \"inference.py\",\n    \"SAGEMAKER_SUBMIT_DIRECTORY\": \"\/opt\/ml\/code\",\n    \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\",\n    \"SAGEMAKER_REGION\": \"eu-west-2\",\n    \"MMS_DEFAULT_RESPONSE_TIMEOUT\": \"500\"\n   },\n  \"Image\": \"[redacted].dkr.ecr.eu-west-2.amazonaws.com\/my-image:latest\",\n  \"ModelDataUrl\": \"s3:\/\/[redacted]\/training\/output\/model.tar.gz\"\n}\n}\n\n\nAnd submit a batch transform job (create-transform-job) using the following configuration:\n\n{\n\"MaxPayloadInMB\": 16,\n\"ModelName\": \"model-test\",\n\"TransformInput\": { \n    \"ContentType\": \"application\/x-image\",\n    \"DataSource\": { \n      \"S3DataSource\": { \n          \"S3DataType\": \"ManifestFile\",\n          \"S3Uri\": \"s3:\/\/[redacted]\/manifests\/input.manifest\"\n      }\n    }\n},\n\"TransformJobName\": \"transform-test\",\n\"TransformOutput\": { \n    \"S3OutputPath\": \"s3:\/\/[redacted]\/predictions\/\"\n},\n\"TransformResources\": { \n    \"InstanceCount\": 1,\n    \"InstanceType\": \"ml.m5.large\"\n}\n}\n\n\nBoth of the above commands submit fine, but the transform job doesn't complete. When I look in the logs, the errors I'm getting seem to indicate that it's not using my inference script (inference.py, specified above) but is instead using the default script (default_pytorch_inference_handler.py) and therefore can't find the model.\n\nWhat am I missing so that it uses my inference script instead, and hence my model?",
        "Answers":[
            {
                "Answer_creation_time":"2022-06-16T20:50:39.142Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nThank you for using AWS SageMaker.\n\nIt is difficult to identify why this behavior is observed without any logs for the mentioned task under your account. Looking at the above snippet shared, I was able to identify that the extending docker image used is based on GPU instance \"pytorch-inference:1.10.2-gpu-py38-cu113-ubuntu20.04-sagemaker \" and the Batch transform job that was created was using CPU instances i.e (\"InstanceType\": \"ml.m5.large\").\n\nI'd recommend to fix that configuration and try running the batch transform job once again. If you still observe similar issue, I'd recommend you to reach out to AWS Support for further investigation of the issue along with all the details and logs as sharing logs is not recommended to share on this platform.\n\nOpen a support case with AWS using the link: https:\/\/console.aws.amazon.com\/support\/home?#\/case\/create",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker metrics persistence",
        "Question_creation_time":1578643279000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU197giXXuRn-4Hz56HZZpSw\/sage-maker-metrics-persistence",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":103.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Quick questions on ML metrics persistence from sagemaker training tasks. The SageMaker regexp-over-CloudWatch is an attractive option, yet the metric retention in Cloudwatch seems to be restricted to 15 days.\n\nHow to persist those metrics longer? Is it common to extract them out of Cloudwatch regularly to persist them somewhere else, eg S3 or an RDS? what is the best practice for long-term persistence of those metrics?\nWould SageMaker Experiments allow a collection of similar data (customer-defined training metrics) over a longer retention?",
        "Answers":[
            {
                "Answer_creation_time":"2020-01-10T18:32:10.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can now persist algorithm metrics from SageMaker Training Jobs (the ones you can collect with regexes or the ones available from built-in algorithms by default) by setting EnableSageMakerTimeSeriesMetrics through the AWS SDK or enable_sagemaker_metrics=true in the SageMaker Python SDK. These metrics are persisted long term, and available through Amazon SageMaker Studio. (Go to \"Metrics\" -> \"Add Chart\" from the detail page of a training job). These are available at no additional cost.\n\nYes, SageMaker Experiments allow collection of similar data\n\nNote that system metrics (CPU\/GPU\/Memory\/Disk) are still available only through CloudWatch.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Create endpoint from Python",
        "Question_creation_time":1625083671000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUTyUMHH4QRDaMa6L24rhOMg\/create-endpoint-from-python",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":83.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"Hello,\n\nI have trained my model on sagemaker. I have deleted the endpoint, but I am keeping the model and the endpoint configuration which points to the model.\n\nFrom the sagemaker dashboard I am able to recreate the endpoint using the existing endpoint configuration. However I don't want to keep the endpoint on all the time, as I will use it only once a day for a few minutes.\n\nIs it possible to create in on demand from a Python script? I would assume that it is possible, but can't find how. Can someone point me in the right direction?\n\nRegards.",
        "Answers":[
            {
                "Answer_creation_time":"2021-07-22T16:36:17.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello hugoflores,\n\nYou can use SageMaker APIs - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DeleteEndpoint.html to delete the endpoint and https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpoint.html. to create an endpoint. This an be automated either using SageMaker Pipelines or a Lambda function.\n\nHere are a few resources towards that:\n\nhttps:\/\/awsfeed.com\/whats-new\/machine-learning\/build-a-ci-cd-pipeline-for-deploying-custom-machine-learning-models-using-aws-services\nhttps:\/\/github.com\/aws-samples\/aws-lambda-layer-create-script\nhttps:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1200\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\nhttps:\/\/www.sagemakerworkshop.com\/step\/deploymodel\/\n\nHTH,\n\nChaitanya",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-08-17T16:35:53.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks Chaitanya,\n\nI was able to create the endpoint using the \"create_endpoint\" method and following one of the links provided.\n\ndef create_endpoint(endpoint_name, config_name):\r\n    \"\"\" Create SageMaker endpoint with input endpoint configuration.\r\n    Args:\r\n        endpoint_name (string): Name of endpoint to create.\r\n        config_name (string): Name of endpoint configuration to create endpoint with.\r\n    Returns:\r\n        (None)\r\n    \"\"\"\r\n    try:\r\n        sagemaker.create_endpoint(\r\n            EndpointName=endpoint_name,\r\n            EndpointConfigName=config_name\r\n        )\r\n    except Exception as e:\r\n        print(e)\r\n        print('Unable to create endpoint.')\r\n        raise(e)\r\n\r\nname = 'name-of-the-endpoint'\r\nconfig = 'name-of-the-endpoint-config' #this one has to exist on the Endpoint configuration list on sagemaker\r\ncreate_endpoint(name, config)\n\nRegards\n\nEdited by: hugoflores on Aug 17, 2021 9:36 AM",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"loading and deploying a previously trained sagemaker xgboost model",
        "Question_creation_time":1563308650000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6Cm7BTSlQ1GtLEzqVeGftQ\/loading-and-deploying-a-previously-trained-sagemaker-xgboost-model",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":475.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to write an inference pipeline where I load a previously trained sagemaker xgboost model stored in s3 as a tar.gz file (following sagemaker tutorial) and deploy it as an endpoint for prediction. Here is my code:\n\ntrainedmodel = sagemaker.model.Model(    \r\n    model_data='data-path-to-my-model-in-s3\/model.tar.gz',\r\n    image=container,  \r\n    role=role)  \r\n\r\nxgb_predictor = trainedmodel.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n\n\nThe code runs fine but after that when I try to call predict() on xgb_predictor I get an error saying 'NoneType' object has no attribute 'predict'. I followed the example here to train the xgboost model:\n\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/simplify-machine-learning-with-xgboost-and-amazon-sagemaker\/\n\nWhy am I getting this error? What's the correct way to load a previously trained model? Help would be appreciated.",
        "Answers":[
            {
                "Answer_creation_time":"2020-10-30T20:06:16.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"thanks for using SageMaker! you're on the right path - you'll need to pass in an argument for \"predictor_cls\" when creating your Model instance in order for a predictor object to be returned after calling deploy(), e.g.\n\nfrom sagemaker.model import Model\r\nfrom sagemaker.predictor import RealTimePredictor, csv_serializer, csv_deserializer\r\n\r\nclass Predictor(RealTimePredictor):\r\n    def __init__(self, endpoint_name, sagemaker_session=None):\r\n        super(Predictor, self).__init__(\r\n            endpoint_name, sagemaker_session, csv_serializer, csv_deserializer\r\n        )\r\n\r\ntrainedmodel = Model(..., predictor_cls=Predictor)\r\nxgb_predictor = trainedmodel.deploy(...)\r\n\r\nxgb_predictor.predict(...)\n\n\nAPI reference:\n\nhttps:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html\nhttps:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\n\nhope that helps!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-09-09T17:50:51.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thank you. This solution worked!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-07-19T21:57:48.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Any special reason for using csv serializer\/deserializer? In my case I reload a model to analyze videos (frames in numpy array actually). What serializer\/deserializer should I use?\nActually, any doc regarding how to properly use the argument predictor_cls would be highly appreicated.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"deploying previously trained model with Sagemaker Python SDK (StatusExceptionError)",
        "Question_creation_time":1661503967725,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXT-lr_7ASxSSzx0lRAaEvg\/deploying-previously-trained-model-with-sagemaker-python-sdk-status-exception-error",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Model Training",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":32.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am using a pertained Random Forest Model and trying to deploy it on Amazon Sagemker using Python SDK:\n\nfrom sagemaker.sklearn.estimator import SKLearn\n\nsklearn_estimator = SKLearn(\n    entry_point='script.py',\n    role = get_execution_role(),\n    instance_count=1,\n    instance_type='ml.m4.xlarge',\n    framework_version='0.20.0',\n    base_job_name='rf-scikit')\n\nsklearn_estimator.fit({'train':trainpath, 'test': testpath}, wait=False)\n\nsklearn_estimator.latest_training_job.wait(logs='None')\nartifact = m_boto3.describe_training_job(\n    TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n\nprint('Model artifact persisted at ' + artifact)\n\n\nI get the following StatusException Error\n\n2022-08-25 12:03:27 Starting - Starting the training job....\n2022-08-25 12:03:52 Starting - Preparing the instances for training............\n2022-08-25 12:04:55 Downloading - Downloading input data......\n2022-08-25 12:05:31 Training - Downloading the training image.........\n2022-08-25 12:06:22 Training - Training image download completed. Training in progress..\n2022-08-25 12:06:32 Uploading - Uploading generated training model.\n2022-08-25 12:06:43 Failed - Training job failed\n---------------------------------------------------------------------------\nUnexpectedStatusException                 Traceback (most recent call last)\n<ipython-input-37-628f942a78d3> in <module>\n----> 1 sklearn_estimator.latest_training_job.wait(logs='None')\n      2 artifact = m_boto3.describe_training_job(\n      3     TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n      4 \n      5 print('Model artifact persisted at ' + artifact)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n   2109             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   2110         else:\n-> 2111             self.sagemaker_session.wait_for_job(self.job_name)\n   2112 \n   2113     def describe(self):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_job(self, job, poll)\n   3226             lambda last_desc: _train_done(self.sagemaker_client, job, last_desc), None, poll\n   3227         )\n-> 3228         self._check_job_status(job, desc, \"TrainingJobStatus\")\n   3229         return desc\n   3230 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n   3390                 message=message,\n   3391                 allowed_statuses=[\"Completed\", \"Stopped\"],\n-> 3392                 actual_status=status,\n   3393             )\n   3394 \n\nUnexpectedStatusException: Error for Training job rf-scikit-2022-08-25-12-03-25-931: Failed. Reason: AlgorithmError: framework error: \nTraceback (most recent call last):\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_trainer.py\", line 84, in train\n    entrypoint()\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py\", line 39, in main\n    train(environment.Environment())\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py\", line 35, in train\n    runner_type=runner.ProcessRunnerType)\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/entry_point.py\", line 100, in run\n    wait, capture_error\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py\", line 291, in run\n    cwd=environment.code_dir,\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py\", line 208, in check_error\n    info=extra_info,\nsagemaker_training.errors.ExecuteUserScriptError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"\"\nCommand \"\/miniconda3\/bin\/python script.py\"\n\nExecuteUserScriptErr\n\n\nThe pertained model works fine and I don't know what the problem is, please help",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-26T14:45:09.876Z",
                "Answer_upvote_count":0,
                "Answer_body":"You might consider reviewing your 'script.py' entry point. There could be a variety of reasons for a training job to fail but the most likely, I can see, from the description and output would be related to \"where\" the model artifacts were written to within your script.\n\nThe SageMaker Github examples contain has an example of using a RandomForestRegressor in a script - https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-script-mode\/sagemaker-script-mode.ipynb\n\nI'm sharing this example because if you refer to the Scikit-learn section, you'll find the \"train_deploy_scikitlearn_without_dependencies.py\" script is referenced and the model is dumped to the model_dir: joblib.dump(model, os.path.join(args.model_dir, \"model.joblib\")). If we were to change that to some arbitrary location in the script then the example training job would fail with an AlgorithmError: framework error as well. As long as the 10 second training is expected then I see the output location as a likely cause.\n\nFor more details on this you can refer to the following two resources:\n\nHow Amazon SageMaker Processes Training Output - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-output.html\nUsing the SageMaker Python SDK - https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\n\nIn the first resource, you'll find that your algorithm should write all final model artifacts to opt\/ml\/model. In the second resource, you'll find more information on proper use of the SageMaker Python SDK and various implementations.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Possible quota issue",
        "Question_creation_time":1644304787619,
        "Question_link":"https:\/\/repost.aws\/questions\/QUnEGxZsbiQPe4oHO0CkRaRg\/possible-quota-issue",
        "Question_topic":[
            "Serverless",
            "Application Integration",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Step Functions",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":200.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I opened up an AWS support case to increase my quota based on the error below, but AWS support states this is not a quota issue. They were unwilling to help me in support but directed me to this forum.\n\nI'm getting the following error when running some AWS SageMaker batch transform jobs on m5.large instances. I'm running 8 transforms in parallel against 8 different models in a step function map. A few of the transforms succeed, but some fail with the following error: { \"resourceType\": \"sagemaker\", \"resource\": \"createTransformJob.sync\", \"error\": \"SageMaker.AmazonSageMakerException\", \"cause\": \"Rate exceeded (Service: AmazonSageMaker; Status Code: 400; Error Code: ThrottlingException; Request ID: 49a80dc1-df06-4b88-a462-24e517d13531; Proxy: null)\" }\n\nWhat is going on here? Am I doing something wrong? What kind of throttling exception is this (what is being throttled?)",
        "Answers":[
            {
                "Answer_creation_time":"2022-02-09T15:37:04.872Z",
                "Answer_upvote_count":0,
                "Answer_body":"I have seen this with batch transform when I try to trigger too many jobs too quickly (usually like 4xjobs within 1-2 seconds).\n\nWhat I did as a workaround was handle and recognize this quota error on my service, sleep for 5 seconds and retry the same job.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Maximising multi-instance and multi-GPU utilisation",
        "Question_creation_time":1548267645000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJdZhEsN_QTWOcvWD7tfcLA\/maximising-multi-instance-and-multi-gpu-utilisation",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":9.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\n\nI've been trying to distribute the MNIST example across instances and GPUs with SageMaker Tensorflow, but I'm not seeing the kind of benefit that I was hoping for. I'm not sure whether I'm just setting the job up incorrectly or whether this example is just not suited to distribution and was wondering if anyone has any ideas which it might be please?\n\nI'm using TensorFlow 1.9 because I think MPI\/Horovod doesn't work with 1.12 which I was using originally?\n\nIn my test I get these results with a batch size of 512:\n1 Instance 1 GPU 13.5 global_step\/sec\n2 Instance 1 GPU 7.3 global_step\/sec\n1 Instance 8 GPU 18.8 global_step\/sec\n\nIf I reduce the batch size I get lower overall throughput, and see little benefit when increasing beyond 512.\n\nMy job specification looks as below and mnist.py is the file which comes with the examples, though I changed batch_size=100 to batch_size=512 in the script:\n\nestimator = TensorFlow(entry_point='mnist.py',\r\n                  role=role,\r\n                  framework_version='1.9.0',\r\n                  training_steps=1250, \r\n                  evaluation_steps=10,\r\n                  train_max_run=5*60,    \r\n                  output_path=output_location,\r\n                  checkpoint_path=output_location,\r\n                  code_location=output_location,\r\n                  model_dir=output_location,\r\n                  train_instance_count=1,\r\n                  train_instance_type='ml.p3.16xlarge',\r\n                  base_job_name='PerformanceTest-p3-16xlarge-1-instance',\r\n                  distributions={\r\n                    'mpi': {\r\n                      'enabled': True,\r\n                      'processes_per_host': 8,\r\n                      'custom_mpi_options': '--NCCL_DEBUG INFO'\r\n                    }\r\n                  })\n\n\nWhat I was hoping to see was a single 8-GPU instance hitting global_step\/sec of 70.2-97.2. Based on 8x the global_step\/sec of a single instance, scaled with 60-90% efficiency. Any help or clarification on this would be greatly appreciated!\n\nThanks,\n\nCarl",
        "Answers":[
            {
                "Answer_creation_time":"2019-01-24T21:50:04.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nSorry that we are not clear in the doc.\n\nMPI\/Horovod support is only available since 1.12 in script mode. So according to your codes, if you use 1.9, then the mpi related parameter doesn't work. We will add version check in the codes.\n\nSo my recommendation for your test is,\n\nYou need to use 1.12 as explained above.\nMNIST may not be a good example for performance testing. It's pretty small.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-01-30T17:31:09.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks for the reply. I'll try to rerun the experiment, I'll also try with a different network.\n\nCheers,\n\nCarl",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do I check my current SageMaker service quotas?",
        "Question_creation_time":1642480202619,
        "Question_link":"https:\/\/repost.aws\/questions\/QUweO83CSlTu-3Zn2RxdESWg\/how-do-i-check-my-current-sage-maker-service-quotas",
        "Question_topic":[
            "Management & Governance",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Management Console",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":533.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"How do I check my current service quotas for Amazon SageMaker?\n\nIn the case of Amazon EC2, service quotas can be checked here: https:\/\/console.aws.amazon.com\/servicequotas\/home\/services\/ec2\/quotas\n\nFor SageMaker, the default quotas are listed here: https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html but there isn't a link to where one can find the current region-specific quotas for an account, which could have changed after a request for a service quota increase.",
        "Answers":[
            {
                "Answer_creation_time":"2022-01-18T22:33:29.893Z",
                "Answer_upvote_count":1,
                "Answer_body":"Amazon SageMaker has now been integrated with Service Quotas. You should be able to find current SageMaker quotas for your account in the Service Quotas console. You can also request for a quota increase right from the Service Quotas console itself. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/regions-quotas.html#regions-quotas-quotas",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-06-15T18:32:09.862Z",
                "Answer_upvote_count":1,
                "Answer_body":"Unfortunately, AWS Sagemaker is not supported for direct visibility into the service quotas. We have an existing feature request for SageMaker Integration with the Service Quotas page as below. However, I currently done have an ETA for the same as it would depend on feasibility and further integration.\n\nHaving said that, the below documentations provides the default values as you mentioned: [+] https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html\n\nI would suggest to contact the AWS Support when ever you wish to get to get insights on current quotas currently.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Retrieve Linear Learner Weights",
        "Question_creation_time":1658859121171,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXh8p1bCgT8a2gLfTDssY7w\/retrieve-linear-learner-weights",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon S3 Glacier",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Pipelines",
            "Amazon SageMaker Model Building"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":49.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Unable to find the correct attribute to list Linear Learner weights. I fitted the estimator with the training data and linear Learner creates a weight vector w, which is fundamental in this algorithm. How can I print the resulting weight vector after training\/fitting?",
        "Answers":[
            {
                "Answer_creation_time":"2022-07-27T16:22:33.082Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hey! You will have to get this weight vector from the model artifact the training job returns in S3. I believe this post will help you retrieve said vector! Hope it helps!",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Training Job. Python modules installation Error",
        "Question_creation_time":1659396236435,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmwxhCTLzTp2f9ePN1V2oLg\/sagemaker-training-job-python-modules-installation-error",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Model Training"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":128.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a problem with Python module installation that requires pre-installation of another module. Both modules were added to the requirement.txt file. However, the error occurs when installing main module:\n\n2022-07-29 01:18:26.460132: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n\"2022-07-29 01:18:26.470589: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\"\n2022-07-29 01:18:26.765280: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n\"2022-07-29 01:18:31,908 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\"\n\"2022-07-29 01:18:31,917 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\"\n\"2022-07-29 01:18:33,117 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\"\n\/usr\/local\/bin\/python3.9 -m pip install -r requirements.txt\nCollecting Cython==0.29.31\nDownloading Cython-0.29.31-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (2.0 MB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0\/2.0 MB 33.1 MB\/s eta 0:00:00\nRequirement already satisfied: wheel==0.37.1 in \/usr\/local\/lib\/python3.9\/site-packages (from -r requirements.txt (line 2)) (0.37.1)\nCollecting scikit-image==0.19.2\nDownloading scikit_image-0.19.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.0\/14.0 MB 83.1 MB\/s eta 0:00:00\nCollecting parallelbar==0.1.19\nDownloading parallelbar-0.1.19-py3-none-any.whl (5.6 kB)\nCollecting albumentations==1.0.3\nDownloading albumentations-1.0.3-py3-none-any.whl (98 kB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 98.7\/98.7 kB 6.6 MB\/s eta 0:00:00\nCollecting tensorflow_addons==0.16.1\nDownloading tensorflow_addons-0.16.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.1\/1.1 MB 54.4 MB\/s eta 0:00:00\nRequirement already satisfied: tensorflow-io==0.24.0 in \/usr\/local\/lib\/python3.9\/site-packages (from -r requirements.txt (line 7)) (0.24.0)\nRequirement already satisfied: tensorboard==2.8.0 in \/usr\/local\/lib\/python3.9\/site-packages (from -r requirements.txt (line 8)) (2.8.0)\nCollecting universal-pathlib==0.0.12\nDownloading universal_pathlib-0.0.12-py3-none-any.whl (19 kB)\nCollecting setuptools==63.2.0\nDownloading setuptools-63.2.0-py3-none-any.whl (1.2 MB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.2\/1.2 MB 58.9 MB\/s eta 0:00:00\nCollecting pynanosvg==0.3.1\nDownloading pynanosvg-0.3.1.tar.gz (346 kB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 346.0\/346.0 kB 17.5 MB\/s eta 0:00:00\nPreparing metadata (setup.py): started\nPreparing metadata (setup.py): finished with status 'error'\n\"error: subprocess-exited-with-error\n  \n  \u00d7 python setup.py egg_info did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [6 lines of output]\n      Traceback (most recent call last):\n        File \"\"<string>\"\", line 2, in <module>\n        File \"\"<pip-setuptools-caller>\"\", line 34, in <module>\n        File \"\"\/tmp\/pip-install-1mt2gkfy\/pynanosvg_d6162ffce95948abb4262061a011908c\/setup.py\"\", line 2, in <module>\n          from Cython.Build import cythonize\n      ModuleNotFoundError: No module named 'Cython'\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\"\nerror: metadata-generation-failed\n\u00d7 Encountered error while generating package metadata.\n\u2570\u2500> See above for output.\n\"note: This is an issue with the package mentioned above, not pip.\"\nhint: See above for details.\n[notice] A new release of pip available: 22.1.2 -> 22.2.1\n\"[notice] To update, run: pip install --upgrade pip\"\n\"2022-07-29 01:18:36,187 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\"\n\"2022-07-29 01:18:36,187 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\"\n\"2022-07-29 01:18:36,188 sagemaker-training-toolkit ERROR    Reporting training FAILURE\"\n\"2022-07-29 01:18:36,188 sagemaker-training-toolkit ERROR    InstallRequirementsError:\"\nExitCode 1\n\"ErrorMessage \"\"      ModuleNotFoundError: No module named 'Cython'\n       [end of output]      note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed  \u00d7 Encountered error while generating package metadata. \u2570\u2500> See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details.\"\"\"\n\"Command \"\"\/usr\/local\/bin\/python3.9 -m pip install -r requirements.txt\"\"\"\n\"2022-07-29 01:18:36,188 sagemaker-training-toolkit ERROR    Encountered exit_code 1\"",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-02T02:49:06.887Z",
                "Answer_upvote_count":0,
                "Answer_body":"Since Cython does seem to be downloaded before the error, I suspect the problem is something in other packages' install process requiring it before pip is done installing it. It looks like others have found similar (non-SageMaker-specific) issues with Cython e.g. here and here.\n\nThings I would suggest to try:\n\nExplicitly specify Cython (maybe without a version at first) right at the top of your requirements.txt file if you're not already - just in case this can convince pip to treat it properly.\n\nCustomize the TensorFlow container image you're targeting to pre-install Cython.\n\nIf you're not sure what base container URI you're using, you can fetch it with sagemaker.image_uris.retrieve(...) (doc here).\n\nFrom that, you can create a minimal Dockerfile something like\n\nFROM XYZ.dkr.ecr.ABC.amazonaws.com\/...\nRUN pip install Cython==0.29.31\n\nOnce you build this customized container image, and push it to Amazon ECR in your AWS account & region, you can use it by setting the image_uri parameter in your TensorFlow Estimator. Note that the frameworks typically have separate container images for training vs serving, and GPU vs CPU-only, so you may need to create a pair of containers if wanting to do inference too.\n\nIf you're working inside SageMaker Studio you won't directly be able to docker build, but you can install the sm-docker build solution based on AWS CodeBuild. The \"Prepare custom training and inference containers\" section of this notebook gives an example of similar approach.\n\nIf you'd really like to avoid touching containers and ECR, you could instead remove your requirements.txt and install dependencies within the script via something like subprocess.check_call([\"pip\", \"install\", ...]). It's hacky, but this way you could run a pip install just for Cython first... Then install all the other dependencies in one other command.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Waiter TrainingJobCompletedOrStopped failed: Waiter encountered a terminal",
        "Question_creation_time":1526422450000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeYBDZTVwQlWovkq2eE1CkQ\/waiter-training-job-completed-or-stopped-failed-waiter-encountered-a-terminal",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":401.0,
        "Question_answer_count":5,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to launch a job using the low level api in boto3 sagemaker client. After calling sagemaker.create_training_job(**params) I try to get a waiter. This code is directly from the documentation for creating a training job (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html)\nI get this error:\n\nTraceback (most recent call last):\r\n  File \"traindeploy.py\", line 97, in create_training_job\r\n    sagemaker.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\r\n  File \"\/path\/to\/lib\/Python\/3.6\/lib\/python\/site-packages\/botocore\/waiter.py\", line 53, in wait\r\n    Waiter.wait(self, **kwargs)\r\n  File \"\/path\/to\/lib\/Python\/3.6\/lib\/python\/site-packages\/botocore\/waiter.py\", line 323, in wait\r\n    last_response=response,\r\nbotocore.exceptions.WaiterError: Waiter TrainingJobCompletedOrStopped failed: Waiter encountered a terminal failure state\n\n\nThese are my job params:\n\n{\r\n  \"AlgorithmSpecification\": {\r\n    \"TrainingImage\": \"<image-url-from-ecr>\",\r\n    \"TrainingInputMode\": \"File\"\r\n  },\r\n  \"RoleArn\": \"<role-arn>\",\r\n  \"OutputDataConfig\": {\r\n    \"S3OutputPath\": \"s3:\/\/path-to-bucket\/some-folder-output\/\"\r\n  },\r\n  \"ResourceConfig\": {\r\n    \"InstanceCount\": 2,\r\n    \"InstanceType\": \"ml.c4.8xlarge\",\r\n    \"VolumeSizeInGB\": 50\r\n  },\r\n  \"TrainingJobName\": \"some-jobname\",\r\n  \"HyperParameters\": {},\r\n  \"StoppingCondition\": {\r\n    \"MaxRuntimeInSeconds\": 3600\r\n  },\r\n  \"InputDataConfig\": [\r\n    {\r\n      \"ChannelName\": \"train\",\r\n      \"DataSource\": {\r\n        \"S3DataSource\": {\r\n          \"S3DataType\": \"S3Prefix\",\r\n          \"S3Uri\": \"s3:\/\/path-to-bucket\/some-folder-input\/\",\r\n          \"S3DataDistributionType\": \"FullyReplicated\"\r\n        }\r\n      },\r\n      \"CompressionType\": \"None\",\r\n      \"RecordWrapperType\": \"None\"\r\n    }\r\n  ]\r\n}\n\n\nCan someone please advise what is causing this and how will I get a waiter on a training job?",
        "Answers":[
            {
                "Answer_creation_time":"2018-05-17T23:27:35.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello rks,\n\nYou received that error message because the waiter you created was expecting the training job to finish in either a Completed or Stopped state, but it reached the Failed state instead. This means there was an issue with your job and it could not be completed correctly.\n\nTo understand what caused your training job to fail, you can follow these steps:\n\nGo to the Amazon SageMaker console in your account\nClick on \"Jobs\" in the navigation bar to the left\nSearch for your training job in the list. You can use the \"Search jobs\" text form to quickly find the job by its name, or filter them by status.\nThe training job details should explain why it failed.\n\nIf you need more help with this issue, please don't hesitate to contact us.\n\nBest regards,\nAmazon SageMaker team",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-05-18T01:37:08.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Rodrigo,\n\nThanks for looking into this. Actually the training job does eventually succeed. I tried putting time.wait(seconds=N) before I get call get_waiter, but no matter what wait time I chose the waiter still failed giving the same exception. I waited 5 seconds, I waited 2 minutes, I even waited 3 minutes which was more than the time my training job took to complete successfully, but I always got the same exception.\n\nSo, I want to emphasize that there is nothing wrong with the training jobs I create. They succeed. But the get_waiter method always fails for me. What am I doing wrong here?\n\nEdited by: rks on May 17, 2018 4:34 PM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-05-17T22:39:01.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi rks,\n\nI'm sorry for the confusion, I misunderstood your issue. I attempted to reproduce it by running the example code myself in a SageMaker notebook, but I was able to run the job and the waiter worked correctly. For the record, I used the low-level KMeans MNIST sample notebook that comes bundled with all SageMaker notebooks. You can find it in \"\/sample-notebooks\/sagemaker-python-sdk\/1P_kmeans_lowlevel\/kmeans_mnist_lowlevel.ipynb\". That notebook should be very similar to the code you tried to run.\n\nCould you tell us more about how the environment you're running the job in? In particular, we'd like to know the version of Python and BOTO you're using. You can run the following commands to get them:\n\nimport sys, boto3\r\nprint(\"boto version = \" + boto3.__version__)\r\nprint(\"python version = \" + sys.version)\n\n\nThank you for your patience.\n\nRodrigo",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-06-12T02:41:29.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Roberto,\n\nThank you for helping me out. This is the version of boto and python I am running:\n\n16:30 $ python3.6\r\nPython 3.6.4 (v3.6.4:d48ecebad5, Dec 18 2017, 21:07:28) \r\n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\nimport sys, boto3\r\nprint(\"boto version = \" + boto3.__version__)\r\n    boto version = 1.5.18\r\nprint(\"python version = \" + sys.version)\r\n    python version = 3.6.4 (v3.6.4:d48ecebad5, Dec 18 2017, 21:07:28) \r\n    [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]\n\n\nI'll review my code based on the notebook you've pointed out.\n\nEdited by: rks on May 18, 2018 4:35 PM\n\nEdited by: rks on May 18, 2018 4:35 PM\n\nEdited by: rks on May 18, 2018 4:36 PM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-05-18T23:34:52.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi rks,\n\nWe have tried to troubleshoot this issue but it doesn't seem like it's reproducible. It's likely that the training-job name was invalid. Is this issue still occurring?\n\nThanks,\nIngrid",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Amazon SageMaker Local Mode raised boto3.exceptions.RetriesExceededError: Max Retries Exceeded",
        "Question_creation_time":1640615850316,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVLhj-0JzTUCLcs7yH5Kznw\/amazon-sage-maker-local-mode-raised-boto-3-exceptions-retries-exceeded-error-max-retries-exceeded",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Elastic Container Registry (ECR)"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":83.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I try to run the SageMaker local mode example without any modification at https:\/\/github.com\/aws-samples\/amazon-sagemaker-local-mode\/tree\/main\/pytorch_nlp_script_mode_local_model_inference on my local machine.\n\nHowever I encountered the **boto3.exceptions.RetriesExceededError: Max Retries Exceeded ** exception when the example tries to deploy the inference endpoint to 'local' instance type.\n\nI checked with\n\ndocker images -a \n\n\ncommand and it does not pull the expected pre-built SageMaker deep learning container image from ECR. The code example is using a dummy role for the local SageMaker session. I need help as I am blocked at this point as the exception error message is not helpful to pinpoint the actual root cause of this issue. Thanks in advance.\n\nBelow are my configurations:\n\nUbuntu: 20.04.3 LTS\nAWS CLI version: 2.4.7\nPython: 3.8.12\nDocker: 20.10.12\nDocker Compose: 1.29.2\nboto3: 1.20.26\nsagemaker: 2.72.1",
        "Answers":[
            {
                "Answer_creation_time":"2021-12-27T15:10:36.523Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, if you aren't able to pull the expected pre-built SageMaker deep learning container image, I would check your network settings. From the local mode sample documentation, \"you'll need to be able to access a public Amazon ECR repository from your local environment.\"\n\nFor more detailed information from boto3, you can enable logging and debug mode as well, which could help pinpoint the exact cause of the error. More information on Boto3 Retries: https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/guide\/retries.html.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Prevent boto3.client('sagemaker').create_auto_ml_job() from deploying endpoint",
        "Question_creation_time":1669662766261,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGEnbfLBgQJKrbdKfizHkEw\/prevent-boto-3-client-sagemaker-create-auto-ml-job-from-deploying-endpoint",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":18.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"When I invoke the .create_auto_ml_job() method both with and without the optional ModelDeployConfig kwarg, the autopilot job deploys an endpoint using the best model. Is there a way to prevent the .create_auto_ml_job() method from behaving this way? I do not wish to deploy the best model to an endpoint, and do not wish to have to delete this endpoint.",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Failing to create multi-model endpoint",
        "Question_creation_time":1613660927000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcIYmAjUlRn-b_PqcIjk08A\/failing-to-create-multi-model-endpoint",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":35.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I have been trying to create a multi-model endpoint with my own container, using the instructions here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html.\n\nFollowing the instructions here, I am able to successfully create a model and endpoint configuration: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/create-multi-model-endpoint-sdk.html\n\nHowever, when I try to create the endpoint itself, it shows the status of \"Creating\" for over 2 hours, before finally stopping with the status, \"Failed\". It gives no reason for the failure or any other help.\n\nDoes anyone have any ideas?\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_time":"2021-03-19T18:41:32.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi gdaley,\n\nThanks for using Amazon SageMaker!\n\nWe are seeing the following exception when creating your endpoint:\nOCI runtime create failed: container_linux.go:370: starting container process caused: exec: \"python\": executable file not found in $PATH: unknown\"\n\nThis is related to the python path ENTRYPOINT setup in Dockerfile. Please check your Python path setup is the same as https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\n\nTo help you resolve the issue faster, you can create a support case with AWS at https:\/\/console.aws.amazon.com\/support\/home#\/\n\nThanks,\nAmazon SageMaker",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-22T07:25:33.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thank you!",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Pipelines - Is it possible to use a TransformStep with the Catboost Estimator ?",
        "Question_creation_time":1668677758132,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdkeWBFI3SXSA8QznUYgT1Q\/sagemaker-pipelines-is-it-possible-to-use-a-transform-step-with-the-catboost-estimator",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":45.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi! I am trying to implement a Sagemaker Pipeline including the following steps (among other things):\n\nProcessingStep: processing script (PySparkProcessor) generating a train , validation and test dataset (csv)\nTrainingStep: model training, CatBoost Estimator (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/catboost.html)\nTransformStep: batch inference using the model on the test dataset (csv)\n\nThe TransformStep returns the following error: python3: can't open file 'serve': [Errno 2] No such file or directory\n\nI wonder if I'm using TransformStep in the wrong way or if, at the moment, the use of TransformStep with the CatBoost model has not been implemented yet.\n\nCode:\n\n[...]\npyspark_processor = PySparkProcessor(\n    base_job_name=\"sm-spark\",\n    framework_version=\"3.1\",\n    role=role_arn,\n    instance_type=\"ml.m5.xlarge\",\n    instance_count=12,\n    sagemaker_session=pipeline_session,\n    max_runtime_in_seconds=2400,\n)\n\nstep_process_args = pyspark_processor.run(\n    submit_app=os.path.join(\n        s3_preprocess_script_dir, \"preprocess.py\"\n    ),  # Hack to fix cache hit\n    submit_py_files=[os.path.join(\n        s3_preprocess_script_dir, \"preprocess_utils.py\"\n    ), os.path.join(\n        s3_preprocess_script_dir, \"spark_utils.py\"\n    )],\n    outputs=[\n        ProcessingOutput(\n            output_name=\"datasets\",\n            source=\"\/opt\/ml\/processing\/output\",\n            destination=s3_preprocess_output_path,\n        )\n    ],\n    arguments=[\"--aws_account\", AWS_ACCOUNT, \"--aws_env\", AWS_ENV, \"--project_name\", PROJECT_NAME, \"--mode\", \"training\"],\n)\n\nstep_process = ProcessingStep(\n    name=\"PySparkPreprocessing\",\n    step_args=step_process_args,\n    cache_config=cache_config,\n)\n\ntrain_model_id = \"catboost-classification-model\"\ntrain_model_version = \"*\"\ntrain_scope = \"training\"\ntraining_instance_type = \"ml.m5.xlarge\"\n\n# Retrieve the docker image\ntrain_image_uri = image_uris.retrieve(\n    region=None,\n    framework=None,\n    model_id=train_model_id,\n    model_version=train_model_version,\n    image_scope=train_scope,\n    instance_type=training_instance_type,\n)\n\n# Retrieve the training script\ntrain_source_uri = script_uris.retrieve(\n    model_id=train_model_id, model_version=train_model_version, script_scope=train_scope\n)\n\n# Retrieve the pre-trained model tarball to further fine-tune\ntrain_model_uri = model_uris.retrieve(\n    model_id=train_model_id, model_version=train_model_version, model_scope=train_scope\n)\n\ntraining_job_name = name_from_base(f\"jumpstart-{train_model_id}-training\")\n\n# Create SageMaker Estimator instance\ntabular_estimator = Estimator(\n    role=role_arn,\n    image_uri=train_image_uri,\n    source_dir=train_source_uri,\n    model_uri=train_model_uri,\n    entry_point=\"transfer_learning.py\",\n    instance_count=1,\n    instance_type=\"ml.m5.xlarge\",\n    max_run=360000,\n    hyperparameters=hyperparameters,\n    sagemaker_session=pipeline_session,\n    output_path=s3_training_output_path,\n    disable_profiler=True,  # The default profiler rule includes a timestamp which will change each time the pipeline is upserted, causing cache misses. If profiling is not needed, set disable_profiler to True on the estimator.\n)\n\n# Launch a SageMaker Training job by passing s3 path of the training data\nstep_train_args = tabular_estimator.fit(\n    {\n        \"training\": TrainingInput(\n            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n                \"datasets\"\n            ].S3Output.S3Uri\n        )\n    },\n    logs=True,\n    job_name=training_job_name,\n)\n\nstep_train = TrainingStep(\n    name=\"CatBoostTraining\",\n    step_args=step_train_args,\n    cache_config=cache_config,\n)\n\nscript_eval = ScriptProcessor(\n    image_uri=[MASKED],\n    command=[\"python3\"],\n    instance_type=\"ml.m5.xlarge\",\n    instance_count=1,\n    base_job_name=\"script-evaluation\",\n    role=role_arn,\n    sagemaker_session=pipeline_session,\n)\n\neval_args = script_eval.run(\n    inputs=[\n        ProcessingInput(\n            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n            destination=\"\/opt\/ml\/processing\/model\",\n        ),\n        ProcessingInput(\n            source=step_process.properties.ProcessingOutputConfig.Outputs[\n                \"datasets\"\n            ].S3Output.S3Uri,\n            destination=\"\/opt\/ml\/processing\/input\",\n        ),\n    ],\n    outputs=[\n        ProcessingOutput(\n            output_name=\"evaluation\",\n            source=\"\/opt\/ml\/processing\/evaluation\",\n            destination=s3_evaluation_output_path,\n        ),\n    ],\n    code=\"common\/evaluation.py\",\n)\n\nevaluation_report = PropertyFile(\n    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n)\n\nstep_eval = ProcessingStep(\n    name=\"Evaluation\",\n    step_args=eval_args,\n    property_files=[evaluation_report],\n    cache_config=cache_config,\n)\n\nmodel = Model(\n    image_uri=\"467855596088.dkr.ecr.eu-west-3.amazonaws.com\/sagemaker-catboost-image:latest\",\n    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n    sagemaker_session=pipeline_session,\n    role=role_arn,\n)\n\nevaluation_s3_uri = \"{}\/evaluation.json\".format(\n    step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n)\n\nmodel_step_args = model.create(\n    instance_type=\"ml.m5.large\",\n)\ncreate_model = ModelStep(name=\"CatBoostModel\", step_args=model_step_args)\n\nstep_fail = FailStep(\n    name=\"FailBranch\",\n    error_message=Join(\n        on=\" \", values=[\"Execution failed due to F1-score <\", 0.8]\n    ),\n)\n\ncond_lte = ConditionGreaterThanOrEqualTo(\n    left=JsonGet(\n        step_name=step_eval.name,\n        property_file=evaluation_report,\n        json_path=\"classification_metrics.f1-score.value\",\n    ),\n    right=f1_threshold,\n)\n\nstep_cond = ConditionStep(\n    name=\"F1ScoreCondition\",\n    conditions=[cond_lte],\n    if_steps=[create_model],\n    else_steps=[step_fail],\n)\n\n# Transform Job\ns3_test_transform_input = os.path.join(step_process.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"], \"test\")\n\ntransformer = Transformer(model_name=create_model.properties.ModelName,\n                          instance_count=1,\n                          instance_type=\"ml.m5.xlarge\",\n                          assemble_with=\"Line\",\n                          accept=\"text\/csv\",\n                          output_path=s3_test_transform_output_path,\n                          sagemaker_session=pipeline_session)\n\ntransform_step_args = transformer.transform(\n    data=s3_test_transform_input,\n    content_type=\"text\/csv\",\n    split_type=\"Line\",\n)\n\nstep_transform = TransformStep(\n    name=\"InferenceTransform\",\n    step_args=transform_step_args,\n)\n\n\n# Create and execute pipeline\nstep_transform.add_depends_on([step_process, create_model])\n\npipeline = Pipeline(\n    name=pipeline_name,\n    steps=[step_process, step_train, step_eval, step_cond, step_transform],\n    sagemaker_session=pipeline_session,\n)\n\npipeline.upsert(role_arn=role_arn, description=[MASKED])\nexecution = pipeline.start()\nexecution.wait(delay=60, max_attempts=120)",
        "Answers":[
            {
                "Answer_creation_time":"2022-11-17T09:48:18.264Z",
                "Answer_upvote_count":1,
                "Answer_body":"I'd suggest to start out by debugging whether the model created by your pipeline actually deploys or transforms OK (just from notebook), because I think that's where your problem might be.\n\nAs shown in the sample notebooks for classification and regression, deploy(...) and similar calls for CatBoost (and other new JumpStart-based algorithms) require some extra parameters including inference image_uri and source_dir. Unlike, say, the XGBoost algorithm where only one image URI needs to be specified across training and inference - and no source scripts need to be bundled in at either training or inference time.\n\nI haven't been able to test for myself yet, but think you might be able to fix this by adding image_uri and source_dir (specifying the inference container and script bundle as shown in the example notebooks, which are different from the training ones) to your create_model(...) call.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-25T11:12:15.448Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks for your answer. I managed to build and run apipeline with the CatBoost model (jumpstart version) which includes:\n\npreprocessing\ntraining\nmodel registration\nbatch inference\n\nI encounter two difficulties that I wanted to bring up:\n\nUnexpected behavior when registering the model\n\nWhat I tried based on the documentation:\n\n[...]\n\n# Retrieve the inference docker container uri\ndeploy_image_uri = image_uris.retrieve(\n    region=None,\n    framework=None,\n    image_scope=\"inference\",\n    model_id=train_model_id,\n    model_version=train_model_version,\n    instance_type=inference_instance_type,\n)\n\n# Retrieve the inference script uri\ndeploy_source_uri = script_uris.retrieve(\n    model_id=train_model_id, model_version=train_model_version, script_scope=\"inference\"\n)\n\nmodel = Model(\n    image_uri=deploy_image_uri,\n    model_data=\"s3:\/\/[MASKED]\/output\/model.tar.gz\",\n    source_dir=deploy_source_uri ,\n    sagemaker_session=pipeline_session,\n    entry_point=\"inference.py\",\n    role=role_arn,\n)\n\n[...]\n\nregister_model_step_args = model.register(\n    content_types=[\"text\/csv\"],\n    response_types=[\"text\/csv\"],\n    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n    transform_instances=[\"ml.m5.xlarge\"],\n    model_package_group_name=model_package_group_name,\n    approval_status=\"Approved\",\n    model_metrics=model_metrics,\n)\n\n\nBy doing so, the execution of the pipeline returns the following error:\n\nboto3.exceptions.S3UploadFailedError: Failed to upload \/tmp\/tmp5jzb0338\/new.tar.gz to jumpstart-cache-prod-eu-west-3\/source-directory-tarballs\/catboost\/inference\/classification\/v1.1.1\/sourcedir.tar.gz: An error occurred (AccessDenied) when calling the CreateMultipartUpload operation: Access Denied\n\n\nAccording to the documentation (see source_dir in https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html?highlight=model#sagemaker.model.Model), the same s3 path is used to save the model as the one used as source of the files. This is a problem when using the jumpstart inference scripts because you obviously can't upload to this reserved bucket. The workaround I used is to download locally the tarball of the Catboost inference scripts and then specify a local path as the origin of the scripts.\n\nos.makedirs(\"tmp\/deploy_source_uri\", exist_ok=True)\nS3Downloader.download(deploy_source_uri, \"tmp\")\nos.system(\"tar -xf tmp\/sourcedir.tar.gz --directory tmp\/deploy_source_uri\")\n\nmodel = Model(\n    image_uri=deploy_image_uri,\n    model_data=\"s3:\/\/[MASKED]\/output\/model.tar.gz\",\n    source_dir=\"tmp\/deploy_source_uri\",\n    sagemaker_session=pipeline_session,\n    entry_point=\"inference.py\",\n    role=role_arn,\n)\n\n\nIs there a better way ?\n\nFailed to output csv files when using batch transform\n\nI managed to perform a batch transformation step with the previous catboost model. However, I was not able to produce files in csv format, only json format seems to be compatible with catboost inference scripts.\n\nWhat I wanted to do but does not work:\n\ntransformer = Transformer(\n    model_name=model_step.properties.ModelName,\n    instance_count=1,\n    instance_type=\"ml.m5.xlarge\",\n    strategy=\"MultiRecord\",\n    assemble_with=\"Line\",\n    output_path=s3_test_transform_output_path,\n    accept=\"text\/csv\",\n    max_concurrent_transforms=1,\n    max_payload=5,\n    sagemaker_session=pipeline_session,\n)\n\nstep_transform = TransformStep(\n    name=\"InferenceTransform\",\n    transformer=transformer,\n    inputs=TransformInput(\n        data=s3_test_transform_input,\n        content_type=\"text\/csv\",              \n        split_type=\"Line\",\n        input_filter=\"$[1:]\",\n        join_source=\"Input\"                         # Wanted to join input data to prediction in csv format\n    ),\n    depends_on=[model_step]\n)\n\n\nBy doing so, the execution of the pipeline returns the following error:\n\n2022-11-23 13:48:12,273 [INFO ] W-9000-model_1-stdout MODEL_LOG - Failed to do transform\n2022-11-23 13:48:12,273 [INFO ] W-9000-model_1-stdout MODEL_LOG - Traceback (most recent call last):\n2022-11-23 13:48:12,273 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File \"\/opt\/ml\/model\/code\/inference.py\", line 55, in transform_fn\n2022-11-23 13:48:12,273 [INFO ] W-9000-model_1-stdout MODEL_LOG -     return encoder.encode(output, accept)\n2022-11-23 13:48:12,273 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker_inference\/encoder.py\", line 108, in encode\n2022-11-23 13:48:12,274 [INFO ] W-9000-model_1-stdout MODEL_LOG -     return encoder(array_like)\n2022-11-23 13:48:12,274 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker_inference\/encoder.py\", line 79, in _array_to_csv\n2022-11-23 13:48:12,274 [INFO ] W-9000-model_1-stdout MODEL_LOG -     np.savetxt(stream, array_like, delimiter=\",\", fmt=\"%s\")\n2022-11-23 13:48:12,274 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File \"<__array_function__ internals>\", line 5, in savetxt\n2022-11-23 13:48:12,275 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/numpy\/lib\/npyio.py\", line 1380, in savetxt\n2022-11-23 13:48:12,274 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 272\n2022-11-23 13:48:12,275 [INFO ] W-9000-model_1 ACCESS_LOG - \/169.254.255.130:42106 \"POST \/invocations HTTP\/1.1\" 500 288\n2022-11-23 13:48:12,275 [INFO ] W-9000-model_1 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:379f03461a27,timestamp:null\n2022-11-23 13:48:12,276 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:379f03461a27,timestamp:null\n2022-11-23 13:48:12,276 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:11|#Level:Host|#hostname:379f03461a27,timestamp:null\n2022-11-23 13:48:12,276 [INFO ] W-9000-model_1-stdout MODEL_LOG -     raise ValueError(\n2022-11-23 13:48:12,276 [INFO ] W-9000-model_1-stdout MODEL_LOG - ValueError: Expected 1D or 2D array, got 0D array instead\n\n\nAfter analysis of the inference.py script of the jumpstart model, it seems that the implementation of transform_fn is not compatible with the generation of output in csv format (transform_fn in inference.py provides a dict (output variable) to encoder.encode(output, accept) which call np.savetxt(stream, array_like, delimiter=\",\", fmt=\"%s\"), array_like variable is thus a dict which is not compatible with np.savetxt).\n\nThe workaround I used is to output a json file like so:\n\ntransformer = Transformer(\n    model_name=model_step.properties.ModelName,\n    instance_count=1,\n    instance_type=\"ml.m5.xlarge\",\n    strategy=\"MultiRecord\",\n    assemble_with=\"Line\",\n    output_path=s3_test_transform_output_path,\n    accept=\"application\/json\",                                                 # JSON\n    max_concurrent_transforms=1,\n    max_payload=5,\n    sagemaker_session=pipeline_session,\n)\n\nstep_transform = TransformStep(\n    name=\"InferenceTransform\",\n    transformer=transformer,\n    inputs=TransformInput(\n        data=s3_test_transform_input,\n        content_type=\"text\/csv\",\n        split_type=\"Line\",\n        input_filter=\"$[1:]\",\n    ),                                                                # No more join_source :(\n    depends_on=[model_step]\n)\n\n\nIs what I'm trying to do with the CatBoost Jumpstart model not yet implemented or have I misused the pipeline objects?",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[Solved]download image from S3 to Endpoint(made by Sagemaker) with s3url(s3:\/\/~~)",
        "Question_creation_time":1660886637665,
        "Question_link":"https:\/\/repost.aws\/questions\/QULB64ZDsXSPuHBjqAWik3hQ\/solved-download-image-from-s-3-to-endpoint-made-by-sagemaker-with-s-3-url-s-3",
        "Question_topic":[
            "Machine Learning & AI",
            "Storage"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "S3 Select",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":102.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I try to download image(.jpg, .png.) from S3 to Endpoint(made by Sagemaker) with s3url(s3:\/\/~~)\n\nBecause At the endpoint made by sagemaker, To send s3url is faster than to send image.\n\nI can download image at sagemaker notebook, from s3 to sagemaker local.\n\nbut I can't download image from s3 to sagemaker endpoint.\n\nThat local download code can not work.",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-23T00:55:10.041Z",
                "Answer_upvote_count":0,
                "Answer_body":"I solve this! I try to download image at endpoint. but endpoint can not connect outside network except Lambda.\n\nI make request with s3url\nDownload image from s3 to lambda\nTransmit image from lambda to endpoint",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Outgoing mail for sagemaker labeling job",
        "Question_creation_time":1662016903588,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqbxtiU_kSe-GoPcj6g0pzg\/outgoing-mail-for-sagemaker-labeling-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":36.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"When having made a labeling job on Ground Truth, an outgoing mail should be sent to team member, but in my case, mail not be sent with no error message.\n\nin case no private team created (the first job creation) : mail can be sent. (set up a team during job creation)\nin case a private team already set up: mail cannot be sent. (select a existing team during job creation)\n\nI think policies of the job role might not be enough, for example, cognito policy. How can I make sure the cause of the error?",
        "Answers":[
            {
                "Answer_creation_time":"2022-09-02T14:53:23.761Z",
                "Answer_upvote_count":0,
                "Answer_body":"To Successfully create a SageMaker Labeling Job you will need the following Permission Policies applied within your account:\n\nThe IAM entity you have used to create the job will need permissions outlined in the \"Permissions Required to Use the Amazon SageMaker Ground Truth Console\" [1]\nYour Labelling Job Role will need SageMakerFullAccess [2]\n\nWith these permissions in place your job should create successfully.\n\nLinks to documentation provided by AWS:\n\n[1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security_iam_id-based-policy-examples.html#console-permissions\n\n[2] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security-iam-awsmanpol.html#security-iam-awsmanpol-AmazonSageMakerFullAccess",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Use S3 as a git repo",
        "Question_creation_time":1656607653458,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUeFHB_qvQw67d9knOyw1Ig\/use-s-3-as-a-git-repo",
        "Question_topic":[
            "Machine Learning & AI",
            "Storage"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "S3 Object Lock"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":236.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a sagemaker notebook that has no connections to internet or codecommit but has access to 1 s3 bucket. I would like to use that 1 s3 bucket as a place to house git repos, ideally I would like to be able to pull\/push to repos in that bucket from other sagemaker notebooks or ec2 instances that have connections to that bucket. Has anyone tried this before?",
        "Answers":[
            {
                "Answer_creation_time":"2022-06-30T16:53:27.277Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi! I believe from AWS we don't have an official solution for this strategy. The are some solutions out there like https:\/\/github.com\/bgahagan\/git-remote-s3 you could use if you can upload the latest release to the S3 bucket and install it on your notebook. Hope this helps!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-01T12:12:07.011Z",
                "Answer_upvote_count":0,
                "Answer_body":"Check out these assets to see if they can help to accomplish this: https:\/\/aws.amazon.com\/blogs\/machine-learning\/how-to-use-common-workflows-on-amazon-sagemaker-notebook-instances\/ https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-git-repo.html",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to fix SageMaker training job error \"SM_CHANNEL_TRAIN\"?",
        "Question_creation_time":1652689524286,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwxIZtOn8Qg6EHGP_Ehi2mQ\/how-to-fix-sage-maker-training-job-error-sm-channel-train",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":46.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am building a ml workflow using step function following this. However, when I start the state machine, I got error\n\nAlgorithmError: framework error ... SM_CHANNEL_TRAIN ...exit code: 1 \n\n\nDoes anyone know how to fix it? or how to set SM_CHANNEL_TRAIN?\n\nThank you",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-29T06:55:20.947Z",
                "Answer_upvote_count":0,
                "Answer_body":"Assuming you are using the sagemaker python sdk, you'll have to specify the train channel.\n\nThe example below shows how to specify 3 channels and their respective paths to S3. In the training container that is started, these will be translated to the environment variable SM_CHANNEL_{channel_name.upper()}. I.e. train channel is translated to SM_CHANNEL_TRAIN, test123 is translated to SM_CHANNEL_TEST123.\n\nfrom sagemaker.estimator import Estimator\n\n\ns3pth = 's3:\/\/mybucket'\n\ndata = {\n\t'train': f'{s3pth}\/train',\n    'validation': f'{s3pth}\/validation',\n    'test': f'{s3pth}\/test',\n}\n\n# starting the train job with our uploaded datasets as input\nestimator.fit(\n    data,\n    wait=False,\n    # job_name = f\"{experiment_name}--{pd.Timestamp.now().strftime('%y%m%d-%H%M%S')}\",\n    # experiment_config = {\n    #     \"TrialName\": trial.trial_name,\n    #     \"TrialComponentDisplayName\": \"Training\",\n    # },\n)",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to create custom templates for model training and building in sagemaker studio?",
        "Question_creation_time":1663805155971,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-fW_jQSjSKiwACGXqFh1sA\/how-to-create-custom-templates-for-model-training-and-building-in-sagemaker-studio",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":34.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am going through documentation provided here , https:\/\/github.com\/aws-samples\/amazon-sagemaker-build-train-deploy and would like to create my own model building, training and deploying templates. can I download\/clone the sagemaker provided templates and start modifying to my own needs. I understand , we need to set up a aws catalog portfolio and products under it, to be able to use such templates. my question is which project do i need to clone and modify , say if i want to build my own training and model building template, which particular code base or code file do i need to change. I assume , the train.py file here -> https:\/\/github.com\/aws-samples\/amazon-sagemaker-build-train-deploy\/blob\/master\/08_projects\/modelbuild\/pipelines\/endtoendmlsm\/train\/train.py would be one i will customize. but once i change these files, how do i use them. how would i set up my custom template, create a product under my catalog? if yes, how to I link my custom train.py code to this new custom sagemaker project template. the documentation or samples only show how to use the prebuild templates under sagemaker project templates, but how do i get my own template pushed there ? can this be done via say cloudformation ?",
        "Answers":[
            {
                "Answer_creation_time":"2022-09-22T03:53:38.010Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nThis workshop works through the typical ML process to build and deploy a model to predict the fault on machine based on synthetic datasets. To start building it, you will need to have a AWS account, the link is here.\n\nOnce you have an account, you can start from module 1 from this step. I would recommend follow each module in order and try to implement in your account. This will help you understand the methodology and familiar with the service.\n\nTo modify it, you will need to collect and use your datasets, identifying your business problem. You could try to re-use the same algorithm in the module but it depends on the issue you try to resolve.\n\nHope it helps\n\nThanks,",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ModuleNotFoundError when starting a training job on Sagemaker",
        "Question_creation_time":1598912648000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJMd_4s52RpWXDXITXFsQdw\/module-not-found-error-when-starting-a-training-job-on-sagemaker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":357.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I want to submit a training job on sagemaker. I tried it on notebook and it works. When I try the following I get ModuleNotFoundError: No module named 'nltk'\n\nMy code is\n\nimport sagemaker  \nfrom sagemaker.pytorch import PyTorch\n\nJOB_PREFIX   = 'pyt-ic'\nFRAMEWORK_VERSION = '1.3.1'\n\nestimator = PyTorch(entry_point='finetune-T5.py',\n                   source_dir='..\/src',\n                   train_instance_type='ml.p2.xlarge' ,\n                   train_instance_count=1,\n                   role=sagemaker.get_execution_role(),\n                   framework_version=FRAMEWORK_VERSION, \n                   debugger_hook_config=False,  \n                   py_version='py3',\n                   base_job_name=JOB_PREFIX)\n\nestimator.fit()\n\n\nfinetune-T5.py have many other libraries that are not installed. How can I install the missing library? Or is there a better way to run the training job?",
        "Answers":[
            {
                "Answer_creation_time":"2020-08-31T22:47:15.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Check out this link (Using third-party libraries section) on how to install third-party libraries for training jobs. You need to create requirement.txt file in the same directory as your training script to install other dependencies at runtime.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can I use glue interactive sessions with pythonshell?",
        "Question_creation_time":1645223131500,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjwZtVPYTSCG96fKOGn1k4w\/can-i-use-glue-interactive-sessions-with-pythonshell",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Glue"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":138.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Can I use glue interactive sessions with pythonshell?\n\nIn the docs, it hints at choosing the %job_type or is it only available for glueetl. I am looking for a simple way to develop by requiring myself to use a sage maker notebook because it is overkill on half the ETL more of the time. Can I just get the 0.0625 DPU by default with a notebook? And no extras like I get with sagemaker.",
        "Answers":[
            {
                "Answer_creation_time":"2022-02-20T17:42:10.047Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, Glue Interactive Sessions allows you to install Glue PySpark and Glue Scala kernels to any Jupyter Notebook and Glue Studio Notebook is a new Glue Studio experience powered by Interactive Sessions. As such, they are aimed ad interactive Glue ETL (Spark) development.\n\nAs your question is about development with Glue Python Shell jobs, which is the right choice for small data sets, this can be developed with the Glue Studio Python Shell Editor (no DPUs paid before you run the job) or in any Python IDE environment using compatible Python version.\n\nUsing the Glue Studio Python Shell editor you can develop and submit the job to run. For the results you can open the logs directly from the Runs tabs.\n\nAlternatively you could use the legacy script editor, that when you run the PythonShell job in the bottom part of the windows it shows you the logs as the job is running.\n\nhope this helps,",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ImportError: cannot import name 'dataclass_transform' from 'typing_extensions' (\/home\/ec2-user\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/typing_extensions.py)",
        "Question_creation_time":1659014190621,
        "Question_link":"https:\/\/repost.aws\/questions\/QULlX63PqqQ1q-W5kCzwTKow\/import-error-cannot-import-name-dataclass-transform-from-typing-extensions-home-ec-2-user-anaconda-3-envs-tensorflow-2-p-38-lib-python-3-8-site-packages-typing-extensions-py",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":344.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi AWS, I am running the code for dalle mini to convert a text into an image. Here is the code for the same:\n\nimport jax\nimport jax.numpy as jnp\nfrom huggingface_hub import hf_hub_url, cached_download, hf_hub_download\nimport shutil\nfrom dalle_mini import DalleBart, DalleBartProcessor\nfrom vqgan_jax.modeling_flax_vqgan import VQModel\nfrom typing_extensions import dataclass_transform\nfrom transformers import CLIPProcessor, FlaxCLIPModel\nfrom IPython.display import display\n\n# TF_CPP_MIN_LOG_LEVEL=0\nprint(jax.local_device_count())\nprint(jax.devices())\n\ndalle_mini_files_list = ['config.json', 'tokenizer.json', 'tokenizer_config.json', 'merges.txt', 'vocab.json', 'special_tokens_map.json', 'enwiki-words-frequency.txt', 'flax_model.msgpack']\n\nvqgan_files_list = ['config.json',  'flax_model.msgpack']\n\nfor each_file in dalle_mini_files_list:\n   downloaded_file = hf_hub_download(\"dalle-mini\/dalle-mini\", filename=each_file)\n   target_path = '\/home\/ec2-user\/SageMaker\/huggingface-sagemaker\/content\/dalle-mini\/' + each_file\n   shutil.copy(downloaded_file, target_path)\n\nfor each_file in vqgan_files_list:\n   downloaded_file = hf_hub_download(\"dalle-mini\/vqgan_imagenet_f16_16384\", filename=each_file)\n   target_path = '\/home\/ec2-user\/SageMaker\/huggingface-sagemaker\/content\/dalle-mini\/vqgan\/' + each_file\n   shutil.copy(downloaded_file, target_path)\n\nDALLE_MODEL_LOCATION = '\/home\/ec2-user\/huggingface-sagemaker\/dalle_mini\/content\/dalle-mini'\nDALLE_COMMIT_ID = None\nmodel, params = DalleBart.from_pretrained(    \n      DALLE_MODEL_LOCATION, revision=DALLE_COMMIT_ID, dtype=jnp.float32, _do_init=False,\n)\n\nVQGAN_LOCAL_REPO = '\/home\/ec2-user\/SageMaker\/dalle_mini\/content\/dalle-mini\/vqgan'\nVQGAN_LCOAL_COMMIT_ID = None\nvqgan, vqgan_params = VQModel.from_pretrained(\n     VQGAN_LOCAL_REPO, revision=VQGAN_LCOAL_COMMIT_ID, _do_init=False\n)\n\n\nprint(model.config)\nprint(vqgan.config)\n\nDALLE_MODEL_LOCATION = '\/home\/ec2-user\/SageMaker\/dalle_mini\/content\/dalle-mini'\nDALLE_COMMIT_ID = None\nprocessor = DalleBartProcessor.from_pretrained(\n     DALLE_MODEL_LOCATION, \n     revision=DALLE_COMMIT_ID)\n\nprint(processor)\n\n# # Works for all available devices to replicate the module\nfrom flax.jax_utils import replicate\nimport random\n\nparams = replicate(params)\nvqgan_params = replicate(vqgan_params)\n\n@partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(3, 4, 5, 6))\ndef p_generate(\n    tokenized_prompt, key, params, top_k, top_p, temperature, condition_scale\n):\n  return model.generate(\n      **tokenized_prompt,\n      prng_key=key,\n      params=params,\n      top_k=top_k,\n      top_p=top_p,\n      temperature=temperature,\n      condition_scale=condition_scale,\n  )\n\n#decode the images\n@partial(jax.pmap, axis_name=\"batch\")\ndef p_decode(indices, params):\n    return vqgan.decode_code(indices, params=params)\n\n\n# entering the prompts\nprompts = [\n    \"sunset over a lake in the mountains\",\n    \"the Eiffel tower landing on the moon\",\n]\n\ntokenized_prompts = processor(prompts)\ntokenized_prompt = replicate(tokenized_prompts)\n\n\n\n# create a random key\nseed = random.randint(0, 2**32 - 1)\nkey = jax.random.PRNGKey(seed)\n\n\nn_predictions = 4\n\n# We can customize generation parameters (see https:\/\/huggingface.co\/blog\/how-to-generate)\ngen_top_k = None\ngen_top_p = None\ntemperature = None\ncond_scale = 10.0\n\nprint(f\"Prompts: {prompts}\\n\")\n\nimages = []\nfor i in trange(max(n_predictions \/\/ jax.device_count(), 1)):\n    # get a new key\n    key, subkey = jax.random.split(key)\n    # generate images\n    encoded_images = p_generate(\n        tokenized_prompt,\n        shard_prng_key(subkey),\n        params,\n        gen_top_k,\n        gen_top_p,\n        temperature,\n        cond_scale,\n    )\n    # remove BOS\n    encoded_images = encoded_images.sequences[..., 1:]\n    # decode images\n    decoded_images = p_decode(encoded_images, vqgan_params)\n    decoded_images = decoded_images.clip(0.0, 1.0).reshape((-1, 256, 256, 3))\n    for decoded_img in decoded_images:\n        img = Image.fromarray(np.asarray(decoded_img * 255, dtype=np.uint8))\n        images.append(img)\n        display(img)\n\n\nand the error I am getting is:\n\nImportError Traceback (most recent call last) ~\/SageMaker\/huggingface-sagemaker\/code\/inference.py in <module> 5 #import DalleBart 6 #from dalle_mini import DalleBart, DalleBartProcessor ----> 7 from vqgan_jax.modeling_flax_vqgan import VQModel 8 from typing_extensions import dataclass_transform 9 #from transformers import CLIPProcessor, FlaxCLIPModel\n\n~\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/vqgan_jax\/modeling_flax_vqgan.py in <module> 8 import jax.numpy as jnp 9 import numpy as np ---> 10 import flax.linen as nn 11 from flax.core.frozen_dict import FrozenDict 12\n\n~\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/flax\/init.py in <module> 16 \"\"\"Flax API.\"\"\" 17 ---> 18 from . import core as core 19 from . import linen as linen 20 from . import optim as optim\n\n~\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/flax\/core\/init.py in <module> 26 ) 27 ---> 28 from .scope import ( 29 Scope as Scope, 30 Array as Array,\n\n~\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/flax\/core\/scope.py in <module> 26 from flax import config 27 from flax import errors ---> 28 from flax import struct 29 from flax import traceback_util 30 from .frozen_dict import freeze\n\n~\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/flax\/struct.py in <module> 23 24 import jax ---> 25 from typing_extensions import dataclass_transform # pytype: disable=not-supported-yet 26 27\n\nImportError: cannot import name 'dataclass_transform' from 'typing_extensions' (\/home\/ec2-user\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/typing_extensions.py)\n\nPlease help me ASAP as I need to fix it urgently.",
        "Answers":[
            {
                "Answer_creation_time":"2022-07-29T07:46:18.042Z",
                "Answer_upvote_count":0,
                "Answer_body":"I am getting a different error now:\n\n\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/flax\/core\/lift.py:112: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead. scopes, treedef = jax.tree_flatten(scope_tree) \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/flax\/core\/lift.py:729: FutureWarning: jax.tree_leaves is deprecated, and will be removed in a future release. Use jax.tree_util.tree_leaves instead. lengths = set(jax.tree_leaves(lengths)) \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/flax\/core\/axes_scan.py:134: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead. in_avals, in_tree = jax.tree_flatten(input_avals) \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/flax\/linen\/transforms.py:249: FutureWarning: jax.tree_leaves is deprecated, and will be removed in a future release. Use jax.tree_util.tree_leaves instead. jax.tree_leaves(tree))) \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/flax\/core\/axes_scan.py:146: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead. broadcast_in, constants_out = jax.tree_unflatten(out_tree(), out_flat) Killed\n\nI have imported the library in the inference.py script:\n\nfrom jax.tree_util import (tree_flatten, tree_leaves, tree_unflatten)\n\nBut still I am experiencing this warning and the running of code is interrupts and the session gets killed.\n\nAny reason. Please let me know.\n\nThanks",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to specify target feature in Sagemaker XGBoost?",
        "Question_creation_time":1661221322082,
        "Question_link":"https:\/\/repost.aws\/questions\/QUoW_FqSbIQKW0MqNJLlA2AA\/how-to-specify-target-feature-in-sagemaker-xg-boost",
        "Question_topic":[
            "Machine Learning & AI",
            "Database"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Extract Transform & Load Data"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":26.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I am considering migrating a data science project from Datarobot to Sagemaker. I am familiar with writing Python and have been going through one of the tutorial Jupyter notebooks to see how to explore the data and to build and deploy and estimator. But, I cannot see how to specify the target feature. I have entirely numerical data in a csv file. One of the fields in that file is the intended target for estimation, the rest are information from which the estimate is to be made.\n\nHow do I specify the column that is to be estimated? The code I expect should have this is ...\n\ncontainer = sm.image_uris.retrieve(\"xgboost\", session.boto_region_name, \"1.5-1\")\n\nxgb = sm.estimator.Estimator(\n    container,\n    role,\n    instance_count=1,\n    instance_type=\"ml.m4.xlarge\",\n    output_path=\"s3:\/\/xxxxxx001\/\",\n    sagemaker_session=session,\n)\n\nxgb.set_hyperparameters(\n    max_depth=5,\n    eta=0.2,\n    gamma=4,\n    min_child_weight=6,\n    subsample=0.8,\n    verbosity=0,\n    num_round=100,\n)\ns3_input_train = TrainingInput(\n    s3_data=\"s3:\/\/xxxxxx001\/data.csv\", content_type=\"csv\"\n)\nxgb.fit({\"train\": s3_input_train})",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-23T23:42:54.174Z",
                "Answer_upvote_count":0,
                "Answer_body":"On a badly formatted page on the AWS documentation, I found a statement that - the CSV file must have no headers and the target field must be the first field. So, apparently, it is not possible to specify the target. So primitive, yeah?",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS SageMaker Endpoint Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check",
        "Question_creation_time":1639162238458,
        "Question_link":"https:\/\/repost.aws\/questions\/QUV4VuYCKHTEedPanW-keHTQ\/aws-sage-maker-endpoint-failed-reason-the-primary-container-for-production-variant-all-traffic-did-not-pass-the-ping-health-check",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":983.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Links to the AWS notebooks for reference https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/xgboost_bring_your_own_model\/xgboost_bring_your_own_model.ipynb\n\nhttps:\/\/github.com\/aws-samples\/amazon-sagemaker-local-mode\/blob\/main\/xgboost_script_mode_local_training_and_serving\/code\/inference.py\n\nI am using the example from the notebooks to create and deploy an endpoint to AWS SageMaker Cloud. I have passed all the checks locally and when I attempt to deploy the endpoint I run into the issue.\n\nCode\n\nIn my local notebook (my personal machine NOT sagemaker notebook):\n\n    import pandas\n    import xgboost\n    from xgboost import XGBRegressor\n    import numpy as np\n    from sklearn.model_selection import train_test_split, RandomizedSearchCV\n    \n    print(xgboost.__version__)\n    1.0.1\n\n    # Fit model\n    r.fit(X_train.toarray(), y_train.values)\n\n    xgbest = r.best_estimator\n\n\n\nAWS SageMaker Endpoint code\n\nimport boto3\nimport pickle\nimport sagemaker\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\nfrom time import gmtime, strftime\n\nregion = boto3.Session().region_name\n\nrole = 'arn:aws:iam::111:role\/xxx-sagemaker-role'\n\nbucket = 'ml-model'\nprefix = \"sagemaker\/xxx-xgboost-byo\"\nbucket_path = \"https:\/\/s3-{}.amazonaws.com\/{}\".format('us-west-1', 'ml-model')\n\nclient = boto3.client(\n    's3',\n    aws_access_key_id=xxx\n    aws_secret_access_key=xxx\n)\nclient.list_objects(Bucket=bucket)\n\n\n\nSave the model\n\n# save the model, either xgbest \nmodel_file_name = \"xgboost-model\"\n\n# using save_model\n# xgb_model.save_model(model_file_name)\n\npickle.dump(xgbest, open(model_file_name, 'wb'))`\n\n!tar czvf xgboost_model.tar.gz $model_file_name\n\n\n\nUpload to S3\n\nkey = 'xgboost_model.tar.gz'\n\nwith open('xgboost_model.tar.gz', 'rb') as f:\n    client.upload_fileobj(f, bucket, key)\n\n\nImport model\n\n# Import model into hosting\ncontainer = get_image_uri(boto3.Session().region_name, \"xgboost\", \"0.90-2\")\nprint(container)\n\nxxxxxx.dkr.ecr.us-west-1.amazonaws.com\/sagemaker-xgboost:0.90-2-cpu-py3\n\n%%time\n\nmodel_name = model_file_name + datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\nmodel_url = \"https:\/\/s3-{}.amazonaws.com\/{}\/{}\".format(region, bucket, key)\n\nfrom sagemaker.xgboost import XGBoost, XGBoostModel\nfrom sagemaker.session import Session\nfrom sagemaker.local import LocalSession\n\n\nsm_client = boto3.client(\n                         \"sagemaker\",\n                         region_name=\"us-west-1\",\n                         aws_access_key_id='xxxx',\n                         aws_secret_access_key='xxxx'\n                        )\n\n# Define session\nsagemaker_session = Session(sagemaker_client = sm_client)\n\nmodels3_uri = \"s3:\/\/ml-model\/xgboost_model.tar.gz\"\n\nxgb_inference_model = XGBoostModel(\n                                   model_data=models3_uri,\n                                   role=role,\n                                   entry_point=\"inference.py\",\n                                   framework_version=\"0.90-2\",\n                                   # Cloud\n                                   sagemaker_session = sagemaker_session\n                                   # Local\n                                   # sagemaker_session = None\n           \n)\n\n#serializer = StringSerializer(content_type=\"text\/csv\")\npredictor = xgb_inference_model.deploy(\n                                       initial_instance_count = 1,\n                                       # Cloud\n                                       instance_type=\"ml.t2.large\",\n                                       # Local\n                                       # instance_type = \"local\",\n                                       serializer = \"text\/csv\"\n)\n\n\nif xgb_inference_model.sagemaker_session.local_mode == True:\n    print('Deployed endpoint in local mode')\nelse:\n    print('Deployed endpoint to SageMaker AWS Cloud')\n\n\n\/Applications\/Anaconda\/anaconda3\/lib\/python3.9\/site-packages\/sagemaker\/session.py in wait_for_endpoint(self, endpoint, poll)\n   3354         if status != \"InService\":\n   3355             reason = desc.get(\"FailureReason\", None)\n-> 3356             raise exceptions.UnexpectedStatusException(\n   3357                 message=\"Error hosting endpoint {endpoint}: {status}. Reason: {reason}.\".format(\n   3358                     endpoint=endpoint, status=status, reason=reason\n\nUnexpectedStatusException: Error hosting endpoint sagemaker-xgboost-xxxx: Failed. Reason:  The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..",
        "Answers":[
            {
                "Answer_creation_time":"2021-12-13T18:36:06.840Z",
                "Answer_upvote_count":0,
                "Answer_body":"Please make sure that the trained model used was trained on same version of XGBoost that is used while deploying the endpoint.\n\nAlso verify there are no typo's in your script while deploying the endpoint.\n\nI'd also check CloudWatch logs to find any information on the error encountered. If you are still not able to identify the issue, I'd recommend you to reach out to AWS Support for further investigation of the issue:\n\nOpen a support case with AWS using the link: https:\/\/console.aws.amazon.com\/support\/home?#\/case\/create",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to get batch transform with jsonl data?",
        "Question_creation_time":1661703317094,
        "Question_link":"https:\/\/repost.aws\/questions\/QUkP-cRiP3QiCAIqnwyirz1A\/how-to-get-batch-transform-with-jsonl-data",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon CloudWatch Logs"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":34.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am using my own inference.py file as a entry point for inference. I have tested this pytorch model, served as a real time endpoint in amaon sagemaker. but when i try to create a batch job and use multiple json object in my input file (jsonl format) . i get the following error at the input_fn function on this line data = json.loads(request_body), in cloudwatch logs ==>\n\ndata = json.loads(request_body) raise JSONDecodeError(\"Extra data\", s, end) json.decoder.JSONDecodeError: Extra data : line 2 column 1 (Char ..)\n\nI am not sure why am i getting extra data on line 2 error, because this is supposed to be batch job with multiple json input and each line.\n\ninference.py\n\ndef model_fn(model_dir):\n   \/\/load the model\n\n\n\n\ndef input_fn(request_body, request_content_type):\n    input_data= json.loads(request_body)\n    return data\n\ndef predict_fn(input_data, model):\n    return model.predict(input_data)\n\n\nset up batch job\n\nresponse = client.create_transform_job(\n    TransformJobName='some-job',\n    ModelName='mypytorchmodel',\n    ModelClientConfig={\n        'InvocationsTimeoutInSeconds': 3600,\n        'InvocationsMaxRetries': 1\n    },\n    BatchStrategy='MultiRecord',\n    TransformInput={\n        'DataSource': {\n            'S3DataSource': {\n                'S3DataType': 'S3Prefix',\n                'S3Uri': 's3:\/\/inputpath'\n            }\n        },\n        'ContentType': 'application\/json',\n        'SplitType': 'Line'\n    },\n    TransformOutput={\n        'S3OutputPath': 's3:\/\/outputpath',\n        'Accept': 'application\/json',\n        'AssembleWith': 'Line',\n    },\n    TransformResources={\n        'InstanceType': 'ml.g4dn.xlarge'\n        'InstanceCount': 1\n    }\n)\n\n\ninput file\n\n{\"input\" : \"some text here\"}\n{\"input\" : \"another\"}\n...",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-30T16:00:00.274Z",
                "Answer_upvote_count":1,
                "Answer_body":"You're seeing this because of your MultiRecord batch strategy: SageMaker is aware of how to split your source data files into individual records (because you configured SplitType), but is composing batches with multiple records and trying to send those through to your model\/endpoint. It seems like your inference input handler is not capable of interpreting JSONLines chunks, only single JSON objects.\n\nOne way of fixing this would be to switch to SingleRecord batch strategy, which would result in each record triggering a separate inference request to your model.\n\nIf you're concerned about the HTTP overhead of request-per-record limiting your job performance, you could alternatively stick with MultiRecord but edit your input_fn to handle JSONLines data. I'd probably suggest to set a different ContentType to explicitly signal your container when to expect JSONLines vs single-record JSON. Your input_fn can detect that different request_content_type (e.g. application\/x-jsonlines) and use a different parsing logic.\n\nI'm not 100% sure whether the request_body supports iterating through lines like a file would ([json.loads(l) for l in request_body]), whether you could treat it like a string ([json.loads(l) for l in request_body.split(\"\\n\")]), or perhaps it's a binary string you'd need to decode first e.g. request_body.decode(\"utf-8\").split(\"\\n\")... Would need to check - but something along these lines should allow you to first split your body by newlines, then parse each line as a valid JSON object.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Batch Transform local mode?",
        "Question_creation_time":1571055107000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUtNtH0LyFSLCXc0xCV4hYkw\/sage-maker-batch-transform-local-mode",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":151.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nA customer is experimenting with SageMaker batch transform with parquet and is interested is some form of local development to speedup iteration. Does SageMaker Batch Transform support local mode?",
        "Answers":[
            {
                "Answer_creation_time":"2019-10-17T09:18:46.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can do local testing by running the container in serve mode as a docker. Then using Curl\/Postman to send an HTTP request and inspecting the response.\n\nThe request can be CSV\/JSON or binary (a parquet file in your case).\n\nIf you're able to run the Pytorch model in serve mode locally, then this local testing provides a lot of coverage before running in Batch Transform itself.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is there a way to automate failure handling and retries when using Amazon SageMaker batch transform?",
        "Question_creation_time":1593595381000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE10OtSwDRCiB-0pP6wflYQ\/is-there-a-way-to-automate-failure-handling-and-retries-when-using-amazon-sage-maker-batch-transform",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":120.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"How does Amazon SageMaker batch transform handle failures? Is there a way to automate failure handling and retries built into the service?",
        "Answers":[
            {
                "Answer_creation_time":"2020-07-01T15:34:51.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can use the ModelClientConfig API to configure the timeout and maximum number of retries for processing a transform job invocation. The maximum number of automated retries is three.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker inference on inf1 no opencv",
        "Question_creation_time":1649704234930,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUG9fHLN3TNGbXjRmuwRBcA\/sagemaker-inference-on-inf-1-no-opencv",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Inferentia"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":123.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to deploy Pytorch model on ml.inf1.xlarge instance. Image: 301217895009.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-neo-pytorch:1.5.1-inf-py3 My python code using some oepncv functions, and when I am trying to run the infernce I got the following error: ModuleNotFoundError: No module named 'cv2'\n\nI tried to add opencv-python-headless to requirements.txt, but then I got another error ImportError: libgthread-2.0.so.0: cannot open shared object file\n\nHow I can use opencv with the ml.inf1 instances?",
        "Answers":[
            {
                "Answer_creation_time":"2022-05-11T21:49:47.674Z",
                "Answer_upvote_count":0,
                "Answer_body":"When running Neo inference on Sagemaker, it\u2019s possible to now use the Deep Learning Containers (DLC) provided by AWS. It\u2019s also unnecessary to remain on Pytorch 1.5 when using the DLC images. In our latest released image, OpenCV is pre-installed along with a more recent version of AWS Neuron SDK. Here\u2019s a link to the latest docker image for Pytorch 1.10 + Neuron SDK 1.17: https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.10\/py3\/sdk1.17.1\/Dockerfile.neuron#L83\n\nIf you\u2019re unable to easily move to the latest Pytorch+Neuron DLC, the dockerfile link may help with resolving installation errors of OpenCV into your container.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"GPU not detected by tensorflow in SM Studio",
        "Question_creation_time":1627628146000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYk1rHeoARsSfMsuiL-0JgQ\/gpu-not-detected-by-tensorflow-in-sm-studio",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":184.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm currently using SageMaker Studio with kernel \"Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)\".\nTensorflow doesn't detect the GPU on both ml.g4dn.xlarge and ml.g4dn.2xlarge instances (with 1 GPU).\nI would appreciate any advice.\n\nimport tensorflow as tf\ntf.config.list_physical_devices('GPU')\n: []\n\nEdited by: haganHL on Jul 29, 2021 11:55 PM",
        "Answers":[
            {
                "Answer_creation_time":"2021-08-18T14:52:01.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Downgrading to the Python 3 (TensorFlow 2.1 Python 3.6 GPU Optimized) kernel allowed the GPU to be recognized. However, I had to recode my notebook to support that kernel. What a shame. What a pain.\n\nEdited by: jmlineb on Aug 18, 2021 8:00 AM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-08-18T14:46:52.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I am having the very same problem with the very same kernel using the very same instance types. It worked perfectly a month ago. Attempting to downgrade my kernel to see if my notebook will still work.\n\nEdited by: jmlineb on Aug 18, 2021 7:47 AM\n\nEdited by: jmlineb on Aug 18, 2021 8:11 AM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-08-24T11:50:30.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, I'm having the same problem here. I can't believe that Amazon hasn't fixed this - we could really use support for GPUs on new Tensorflow versions!",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"3d Ground truth labelling job issue in bbox",
        "Question_creation_time":1657799060933,
        "Question_link":"https:\/\/repost.aws\/questions\/QUnfsUXUBLT0i2HEn3-CW3jQ\/3-d-ground-truth-labelling-job-issue-in-bbox",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":45.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi, I have created a 3d labelling job for 100 frames and got the job running. When I started annotating the objects with cuboid I am facing 2 issues,\n\nThe size of the bounding box for respective object is changing (increased or decreased) as I move to the next frames and drawing the bounding box (with increased or decreased size to the earlier frames). I believe this could be because of autofill functionality. Please confirm me whether I have an option to off the autofill functionality.\nThe bounding boxes are getting scattered or displaced each time I login and start working. If I annotate the objects for 10 frames, save it and logout for today, later when I login back the bounding boxes were scattered for couple of frames. Can you please let me know what could be done to reduce this issue.",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker g4 and g5 instances do not have working nvidia-drivers",
        "Question_creation_time":1669082188275,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBqYWuFr7SyC6P6Uae9LOww\/sagemaker-g-4-and-g-5-instances-do-not-have-working-nvidia-drivers",
        "Question_topic":[
            "Machine Learning & AI",
            "Compute"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "GPU Development"
        ],
        "Question_upvote_count":3.0,
        "Question_view_count":80.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am a heavy user of g4 and g5 instances on Sagemaker (notebook instances). Today when I tried to use the same instances as I always do I was met with the following when running nvidia-smi\n\nNVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n\nThese are all the exact same instances and workloads I have used before. The same message was found when trying to run on ec2 natively as well.",
        "Answers":[
            {
                "Answer_creation_time":"2022-11-23T04:32:10.418Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nThis is a NVIDIA driver issue which affect all NVIDA functions.\n\nCould you please try using the following cmds to unblock\n\nsudo dkms remove nvidia\/510.47.03 --all\n\nsudo dkms install nvidia\/510.47.03 -k $(uname -r)\n\n\nPlease let me know if this would work.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Running concurrent sessions from SageMaker notebooks on Glue Dev Endpoints.",
        "Question_creation_time":1591020062000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIDitlJMgTlGai61w_Zvqdg\/running-concurrent-sessions-from-sage-maker-notebooks-on-glue-dev-endpoints",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics",
            "Database"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Glue",
            "Extract Transform & Load Data"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":157.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Customer who has created a AWS glue dev endpoint and want to run two Sagemaker notebooks in parallel on same single Dev endpoint but its not working .\n\nThe one which is invoked first is only able to run the job, while another one fails. what could be possible reasons and fix for it?",
        "Answers":[
            {
                "Answer_creation_time":"2020-06-01T16:52:06.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"SageMaker notebooks are Jupyter notebooks that uses the SparkMagic module to connect to a local Livy setup. The local Livy does an SSH tunnel to Livy service on the Glue Spark server. Apache Livy binds to post 8998 and is a RESTful service that can relay multiple Spark session commands at the same time so multiple port binding conflicts cannot happen. So yes, you can have multiple sessions as long as the backend cluster has resources to serve that many sessions.\n\nYou can run the following command in a notebook to check the defaults for Spark sessions:\n\nspark.sparkContext.getConf().getAll()\n\n\nI see the following defaults in my Spark session. You can easily override them from the config file at ~\/.sparkmagic\/config.json or by using the %%configure magic from within the notebook.\n\nspark.executor.cores 4\nspark.executor.memory 5g\nspark.driver.memory 5g\n\n\nNote that spark.executor.instances is not set and spark.dynamicAllocation.enabled is not overridden which means that it is true, so if you have a demanding Spark job in one notebook, it can take over all resources in the cluster and prevent other Spark sessions from starting. The recommendation when sharing a single Glue Dev endpoint is to limit each session to a few executors so that multiple sessions can acquire resources from the cluster e.g.:\n\n%%configure -f\n{\"executorMemory\": \"5G\", \"executorCores\":4,\"numExecutors\":2}\n\n\n(Note: Tested on multiple SageMaker PySpark notebooks in single SageMaker notebook instances as well as multiple SageMaker notebook instances.)",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Lambda Function to invoke sagemaker endpoint",
        "Question_creation_time":1657965474513,
        "Question_link":"https:\/\/repost.aws\/questions\/QU33wE3pnRS9Om2yfVt4EIAg\/lambda-function-to-invoke-sagemaker-endpoint",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Lambda",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":107.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi AWS, I need to create a lambda function that will invoke the SageMaker endpoint that will send a text description for which it will return a generated image.\n\nThe endpoint is generated using HuggingFace model.\n\nI need your help with the code and the steps to obtain it.\n\nThanks Arjun Goel",
        "Answers":[
            {
                "Answer_creation_time":"2022-07-17T16:20:21.662Z",
                "Answer_upvote_count":0,
                "Answer_body":"There is a blog that shows how you can invoke a Sagemaker endpoint from a lambda function - https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-16T18:28:31.470Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yes I have gone through the approach as mentioned by you above but the problem is I am not a Machine Learning Expert so it will take me sometime to write inference for Text-to-Image. If you have python code for that it would be great as I need to complete that on urgent basis.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to set up a pipe mode in sagemaker?",
        "Question_creation_time":1650141519238,
        "Question_link":"https:\/\/repost.aws\/questions\/QUoMtSoBPGQpabpreLah_Fjg\/how-to-set-up-a-pipe-mode-in-sagemaker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":113.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"what other data input types can be used pipe input mode in sagemaker? an example of implementation is here https:\/\/aws.amazon.com\/blogs\/aws\/sagemaker-nysummit2018\/, and can this be used for inference as well as, similar to training?",
        "Answers":[
            {
                "Answer_creation_time":"2022-04-19T18:16:18.685Z",
                "Answer_upvote_count":0,
                "Answer_body":"Pipe mode support reading data from Amazon S3. Pipe mode can be used for Batch inference as described in the blog.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deploying a Random Forest Model on Amazon Sagemaker always getting a UnexpectedStatusException with Reason: AlgorithmError",
        "Question_creation_time":1661503022955,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMmnoFG_HQ0qiMWxlSlPlcQ\/deploying-a-random-forest-model-on-amazon-sagemaker-always-getting-a-unexpected-status-exception-with-reason-algorithm-error",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Developer Tools",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Lambda",
            "AWS CodeDeploy",
            "Amazon SageMaker",
            "Amazon Machine Images (AMI)"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":21.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hey I am trying to deploy my RandomForest Classifier on Amazon Sagemaker but get a StatusException Error even though the script worked fine before:\n\nThe script runs fine and prints out the confusion matrix and accuracy as expected. When I try to deploy the model to amazon Sagemaker using the script it does not work.\n\n! python script.py --n-estimators 100\n--max_depth 2\n--model-dir .\/\n--train .\/\n--test .\/ \\\n\nConfusion Matrix: [[13 8] [ 1 17]] Accuracy: 0.7692307692307693\n\nI used the Estimator from Sagemaker Python SDK\n\nfrom sagemaker.sklearn.estimator import SKLearn sklearn_estimator = SKLearn( entry_point='script.py', role = get_execution_role(), instance_count=1, instance_type='ml.m4.xlarge', framework_version='0.20.0', base_job_name='rf-scikit')\n\nI launched the training job as follows\n\nsklearn_estimator.fit({'train':trainpath, 'test': testpath}, wait=False)\n\nHere I am trying to deploy the model which leads to the StatusExceptionError that I cannot seem to fix\n\nsklearn_estimator.latest_training_job.wait(logs='None') artifact = m_boto3.describe_training_job( TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts'['S3ModelArtifacts']\n\nprint('Model artifact persisted at ' + artifact)\n\n2022-08-25 12:03:27 Starting - Starting the training job.... 2022-08-25 12:03:52 Starting - Preparing the instances for training............ 2022-08-25 12:04:55 Downloading - Downloading input data...... 2022-08-25 12:05:31 Training - Downloading the training image......... 2022-08-25 12:06:22 Training - Training image download completed. Training in progress.. 2022-08-25 12:06:32 Uploading - Uploading generated training model. 2022-08-25 12:06:43 Failed - Training job failed\n\nUnexpectedStatusException Traceback (most recent call last) <ipython-input-37-628f942a78d3> in <module> ----> 1 sklearn_estimator.latest_training_job.wait(logs='None') 2 artifact = m_boto3.describe_training_job( 3 TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts'] 4 5 print('Model artifact persisted at ' + artifact)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in wait(self, logs) 2109 self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs) 2110 else: -> 2111 self.sagemaker_session.wait_for_job(self.job_name) 2112 2113 def describe(self):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_job(self, job, poll) 3226 lambda last_desc: _train_done(self.sagemaker_client, job, last_desc), None, poll 3227 ) -> 3228 self._check_job_status(job, desc, \"TrainingJobStatus\") 3229 return desc 3230\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name) 3390 message=message, 3391 allowed_statuses=[\"Completed\", \"Stopped\"], -> 3392 actual_status=status, 3393 ) 3394\n\nUnexpectedStatusException: Error for Training job rf-scikit-2022-08-25-12-03-25-931: Failed. Reason: AlgorithmError: framework error: Traceback (most recent call last): File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_trainer.py\", line 84, in train entrypoint() File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py\", line 39, in main train(environment.Environment()) File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py\", line 35, in train runner_type=runner.ProcessRunnerType) File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/entry_point.py\", line 100, in run wait, capture_error File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py\", line 291, in run cwd=environment.code_dir, File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py\", line 208, in check_error info=extra_info, sagemaker_training.errors.ExecuteUserScriptError: ExecuteUserScriptError: ExitCode 1 ErrorMessage \"\" Command \"\/miniconda3\/bin\/python script.py\"\n\nExecuteUserScriptErr\n\nI am happy for some help",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Exporting Sagemaker model to local computer",
        "Question_creation_time":1649274099076,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKgLZZhWVSg2d5XJWwbaTiA\/exporting-sagemaker-model-to-local-computer",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon S3 Glacier",
            "Build & Train ML Models",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":341.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I hyper-tuned an XGBoost model, deployed the model and created an endpoint. Is there a way to export the model to my local computer? That way I can test the model locally.",
        "Answers":[
            {
                "Answer_creation_time":"2022-04-08T09:11:28.188Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nYou can easily do this, either from the console or using the SageMaker SDK.\n\nFrom the console navigate to:\n\nSageMaker > Training > Training Jobs > Select the training job you wish > Scroll near the bottom and find the Output > click on the S3 link > click on download.\n\nIn case you used SageMaker model tuning and you have a lot of training jobs, you can instead go to\n\nSageMaker > Training > Hyperparameter tuning jobs > Select tuning job > Best training job > click on the name link > this takes you to the training job, > Scroll near the bottom and find the Output > click on the S3 link > click on download.\n\nOnce you have downloaded the model, you can use the open-source xgboost package to load the model and perform predictions as you wish on your local system.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"using transformers module with sagemaker studio project: ModuleNotFoundError: No module named 'transformers'",
        "Question_creation_time":1664396753855,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdd2zOBY0Q4CEG1ZdbgNsgA\/using-transformers-module-with-sagemaker-studio-project-module-not-found-error-no-module-named-transformers",
        "Question_topic":[
            "Developer Tools",
            "Machine Learning & AI",
            "Management & Governance",
            "DevOps"
        ],
        "Question_tag":[
            "AWS CodePipeline",
            "Amazon SageMaker",
            "AWS Command Line Interface",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":81.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"So as mentioned in my other recent post, I'm trying to modify the sagemaker example abalone xgboost template to use tensorfow.\n\nMy current problem is that running the pipeline I get a failure and in the logs I see:\n\nModuleNotFoundError: No module named 'transformers'\n\n\nNOTE: I am importing 'transformers' in preprocess.py not in pipeline.py\n\nNow I have 'transformers' listed in various places as a dependency including:\n\nsetup.py - required_packages = [\"sagemaker==2.93.0\", \"sklearn\", \"transformers\", \"openpyxl\"]\npipelines.egg-info\/requires.txt - transformers (auto-generated from setup.py?)\n\nbut so I'm keen to understand, how can I ensure that additional dependencies are available in the pipline itself?\n\nMany thanks in advance\n\nADDITIONAL DETAILS ON HOW I ENCOUNTERED THE ERROR\n\nFrom one particular notebook (see previous post for more details) I have succesfully constructed the new topic\/tensorflow pipeline and run the following steps:\n\npipeline.upsert(role_arn=role)\nexecution = pipeline.start()\nexecution.describe()\n\n\nthe describe() method gives this output:\n\n{'PipelineArn': 'arn:aws:sagemaker:eu-west-1:398371982844:pipeline\/topicpipeline-example',\n 'PipelineExecutionArn': 'arn:aws:sagemaker:eu-west-1:398371982844:pipeline\/topicpipeline-example\/execution\/0aiczulkjoaw',\n 'PipelineExecutionDisplayName': 'execution-1664394415255',\n 'PipelineExecutionStatus': 'Executing',\n 'PipelineExperimentConfig': {'ExperimentName': 'topicpipeline-example',\n  'TrialName': '0aiczulkjoaw'},\n 'CreationTime': datetime.datetime(2022, 9, 28, 19, 46, 55, 147000, tzinfo=tzlocal()),\n 'LastModifiedTime': datetime.datetime(2022, 9, 28, 19, 46, 55, 147000, tzinfo=tzlocal()),\n 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:eu-west-1:398371982844:user-profile\/d-5qgy6ubxlbdq\/sjoseph-reg-genome-com-273',\n  'UserProfileName': 'sjoseph-reg-genome-com-273',\n  'DomainId': 'd-5qgy6ubxlbdq'},\n 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:eu-west-1:398371982844:user-profile\/d-5qgy6ubxlbdq\/sjoseph-reg-genome-com-273',\n  'UserProfileName': 'sjoseph-reg-genome-com-273',\n  'DomainId': 'd-5qgy6ubxlbdq'},\n 'ResponseMetadata': {'RequestId': 'f949d6f4-1865-4a01-b7a2-a96c42304071',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': 'f949d6f4-1865-4a01-b7a2-a96c42304071',\n   'content-type': 'application\/x-amz-json-1.1',\n   'content-length': '882',\n   'date': 'Wed, 28 Sep 2022 19:47:02 GMT'},\n  'RetryAttempts': 0}}\n\n\nWaiting for the execution I get:\n\n---------------------------------------------------------------------------\nWaiterError                               Traceback (most recent call last)\n<ipython-input-14-72be0c8b7085> in <module>\n----> 1 execution.wait()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in wait(self, delay, max_attempts)\n    581             waiter_id, model, self.sagemaker_session.sagemaker_client\n    582         )\n--> 583         waiter.wait(PipelineExecutionArn=self.arn)\n    584 \n    585 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/waiter.py in wait(self, **kwargs)\n     53     # method.\n     54     def wait(self, **kwargs):\n---> 55         Waiter.wait(self, **kwargs)\n     56 \n     57     wait.__doc__ = WaiterDocstring(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/waiter.py in wait(self, **kwargs)\n    376                     name=self.name,\n    377                     reason=reason,\n--> 378                     last_response=response,\n    379                 )\n    380             if num_attempts >= max_attempts:\n\nWaiterError: Waiter PipelineExecutionComplete failed: Waiter encountered a terminal failure state: For expression \"PipelineExecutionStatus\" we matched expected path: \"Failed\"\n\n\nWhich I assume is corresponding to the failure I see in the logs:\n\nI did also run python setup.py build to ensure my build directory was up to date ... here's the terminal output of that command:\n\nsagemaker-user@studio$ python setup.py build\n\/opt\/conda\/lib\/python3.9\/site-packages\/setuptools\/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n  warnings.warn(\n\/opt\/conda\/lib\/python3.9\/site-packages\/setuptools\/config\/setupcfg.py:508: SetuptoolsDeprecationWarning: The license_file parameter is deprecated, use license_files instead.\n  warnings.warn(msg, warning_class)\nrunning build\nrunning build_py\ncopying pipelines\/topic\/pipeline.py -> build\/lib\/pipelines\/topic\nrunning egg_info\nwriting pipelines.egg-info\/PKG-INFO\nwriting dependency_links to pipelines.egg-info\/dependency_links.txt\nwriting entry points to pipelines.egg-info\/entry_points.txt\nwriting requirements to pipelines.egg-info\/requires.txt\nwriting top-level names to pipelines.egg-info\/top_level.txt\nreading manifest file 'pipelines.egg-info\/SOURCES.txt'\nadding license file 'LICENSE'\nwriting manifest file 'pipelines.egg-info\/SOURCES.txt'\n\n\nIt seems like the dependencies are being written to pipelines.egg-info\/requires.txt but are these not being picked up by the pipeline?",
        "Answers":[
            {
                "Answer_creation_time":"2022-09-29T11:42:35.510Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi! There are two places where you need to install the dependencies \/ requirements:\n\nIn your environment where you execute pipeline.start() \u2013 can be Amazon SageMaker Studio, your local machine or CI\/CD pipeline executor, e. g. AWS CodeBuild. These dependencies are installed in setup.py.\nInside the SageMaker processing and training jobs as well as in inference endpoints. This is usually done via requirements.txt file that you submit as part of your source_dir.\n\nIn your example, I recommend you to use the TensorFlowProcessor. The way how to install dependencies into it is described in the corresponding section of the documentation, in particular:\n\nSageMaker Processing installs the dependencies in requirements.txt in the container for you.\n\nSame applies to your model training and to the TensorFlow estimator. See the section Use third-party libraries in the TensorFlow documentation of the SageMaker Python SDK, in particular:\n\nIf there are other packages you want to use with your script, you can use a requirements.txt to install other dependencies at runtime.\n\nHope it helps!",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Relevancy of SageMaker Model Monitor for NLP?",
        "Question_creation_time":1605005518000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxCKLg-eiQ1mwZvzFyczBEg\/relevancy-of-sage-maker-model-monitor-for-nlp",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":61.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nCan SageMaker Model Monitor be applied in NLP models? Is it necessary to do some preprocessing of the data? How can we use SageMaker Model Monitor? sentence length, unseen words, language etc. Any thoughts or experience on that?",
        "Answers":[
            {
                "Answer_creation_time":"2020-11-10T11:24:02.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yes, You can use model monitor for data capture and scheduling in your own custom container with the relevant monitoring for NLP use case.\nFor example, there's a blog post for model monitor for computer vision classification prediction with defined *alert * of predict more than expected.\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/automated-monitoring-of-your-machine-learning-models-with-amazon-sagemaker-model-monitor-and-sending-predictions-to-human-review-workflows-using-amazon-a2i\/?nc1=b_rp",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Has SAS code ever been successfully ran on SageMaker?",
        "Question_creation_time":1596105364000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqMU2EBqGTDCMTPsB5rjNoQ\/has-sas-code-ever-been-successfully-ran-on-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":94.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Has SAS code ever been successfully ran on SageMaker?",
        "Answers":[
            {
                "Answer_creation_time":"2020-07-30T14:02:45.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I\u2019ve helped customers run SAS on a notebook through a kernel and that was their main use case, but we also showed them how they can containerize SAS. Worked well",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can make multi model endpoint with SageMaker?",
        "Question_creation_time":1660122368052,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJQBp6A_dSQm1RJ3f8AYMmg\/how-can-make-multi-model-endpoint-with-sage-maker",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":53.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"This is my code.\n\nfrom datetime import datetime\nfrom sagemaker.multidatamodel import MultiDataModel\nmme = MultiDataModel(\n    name=\"LV-multi-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"),\n    model_data_prefix=model_dir, # 2\uc5d0\uc11c \uad6c\ud55c \ubaa8\ub378\uc774 \ubaa8\uc5ec\uc788\ub294 \ud3f4\ub354(\uacbd\ub85c)!!,\n    model=sagemaker_model,  # \ubaa8\ub378 \uac1d\uccb4 1\uac1c \uc6b0\uc120 \ub123\uae30\n    sagemaker_session=sess\n)\n\npredictor = mme.deploy(\n    initial_instance_count=1,\n    instance_type=\"ml.g4dn.xlarge\"\n)\n\nAnd error message. How can I find Ecr Image(within multi-models=true)?\n\nClientError: An error occurred (ValidationException) when calling the CreateModel operation: Your Ecr Image 763104351884.dkr.ecr.ap-northeast-2.amazonaws.com\/pytorch-inference:1.8.1-gpu-py3 does not contain required com.amazonaws.sagemaker.capabilities.multi-models=true Docker label(s).",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-10T13:57:13.476Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi there - thanks for opening this thread. Multi-model endpoints are not supported on GPU instance types, see here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-model-endpoints.html#multi-model-endpoint-instance\n\nIn order to host a multi-model endpoint, choose a CPU instance type instead. The ECR image for CPUs will contain the required com.amazonaws.sagemaker.capabilities.multi-models=true label, see here: https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Neo compilation load error",
        "Question_creation_time":1548810043000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUW8F63wxUTSCCdXf0MESXIg\/neo-compilation-load-error",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":24.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\n\nI tried to follow the documentation to run a Neo compilation job on AWS console. I downloaded the model from http:\/\/download.tensorflow.org\/models\/mobilenet_v1_2018_02_22\/mobilenet_v1_1.0_224.tgz and uploaded it to S3. I did everything else the same as the documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo-job-compilation-console.html. But I got this error:\n\"Load Error: InputConfiguration: Exactly one .pb file is allowed for Tensorflow models.\"\nI checked the tar.gz file that there is only one .pn file. What caused the error?\nThank you very much!",
        "Answers":[
            {
                "Answer_creation_time":"2019-08-04T20:30:32.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Well. My bad. \"Exactly one pb file\" means there can be only one file in the gz compressed file and that single file must have the format pb. I hope the example can make it more clear in the future.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-02-15T17:21:13.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks ac4289 ,\n\nI was facing the same error and then removing unnecessary files from tar file helped me solve my issue.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Studio projects in VpcOnly mode without internet access",
        "Question_creation_time":1615480055000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcyhpq1pxRTmtjkDRAh_MDA\/sage-maker-studio-projects-in-vpc-only-mode-without-internet-access",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":323.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"A customer is using SageMaker Studio in VpcOnly mode (VPC, protected subnets without internet access, NO NAT gateways). The all functionality is fine. However, when I try create a SageMaker projects - as described here, SageMaker Studio is unable to list the project templates (timeout and unspecified error) resulting in empty list of the available project templates.\n\nProjects are enabled for the users - as described here. The problem is with project creation.\n\nIs internet access (e.g. via NAT gateways) is needed for SageMaker projects?",
        "Answers":[
            {
                "Answer_creation_time":"2021-04-10T20:10:40.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Figured it out. SageMaker Studio projects need Service Catalog access and VPCE for com.amazonaws.${AWS::Region}.servicecatalog",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Error on DeleteEndpoint operation: Cannot update in-progress endpoint",
        "Question_creation_time":1652103997301,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2NawO4aWQvmytekX8xJJNQ\/error-on-delete-endpoint-operation-cannot-update-in-progress-endpoint",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":42.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\n\nI have created and endpoint in sagemaker using boto3 but never finishes creation, is stuck in Creating status for few days now. I have tried to delete it using the aws cli api but i get the message:\n\nAn error occurred (ValidationException) when calling the DeleteEndpoint operation: Cannot update in-progress endpoint\n\nUsually endpoint fails after some time and can deleted but this time doesn't fail. Is there any way to force deletion?",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Training a classifier on parquet with SageMaker ?",
        "Question_creation_time":1588841008000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCqvDUq4hSQqRT97tBUvE8Q\/training-a-classifier-on-parquet-with-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":188.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nWhat parquet data loading logic is known to work well to train with SageMaker on parquet? ml-io? pyarrow? any examples? That would be to train a classifier, either logistic regression, XGBoost or custom TF.",
        "Answers":[
            {
                "Answer_creation_time":"2020-05-07T09:21:42.000Z",
                "Answer_upvote_count":1,
                "Answer_body":"XGBoost as a framework container (v0.90+) can read parquet for training (see example notebook).\nThe full list of valid content types are CSV, LIBSVM, PARQUET, RECORDIO_PROTOBUF (see source)\n\nAdditionally:\nUber Petastorm for reading parquet into Tensorflow, Pytorch, and PySpark inputs.\nAs XGBoost accepts numpy, you can convert from PySpark to numpy\/pandas using the mentioned PyArrow.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can we connect a Sagemaker Studio user to a gitlab repo within a private VPN?",
        "Question_creation_time":1643132865842,
        "Question_link":"https:\/\/repost.aws\/questions\/QURGs7VOVlTzKCG7H2AFLWww\/how-can-we-connect-a-sagemaker-studio-user-to-a-gitlab-repo-within-a-private-vpn",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":115.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"We have a gitlab repo within a private VPN and would like to setup Studio to clone that repo and to push and pull updates. Is that possible yet from within Studio?",
        "Answers":[
            {
                "Answer_creation_time":"2022-04-26T17:41:40.956Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thank you for your response. For those looking to do the same thing, according to AWS Support AWS SageMakers does NOT support GitLab yet and there is no ETA for that feature.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-02-24T18:06:03.844Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nThank you for using AWS services.\n\nAWS provides different VPN options like Client VPN and Site-to-Site VPN which might required different configuration and setup to get associated with other AWS resources like SageMaker Studio.\n\nTo answer your question in precise way and provide better assistance, we would like to understand the architecture at your end while implementing this configuration. The best way to understand the architecture is by opening a support case with networking team and discussed more about the options available to connect the Gitlab repo within private VPN to SageMaker Studio.\n\nhttps:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html\n\nFor more reference on AWS VPN:\n\nClient VPN: AWS Client VPN is a managed client-based VPN service that enables you to securely access your AWS resources and resources in your on-premises network. With Client VPN, you can access your resources from any location using an OpenVPN-based VPN client.\n\nSite-to-Site VPN: AWS Virtual Private Network solutions establish secure connections between your on-premises networks, remote offices, client devices, and the AWS global network.\n\nhttps:\/\/docs.aws.amazon.com\/vpn\/latest\/clientvpn-admin\/what-is.html\n\nhttps:\/\/docs.aws.amazon.com\/vpn\/latest\/s2svpn\/VPC_VPN.html\n\nhttps:\/\/docs.aws.amazon.com\/vpn\/index.html",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is there a way to install R libraries in SageMaker that receive a non-zero exit status?",
        "Question_creation_time":1527798496000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE-0c9SwxRViGhd-lAYtsuw\/is-there-a-way-to-install-r-libraries-in-sage-maker-that-receive-a-non-zero-exit-status",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":464.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi all, I'm having an issue with an R kernel\/Jupyter notebook. I've come across two different libraries that result in the following error:\n\nWarning message in install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\"):\n\u201cinstallation of package \u2018XML\u2019 had non-zero exit status\u201dUpdating HTML index of packages in '.Library'\nMaking 'packages.html' ... done\n\n\nXML is the second package that I have run into this issue with. The other is rJava.\n\nI found a workaround that could work if I had root access, which is installing via the command line in a terminal. Which involves commands such as:\n\nyum install r-cran-rjava\n\n\nHowever, I don't have root access and cannot install as I get the message \"You need to be root to perform this command.\" So this workaround hasn't been possible.\n\nAfter checking the documentation for rJava and XML, I am running the requirements for JDK and other system requirements in SageMaker. This issue wasn't reproducible on a local RStudio environment. XML is a dependency for multiple R libraries (as is rJava). Is there a way that I can still install these packages?",
        "Answers":[
            {
                "Answer_creation_time":"2018-06-01T02:52:15.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"For Amazon SageMaker notebook Instances, you have the ability to assume root privileges, so instead of:\n\n$ yum install r-cran-rjava\n\nyou can try:\n\n$ sudo yum install r-cran-rjava\n\nwhich will allow you to impersonate the superuser (ie. root) for that command 1\n\nBut I don't believe that package exists in the available repos (ie. may be valid for another distro of Linux, but does not appear to be available in the yum repos -- running yum search 'r-cran-rjava' returned no results 2)\n\nInstead, from a prompt, install R + the necessary development files for later installation of R packages:\n\n$ sudo yum install -y R-java-devel.x86_64\n\nAnd finally, install the necessary XML libraries to support the XML package in R:\n\n$ sudo yum install -y libxml2-devel 3\n\nAfter which you can then open R (either as root user...)\n\n$ sudo R\n\nor personal\/local user\n\n$ R\n\nand execute the package installation:\n\n> install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\")\n\nEDITED TO FIX RJAVA PACKAGE INSTALLATION\n\nIt looks like the installation is requiring libgomp.spec\/libgomp.a files, so you can first find that file:\n\n$ sudo find \/ -iname libgomp.spec\n\nwhich should be located at \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec -- if so, you can manually create symlinks to fix this:\n\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec \/usr\/lib64\/\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.a \/usr\/lib64\/\n\n\nIf that ran correctly, you should now see both files in \/usr\/lib64 path:\n\n$ ls \/usr\/lib64\/libgomp*\n\nOnce confirmed, you can run the install.package('rJava') command.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Custom container not running under root account?",
        "Question_creation_time":1607710724000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYAkZepq4SgyArKZCC7gT_A\/custom-container-not-running-under-root-account",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":76.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"A customer wants to enforce these rules in their custom SageMaker containers:\n\n\u2022\tProcesses running inside a container must run with a known UID\/GUID and never as root.\n\u2022\tAvoid using privilege escalation methods that grant root access (e.g. sudo)\n\n\nHow do we ensure this?",
        "Answers":[
            {
                "Answer_creation_time":"2020-12-18T15:45:37.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"SageMaker requires that Docker containers run without privileged access. See: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-toolkits.html SageMaker Docker containers do not run in Privileged mode and have the following Linux capabilities removed: SETPCAP, SETFCAP, NET_RAW, MKNOD",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Column header is not showing when reading data from redshift to jupyter on Sagemaker",
        "Question_creation_time":1669209147429,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxjkMWSCuS2ezRSYihJWHnA\/column-header-is-not-showing-when-reading-data-from-redshift-to-jupyter-on-sagemaker",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon Redshift"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":32.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello,\n\nI am reading a specific table from a redshift database using redshift connector. when i am viewing the dataframe it does not show the column headers. It shows only numbers as the column headers.\n\nCan anyone help whats wrong here? we need to view the table headers",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Multi-file source_dir bundle with SM Training Compiler (distributed)",
        "Question_creation_time":1639669045329,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwcM0XER5TcOggtQ_5cfVPw\/multi-file-source-dir-bundle-with-sm-training-compiler-distributed",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Natural Language Processing",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":29.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I'm hoping to use SageMaker Training Compiler with a (Hugging Face Trainer API, PyTorch) program split across multiple .py files for maintainability. The job needs to run on multiple GPUs (although at the current scale, multi-device single-node would be acceptable).\n\nFollowing the docs, I added the distributed_training_launcher.py launcher script to my source_dir bundle, and passed in the true training script via a training_script hyperparameter.\n\n...But when the job tries to start, I get:\n\nTraceback (most recent call last):\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\nreturn _run_code(code, main_globals, None,\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 90, in <module>\nmain()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 86, in main\nxmp.spawn(mod._mp_fn, args=(), nprocs=args.num_gpus)\nAttributeError: module 'train' has no attribute '_mp_fn'\n\n\nAny ideas what might be causing this? Is there some particular limitation or additional requirement for training scripts that are written over multiple files?\n\nI also tried running in single-GPU mode (p3.2xlarge) instead - directly calling the train script instead of the distributed launcher - and saw the below error which seems to originate within TrainingArguments itself? Not sure why it's trying to call a 'tensorflow\/compiler' compiler when running in PT..?\n\nEDIT: Turns out the below error can be solved by explicitly setting n_gpus as mentioned on the troubleshooting doc - but that takes me back to the error message above\n\nFile \"\/opt\/ml\/code\/code\/config.py\", line 124, in __post_init__\nsuper().__post_init__()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 761, in __post_init__\nif is_torch_available() and self.device.type != \"cuda\" and (self.fp16 or self.fp16_full_eval):\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 975, in device\nreturn self._setup_devices\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1754, in __get__\ncached = self.fget(obj)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 918, in _setup_devices\ndevice = xm.xla_device()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 231, in xla_device\ndevices = get_xla_supported_devices(\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 137, in get_xla_supported_devices\nxla_devices = _DEVICES.value\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/utils\/utils.py\", line 32, in value\nself._value = self._gen_fn()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 19, in <lambda>\n_DEVICES = xu.LazyProperty(lambda: torch_xla._XLAC._xla_get_devices())\nRuntimeError: tensorflow\/compiler\/xla\/xla_client\/computation_client.cc:273 : Missing XLA configuration",
        "Answers":[
            {
                "Answer_creation_time":"2022-07-15T08:01:47.440Z",
                "Answer_upvote_count":0,
                "Answer_body":"Ahh I solved this a while ago and forgot to update -\n\nYes, the training script needs to define a _mp_fn (which can just execute the same code as gets run if __name__ == \"__main__\") and number of GPUs (at least the last time I checked - hopefully this could change in future) needs to be explicitly configured.\n\nFor my particular project the fix to enable SMTC on the existing job is available online here. For others would also suggest referring to the official SMTC example notebooks & scripts!",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Do Amazon SageMaker manifest files enable dataset versioning?",
        "Question_creation_time":1607681930000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq44kZCYWTiOnwXblHSQSTA\/do-amazon-sage-maker-manifest-files-enable-dataset-versioning",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":65.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Some Amazon SageMaker algorithms can train with a manifest JSON file that stores the mapping between images and their Amazon S3 ARNs and metadata, such as labels. This is a great option, because the manifest file is much smaller than the dataset itself. Because the manifest files are small, they can be used easily in versioning tools or saved as part of the model artifact. This appears to be the best construct enabling exact dataset versioning within SageMaker. i.e., if we exclude the creation of a unique training set hard copy per training job that can't be scaled to large datasets. Is my understanding accurate?",
        "Answers":[
            {
                "Answer_creation_time":"2020-12-11T10:56:42.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"If you create the conditions for immutability of the assets the manifest points to, then manifest enables exact dataset versioning with SageMaker. You can have a data store in Amazon S3 with all versions of the data assets and use the manifest files for creating and versioning datasets for specific usage.\n\nIf you don't guarantee immutability for the assets that the manifest points to, then your manifest becomes invalid.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Increase Limit on Lineage Tracking entities for sagemaker",
        "Question_creation_time":1657755893027,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwNhM5g6qRN2orm9ttoPV6w\/increase-limit-on-lineage-tracking-entities-for-sagemaker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":38.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"My Team is trying to onboard to sagemaker lineage tracking entities and we basically track models, datasets, associations between these entities all the way to endpoints. Currently, we have been using another system for the same. We currently require that we create dataset entities prior to our training job so that we can use this to reference during our training jobs. The problem comes with the constraints on the amount of manual entities that can be created. As per the doc, the limits are\n\nMaximum number of manually created lineage entities Actions: 3000 Artifacts: 6000 Associations: 6000 Contexts: 500\n\nOur current system holds about 1500 datasets and 1000 models which means that we might hit the limit in the near future if we onboard to sagemaker. Is there a provision to increase the limits on these? I am not sure why these limits are placed. These entities must be pretty cheap to store. Please let me know if there is any way to get this limit increased",
        "Answers":[
            {
                "Answer_creation_time":"2022-07-15T04:37:37.084Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can double-check in the self-service AWS Service Quotas tool on your AWS account, but it looks to me like these quotas are not listed there (for now, at least).\n\n...So please raise a request via AWS Support from your account: Yes I believe there is flexibility in these quotas, but if you can include information in the request about projected requirements, usage pattern and the use case, that should help the team there assess the ask as quickly as possible.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Accessing SageMaker Notebooks without accessing the console",
        "Question_creation_time":1543947347000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUp9lMw9-ESm-27BWY_RgCSg\/accessing-sage-maker-notebooks-without-accessing-the-console",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":240.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Is it possible to access SageMaker Notebooks without accessing the console?\n\nDo we have a best practice for that? In the create-presigned-notebook-instance-url command, what is the --session-expiration-duration-in-seconds: is it the validity duration of the URL or the max session duration once the URL has been clicked?",
        "Answers":[
            {
                "Answer_creation_time":"2018-12-04T19:28:56.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I have experimented with CreatePresignedNotebookInstanceUrl a number of times. It returns an \"AuthorizedUrl\" string in the form: https:\/\/<notebook_instance_name>.notebook.<region>.sagemaker.aws?authToken=<a_very_long_string>\n\nI used the URL in another browser with no AWS console's session cookies (not logged in to the console) and it worked (could access my notebooks).\n\nThe parameter SessionExpirationDurationInSeconds is... well, exactly what it says, The number of seconds the presigned url is valid for. The API accepts a range of [1800, 43200] in seconds, which is equivalent to : 30 minutes to 12 hours .\n\nI hope this helps",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is there a solution for multi-user Notebook on SageMaker?",
        "Question_creation_time":1592989945000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4mxRvXy2QYmkYCvdqNVa2g\/is-there-a-solution-for-multi-user-notebook-on-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":457.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Is there a solution for multi-user Notebook on Studio Notebook or Notebook Instances? Eg if we want several developers to interact on the same notebook at the same time",
        "Answers":[
            {
                "Answer_creation_time":"2020-06-24T14:42:53.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Notebook instances are not connected to the user. So if two users has the same access rights they will see and will be able to access the same instance (even in the same time).\n\nThe issue is - Jupyter Notebook is not ready for that, both users will have the same privileges, no tracking who did what, ... And working on the same notebook on the same time - basically they will overwrite each other saves.\n\nI had a need for similar thing (pair programming - data scientist and software engineer) - the only viable solution we were able to find was desktop sharing (like TeamViewer, ...)",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Does SageMaker Multi-Model Endpoint support SageMaker Model Monitor?",
        "Question_creation_time":1590501108000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq2z-BEt7TnmZ8vFYs-Hu7g\/does-sage-maker-multi-model-endpoint-support-sage-maker-model-monitor",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":130.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Does SageMaker Multi-Model Endpoint support SageMaker Model Monitor?",
        "Answers":[
            {
                "Answer_creation_time":"2020-05-26T13:58:28.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Amazon SageMaker Model Monitor currently supports only endpoints that host a single model and does not support monitoring multi-model endpoints. For information on using multi-model endpoints, see Host Multiple Models with Multi-Model Endpoints . https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor.html",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What is required in HumanLoopInput.InputContent for start_human_loop",
        "Question_creation_time":1645624147204,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJast4QKSTjCYWe-pcQFyHw\/what-is-required-in-human-loop-input-input-content-for-start-human-loop",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon Textract",
            "Amazon Augmented AI",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":33.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have built a custom key-value extraction workflow that leverages textract Tables and Forms. It does a whole heap of post processing using the output of Textract to extract a small number of highly important fields from documents that are of very poor quality.\n\nMy client would like a human-in-the-loop to make minor changes to the results where certain fields are missing. I think that the sagemaker_a2i_runtime.start_human_loop is the perfect tool for this.\n\nI want to send the human reviewer, the input image and the current Key-Value pairs that I have extracted and have them find any that are missing, or mark them as not there. I have setup and tested the textract.analyse_image workflow with a human reviewer and like the results.\n\nWhat structure and data fields do I need to set in the HumanLoopInput field of sagemaker_a2i_runtime.start_human_loop to get this to work. I assume that it will look something like a dictionary of current K-V pairs and the s3 image file but I cannot find any documentation on how to do this.",
        "Answers":[
            {
                "Answer_creation_time":"2022-03-01T02:18:34.042Z",
                "Answer_upvote_count":0,
                "Answer_body":"The HumanLoopInput field accepts JSON.\n\nhttps:\/\/docs.aws.amazon.com\/augmented-ai\/2019-11-07\/APIReference\/API_HumanLoopInput.html\n\nPlease take a look at the following documentation on how to get started with A2I. Specifically, the Create a Human Loop -> Custom Integration section has an example for \"sagemaker_a2i_runtime.start_human_loop\".\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/a2i-get-started-api.html#a2i-get-started-api-create-human-loop",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Running a request against all variants in an endpoint",
        "Question_creation_time":1604486652000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6bm-EMtOQV6robgbTXClLQ\/running-a-request-against-all-variants-in-an-endpoint",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":14.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I have a customer asking me about the Rendezvous architecture. What I'm thinking is, we could implement this in a number of ways, all using endpoint variants:\n\nLambda (and probably SQS) around the endpoint;\nA custom monitoring job;\nStep Functions\n\nWithout going into details of the above options or of how the evaluation and SLA check will be done, it looks like the several models would fit very well as variants of an endpoint. The thing is, the architecture expects to call them all. Is there a way to directly call all variants of a model, or will a wrapper to identify the variants, call them all and process the results be needed?",
        "Answers":[
            {
                "Answer_creation_time":"2020-11-04T16:22:44.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"When I last looked into it, it was not possible to query all versions\/variants of the model automatically. You can specify what variant to use when using the invoke_endpoint method. I would therefore write a lambda function to invoke each of the endpoints one-by-one (see here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpoint.html). To be especially rigorous about it, you can add a function in your lambda code that first retrieves all the endpoint variants (see here: https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.describe_endpoint) then queries them one-by-one, and returns all the results.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Studio will not load",
        "Question_creation_time":1576685163000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxoSA7eTzQbK-T4OWjJvSmQ\/sage-maker-studio-will-not-load",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":904.0,
        "Question_answer_count":8,
        "Question_has_accepted_answer":false,
        "Question_body":"Morning of 12\/17 I loaded SageMaker Studio, and created an Autopilot experiment. It ran for 2 hours and was successful.\n\nAfterwards, I exited SageMaker Studio. Ever since that point, I have been unable to re-enter Studio. It is now 24 hours later.\n\nI either receive a response from Chrome:\n\nThis page isn\u2019t working\nd-*************.studio.us-east-2.sagemaker.aws didn\u2019t send any data.\nERR_EMPTY_RESPONSE\n\nOr, I get an error message:\nThe JupyterServer app default encoutered a problem and was stopped.\nDetails: InternalFailure\n\nI get the option to \"Restart Now,\" but it never works.",
        "Answers":[
            {
                "Answer_creation_time":"2020-01-06T22:02:27.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello. I ran into a similar problem. I think SageMaker Studio, since it's in preview, is not yet a super-stable platform. To resolve this issue I just kept trying to launch the sagemaker studio from the aws console ('open amazon sagemaker studio' button from within the amazon SageMaker > Amazon SageMaker Stuiod > d-***** men). I received the same error about ten times but eventually I was able to get through to see the notebook I had previously created.\n\nHope this helps!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-07-21T17:45:56.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I see the same issue. SM Studio was working on the evening of 1\/6\/2020. I shut down the Juypter Lab and haven't been able to access it since. I've tried accessing via the SSO dashboard, as well as an admin user via the AWS console.\n\nCHROME returns:\n\nThis page isn\u2019t working d-*********.studio.us-east-2.sagemaker.aws didn\u2019t send any data.\nERR_EMPTY_RESPONSE\n\nFIREFOX returns:\n\nSecure Connection Failed\nAn error occurred during a connection to d-*********.studio.us-east-2.sagemaker.aws.\nThe page you are trying to view cannot be shown because the authenticity of the received data could not be verified.\nPlease contact the website owners to inform them of this problem.\n\nSAFARI returns:\n\nSafari Can't Open the Page\nSafari can't open the page \"https:\/\/d-*******.studio.us-east-2.sagemaker.aws\/jupyter\/default\" because the server unexpectedly dropped the connection. This sometimes occurs when the server is busy. Wait for a few minutes, and then try again.\n\nI tried for a few hours last night and again this morning. The problem persists. Any suggestions or support with this would be greatly appreciated. Thanks.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-01-08T16:49:47.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Still having the same issue, trying about once a day since the original errors.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-01-13T11:10:40.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I have the same issue. I have shut it down last night and could not find a way to open it or restart the whole amazon sagemaker studio service.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-04-20T02:21:05.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I was able to fix this by completely deleting my Studio instance with the AWS CKI. Of course you'll use any ephemaral data. You can then start over from scratch. Here is the sequence of commands.\n\naws sagemaker delete-app --domain-id yourDomainID --user-profile-name yourProfileName --app-type KernelGateway --app-name base-python\naws sagemaker delete-app --domain-id yourDomainID --user-profile-name yourProfileName --app-type JupyterServer --app-name default\naws sagemaker delete-user-profile --domain-id yourDomainID --user-profile-name yourProfileName\naws sagemaker delete-domain --domain-id yourDomainID",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-01-17T17:05:49.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I am facing a similar problem for one of my SageMaker users. The default app, which is the Jupyter server, keeps failing while trying to launch SageMaker studio. I am receiving the following message: \"The JupyterServer app default encountered a problem and was stopped.\". I click on the Restart button, but the message appears again. Does anyone know how can I solve this issue? Or do you recommend paying for AWS support?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-10-04T23:40:41.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Had the same issue and managed to fix it (and preserve my data).\n\nIn AWS console navigate to SageMaker Studio\nIn the users section at the top, open the user (user details page) that is experiencing this issue.\nAt the bottom of the page, delete all apps for this user. There will be the default app (which is jupyter server) and sometimes others like KernelGateway. DO NOT DELETE THE ACTUAL USER PROFILE.\nWhen all apps for this user is deleted, go back to SageMaker Studio and click on the \"Open Studio\" link for that user.\nIt will take some time, but SageMaker Studio will reinitialize that Jupyter Notebook.\n\nThe notebook will open and you should have all your data as it was before.\n\nEdited by: Noobie on Oct 4, 2020 4:41 PM\n\nEdited by: Noobie on Oct 4, 2020 4:43 PM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-01-07T14:18:00.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"This method works, thanks for sharing.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to use sagemaker.processing.Processor run method",
        "Question_creation_time":1661777469790,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhZktmd-_Q3mFdOyFaNU9NA\/how-to-use-sagemaker-processing-processor-run-method",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":31.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"This is sagemaker docs. What is the purpose of sagemaker.processing.Processor as its run method does not have input for code or script? then how can I use it?\n\nOf course, I can use FrameworkProcessor, ScriptProcessor, SklearnProcessor because I can provide my processing.py. But for the sagemaker.processing.Processor, how can I use it?",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-30T01:07:27.065Z",
                "Answer_upvote_count":1,
                "Answer_body":"sagemaker.processing.Processor is the base class which is extended by ScriptProcessor, SKLearnProcessor etc... Its not recommended to directly use the base Processsor class. However you can still have a custom docker image and Run it using the base Processor module.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-01T00:02:44.729Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thank you, I understand the Processor is a base class. Then, find out how to provide my script to Processor via the entrypoint and run it.\n\ncontainer_base_path = \"\/opt\/ml\/processing\"\n\nprocessor = Processor(\n    role=os.environ['ROLE'],\n    image_uri=image_url,\n    instance_count=1,\n    instance_type='ml.m4.xlarge',\n    entrypoint=[\"python\", f\"{container_base_path}\/input\/process-data.py\"]\n)\n\nprocessor.run(\n    job_name=f'processor-{strftime(\"%Y-%m-%d-%H-%M-%S\")}',\n    inputs=[\n        ProcessingInput(\n            source=data_input_path,\n            destination=f\"{container_base_path}\/data\/\"\n        ),\n        ProcessingInput(\n            source=code_input_path,\n            destination=f\"{container_base_path}\/input\/\"\n        )\n    ],\n...\n\n\nI feel that SageMaker SDK is not consistent for developers because each processor has a different way to pass my code into it.\n\nProcessor: entrypoint\nScriptProcessor: command\nSklearnProcessor='my_script.py' in the run method",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can I use serverless inference with models generated by SageMaker Autopilot?",
        "Question_creation_time":1661981861600,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJpArvoZTSkiWkDMVDW1avw\/how-can-i-use-serverless-inference-with-models-generated-by-sage-maker-autopilot",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Amazon SageMaker Autopilot"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":32.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"There are a few articles about deploying SageMaker models to use serverless inference, but I am not clear on how to do that with autopilot models in particular. In other words, I do not understand which steps should be different and how to find information such as what my model ARN is. Thanks.",
        "Answers":[
            {
                "Answer_creation_time":"2022-09-05T22:49:27.371Z",
                "Answer_upvote_count":0,
                "Answer_body":"At this point there is no direct integration between SageMaker Autopilot and Serverless inferencing. One way to do this take the model.tar.gz from the best training job and deploy it manually as an serverless endpoint. Usually Autopilot uses one of the SageMaker Built in Algorithms and you will be able to us the specific container to deploy it.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Why does my kernal keep dying when I try to import Hugging Face BERT models to Amazon SageMaker?",
        "Question_creation_time":1604517955000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsO3sfUGpTKeHiU8W9k1Kwg\/why-does-my-kernal-keep-dying-when-i-try-to-import-hugging-face-bert-models-to-amazon-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":458.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"When I try to import Hugging Face BERT models to the conda_pytorch_p36 kernal of my Amazon SageMaker Notebook instance using the following pip command, the kernal always dies:\n\n! pip install transformers\n\n\nThe result is the same for Hugging Face BERT, RoBERTa, and GPT2 models on ml.c5.2xlarge and ml.c5d.4xlarge Amazon SageMaker instances.\n\nWhy is this happening, and how do I resolve the issue?",
        "Answers":[
            {
                "Answer_creation_time":"2020-11-04T22:05:35.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"This issue occurs when the latest sentence piece breaks. The workaround is to force install sentencepiece==0.1.91.\n\npip install sentencepiece==0.1.91",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"LightGBM on SageMaker",
        "Question_creation_time":1516632842000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPwkZcylKQR6-u0pghgrseA\/light-gbm-on-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":388.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I have a customer who wants to install LightGBM on SageMaker notebooks, as they are currently using it outside of SageMaker.\n\nRight now, they are interested in the ability to SSH into the instance, but it would be great if we could provide them a way to install LightGBM right now.\n\nCheers",
        "Answers":[
            {
                "Answer_creation_time":"2018-01-22T14:58:17.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"It's possible to do, I have used it myself for gradient boosting, from within Jupyter you can simply run:\n\n!conda install -y -c conda-forge lightgbm\n\n\nWithin a selected conda environment. No terminal access is needed, however it must be done, On the top right of the Jupyter notebook you can choose a terminal environment which will give you a shell to the backend instance and you can install there.\n\nHowever if you want the notebook to be immutable\/transferable you can do the install within the notebook .\n\nThanks",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to use custom functions for a model in a Sagemaker pipeline?",
        "Question_creation_time":1665782248751,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjjEHGriWRBCoNnc8luz-6Q\/how-to-use-custom-functions-for-a-model-in-a-sagemaker-pipeline",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":33.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"If I want to use a custom function transformer in preprocessing, how do I ensure that it's detected at both pipeline building and deployment?\n\nI'm building a sklearn pipeline, and in preprocessing I use a custom FunctionTransformer. In Sagemaker, I am able to train, evaluate, and register the model, but get the below error when I try to deploy it: AttributeError: Can't get attribute 'truncate_function' on <module '__main__' from '\/miniconda3\/bin\/gunicorn'>\n\nI've tried putting the functions into a helper.py file, and including it as a dependency during training, but then get the following error when evaluating in a ProcessingStep: \"ModuleNotFoundError: No module named 'helper'.",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to Resolve \"ERROR execute(301) Failed to execute model:\"",
        "Question_creation_time":1667853055854,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwmKbCBpXSym2Vh_4Z0cW0g\/how-to-resolve-error-execute-301-failed-to-execute-model",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Panorama"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":17.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"We have two applications working on the same AWS Panorama Appliance and processing different video streams. Unfortunately, we are catching the following error.\n\n2022-10-09 21:25:32.360 ERROR executionThread(358) Model 'model': 2022-10-09 21:25:32.359 ERROR execute(301) Failed to execute model:\nTVMError: \n'\"---------------------------------------------------------------\"\nAn error occurred during the execution of TVM.\nFor more information, please see: https:\/\/tvm.apache.org\/docs\/errors.html\n'\"---------------------------------------------------------------\n  Check failed: (context->execute(batch_size\n\"Stack trace:\n  File \"\/home\/nvidia\/neo-ai-dlr\/3rdparty\/tvm\/src\/runtime\/contrib\/tensorrt\/tensorrt_runtime.cc\", line 177\n  [bt] (0) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(+0x381358) [0x7f81e66358]\n  [bt] (1) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(tvm::runtime::detail::LogFatal::Entry::Finalize()+0x88) [0x7f81bb64a0]\n  [bt] (2) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(tvm::runtime::contrib::TensorRTRuntime::Run()+0x12b8) [0x7f81e243b0]\n  [bt] (3) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::json::JSONRuntimeBase::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)+0x5c) [0x7f81e1bfc4]\n  [bt] (4) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(+0x3c0dc4) [0x7f81ea5dc4]\n  [bt] (5) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(+0x3c0e4c) [0x7f81ea5e4c]\n  [bt] (6) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(dlr::TVMModel::Run()+0xc0) [0x7f81c258e0]\n  [bt] (7) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(RunDLRModel+0x1c) [0x7f81bea304]\n  [bt] (8) \/usr\/lib\/libAwsOmniInferLib.so(awsomniinfer::CNeoModel::SNeoModel::execute()+0x3c) [0x7f887db978]\"\n2022-10-09 21:25:32.437 ERROR executionThread(358) Model 'model': 2022-10-09 21:25:32.437 ERROR setData(279) Failed to set model input 'data':\n\n\nThe error isn't persistent. It may happen once in 2-3 weeks, and I need to know which place to investigate. The application logs are in the attachment. I am trying to avoid this issue.\n\nHowever, I would appreciate it if somebody knew how to cook this properly.",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker XGBoost batch transform AttributeError",
        "Question_creation_time":1657717396655,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3Hva4yNpSfOtRQTjKVMvvg\/sage-maker-xg-boost-batch-transform-attribute-error",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":71.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\n\nAfter training XGBoost model using SageMaker inbuilt algorithm, I am trying to perform batch transform operation. I am doing the same steps as for linear learner model which worked fine there. However in case of XGBoost I get a following error while creating a transformer:\n\nAttributeError: module 'sagemaker' has no attribute 'utils'\n\n\nThe piece of code causing the error is:\n\nxgb_transformer = xgb_estimator.transformer(\n    instance_count = 1, \n    instance_type = 'ml.m4.10xlarge',\n    output_path = '{}\/{}'.format(output_path,'output')\n)\n\n\nI use '1.5-1' version of XGBoost as image in training, and 2.86.2 version of SageMaker.\n\nAny help would be highly appreciated!",
        "Answers":[
            {
                "Answer_creation_time":"2022-07-15T17:46:23.737Z",
                "Answer_upvote_count":0,
                "Answer_body":"I have tried replicating the issue and by updating the SageMaker Package to the latest version sagemaker-2.99.0 I did not find any issues creating the batch transform job. Requesting you to update the sagemaker package to the latest version and let us know if you continue to face the issue.\n\n!pip3 install -U sagemaker\n\nIf you continue to face the issue I would encourage you to open a support ticket with AWS along with the sample code, the training job and the batch transform job ARN. Due to security reason, this post is not suitable for sharing customer's resource.\n\nReferences:\n\n[1] https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/aws_sagemaker_studio\/sagemaker_studio_image_build\/xgboost_bring_your_own\/Batch_Transform_BYO_XGB.html\n\n[2] https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_applying_machine_learning\/xgboost_customer_churn\/xgboost_customer_churn.ipynb",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How does sagemaker creates a model?",
        "Question_creation_time":1666386495979,
        "Question_link":"https:\/\/repost.aws\/questions\/QUTsasf-AKRriQkvxUPXGlxg\/how-does-sagemaker-creates-a-model",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":1.0,
        "Question_view_count":31.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I am creating sagemaker resources such as model, endpoint configuration and real time endpoint via cloudformation ( sample below) . in the template below, we provide the s3 bucket URI for the model artifact in the ModelDataUrl argument. if we update the model artifact , or delete a older version and upload a new model.tar file in the same bucket. will that work, instead of creating a new model resource everytime there is a new version of model.tar file ? when making a inference, I understand , sagemaker downloads the model.tar file in the container specified , unpack the model.tar file and call the binary file for inference ,so it doesn't matter if we update the model.tar file , right? sagemaker will simply download whatever tar file is present in the s3 URI and works with that.\n\nSageMakerModel:\n    Type: AWS::SageMaker::Model\n    Properties: \n      Containers: \n        -\n          Image: !Ref ImageURI\n          ModelDataUrl: 's3:\/\/some-bucket\/model.tar'\n          Mode: SingleModel\n      ExecutionRoleArn: !Ref RoleArn",
        "Answers":[
            {
                "Answer_creation_time":"2022-10-22T12:21:49.235Z",
                "Answer_upvote_count":1,
                "Answer_body":"If you trained your model in SageMaker, the model artifacts are saved as a single compressed tar file in Amazon S3. If you trained your model outside SageMaker, you need to create this single compressed tar file and save it in a S3 location. SageMaker decompresses this tar file into \/opt\/ml\/model directory before your container starts.\n\nReference : https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-22T03:56:46.545Z",
                "Answer_upvote_count":1,
                "Answer_body":"Just to add a bit more detail to Nitin's answer since you raised a couple of interesting secondary questions:\n\nFirst, to be picky, your artifact does need to be a compressed .tar.gz file as mentioned here. Plain .tar files (which you mentioned a couple of times) likely won't have this compression and won't work. From my recent tests the filename extension itself in S3 doesn't matter though - so if your object is in the correct format but just happens to be called .tar, this should be fine.\n\nSecond, yes today you can simply update the saved model artifact in S3 and continue referencing the same SageMaker model - but when can you do it and should you?\n\nOne important caveat to bear in mind is that if you're deploying your models to online inference endpoints, there's no API available to force running instances to re-fetch an updated model tarball. If you update your artifact while an endpoint is live (especially if auto-scaling is enabled or some endpoint update is in progress) you could end up with endpoint instances running a mixture of old vs new model.\n\nThe second is that, even if you only run defined batch jobs and can guarantee the timing of artifact updates won't overlap with job start-up, keeping track of your model versions is a pretty important best practice to help keep your operations in order.\n\nSo even though it's possible to just update the artifact in S3, I'd usually suggest users track their historical models: For example by treating the S3 prefix as immutable, re-creating Models for new artifact versions, and using SageMaker Model Registry to track the history of versions. It's worth mentioning that storing your history of versions (\"ModelPackages\") in Model Registry doesn't necessarily have to balloon the number of \"Models\" in SageMaker: As discussed here, a ModelPackage isn't necessarily tied to a Model, and you need to create a Model from it to be able to use in deployment\/inference.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"I want to deploy my model as a Serverless inference",
        "Question_creation_time":1661852626896,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGFC_kpAJTx6NcEG9_ZqUyQ\/i-want-to-deploy-my-model-as-a-serverless-inference",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":58.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hey I trained a sickst learn model using python sdk and I want to deploy the model as a Serverless inference now. I am new to AWS and can't seem to make sense of the documentation. the model is fit it an estimator as follow:\n\nfrom sagemaker.sklearn.estimator import SKLearn\nenable_local_mode_training = False\n\n\ninputs = {\"train\": trainpath, \"test\": testpath}\n\nestimator_parameters = {\n    \"entry_point\": \"script_rf.py\",\n    \"framework_version\": \"1.0-1\",\n    \"py_version\": \"py3\",\n    \"instance_type\": 'ml.c5.xlarge',\n    \"instance_count\": 1,\n    \"role\": role,\n    \"base_job_name\": \"randomforestclassifier-model\"\n}\n\nestimator = SKLearn(**estimator_parameters)\nestimator.fit(inputs)\n\n\n\nthis works fine but now when I try to deploy it it doesn't work. I tried this code: https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploying-ml-models-using-sagemaker-serverless-inference-preview\/ but i keep getting errors because somethings are not defined like the image_uri which I am not using.\n\nI used this\n\nm_boto3 = boto3.client('sagemaker')\n\nestimator.latest_training_job.wait(logs='None')\nartifact = m_boto3.describe_training_job(\n    TrainingJobName=estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n\nprint('Model artifact persisted at ' + artifact)\n\n\n\nbut then the endpoint is not Serverless. please help",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-30T19:53:33.202Z",
                "Answer_upvote_count":0,
                "Answer_body":"To deploy a serverless endpoint on Amazon SageMaker you will need 3 steps:\n\n(1) Model creation\n\nWhere you create the model with client.create_model\n\n(2) Endpoint configuration creation\n\nWhere you create a configuration file with client.create_endpoint_config, and make sure you have a serverless configuration with two parameters: MemorySizeInMB, and MaxConcurrency within the config, which should look like\n\n\"ServerlessConfig\": {\n        \"MemorySizeInMB\": 4096,\n        \"MaxConcurrency\": 1,\n},\n\n\n(3) Endpoint creation and invocation\n\nOnce you have created a model, with endpoint configuration with serverless endpoint detail, then you can run client.create_endpoint to create a serverless endpoint.\n\nReference: Deploying ML models using SageMaker Serverless Inference",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-30T16:03:41.716Z",
                "Answer_upvote_count":0,
                "Answer_body":"Actually creating a serverless endpoint has become much easier because you can now use the SageMaker Python SDK to so, i.e. you don't have to use boto3 anymore, you don't have to create models or endpoint configurations anymore, and you also don't have to specify the image_uri anymore.\n\nInstead, once you your estimator you can just use these lines of code:\n\nfrom sagemaker.serverless import ServerlessInferenceConfig\nserverless_config = ServerlessInferenceConfig()\n\nserverless_config = ServerlessInferenceConfig(\n  memory_size_in_mb=4096,\n  max_concurrency=10,\n)\n\nserverless_predictor = estimator.deploy(serverless_inference_config=serverless_config)\n\n\nSee also the documentation: https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#sagemaker-serverless-inference",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Studio Jupyterlab 3.0 working poorly with SM Resources UI",
        "Question_creation_time":1656420408518,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOmPLv2iwRyuEcQom58DgbA\/sage-maker-studio-jupyterlab-3-0-working-poorly-with-sm-resources-ui",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":3.0,
        "Question_view_count":195.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi all,\n\nSince Jupyterlab 3.0 was finally released on SM Studio, we have been super happy with it, however, for reasons unknown to us, the jupterlab interface works very poorly with SM resources, the following phenomenon have been observes:\n\nIt takes FOREVER to load the page for SM pipelines, and half the time it reports error (\"Error listing pipeline executions: Rate exceeded\")\nChanging instance type and size for a notebook is now super laggy, and do not work half the time\n\nAnyone knows if this is merely a lack of optimisation on the service team's part or is there something I can do to stop this behaviour? it's making our work very slow and unbearable, we don't want to revert back to 1.0 so any help would be greatly appreciated!\n\nBest, RUoy",
        "Answers":[
            {
                "Answer_creation_time":"2022-06-29T19:26:38.598Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thank you for sharing your observations.\n\nIn order to troubleshoot the issue, I would request you to delete the existing default Jupyter server app for the user profile and try creating a new app by launching the studio. Let us know if you are still facing the issue\n\nRequesting you to confirm if it is same behavior with all the users of the studio or specific user profiles.\n\nIn order deep-dive further and investigate I would request you to create a support ticket with the AWS technical team with the following information -- Domain ID -- User-profile ARN -- Cloudwatch logs\n\nand also the requesting you to share the list-pipelines[1] output from the CLI command.\n\nNote:If you still have difficulties, I recommend to cut a support case and provide more detail about your account information and above requested details. Due to security reason, we cannot discuss account specific issue in the public posts.\n\nThank you. Reference:\n\n[1] https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/list-pipelines.html",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using R model in SageMaker ML pipelines",
        "Question_creation_time":1643230196748,
        "Question_link":"https:\/\/repost.aws\/questions\/QU17aS4s7uSRqmiLuveuchBw\/using-r-model-in-sage-maker-ml-pipelines",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":97.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi there,\n\nIs it possible to use R model training and serving in SageMaker ML Pipelines? Looked in examples here. And it doesn't look that R is fully supported currently by ML Pipelines. Any examples and success stories are very welcome.\n\nThanks.",
        "Answers":[
            {
                "Answer_creation_time":"2022-01-28T21:07:43.709Z",
                "Answer_upvote_count":1,
                "Answer_body":"In general it is possible to use the SageMaker python SDK and boto3 using the reticulate package in R, However do not have direct examples of SageMaker Pipelines using R.\n\nIt is possible to orchestrate the production pipeline using the R Containers for training and serving and setting up the DAG can be done with reticulate and SageMaker Python SDK and can be achieved using the AWS Step Functions. Please refer to the following example for reference.\n\nhttps:\/\/github.com\/aws-samples\/reinvent2020-aim404-productionize-r-using-amazon-sagemaker https:\/\/www.youtube.com\/watch?v=Zpp0nfvqDCA",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Async Inference not able to process later requests",
        "Question_creation_time":1648126119561,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqAl1qUyYRK-cbY3DGH-X9g\/async-inference-not-able-to-process-later-requests",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":172.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi there, hope all of you are fine.\n\nI am trying to deploy a train-on-inference type model. I am done with BYOC, and it is working completely fine with real-time inference endpoints. Also, I am able to make it work with Async inference, and concurrent requests on the same instance are also being handled. But, the later requests, never get processed, without any logical error. Also once the endpoint gets scaled down to 0 instance, it fails to scales up.\n\nThese are some of error and warning messages which I get intermittently:\n\n\n\ndata-log:\n2022-03-23T11:23:17.723:[sagemaker logs] [5ea751c9-9271-4533-bc09-c117791e1372] Received server error (500) from primary with message \"<!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\">\n\n\n\nwarnings:\n\/usr\/local\/lib\/python3.8\/dist-packages\/numpy\/core\/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n  setattr(self, word, getattr(machar, word).flat[0])\n\n\nKindly help me with this. Thanks.",
        "Answers":[
            {
                "Answer_creation_time":"2022-03-29T06:30:15.076Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hello, I'm running into the exact same issue. I used the same guide and the async endpoint doesn't scale up or down.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-14T14:12:50.944Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, hope you are fine. Thanks for getting back to me. This is what I am using:\n\n\n# Configure Autoscaling on asynchronous endpoint down to zero instances\nresponse = client.register_scalable_target(\n    ServiceNamespace=\"sagemaker\",\n    ResourceId=resource_id,\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n    MinCapacity=0,\n    MaxCapacity=4,\n)\n\nresponse = client.put_scaling_policy(\n    PolicyName=\"Invocations-ScalingPolicy\",\n    ServiceNamespace=\"sagemaker\",  # The namespace of the AWS service that provides the resource.\n    ResourceId=resource_id,  # Endpoint name\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",  # SageMaker supports only Instance Count\n    PolicyType=\"TargetTrackingScaling\",  # 'StepScaling'|'TargetTrackingScaling'\n    TargetTrackingScalingPolicyConfiguration={\n        \"TargetValue\": 2.0,  # The target value for the metric. - here the metric is - SageMakerVariantInvocationsPerInstance\n        \"CustomizedMetricSpecification\": {\n            \"MetricName\": \"ApproximateBacklogSizePerInstance\",\n            \"Namespace\": \"AWS\/SageMaker\",\n            \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": endpoint_name}],\n            \"Statistic\": \"Average\",\n        },\n        \"ScaleInCooldown\": 300,  # The cooldown period helps you prevent your Auto Scaling group from launching or terminating\n        # additional instances before the effects of previous activities are visible.\n        # You can configure the length of time based on your instance startup time or other application needs.\n        # ScaleInCooldown - The amount of time, in seconds, after a scale in activity completes before another scale in activity can start.\n        \"ScaleOutCooldown\": 300  # ScaleOutCooldown - The amount of time, in seconds, after a scale out activity completes before another scale out activity can start.\n        # 'DisableScaleIn': True|False - ndicates whether scale in by the target tracking policy is disabled.\n        # If the value is true , scale in is disabled and the target tracking policy won't remove capacity from the scalable resource.\n    },\n)",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Data Wrangler: Data Flow: Export to S3 using Jupyter Notebook",
        "Question_creation_time":1665658565074,
        "Question_link":"https:\/\/repost.aws\/questions\/QUskLbkfD8RW2YzO9Vr7XggA\/data-wrangler-data-flow-export-to-s-3-using-jupyter-notebook",
        "Question_topic":[
            "Machine Learning & AI",
            "Compute"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon EC2",
            "Amazon SageMaker Data Wrangler"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":25.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"When I have created a Data flow using data wrangler and when I am trying to Export to export to S3 using Jupyter Notebook and when I am running the notebook, I am getting the below mentioned error every time when creating a processing job:\n\nError: An error occurred (ResourceLimitExceeded) when calling the CreateProcessingJob operation: The account-level service limit 'ml.m5.4xlarge for processing job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 2 Instances. Please contact AWS support to request an increase for this limit.\n\nPlease provide me with the solution for this. I have increased the service quota for running apps and Notebook instance also but then also same issue arises.",
        "Answers":[
            {
                "Answer_creation_time":"2022-10-17T15:05:05.558Z",
                "Answer_upvote_count":1,
                "Answer_body":"When you export a data flow to S3, you're starting a SageMaker processing job that will run the script and store data in S3.\n\nSo, increase your account limit for Processing Job instances, for the instance type ml.m5.4xlarge (instead of running apps\/notebook instances).",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Amazon CloudWatch Metric ModelSetupTime not available",
        "Question_creation_time":1665007033890,
        "Question_link":"https:\/\/repost.aws\/questions\/QUu2KU0TNvTu-2V6sOylZJsg\/amazon-cloud-watch-metric-model-setup-time-not-available",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Amazon CloudWatch",
            "Monitoring"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":23.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"The ModelSetupTime metric is not available for monitoring the serverless endpoint(https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html) in Amazon CloudWatch.Able to see only the below mentioned metrics Invocations Invocation5XXErrors Invocation4XXErrors ModelLatency OverheadLatency\n\nIs there any role\/configuration changes I have to do to view the ModelSetupTime ?",
        "Answers":[
            {
                "Answer_creation_time":"2022-10-17T16:57:04.142Z",
                "Answer_upvote_count":0,
                "Answer_body":"Currently the metric is not published to CW.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Getting TrainingJobName from Training step of Sagemaker Pipeline",
        "Question_creation_time":1669298009010,
        "Question_link":"https:\/\/repost.aws\/questions\/QU21YXYHCuRhip83ELnFlYHg\/getting-training-job-name-from-training-step-of-sagemaker-pipeline",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Lambda",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":77.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I am running a sagemaker pipeline with a training step. This whole setup runs from a Lambda function while I pass a few parameters to the pipeline.py file. To get the TrainingJobName from the training step, my code is step_train.__dict__['step_args']['TrainingJobName'] This works fine while I running it in sagemaker notebook but when I execute the same code to get the train job name from lambda, I get this error [ERROR] TypeError: '_StepArguments' object is not subscriptable [ERROR] TypeError: '_StepArguments' object is not subscriptable Traceback (most recent call last): File \"\/var\/task\/lambda_function.py\", line 46, in lambda_handler pipeline = create_pipeline(validated_api_input) File \"\/tmp\/pipeline.py\", line 105, in create_pipeline \"job_name\" : training_step['step_args']['TrainingJobName']\n\nHow do I resolve this?",
        "Answers":[
            {
                "Answer_creation_time":"2022-11-27T12:32:12.557Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello there,\nThank you for contacting. The error specifically comes up when you are trying to use a subscript for a data-type which isn't subscript-able, in your case this is coming when you are treating the object _StepArguments as a dictionary. A workaround for the same will be to implement a __get_item___ function in your code. A sample for the same is as follows:\n\ndef __getitem__(self, key):\n        return self.__dict__[key]\n\n\nHowever, as this was working on Sage-maker instance by default, also share the Python version that you are using and if you are using the same version of library in both Lambda function and SageMaker.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-28T23:16:55.719Z",
                "Answer_upvote_count":0,
                "Answer_body":"_StepArguments is a new concept introduced in newer version of SageMaker Python SDK. If you want to keep you original code, you need to pin the same SDK version in Lambda as in your notebook instance. Normally, I guess you will be installing the latest version but there has been a compatibility-broken change since then.\n\nIf you are OK to code changes, you could follow https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-and-manage-steps.html#build-and-manage-properties to use properties of each step, which maps to Describe* response. You will still be able to retrieve any job info. This is actually the recommended way.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to keep sagemaker inference warm-up",
        "Question_creation_time":1669198631982,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVZqbuLPeSLWOe3TobeZHtQ\/how-to-keep-sagemaker-inference-warm-up",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":28.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Calling sagemaker inference frequently (3-5 calls in a minute) reduces runtime duration from ~200ms to ~50ms, so it seems there is similar warm-up behaviour like in Lambda. Do you have any suggestions how to keep sagemaker inference responsive always fast?",
        "Answers":[
            {
                "Answer_creation_time":"2022-11-23T11:39:43.954Z",
                "Answer_upvote_count":0,
                "Answer_body":"You may need to check where this acceleration comes from to determine the warm up process. In CloudWatch metrics, you have ModelLatency and OverheadLatency.\n\nSageMaker Endpoint has a front-end router which maintains some caches for meta data and credentials. If the requests are frequent enough, the cache will be retained and auto renewed. This will reduce the OverheadLatency.\n\nIf you see a big drop in ModeLatency with warm-up requests, this may mean your algorithm container could have been configured to retrain some temporary data longer.\n\nNormally, you could schedule an invocation Lambda with CloudWatch Alarms to target tracking the metricInvokationPerInstance. This will make sure you always maintain a certain invocation rate when idle and those fake requests could settle down when real requests are picking up.\n\nThe issue with warm-up is that we stops the normal auto-scaling process of endpoints. The endpoint may not scale down properly.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"java.lang.IllegalArgumentException in SageMaker",
        "Question_creation_time":1642538521034,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXl66qTr3TBWJjO5td_K0jw\/java-lang-illegal-argument-exception-in-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":43.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm unable to invoke the my SageMaker endpoint. I'm seeing this error in the endpoint logs\n\njava.lang.IllegalArgumentException: reasonPhrase contains one of the following prohibited characters: \\r\\n: tokenizers>=0.10.1,<0.11 is required for a normal functioning of this module, but found tokenizers==0.11.2.\n\n\nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git\n\n\nMy Sagemaker endpoint is invoked through a lambda function. The code that calls the sagemaker endpoint is:\n\nSM_ENDPOINT_NAME = \"pytorch-inference-2021-xx-xx\"\nsm_runtime= boto3.client('runtime.sagemaker')\ntxt = \"Canon SELPHY CP1300 Compact Photo Printer\"\nresponse = sm_runtime.invoke_endpoint(EndpointName=SM_ENDPOINT_NAME, ContentType='text\/plain', Body=txt)\n\n\nThe response is supposed to contain a vector.\n\nIt's been working fine previously but I started seeing this exception today.\n\nIs this a bug in SageMaker? If not, how do I fix it?",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Image Classification Algorithm Class Activation Map",
        "Question_creation_time":1552316346000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUdCD7nXuTJegFm_lGunPzw\/image-classification-algorithm-class-activation-map",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":29.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hey!\n\nI am using SageMaker's built in image classification algorithm and currently have a trained model that can make predictions. I am wondering if there is an easy way to use this model to make a class activation map to better debug my classifier and see why it is making the predictions it is?\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_time":"2019-03-14T17:05:21.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Steveleo,\n\nThanks for trying out the image classification algorithm. The trained model from SageMaker built-in image classification is a standard MXNet model. Here is a notebook https:\/\/github.com\/dmlc\/mxnet-notebooks\/blob\/master\/python\/how_to\/predict.ipynb that you can follow to extract features.\n\nThanks,\nXiong",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-03-14T20:59:33.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"For the class activation map, please check this http:\/\/mxnet.incubator.apache.org\/versions\/master\/tutorials\/vision\/cnn_visualization.html .",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Uncaught exception in ZMQStream callback -- trying to run Jupyter notebook with Julia kernel in SageMaker",
        "Question_creation_time":1644454296572,
        "Question_link":"https:\/\/repost.aws\/questions\/QUM2y8-rjDS1Wp5LUkdLMhyA\/uncaught-exception-in-zmq-stream-callback-trying-to-run-jupyter-notebook-with-julia-kernel-in-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":1.0,
        "Question_view_count":172.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I want to run a Jupyter notebook with Julia kernel in Amazon SageMaker. The Julia 1.7.1 icon shows up in the Jupyterlab launcher and accepts the kernel on launch, but then the kernel dies immediately after launch (it never works). I have posted about this here\n\nhttps:\/\/www.repost.aws\/questions\/QU2PXu3tbpQ7-V2OTlD_I07Q\/make-julia-notebooks-work-in-sage-maker\n\nand Alex_T has made some very good suggestions, but even they get stumped at this point. Log info:\n\n[E 00:25:58.760 NotebookApp] Uncaught exception in ZMQStream callback\n    Traceback (most recent call last):\n      File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/zmq\/eventloop\/zmqstream.py\", line 431, in _run_callback\n        callback(*args, **kwargs)\n      File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/notebook\/services\/kernels\/kernelmanager.py\", line 391, in record_activity\n        msg = session.deserialize(fed_msg_list)\n      File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/jupyter_client\/session.py\", line 929, in deserialize\n        raise ValueError(\"Invalid Signature: %r\" % signature)\n    ValueError: Invalid Signature: b'ec1d1093f3b6505658469b860c203f696bc39cf8fcea1672cb55802fc57592eef57b8db0b5cb603d1bcada6f41060f0819f6002a7a31f309fbaa1a701cc13f5b'\n\n\nThere are some posts from 2018 recommending updating tornado and ipykernel, but 4 years later this should hardly apply (and I tried it, and it didn't work). Any suggestions? What could be going wrong here? Everything else seems to be in place. There is a white paper on running a Jupyter notebook with Julia kernel in SageMaker here [https:\/\/d1.awsstatic.com\/whitepapers\/julia-on-sagemaker.pdf?did=wp_card&trk=wp_card], but it is incomplete, as you can see in the other post.",
        "Answers":[
            {
                "Answer_creation_time":"2022-02-10T03:52:32.909Z",
                "Answer_upvote_count":1,
                "Answer_body":"Thanks for raising this again!\n\nSo as mentioned on the other question, I tentatively believe this to be caused by this open IJulia issue... And there I see one participant commented they managed to resolve the issue by not installing Julia inside conda.\n\nHappy to share I seem to have been able to reproduce this success with the following steps... :D\n\nInstead of creating a conda env and installing Julia through it as advised by the whitepaper (which would of course be a good practice for proper environment separation, if it weren't for this issue), wget the generic Linux x86 64-bit (glibc) tarball from the Julia downloads page https:\/\/julialang.org\/downloads\/ (for example, https:\/\/julialang-s3.julialang.org\/bin\/linux\/x64\/1.7\/julia-1.7.2-linux-x86_64.tar.gz)\n(Because uname -a shows us that our NBI is running in x86 64-bit Linux, and a couple of hacky checking methods I found online seemed to suggest it's a glibc setup rather than musl)\nDon't forget to cd SageMaker in terminal before your wget, so the file actually downloads to somewhere you can see it in JupyterLab\n(Verify the checksum of the downloaded file first, as good security practice and then) Untar the bundle with tar -xvf {YourFileName.tar.gz}. You should see a folder something like julia-1.7.2 containing (amongst other things) a bin subfolder with a julia executable in it.\nMake julia executable visible in your system PATH\nThere are many different ways you might want to set this up, depending what you want on persistent EBS storage vs ephemeral, whether you want to use multiple versions of julia in parallel, and whether you have preferences about where in the filesystem the application should reside.\nFor this particular test I chose to cp -R .\/julia-1.7.2 \/usr\/local\/julia (copy Julia to a folder where I wouldn't accidentally edit it) and then sudo ln -s \/usr\/local\/julia\/bin\/julia \/bin\/julia (create a julia symlink to the main executable inside the \/bin folder, which is already on PATH).\nStart julia from your terminal (julia) and then install & configure IJulia as per the original instructions:\nusing Pkg\nPkg.add(\"IJulia\")\nusing IJulia\njupyterlab(detached=true)\n# I got an error on this last command but it still seemed to work?\n\nVerify that you should now have a julia entry in your Jupyter kernels folder. I copied out the contents into my JupyterLab working folder to inspect it: cp -R ~\/.local\/share\/jupyter\/kernels\/julia-1.7 .\/julia-1.7-copy. It should contain a kernel.json.\nIf all is well (and it was for me) you should now see a \"Julia 1.7.2\" (or similar version) kernel on your launcher and be able to launch a notebook successfully and run commands (\ud83c\udf89)\nWhat about productionization?\n\nSo of course this installation is a bit of a hacky workaround for whatever underlying issue is causing Julia to not run properly inside conda... I'd really prefer to see a direct resolution for that problem if possible. The procedure described here won't persist between Notebook Instance stop\/starts (at which time everything outside of \/home\/ec2-user\/SageMaker EBS mount is reset), so you'd probably want to explore automating the setup with a lifecycle configuration script.\n\nI see by default that Julia packages (including IJulia) are installed to \/home\/ec2-user\/.julia\/packages\/ (which is outside the SageMaker mount), so may be interesting to explore configuration options for moving .julia under the SageMaker folder for persistence. Files and folders beginning with a dot are hidden in JupyterLab file tree, which is worth remembering if you explore customizing the setup.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to tune SageMaker Studio Notebooks hardware config?",
        "Question_creation_time":1576675818000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUp5wKTB0URcCPyBgUcAWMww\/how-to-tune-sage-maker-studio-notebooks-hardware-config",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":70.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"How does one choose or tune the hardware backend of a Sagemaker Studio Notebook?",
        "Answers":[
            {
                "Answer_creation_time":"2019-12-18T22:33:24.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"At the top right of a notebook (near the kernel ) there will be a resource configurations button, you'll be able to choose the instance you want to run the notebook on.\n\nA nice feature of that is that all the instances shares the same EFS mount (SageMaker studio uses EFS for notebook storage) if you save a dataframe to the local disk (EFS) you can change instance type during your work and continue from the place you've been in (Move from a GPU instance to a CPU instance for cost effectiveness \/ back to GPU for performance)",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to access file system in Sagemaker notebook instance from outside of that instance (ie via Python Sagemaker Estimator training call)",
        "Question_creation_time":1638914293851,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3yXAL7d7Sl--kKO3TTZf1g\/how-to-access-file-system-in-sagemaker-notebook-instance-from-outside-of-that-instance-ie-via-python-sagemaker-estimator-training-call",
        "Question_topic":[
            "Machine Learning & AI",
            "Storage"
        ],
        "Question_tag":[
            "Build & Train ML Models",
            "Amazon SageMaker",
            "Storage"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":690.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nI have large image dataset stored in a Sagemaker notebook instance, in the file system. I was hoping to learn how I could access this data from outside of that particular notebook instance. I have done quite a bit of researching but can't seem to find much - I am relatively new to this.\n\nI want to be able to access the data in that notebook in a fast manner as I will be using the data to train an AI model. Is there any recommended way to do this?\n\nI originally uploaded the data within that notebook instance to train a model within that instance in exactly the same file system. Note that it is a reasonably large dataset which I had to do some preprocessing on within Sagemaker.\n\nWhat is the best way to store data when using the Sagemaker estimators from training AI models?\n\nMany thanks\n\nTim",
        "Answers":[
            {
                "Answer_creation_time":"2021-12-07T22:53:56.668Z",
                "Answer_upvote_count":2,
                "Answer_body":"Hi Tim, when you create a sagemaker training job using the estimator, the general best practice is to store your data on S3 and the training job will launch instances as requested by the training job configuration. As now we support fast file mode, which allows faster training job start compared to the file mode (which downloads the data from s3 to the training instance). But when you say you used sagemaker notebook instance to train the model, I assume you were not using SageMaker Training jobs but rather running the notebook (.ipynb) on the SageMaker notebook instance. Please note that as SageMaker is a fully managed service, the notebook instance (also training instances, hosting instances etc.) are launched in the service account, so you will not have directly access to those instance. The SageMaker notebook instance use EBS to store data and the EBS volume is mounted to the \/home\/ec2-user\/SageMaker. Please note that the EBS volume used by a SageMaker notebook instance can only be increased but not decrease. If you want to reduce the EBS volume, you need to create a new notebook instance with a smaller volume and move your data from the previous instance via s3. You will not be able to access that EBS volume from outside of the SageMaker notebook instance. The general best practice is to store large dataset on s3 and only use sample data on the SageMaker notebook instance (reduce the storage). Then use that small amount of sample data to test\/build your code. Then when you are ready to train on the whole dataset, you can launch a SageMaker training job and use the whole dataset stored on s3. Note that, running the training on the whole dataset on a SageMaker notebook instance will require you to use a big instance with enough computing power and also will not be able to perform distributed training with multiple instances. Comparatively, if you run the training job use SageMaker training instances, it gives you more flexibility of choosing the instance type and allow you to run on multiple instances for distributed training. Lastly, once the SageMaker training job is done, all the resources will be terminated which will save cost compared to continue using the big instance with a SageMaker notebook instance. Hope this has helped answer your question",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS Sagemaker - Either the training channel is empty or the mini-batch size",
        "Question_creation_time":1559545390000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq0KSPPCBT1qrotLu0NJyBw\/aws-sagemaker-either-the-training-channel-is-empty-or-the-mini-batch-size",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":38.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to train a linear learner model in Sagemaker. My training set is 422 rows split into 4 files on AWS S3. The mini-batch size that I set is 50.\n\nI keep on getting this error in Sagemaker.\n\nCustomer Error: No training data processed. Either the training\nchannel is empty or the mini-batch size is too high. Verify that\ntraining data contains non-empty files and the mini-batch size is less\nthan the number of records per training host.\n\nI am using this InputDataConfig\n\nInputDataConfig=\\[  \n            {  \n                'ChannelName': 'train',  \n                'DataSource': {  \n                    'S3DataSource': {  \n                        'S3DataType': 'S3Prefix',  \n                        'S3Uri': 's3:\/\/MY_S3_BUCKET\/REST_OF_PREFIX\/exported\/',  \n                        'S3DataDistributionType': 'FullyReplicated'  \n                    }  \n                },  \n                'ContentType': 'text\/csv',  \n                'CompressionType': 'Gzip'  \n            }  \n        ],  \n\n\nI am not sure what I am doing wrong here. I tried increasing the number of records to 5547495 split across 6 files. The same error. That makes me think that somehow the config itself has something missing. Due to which it seems to think training channel is just not present. I tried changing 'train' to 'training' as that is what the erorr message is saying. But then I got\n\nCustomer Error: Unable to initialize the algorithm. Failed to validate\ninput data configuration. (caused by ValidationError)\n\nCaused by: {u'training': {u'TrainingInputMode': u'Pipe',\nu'ContentType': u'text\/csv', u'RecordWrapperType': u'None',\nu'S3DistributionType': u'FullyReplicated'}} is not valid under any of\nthe given schemas\n\nI went back to train as that seems to be what is needed. But what am I doing wrong with that?\n\nEdited by: anshbansal on Jun 3, 2019 12:06 AM",
        "Answers":[
            {
                "Answer_creation_time":"2019-06-03T13:03:18.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Found the problem. The CompressionType was mentioned as 'Gzip' but I had changed the actual file to be not compressed when doing the exports. As soon as I changed it to be 'None' the training went smoothly.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Did the SageMaker PyTorch deployment process change?",
        "Question_creation_time":1594979513000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFIru4hJ2TcWLi7CYt3mnuw\/did-the-sage-maker-py-torch-deployment-process-change",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":134.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Did the SageMaker PyTorch deployment process change?\n\nIt use to be the case that people needed to have a model.tar.gz in s3, and an inference script locally or in git. Now, it seems that the inference script must also be part of the model.tar.gz. This is new, right?\n\nFrom the docs, https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#for-versions-1-2-and-higher:\n\n*For PyTorch versions 1.2 and higher, the contents of model.tar.gz should be organized as follows:\n\nModel files in the top-level directory\nInference script (and any other source files) in a directory named code\/ (for more about the inference script, see The SageMaker PyTorch Model Server)\nOptional requirements file located at code\/requirements.txt (for more about requirements files, see Using third-party libraries)*\n\nThis may be confusing, because this new mode of deployment means that people creating the model artifact need to know in advanced how the inference is going to look like. The previous design, with separation of artifact and inference code, was more agile.",
        "Answers":[
            {
                "Answer_creation_time":"2020-07-17T10:23:38.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"When AWS Sample - BERT sample using torch 1.4 was published, advance knowledge of the inference seems to be necessary. If you use the PyTorch SageMaker SDK to create or deploy the model after it is trained, it automatically re-packages the model.tar.gz to include the code files and the inference files. As an example, when you use the following script, the model.tar.gz is repackaged so the contents of the src directory is automatically added to the code directory model.tar.gz, which initially only contains model files. You don't need to know the inference code in advance.\n\nfrom sagemaker.pytorch import PyTorchModel\nfrom sagemaker import get_execution_role\nrole = get_execution_role()\n\nmodel_uri = estimator.model_data\n\nmodel = PyTorchModel(model_data=model_uri,\n                     role=role,\n                     framework_version='1.4.0',\n                     entry_point='serve.py',\n                     source_dir='src')\n\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge')\n\nFor the older versions, you couldn't include additional files \/dependencies during inference unless you built a custom container. The source.tar.gz was only used during training.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Data Wrangler UI Features",
        "Question_creation_time":1641943985816,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcsIt78jnSTW8Ta9__kUm-w\/sage-maker-data-wrangler-ui-features",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":1.0,
        "Question_view_count":61.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":true,
        "Question_body":"The SageMaker Data Wrangler UI in SageMaker Studio doesn't seem to support all the features that the API does. When will the UI support:\n\nLoading all s3 objects under a prefix? https:\/\/aws-data-wrangler.readthedocs.io\/en\/stable\/stubs\/awswrangler.s3.read_csv.html#awswrangler.s3.read_csv\nLoading JSON objects in addition to CSV and Parquet files? https:\/\/aws-data-wrangler.readthedocs.io\/en\/stable\/stubs\/awswrangler.s3.read_json.html#awswrangler.s3.read_json",
        "Answers":[
            {
                "Answer_creation_time":"2022-01-12T20:59:06.281Z",
                "Answer_upvote_count":2,
                "Answer_body":"As mentioned by Tulio Alberto in comments, Amazon SageMaker Data Wrangler (the graphical data preparation feature inside Amazon SageMaker) is separate from AWS Data Wrangler (an open-source data prep utility published by AWS Labs): The two tools are based on different technologies and don't necessarily aim for full feature parity - they just happen to share similar names.\n\nTo my knowledge there's no committed timeline we can share at the moment for when these particular features will make it to SageMaker Data Wrangler, but I think as feature requests they make sense and the reasoning for both is pretty clear: I'm aware that both have been discussed to some extent internally already, and I'd personally like to see them launch too!\n\nThanks for sharing the feedback, and apologies for the naming confusion!",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-02-12T01:48:49.303Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Tulio, thanks for the clarification. But doesn't SageMaker Data Wrangler generate code that complies with\/uses AWS Data Wrangler? Isn't there some (if tenuous) connection between the two?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-13T05:22:27.466Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nSageMaker Data Wrangler in Studio just launched the JSON\/ORC support and we support import files under a prefix already. Please see the following links\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/data-wrangler-import.html#data-wrangler-import-s3\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/prepare-and-analyze-json-and-orc-data-with-amazon-sagemaker-data-wrangler\/",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Asynchronous Endpoint Configuration",
        "Question_creation_time":1653000488230,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZNbZZQHhSl2RYUtLU8zpSQ\/sagemaker-asynchronous-endpoint-configuration",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Autopilot",
            "Amazon SageMaker Model Building",
            "Amazon SageMaker JumpStart",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":80.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"We deployed a LighGBM Regression model and endpoint using Sagemaker Jumpstart. We have attempted to configure this endpoint as 'asynchronous' via the console. Receiving Error: ValidationException-Network Isolation is not supported when specifying an AsyncInferenceConfig.\n\nLooking at the model's network details the model has Enable Network Isolation set as 'True'. This was default output setting set by JumpStart.\n\nHow can we diasble Network Isolation to in order to make this endpoint asynchronous?",
        "Answers":[
            {
                "Answer_creation_time":"2022-05-20T05:18:58.005Z",
                "Answer_upvote_count":1,
                "Answer_body":"Vanilla SageMaker \"Models\" (as opposed to versioned ModelPackages) are immutable in the API with no \"UpdateModel\" action... But I think you should be able to create a new Model copying the settings of the current one.\n\nI'd suggest to:\n\nUse DescribeModel (via boto3.client(\"sagemaker\").describe_model(), assuming you're using Python) to fetch all the parameters of the existing JumpStart model such as the S3 artifact location and other settings\nUse CreateModel (create_model()) to create a new model with same configuration but network isolation disabled\nUse your new model to try and deploy an async endpoint\n\nProbably you'd find the low-level boto3 SDK more intuitive for this task than the high-level sagemaker SDK's Model class - because the latter does some magic that makes typical build\/train\/deploy workflows easier but can be less natural for hacking around with existing model definitions. For example, creating an SMSDK Model object doesn't actually create a Model in the SageMaker API, because deployment instance type affects choice of container image so that gets deferred until a .deploy() call or similar later.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker AutoML generates ExpiredTokenException",
        "Question_creation_time":1642293400372,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPU-nfYYIRbmM-tJpkG6XqA\/sage-maker-auto-ml-generates-expired-token-exception",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":21.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\n\nI can train models using different AWS SageMaker estimators, but when I use SageMaker AutoML Python SDK the following error occurs about 15 minutes into the model training process:\n\n\"botocore.exceptions.ClientError: An error occurred (ExpiredTokenException) when calling the DescribeAutoMLJob operation: The security token included in the request is expired\"\n\nThe role used to create the AutoML object is associated with the following AWS pre-defined policies as well as one inline policy. Can you please let me know what I\u2019m missing that's causing this ExpiredTokenException error?\n\nAmazonS3FullAccess AWSCloud9Administrator AWSCloud9User AmazonSageMakerFullAccess\n\nInline policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"iam:PassRole\" ], \"Resource\": \"\", \"Condition\": { \"StringEquals\": { \"iam:PassedToService\": \"sagemaker.amazonaws.com\" } } }, { \"Effect\": \"Allow\", \"Action\": [ \"sagemaker:DescribeEndpointConfig\", \"sagemaker:DescribeModel\", \"sagemaker:InvokeEndpoint\", \"sagemaker:ListTags\", \"sagemaker:DescribeEndpoint\", \"sagemaker:CreateModel\", \"sagemaker:CreateEndpointConfig\", \"sagemaker:CreateEndpoint\", \"sagemaker:DeleteModel\", \"sagemaker:DeleteEndpointConfig\", \"sagemaker:DeleteEndpoint\", \"cloudwatch:PutMetricData\", \"logs:CreateLogStream\", \"logs:PutLogEvents\", \"logs:CreateLogGroup\", \"logs:DescribeLogStreams\", \"s3:GetObject\", \"s3:PutObject\", \"s3:ListBucket\", \"ecr:GetAuthorizationToken\", \"ecr:BatchCheckLayerAvailability\", \"ecr:GetDownloadUrlForLayer\", \"ecr:BatchGetImage\" ], \"Resource\": \"\" } ] }\n\nThanks, Stefan",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[errno 28] no space left on disk",
        "Question_creation_time":1650457037741,
        "Question_link":"https:\/\/repost.aws\/questions\/QUInf7H_9bQpCPMROevLa2fA\/errno-28-no-space-left-on-disk",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":57.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to install a python package called rapidsai in aws notebook instance but I am getting this error errno 28 no space left on disk. I am using g4dn.xlarge instance for it. The package is almost 3 GB. While opening the notebook instance I have tried increasing volume size of notebook upto 50 GB, I am still getting this error. Let me know the solution to it. Thanks",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[Sagemaker] - Does Sagemaker support Xpress model",
        "Question_creation_time":1657298680086,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOPI761YwSQWqpUusp3Mo_g\/sagemaker-does-sagemaker-support-xpress-model",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":63.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\n\nWe have a use case to use Sagemaker towards running Xpress based science models. Can anyone please tell what are the pros and cons of using sagemaker towards running the Xpress models\n\nThanks",
        "Answers":[
            {
                "Answer_creation_time":"2022-07-11T21:17:16.850Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nThank you for using AWS SageMaker.\n\nUsing Xpress models on SageMaker will have similar pros and cons of using Bring your container(BYOC) on SageMaker.\n\nWhile using BYOC users have all the options to control the package version and their desire packages at their end and will have full control over it with the help of docker image. But it requires intermediate knowledge of Docker and is not recommended unless you are comfortable writing your own machine learning algorithm. Along with that you will need to maintain your docker image and any sort of update that is required for the dependent packages to run your inference code.\n\nFor more details, I'd recommend you to visit our AWS public documentation in BYOC. If incase any further assistance is required, I'd recommend to open a case with SageMaker Support engineering team for further guidance. To open a support case with AWS using the link: https:\/\/console.aws.amazon.com\/support\/home?#\/case\/create",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker endpoint running but constantly restarting",
        "Question_creation_time":1660140840225,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQi42sIDTTSW5KN3P-DT4LQ\/sagemaker-endpoint-running-but-constantly-restarting",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon CloudWatch Logs",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":81.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I have deployed a model to a Sagemaker endpoint using BentoML\/BentoCTL. This is a tool for building APIs and containerizing models. To test, I use curl with a JSON payload to make a request. When I run the created docker container on my local machine I can successfully invoke it and get responses back. So I don't think the problem is in the docker image.\n\nWhen I deploy to sagemaker, I receive the message {\"message\":\"Service Unavailable\"} as a response to my curl request. I can see the endpoint running in the Sagemaker\/Endpoints dashboard. Viewing the cloudwatch logs, it appears that the the endpoint is constantly restarting. There are messages that are printed at startup (e.g. Tensorflow loading messages) that are written to the log over and over.\n\nI thought that this might be due to using an instance type with low memory (t2.medium) so I switched to m5.4xlarge as a test, but the result is the same.\n\nWhat can I do? How can I determine what's causing the endless restarts?",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-12T17:17:23.953Z",
                "Answer_upvote_count":0,
                "Answer_body":"When you mean restart? Does it mean \"Updating\" the endpoint? Do you have an autoscaling policy attached to the endpoint? Do you see any errors in the Cloudwatch logs?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-09T05:06:38.531Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello - can you check the Tensorflow version and use the latest supported version 2.2? Thank you!\n\nhttps:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images\/ https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/deploying_tensorflow_serving.html",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"how to set up autoscaling for async sagemaker endpoint?",
        "Question_creation_time":1646861290947,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjjLx7h0TR0q1KwubwQjU9A\/how-to-set-up-autoscaling-for-async-sagemaker-endpoint",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Management & Governance",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Lambda",
            "AWS Auto Scaling",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":256.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"working with an example documented here -> https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/async-inference\/Async-Inference-Walkthrough.ipynb. I was able to set up the sagemaker model, config and aync endpoint via lambda, now I'm trying to re-create the stack via terraform. based on the documentation on terraform, i was able to set up the model, config and the endpoint but couldn't find how to go about setting up the auto scaling ( sample code below). is this possible?\n\nclient = boto3.client(    \"application-autoscaling\") \nresource_id = (    \"endpoint\/\" + endpoint_name + \"\/variant\/\" + \"variant1\")  \nresponse = client.register_scalable_target(\n    ServiceNamespace=\"sagemaker\",\n    ResourceId=resource_id,\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n    MinCapacity=0,\n    MaxCapacity=5,\n)\nresponse = client.put_scaling_policy(\n    PolicyName=\"Invocations-ScalingPolicy\",\n    ServiceNamespace=\"sagemaker\", \n    ResourceId=resource_id,  # Endpoint name\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",  \n    PolicyType=\"TargetTrackingScaling\",  # 'StepScaling'|'TargetTrackingScaling'\n    TargetTrackingScalingPolicyConfiguration={\n        \"TargetValue\": 5.0,  \nSageMakerVariantInvocationsPerInstance\n        \"CustomizedMetricSpecification\": {\n            \"MetricName\": \"ApproximateBacklogSizePerInstance\",\n            \"Namespace\": \"AWS\/SageMaker\",\n            \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": endpoint_name}],\n            \"Statistic\": \"Average\",\n        },\n        \"ScaleInCooldown\": 600,\n   ....\n    },\n)\n\n\nclean up\n\nresponse = client.deregister_scalable_target(\n    ServiceNamespace='sagemaker',\n    ResourceId='resource_id',\n    ScalableDimension='sagemaker:variant:DesiredInstanceCount'\n)",
        "Answers":[
            {
                "Answer_creation_time":"2022-03-11T17:05:39.186Z",
                "Answer_upvote_count":1,
                "Answer_body":"You will using the regular autoscaling config outlined in the doc here to configure it for the SageMaker Async endpoint. There are no specifics for SageMaker.\n\nFirst, you define the \"aws_appautoscaling_target\" with minimum and maximum capacities. Then go ahead and define your \"TargetTrackingScaling\" in the autoscaling policy",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Annotation of PDF document using Bounding Box",
        "Question_creation_time":1665169244843,
        "Question_link":"https:\/\/repost.aws\/questions\/QUK0c3Zp1jQ8SFMF3Mfp7SmA\/annotation-of-pdf-document-using-bounding-box",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon Augmented AI",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":25.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi! Is it possible to develop some kind of template for SageMaker that uses bounding boxes to annotate a PDF document? I was looking for something similar to the crowd-bounding-box HTML tag, but instead of only capturing portions of the PDF's image, I also wanted to extract data regarding the text located inside that portion, along with it's coordinates and stuff like that, so I could give context to my annotation.",
        "Answers":[
            {
                "Answer_creation_time":"2022-10-17T17:20:33.352Z",
                "Answer_upvote_count":0,
                "Answer_body":"Your usecase sounds very close to the core Textract workflow - check out this AWS ML blog post that provides a solution that generates searchable PDF's, including bounding boxes for the text and such\n\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/generating-searchable-pdfs-from-scanned-documents-automatically-with-amazon-textract\/",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to check\/determine image\/container size for aws managed images ?",
        "Question_creation_time":1645022076554,
        "Question_link":"https:\/\/repost.aws\/questions\/QU35dVp2D9SKKUnnVYGw9Z7A\/how-to-check-determine-image-container-size-for-aws-managed-images",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":1.0,
        "Question_view_count":108.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm using one of the images listed here https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md, to create an model such that I can tie that up with a sagemaker serverless endpoint , but I keep getting \"failed reason: Image size 15136109518 is greater that suppported size 1073741824\" . this work when the endpoint configuration is not serverless. is there any documentation around image\/container size for aws managed images?",
        "Answers":[
            {
                "Answer_creation_time":"2022-02-16T19:28:16.051Z",
                "Answer_upvote_count":1,
                "Answer_body":"It sounds like you set up a serverless endpoint with 1GB of memory and the image is larger than that. You can increase the memory size of your endpoint with the MemorySizeInMB parameter, more info in this documentation: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config\n\nIf you pick a larger value for that (e.g. 4096 MB) then it should hopefully work.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What is value and use case for Deep Learning AMI (DLAMI)?",
        "Question_creation_time":1594209626000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQInSlgeCS6mIe4DJv3KwnQ\/what-is-value-and-use-case-for-deep-learning-ami-dlami",
        "Question_topic":[
            "Machine Learning & AI",
            "Compute"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon EC2",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":77.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"What is value and use case for Deep Learning AMI (DLAMI)?\n\nIt seems that customers often pack ML dependencies at the docker level (themselves, or with DL containers or with SageMaker containers), instead of the AMI level. So what is the value and use-case of DL AMI ?",
        "Answers":[
            {
                "Answer_creation_time":"2020-07-08T13:58:25.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"The value of the DLAMI (https:\/\/docs.aws.amazon.com\/dlami\/latest\/devguide\/what-is-dlami.html) is ease of use and saving time to get up to speed in a development environment. If you are developing code for ML there is a huge variety of frameworks and software that you might need to install. The DLAMI includes the more popular ones, so you may quickly deploy a machine complete with common dependencies. This results in a reduction of the time needed for installing and configuring things. It speeds up experimentation and evaluation. If you want to try a new framework, it is already there.\n\nThe second reason is that AWS keeps the AMI up to date, so you may just deploy a new AMI periodically rather than having to patch. Again, this saves you time and lets you concentrate on the underlying development and business activities.\n\nAll that said, for running in production and at volume you might want to use a different tool, I would imagine that for most cases creating docker images to your specific requirements would make a lot of sense. No need to go over the good and bad points of containers here.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Model Monitor Missing Columns Constraint Violation",
        "Question_creation_time":1586974280000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU8Xkelo1ARA2zcn4rHuk09w\/sage-maker-model-monitor-missing-columns-constraint-violation",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":150.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I have an Endpoint inference pipeline model deployed from an AutoPilot training job. Now that this is successful, I want to add model monitor. I have a script for online validation of the endpoint, and the F1 score is ~99%. This indicates that the endpoint interprets the call correctly.\n\nModel Monitor is recognizing the data in my jsonl files as the data not being CSV formatted. When my Model Monitor processing job runs, I receive the following constraint violation: \"There are missing columns in current dataset. Number of columns in current dataset: 1, Number of columns in baseline constraints: 225\".\n\nGiven the results from the Endpoint and this Model Monitor constraint violation, I perceive there is a conflict between how the Endpoint is storing the data and how the Model Monitor Processing Job wants to consume the data.\n\nHere is one sample prediction from the jsonl file. The data value is comma separated.\n\n{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text\/csv\",\"mode\":\"INPUT\",\"data\":\"JHB,44443000.0,-0.0334,,44264000.0,,,,-2014000.0,,-2014000.0,,,,,,,-0.04,-0.04,55872000.0,,,0.996,,,,,,,,-0.0453,,2845000.0,,2845000.0,11636000.0,,,,,,,,,,,,190000000.0,,,,,,,,-18718000.0,,,,,,,,29000000.0,,,,,,,,-33000000.0,,-4000000.0,,,,,,,,,,,,,,,0.0,,,0.995972369102,1.0,-0.045316472785366,0.0,,,,,,,0.0,,,,,,,,,95.5638,,,,,,1.0,1.0,,0.15263157894737,,,,,,0.65252120693923,0.0,0.15263157894737,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18606500.0,,,95.5638,,,2.3886,,,,,-0.0326,,-1.0449,,-1.05,-1.05,,0.0,,-0.1471,,,,,,,,,,,,,,,,,-0.5451,,,,,,,Financial Services,16.67890010036862\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text\/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"1\\n\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"c97df615-0a2e-414d-9be3-bf3a14eb6363\",\"inferenceTime\":\"2020-04-15T16:26:46Z\"},\"eventVersion\":\"0\"}\n\n\nHere is the point within the log that the processing job recognizes a column mismatch. I see that it pulls down the data to store locally, pulls down the statistics and constraints files, errors with this constraint, and then gracefully ends the Processing Job. If more logs are needed to analyze, I have the Processing Job logs in CloudWatch Logs.\n\n2020-04-15 17:11:49 INFO  FileUtil:66 - Read file from path \/opt\/ml\/processing\/baseline\/constraints\/constraints.json.\n2020-04-15 17:11:50 INFO  FileUtil:66 - Read file from path \/opt\/ml\/processing\/baseline\/stats\/statistics.json.\n2020-04-15 17:11:50 ERROR DataAnalyzer:65 - There are missing columns in current dataset. Number of columns in current dataset: 1, Number of columns in baseline constraints: 225\nSkipping further processing because of column count mismatch.\n\n\nI could not find Model Monitor documentation on how to deal with column mismatch constraint violations.",
        "Answers":[
            {
                "Answer_creation_time":"2020-04-15T18:43:05.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"That violation fires when, for example, input to your endpoint has fewer columns than baseline input does. This is helpful to flag data quality issues. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-interpreting-violations.html\n\nIn this case, however, this is an artifact of how we perform the analysis. We concatenate output and input CSVs into a single CSV to analyze the whole thing in one go. E.g. it would look like:\n\noutput_col,input_col_1,input_col_2,...,input_col_n\n\n\nIn this case, however, your output has a trailing newline which means that after concatenating this looks like:\n\noutput_col # embedded newline in your output\n,input_col_1,input_col_2,...,input_col_n\n\n\nTriggering the code to think there is only one column in dataset and hence failing the job.\n\nWe have a fix flowing through the pipeline now, while that goes out you can add a preprocessing script to your schedule to strip out the trailing newline from the output. We will create a sample notebook for this, in the meantime docs are at https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-pre-and-post-processing.html#model-monitor-pre-processing-script",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker batch transform 415 error",
        "Question_creation_time":1532625720000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUr4Vq95ScROqSguzxNQYDOg\/sagemaker-batch-transform-415-error",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":424.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi, I need to run XGBoost inferences on 15MM samples (3.9Gb when stored as csv). Since Batch transform does not seem to work on such large batches (max payload 100MB) I split my input file into 646 files, each around 6Mb, stored in S3. I am running the code below:\n\ntransformer = XGB.transformer(\n    instance_count=2, instance_type='ml.c5.9xlarge',\n    output_path='s3:\/\/xxxxxxxxxxxxx\/sagemaker\/recsys\/xgbtransform\/',\n    max_payload=100)\n\ntransformer.transform(\n    data='s3:\/\/xxxxxxxxxxxxx\/sagemaker\/recsys\/testchunks\/',\n    split_type='Line')\n\n\nBut the job fails - Sagemaker tells \"ClientError: Too many objects failed. See logs for more information\" and cloudwatch logs show:\n\nBad HTTP status returned from invoke: 415\n'NoneType' object has no attribute 'lower'\n\n\nDid I forget something in my batch transform settings?",
        "Answers":[
            {
                "Answer_creation_time":"2018-07-26T19:42:05.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"This indicates that the algorithm thinks it has been passed bad data. Perhaps a problem with your splitting?\n\nI would suggest two things:\n\nTry running the algorithm on the original data using the \"SplitType\": \"Line\" and \"BatchStrategy\": \"MultiRecord\" arguments and see if you have better luck.\nLook in the cloudwatch logs for your run and see if there's any helpful information about what the algorithm didn't like. You can find these in the log group \"\/aws\/sagemaker\/TransformJobs\" in the log stream that begins with your job name.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker batch transform",
        "Question_creation_time":1532619204000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlefH1ni4QOaulUT4870D5g\/sagemaker-batch-transform",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":254.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi, it seems that Sagemaker Batch Transform is limited to 100MB payloads I'd like to run preds against a 5GB csv file, what the recommended way to do so?",
        "Answers":[
            {
                "Answer_creation_time":"2018-07-26T18:02:46.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"SageMaker Batch Transform will automatically split your input file into whatever payload size is specified if you use \"SplitType\": \"Line\" and \"BatchStrategy\": \"MultiRecord\". There's no need to split files yourself or to use large payload sizes unless you have very large single records.\n\nHope that helps!",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Problem Sagemaker and Spark",
        "Question_creation_time":1536801534000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3gi1LMvEQNiiGlYooXnUQA\/problem-sagemaker-and-spark",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":792.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi there,\n\nI followed this tutorial to set up Sagemaker Notebook with Spark (EMR): https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/\n\nI launched a notebook with sparkmagic (pyspark3) and tried to call the Spark context but got the following error:\n\"\"\"\nThe code failed because of a fatal error:\nInvalid status code '400' from http:\/\/xxx.xx.xx.xx:8998\/sessions with error payload: \"Invalid kind: pyspark3 (through reference chain: org.apache.livy.server.interactive.CreateInteractiveRequest[\"kind\"])\".\n\nSome things to try:\na) Make sure Spark has enough available resources for Jupyter to create a Spark context.\nb) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\nc) Restart the kernel.\n\"\"\"\n\nAnyone encountered the same issue?",
        "Answers":[
            {
                "Answer_creation_time":"2018-10-01T22:17:14.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hey,\n\nThanks for using SageMaker! This is an issue in pyspark3 with latest Livy. Starting with version 0.5.0-incubating, session kind \u201cpyspark3\u201d is removed, instead users require to set PYSPARK_PYTHON to python3 executable[1].\n\nSo there're two options:\n\nYou can switch to use pyspark kernel instead of pyspark3.\nYou can set PYSPARK_PYTHON variable in EMR's config file for spark: spark-env.sh\n[\n{\n\"Classification\": \"spark-env\",\n\"Configurations\": [\n{\n\"Classification\": \"export\",\n\"Configurations\": [],\n\"Properties\": {\n\"PYSPARK_PYTHON\": \"\/usr\/bin\/python3\"\n}\n}\n],\n\"Properties\": {}\n}\n]\n\nLet us know if you have any other question.\n\nThanks,\nHan\n\n[1]https:\/\/livy.incubator.apache.org\/docs\/latest\/rest-api.html#pyspark\n[2]https:\/\/docs.aws.amazon.com\/emr\/latest\/ReleaseGuide\/emr-spark-configure.html",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-01-31T21:37:35.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Submitting these EMR configuration options at the time of cluster creation worked for me:\n[\n{\n\"Classification\": \"spark-env\",\n\"Configurations\": [\n{\n\"Classification\": \"export\",\n\"Properties\": {\n\"PYSPARK_PYTHON\": \"\/usr\/bin\/python3\"\n}\n}\n]\n},\n{\n\"Classification\": \"yarn-env\",\n\"Properties\": {},\n\"Configurations\": [\n{\n\"Classification\": \"export\",\n\"Properties\": {\n\"PYSPARK_PYTHON\": \"\/usr\/bin\/python3\",\n}\n}\n]\n}\n]",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS Sagemaker Notebook Not working ,how can i solve the issue?",
        "Question_creation_time":1665720123752,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhUbNweWRR0-oROL-jwKIRQ\/aws-sagemaker-notebook-not-working-how-can-i-solve-the-issue",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":39.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"The code failed because of a fatal error: Error sending http request and maximum retry encountered..\n\nSome things to try: a) Make sure Spark has enough available resources for Jupyter to create a Spark context. b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly. c) Restart the kernel.\n\nNote: There are no such logs on cloudwatch to figureout the issue.",
        "Answers":[
            {
                "Answer_creation_time":"2022-10-14T15:26:34.864Z",
                "Answer_upvote_count":0,
                "Answer_body":"Try using a Python 3 kernel instead of the PySpark kernel. Found that suggestion here: https:\/\/stackoverflow.com\/a\/68663849",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Lifecycle scripts to access the notebook instance git repository",
        "Question_creation_time":1554297310000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhTwl-BZURCGatI4Rm9fFqg\/lifecycle-scripts-to-access-the-notebook-instance-git-repository",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":132.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi there,\nIs it possible for the lifecycle scripts to access the content of the checkout-ed git repository? A use case would be to access the already available in the repository pip requirements file and to populate the notebook instance with the required python modules on start up.\nI guess the answer to this question depends on the order of the executed events when a notebook is created. Are you executing the lifecycle scripts first and then checkout the repository or vice versa.\n\nThanks!\n\nEdit:\n\nWell, I did my experiment as follows.\n\nI added a simple 'ls -al SageMaker' in the start and create lifecycle scripts and inspected the logs.\nIt seems that on initial notebook instance creation the git repository is checked out after the execution of the start and create scripts.\nOn subsequent notebook starts, the start script is executed and the repository folder is present in the SageMaker folder with a timestamp indicating that the repository folder was created after the initial start\/create scripts executions.\n\nSo, can someone confirm that this is what's expected and that we can access the repository only on subsequent notebook starts?\n\nEdited by: ainkov on Apr 3, 2019 7:25 AM",
        "Answers":[
            {
                "Answer_creation_time":"2019-04-11T18:41:49.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi and thank you for using SageMaker!\n\nCurrently, Git repositories are checked out after Lifecycle Configuration scripts are executed, so unfortunately they will not have access to them. We are always considering new features and functionality, so I've added this as a feature request.\n\nIn the mean time, I have two recommendations to workaround this limitation:\n\nManually clone or download the requirements.txt files from within your Lifecycle Configuration.\nFrom your Lifecycle Configuration, create a cron job or similar background process that waits until the Git Clone operation is complete.\n\nLet me know if that helps!\n\nBest,\nKevin",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-10-01T12:18:00.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hey there, sorry for this late reply.\nThanks much, the workarounds would help!",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Spot instances for inference and sagemaker?",
        "Question_creation_time":1649350121825,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcQU2DOmNQdyI8HWeIMzzdg\/spot-instances-for-inference-and-sagemaker",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Lambda",
            "Amazon SageMaker",
            "Amazon EC2",
            "Spot Instances"
        ],
        "Question_upvote_count":1.0,
        "Question_view_count":184.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Is it possible to deploy spot inf1 instances on sagemaker? We run an API 24\/7, and it's costly to keep it up, considering we only have 2 hours of peak performance a day.\n\nWe don't shut off those machines because we might have random bursts of traffic during the day that CPU instances can't hold. Alternatively, we could deploy spot EC2 inf machines; however, I'm unsure how I would invoke them from gateway and lambda. Does anybody have a tip or recommendation for our case?\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_time":"2022-04-07T23:39:13.030Z",
                "Answer_upvote_count":0,
                "Answer_body":"You could possibly integrate EC2 Spot instance fleet with Application Auto Scaling service to spin up or down spot instances when you receive traffic. To scale it down to 0 instances, you will need to configure a queue to hold the requests while you spin up from 0 instances to 1 or more. Then your application would insert the requests in the SQS queue and wait for an instance to be available. Take a look at this link for more information on how to configure application autoscaling with Spot instances: https:\/\/docs.aws.amazon.com\/autoscaling\/application\/userguide\/services-that-can-integrate-ec2.html\n\nTo configure your policy for the autoscaling, you can look at SQS queue length metric. Here is how you can set a target tracking policy for the application autoscaling: https:\/\/docs.aws.amazon.com\/autoscaling\/application\/userguide\/create-target-tracking-policy-cli.html",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Rekognition Custom Labels and Ground Truth integration question",
        "Question_creation_time":1594736742000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUH-j5YmdZTiSEvLnm1ghztw\/rekognition-custom-labels-and-ground-truth-integration-question",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":68.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello!\n\nI've created a semantic segmentation (SS) job in Ground Truth (GT), and it shows all objects are labeled, and the status is Complete. The manifest file appears to be syntactically correct.\n\nWithin Custom Labels (CL), I've created a dataset by choosing \"Import images labeled by Ground Truth\" option and using the manifest file created in GT. It parses correctly. However, when I look at the available dataset in the console, I see that there are zero labeled images. Viewing the images also displays zero labels.\n\nI have done the same with bounding box (BB) jobs in GT, and they import into CL correctly.\n\nDo CL datasets not support import of SS GT manifest files? I can't find anything in the documentation that states either way.\n\nThanks for your help.",
        "Answers":[
            {
                "Answer_creation_time":"2020-07-14T16:18:36.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hey lkoivu\n\nThank you for using CustomLabels!\n\nUnfortunately we do not support semantic segmentation jobs. Currently we support 2 groundtruth formats - Classification Job Output & Bounding Box Job Output.\n\nYou can find the formats documented here:\n(a) Sagemaker GT - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-output.html\n- See section \"Classification Job Output\" for image level labels\n- See section \"Bounding Box Job Output\" for bounding box format\n(b) Custom Labels\n- Image level labels - https:\/\/docs.aws.amazon.com\/rekognition\/latest\/customlabels-dg\/cd-manifest-files-classification.html\n- Bounding boxes - https:\/\/docs.aws.amazon.com\/rekognition\/latest\/customlabels-dg\/cd-manifest-files-object-detection.html\n\nI have passed on feedback to the team to make this more clear in our documentation as well as on our console.\n\nWe do have roadmap items to expand our support to other ground truth formats.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Experiment tracking with Sagemaker Pipelines",
        "Question_creation_time":1667379630510,
        "Question_link":"https:\/\/repost.aws\/questions\/QUttCrJyfVQYyAE-AO0vg-EA\/experiment-tracking-with-sagemaker-pipelines",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":21.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Is it possible to track only TrainingSteps in a Sagemaker Pipeline that contains multiple Processing & other steps? I don't really see big benefit of creating Trial Components for Processing Jobs or Model Repacking jobs into the experiments as they just overflow the UI.\n\nBasically could the pipeline_experiment_config parameter be used for defining which steps of the Pipeline should be tracked or should I disable automatic experiment creation and just try to create a manual experiment tracker during the Training Job.",
        "Answers":[
            {
                "Answer_creation_time":"2022-11-02T13:40:54.790Z",
                "Answer_upvote_count":0,
                "Answer_body":"Sure, if you don't want an experiment and trial created for the pipeline, you could easily achieve this by setting the pipeline_experiment_config to None.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to prevent disassociating SageMaker LifecycleConfig unintentionally",
        "Question_creation_time":1646363723581,
        "Question_link":"https:\/\/repost.aws\/questions\/QUTvkDhX_yQXyW7WpFinO_vA\/how-to-prevent-disassociating-sage-maker-lifecycle-config-unintentionally",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":28.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"When you go to SageMaker Notebook Instance edit screen in AWS Web Console (to change the Instance Type for example), it is sometimes the case that Lifecycle configuration is popped up as No Configuration even though the configuration is actually set earlier. This results in an unintentional disassociation of the LifecycleConfig because it's easy to save the instance change without noticing the change in Lifecycle Config. This is a serious problem for us. I was able to reproduce this issue in Chrome and Firefox (but you need to try several times to repro the issue).\n\nI am in the position of provisioning different cloud resources for the end users and I need a way to systematically prevent this disassociation to happen. I considered applying an IAM policy that denies the update operation containing the change in the LifecycleConfig of notebooks, but there seems no condition key for LifecycleConfig which makes me think this approach isn't feasible.\n\nWhat can I do?\n\nThanks.",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"In Sagemaker endpoint, pip download fails when connected through VPC",
        "Question_creation_time":1648489585321,
        "Question_link":"https:\/\/repost.aws\/questions\/QU7Q7e634rRAidb_GUu2ZhXw\/in-sagemaker-endpoint-pip-download-fails-when-connected-through-vpc",
        "Question_topic":[
            "Networking & Content Delivery",
            "Machine Learning & AI",
            "AWS Well-Architected Framework"
        ],
        "Question_tag":[
            "Amazon VPC",
            "Amazon SageMaker",
            "Networking & Content Delivery",
            "Security",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":90.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Pip download fails in my instance when sagemaker is connected through VPC. It is successful when VPC is not specified. I have internet gateway configured for my public subnets. I was able to pip download successfully in EC2 with same security groups and subnets. Is this any bug in sagemaker side?\n\nAnd also, can we log into EC2 instance that sagemaker created? It is more of like a blackbox testing without it.",
        "Answers":[
            {
                "Answer_creation_time":"2022-04-05T02:01:23.429Z",
                "Answer_upvote_count":0,
                "Answer_body":"When specifying a VPC config, ENIs are launched into your VPC which your Endpoint will use to communicate within your VPC. These ENIs have private IPs. In order to communicate with the internet you would need to make use of a NAT. SageMaker Endpoints are managed and it is currently not possible to SSH into the EC2 instance(s) backing your Endpoint.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"\u00bfHow can we crate a lambda which uses a Braket D-Wave device?",
        "Question_creation_time":1650979594609,
        "Question_link":"https:\/\/repost.aws\/questions\/QUt5r-dbkZT1O4yQM_TszxHw\/how-can-we-crate-a-lambda-which-uses-a-braket-d-wave-device",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Quantum Technologies",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "AWS Lambda",
            "Amazon Braket",
            "Amazon SageMaker",
            "Quantum Technologies"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":97.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"We are trying to deploy a Lambda with some code which works in a Notebook. The code is rather simple and uses D-Wave \u2014 DW_2000Q_6. The problem is that when we execute the lambda (container lambda due to size problems), it give us the following error:\n\n{\n  \"errorMessage\": \"[Errno 30] Read-only file system: '\/home\/sbx_user1051'\",\n  \"errorType\": \"OSError\",\n  \"stackTrace\": [\n    \"  File \\\"\/var\/lang\/lib\/python3.8\/imp.py\\\", line 234, in load_module\\n    return load_source(name, filename, file)\\n\",\n    \"  File \\\"\/var\/lang\/lib\/python3.8\/imp.py\\\", line 171, in load_source\\n    module = _load(spec)\\n\",\n    \"  File \\\"<frozen importlib._bootstrap>\\\", line 702, in _load\\n\",\n    \"  File \\\"<frozen importlib._bootstrap>\\\", line 671, in _load_unlocked\\n\",\n    \"  File \\\"<frozen importlib._bootstrap_external>\\\", line 843, in exec_module\\n\",\n    \"  File \\\"<frozen importlib._bootstrap>\\\", line 219, in _call_with_frames_removed\\n\",\n    \"  File \\\"\/var\/task\/lambda_function.py\\\", line 6, in <module>\\n    from dwave.system.composites import EmbeddingComposite\\n\",\n    \"  File \\\"\/var\/task\/dwave\/system\/__init__.py\\\", line 15, in <module>\\n    import dwave.system.flux_bias_offsets\\n\",\n    \"  File \\\"\/var\/task\/dwave\/system\/flux_bias_offsets.py\\\", line 22, in <module>\\n    from dwave.system.samplers.dwave_sampler import DWaveSampler\\n\",\n    \"  File \\\"\/var\/task\/dwave\/system\/samplers\/__init__.py\\\", line 15, in <module>\\n    from dwave.system.samplers.clique import *\\n\",\n    \"  File \\\"\/var\/task\/dwave\/system\/samplers\/clique.py\\\", line 32, in <module>\\n    from dwave.system.samplers.dwave_sampler import DWaveSampler, _failover\\n\",\n    \"  File \\\"\/var\/task\/dwave\/system\/samplers\/dwave_sampler.py\\\", line 31, in <module>\\n    from dwave.cloud import Client\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/__init__.py\\\", line 21, in <module>\\n    from dwave.cloud.client import Client\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/client\/__init__.py\\\", line 17, in <module>\\n    from dwave.cloud.client.base import Client\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/client\/base.py\\\", line 89, in <module>\\n    class Client(object):\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/client\/base.py\\\", line 736, in Client\\n    @cached.ondisk(maxage=_REGIONS_CACHE_MAXAGE)\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/utils.py\\\", line 477, in ondisk\\n    directory = kwargs.pop('directory', get_cache_dir())\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/config.py\\\", line 455, in get_cache_dir\\n    return homebase.user_cache_dir(\\n\",\n    \"  File \\\"\/var\/task\/homebase\/homebase.py\\\", line 150, in user_cache_dir\\n    return _get_folder(True, _FolderTypes.cache, app_name, app_author, version, False, use_virtualenv, create)[0]\\n\",\n    \"  File \\\"\/var\/task\/homebase\/homebase.py\\\", line 430, in _get_folder\\n    os.makedirs(final_path)\\n\",\n    \"  File \\\"\/var\/lang\/lib\/python3.8\/os.py\\\", line 213, in makedirs\\n    makedirs(head, exist_ok=exist_ok)\\n\",\n    \"  File \\\"\/var\/lang\/lib\/python3.8\/os.py\\\", line 213, in makedirs\\n    makedirs(head, exist_ok=exist_ok)\\n\",\n    \"  File \\\"\/var\/lang\/lib\/python3.8\/os.py\\\", line 223, in makedirs\\n    mkdir(name, mode)\\n\"\n  ]\n}\n\nIt seems that the library tries to write to some files which are not in \/tmp folder.\n\nI'm wondering if is possible to do this, and if not, what are the alternatives.\n\nimports used:\n\nimport boto3\nfrom braket.ocean_plugin import BraketDWaveSampler\nfrom dwave.system.composites import EmbeddingComposite\nfrom neal import SimulatedAnnealingSampler",
        "Answers":[
            {
                "Answer_creation_time":"2022-04-28T00:03:56.649Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, and thank you very much for your question!\n\nAs you noted, AWS Lambda allows for writing in the \/tmp folder and the dwave determines where it will cache data based on this algorithm.\n\nWe currently support running Braket in Notebook Instances you can create through our console, or as Braket Hybrid Jobs, which is more similar to your use of containers.\n\nHowever, we would really appreciate to understand more about your use case to see if that's something we should look into more deeply. Could you please give us more information about your use case, what you're trying to do, and why you think AWS Lambda will work well for it?",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Host multiple TensorFlow computer vision models using Amazon SageMaker multi-model endpoints",
        "Question_creation_time":1644300080795,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxAPkO75GTnSERkxpABFgSQ\/host-multiple-tensor-flow-computer-vision-models-using-amazon-sage-maker-multi-model-endpoints",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Computer Vision",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":112.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi All,\n\nGreetings!!\n\nCould you please clarify on two questions below which are related to this post.\n\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/host-multiple-tensorflow-computer-vision-models-using-amazon-sagemaker-multi-model-endpoints\/\n\nAs per AWS documentation (https:\/\/docs.aws.amazon.com...] \"Multi-model endpoints are not supported on GPU instance types\". I see we are using 'instance_type': 'ml.m5.2xlarge' to train both image classification and sign language digit classification models in this post.\n\nAs per instance configuration we don't have any GPU cores and GPU memory available in 'ml.m5.2xlarge' instance type. But we all know that we need a GPU instance for deep learning model training, we are not getting how we are able to train both classification models using 'ml.m5.2xlarge' instance type.\n\nDoes this image - IMAGE_URI = '763104351884.dkr.ecr.us-eas... has GPU cores?\n\nml.m5.2xlarge - Instance details:\n\nCompute Type: Standard Instances V CPU: 8 Memory: 32 GiB Clock Speed: undefined GPU: 0 Network Performance: Up to 10 Gigabit Storage: EBS only GPU Memory: 0\n\nAs mentioned in this post, I believe we have one production variant of both CIFAR and sign-language models meaning we have v1 models. Let's assume that we have a new set of images for classification then we have decided to train a new variant CIFAR model and want to create that model.\n\nSo we have 2 versions of the CIFAR model and 1 version of the sign-language model.\n\nHow can we get the inference from two production variants of CIFAR models? How can we do A\/B testing for CIFAR models?\n\nThanks in advance.",
        "Answers":[
            {
                "Answer_creation_time":"2022-11-16T18:43:01.669Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Vinayak & hope I can help:\n\nLet's take (1) GPU vs CPU first:\n\nYes, you're correct that Multi-Model Endpoints don't currently support GPU instance types (as documented here, and a lot of ML frameworks automatically reserve full GPU capacity which would make this tricky in practice anyway).\nI see from over on the blog post that the full container image URI you linked (which got chopped off here on rePost) was: 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-inference:2.3.1-cpu-py37-ubuntu18.04 - which is a -cpu- image so is set up for CPU-only instance types - excluding GPU tools like CUDA, etc.\nFrom the notebook of this sample that it's actually training the model on CPU-only instances too (using ml.m5.2xlarge in the Estimator constructor)\n\nTraining small DL models on CPU only, without GPU acceleration, is perfectly possible... Just often much slower because of the nature of the workload - especially as models and datasets get bigger. It should also be fine (for faster training speed) to train on GPU-accelerated instances (setting a different instance type in the Estimator) but then deploy for CPU-only inference (keeping m5 or similar in the MultiDataModel).\n\nGenerally this could be as easy as just changing the instance types in the notebook, but there's a couple of subtle things you might need to bear in mind if you see problems:\n\nYour SageMaker \"Model\" will reference a container image, which will be optimized for either CPU or GPU instances. This is why the SageMaker Python SDK Estimator.fit() API that runs a \"Training Job\" doesn't automatically register the output in the SageMaker \"Models\" list - it doesn't know whether the image will be the same or different until you define target infrastructure (e.g. with mme.deploy() in the notebook). I think you'll see that even creating the mme MultiDataModel in the notebook doesn't create a \"Model\" in SageMaker console until that deploy call.\nIf you're providing any custom inference scripts e.g. input_handler that reference CUDA devices or other GPU-specific tools, you'll need to check these scripts are set up to work okay on either GPU or CPU-only deployments. The example you linked is already CPU-only so that should be fine.\n\nNow for (2) A\/B testing:\n\nBetween variants, MME, multi-container endpoints, or even separate SageMaker endpoints with some proxy in front - you have multiple options for A\/B testing models depending what's easiest for your overall deployment, monitoring, integration and rollback requirements.\n\nSome tips I would suggest that might guide your choice:\n\nFor MCE, MME and separate-endpoints, your client logic would always need to explicitly choose one of the options\/versions to invoke. Using ProductionVariants, you can either explicitly specify a Target-Variant from the client side or let the configured variant weights do their thing and weight invocations automatically.\nMME supports a potentially large number of available models at a time (e.g. more than memory), but doesn't (yet?) offer a way to explicitly purge cached models from endpoint memory. This means you can't really overwrite the \"same\" model name on S3 within one MME - You'd need to add new versions in as new models on S3, and make sure your clients start requesting the new model names.\nI believe yes it should be possible to set up an endpoint with multiple variants, each of which is a MultiModel... Each variant would run on separate infrastructure instance(s). I guess you could probably use a new variant pointing to the same S3 models folder as a way to force clear an MME cache - because you could spin up new serving containers, push traffic over to them, and shut the old variant down? But this would be a pretty niche case.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-17T05:58:36.671Z",
                "Answer_upvote_count":0,
                "Answer_body":"@Alex_T\n\nThanks for your reply.\n\nLet's talk about MME only.\n\nAs mentioned in this post, I believe we have one production variant of both CIFAR and sign-language models meaning we have v1 models. Let's assume that we have a new set of images for classification then we have decided to train a new variant CIFAR model and want to create that model. So we have 2 versions of the CIFAR model and 1 version of the sign-language model.\n\nBoth CIFAR and sign-language models v1 models will be running in one MME then new production variant (version) of CIFAR model will be running in new MME?\n\nIn general, how can we do A\/B testing in MME? If possible then please clarify it with simple example.\n\nThanks",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-08T07:26:55.384Z",
                "Answer_upvote_count":0,
                "Answer_body":"MME now Support GPU instances as well. Checkout \u2013\n\nWhats New- https:\/\/aws.amazon.com\/about-aws\/whats-new\/2022\/10\/amazon-sagemaker-cost-effectively-host-1000s-gpu-multi-model-endpoint\/\n\nLaunch Blog-- https:\/\/aws.amazon.com\/blogs\/machine-learning\/run-multiple-deep-learning-models-on-gpu-with-amazon-sagemaker-multi-model-endpoints\/",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"fail to run guide demo of DeepRacer SageMaker",
        "Question_creation_time":1666058970622,
        "Question_link":"https:\/\/repost.aws\/questions\/QUWyDmIyDjQu2l8OJyKBFHNw\/fail-to-run-guide-demo-of-deep-racer-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":16.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I try to follow the developer guide of training model of DeepRacer on SageMaker, but fail in \"traning_job = sagemaker.create_training_job\" part:",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Excessive Memory use when deploying PipelineModel using the pre-build Scikit container in Sagemaker",
        "Question_creation_time":1663945337107,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2hdNVBreSFyX1iSPDRPMXw\/excessive-memory-use-when-deploying-pipeline-model-using-the-pre-build-scikit-container-in-sagemaker",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":17.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I am using the pre-build Scikit container in Sagemaker to deploy an endpoint based on a model that contains a 59.4 MB model.tar.gz file. The following line was used to deploy the endpoint:\n\nsm_model.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\", endpoint_name=endpoint_name)\n\nHowever, the after the endpoint was created, it fails to allocate memory to works. These error messages and warnings keep showing in the logs:\n\n[Errno 12] Cannot allocate memory [WARNING] Worker with pid 242 was terminated due to signal 9\n\nAs far as I know, the xlarge instance has 16 GB of memory. The endpoint memory usage is at 60% while it still fails to allocate memory to workers. May I ask if anyone has any insight on why this is happening and how to solve this issue without using an instance that has more memory?",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Access async endpoint created in console from notebook instance",
        "Question_creation_time":1668007391842,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwLeSd5B8RHS7RD7KQIl5TQ\/access-async-endpoint-created-in-console-from-notebook-instance",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":15.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"In SageMaker, I've created an async endpoint from a model. How does one access this endpoint from a notebook instance so that it can be used to make predictions?\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_time":"2022-11-09T21:26:59.952Z",
                "Answer_upvote_count":0,
                "Answer_body":"You may consider using invoke_endpoint_async function from sagemakerruntime in boto3 SDK. Doc here Just make sure your notebook's execution role has proper IAM permissions and fill in required parameters, boto3 will handle the rest.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is it possible to create Parallel Pipelines in Sagemaker",
        "Question_creation_time":1649666083053,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZtsbNf1GTAOnaTCrif-WJg\/is-it-possible-to-create-parallel-pipelines-in-sagemaker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":373.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I want to bind processing pipeline to multiple training pipeline. I just want to compare algorithm accuracy. Same dataset will be trained by multiple algorithms and will be predicted by them. My goal for the future is consolidate predict results of different algorithms and generate combined\/consolidated resulst. Is is possible to do in SageMaker.\n\nExample Schema:\n\n            - Train_Algo1      \n Process    - Train_Algo2    - Predict Result\n            - Train_AlgoN",
        "Answers":[
            {
                "Answer_creation_time":"2022-04-11T09:48:58.637Z",
                "Answer_upvote_count":0,
                "Answer_body":"I'd recommend checking out SageMaker Pipelines for this - especially if you're able to use SageMaker Studio for the graphical pipeline management UIs.\n\nYou can build your pipeline definition through the SageMaker Python SDK, just like you might normally define Training and Processing jobs. In fact pipeline steps (like TrainingStep) typically just wrap around the standalone constructs (like Estimator) that you might be using already.\n\nPipeline steps are executed in paralllel by default, unless there is an implicit (properties data) or explicit (depends_on) dependency between them.\n\nSM Pipelines can take parameters, so you could expose necessary training hyperparameters or pre-processing parameters up to the pipeline level, and use the pipeline to kick off multiple end-to-end runs with different configurations.\n\nBy turning on step caching, you could prevent your pre-processing from being re-run if the input parameters are unchanged (however, note that caching doesn't look at ongoing executions: So better to trigger one pipeline execution first and wait a bit for the processing step to complete, rather than triggering ~20 all at once so none of them see a cached processing result and all re-run the job).\n\n...And Pipelines automatically tag SageMaker Experiments config (Pipeline = Experiment; Execution = Trial; Step = Trial Component) which you can then use to plot and compare multiple training jobs in the SM Studio UI. So for example your pipeline might just be Pre-process > Train > Evaluate > RegisterModel. If you right click your pipeline's \"Experiment\" in SMStudio Experiments and Trials view, you can open a list of the executed training jobs and select multiple to scatter-plot the final loss\/accuracy vs the hyperparameters.\n\nIf you run your evaluation as a SageMaker Processing Job which outputs a JSON in model quality metrics format, you can even have your pipeline load the model into SM Model Registry tagged with this data. This way, you'd be able to see and compare the metrics between model versions (and even charts e.g. ROC curves in classification case) through the SMStudio Model Registry UI.\n\nSome relevant code samples:\n\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/main\/sagemaker-pipeline-compare-model-versions\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/main\/sagemaker-pipeline-parameterization",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"can sagemaker batch transform process input files with new line character ?",
        "Question_creation_time":1649208896916,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrZKVALwvQjCcoBn4bYUayg\/can-sagemaker-batch-transform-process-input-files-with-new-line-character",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":168.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"example provided in the aws documentation , https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html, see sample input csv can be structured like a sample below. is there a link, to see the actual csv file that is used in an working example , instead of the sample posted in the docs. it is also mentioned that each record is one per line and no end of line character is allowed. what if the input type is other than csv, like json? do the same rules apply to json files as well, one record per line, or can one record be spread across multiple lines ,what about new line character in that case?\n\nRecord1-Attribute1, Record1-Attribute2, Record1-Attribute3, ..., Record1-AttributeM\nRecord2-Attribute1, Record2-Attribute2, Record2-Attribute3, ..., Record2-AttributeM\n...\n...",
        "Answers":[
            {
                "Answer_creation_time":"2022-04-11T17:27:14.573Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yes, you will need to specify SplitType parameter ( Reference)\n\n   Split_type=\"Line\",",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using Lambda for data processing - Sagemaker",
        "Question_creation_time":1669522752249,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-EUVNgsoRViEFMwlZbcIAg\/using-lambda-for-data-processing-sagemaker",
        "Question_topic":[
            "Storage",
            "Serverless",
            "Compute",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon Simple Storage Service",
            "AWS Lambda",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":20.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I have created a docker image which has Entrypoint as processing.py. This script is taking data from \/opt\/ml\/processing\/input and after processing putting it \/opt\/ml\/processing\/output folder.\n\nFor processing the data I should put the file in \/opt\/ml\/processing\/input from s3 and then pick processed file from \/opt\/ml\/processing\/output into S3.\n\nFollowing script in sagemaker is doing it properly:\n\nfrom sagemaker.processing import Processor, ProcessingInput, ProcessingOutput import sagemaker\n\ninput_data = 's3:\/\/sagemaker-ap-south-1-057036842446\/sagemaker\/Data\/Training\/Churn_Modelling.csv' output_dir = 's3:\/\/sagemaker-ap-south-1-057036842446\/sagemaker\/Outputs\/' image_uri = '057036842446.dkr.ecr.ap-south-1.amazonaws.com\/aws-docker-repo:latest' aws_role = sagemaker.get_execution_role()\n\nprocessor = Processor(image_uri= image_uri, role=aws_role, instance_count=1, instance_type=\"ml.m5.xlarge\")\n\nprocessor.run(inputs=[ProcessingInput( source=input_data, destination='\/opt\/ml\/processing\/input')], outputs=[ProcessingOutput(source='\/opt\/ml\/processing\/output', destination=output_dir)] )\n\nCould someone please guide how this can be executed with lambda function? It is not recognizing sagemaker package, second there is a challenge in placing file before the script execution and pick processed files.\n\nI am trying codepipeline to automate this operation.",
        "Answers":[
            {
                "Answer_creation_time":"2022-11-27T22:22:23.442Z",
                "Answer_upvote_count":0,
                "Answer_body":"SageMaker Python SDK, as a third party Python package, is not naturally installed Lambda's default runtime. You need to install this package.\n\nYou can either follow steps in https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/python-package.html to form a zip archive or similarly install sagemaker Python SDK as a layer as in https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/configuration-layers.html\n\nThis should provide a similar runtime env as you container.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-28T09:53:48.959Z",
                "Answer_upvote_count":0,
                "Answer_body":"I have build a codepipeline where after build I am executing lambda in the deploy stage.\n\nIn Lamdba I am executing the docker image.\n\nTo execute docker image I need to first provide it file from s3 and after script processing file generated need to move to S3.\n\nIs it actually possible in Lambda limitations?",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Studio PyTorch 1.8 kernel has no PyTorch, Numpy, or Matplotlib module",
        "Question_creation_time":1639384597526,
        "Question_link":"https:\/\/repost.aws\/questions\/QUns1rahq-ShmzYk0GJoLGWA\/sage-maker-studio-py-torch-1-8-kernel-has-no-py-torch-numpy-or-matplotlib-module",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":210.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm working with SageMaker studio with the following options:\n\nkernel: PyTorch 1.8 Python 3.6 GPU optimized.\ninstance: ml.g4dn.xlarge\n\nWhen running import torch numpy, matplotlib or PIL, I'm getting the No module named 'X' error. No matter when using pip install in a cell above, it will not be imported. Is this a problem only I am encountering with the new PyTorch 1.8 kernel? It also happens with the CPU-optimized version. However, PyTorch 1.6 kernel does not throw an error.\n\nWhen running conda list, I get the output without any of the previously mentioned modules.",
        "Answers":[
            {
                "Answer_creation_time":"2021-12-13T08:55:47.690Z",
                "Answer_upvote_count":0,
                "Answer_body":"I have followed your explanation to recreate the No module named 'X'. After instance lunch, I could import numpy, matplotlib and PIL but could not import torch because torch is not initially installed.\n\nI followed these steps to install and import torch:\n\nShut down your instance completely.\nAttach the kernel to your notebook again\nLunch your ml.g4dn.xlarge instance again\npip3 install torch\nimport torch",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-14T20:32:06.803Z",
                "Answer_upvote_count":0,
                "Answer_body":"No module named 'X' should not be the expected behavior. Thanks for reporting this issue and sorry for the inconvenience it caused.\n\nIs your use case flexible to use other PyTorch versions prior to 1.8? If yes, please try other versions. If PT1.8 is the only choice, please try following workaround for unblocking(while service team is working on the fix).\n\nTwo options:\n1 - executed following in a notebook cell\n\n# switch the python execution to \/usr\/local\/bin\/python in the kernel.json file. \n!sed 's|^ *\"python\",|  \"\/usr\/local\/bin\/python\",|g' \/usr\/local\/share\/jupyter\/kernels\/python3\/kernel.json>\/tmp\/kernel.json; cp -f \/tmp\/kernel.json \/usr\/local\/share\/jupyter\/kernels\/python3\/kernel.json;\n\n\n2 - directly execute following shell command in kernel image specific terminal(not the global terminal).\n\nsed 's|^ *\"python\",|  \"\/usr\/local\/bin\/python\",|g' \/usr\/local\/share\/jupyter\/kernels\/python3\/kernel.json>\/tmp\/kernel.json; cp -f \/tmp\/kernel.json \/usr\/local\/share\/jupyter\/kernels\/python3\/kernel.json;`\n\n\nThe above command is only needed once per kernel gateway app. After above, please restart the kernel. You can verify using following command in a notebook cell. The '\/usr\/local\/bin\/python' should be shown as python executable.\n\nimport sys\nprint(sys.executable)",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Studio domain lifecycle configuration for hosting VSCode not working and no error logs",
        "Question_creation_time":1652176660406,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfxuiAWeXShmE1q7BfMdhCw\/sage-maker-studio-domain-lifecycle-configuration-for-hosting-vs-code-not-working-and-no-error-logs",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":42.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi all,\n\nI've followed the guide provided in this medium blog by senior AWS ML SA: https:\/\/towardsdatascience.com\/hosting-vs-code-in-sagemaker-studio-f211385e25f7\n\nI've created the lifecycle configuration (jupyterlab app) for the following bash script:\n\n#!\/bin\/bash\n\nmkdir -p vscode\ncd vscode\nsudo su\ncurl -fsSL https:\/\/code-server.dev\/install.sh | sh\nexit\n\n\nHowever, when I attach the configuration to the domain or a specific user's jupyter server in the SM Studio, the JupyterServer App fails to be created with the following message displayed on the loading page of SM Studio's jupyterlab:\n\n\"The JupyterServer app default encountered a problem and was stopped.Details: ConfigurationError: LifecycleConfig execution failed with non zero exit code 1 for script arn:aws:sagemaker:eu-west-3:615740825886:studio-lifecycle-config\/install-vscode-on-jupyterserver. Check https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-lcc-debug.html for debugging instructions.\"\n\nI checked the page for debugging instructions, specifically the part where I am instructed to check the cloudwatch logs for the log group: aws\/sagemaker\/studio The problem is this log group does not exist in CloudWatch (I have admin permission).\n\nI would like to have this set up for the entire team, Any help would be greatly appreciated!\n\nBest, Ruoy",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Please validate: SageMaker Endpoint URL Authentication\/Authorization",
        "Question_creation_time":1602169065000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFlHNZ7JxTFGIkPHQ75u44w\/please-validate-sage-maker-endpoint-url-authentication-authorization",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":282.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Need validation:\n\nOnce the SageMaker endpoint is deployed. It can be invoked with the Sagemaker Runtime API InvokeEndpoint OR it can be invoked using the endpoint URL+HTTP AZ headers (below).\n\nSuccessful deployment also exposes a URL (on the console) that has the format:\n\nhttps:\/\/runtime.sagemaker.us-east-1.amazonaws.com\/endpoints\/ENDPOINT-NAME\/invocations\n\nWhat is the purpose of this URL (shown on console)?\n\nIn my understanding this URL Cannot be invoked w\/o appropriate headers as then there will be a need to have globally unique endpoint name!! THAT IS to invoke this URL it needs to have the \"HTTP Authorization headers\" (refer: https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/API\/sig-v4-authenticating-requests.html)\n\nI have a customer who is concerned that anyone can invoke the URL even from the internet. Tried to do it and received the <MissingTokenException> so I know it can't be done but just want to ensure I have the right explanation. (Test with HTTP\/AZ headers pending)",
        "Answers":[
            {
                "Answer_creation_time":"2020-10-08T15:38:33.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Your understanding is correct. From the docs:\n\nAmazon SageMaker strips all POST headers except those supported by the API. Amazon SageMaker might add additional headers. You should not rely on the behavior of headers outside those enumerated in the request syntax.\n\nCalls to InvokeEndpoint are authenticated by using AWS Signature Version 4.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"sagemakee endpoint failing with \"\"An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body\"\"",
        "Question_creation_time":1669407059790,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbzR_PqclRoK5dmfEzR53wg\/sagemakee-endpoint-failing-with-an-error-occurred-model-error-when-calling-the-invoke-endpoint-operation-received-client-error-413-from-primary-and-could-not-load-the-entire-response-body",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":64.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello, I have created sagemaker endpoint by following https:\/\/github.com\/huggingface\/notebooks\/blob\/main\/sagemaker\/20_automatic_speech_recognition_inference\/sagemaker-notebook.ipynb and this is failing with error \"\"An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body\"\".\n\nThe predict function returning me following error but CW log does not have any error details for the endpoint.\n\nModelError Traceback (most recent call last)\n\/tmp\/ipykernel_16248\/2846183179.py in\n2 # audio_path = \"s3:\/\/ml-backend-sales-call-audio\/sales-call-audio\/1279881599154831602.playback.mp3\"\n3 audio_path = \"\/home\/ec2-user\/SageMaker\/finetune-deploy-bert-with-amazon-sagemaker-for-hugging-face\/1279881599154831602.playback.mp3\" ## AS OF NOW have stored locally in notebook instance\n----> 4 res = predictor.predict(data=audio_path)\n5 print(res)\n\n~\/anaconda3\/envs\/amazonei_pytorch_latest_p37\/lib\/python3.7\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model, target_variant, inference_id)\n159 data, initial_args, target_model, target_variant, inference_id\n160 )\n--> 161 response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n162 return self._handle_response(response)\n163\n\n~\/anaconda3\/envs\/amazonei_pytorch_latest_p37\/lib\/python3.7\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n493 )\n494 # The \"self\" in this scope is referring to the BaseClient.\n--> 495 return self._make_api_call(operation_name, kwargs)\n496\n497 _api_call.name = str(py_operation_name)\n\n~\/anaconda3\/envs\/amazonei_pytorch_latest_p37\/lib\/python3.7\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n912 error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n913 error_class = self.exceptions.from_code(error_code)\n--> 914 raise error_class(parsed_response, operation_name)\n915 else:\n916 return parsed_response\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body. See https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/asr-facebook-wav2vec2-base-960h-2022-11-25-19-27-19 in account xxxx for more information.\n\n`",
        "Answers":[
            {
                "Answer_creation_time":"2022-11-26T15:49:44.270Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello AmitKayal,\n\nI understand that you have successfully created an Endpoint. However, when you try to invoke this Endpoint you get the following error:\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body. See https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/asr-facebook-wav2vec2-base-960h-2022-11-25-19-27-19 in account xxxx for more information.\n\n\n\nAnd when you looked at your CloudWatch logs there was nothing related to this error. Let me know if I have misunderstood anything.\n\nThe ClientError 413 usually occurs when the payload size for the endpoint invocation exceeds the limit of 6 MB [1,2], this could be the reason why your Endpoint is throwing the error 413.\n\nIf your payload size is more than 6MB you can work around this by using either Batch Transform [3] or Asynchronous Inference [4]. Batch Transform can be used if you would like to process the request to your model in batches. With Batch Transform you have an option to define your own maximum payload size [5]. Otherwise, if you want to receive inference for each request to your model you can use Asynchronous Inference which takes up to 1 GB of payload size with the runtime of about 15 minutes [6]. Asynchronous Inference queues requests to your model and processes them asynchronously, this is ideal for payloads that are greater than 6MB but not more than 1GB.\n\nShould the suggested work around not work, I recommend that you open a Support case with AWS Technical Support [7]. The Technical Support team will be able to help you to further troubleshoot this issue.\n\nI trust this information finds you well. Should you have any further questions, please feel free to reach out.\n\nReferences:\n\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/245\nhttps:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html#limits_sagemaker\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-batch.html\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/async-inference.html\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/hosting-faqs.html#hosting-faqs-general\nhttps:\/\/support.console.aws.amazon.com\/support\/home?region=us-east-1#\/case\/create",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Mandate user to enable encryption while Sagemaker notebook creation?",
        "Question_creation_time":1658816453567,
        "Question_link":"https:\/\/repost.aws\/questions\/QUA0H60oMZTUy1JgSwFVXV4g\/mandate-user-to-enable-encryption-while-sagemaker-notebook-creation",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Encryption"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":49.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"1.We would like to mandate user to enable KMS encryption while creating Sagemaker notebooks, I would like to know any methods via policy or any other way?",
        "Answers":[
            {
                "Answer_creation_time":"2022-07-27T02:04:13.390Z",
                "Answer_upvote_count":0,
                "Answer_body":"If the user is creating the notebook from the console the encryption is an optional field and we will not be able to enforce it. One alternate way to do this is to use the Boto3 API to create the notebook instance programatically. This way we can check for the encryption or automatically add encryption fields.\n\nhttps:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_notebook_instance",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-27T00:21:44.051Z",
                "Answer_upvote_count":0,
                "Answer_body":"Sorry I haven't been able to test this yet, but thought it was worth adding:\n\nAccording to the IAM reference page for Amazon SageMaker, the sagemaker:CreateNotebookInstance action supports specifying the sagemaker:VolumeKmsKey condition key.\n\nTherefore I believe you should be able to prevent users creating notebook instances by modifying their IAM permissions to only allow CreateNotebookInstance where VolumeKmsKey is provided. If you're new to the concept of condition keys in IAM, you can find more info here.\n\nI would mention that even if this works as expected, the error message a user sees when they're prevented from creating the instance will be a pretty generic \"Access denied\" - so you'll need to educate them on the requirement for a good user experience.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to create a serverless endpoint in sagemaker?",
        "Question_creation_time":1645674841041,
        "Question_link":"https:\/\/repost.aws\/questions\/QULRy50Vd7SW6KT0MMzk4NeQ\/how-to-create-a-serverless-endpoint-in-sagemaker",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":277.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am recreating an endpoint currently working in sagemaker for inference to a serverless endpoint. I am using one of the images ( huggingface-pytorch-inference:1.9.1-transformers4.12.3-cpu-py38-ubuntu20.04) found here -> https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md.\n\neverything works when i choose non serverless, i.e. provisioned option for endpoint configuration , but when i try to create one with serverless option it fails. error messages are below ( from the logs in cloudwatch) . starting with python and log4j error at the end.\n\n'python: can't open file '\/user\/local\/bin\/deep_learning_container.py': [Errno 13] permission denied. Requirement already satisfied: transformers in \/opt\/conda\/lib\/pythong3.6 ..... ..... Warning: MMS is using non-default JVM parameters: -XX: -UseContainersupport Failed to reap children process log4j: ERROR setfile(null,true) call failed. java.io.FileNotFoundException: logs\/mms_log.log (No Such file or directory)\n\nwhy am i getting this error ???\n\nFYI - i have set memory to maximum allowed memory size of 6gb. for the serverless option.",
        "Answers":[
            {
                "Answer_creation_time":"2022-02-24T08:28:43.776Z",
                "Answer_upvote_count":0,
                "Answer_body":"The cause might be that your SageMaker Python SDK is not updated to the latest version. Please make sure you update it to the latest version as well as the AWS SDK for Python (boto3). You can use pip:\n\npip install --upgrade boto3\npip install --upgrade sagemaker\n\n\nFor a sample notebook you can have a look here. More information on the documentation page.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Custom Post Annotation Lambda Function for Custom Labeling Job",
        "Question_creation_time":1659025093082,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYzKc8ISiT4eU9cXCsdUqEg\/custom-post-annotation-lambda-function-for-custom-labeling-job",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "AWS Lambda",
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":71.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello,\n\nI have implemented a post annotation lambda function for my sagemaker ground truth custom job.\n\nAfter the annotations are finished, the results come after the consolidation of the annotations are saved in a subdirectory called \"iteration_X\" of \"annotations\/consolidated-annotation\/consolidation-response\/\".\n\nHowever, the outcome of the annotations is never successful and from the log of the lambda function used for the post annotation I always receive this type of error:\n\n{\n    \"labeling-job-name\": \"labeling-job-full-dataset-test-giusy-10\",\n    \"event-name\": \"ANNOTATION_CONSOLIDATION_LAMBDA_SCHEMA_MATCHING_FAILED\",\n    \"event-log-message\": \"ERROR: Annotation consolidation Lambda response did not match expected data format for line 1.\"\n}\n\n\nBased on this guide (https:\/\/docs.aws.amazon.com\/id_id\/sagemaker\/latest\/dg\/sms-custom-templates-step3-lambda-requirements.html) I made sure that my lambda function returns:\n\nRESPONSE:\n\n\n[\n  {\n    \"datasetObjectId\": \"1\",\n    \"consolidatedAnnotation\": {\n      \"content\": {\n        \"annotations\": {\n          \"relations\": [\n            {\n              \"subj\": \"CW\",\n              \"predicate\": \"adjust\",\n              \"obj\": \"key\"\n            },\n            {\n              \"subj\": \"key\",\n              \"predicate\": \"with\",\n              \"obj\": \"right_hand\"\n            },\n            {\n              \"subj\": \"key\",\n              \"predicate\": \"on\",\n              \"obj\": \"lock\"\n            }\n          ],\n          \"groundings\": {\n            \"pre_frame\": [\n              {\n                \"object\": \"right_hand\",\n                \"left\": 776.5,\n                \"top\": 219.5,\n                \"width\": 282.52,\n                \"height\": 246.5\n              },\n              {\n                \"object\": \"lock\",\n                \"left\": 716.4,\n                \"top\": 255.6,\n                \"width\": 93.60000000000002,\n                \"height\": 111.6\n              }\n            ],\n            \"pnr_frame\": [\n              {\n                \"object\": \"right_hand\",\n                \"left\": 974.16,\n                \"top\": 275.14,\n                \"width\": 287.21,\n                \"height\": 215.84\n              },\n              {\n                \"object\": \"lock\",\n                \"left\": 914.4,\n                \"top\": 291.6,\n                \"width\": 97.20000000000005,\n                \"height\": 90\n              }\n            ],\n            \"post_frame\": [\n              {\n                \"object\": \"right_hand\",\n                \"left\": 858.58,\n                \"top\": 240.54,\n                \"width\": 316.28,\n                \"height\": 209.37\n              },\n              {\n                \"object\": \"lock\",\n                \"left\": 741.6,\n                \"top\": 237.6,\n                \"width\": 61.19999999999993,\n                \"height\": 169.20000000000002\n              }\n            ]\n          },\n          \"timestamp\": \"0\",\n          \"clip_uid\": \"undefined\"\n        }\n      }\n    }\n  }\n]\n\n\nI can't figure out how to avoid this type of error and make the annotations go through when the job is finished.",
        "Answers":[
            {
                "Answer_creation_time":"2022-07-30T16:31:54.591Z",
                "Answer_upvote_count":0,
                "Answer_body":"It may be possible that ground truth is attempting to read the result from the annotations\/consolidated-annotation\/ directory and thus not actually reading the consolidated response but the file containing them",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-30T08:20:10.765Z",
                "Answer_upvote_count":0,
                "Answer_body":"inside the lambda function method I'm reading from the data from: \"annotations\/consolidated-annotation\/consolidation-request\/\" because it contains all the data necessary for the desired response.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to debug invocation timeout in sagemaker?",
        "Question_creation_time":1649727502799,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE4UPZjwNQveIG8zuZeXIgA\/how-to-debug-invocation-timeout-in-sagemaker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":432.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am testing inference in sagemaker , by using one of the container listed here -> https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md. the model is zipped up as below and with in inference.py file , i am overwriting functions like model_fn method and predict_fn. I tested this with batch transform and it worked but for few small input files but for other larger files, i keep getting \"Model server did not respond to \/invocations request within 3600 seconds\" . I'm trying to find out what is the cause of it? 3600 is the max we can set for \"invocation timeout in seconds\" parameter and the default input size for batch is 6mb , the input files i'm using are way smaller than that but i still get that error.\n\nDirectory structure\n\nmodel.tar.gz\/\n|- model.pth\n|- code\/\n  |- inference.py\n  |- requirements.txt  \n\n\n\nfile : inference.py\n\nimport torch\nimport os\n\ndef model_fn(model_dir):\n    model = Your_Model()\n    with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:\n        model.load_state_dict(torch.load(f))\n    return model\n\ndef predict_fn():\n    \/\/\n\n\nbased on docs here, https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html#your-algorithms-batch-code-how-containers-should-respond-to-inferences, do we need to install flask and have an \/invocations endpoint , that responds 200 ok , when we are using custom container?",
        "Answers":[
            {
                "Answer_creation_time":"2022-04-15T20:48:32.532Z",
                "Answer_upvote_count":0,
                "Answer_body":"One of the best ways to debug a custom inference script would be to start off with using the SageMaker \"local mode\". Once you are sure that your script is working fine, move over to hosting on the SageMaker endpoint. Here are some of the examples to get started.\n\nExample for a TF serving model that I have a custom Inference script, I would use local mode as shown below for my testing-\n\nfrom sagemaker.tensorflow.model import TensorFlowModel\nfrom sagemaker.local import LocalSession\n\ntensorflow_serving_model = TensorFlowModel(\n    model_data=model_data,\n    role=sagemaker_role,\n    framework_version=\"2.6\",\n  # sagemaker_session=sagemaker_session,\n  sagemaker_session=LocalSession()\n)",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Debugger: cannot load training information of estimator",
        "Question_creation_time":1660301495882,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUl_ylpIuQUWy0N-CDqP_ag\/sage-maker-debugger-cannot-load-training-information-of-estimator",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Model Training"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":72.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am using a SageMaker notebook for training a ML model. When I created and trained the estimator successfully with the following script, I could load the debugging information (s3_output_path) as expected:\n\nfrom sagemaker.debugger import Rule, DebuggerHookConfig, CollectionConfig, rule_configs\nrules = [\n    Rule.sagemaker(rule_configs.loss_not_decreasing()),\n    Rule.sagemaker(rule_configs.vanishing_gradient()),\n    Rule.sagemaker(rule_configs.overfit()),\n    Rule.sagemaker(rule_configs.overtraining()),\n    Rule.sagemaker(rule_configs.poor_weight_initialization())]\n\ncollection_configs=[CollectionConfig(name=\"CrossEntropyLoss_output_0\", parameters={\n    \"include_regex\": \"CrossEntropyLoss_output_0\", \"train.save_interval\": \"100\",\"eval.save_interval\": \"10\"})]\n\ndebugger_config = DebuggerHookConfig(\n    collection_configs=collection_configs)\n\nestimator = PyTorch(\nrole=sagemaker.get_execution_role(),\ninstance_count=1,\ninstance_type=\"ml.m5.xlarge\",\n#instance_type=\"ml.g4dn.2xlarge\",\nentry_point=\"train.py\",\nframework_version=\"1.8\",\npy_version=\"py36\",\nhyperparameters=hyperparameters,\ndebugger_hook_config=debugger_config,\nrules=rules,\n)\n\nestimator.fit({\"training\": inputs})\n\ns3_output_path = estimator.latest_job_debugger_artifacts_path()\n\n\nAfter the kernel died, I attached the estimator and tried to access the debugging information of the training:\n\nestimator = sagemaker.estimator.Estimator.attach('pytorch-training-2022-06-07-11-07-09-804')\n\ns3_output_path = estimator.latest_job_debugger_artifacts_path()\nrules_path = estimator.debugger_rules\n\n\nThe return values of these 2 functions were None. Could this be a problem with the attach-function? And how can I access training information of the debugger after the kernel was shut down?",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-30T02:52:34.475Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nIt will be difficult to analyze why the two values of the functions were none. We will require more information on the same. I will request you to open a support case with AWS PS and one of our Sagemaker engineer should be able to assist you.\n\nThank you.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Ground Truth Job Validation post completion",
        "Question_creation_time":1668077364552,
        "Question_link":"https:\/\/repost.aws\/questions\/QURI4l_IFoR9SZyc-6wNPqgQ\/ground-truth-job-validation-post-completion",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Ground Truth"
        ],
        "Question_upvote_count":1.0,
        "Question_view_count":21.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"We annotated 10 pdf files in Ground Truth, how do I validate the annotations done by the team ? Do we have any metrics ? Ex - How many annotations done in one pdf ? What is the confidence score for each annotation ?\n\nMy idea is that if i get this metrics, I will review the doc with less number of annotations and doc with low confidence score.\n\nCan Ground truth expers provide some insights in this ?",
        "Answers":[
            {
                "Answer_creation_time":"2022-11-21T11:54:18.048Z",
                "Answer_upvote_count":0,
                "Answer_body":"This is a machine learning problem and goes beyond AWS Ground Truth.\n\nUsually you do not get to measure how confident each annotation is, unless you asked the annotators to say how confident they are on each annotation.\n\nUsually you have to provide some gold standard annotations that you think are correct. You do this on some sample PDFs that you annotated yourself. Then you check the performance of each annotator on your set. You can compute metrics such as Cohen's Kappa to assess agreement between annotators: https:\/\/en.wikipedia.org\/wiki\/Cohen%27s_kappa",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"aws.amazon.commachine-learning\/pricing link is broken",
        "Question_creation_time":1664336034992,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvybuFyXJRnW4Q_LvXxbEaA\/aws-amazon-commachine-learning-pricing-link-is-broken",
        "Question_topic":[
            "Machine Learning & AI",
            "Cloud Financial Management"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Pricing Calculator",
            "Pricing"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":40.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"https:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/requesting-real-time-predictions.html article\n\nlink => http:\/\/aws.amazon.commachine-learning\/pricing\/",
        "Answers":[
            {
                "Answer_creation_time":"2022-09-28T08:25:45.712Z",
                "Answer_upvote_count":2,
                "Answer_body":"'Amazon Machine Learning service' is now deprecated and is replaced by Amazon SageMaker. You can see the warning on the top of the page you shared in the post or this is from the documentation page - \"We are no longer updating the Amazon Machine Learning (Amazon ML) service or accepting new users for it. This documentation is available for existing users, but we are no longer updating it.\"\n\nAnd for SageMaker, you can see the pricing here. Real time predictions (equivalent to what your posted link was suggesting) on SageMaker documentation can be found here.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-28T03:42:21.132Z",
                "Answer_upvote_count":1,
                "Answer_body":"Feedback on AWS official documentation can be provided via the feedback link in the upper right corner of the document, for example.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS Sagemaker model endpoint data capture",
        "Question_creation_time":1668105022579,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJfA77iAoT2-EkVff7sc7LA\/aws-sagemaker-model-endpoint-data-capture",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":35.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have deployed a sagemaker endpoint, with data capture enabled with a 100% sampling rate. But after running it multiple times, it is not storing all the inputs to the model,\n\nFor instance, after running 3 times, it saves only one time.",
        "Answers":[
            {
                "Answer_creation_time":"2022-11-14T12:08:34.106Z",
                "Answer_upvote_count":0,
                "Answer_body":"CLOSED,\n\nSolved something else was going wrong",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"In SageMaker Studio, how to decide on which instance to open a terminal?",
        "Question_creation_time":1606945520000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUo5ycye8jQ7Cw8dgSAfE9RQ\/in-sage-maker-studio-how-to-decide-on-which-instance-to-open-a-terminal",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":495.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nI was writing some notebook on a t2.Medium Studio Notebook. Now I just switched to an m5.8xlarge. However, when I launch a terminal, it still shows up only 2 CPUs, not the 32 I expected. How to open a terminal on that m5.8xlarge instance?",
        "Answers":[
            {
                "Answer_creation_time":"2020-12-03T11:09:16.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Where do you launch the terminal from? If you use the launcher window, it would start on the t2.medium as you are experiencing.\n\nHowever, if you use the launch terminal button in the toolbar that is displayed at the top of your notebook, it will launch the image terminal on the new instance the notebook's kernel is running on (your m5.8xlarge instance).",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Clone Failed SageMaker MLOps Project Using Third-party Git Repos",
        "Question_creation_time":1668586359113,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhIvfTpxzRtWRE-dO20bHmQ\/clone-failed-sage-maker-ml-ops-project-using-third-party-git-repos",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":37.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm trying to use MLOps template for model building, training, and deployment with third-party Git repositories using CodePipeline in my ML project. I created the project successfully using the template and all the seed code is available in the GitHub repos I specified. But when I try to clone the repo, I am getting the below error\n\nI see that the local path that has been specified in\n\nNo such file or directory: '\/home\/sagemaker-user\/home\/sagemaker-user\/cat-ml-test-1-p-mtd5ofsbdgva\/sagemaker-p-mtd5ofsbdgva-modeldeploy'\n\n\ndoes not sound quite right. But there's is no option to change the local path by myself also.\n\nHow can I solve this? Any leads are welcome TIA",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS sagemaker abalone example pipeline endpoint json rejected",
        "Question_creation_time":1656607883243,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJRDfNUpdR-WQgjsw_UOi5g\/aws-sagemaker-abalone-example-pipeline-endpoint-json-rejected",
        "Question_topic":[
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":145.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"We've just created a train\/build\/deploy template in AWS SageMaker which provides a deployment of an Abalone model. We're trying to test it via the Test Inference endpoint, but the JSON there is rejected with the following message:\n\nError invoking endpoint: Received client error (415) from model with message \"application\/json is not an accepted ContentType: csv, libsvm, parquet, recordio-protobuf, text\/csv, text\/libsvm, text\/x-libsvm, application\/x-parquet, application\/x-recordio-protobuf.\". See https:\/\/eu-west-1.console.aws.amazon.com\/cloudwatch\/home?region=eu-west-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/USEngProbOfConversion-staging in account 607522716587 for more information.\n\nHowever the Test Inference endpoint only allows us to hit the endpoint with JSON - what can we do? Here's a screenshot (if this dropbox embed works):\n\nbut that's not working so here's the request dump:\n\n{\n  \"body\": {\n    \"s-x\": \"M\",\n    \"length\": 3,\n    \"diameter\": 5,\n    \"height\": 7,\n    \"whole_weight\": 45,\n    \"shucked_weight\": 34,\n    \"viscera_weight\": 23,\n    \"shell_weight\": 76\n  },\n  \"contentType\": \"application\/json\",\n  \"endpointName\": \"USEngProbOfConversion-staging\",\n  \"customURL\": \"\",\n  \"customHeaders\": [\n    {\n      \"Key\": \"sm_endpoint_name\",\n      \"Value\": \"USEngProbOfConversion-staging\"\n    },\n    {\n      \"Key\": \"\",\n      \"Value\": \"\"\n    }\n  ]\n}",
        "Answers":[
            {
                "Answer_creation_time":"2022-07-01T03:03:27.263Z",
                "Answer_upvote_count":0,
                "Answer_body":"So as you saw already, the SageMaker Studio \"Test inference\" UI currently only supports JSON format... But this is a constraint of the UI, not your endpoint.\n\nIf you want to test your deployed endpoint with non-JSON data, you can do this from code (e.g. from a notebook):\n\nUsing the sagemaker Python SDK, create a Predictor specifying your endpoint name and the relevant de\/serializers (from sagemaker.(de)serializers - for example sagemaker.serializers.CSVSerializer). Then call predictor.predict(data).\nUsing a boto3.client(\"sagemaker-runtime\"), serialize your data to required format yourself (e.g. \"M,3,5,7,45,34,23,76\") and then call invoke_endpoint().\n\nThis would be necessary if you're using a pre-built algorithm that doesn't support JSON as a request\/response format. Since you're using the Abalone pipeline example, I guess it's likely you're using XGBoost as a pre-built SageMaker algorithm?\n\nAlternatively, if you're building a custom algorithm with your own training script OR would be interested in using XGBoost as a script-mode framework - more information in the SageMaker SDK doc), you may like to extend your algorithm to accept application\/json requests and return JSON responses.\n\nThe process for this will vary a little by framework, but should be documented here for XGBoost. Essentially, you'll want to provide a script file e.g. inference.py which defines special functions input_fn() and output_fn(). You can provide implementations of these functions that accept application\/json content types and de\/serialize appropriately.\n\nThat way you could make your deployed endpoint support JSON format and therefore be able to use the test UI in SageMaker Studio.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"make julia notebooks work in SageMaker",
        "Question_creation_time":1644167031317,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2PXu3tbpQ7-V2OTlD_I07Q\/make-julia-notebooks-work-in-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":1.0,
        "Question_view_count":95.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I want to run a Jupyter notebook in SageMaker with a Julia kernel. There is very little documentation about this. There is this:\n\nhttps:\/\/d1.awsstatic.com\/whitepapers\/julia-on-sagemaker.pdf?did=wp_card&trk=wp_card\n\nI followed all the instructions, and Julia shows up in the JupyterLab launcher; but when I run it, Julia 1.17.1 shows up as the kernel and then dies. It appears to be trying, but then gives up and says \"No Kernel\" instead of \"Julia 1.17.1\" in the status line.\n\nIf I run the R kernel, all goes well. If I run the Julia kernel (which shows up in the list of available kernels!), I get the following error message:\n\nConnection failed\n\nA connection to the notebook server could not be established.\n\nThe notebook will continue trying to reconnect.\n\nCheck your network connection or notebook server configuration.",
        "Answers":[
            {
                "Answer_creation_time":"2022-02-10T18:11:39.365Z",
                "Answer_upvote_count":1,
                "Answer_body":"Sorry this is not a full working solution, but too much for a comment & hopefully will still be useful to you:\n\n(Assuming you're talking here about SageMaker Notebook Instances rather than SageMaker Studio, same as the whitepaper; that you're trying to install the latest version of Julia from conda rather than v1.0.3 explicitly specified in the whitepaper, currently 1.7.1)\n\nThe first thing to point out is that you should be able to debug this via the notebook's logs: Either click the \"View logs\" link on the notebook's detail page in Amazon SageMaker console, or look in the Amazon CloudWatch console for log group \/aws\/sagemaker\/NotebookInstances, stream {YOUR-NBI-NAME}\/jupyter.log.\n\nFollowing through the whitepaper instructions myself (on an notebook-al2-v1 notebook instance), the errors I saw preventing the kernel from loading were like:\n\nArgumentError: Package IJulia not found in current path:\n- Run `import Pkg; Pkg.add(\"IJulia\")` to install the IJulia package.\n\n\nLooking at ~\/.local\/share\/jupyter\/kernels\/julia-1.7\/kernel.json, I saw the created kernel was defined as follows:\n\n{\n  \"display_name\": \"Julia 1.7.1\",\n  \"argv\": [\n    \"\/home\/ec2-user\/anaconda3\/envs\/julia\/bin\/julia\",\n    \"-i\",\n    \"--color=yes\",\n    \"--project=@.\",\n    \"\/home\/ec2-user\/anaconda3\/envs\/julia\/share\/julia\/packages\/IJulia\/e8kqU\/src\/kernel.jl\",\n    \"{connection_file}\"\n  ],\n  \"language\": \"julia\",\n  \"env\": {},\n  \"interrupt_mode\": \"signal\"\n}\n\nIf we run julia from within the julia conda environment, using IJulia works no problem... However, if you source activate JupyterSystemEnv from the terminal - you can still run \/home\/ec2-user\/anaconda3\/envs\/julia\/bin\/julia from the terminal but the interpreter will not think the IJulia package is installed... I think this is closer to what the above kernel definition is doing (NBI JupyterServer itself runs in this system conda env).\n\nI tried a couple of hacky solutions:\n\nSimply start Julia in JupyterSystemEnv as above and Pkg.add(\"IJulia\") to install it there\nCopy the setup you'll see in the R kernel, \/home\/ec2-user\/.local\/share\/jupyter\/kernels\/ir - where kernel.json points to a run.sh script which first activates the target conda environment and then runs the interpreter\n\nkernel.json (with a different display name & file path to visually confirm JupyterLab has picked the new one up):\n\n{\n  \"display_name\": \"Julia Fix\",\n  \"argv\": [\n    \"\/home\/ec2-user\/.local\/share\/jupyter\/kernels\/julia-fix\/run.sh\",\n    \"{connection_file}\"\n  ],\n  \"language\": \"julia\",\n  \"env\": {},\n  \"interrupt_mode\": \"signal\"\n}\n\n\nrun.sh (remember to chmod +x this file to avoid 500 server errors as Jupyter can't execute it)\n\n#!\/bin\/bash\n\nsource activate julia\n\/home\/ec2-user\/anaconda3\/envs\/julia\/bin\/julia -i --project=@. \/home\/ec2-user\/anaconda3\/envs\/julia\/share\/julia\/packages\/IJulia\/e8kqU\/src\/kernel.jl $1\n\n\nHOWEVER, unfortunately both approaches yield a ZMQStream Invalid Signature error, which looks to me like this open IJulia issue. I see speculation there that Julia isn't playing nice with conda, but am not deep enough with Julia to know if that's really the root cause or how best to mitigate it if so.\n\nMaybe you could install Julia itself outside of conda (conda deactivate in terminal to check you're in base environment) and, if still getting the signature error, use a run.sh kernel script to ensure Jupyter also runs the command outside of conda rather than in the JupyterSystemEnv? Or perhaps there's some other cause for the signature issue that can be resolved while keeping conda environments set up...\n\nYou could also look into the SageMaker Studio Custom Image Samples, instead of Notebook Instances, since Studio kernels are isolated by full container images rather than conda? There is an example image there for Julia (v1.5), although it's not been updated in some time so of course could have some issues of its own.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-07T03:18:25.173Z",
                "Answer_upvote_count":0,
                "Answer_body":"Note that Alex_T found a (hacky) solution to the ZMQStream problem here:\n\nhttps:\/\/www.repost.aws\/questions\/QUM2y8-rjDS1Wp5LUkdLMhyA\/uncaught-exception-in-zmq-stream-callback-trying-to-run-jupyter-notebook-with-julia-kernel-in-sage-maker\n\nand got the Jupyter notebook with Julia kernel to work in SageMaker.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Training Metric logging on SageMaker experiment tracking: how to get time-series metrics with visualisation",
        "Question_creation_time":1648058261935,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDNp9HXW9SCqdadORoXUX9g\/training-metric-logging-on-sage-maker-experiment-tracking-how-to-get-time-series-metrics-with-visualisation",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon CloudWatch"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":702.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I am using the sagemaker python SDK to train a bespoke model. I have defined my metric_definition regexes and passed them to the estimator like:\n\nnum_re = \"([0-9\\\\.]+)(e-?[[01][0-9])?\"\nmetrics = [\n    {\"Name\": \"learning-rate\", \"Regex\": f\"lr: {num_re}\"},\n    {\"Name\": \"training:loss\", \"Regex\": f\"loss: {num_re}\"},\n    # ...\n]\nestimator = Estimator(\n    image_uri=training_image_uri,\n    # ...\n    metric_definitions=metrics,\n    enable_sagemaker_metrics=True,\n)\n\nWhen I run training, these metrics are visible in my logs and I can also see them in SageMaker Studio in Trial Components > Metrics (tab) as a grid of numbers like:\n\nName | Minimum | Maximum | Standard Deviation | Average | Count | Final value\n\nlearning-rate | 8.889 | 8.907 | 0.010392304845413657 | 8.898 | 4 |8.907\n\n...\n\nWhich suggests that the regexes are correctly matching on the logs\n\nHowever, I am not able to visualise any graphs for my metrics. I have tried all of:\n\nSagemaker Studio > Trial components > charts. It is only possible to plot things like learning-rate_min (i.e. a point value not a time-series metric)\nSageMaker aws console > training > training jobs > <select job> > Scroll to Monitor section. Here I can see metrics like CPUUtilization over time but for my metrics there is just an empty graph for each metric that I have defined that says 'No data available'\nSageMaker aws console > training > training jobs > <select job> > Scroll to Monitor section > View algorithm metrics (opens in CloudWatch) > Browse > select metric (e.g. learning-rate and 'Add to Graph' . I filter by the correct time period and go the Graphed metrics (1) tab, even after updating the period to 1 second I am not able to see anything on the graph.\n\nI'm not sure what the issue is here but any help would be much appreciated",
        "Answers":[
            {
                "Answer_creation_time":"2022-03-25T21:34:45.463Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello, Thank you for contacting us.\n\nI understand you are not able to visualize any graphs for your metrics, even though you see them in \"Trial Components > Metrics (tab)\".\n\nSageMaker parses the Cloudwatch logs for your training job and emits metrics from the parsed logs as defined in the metrics_definition. The Cloudwatch logs for your training job depends on your train.py script. For example, If you wish to have metrics per step (or per 100 steps), your script needs to print the metrics per step (or per 100 steps) so that it is there in the Cloudwatch logs. Please see the documentation linked below for more information.\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/training-metrics.html#define-train-metrics\n\nAnd I am providing you with examples from our official AWS Github Repository in [1] and [2] which provides an Entry Script which emits custom metrics for a Hyper-parameter Tuning Job. Please compare them with your script. [1] https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/hyperparameter_tuning\/tensorflow2_mnist [2] https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/hyperparameter_tuning\/tensorflow_mnist\n\nIf you still have difficulties, I recommend to cut a support case and provide more detail about your account information and script\/config. Due to security reason, we cannot discuss account specific issue in the public posts.\n\nThank you.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-29T12:22:40.592Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi - this is the OP:\n\nThanks for your response. Yes I should have stated in the original question: I am logging these metrics to the console every iteration and can see them in view logs in the console. The issue is that I'm not able to:\n\nview the parsed metrics for the period (I can only see the mean\/max\/min\/...)\nget visualisations of these metrics",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Model Spend",
        "Question_creation_time":1592312639000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlNS8ujYmQqePwWS-mgso3Q\/sage-maker-model-spend",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":71.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"If I deploy a SageMaker model, am I incurring hosting charges even while no one is accessing my model?",
        "Answers":[
            {
                "Answer_creation_time":"2020-06-16T13:24:24.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"When you deploy a SageMaker model, it deploys it behind a SageMaker endpoint for real-time inference. You are charged by the second for on-demand ML hosting. Check the model deployment section of each region on the SageMaker Pricing page. In some use cases, you can save on inference cost by hosting several models behind the same endpoint (check this blog post).",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Jumpstart in Sagemaker does not work",
        "Question_creation_time":1664537965056,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOGpNbAjoRASDDw1pQ5F6Fg\/jumpstart-in-sagemaker-does-not-work",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":32.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello,\n\nI am new to SageMaker, I created a domain, and I have an execution role, although when I use Sagemaker studio I cannot see all functionalities, such as the resources button, jumpstart button, etc. I have enabled 'SageMaker Projects and JumpStart '. I am also attaching a photo of what my UI in the studio looks like.\nAny help would be appreciated. Thanks in advance",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker notebook instance in VPC failed to connect to local database",
        "Question_creation_time":1557130614000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUUm8LxKZTzixOCI_IovK1A\/sage-maker-notebook-instance-in-vpc-failed-to-connect-to-local-database",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":260.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"hi there,\n\nI am setting up a jupyter notebook in SageMaker within the VPC and using the jdbc jars to connect to the local database. But it shows the following error messages.\n\": com.microsoft.sqlserver.jdbc.SQLServerException: The TCP\/IP connection to the host xxx.xxx.xxx.xxx, port 21000 has failed. Error: \"connect timed out. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP\/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.\".\"\n\nI used the exactly the same VPC, subnet, security group as what i used in a glue job to extract the data from the local db. While the glue job works but the SageMaker notebook failed. I am sure the firewalls are opened.\nCould anyone tell me how to solve it?\nI also came across the following articles, but i am not sure if it is the root cause.\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options\/",
        "Answers":[
            {
                "Answer_creation_time":"2019-05-06T22:04:11.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\nThe principle here is that there much be network connectivity between the Notebook Instance and the DB Instance, and the security groups on the DB Instance should allow in-bound traffic from the Notebook Instance\n\nOne example of such as setup is\n\nRDS DB Instance is VPC vpc-a and Subnet subnet-b.\n\nSageMaker Notebook is launched in VPC vpc-a, Subnet subnet-b, with Security Group sg-c with DirectIntenetAccess \"Disabled\"\n\nIn the RDS DB Instance's Security Group rules, you can add an Inbound Rule to allow inbound traffic from the SageMaker Notebook security group \"sg-c\"\n-- Type - Protocol - Port Range - Source\n-- MYSQL\/Aurora - TCP - 3306 - sg-c\n\nSample Code:\n\n! pip install mysql-connector\r\n\r\nimport mysql.connector\r\nmydb = mysql.connector.connect(\r\nhost=\"$RDS_ENDPOINT\",\r\nuser=\"$RDS_USERNAME\",\r\npasswd=\"$RDS_PASSWORD\"\r\n)\r\ncursor = mydb.cursor()\r\ncursor.execute(\"SHOW DATABASES\") \n\n\nThanks for using Amazon SageMaker and let us know if there's anything else we can help with!\n\nEdited by: JaipreetS-AWS on May 6, 2019 3:04 PM",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS SageMaker - Upload our own docker image on Amazon SageMaker",
        "Question_creation_time":1643870744946,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYIfUo3-0Qqyr4XunexHJXw\/aws-sage-maker-upload-our-own-docker-image-on-amazon-sage-maker",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Containers"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":111.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am new to AWS SageMaker and i am using this technology for building and training the machine learning models. I have now developed a docker image which contains our custom code for tensorflow. I would like to upload this custom docker image to AWS SageMaker and make use of it.\n\nI have searched various links but could not find proper information on how to upload our own custom docker image.\n\nCan you please suggest me the process of uploading our own docker image to AWS SageMaker?",
        "Answers":[
            {
                "Answer_creation_time":"2022-02-03T08:16:26.646Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi, You can find detailed instructions in the SageMaker Immersion Day. In Lab3 there is a sample called \"Option 2: Bring your own Container\". And the corresponding notebook is here. The content of the sample Docker container is inside this zip file and showcases the decision tree algorithm from scikit-learn package. Hope this helps.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker training instance",
        "Question_creation_time":1660674061917,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4_NA-4cTS8aGkpKawI0qVA\/sagemaker-training-instance",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Model Training"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":61.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a doubt with choosing instance for training job in sagemaker. Is ml.m5.2xlarge with count as 2 and ml.m5.4xlarge are same ?\n\nI would like to know if there is any best practice guide to choose the instance for training in sagemaker.\n\nThank you \ud83d\ude42",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-16T19:10:15.637Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nThe question is quite broad since there will be multiple factors to consider.\n\nHere is the pricing range provided. You can calculate the same and find out if using 2ml.m5.2xlarge with count as 2 OR ml.m5.4xlarge would be better for your use-case.\n\nPricing- https:\/\/aws.amazon.com\/sagemaker\/pricing\/\n\nYou can refer to the training best practice #6 and find out what is the specification and which instance would you choose from Compute\/Memory Optimized image or the Standard one.\n\nHere is a 3rd party link provided that would could be helpful, however it is not verified by AWS and is based on the research that I found on the internet based on what to consider while choosing a training instance.\n\nLink- https:\/\/datachef.co\/blog\/how-to-choose-the-best-training-instance-on-sagemaker\/",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-19T02:01:30.079Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello, to help on the experimentation, you can create a pipeline and run multiple training jobs simultaneously with different instance configurations https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/main\/sagemaker-pipeline-multi-model. Hope this helps!",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to export tresained models to ECR as container image",
        "Question_creation_time":1663258467464,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZHWz5-hpSc-80dEIkuxwQw\/how-to-export-tresained-models-to-ecr-as-container-image",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon Elastic Container Registry (ECR)",
            "Containers",
            "Amazon SageMaker Studio Lab"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":29.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I want to train and build the model in Sagemaker studio and then be able to export the model as a container image to ECR, so I can use the model in external platform by sharing the ECR image to another account where I Can create container with the image from ECR",
        "Answers":[
            {
                "Answer_creation_time":"2022-09-16T23:05:33.114Z",
                "Answer_upvote_count":0,
                "Answer_body":"The models you train in SageMaker are stored in S3 as .tar.gz files that you can use to deploy to an endpoint, or even test locally (extracting the model file from the tar file). If you are using a built-in algorithm, you can share the .tar.gz file to the second account and deploy the model in the second account, since built-in algorithm containers can be accessed from any AWS account.\n\nIf you are using a custom training image (docs here), you can push this image to ECR and allow a second account to pull the image and then use the image with the model that you have trained. However, note that Studio at this time does not support building Docker images out of the box. You can use SageMaker Notebook Instances instead.\n\nI would recommend keeping the model (.tar.gz) and the image (Docker) separate, since you can easily retrain and deploy the newer versions of models without updating the image every single time.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What is the cluster manager in SageMaker Spark Processing?",
        "Question_creation_time":1602770746000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUShPm0t4vR4S8XBKMiAcA6g\/what-is-the-cluster-manager-in-sage-maker-spark-processing",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":81.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"SageMaker Processing can launch multi-instance jobs. What is the underlying cluster manager? Yarn? Mesos? Something custom?",
        "Answers":[
            {
                "Answer_creation_time":"2020-10-15T14:12:23.000Z",
                "Answer_upvote_count":1,
                "Answer_body":"The Spark container uses YARN - for ref the bootstrap script on github: https:\/\/github.com\/aws\/sagemaker-spark-container\/blob\/master\/src\/smspark\/bootstrapper.py and the Dockerfile with hadoop-yarn dependencies",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to specify instance type when training models using SageMaker AutoML Python SDK",
        "Question_creation_time":1642292454744,
        "Question_link":"https:\/\/repost.aws\/questions\/QUClX3PJmFSX-aIaTBlaGidw\/how-to-specify-instance-type-when-training-models-using-sage-maker-auto-ml-python-sdk",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":62.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nIs there a way to specify the instance type when training models using the SageMaker AutoML Python SDK? The AutoML.deploy method takes an instance_type argument, but the AutoML.fit method does not take an instance_type argument.\n\nThanks, Stefan",
        "Answers":[
            {
                "Answer_creation_time":"2022-01-17T00:59:16.641Z",
                "Answer_upvote_count":0,
                "Answer_body":"As I understand it's not currently possible to customize this instance type selection - the underlying CreateAutoMLJob API doesn't offer any instance type controls so there's no way for the Python SDK AutoML class to expose them.\n\nAlthough it might be nice to have some more user control over this in future, it's worth mentioning that the options might be non-trivial: For example different stages of the autoML process encapsulated within fit() (like pre-processing, training, explainability analysis) might have different infrastructure needs, and different tested algorithms running in parallel might also have different optimal choices for a given dataset.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Built-in Algorithms",
        "Question_creation_time":1652686627960,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDkYruiibS9S05bzFSkLaxg\/sagemaker-built-in-algorithms",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Service Catalog"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":76.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I am exploring the Sagemaker Built-in algorithms, and I am curious to learn more about the details of the algorithms. However, I am surprised that it is hard to find any references for the research background and implementation details in the numerous documents and tutorials for particular algorithms. If such information exists somewhere, I would highly appreciate a pointer. Thanks a lot in advance!",
        "Answers":[
            {
                "Answer_creation_time":"2022-05-16T08:11:20.111Z",
                "Answer_upvote_count":0,
                "Answer_body":"thanks for your interest in the built-in algorithms! You can find research papers in the documentation of many of them. And documentation page has a section \"how it works\" explaining the science of every algorithm. For example:\n\nBlazingText: BlazingText: Scaling and Accelerating Word2Vec using Multiple GPUs, Gupta et Khare\nDeepAR DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks, Salinas et al.\nFactorization Machines\nIP Insights\nKMeans\nKNN\nLDA\nLinear Learner\nNTM\nObject2Vec\nObject Detection (it's an SSD model)\nPCA\nRandom Cut Forest: Robust Random Cut Forest Based Anomaly Detection On Streams, Guha et al\nSemantic Segmentation\nSeq2seq\nXGBoost",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unauthorized AWS account racked up charges on stolen credit card.",
        "Question_creation_time":1649524594036,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhV-lkkYyS1qaYFvsoPYiWg\/unauthorized-aws-account-racked-up-charges-on-stolen-credit-card",
        "Question_topic":[
            "Compute",
            "Machine Learning & AI",
            "AWS Well-Architected Framework",
            "Security, Identity, & Compliance"
        ],
        "Question_tag":[
            "Amazon Lightsail",
            "Amazon SageMaker",
            "Security",
            "Support Case",
            "Shared Responsibility Model"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":280.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"My mother was automatically signed up for an AWS account or someone used her credentials to sign up. She did not know that she had been signed up, and it sat unused for 3 years. Last month, she got an email from AWS for \"unusual activity\" and she asked me to help her look into it. Someone racked up $800+ in charges in 10 days for AWS services she has never heard of, let alone used (SageMaker, LightSail were among the services). The card on the AWS account is a credit card that was stolen years ago and has since been cancelled. So when AWS tried to charge the card, it didn't go through.\n\nMy experience with AWS customer service has been unhelpful so far. Mom changed her AWS password in time so we could get into the account and contact support. I deleted the instances so that the services incurring charges are now stopped. But now AWS is telling me to put in a \"valid payment method\" or else they will not review the fraudulent bill. They also said that I have to set up additional AWS services (Cost Management, Amazon Cloud Watch, Cloud Trail, WAF, security services) before they'll review the bill. I have clearly explained to them that this entire account is unauthorized and we want to close it ASAP, so adding further services and a payment method doesn't make sense.\n\nWhy am I being told to use more AWS services when my goal is to use zero? Why do I have to set up \"preventative services\" when the issue I'm trying to resolve is a PAST issue of fraud? They also asked me to write back and confirm that we have read and understood the AWS Customer Agreement and shared responsibility model.\" Of course we haven't, because we didn't even know the account existed!\n\nAny advice or input into this situation? It's extremely frustrating to be told that AWS won't even look into the issue unless I set up these additional AWS services and give them a payment method. This is a clear case of identity fraud. We want this account shut down.\n\nSupport Case # is xxxxxxxxxx.\n\nEdit- removed case ID -Ann D",
        "Answers":[
            {
                "Answer_creation_time":"2022-04-10T21:14:09.639Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nFirst, I want to apologize for the wait and the frustration this situation has caused.\n\nWhile we can\u2019t discuss account or case details, via this platform, I can assure you that the issue is currently under investigation. Our team takes these matters seriously and will update you with any action needed\/taken.\n\nPlease continue to address any further concerns or questions, with our teams, via your case.\n\nThank you,\n\nRandi S.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Which connection method when using SageMaker Notebook through VPC Interface Endpoint?",
        "Question_creation_time":1541494962000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5z-7bQ9zQOi_NrVlHy_5oA\/which-connection-method-when-using-sage-maker-notebook-through-vpc-interface-endpoint",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":280.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi, I see in this page of documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-interface-endpoint.html that:\n\n\"You can connect to your notebook instance from your VPC through an interface endpoint in your Virtual Private Cloud (VPC) instead of connecting over the internet. When you use a VPC interface endpoint, communication between your VPC and the notebook instance is conducted entirely and securely within the AWS network.\"\n\nHow would customer interact on their laptop with the UI of a notebook instance sitting in a VPC?",
        "Answers":[
            {
                "Answer_creation_time":"2018-11-06T15:02:57.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"If you are trying to access from within VPC, you'll have a direct connection. Otherwise, you'll need a configuration in place, such as Amazon VPN or AWS Direct Connect, to connect to your notebooks. Here is the blog post where we tried to explain how to set up AWS PrivateLink for Amazon SageMaker notebooks: https:\/\/aws.amazon.com\/blogs\/machine-learning\/direct-access-to-amazon-sagemaker-notebooks-from-amazon-vpc-by-using-an-aws-privatelink-endpoint\/",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to pass null\/Nan values into the dataframe passed into batch transform",
        "Question_creation_time":1549531218000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUtKq2ZetyTUmvDwS7i3ot2Q\/how-to-pass-null-nan-values-into-the-dataframe-passed-into-batch-transform",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":92.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to make inference using my Xgboost model on a dataset which has NaN values, now inherently Xgboost handles the NaN values, it does not throw any error while training with NaN values whereas the batch transform job gives the error ''could not convert string to float'' when it encounters NaN values into the dataset that is to be transformed.\nCan anyone help me as to how can I pass NaN values into my input dataset for the batch transform job?\nThanks a ton!",
        "Answers":[
            {
                "Answer_creation_time":"2019-03-12T17:50:37.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi bharat-patidar,\n\nThanks for your interest in Amazon SageMaker XGBoost. We recently made a change to address empty values in the dataset. Now you should be able to pass empty strings or \"NaN\" strings with CSV data. Can you please verify and let us know if the change addresses the issue at your end?\n\nThanks!\nRegards,\nAmazon SageMaker Team",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Export Autopilot model to GovCloud region",
        "Question_creation_time":1657645831488,
        "Question_link":"https:\/\/repost.aws\/questions\/QUV6_OvmWMRjiuQIx82Z4_Eg\/export-autopilot-model-to-gov-cloud-region",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Autopilot"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":41.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi As AWS Sagemaker autopilot is not available in GovCloud region, is it possible to export a model trained on non-GovCloud environment in GovCloud environment.\n\nWhat I have done:\n\nRan Autopilot on non-GovCloud environment\nI was able to download the output model.joblib(preprocessing) and xgboost models from output bucket in S3 bucket\nI was not able to load the model.joblib preprocessing model since the sagemaker-sklearn-extention gives symbol not found error(https:\/\/issuehint.com\/issue\/awslabs\/ml-io\/28)\n\nThanks for your help and insights in exporting the model to GovCloud environment.",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"CloudFormation with SageMaker LifeCycleConfig without leaving the instance running",
        "Question_creation_time":1539775195000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU43NLxohAQvmSL3aH-KpPaw\/cloud-formation-with-sage-maker-life-cycle-config-without-leaving-the-instance-running",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":306.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I have a use case with SageMaker in which I want to create a notebook instance using CloudFormation. I have some initialization to do at creation time (clone a github repo, etc.). That all works fine. The only problem is that I would like to do this ahead of time in a set of accounts, and there doesn't appear to be any way to leave the newly-created instance in a Stopped state. A property in the CFT would be helpful in this regard.\n\nI tried using the aws cli to stop the instance from the lifecycle create script, but that fails as shown in the resulting CloudWatch logs:\n\nAn error occurred (ValidationException) when calling the StopNotebookInstance operation: Status (Pending) not in ([InService]). Unable to transition to (Stopping) for Notebook Instance (arn:aws:sagemaker:us-east-1:147561847539:notebook-instance\/birdclassificationworkshop).\n\n\n\nInterestingly, when I interactively open a notebook instance, open a terminal in the instance, and execute a \"stop-notebook-instance\" command, SageMaker is happy to oblige. I would have thought it would let me do the same in the lifecycle config. Unfortunately, SageMaker still has the notebook in the Pending state at that point, so \"stop\" is not permitted.\n\nAre there other hooks or creative options anyone can provide for me?",
        "Answers":[
            {
                "Answer_creation_time":"2018-10-17T11:53:11.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"One solutions will be to create a CFN custom resource backed by lambda. You can configure to run this resource only when the notebook resource completed. and use the lambda function to stop the notebook using one of our SDKs.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Receiving consistent AccessDenied errors",
        "Question_creation_time":1606179025000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQ3KwMxUsREup-lSqlA44Hg\/receiving-consistent-access-denied-errors",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":55.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to use SageMaker Notebook Instances, but consistently receive AccessDenied errors for commands that my IAM role should have access to (and for commands that worked the last time I tried several weeks ago). For example:\n\naws s3 ls results in An error occurred (AccessDenied) when calling the ListBuckets operation: Access Denied despite my role having the AmazonS3FullAccess policy attached.\n\nAlso aws ecr describe-repositories --repository-names \"sagemaker-decision-trees\" results in An error occurred (AccessDeniedException) when calling the DescribeRepositories operation: User: arn:aws:sts::XXXXXXXXXX:assumed-role\/AmazonSageMaker-ExecutionRole-20201123T151452\/SageMaker is not authorized to perform: ecr:DescribeRepositories on resource: arn:aws:ecr:us-east-2:XXXXXXXXXX:repository\/sagemaker-decision-trees with an explicit deny despite my role having the AmazonEC2ContainerRegistryFullAccess policy attached.\n\nOne thing that seems new is that \"SageMaker\" is appended to my user ARN. I can't remember seeing errors with this appended before.\n\nNote: I've replicated these errors with several combinations of configurations:\n\na new IAM role (which I created in the SageMaker console to have AmazonSageMakerFullAccess to any S3 bucket)\nfresh notebook instance\nwith (and without) a VPC\nAlso, these commands all work when run outside of a notebook instance (i.e. when run locally from my laptop).\n\nI'm guessing there's some problem with my account setup, but not sure what to try next.\nThanks.\n\nEdited by: DJAIndeed on Nov 24, 2020 8:35 AM",
        "Answers":[
            {
                "Answer_creation_time":"2020-12-22T22:39:21.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I am sorry to hear that you are not able to access required services from Sagemaker.\nYou can run below command to check the execution role that is getting used and then verify that required permission are present.\n\nimport sagemaker  \nsagemaker.get_execution_role()  \n\n\nOther useful links -\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html\nhttps:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/s3-troubleshoot-403\/\n\nEdited by: amitsur on Dec 22, 2020 2:41 PM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-23T00:15:18.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks, @amitsur. We had confirmed that the SageMaker Notebook Instance was using the desired execution role and that that role had the required permissions. The issue appears to have resolved itself though, since we're no longer receiving these errors. So it must have been a configuration elsewhere? We appreciate the help anyway.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker requirements.txt unable to find certain packages",
        "Question_creation_time":1649797399439,
        "Question_link":"https:\/\/repost.aws\/questions\/QUl7yzFGwfSEauI7yesYeXiw\/sagemaker-requirements-txt-unable-to-find-certain-packages",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":181.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\n\nI'm trying to run a sagemaker job on p3.2xlarge instance using the PyTorch estimator. My script has dependencies on several packages (possibly not pre-installed on the instance) for which I have a requirements.txt file. After loading the instance, it installs several packages but the job fails with following error:\n\nsagemaker-training-toolkit ERROR InstallRequirementsError: Command \"\/opt\/conda\/bin\/python3.6 -m pip install -r requirements.txt\" ERROR: Could not find a version that satisfies the requirement scikit_image==0.18.3 ERROR: No matching distribution found for scikit_image==0.18.3\n\nIt fails for other packages too, like h5py, numpy etc.\n\nAny help is greatly appreciated.\n\nThank you, Aditya",
        "Answers":[
            {
                "Answer_creation_time":"2022-04-20T16:55:57.266Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nThank you for using Amazon SageMaker.\n\nUsually this kind of error is occurred due version mismatch or missing dependencies. As you have mentioned that it worked locally on your system without any issue, we would like to dive deep into this issue for which we will required your script and requirement.txt file and sharing those files with us will help us identify what is the root cause of this issue and help you overcome this issue. Sharing any files on this medium is not encourage, so I recommend you to open a case with SageMaker Premium Support team so that you can share above mentioned details securely.\n\nOpen a support case with AWS using the link:\n\nhttps:\/\/console.aws.amazon.com\/support\/home?#\/case\/create",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to load large amount of data from S3 onto Sagemaker?",
        "Question_creation_time":1641932571368,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4m2DyyJQSSCL1QqclXS6ZA\/how-to-load-large-amount-of-data-from-s-3-onto-sagemaker",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon S3 Glacier",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":1033.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a notebook on Sagemaker Studio, I want to read data from S3, I am using the code bellow:\n\ns3_client = boto3.client('s3') bucket = 'bucket_name' data_key = 'file_key.csv' obj = s3_client.get_object(Bucket=bucket, Key=data_key) df = pd.read_csv(io.BytesIO(obj['Body'].read())) df.head()\n\nIt works for small datasets but fails along the way with the dataset I'm trying to load which is 15GB. I changed the instance to ml.g4dn.xlarge ( accelerated computing, 4vCPU + 16GiB + 1 GPU), still fails. what am I missing here? Is is about the instance type, or about the code? What is the best way to import large datasets from S3 to sagemaker?\n\nThank you",
        "Answers":[
            {
                "Answer_creation_time":"2022-01-12T15:20:57.834Z",
                "Answer_upvote_count":1,
                "Answer_body":"What is the need to load large dataset onto the notebook? If you are pre-processing then there are better ways to do this - Sagemaker Spark processing job, or have your own spark cluster and process or even possibly Glue. If you are exploring the data, you should just use a smaller data set. If you are loading the data for training, Sagemaker supports different modes to read the data and data doesnt have to be downloaded on the notebook.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-11T21:00:32.086Z",
                "Answer_upvote_count":1,
                "Answer_body":"If you want to download the data onto the notebook so that you don't have to load it from S3 each time you want it in Pandas, you should confirm that the volume size is sufficient for the data. It is set to 5 GB by default, which lines up with your scenario.\n\nTo change this, you'll need to edit your notebook instance, expand the \"additional configuration\" drop down and look for the \"Volume size in GB\" field.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-11T21:31:44.046Z",
                "Answer_upvote_count":0,
                "Answer_body":"I have used a much simpler approach to reading a single data file into S3 using pandas. For example:\n\nimport pandas as pd\n\nbucket = 'bucket_name'\n\ndata_key = 'file_key.csv'\n\ndf = pd.read_csv( 's3:\/\/{}\/{}'.format(bucket,data_key) )\n\ndf.head()\n\nMaybe this will perform better?",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Giving weights to event types in amazon personalize",
        "Question_creation_time":1639825094332,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSogRKFlfRzC5b8afIwPybQ\/giving-weights-to-event-types-in-amazon-personalize",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Personalize"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":174.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"For the VIDEO_ON_DEMAND domain, some use cases include multiple event types. For example, the 'Top picks for you' use case includes two event types 'watch' and 'click'. Is 'watch' given more weight than 'click' when training the model? In general, when there is more than one event type, do domain recommenders give more weight to some event types?\n\nIn our use case, we have a platform that recommends video content. However, we have multiple event types, and some events need to be given more weight than others. Below is the list of our event types in the order of their importance:\n\nSHARE > LIKE > WATCH_COMPLETE > WATCH_PARTIAL > STARTED > SKIP\n\nSo when training the model, we would want 'SHARE' to have more weight than 'LIKE', and 'LIKE' to have more weight than 'WATCH_COMPLETE' and so on.\n\nI was looking into custom solutions. It looks like there is no way to give weights when using Personalize's custom solutions as mentioned in this post...\n\nSo when using Amazon Personalize, should we use domain recommenders or build custom solutions for our use case?\n\n**If we cannot give weights to different event types using Personalize, then what are alternatives? **Should we use Amazon SageMaker and build models from scratch? Open to any and all suggestions.\n\nThank you!",
        "Answers":[
            {
                "Answer_creation_time":"2021-12-21T19:25:06.908Z",
                "Answer_upvote_count":0,
                "Answer_body":"The VOD recommender for \"Top picks for you\" uses the same underlying HRNN-based algorithm noted in the StackOverflow answer you linked. Therefore the answer still applies with respect to weighting event types. That is, Personalize does not support weighting specific event types or specific interactions more than others. Instead, the Top picks for you recommender (as well as the underlying user-personalization recipe) builds sequence models from user sessions which are used to learn each user's interest based on a sequence of events rather than specific event types.\n\nGiven your event taxonomy, including interactions for SHARE, LIKE, and WATCH_COMPLETE in your interactions dataset are good choices since they indicate positive intent by the user. It may make sense to include WATCH_PARTIAL interactions as well (particularly if they represent the user watching the majority of the content, there is not a subsequent WATCH_COMPLETE for the user for the video, and\/or you do not have a sufficient number of WATCH_COMPLETE events across your user base). Otherwise, use WATCH_COMPLETE. If using one of the VOD recommenders, you will need to map your WATCH_COMPLETE events to the required Watch type and you could map the STARTED events to View. The SKIP events could be used as impressions if they can be correlated to a WATCH_COMPLETE or WATCH_PARTIAL event for a video that the user eventually watched (e.g., the user skips through the first 3 videos in a sequence and watches the 4th video could be expressed in a PutEvents call with the 4 videos as impressions and the 4th video as the ItemId that the user Watched).\n\nI suggest not basing the choice of whether to use Personalize or a custom SageMaker model on whether event type weighting is supported. Rather, the choice should be based on the approach that drives the most impact to your business metric (CTR, watch time, etc) with online testing.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to use categorical data without converting?",
        "Question_creation_time":1638858572453,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_mbjTldXQY2DHVM7-rMb_g\/how-to-use-categorical-data-without-converting",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":69.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to develop a model in SageMaker, but my data is contained categorical type of data, and I would not want to convert it. In the old Machine Learning, I can use back the same data and train the model without any issue. However, when I tried the built-in algorithm, I got the error message that wanted me to convert the data. Is there anyway to do the same as the old Machine Learning without converting? Thank you.",
        "Answers":[
            {
                "Answer_creation_time":"2021-12-07T08:29:10.669Z",
                "Answer_upvote_count":0,
                "Answer_body":"Have you tried SageMaker Autopilot? https:\/\/aws.amazon.com\/sagemaker\/autopilot\/",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-13T16:33:38.957Z",
                "Answer_upvote_count":0,
                "Answer_body":"What kind of ML problem are you working with? If it's regression or classification, you could try SageMaker Autopilot to avoid performing feature engineering steps yourself. If you're using Jupyter notebooks on Sagemaker Studio to do your model training, adding a preprocessing step to perform categorical encoding isn't too difficult. We have plenty of examples on Github.\n\nLet us know in more detail what kind of algorithm are you dealing with and how you're doing ML on AWS today and I can give you some pointers.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-07T17:22:15.133Z",
                "Answer_upvote_count":0,
                "Answer_body":"In addition to SageMaker Autopilot other folks mentioned, I'd suggest you to try SageMaker Canvas:\n\nhttps:\/\/aws.amazon.com\/sagemaker\/canvas\/\nhttps:\/\/aws.amazon.com\/blogs\/aws\/announcing-amazon-sagemaker-canvas-a-visual-no-code-machine-learning-capability-for-business-analysts\/\n\nSageMaker Canvas offers a similar yet more powerful experience when compared to the previous-generation Amazon Machine Learning service.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-07T06:36:23.653Z",
                "Answer_upvote_count":0,
                "Answer_body":"There are a couple of approaches that might be suitable, feel free to pick the one that fits your use case best.\n\nSageMaker Canvas supports the ability to do machine learning on datasets in a no-code fashion, and performs this pre-processing for you. As described in that page, Canvas is suited for business analysts who may not have any coding experience.\n\nSageMaker AutoPilot allows for AutoML where AutoPilot takes care of feature engineering and hyperparameter optimization. Autopilot is suited for developers who want to perform AutoML without doing the pre-processing themselves.\n\n*You can preprocess the input prior to using prebuilt SageMaker algorithms, a basic example of that is shown here in the section \"Data_Transformation\". This approach is suited for data scientists who want fined-grained control over the transformation steps.\n\nIn your context, it seems to me that Canvas or AutoPilot might be most appropriate.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"greengrass ml component",
        "Question_creation_time":1666267168133,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4YAAy55MQQChcPc0JhyWWA\/greengrass-ml-component",
        "Question_topic":[
            "Internet of Things (IoT)",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS IoT Greengrass",
            "AWS IoT Core",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Internet of Things (IoT)"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":38.0,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello I am studying Greengrass machine learning components. I have a question. I'm going to distribute Greengrass custom machine learning components to Raspberry Pi now. The artifacts associated with the recipe in this component contain inference codes uploaded to S3. And this inference code has a function of determining who is by photographing the user's face in real time. I'm curious. If the components are distributed well in Raspberry Pi, will the inference code continue to run? Or, do I have to deploy components every time I try to run an inference? Thank you.",
        "Answers":[

        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Multi Model endpoint creation fails while creating for model built on container sagemaker-scikit-learn:0.23-1-cpu-py3",
        "Question_creation_time":1653574428837,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHZiKPwmxRyy0C0Nc0ONwuQ\/sage-maker-multi-model-endpoint-creation-fails-while-creating-for-model-built-on-container-sagemaker-scikit-learn-0-23-1-cpu-py-3",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":98.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am working on a use-case where I am using SageMaker multi-model endpoint for model inference and the models are trained using Databricks MLFlow platform. When I tried deploying a model trained from Databricks MLFlow platform on a single endpoint on SageMaker then it worked fine but the creation of multi-model endpoint for 'sagemaker-scikit-learn:0.23-1-cpu-py3' container is failed with the following error:\n\nCode Snippet::>> name = \"sample-mme\"\n\nsagemaker_client = boto3.client('sagemaker')\n\nmodel_path = \"s3:\/\/test-bucket\/multi-models\"\n\nexecution_role_arn = \"IAM:\/\/sample-role\"\n\nBASE_IMAGE = image_uris.retrieve( region=region, framework=\"sklearn\",version='0.23-1',image_scope='inference' )\n\ncontainer = { 'Image': BASE_IMAGE, 'ModelDataUrl': model_path, 'Mode': 'MultiModel', 'MultiModelConfig': { 'ModelCacheSetting': 'Enabled' } }\n\nmodel_response = sagemaker_client.create_model( ModelName=name, ExecutionRoleArn=execution_role_arn, Containers=[container] )\n\nconfig_response = sagemaker_client.create_endpoint_config( EndpointConfigName=f'{name}-config', ProductionVariants=[ { 'InstanceType': instance_type, 'InitialInstanceCount': instance_count, 'InitialVariantWeight': 1, 'ModelName': name, 'VariantName': 'AllTraffic' } ] )\n\nresponse = sagemaker_client.create_endpoint( EndpointName=f'{name}-endpoint', EndpointConfigName=f'{name}-config' )\n\nEndpoint creation is taking a lot if time and failing with the following error message :\n\nsagemaker_containers._errors.ImportModuleError: 'NoneType' object has no attribute 'startswith'\n\nPlease provide me with some help to fix this.\n\nAlso, my understanding is that I can train a model on the DataBricks MLFlow platform using sklearn libraries, and then I can store model artifacts \"model.tar.gz\" under the s3 directory for storing all multi-models. Now I can create a multi-model endpoint in SageMaker using the same s3 directory as the model path and using the above code. Once the endpoint is ready, I can do inference by providing the target model. Please let me know if my understanding is correct and share any relevant documents to follow for my use case.",
        "Answers":[
            {
                "Answer_creation_time":"2022-05-31T20:01:20.662Z",
                "Answer_upvote_count":0,
                "Answer_body":"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/multi_model_sklearn_home_value\/sklearn_multi_model_endpoint_home_value.ipynb https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/advanced_functionality\/kmeans_bring_your_own_model\/kmeans_bring_your_own_model.html\n\nThe above notebook shows how to seed a pre-existing model in an already built container. This functionality could be replicated with other Amazon SageMaker Algorithms, as well as the TensorFlow and MXNet containers. Although this is certainly an easy method to bring your own model, it is not likely to provide the flexibility of a bringing your own scoring container. Please refer to other example notebooks which show how to dockerize your own training and scoring container which could be modified appropriately to your use case.\n\nIn general it is recommended to Bring your Own docker container along with your custom model, SageMaker Inference Toolkit is a library that bootstraps MMS in a way that is compatible with SageMaker multi-model endpoints, while still allowing you to tweak important performance parameters, such as the number of workers per model. The inference container in this example uses the Inference Toolkit to start MMS which can be seen in the container\/dockerd-entrypoint.py file.\n\nhttps:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.html#Upload-model-artifacts-to-S3\n\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\n\nIn order to deep-dive further I would request you to open a support ticket with the aws premium support for further investigation in to the cloudwatch logs and the specific resource of the endpoint.\n\nIf you still have difficulties, I recommend to cut a support case and provide more detail about your account information and cloudwatch logs. Due to security reason, we cannot discuss account specific issue in the public posts.\n\nThank you",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"GC overhead limit exceeded",
        "Question_creation_time":1654390632353,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDc_WeDqcTjitN1bkIJRosg\/gc-overhead-limit-exceeded",
        "Question_topic":[
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "High Performance Compute",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":87.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a modest size dataset, and I am running Jupyter Notebook in Sagemaker (instance type ml.c5.xlarge with 200G instance size). I receive the error message \" GC overhead limit exceeded\" Everything ran fine with small data size. BTW, I need to go through the dataframe one row at a time using df.collect(), which seems t be an expensive operation... Would you suggest another way of accomplishing this? I would appreciate your kind help.",
        "Answers":[
            {
                "Answer_creation_time":"2022-06-08T07:40:20.941Z",
                "Answer_upvote_count":0,
                "Answer_body":"The GC overhead limit exceeded error indicates that the JVM spent a lot of time on garbage collection but recovered very little memory, so it throws this error to let you know that your program is not making much progress but wasting time on doing useless garbage collection task. Iterating through the dataframe might be the problem, because you might be creating a lot of temporary objects when you go through each line, and they couldn't be garbage collected. What is the framework that you are using? And what are you trying to do by going through the dataframe row-by-row? Maybe you can think about processing multiple lines in a batch? For example using some vectorization or matrix operation as georgios_s suggested in the comment.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Jumpstart 'Explain Credit Decisions' fails to deploy",
        "Question_creation_time":1660818001611,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMx_45pb_TLOfMOZINMG2JA\/sagemaker-jumpstart-explain-credit-decisions-fails-to-deploy",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Glue",
            "Amazon SageMaker JumpStart"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":85.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Im trying to use the Sagemaker Jumpstart 'Explain Credit Decisions' via the Studio Jumpstart menu. However, everything works as instructed until it hits the 'glue.wait_for_workflow_finished(config.GLUE_WORKFLOW, glue_run_id)' step in the datasets notebook.\n\nThis produces a \"failed to execute with exception Internal service error: Invalid Input Provided\" (error in the Glue console) and falls over on the job part of the glue job.\n\nDoes anyone have any ideas? This is as much information as is available in the console logs.",
        "Answers":[
            {
                "Answer_creation_time":"2022-09-01T23:31:00.426Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello, Hope you are doing well.\n\nTo unblock the issue, Please find the below instruction.\n\nIf you are using the guleRole: AmazonSageMakerServiceCatalogProductsGlueRole. Nothing need to be done. It should be good to execute the notebook with the latest version.\n\nIf you are using the role: AmazonSageMakerServiceCatalogProductsUseRole. Please follow the instruction below:\n\nAttach the following inline policy to AmazonSageMakerServiceCatalogProductsUseRole and AmazonSageMakerServiceCatalogProductsGlueRole:\n\n  Navigate to IAM Console, find the role named AmazonSageMakerServiceCatalogProductsUseRole.\n  Click on the role -> click \"Add Permissions\" -> Create Inline Policy\n  In the pop up menu, for \"Service\", choose \"Glue\"\n  In \"Actions\", choose \"Read\" permission, and select \"GetUserDefinedFunctions\".\n  In \"Resources\", choose \"All Resources\" -> Review Policy\n  Name the inline policy as \"glueGetUserDefinedFunctions\" and then click \"Create Policy\".\n  Repeat the above steps to add the same inline policy for AmazonSageMakerServiceCatalogProductsGlueRole\n  Return to SageMaker Solutions Notebook and re-run the Glue workflow.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Code running slow on Sagemaker notebook instance for the first time it runs",
        "Question_creation_time":1591811233000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeQJN4BFCTsikAct_m9BeZw\/code-running-slow-on-sagemaker-notebook-instance-for-the-first-time-it-runs",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":615.0,
        "Question_answer_count":7,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello!\n\nI've an issue running the code on SageMaker. I am running my code on SageMaker, which runs my code slowly for the first time, but runs with proper speed, the second time around (I guess there's something getting stored in the cache). Few days back, it was running with the same speed all the time. Whatever I run, be it a model (The model which took just 5 minutes for one epoch when it worked fine estimates 3 hours of running time) \/ just a code reading the data present in my files, it runs too slow. What could be a possible solution for this? I tried changing the notebook instance types as well, but in vain. I've been struggling for 2 days. It'll be great if someone could help me out a bit soon so that I progress ahead in my project. Thanks in advance!\n\nEdited by: vbsrinivasan on Jun 10, 2020 10:47 AM",
        "Answers":[
            {
                "Answer_creation_time":"2020-06-26T10:12:46.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Can confirm the speed issues. Migrated yesterday to Sagemaker and code runs very slowly the first time, the second time is way faster but is slowing down again mid training. With the same code and training data, training on a K80 is way slower than on Colabs K80.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-08-18T10:07:41.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yeah, you are right. Also, there's one update. Like, whenever I turn off the notebook instance and turn on once again, the code runs very very slowly. But, after running the code once, if I don't turn off the notebook, it runs faster. But, this incurs a lot of cost for me. It would be great if someone from AWS or an expert responds to this!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-06-13T23:33:54.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Can you confirm if this is the notebook taking time to spin up the kernel, then load the libraries (at the start of your script presumably) or if this is all cells taking longer to run?\n\nEdited by: MikeChambers on Jun 13, 2020 4:20 AM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-08-18T04:56:07.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello!\n\nThe notebook is taking time to run every cell. Not just libraries and stuff.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-06-11T11:25:54.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Exact same experience here. Seeing substantial variation in runtimes between instance restarts. Running the exact same code can take up to a factor of 3 longer (regardless of whether this is just I\/O, model training or something else entirely). I had originally attributed this to slow EBS I\/O (which by experience has been patchy in the past) but doesn't seem to be related. Real showstopper for sagemaker at this point.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-06-13T11:20:32.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Having the same problem too. But for me it is mainly disk I\/O. So every time I stop and restart the notebook instance, I need to re-download the data even though they are sitting right there on disk, because if I don't, then it takes a insane amount of time to load the data (even slower than re-download the data and load them). Quite annoying but have not idea how to fix it.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-06-12T17:25:56.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yeah, I've narrowed it down to disk I\/O. Extremely slow on first read -- as if the files aren't on the EBS volume but downloaded from elsewhere. Moving away from Sagemaker NBs now for interactive work",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker python sdk installation troubles",
        "Question_creation_time":1597398744000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZe9xubsHTuyRPCUGXvi40Q\/sagemaker-python-sdk-installation-troubles",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":145.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nI've tried following a tutorial to use AWS sagemaker in script mode from my local linux VM, but I can't even get the basics working.\n\nSteps:\n\n\u279c ~> python3 --version\nPython 3.6.9\n\n\u279c ~> pip3 install sagemaker\n...\nSuccessfully installed boto3-1.14.42 botocore-1.17.42 importlib-metadata-1.7.0 packaging-20.4 protobuf3-to\n-dict-0.1.5 s3transfer-0.3.3 sagemaker-2.3.0 smdebug-rulesconfig-0.1.4 zipp-3.1.0\n\n\u279c ~> cat sagemaker.py\nimport sagemaker\nimport boto3\n\nsess = sagemaker.Session()\n\n\u279c > python3 sagemaker.py\nTraceback (most recent call last):\nFile \"sagemaker.py\", line 1, in <module>\nimport sagemaker\nFile \"\/sagemaker.py\", line 4, in <module>\nsess = sagemaker.Session()\nAttributeError: module 'sagemaker' has no attribute 'Session'\n\nI also have the aws cli (version 2) installed, and configured using IAM credentials that have full rights, so that's not related.\n\nWhat is the problem with my python sdk install? TIA",
        "Answers":[
            {
                "Answer_creation_time":"2020-08-14T17:11:58.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"You should name your script something else than sagemaker.py, since python will look in the current directory first for a module when doing an import, so the import sagemaker will not import the Sagemaker SDK, but your script.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2020-08-15T12:15:01.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Well, that one is for my list of top 10 most stupid programming things I've ever done... Thanks!",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"\"Failure reason Image size 12704675783 is greater than supported size 10737418240\" when creating serverless endpoint in SageMaker.",
        "Question_creation_time":1657243407002,
        "Question_link":"https:\/\/repost.aws\/questions\/QU90699ONgQD2t2HUKzm9AUA\/failure-reason-image-size-12704675783-is-greater-than-supported-size-10737418240-when-creating-serverless-endpoint-in-sage-maker",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":220.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"How to reproduce the error: We want to run Python Inference in SageMaker. Because our model is pre-trained out side the SageMaker and has some special logic, so we need to create customer image. We see the document https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/prebuilt-containers-extend.html#prebuilt-containers-extend-tutorial We use the 763104351884.dkr.ecr.us-east-1.amazonaws.com\/pytorch-inference:1.11.0-gpu-py38-cu113-ubuntu20.04-sagemaker to be the base image. We wrote a dockerfile and use \"docker build\" to create a new image. Also, use \"docker push\" to push new image to Amazon ECR. We pushed it to 935877503070.dkr.ecr.us-east-1.amazonaws.com\/pytorch-inference:testaisage Then, we follow the document: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html Then, we went to SageMaker console https:\/\/us-east-1.console.aws.amazon.com\/sagemaker\/home?region=us-east-1#\/models We created model. We input the \"935877503070.dkr.ecr.us-east-1.amazonaws.com\/pytorch-inference:testaisage\" of our new image to \"Location of inference code image\". Then, we create Endpoint configuration. Then, we create Endpoint. But the Endpoint shows \"Failure reason Image size 12704675783 is greater than supported size 10737418240\".",
        "Answers":[
            {
                "Answer_creation_time":"2022-07-08T09:20:53.961Z",
                "Answer_upvote_count":1,
                "Answer_body":"As you're seeing in the error message, SageMaker Serverless Inference imposes a limit of 10GiB (10737418240 bytes) on your deployed container size - which helps deliver quality of service for considerations like cold-start time. From a quick look I didn't see this mentioned in the SageMaker serverless docs, but as mentioned in the launch blog post, SageMaker Serverless is backed by AWS Lambda and the AWS Lambda quotas page lists the limit.\n\nSo to solve the issue (and still use SageMaker Serverless Inference), you'll need to look at optimizing that container image size by removing any unnecessary bloat (need to find almost 2GiB from the number you posted).\n\nSome suggestions on that:\n\nAre you currently building your actual model in to the image itself? The typical pattern on SageMaker is to host a model.tar.gz tarball on S3, which gets downloaded and extracted into your container at runtime. For large language models and similar, this can be a big size saving (although of course, optimizing overall S3+image size can still help give you the best start-up times). The contents of this file are flexible so you could offload multiple artifacts.\nI saw you're using the standard PyTorch DLC as a base... Are you replacing the entire serving stack, or slotting your custom logic into the one the DLC provides? The stack already provided in the PyTorch container already provides (see docs here) customization to model loading via model_fn, input de-serialization via input_fn, output serialization via output_fn, and actual prediction via predict_fn. The APIs between these user-defined functions are very flexible (for example can return pretty much whatever you like from model_fn, so long as predict_fn knows how to use it) - so I find in practice that it can support even complex requirements like custom request formats, pipelining multiple models together, advanced pre-processing, etc. I've seen some customers go straight to building custom serving stacks (and installing their dependencies alongside the existing e.g. TorchServe in the image) before realising that the pre-built could already support what they needed. Again, this inference.py script would live in your model.tar.gz.\nGeneral non-SageMaker-specific container image optimization guidelines would still apply: Like for e.g. you might see the AWS DLCs clearing apt caches in the same RUN command as performing apt installs. If you find yourself really struggling with the size of the base AWS DLC you could look in to building from scratch \/ another base, and installing everything you need... But of course, would need to do the due diligence to check you're including everything you need & it's optimized well.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-08T09:18:11.681Z",
                "Answer_upvote_count":0,
                "Answer_body":"You need a smaller container image. Also, take into consideration that at the moment SageMaker serverless endpoints do not support GPU acceleration (see https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html#serverless-endpoints-how-it-works-exclusions).",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker GT Streaming Labelling Job Internal Server Error (Job Failed)",
        "Question_creation_time":1661525322145,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZD0isCgLSYqxXhdah_0jkQ\/sage-maker-gt-streaming-labelling-job-internal-server-error-job-failed",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth",
            "Amazon CloudWatch Logs"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":61.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I launched a Sagemaker Ground Truth Labeling Job with a vendor for 3D Point Cloud Object Tracking using the API. Yesterday, my job failed with the reason for failure\n\nInternalServerError: We encountered an internal error. Submit a new job.\n\n\nThe last three Cloudwatch logs before I saw the failure were\n\n{ \"labeling-job-name\": \"scand-trial2-seq10v2stream\", \"event-name\": \"BATCH_ANNOTATION_STATUS_EVALUATED\", \"event-log-message\": \"Batch of annotation tasks completed with status FAILED. \" }\n\n{ \"labeling-job-name\": \"scand-trial2-seq10v2stream\", \"event-name\": \"EXPORTED_LABELED_MANIFEST\", \"event-log-message\": \"Labeled manifest written to S3 output location.\" }\n\n{ \"labeling-job-name\": \"scand-trial2-seq10v2stream\", \"event-name\": \"IDLE_TIMER_COUNT_INTERRUPTED\", \"event-log-message\": \"System detected incoming objects. Timer to monitor idleness was reset.\" }\n\nMy vendor confirmed that they had annotations saved before this happened, but my output manifest is empty. In addition, I set the expiration time for my job to be 30 days, and this failed at 20 days. What could have caused this error and is there any known method for retrieving the worker annotations that were saved but not submitted?",
        "Answers":[
            {
                "Answer_creation_time":"2022-09-01T20:36:14.799Z",
                "Answer_upvote_count":0,
                "Answer_body":"I am afraid to inform you that the \"Internal Server Error\" can occur due to various reasons, In order to deep-dive further and investigate in to the service level logs we would request you to open a support ticket with the AWS premium support with the following information.\n\n-- Account Id\n\n-- Region\n\n-- Failed Job ARNs and cloudwatch logs\n\nNote :Due to security reason, this post is not suitable for sharing customer's resource and account details.\n\nThank you.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Closing up a Sagemaker user profile - intended behavior?",
        "Question_creation_time":1640092026826,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmSJa7T1nRm6PQkiXDxZJqA\/closing-up-a-sagemaker-user-profile-intended-behavior",
        "Question_topic":[
            "Machine Learning & AI",
            "Cloud Financial Management"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Billing"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":529.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I had a Sagemaker user I wasn't using, so I tried to delete it and initially came across this tutorial: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-delete-domain.html . As part of the larger process for how to delete a domain, it shows how to delete any user profiles within that domain. The steps are:\n\nChoose the user.\nOn the User Details page, for each non-failed app in the Apps list, choose Delete app.\nOn the Delete app dialog, choose Yes, delete app, type delete in the confirmation field, and then choose Delete.\nWhen the Status for all apps show as Deleted, choose Delete user.\n\nThe problem comes on the final step: I wasn't able to find a \"Delete user\" button. This feels like a bug, because without such a button the only way to stop charges on a Sagemaker user is to use the CLI, which I eventually did. You can only delete the domain if you have deleted all users, meaning it only works using the CLI for that as well. For every other AWS service I've used, there is an easy way to delete everything from the GUI.",
        "Answers":[
            {
                "Answer_creation_time":"2021-12-21T13:51:27.751Z",
                "Answer_upvote_count":0,
                "Answer_body":"Try clicking on the user, then Edit, and then Delete? I don't remember if that is the exact flow, but I do know that you can do it in the GUI. I've done it a few times.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS SageMaker - Extending Pre-built Container, Deploy Endpoint Failed. No such file or directory: 'serve'\"",
        "Question_creation_time":1664542013756,
        "Question_link":"https:\/\/repost.aws\/questions\/QUR-uTDaDsQBGjMoAUcsi2sQ\/aws-sage-maker-extending-pre-built-container-deploy-endpoint-failed-no-such-file-or-directory-serve",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":97.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"I am trying to deploy the SageMaker Inference Endpoint by extending the Pre-built image. However, it failed with \"FileNotFoundError: [Errno 2] No such file or directory: 'serve'\"\n\nMy Dockerfile\n\nARG REGION=us-west-2\n\n# SageMaker PyTorch image\nFROM 763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-ec2\n\nRUN apt-get update\n\nENV PATH=\"\/opt\/ml\/code:${PATH}\"\n\n# this environment variable is used by the SageMaker PyTorch container to determine our user code directory.\nENV SAGEMAKER_SUBMIT_DIRECTORY \/opt\/ml\/code\n\n# \/opt\/ml and all subdirectories are utilized by SageMaker, use the \/code subdirectory to store your user code.\nCOPY inference.py \/opt\/ml\/code\/inference.py\n\n# Defines inference.py as script entrypoint \nENV SAGEMAKER_PROGRAM inference.py\n\n\nCloudWatch Log From \/aws\/sagemaker\/Endpoints\/mytestEndpoint\n\n2022-09-30T04:47:09.178-07:00\nTraceback (most recent call last):\n  File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 20, in <module>\n    subprocess.check_call(shlex.split(' '.join(sys.argv[1:])))\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 359, in check_call\n    retcode = call(*popenargs, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 340, in call\n    with Popen(*popenargs, **kwargs) as p:\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 858, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 1704, in _execute_child\n    raise child_exception_type(errno_num, err_msg, err_filename)\nTraceback (most recent call last): File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 20, in <module> subprocess.check_call(shlex.split(' '.join(sys.argv[1:]))) File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 359, in check_call retcode = call(*popenargs, **kwargs) File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 340, in call with Popen(*popenargs, **kwargs) as p: File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 858, in __init__ self._execute_child(args, executable, preexec_fn, close_fds, File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 1704, in _execute_child raise child_exception_type(errno_num, err_msg, err_filename)\n\n2022-09-30T04:47:13.409-07:00\nFileNotFoundError: [Errno 2] No such file or directory: 'serve'",
        "Answers":[
            {
                "Answer_creation_time":"2022-10-01T09:28:00.773Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi, @holopekochan!\n\nThe serve script is installed by SageMaker PyTorch Inference Toolkit when you pip-install it in the Dockerfile.\n\nHowever, it's hard to say why it's not found in your container. Are you sure that you use the inference container, not training container, for your endpoint? If you go to the AWS Console > Amazon SageMaker > Models > your model, what ECR image it shows in Container 1 - Image?\n\nIt will be useful if you can share the code that you used to setup the SageMaker PyTorch estimator (if any) how you define your PyTorchModel and how you deploy() it.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-30T15:14:58.846Z",
                "Answer_upvote_count":0,
                "Answer_body":"Should use the Sagemaker image\n\n763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-sagemaker\n\n\ninstead of ec2\n\n763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-ec2",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Multi-model, Multi-container and Variants - what are the possible combinations?",
        "Question_creation_time":1643789695844,
        "Question_link":"https:\/\/repost.aws\/questions\/QUw1jokF5cQLmWHpj0hBGgtg\/multi-model-multi-container-and-variants-what-are-the-possible-combinations",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":126.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"This question is mostly for educational purposes, but the current SageMaker documentation does not describe whether these things are allowed or not.\n\nLets suppose I have:\n\na XGBoost_model_1 (that needs a XGBoost container)\na KMeans_model_1 and a KMeans_model_2 (both require a KMeans container)\n\n1. Here's the first question - can I do the following:\n\ncreate a Model with InferenceExecutionConfig.Mode=Direct and specify two cointainers (XGBoost and KMeans with Mode: MultiModel)\n\nThat would enable the client:\n\nto call invoke_endpoint(TargetContainer=\"XGBoost\") to access the XGBoost_model_1\nto call invoke_endpoint(TargetContainer=\"KMeans\", TargetModel=\"KMeans_model_1\") to access the KMeans_model_1\nto call invoke_endpoint(TargetContainer=\"KMeans\", TargetModel=\"KMeans_model_2\") to access the KMeans_model_2\n\nI don't see a straight answer in the documentation whether combining Multi-Model containers with Multi-container endpoint is possible.\n\n2. The second question - how does the above idea work with ProductionVariants. Can I create something like this:\n\nVariant1 with XGBoost serving XGBoost_model_1 having a weight of 0.5\nVariant2 with a Multi-container having both XGBoost and KMeans (with a MultiModel setup) having a weight of 0.5\n\nSo that the client could:\n\ncall invoke_endpoint(TargetVariant=\"Variant2\", TargetContainer=\"KMeans\", TargetModel=\"KMeans_model_1\") to access the KMeans_model_1\ncall invoke_endpoint(TargetVariant=\"Variant2\", TargetContainer=\"KMeans\", TargetModel=\"KMeans_model_2\") to access the KMeans_model_2\ncall invoke_endpoint(TargetVariant=\"Variant1\") to access the XGBoost_model_1\ncall invoke_endpoint(TargetVariant=\"Variant2\", TargetContainer=\"XGBoost\") to access the XGBoost_model_1\n\nIs that combination even possible?\n\nIf so, what happens when the client calls the invoke_endpoint without specifying the variant? For example:\n\nwould invoke_endpoint(TargetContainer=\"KMeans\", TargetModel=\"KMeans_model_2\") fail 50% of the time (if it hits the right variant then it works just fine, if it hits the wrong one it would most likely result with a 400\/500 error (\"incorrect payload\")?",
        "Answers":[
            {
                "Answer_creation_time":"2022-03-16T12:04:45.094Z",
                "Answer_upvote_count":0,
                "Answer_body":"Well, I've checked that myself.\n\nTurns out NONE of these combinations are possible. :)\n\nMulti-model + Multi-container is NOT possible\nVariants + Multi-container is NOT possible\nVariants + Multi-model is NOT possible\n\nIn all cases, you get a corresponding error while invoking create_endpoint_configuration:\n\nMultiple ProductionVariants is currently not supported when a Model uses a Direct InferenceExecutionMode.\nDirect InferenceExecutionMode is not supported when a Container uses MultiModel mode.\nMultiModel mode is not supported with the current model specification.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    }
]