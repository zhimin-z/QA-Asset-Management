[
    {
        "Question_title":"Unable to create endpoint",
        "Question_creation_time":1553516561000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUySs_fgNpSE6wuY-6W7MwqQ\/unable-to-create-endpoint",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":301.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nI am new to SageMaker and I am trying to deploy my model to an endpoint but am getting the following error:\n\nFailure reason\nUnable to locate at least 2 availability zone(s) with the requested instance type ml.t2.medium that overlap with SageMaker subnets\n\nI have tried using different instance types but always the same error\n\nI was under the impression that SageMaker will create the required instances for me and I do not need to create the instances first? I am using the EU-WEST-1 zone and using the console to setup the endpoint",
        "Answers":[
            {
                "Answer_creation_time":"2019-03-25T23:31:36.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nSagemaker engineer here. I looked at the VpcConfig of your model and found only one subnet configured.\n\nThe error message \"Unable to locate at least 2 availability zone(s) with the requested instance type XYZ that overlap with SageMaker subnets\" usually indicates misconfigured VPCs. Sagemaker imposes mandatory requirement for at least 2 availability zones in your VPC subnets even if you only request one instance, to account for the potential use of auto-scaling in the future.\n\nIn order to create the endpoint, the number of subnets in your model needs to be at least 2 in distinct availability zones, and ideally as close to the total number of availability zones as possible in the region.\n\nHope it helps,\nWenzhao",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Canvas connect Redshift failed",
        "Question_creation_time":1641573857407,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJvjatAJaQv-Ist96WT1IIw\/sage-maker-canvas-connect-redshift-failed",
        "Question_topic":[
            "Security, Identity, & Compliance",
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "AWS Identity and Access Management",
            "Amazon SageMaker",
            "Amazon Redshift",
            "Amazon SageMaker Canvas"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":135.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Try to add the Redshift connection on SageMaker Canvas to import the data\n\nThe cluster identify: redshift-cluster-1\ndatabase name: dev\ndatabase user: awsuser\nunload IAM Role: my-reshift-role\nconnection name: redshift\ntype: IAM\n\nmy-reshift-role trust-relationship is trust the \"redshift.amazonaws.com\" and \"sagemaker.amazonaws.com\"\n\nExpectation: create connection successfully\n\nActually result: RedshiftCreateConnectionError Unable to validate connection. An error occurred when trying to list schema from Redshift",
        "Answers":[
            {
                "Answer_creation_time":"2022-01-08T09:30:21.918Z",
                "Answer_upvote_count":0,
                "Answer_body":"The sagemaker canvas using sagemaker domain user, so need add the Redshift permission to the IAM Role attached to domain user. After add the permission, the connection can be setup",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Custom packages in Sagemaker studio",
        "Question_creation_time":1592469976000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZZFjMw_gS5Cz8sh-TK4J3w\/custom-packages-in-sagemaker-studio",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":266.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi everyone,\n\nhow can i install custom OS libraries on Sagemaker studio? When I open a terminal it states:\n\nroot@0f04278e59cf:~\/# yum install unzip\n\nbash: yum: command not found\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_time":"2020-06-18T09:18:20.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Short answer: [Studio UI] > File > New > Terminal > sudo yum install unzip\nThen unzip away...\n\nLong answer:\nYou can open a terminal in two different types of compute environment:\n\nOn a specific kernel you're running: [Studio UI] > kernal tab > Terminial icon.\nOn the compute studio (jupyter) itself: [Studio UI] > File > New > Terminal\n\nIn both options your personal files folder is accessible. In a kernel terminal: \/root. In a Jupyter terminal: \/home\/sagemaker-user.\nWhen opening a kernel terminal you'll have access to the software that is part of the kernel's container (say tensorflow container). Which in your case is missing yum. You can of course try apt-get, and such to install more tools.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unable to use the same HumanWorkflow within textract for more than 1 file\/call",
        "Question_creation_time":1639923120530,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDgM77ZgnTbWjW57_v9rCGw\/unable-to-use-the-same-human-workflow-within-textract-for-more-than-1-file-call",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon Textract",
            "Amazon Augmented AI",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":50.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"I created the private team from the Amazon SageMaker console for labeling tasks followed by the creation of the human review workflow, which I later integrated with the Amazon Textract for Key-Value pair extraction.\n\nWhile I called the analyze_document (along with HumanLoop configuration) to extract key-value pairs for the first time it worked as expected and I was able to see the Job in the labeling project console. However, when I called it again (irrespective of the same or different file) the HumanLoop started giving the below error.\n\n\"[ERROR] InvalidParameterException: An error occurred (InvalidParameterException) when calling the AnalyzeDocument operation: HumanLoop 'textractworkflow1' already exists and it is associated with a different InputContent. Please use a new HumanLoopName and try your request again.\"\n\nDo we have to create a new Human Review Loop each time we trigger analyze_document with another file?",
        "Answers":[
            {
                "Answer_creation_time":"2021-12-29T06:21:48.043Z",
                "Answer_upvote_count":1,
                "Answer_body":"I think you are using the same human loop name for multiple tasks and that is causing this issue. You have to make sure that within the HumanLoopConfig configuration, the HumanLoopName should be unique for each task. You can also refer to this video",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-12-27T14:03:50.873Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hello,\n\nI'm from the AWS Textract team. Thanks for using Textract.\n\nMost likely as per the details of the exception mentioned suggests there could be a need of using a new HumanLoopName. I believe someone from sageMaker team will shortly respond to this.\n\nHowever to check if it is due to Textract calls, could you please help us out with the following questions:\n\nCould you please share the entire stack trace of the exception ?\nIs this exception being thrown at the time Textract is called ? If yes, could you check and compare the i\/p parameters passed at the initial time when it passes and the next call when it fails ? Also please share the i\/p parameters for both.\n\nThanks,\n\nNitish",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do you analyze Autopilot results in Amazon SageMaker Studio?",
        "Question_creation_time":1601332827000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU7p9B6zcpSIeTG4MbzrSjKA\/how-do-you-analyze-autopilot-results-in-amazon-sage-maker-studio",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":38.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I launched an Autopilot job in SageMaker Studio, and now I'm trying to figure out how to compare autoML iterations. Is there a way to list them, see their metrics, and see the configuration of the best job?",
        "Answers":[
            {
                "Answer_creation_time":"2020-09-29T00:33:01.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Watch the Choose and deploy the best model video tutorial in the SageMaker developer guide. The video shows how to use SageMaker Autopilot to visualize and compare model metrics.\n\nFor more SageMaker Autopilot tutorials, see Videos: Use Autopilot to automate and explore the machine learning process.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Pros and cons of restricting user access to certain regions",
        "Question_creation_time":1642700804560,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxt7fqO9HQrKWDfi4V4Lagg\/pros-and-cons-of-restricting-user-access-to-certain-regions",
        "Question_topic":[
            "Security, Identity, & Compliance",
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "AWS Identity and Access Management",
            "Amazon SageMaker",
            "AWS Account Management"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":43.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hello, Are there any drawbacks I should be aware of if we restrict user access to only a single region?\n\nWe use a variety of AWS services but mainly S3 and Sagemaker Studio. Our team is located in various locations so their default regions are different. It has been a challenge to keep track of studio instances when they are created in different regions so we are now considering restricting access to a single region. Are there issues that we may face in that case? Any services we may miss?",
        "Answers":[
            {
                "Answer_creation_time":"2022-01-20T19:09:44.219Z",
                "Answer_upvote_count":4,
                "Answer_body":"I would take a look at this for some potential edge cases. In summary, you may need to allow us-east-1 and us-west-2 in addition to whatever regions your team is in since they host some of the global service endpoints (like IAM, Route 53, Global Accelerator, and a few others). For STS, I would use the regional endpoints if you aren't already.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"aws textract human review flow, failed to load image",
        "Question_creation_time":1665415509779,
        "Question_link":"https:\/\/repost.aws\/questions\/QUo48ev4bTTvO-GjsezfAmuQ\/aws-textract-human-review-flow-failed-to-load-image",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon Textract",
            "Amazon Augmented AI",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":48.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nI`m using aws textract to extract key-value pairs from an pdf. Because sometimes the accucary is low i use augmented AI (human review worflows) to involve a human worker. That works fine with png files, but when I use pdf files (which textract supports), I get an \"Failed to load image\". How do I get around this? I tried using a custom template, but can't find a way to insert the file type.\n\nBest regards,\n\nPaul",
        "Answers":[
            {
                "Answer_creation_time":"2022-10-11T04:31:12.324Z",
                "Answer_upvote_count":1,
                "Answer_body":"The underlying challenge here is that, while modern browsers can natively render PDFs, they require different embedding methods for PDFs vs images. To my knowledge there's no built-in SageMaker Crowd HTML Element that's capable of handling both types interchangeably - and your experience with the pre-built UI seems to confirm this.\n\nDisplaying PDFs in A2I\/SMGT\n\nThis simple sample suggests to use an <iframe type=\"application\/pdf\"> to display PDFs via the browser's native renderer. You could try this approach... but as of ~March 2022, I found support was patchy because some browsers' default security policies didn't like loading a cross-origin iframe with interactive content.\n\nIf relying on the browser native renderer won't work for your users, you can use the open-source PDF.js renderer instead. Here is a more complex sample template that does that. PDF.js is powerful, but can be pretty tricky to get started with from my experience... Note that the basic process in this sample is:\n\nTag the <script>s and stylesheets for PDF.js in from a CDN\nInclude a PDF viewer structure in your HTML\nPass your A2I object URL in through JavaScript and set up your viewer there - including any interactivity you need\n(The second inline script tag there you can probably ignore: It's specific to what that data that template collects)\nScaling template complexity\n\nAlthough the situation has improved a lot in recent years, writing direct-to-browser inline JavaScript in HTML can be tricky due to browser diversity and developer tooling limitations. If you want to build more advanced, interactive task templates, you might want to explore using front-end frameworks like React\/Angular\/Vue within A2I\/Ground Truth.\n\nThe above-mentioned PDF.js template is actually a legacy that's since been replaced by this VueJS app in the sample that uses it. In that case, the switch was made because we wanted to customize the PDF viewer (rendering detection boxes over the document), and the complexity of the app justified setting up a proper toolchain. You can find discussion there about using frameworks in general and VueJS in particular with A2I, and could use the app as a starting point for building your own complex template in advance. Note if I was re-building that from scratch, I'd probably use much less liquid templating, and implement more within the JS framework itself as discussed here.\n\nYou can see the complex template being built\/deployed from (SageMaker) Python notebook here, and a screenshot of it in action here. This end-to-end sample is discussed further in an AWS ML blog post.\n\nHandling mixed PDF\/Image content\n\nIf you need your template to handle both PDFs and images, this will add extra complexity. Could your JavaScript infer from the object URL (filename) which category the input object falls into, and dynamically set up either an <img> tag or a PDF viewer? Could you fetch the object from JS and check the Content-Type response header? Might it be simpler to add the file type as an input to your A2I loop, and pass it in that way? (e.g. using conditional liquid template to either render an <img> or not?)\n\nDepending on what points in the flow you know the file type, there are multiple different ways you might tackle this. Ultimately though, you'll probably be switching between either generating an img or a PDF viewer: Whether those HTML elements are created by static Liquid templating or by dynamic JS.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-10-11T03:30:51.984Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Paul, have you tried this pdf using normal AnalyzeDocument API? And also could you check if the IAM role for your human review workflow has sufficient permission to access your s3 bucket?",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to deploy lifecicle configuration to Sagemaker Studio via Cloudformation",
        "Question_creation_time":1666655893373,
        "Question_link":"https:\/\/repost.aws\/questions\/QUb1jvjl80RCClOggimxYaTQ\/how-to-deploy-lifecicle-configuration-to-sagemaker-studio-via-cloudformation",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS CloudFormation"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":23.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hello,\n\nI need to deploy a new lifecicle configuration to Sagemaker Studio. I was searching in the Cloudformation Documentation and found nothing but this service.\n\nhttps:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-notebookinstancelifecycleconfig.html\n\nAs the name of the service says, it was deployed as a Lifecicle config for a Notebook Instance.\n\nIs there any way to deploy a Lifecicle configuration for Sagemaker studio via Cloudformation ?\n\nThanks. Anderson",
        "Answers":[
            {
                "Answer_creation_time":"2022-10-25T07:12:22.487Z",
                "Answer_upvote_count":1,
                "Answer_body":"CloudFormation currently does not support SageMaker Studio lifecycle configurations out-of-the-box. See also: https:\/\/github.com\/aws-cloudformation\/cloudformation-coverage-roadmap\/issues\/1132\n\nI assume it would be possible to create a custom resource for that purpose.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do I achieve the least-access secure networking for SageMaker Training on Amazon FSx for Lustre?",
        "Question_creation_time":1605279993000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrTkxH_kIT-a_LJSGYS5SXA\/how-do-i-achieve-the-least-access-secure-networking-for-sage-maker-training-on-amazon-f-sx-for-lustre",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI",
            "Networking & Content Delivery"
        ],
        "Question_tag":[
            "Amazon FSx for Lustre",
            "Amazon SageMaker",
            "Networking & Content Delivery",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":56.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I'm trying to figure out a minimally permissive yet operational network configuration for Amazon SageMaker training to train on data from Amazon FSx for Lustre. My understanding is that both the file system and the SageMaker instance can have their own security groups and that FSx uses TCP on ports 988 and 1021-1023. Therefore, I think a good network configuration for using SageMaker with FSx is the following:\n\nSageMaker EC2 equipped with the security group SM-SG that allows Inbound only with TCP on 988 and 1021-1023 from FSX-SG only.\nAmazon FSx equipped with the security group FSX-SG that allows outbound only with TCP on 988 and 1021-1023 towards SM-SG only. Is this configuration enough for the training to work? Do FSx and SageMaker need other ports and sources to be opened to operate normally?",
        "Answers":[
            {
                "Answer_creation_time":"2020-11-13T15:26:19.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"For the security group for Amazon FSx (Example: FSx-SG), you need to add the following additional rules:\n\nFSx-SG needs inbound access from the security group for SageMaker (Example: SM-SG). The SageMaker instance needs to initiate a connection to the Amazon FSx file system, which is an inbound TCP packet to FSx.\nFSx-SG needs inbound and outbound access to itself. This is because, Amazon FSx for Lustre is a clustered file system, where each file system is typically powered by multiple file servers, and the file servers need to communicate with one another.\n\nFor more information on the minimum set of rules required for FSx-SG, see [File system access control with Amazon VPC][1]. [1]: https:\/\/docs.aws.amazon.com\/fsx\/latest\/LustreGuide\/limit-access-security-groups.html",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to checkpoint SageMaker model artifact during a training job?",
        "Question_creation_time":1586331915000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrXX2MIygS5igas27GrAhHw\/how-to-checkpoint-sage-maker-model-artifact-during-a-training-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":182.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nIs there a way to regularly checkpoint model artifact in a SageMaker training job for BYO training container?",
        "Answers":[
            {
                "Answer_creation_time":"2020-04-08T15:22:36.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"If you specify a checkpoint configuration (regardless of managed spot training) when starting a training job, checkpointing will work. You can provide a local path and S3 path as follows (API reference):\n\n\"CheckpointConfig\": { \n  \"LocalPath\": \"string\",\n  \"S3Uri\": \"string\"\n}\n\n\nThe local path defaults to \/opt\/ml\/checkpoints\/, and then you specify the target path in S3 with S3Uri.\n\nGiven this configuration, SageMaker will configure an output channel with Continuous upload mode to Amazon S3. At the time being, this results in running an agent on the hosts that watches the file system and continuously uploads data to Amazon S3. Similar behavior is applied when debugging is enabled, for delivering tensor data to Amazon S3.\n\nAs commented, sagemaker-containers implements its own code to save intermediate outputs and watching files on the file system, but I would rather rely on the functionality offered by the service to avoid dependencies on specific libraries where possible.\n\nNote: when using SageMaker Processing, which in my view can be considered an abstraction over training or, from another perspective, the foundation for training, you can configure an output channel to use continuous upload mode; further info here.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Data Wrangler UI Features",
        "Question_creation_time":1641943985816,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcsIt78jnSTW8Ta9__kUm-w\/sage-maker-data-wrangler-ui-features",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":1.0,
        "Question_view_count":61.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":true,
        "Question_body":"The SageMaker Data Wrangler UI in SageMaker Studio doesn't seem to support all the features that the API does. When will the UI support:\n\nLoading all s3 objects under a prefix? https:\/\/aws-data-wrangler.readthedocs.io\/en\/stable\/stubs\/awswrangler.s3.read_csv.html#awswrangler.s3.read_csv\nLoading JSON objects in addition to CSV and Parquet files? https:\/\/aws-data-wrangler.readthedocs.io\/en\/stable\/stubs\/awswrangler.s3.read_json.html#awswrangler.s3.read_json",
        "Answers":[
            {
                "Answer_creation_time":"2022-01-12T20:59:06.281Z",
                "Answer_upvote_count":2,
                "Answer_body":"As mentioned by Tulio Alberto in comments, Amazon SageMaker Data Wrangler (the graphical data preparation feature inside Amazon SageMaker) is separate from AWS Data Wrangler (an open-source data prep utility published by AWS Labs): The two tools are based on different technologies and don't necessarily aim for full feature parity - they just happen to share similar names.\n\nTo my knowledge there's no committed timeline we can share at the moment for when these particular features will make it to SageMaker Data Wrangler, but I think as feature requests they make sense and the reasoning for both is pretty clear: I'm aware that both have been discussed to some extent internally already, and I'd personally like to see them launch too!\n\nThanks for sharing the feedback, and apologies for the naming confusion!",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-02-12T01:48:49.303Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Tulio, thanks for the clarification. But doesn't SageMaker Data Wrangler generate code that complies with\/uses AWS Data Wrangler? Isn't there some (if tenuous) connection between the two?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-13T05:22:27.466Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nSageMaker Data Wrangler in Studio just launched the JSON\/ORC support and we support import files under a prefix already. Please see the following links\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/data-wrangler-import.html#data-wrangler-import-s3\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/prepare-and-analyze-json-and-orc-data-with-amazon-sagemaker-data-wrangler\/",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Where can I find guidance for getting a customer started with SageMaker sizing and cost?",
        "Question_creation_time":1603285551000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq-Kaj1bLStK6Bs2gCUZ1Iw\/where-can-i-find-guidance-for-getting-a-customer-started-with-sage-maker-sizing-and-cost",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":33.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"A customer wants to use SageMaker, but doesn't know how to get started with instance sizes or how to forecast the cost for it. I've looked at the SageMaker TCO PDF we have online, but that appears more marketing than helpful, i.e. more price comparison than guidance.\n\nI know that the SageMaker cost is really the underlying EC2 and storage pieces, not SageMaker itself. However, I feel it is incorrect to say that they start with (say) t3.medium and see if that fits and scale up if they need more power behind it. As well, that doesn't help them to forecast either.\n\nAny thoughts here?",
        "Answers":[
            {
                "Answer_creation_time":"2020-10-21T13:22:02.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"See the performance efficiency and cost optimization pillars in Machine Learning Lens. Additionally this is an EC2 based right sizing best practices guide.\nOverall, it's better to start small, then increase instance size as needed (as those that start large, never bother reduce the size), or apply auto scaling for SageMaker hosting.\nAssuming a CPU ML predictions: When choosing ml.t2.medium instances the customer will need to keep an eye on the instance CPU credits. If they lack the knowledge, just start with M5.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Accessing SageMaker Notebooks without accessing the console",
        "Question_creation_time":1543947347000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUp9lMw9-ESm-27BWY_RgCSg\/accessing-sage-maker-notebooks-without-accessing-the-console",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":240.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Is it possible to access SageMaker Notebooks without accessing the console?\n\nDo we have a best practice for that? In the create-presigned-notebook-instance-url command, what is the --session-expiration-duration-in-seconds: is it the validity duration of the URL or the max session duration once the URL has been clicked?",
        "Answers":[
            {
                "Answer_creation_time":"2018-12-04T19:28:56.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I have experimented with CreatePresignedNotebookInstanceUrl a number of times. It returns an \"AuthorizedUrl\" string in the form: https:\/\/<notebook_instance_name>.notebook.<region>.sagemaker.aws?authToken=<a_very_long_string>\n\nI used the URL in another browser with no AWS console's session cookies (not logged in to the console) and it worked (could access my notebooks).\n\nThe parameter SessionExpirationDurationInSeconds is... well, exactly what it says, The number of seconds the presigned url is valid for. The API accepts a range of [1800, 43200] in seconds, which is equivalent to : 30 minutes to 12 hours .\n\nI hope this helps",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is it possible to test locally SageMaker Inference Pipelines?",
        "Question_creation_time":1600158011000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU8R_MjbU1QPm66SCgld4spQ\/is-it-possible-to-test-locally-sage-maker-inference-pipelines",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":202.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Is it possible to test locally SageMaker Inference Pipelines? I would like to be able to easily troubleshoot and find the appropriate serialization between containers",
        "Answers":[
            {
                "Answer_creation_time":"2020-09-25T12:39:21.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"If you are referring to using local mode via the SM PySDK, then pipeline deployment is not supported.\n\nAs an alternative, given your three inference containers, you could manually run the services locally and then implement a kind of facade function that invokes the three services in pipeline and manages input\/output accordingly.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Custom Amazon SageMaker container registration and deployment tracking",
        "Question_creation_time":1607710961000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdJqWN_WJQeuYrkZMikkibQ\/custom-amazon-sage-maker-container-registration-and-deployment-tracking",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":34.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"My customer asks that:\n\nContainer images must be registered and deployments tracked\n\nContainers must be registered within a private customer-owned registry prior to deployment\n\nOnly registered containers are to be deployed.\n\nPart of the registration process must include verification the containers have comes from a trusted source and that they have been scanned and found to be free of malware and vulnerabilities.\n\nAn inventory of all deployed containers must be maintained at all times.\n\nThe inventory must include: Software installed within the container version of all software and patch level . Where the container has been deployed . Owner of the container\n\nDo we do any of these? Please provide documentation on AWS\/SageMaker vs custom container provider's responsibilities.",
        "Answers":[
            {
                "Answer_creation_time":"2020-12-18T15:43:38.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Amazon Elastic Container Registry (Amazon ECR) enables customers to store images, secure their images using AWS Identity and Access Management (IAM), and scan their containers for vulnerabilities. Open Policy Agent (OPA) is an open-source project focused on codifying policy such as the approved image registries. OPA is integrated with Kubernetes via Gatekeeper, an admission controller that checks if the image is from an approved registry prior to allowing it to be deployed on the cluster. For more details see: https:\/\/aws.amazon.com\/blogs\/containers\/designing-a-secure-container-image-registry",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS StepFunctions - SageMaker's InvokeEndpoint block throws \"validation error\" when fetching parameters for itself inside iterator of Map block",
        "Question_creation_time":1647503861594,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDc1foN9TQhe3OYkkGzCKhQ\/aws-step-functions-sage-makers-invoke-endpoint-block-throws-validation-error-when-fetching-parameters-for-itself-inside-iterator-of-map-block",
        "Question_topic":[
            "Serverless",
            "Application Integration",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "AWS Step Functions",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":110.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I have a state-machine workflow with 3 following states:\n\nscreenshot-of-my-workflow\n\nA 'Pass' block that adds a list of strings(SageMaker endpoint names) to the original input. (this 'Pass' will be replaced by a call to DynamoDB to fetch list in future.)\nUse map to call SageMaker endpoints dictated by the array(or list) from above result.\nSend the result of above 'Map' to a Lambda function and exit the workflow.\n\nHere's the entire workflow in .asl.json, inspired from this aws blog.\n\n{\n  \"Comment\": \"A description of my state machine\",\n  \"StartAt\": \"Pass\",\n  \"States\": {\n    \"Pass\": {\n      \"Type\": \"Pass\",\n      \"Next\": \"InvokeEndpoints\",\n      \"Result\": {\n        \"Endpoints\": [\n          \"sagemaker-endpoint-1\",\n          \"sagemaker-endpoint-2\",\n          \"sagemaker-endpoint-3\"\n        ]\n      },\n      \"ResultPath\": \"$.EndpointList\"\n    },\n    \"InvokeEndpoints\": {\n      \"Type\": \"Map\",\n      \"Next\": \"Post-Processor Lambda\",\n      \"Iterator\": {\n        \"StartAt\": \"InvokeEndpoint\",\n        \"States\": {\n          \"InvokeEndpoint\": {\n            \"Type\": \"Task\",\n            \"End\": true,\n            \"Parameters\": {\n              \"Body\": \"$.InvocationBody\",\n              \"EndpointName\": \"$.EndpointName\"\n            },\n            \"Resource\": \"arn:aws:states:::aws-sdk:sagemakerruntime:invokeEndpoint\",\n            \"ResultPath\": \"$.InvocationResult\"\n          }\n        }\n      },\n      \"ItemsPath\": \"$.EndpointList.Endpoints\",\n      \"MaxConcurrency\": 300,\n      \"Parameters\": {\n        \"InvocationBody.$\": \"$.body.InputData\",\n        \"EndpointName.$\": \"$$.Map.Item.Value\"\n      },\n      \"ResultPath\": \"$.InvocationResults\"\n    },\n    \"Post-Processor Lambda\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::lambda:invoke\",\n      \"Parameters\": {\n        \"Payload.$\": \"$\",\n        \"FunctionName\": \"arn:aws:lambda:<my-region>:<my-account-id>:function:<my-lambda-function-name>:$LATEST\"\n      },\n      \"Retry\": [\n        {\n          \"ErrorEquals\": [\n            \"Lambda.ServiceException\",\n            \"Lambda.AWSLambdaException\",\n            \"Lambda.SdkClientException\"\n          ],\n          \"IntervalSeconds\": 2,\n          \"MaxAttempts\": 6,\n          \"BackoffRate\": 2\n        }\n      ],\n      \"End\": true\n    }\n  }\n}\n\n\nAs can be seen in the workflow, I am iterating over the list from the previous 'Pass' block and mapping those to iterate inside 'Map' block and trying to access the Parameters of 'Map' block inside each iteration. Iteration works fine with number of iterators, but I can't access the Parameters inside the iteration. I get this error:\n\n{\n  \"resourceType\": \"aws-sdk:sagemakerruntime\",\n  \"resource\": \"invokeEndpoint\",\n  \"error\": \"SageMakerRuntime.ValidationErrorException\",\n  \"cause\": \"1 validation error detected: Value '$.EndpointName' at 'endpointName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9])* (Service: SageMakerRuntime, Status Code: 400, Request ID: ed5cad0c-28d9-4913-853b-e5f9ac924444)\"\n}\n\n\nSo, I presume the error is because \"$.EndpointName\" is not being filled with the relevant value. How do I avoid this.\n\nBut, when I open the failed execution and check the InvokeEndpoint block from graph-inspector, input to that is what I expected and above JSON-Paths to fetch the parameters should work, but they don't.\nscreenshot-of-graph-inspector\n\nWhat's causing the error and How do I fix this?",
        "Answers":[
            {
                "Answer_creation_time":"2022-03-17T10:46:07.263Z",
                "Answer_upvote_count":1,
                "Answer_body":"In general (as mentioned here in the parameters doc), you also need to end the parameter name with .$ when using a JSON Path.\n\nIt looks like you're doing that some places in your sample JSON (e.g. \"InvocationBody.$\": \"$.body.InputData\"), but not in others (\"EndpointName\": \"$.EndpointName\"), so I think the reason you're seeing the validation error here is that Step Functions is trying to interpret $.EndpointName as literally the name of the endpoint (which doesn't satisfy ^[a-zA-Z0-9](-*[a-zA-Z0-9])*!)\n\nSo suggest you change to EndpointName.$ and Body.$ in your InvokeEndpoint parameters",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Custom container not running under root account?",
        "Question_creation_time":1607710724000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYAkZepq4SgyArKZCC7gT_A\/custom-container-not-running-under-root-account",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":76.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"A customer wants to enforce these rules in their custom SageMaker containers:\n\n\u2022\tProcesses running inside a container must run with a known UID\/GUID and never as root.\n\u2022\tAvoid using privilege escalation methods that grant root access (e.g. sudo)\n\n\nHow do we ensure this?",
        "Answers":[
            {
                "Answer_creation_time":"2020-12-18T15:45:37.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"SageMaker requires that Docker containers run without privileged access. See: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-toolkits.html SageMaker Docker containers do not run in Privileged mode and have the following Linux capabilities removed: SETPCAP, SETFCAP, NET_RAW, MKNOD",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Tracking the lineage between Amazon SageMaker endpoint model and Model Monitor captured data",
        "Question_creation_time":1602088286000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPWBH_xFoS4aq5i41Za5qaQ\/tracking-the-lineage-between-amazon-sage-maker-endpoint-model-and-model-monitor-captured-data",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":84.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"I have an Amazon SageMaker endpoint with A1, a model with data capture activated, and I want to update the endpoint with A2, a new model.\n\nHow do I track the Model Monitor Data Capture that captured data in Amazon S3, and identify which data referred to model A1 and which data referred to model A2?",
        "Answers":[
            {
                "Answer_creation_time":"2020-10-08T08:01:12.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"using boto3:\nwhen you update the model endpoint, you need to create a new EndpointConfig where you specify a new s3 uri where data capture will be stored and thats how you can see different data captures from different versions of the model.\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateEndpoint.html\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpointConfig.html",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-09-23T19:58:35.449Z",
                "Answer_upvote_count":0,
                "Answer_body":"When you update the model endpoint (using boto3), you must to create a new EndpointConfig where you specify a new Amazon S3 URI. The new Amazon S3 URI is where the data capture will be stored so that you can see different data captures from different versions of the model.\n\nFor more information, see the following: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateEndpoint.html https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpointConfig.html",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker metrics persistence",
        "Question_creation_time":1578643279000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU197giXXuRn-4Hz56HZZpSw\/sage-maker-metrics-persistence",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":103.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Quick questions on ML metrics persistence from sagemaker training tasks. The SageMaker regexp-over-CloudWatch is an attractive option, yet the metric retention in Cloudwatch seems to be restricted to 15 days.\n\nHow to persist those metrics longer? Is it common to extract them out of Cloudwatch regularly to persist them somewhere else, eg S3 or an RDS? what is the best practice for long-term persistence of those metrics?\nWould SageMaker Experiments allow a collection of similar data (customer-defined training metrics) over a longer retention?",
        "Answers":[
            {
                "Answer_creation_time":"2020-01-10T18:32:10.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can now persist algorithm metrics from SageMaker Training Jobs (the ones you can collect with regexes or the ones available from built-in algorithms by default) by setting EnableSageMakerTimeSeriesMetrics through the AWS SDK or enable_sagemaker_metrics=true in the SageMaker Python SDK. These metrics are persisted long term, and available through Amazon SageMaker Studio. (Go to \"Metrics\" -> \"Add Chart\" from the detail page of a training job). These are available at no additional cost.\n\nYes, SageMaker Experiments allow collection of similar data\n\nNote that system metrics (CPU\/GPU\/Memory\/Disk) are still available only through CloudWatch.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Please validate: SageMaker Endpoint URL Authentication\/Authorization",
        "Question_creation_time":1602169065000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFlHNZ7JxTFGIkPHQ75u44w\/please-validate-sage-maker-endpoint-url-authentication-authorization",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":282.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Need validation:\n\nOnce the SageMaker endpoint is deployed. It can be invoked with the Sagemaker Runtime API InvokeEndpoint OR it can be invoked using the endpoint URL+HTTP AZ headers (below).\n\nSuccessful deployment also exposes a URL (on the console) that has the format:\n\nhttps:\/\/runtime.sagemaker.us-east-1.amazonaws.com\/endpoints\/ENDPOINT-NAME\/invocations\n\nWhat is the purpose of this URL (shown on console)?\n\nIn my understanding this URL Cannot be invoked w\/o appropriate headers as then there will be a need to have globally unique endpoint name!! THAT IS to invoke this URL it needs to have the \"HTTP Authorization headers\" (refer: https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/API\/sig-v4-authenticating-requests.html)\n\nI have a customer who is concerned that anyone can invoke the URL even from the internet. Tried to do it and received the <MissingTokenException> so I know it can't be done but just want to ensure I have the right explanation. (Test with HTTP\/AZ headers pending)",
        "Answers":[
            {
                "Answer_creation_time":"2020-10-08T15:38:33.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Your understanding is correct. From the docs:\n\nAmazon SageMaker strips all POST headers except those supported by the API. Amazon SageMaker might add additional headers. You should not rely on the behavior of headers outside those enumerated in the request syntax.\n\nCalls to InvokeEndpoint are authenticated by using AWS Signature Version 4.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker AutoPilot Regions",
        "Question_creation_time":1596620138000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_7jk19ozQSeyjTAR_D_hEA\/sage-maker-auto-pilot-regions",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":34.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Is there official documentation showing the regions in which SageMaker AutoPilot is supported? From my understanding, it should work with the SDK wherever SageMaker is supported, while in the no-code mode only where SageMaker Studio is available. Is this true?\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_time":"2020-08-05T13:44:15.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"SageMaker Autopilot works in all the regions where Amazon SageMaker is available today as noted in this blog post \"Amazon SageMaker Autopilot \u2013 Automatically Create High-Quality Machine Learning Models With Full Control And Visibility\". In addition, Autopilot is also integrated with Amazon SageMaker Studio, which is available in us-east-1, us-east-2, us-west-2 and eu-west-1. For a current list of available regions, please check the AWS Regional Services List.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Which Amazon SageMaker algorithms can only use GPU for training?",
        "Question_creation_time":1597251203000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdTLbPM2STGelSj1g3TIjpA\/which-amazon-sage-maker-algorithms-can-only-use-gpu-for-training",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":121.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I read somewhere that some Amazon SageMaker's built-in algorithms can only be trained using GPU, whereas some can use either GPU or CPU, and some can only be used on CPU.\n\nIs there any official documentation explicitly stating which algorithms can only use GPU or both?",
        "Answers":[
            {
                "Answer_creation_time":"2020-08-12T17:02:17.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Documentation for Amazon SageMaker built-in algorithms provides recommendations around choice of Amazon EC2 instances and whether given algorithm supports GPU or CPU devices.\n\nLet's take Image Classification as an example. Here is a excerpt from online documentation:\n\nFor image classification, we support the following GPU instances for training: ml.p2.xlarge, ml.p2.8xlarge, ml.p2.16xlarge, ml.p3.2xlarge, ml.p3.8xlargeand ml.p3.16xlarge. We recommend using GPU instances with more memory for training with large batch sizes. However, both CPU (such as C4) and GPU (such as P2 and P3) instances can be used for the inference. You can also run the algorithm on multi-GPU and multi-machine settings for distributed training.\n\nFor more complex scenarios, such as Script or BYO Container modes, customers have flexibility to choose which device (GPU or CPU) to utilize for which operation. This is configured as part of their training scripts.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Greengrass for data processing and ML model training",
        "Question_creation_time":1556295399000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2FEvRboNRL2Ipn1yE7IBvg\/greengrass-for-data-processing-and-ml-model-training",
        "Question_topic":[
            "Internet of Things (IoT)",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS IoT Greengrass",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":38.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Is it possible to train and deploy ML models in Greengrass? Or is Greengrass limited to inference while training is done using SageMaker in cloud?",
        "Answers":[
            {
                "Answer_creation_time":"2019-04-26T16:21:21.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"As a native service offering, Greengrass has support for deploying models to the edge and running inference code against those models. Nothing prevents you from deploying your own code to the edge that would train a model, but I suspect you wouldn't be able to store it as a Greengrass local resource for later inferences without doing a round trip to the cloud and redeploying to GG.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Is there a way to install R libraries in SageMaker that receive a non-zero exit status?",
        "Question_creation_time":1527798496000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE-0c9SwxRViGhd-lAYtsuw\/is-there-a-way-to-install-r-libraries-in-sage-maker-that-receive-a-non-zero-exit-status",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":464.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi all, I'm having an issue with an R kernel\/Jupyter notebook. I've come across two different libraries that result in the following error:\n\nWarning message in install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\"):\n\u201cinstallation of package \u2018XML\u2019 had non-zero exit status\u201dUpdating HTML index of packages in '.Library'\nMaking 'packages.html' ... done\n\n\nXML is the second package that I have run into this issue with. The other is rJava.\n\nI found a workaround that could work if I had root access, which is installing via the command line in a terminal. Which involves commands such as:\n\nyum install r-cran-rjava\n\n\nHowever, I don't have root access and cannot install as I get the message \"You need to be root to perform this command.\" So this workaround hasn't been possible.\n\nAfter checking the documentation for rJava and XML, I am running the requirements for JDK and other system requirements in SageMaker. This issue wasn't reproducible on a local RStudio environment. XML is a dependency for multiple R libraries (as is rJava). Is there a way that I can still install these packages?",
        "Answers":[
            {
                "Answer_creation_time":"2018-06-01T02:52:15.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"For Amazon SageMaker notebook Instances, you have the ability to assume root privileges, so instead of:\n\n$ yum install r-cran-rjava\n\nyou can try:\n\n$ sudo yum install r-cran-rjava\n\nwhich will allow you to impersonate the superuser (ie. root) for that command 1\n\nBut I don't believe that package exists in the available repos (ie. may be valid for another distro of Linux, but does not appear to be available in the yum repos -- running yum search 'r-cran-rjava' returned no results 2)\n\nInstead, from a prompt, install R + the necessary development files for later installation of R packages:\n\n$ sudo yum install -y R-java-devel.x86_64\n\nAnd finally, install the necessary XML libraries to support the XML package in R:\n\n$ sudo yum install -y libxml2-devel 3\n\nAfter which you can then open R (either as root user...)\n\n$ sudo R\n\nor personal\/local user\n\n$ R\n\nand execute the package installation:\n\n> install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\")\n\nEDITED TO FIX RJAVA PACKAGE INSTALLATION\n\nIt looks like the installation is requiring libgomp.spec\/libgomp.a files, so you can first find that file:\n\n$ sudo find \/ -iname libgomp.spec\n\nwhich should be located at \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec -- if so, you can manually create symlinks to fix this:\n\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec \/usr\/lib64\/\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.a \/usr\/lib64\/\n\n\nIf that ran correctly, you should now see both files in \/usr\/lib64 path:\n\n$ ls \/usr\/lib64\/libgomp*\n\nOnce confirmed, you can run the install.package('rJava') command.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Train machine learning model using reserved instance",
        "Question_creation_time":1641871148701,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsy3vkTMkSA2ojA1bmafDSA\/train-machine-learning-model-using-reserved-instance",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":151.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi.\n\nIs it possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance every time which is somewhat time consuming? I'm familiar with local mode, but I understand this is not supported when using AWS SageMaker machine learning estimators.\n\nAppreciate any suggestions for how to make the model training process in SageMaker go faster when using AWS SageMaker machine learning estimators.\n\nThanks, Stefan",
        "Answers":[
            {
                "Answer_creation_time":"2022-01-14T08:13:53.228Z",
                "Answer_upvote_count":0,
                "Answer_body":"As of today, it's not possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance. The service team is currently working on it, unfortunately I don't have an ETA as to when the feature will be released.\n\nLocal Mode is supported for frameworks images (TensorFlow, MXNet, Chainer, PyTorch, and Scikit-Learn) and images you supply yourself.\n\nUsing the SageMaker Python SDK \u2014 sagemaker 2.72.3 documentation\n\nIf you want to train Built-in algorithm models simply faster, you should check the recommendation in the SageMaker document.\n\nExample Blazingtext-instances, Deepar-instances\n\nIf the algorithm supports it, one can also try using Pipe mode or FastFile mode. These offer some fast training job startup time. Accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"how to version step functions for ML?",
        "Question_creation_time":1549456954000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdG1tanW-TXy-vY0YrCsVeg\/how-to-version-step-functions-for-ml",
        "Question_topic":[
            "Serverless",
            "Application Integration",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Step Functions",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":230.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi, Step Functions can be used to create ML workflows. What is the best practice to version the code creating those workflows? boto3 code in CodeCommit? Something else?\n\nCheers Olivier",
        "Answers":[
            {
                "Answer_creation_time":"2019-02-06T18:38:37.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"A Step Functions state machine usually doesn't come alone and typically relies on other resources such as Lambda, EC2, DynamoDB, etc. You might want to package these dependent artifacts\/resources altogether within a version otherwise you might have a state machine that doesn't fully work (eg, state machine version doesn't match Lambda version). I guess the simplest way to achieve this is to provision these resources together as code (eg, CDK or CloudFormation) and store them in a Git repo. You could then use Git tags for versioning.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"FileNotFoundError: [Errno 2] No such file or directory: '\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.8\/site-packages\/sagemaker\/image_uri_config\/catboost.json'",
        "Question_creation_time":1658343315554,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3HFACW88SuKcGZ2izeOsuA\/file-not-found-error-errno-2-no-such-file-or-directory-home-ec-2-user-anaconda-3-envs-python-3-lib-python-3-8-site-packages-sagemaker-image-uri-config-catboost-json",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "AWS Deep Learning Containers",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Elastic Container Registry (ECR)",
            "Amazon Elastic Container Service"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":87.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"I have an issue while getting Catboost image URI. It is a function for generating ECR image URIs for pre-built SageMaker Docker images. Here is my code catboost_container = sagemaker.image_uris.retrieve(\"catboost\", my_region, \"latest\")",
        "Answers":[
            {
                "Answer_creation_time":"2022-07-21T02:18:42.829Z",
                "Answer_upvote_count":1,
                "Answer_body":"As illustrated here in the docs for the algorithm, the parameters for retrieving this URI are a bit different: It's more like using the new JumpStart models (if you're familiar with that) than the old-style pre-built algorithms.\n\ntrain_model_id, train_model_version, train_scope = \"catboost-classification-model\", \"*\", \"training\"\ntraining_instance_type = \"ml.m5.xlarge\"\n\n# Retrieve the docker image\ntrain_image_uri = image_uris.retrieve(\n    region=None,\n    framework=None,\n    model_id=train_model_id,\n    model_version=train_model_version,\n    image_scope=train_scope,\n    instance_type=training_instance_type\n)\n\n\nI tested the above snippet from the doc page on SageMaker Studio and it worked OK. If you still see errors, it's likely your SageMaker Python SDK version is outdated (which can happen if for example you don't restart SM Studio apps or SM Notebook Instances regularly). Can check with sagemaker.__version__ and upgrade with !pip install --upgrade sagemaker if needed.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-07-22T17:41:29.834Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks for your guidence, I make the JSON file accordingly however another issue come up : I posted it in https:\/\/repost.aws\/questions\/QUVfbc_AsXRzaxAl69MMFlsQ\/error-for-training-job-catboost-classification-model-error-message-type-error-cannot-convert-xxx-to-float\n\nI'm trying to find a solution and I have searched a lot but with no success. Please share your thoughts on this as well! Thanks",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to tune SageMaker Studio Notebooks hardware config?",
        "Question_creation_time":1576675818000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUp5wKTB0URcCPyBgUcAWMww\/how-to-tune-sage-maker-studio-notebooks-hardware-config",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":70.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"How does one choose or tune the hardware backend of a Sagemaker Studio Notebook?",
        "Answers":[
            {
                "Answer_creation_time":"2019-12-18T22:33:24.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"At the top right of a notebook (near the kernel ) there will be a resource configurations button, you'll be able to choose the instance you want to run the notebook on.\n\nA nice feature of that is that all the instances shares the same EFS mount (SageMaker studio uses EFS for notebook storage) if you save a dataframe to the local disk (EFS) you can change instance type during your work and continue from the place you've been in (Move from a GPU instance to a CPU instance for cost effectiveness \/ back to GPU for performance)",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to use an Augmented Manifest File for AWS SageMaker Ground Truth?",
        "Question_creation_time":1546562747000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1LLbT-AYQDO-XXrjPUFl9w\/how-to-use-an-augmented-manifest-file-for-aws-sage-maker-ground-truth",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":98.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hey,\n\nI'm trying to use Ground Truth to do image classification but with a different set of label options for each image. I have the custom labeling task template and pre-\/post-labeling Lambda functions set up and I figured I could pass in the labels through the manifest file.\n\nMy issue is that the Ground Truth job ignores the attributes in the manifest file that are not \"source-ref\" (or \"source\"). This causes the pre-processing Lambda function to fail because the request it is passed only contains the \"source-ref\" attribute, but the Lambda function also references a different attribute. Are augmented manifest files supported for Ground Truth and if they are, how can I make use of the extra attributes?\n\nReferences:\nGround Truth Input Data: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html\nSageMaker Augmented Manifest Files: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/augmented-manifest.html\n\nExample:\n\nA normal Ground Truth manifest file:\n\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img1.png\"}\r\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img2.png\"}\r\n...\n\n\nWhat I want to be able to use:\n\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img1.png\",\"labels\":[\"pen\",\"pencil\",\"stick\"]}\r\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img2.png\",\"labels\":[\"tv\",\"laptop\",\"phone\"]}\r\n...",
        "Answers":[
            {
                "Answer_creation_time":"2019-01-07T19:20:40.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi sageuser, I'm an engineer at AWS. Augmented manifests are not supported for custom workflows, and so it is not possible to pass through additional parameters, e.g., \"labels\" in your example. We appreciate that you are using the service and welcome customer feedback. We can always be reached at https:\/\/aws.amazon.com\/contact-us\/.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Asynchronous Endpoint Configuration",
        "Question_creation_time":1653000488230,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZNbZZQHhSl2RYUtLU8zpSQ\/sagemaker-asynchronous-endpoint-configuration",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Autopilot",
            "Amazon SageMaker Model Building",
            "Amazon SageMaker JumpStart",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":80.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"We deployed a LighGBM Regression model and endpoint using Sagemaker Jumpstart. We have attempted to configure this endpoint as 'asynchronous' via the console. Receiving Error: ValidationException-Network Isolation is not supported when specifying an AsyncInferenceConfig.\n\nLooking at the model's network details the model has Enable Network Isolation set as 'True'. This was default output setting set by JumpStart.\n\nHow can we diasble Network Isolation to in order to make this endpoint asynchronous?",
        "Answers":[
            {
                "Answer_creation_time":"2022-05-20T05:18:58.005Z",
                "Answer_upvote_count":1,
                "Answer_body":"Vanilla SageMaker \"Models\" (as opposed to versioned ModelPackages) are immutable in the API with no \"UpdateModel\" action... But I think you should be able to create a new Model copying the settings of the current one.\n\nI'd suggest to:\n\nUse DescribeModel (via boto3.client(\"sagemaker\").describe_model(), assuming you're using Python) to fetch all the parameters of the existing JumpStart model such as the S3 artifact location and other settings\nUse CreateModel (create_model()) to create a new model with same configuration but network isolation disabled\nUse your new model to try and deploy an async endpoint\n\nProbably you'd find the low-level boto3 SDK more intuitive for this task than the high-level sagemaker SDK's Model class - because the latter does some magic that makes typical build\/train\/deploy workflows easier but can be less natural for hacking around with existing model definitions. For example, creating an SMSDK Model object doesn't actually create a Model in the SageMaker API, because deployment instance type affects choice of container image so that gets deferred until a .deploy() call or similar later.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS SageMaker - Extending Pre-built Container, Deploy Endpoint Failed. No such file or directory: 'serve'\"",
        "Question_creation_time":1664542013756,
        "Question_link":"https:\/\/repost.aws\/questions\/QUR-uTDaDsQBGjMoAUcsi2sQ\/aws-sage-maker-extending-pre-built-container-deploy-endpoint-failed-no-such-file-or-directory-serve",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":97.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"I am trying to deploy the SageMaker Inference Endpoint by extending the Pre-built image. However, it failed with \"FileNotFoundError: [Errno 2] No such file or directory: 'serve'\"\n\nMy Dockerfile\n\nARG REGION=us-west-2\n\n# SageMaker PyTorch image\nFROM 763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-ec2\n\nRUN apt-get update\n\nENV PATH=\"\/opt\/ml\/code:${PATH}\"\n\n# this environment variable is used by the SageMaker PyTorch container to determine our user code directory.\nENV SAGEMAKER_SUBMIT_DIRECTORY \/opt\/ml\/code\n\n# \/opt\/ml and all subdirectories are utilized by SageMaker, use the \/code subdirectory to store your user code.\nCOPY inference.py \/opt\/ml\/code\/inference.py\n\n# Defines inference.py as script entrypoint \nENV SAGEMAKER_PROGRAM inference.py\n\n\nCloudWatch Log From \/aws\/sagemaker\/Endpoints\/mytestEndpoint\n\n2022-09-30T04:47:09.178-07:00\nTraceback (most recent call last):\n  File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 20, in <module>\n    subprocess.check_call(shlex.split(' '.join(sys.argv[1:])))\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 359, in check_call\n    retcode = call(*popenargs, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 340, in call\n    with Popen(*popenargs, **kwargs) as p:\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 858, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 1704, in _execute_child\n    raise child_exception_type(errno_num, err_msg, err_filename)\nTraceback (most recent call last): File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 20, in <module> subprocess.check_call(shlex.split(' '.join(sys.argv[1:]))) File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 359, in check_call retcode = call(*popenargs, **kwargs) File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 340, in call with Popen(*popenargs, **kwargs) as p: File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 858, in __init__ self._execute_child(args, executable, preexec_fn, close_fds, File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 1704, in _execute_child raise child_exception_type(errno_num, err_msg, err_filename)\n\n2022-09-30T04:47:13.409-07:00\nFileNotFoundError: [Errno 2] No such file or directory: 'serve'",
        "Answers":[
            {
                "Answer_creation_time":"2022-10-01T09:28:00.773Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi, @holopekochan!\n\nThe serve script is installed by SageMaker PyTorch Inference Toolkit when you pip-install it in the Dockerfile.\n\nHowever, it's hard to say why it's not found in your container. Are you sure that you use the inference container, not training container, for your endpoint? If you go to the AWS Console > Amazon SageMaker > Models > your model, what ECR image it shows in Container 1 - Image?\n\nIt will be useful if you can share the code that you used to setup the SageMaker PyTorch estimator (if any) how you define your PyTorchModel and how you deploy() it.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-30T15:14:58.846Z",
                "Answer_upvote_count":0,
                "Answer_body":"Should use the Sagemaker image\n\n763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-sagemaker\n\n\ninstead of ec2\n\n763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-ec2",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Has SAS code ever been successfully ran on SageMaker?",
        "Question_creation_time":1596105364000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqMU2EBqGTDCMTPsB5rjNoQ\/has-sas-code-ever-been-successfully-ran-on-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":94.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Has SAS code ever been successfully ran on SageMaker?",
        "Answers":[
            {
                "Answer_creation_time":"2020-07-30T14:02:45.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I\u2019ve helped customers run SAS on a notebook through a kernel and that was their main use case, but we also showed them how they can containerize SAS. Worked well",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can you share success stories of AWS customers performing ML CI\/CD?",
        "Question_creation_time":1592577511000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwLq6HNRZSOK7ODKKc_lC3Q\/can-you-share-success-stories-of-aws-customers-performing-ml-ci-cd",
        "Question_topic":[
            "Developer Tools",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_tag":[
            "AWS CodePipeline",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":32.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I want to create simple templates for scientists so that they can fit their models easily into a continuous integration\/continuous delivery (CI\/CD) pipeline. I want to know about success stories of AWS customers performing CI\/CD on machine learning pipelines.",
        "Answers":[
            {
                "Answer_creation_time":"2020-06-19T15:15:42.000Z",
                "Answer_upvote_count":1,
                "Answer_body":"Amazon has released the [Amazon SageMaker Pipelines][1] that are the first purpose-built CI\/CD service for machine learning: [1]: https:\/\/aws.amazon.com\/sagemaker\/pipelines\/\n\nFor more information, see [New \u2013 Amazon SageMaker Pipelines brings DevOps capabilities to your machine learning projects] [2] [2]: https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-pipelines-brings-devops-to-machine-learning-projects\/\n\nAdditionally, we have a case-study where a customer created one on their own for model development using Airflow. For more information, see [NerdWallet uses machine learning on AWS to power recommendations platform][3] and [Using Amazon SageMaker to build a machine learning platform with just three engineers][4]. [3]: https:\/\/aws.amazon.com\/solutions\/case-studies\/nerdwallet-case-study\/ [4]: https:\/\/www.nerdwallet.com\/blog\/engineering\/machine-learning-platform-amazon-sagemaker\/",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker framework processor compatibility with sagemaker pipelines",
        "Question_creation_time":1652344291417,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbY_u2lSORnmomHzZsGOZAA\/sage-maker-framework-processor-compatibility-with-sagemaker-pipelines",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":201.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi all,\n\nI am asking if it's possible to use framework processor inside a sagemaker pipeline.\n\nI am asking because the to submit the source_dir for the framework processor, we have to do so when calling the .run() method, when wrapping the processor inside a sagemaker.workflow.steps.ProcessingStep, there isn't an available argument to specify the source_dir.\n\nThank you! Best, Ruoy",
        "Answers":[
            {
                "Answer_creation_time":"2022-05-12T19:17:46.859Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can do this with the latest version of the sagemaker sdk 2.89.0\n\nfrom sagemaker.workflow.pipeline_context import PipelineSession\n\nsession = PipelineSession()\n\ninputs = [\n    ProcessingInput(\n    source=\"s3:\/\/my-bucket\/sourcefile\", \n    destination=\"\/opt\/ml\/processing\/inputs\/\",),\n]\n\nprocessor = FrameworkProcessor(...)\n\nstep_args = processor.run(inputs=inputs, source_dir=\"...\")\n\nstep_sklearn = ProcessingStep(\n    name=\"MyProcessingStep\",\n    step_args=step_args,\n)",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Multi-file source_dir bundle with SM Training Compiler (distributed)",
        "Question_creation_time":1639669045329,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwcM0XER5TcOggtQ_5cfVPw\/multi-file-source-dir-bundle-with-sm-training-compiler-distributed",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Natural Language Processing",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":29.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I'm hoping to use SageMaker Training Compiler with a (Hugging Face Trainer API, PyTorch) program split across multiple .py files for maintainability. The job needs to run on multiple GPUs (although at the current scale, multi-device single-node would be acceptable).\n\nFollowing the docs, I added the distributed_training_launcher.py launcher script to my source_dir bundle, and passed in the true training script via a training_script hyperparameter.\n\n...But when the job tries to start, I get:\n\nTraceback (most recent call last):\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\nreturn _run_code(code, main_globals, None,\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 90, in <module>\nmain()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 86, in main\nxmp.spawn(mod._mp_fn, args=(), nprocs=args.num_gpus)\nAttributeError: module 'train' has no attribute '_mp_fn'\n\n\nAny ideas what might be causing this? Is there some particular limitation or additional requirement for training scripts that are written over multiple files?\n\nI also tried running in single-GPU mode (p3.2xlarge) instead - directly calling the train script instead of the distributed launcher - and saw the below error which seems to originate within TrainingArguments itself? Not sure why it's trying to call a 'tensorflow\/compiler' compiler when running in PT..?\n\nEDIT: Turns out the below error can be solved by explicitly setting n_gpus as mentioned on the troubleshooting doc - but that takes me back to the error message above\n\nFile \"\/opt\/ml\/code\/code\/config.py\", line 124, in __post_init__\nsuper().__post_init__()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 761, in __post_init__\nif is_torch_available() and self.device.type != \"cuda\" and (self.fp16 or self.fp16_full_eval):\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 975, in device\nreturn self._setup_devices\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1754, in __get__\ncached = self.fget(obj)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 918, in _setup_devices\ndevice = xm.xla_device()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 231, in xla_device\ndevices = get_xla_supported_devices(\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 137, in get_xla_supported_devices\nxla_devices = _DEVICES.value\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/utils\/utils.py\", line 32, in value\nself._value = self._gen_fn()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 19, in <lambda>\n_DEVICES = xu.LazyProperty(lambda: torch_xla._XLAC._xla_get_devices())\nRuntimeError: tensorflow\/compiler\/xla\/xla_client\/computation_client.cc:273 : Missing XLA configuration",
        "Answers":[
            {
                "Answer_creation_time":"2022-07-15T08:01:47.440Z",
                "Answer_upvote_count":0,
                "Answer_body":"Ahh I solved this a while ago and forgot to update -\n\nYes, the training script needs to define a _mp_fn (which can just execute the same code as gets run if __name__ == \"__main__\") and number of GPUs (at least the last time I checked - hopefully this could change in future) needs to be explicitly configured.\n\nFor my particular project the fix to enable SMTC on the existing job is available online here. For others would also suggest referring to the official SMTC example notebooks & scripts!",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker XGBoost Parquet Example Code Fails and Errors out. Bug?",
        "Question_creation_time":1648146766576,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqqbIbodsT42efRxxi1FLzw\/sage-maker-xg-boost-parquet-example-code-fails-and-errors-out-bug",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Model Building"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":117.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi, I'm trying to run the SageMaker XGBoost Parquet example linked here. I followed the exact same steps but using my own data. I uploaded my data, converted it to a pandas df. The train_df shape is (15279798, 32) while the test_df shape is (150848, 32). I then converted it to parquet files and uploaded it to an S3 bucket - per example instructions.\n\nMy error is as follows:\n\nFailure reason\nAlgorithmError: framework error: Traceback (most recent call last): File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_xgboost_container\/data_utils.py\", line 422, in _get_parquet_dmatrix_pipe_mode data = np.vstack(examples) File \"<__array_function__ internals>\", line 6, in vstack File \"\/miniconda3\/lib\/python3.7\/site-packages\/numpy\/core\/shape_base.py\", line 283, in vstack return _nx.concatenate(arrs, 0) File \"<__array_function__ internals>\", line 6, in concatenate ValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 32 and the array at index 1 has size 9 During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_trainer.py\", line 84, in train entrypoint() File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_xgboost_container\/training.py\", line 94, in main train(framework.tr\n\n\n\nBut I'm confused because the train and test are the same shape and I added no extra code. My code below:\n\n# requires PyArrow installed\ntrain.to_parquet(\"Xgb_train.parquet\")\ntest.to_parquet(\"Xgb_test.parquet\")\n\n%%time\nsagemaker.Session().upload_data(\n    \"Xgb_train.parquet\", bucket=bucket, key_prefix=prefix + \"\/\" + \"Ptrain\"\n)\n\nsagemaker.Session().upload_data(\n    \"Xgb_test.parquet\", bucket=bucket, key_prefix=prefix + \"\/\" + \"Ptest\"\n)\n\ncontainer = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-2\")\n\n%%time\nimport time\nfrom time import gmtime, strftime\n\njob_name = \"xgboost-parquet-example-training-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nprint(\"Training job\", job_name)\n\n# Ensure that the training and validation data folders generated above are reflected in the \"InputDataConfig\" parameter below.\n\ncreate_training_params = {\n    \"AlgorithmSpecification\": {\"TrainingImage\": container, \"TrainingInputMode\": \"Pipe\"},\n    \"RoleArn\": role,\n    \"OutputDataConfig\": {\"S3OutputPath\": bucket_path + \"\/\" + prefix + \"\/single-xgboost\"},\n    \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.m5.2xlarge\", \"VolumeSizeInGB\": 20},\n    \"TrainingJobName\": job_name,\n    \"HyperParameters\": {\n        \"max_depth\": \"5\",\n        \"eta\": \"0.2\",\n        \"gamma\": \"4\",\n        \"min_child_weight\": \"6\",\n        \"subsample\": \"0.7\",\n        \"objective\": \"reg:linear\",\n        \"num_round\": \"10\",\n        \"verbosity\": \"2\",\n    },\n    \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 3600},\n    \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": bucket_path + \"\/\" + prefix + \"\/Ptrain\",\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                }\n            },\n            \"ContentType\": \"application\/x-parquet\",\n            \"CompressionType\": \"None\",\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": bucket_path + \"\/\" + prefix + \"\/Ptest\",\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                }\n            },\n            \"ContentType\": \"application\/x-parquet\",\n            \"CompressionType\": \"None\",\n        },\n    ],\n}\n\n\nclient = boto3.client(\"sagemaker\", region_name=region)\nclient.create_training_job(**create_training_params)\nprint(client)\nstatus = client.describe_training_job(TrainingJobName=job_name)[\"TrainingJobStatus\"]\nprint(status)\nwhile status != \"Completed\" and status != \"Failed\":\n    time.sleep(60)\n    status = client.describe_training_job(TrainingJobName=job_name)[\"TrainingJobStatus\"]\n    print(status)",
        "Answers":[
            {
                "Answer_creation_time":"2022-03-24T19:11:38.277Z",
                "Answer_upvote_count":0,
                "Answer_body":"I just changed my bucket name and file names. It worked now.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Studio encountered an error when creating your project(github and codepipeline template)",
        "Question_creation_time":1658949790257,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOCKdskABQumCC7OnzBZR4g\/sagemaker-studio-encountered-an-error-when-creating-your-project-github-and-codepipeline-template",
        "Question_topic":[
            "Developer Tools",
            "Machine Learning & AI",
            "Management & Governance",
            "DevOps"
        ],
        "Question_tag":[
            "AWS CodePipeline",
            "Amazon SageMaker",
            "AWS CloudFormation",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":68.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I trying tutorial on \"MLOps template for model building, training, and deployment with third-party Git repositories using CodePipeline\". But I am getting error as shown in image",
        "Answers":[
            {
                "Answer_creation_time":"2022-07-28T07:44:00.283Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hello. It seems like you are having permission problems according to the snapshot you provided. If you head to the Cloudformation service, you will probably get a better understanding of where the tamplate is failing. Make sure to have followed the prerequisites and check out this section.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Create endpoint from Python",
        "Question_creation_time":1625083671000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUTyUMHH4QRDaMa6L24rhOMg\/create-endpoint-from-python",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":83.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"Hello,\n\nI have trained my model on sagemaker. I have deleted the endpoint, but I am keeping the model and the endpoint configuration which points to the model.\n\nFrom the sagemaker dashboard I am able to recreate the endpoint using the existing endpoint configuration. However I don't want to keep the endpoint on all the time, as I will use it only once a day for a few minutes.\n\nIs it possible to create in on demand from a Python script? I would assume that it is possible, but can't find how. Can someone point me in the right direction?\n\nRegards.",
        "Answers":[
            {
                "Answer_creation_time":"2021-07-22T16:36:17.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello hugoflores,\n\nYou can use SageMaker APIs - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DeleteEndpoint.html to delete the endpoint and https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpoint.html. to create an endpoint. This an be automated either using SageMaker Pipelines or a Lambda function.\n\nHere are a few resources towards that:\n\nhttps:\/\/awsfeed.com\/whats-new\/machine-learning\/build-a-ci-cd-pipeline-for-deploying-custom-machine-learning-models-using-aws-services\nhttps:\/\/github.com\/aws-samples\/aws-lambda-layer-create-script\nhttps:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1200\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\nhttps:\/\/www.sagemakerworkshop.com\/step\/deploymodel\/\n\nHTH,\n\nChaitanya",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-08-17T16:35:53.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks Chaitanya,\n\nI was able to create the endpoint using the \"create_endpoint\" method and following one of the links provided.\n\ndef create_endpoint(endpoint_name, config_name):\r\n    \"\"\" Create SageMaker endpoint with input endpoint configuration.\r\n    Args:\r\n        endpoint_name (string): Name of endpoint to create.\r\n        config_name (string): Name of endpoint configuration to create endpoint with.\r\n    Returns:\r\n        (None)\r\n    \"\"\"\r\n    try:\r\n        sagemaker.create_endpoint(\r\n            EndpointName=endpoint_name,\r\n            EndpointConfigName=config_name\r\n        )\r\n    except Exception as e:\r\n        print(e)\r\n        print('Unable to create endpoint.')\r\n        raise(e)\r\n\r\nname = 'name-of-the-endpoint'\r\nconfig = 'name-of-the-endpoint-config' #this one has to exist on the Endpoint configuration list on sagemaker\r\ncreate_endpoint(name, config)\n\nRegards\n\nEdited by: hugoflores on Aug 17, 2021 9:36 AM",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker training with FSx: what is \"directory_path\"",
        "Question_creation_time":1604678225000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCaemzfoDRIy9AgLRW8suqw\/sage-maker-training-with-f-sx-what-is-directory-path",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon FSx for Lustre",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":105.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"SageMaker can train on FSx data. One SageMaker SDK parameter for FSx training is directory_path. Where do we find that?",
        "Answers":[
            {
                "Answer_creation_time":"2020-11-06T16:13:42.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"FSx for Lustre is a file system that you can use to provide high performance for ML training workloads. The directory_path should point to the location on your file system where your dataset is stored.\n\nIn the example in the docs: directory_path='\/fsx\/tensorflow',\n\n\/fsx is the directory you define on your compute instances where you are mounting the file system \/tensorflow would represent a folder within the fsx directory\n\nIf you are using an S3-linked FSx for Lustre file system \/tensorflow would be a prefix within your S3-linked bucket.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to check smdistributed-modelparallel version?",
        "Question_creation_time":1660805972479,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsfpWY8CuRsiyHg_x7qyJzw\/how-to-check-smdistributed-modelparallel-version",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":31.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"According to the doc ( https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/smd_model_parallel_general.html ), there are different parameters depending on the version of smdistributed-modelparallel module \/ package. However, I am unable to find a way to check the version (e.g. via sagemaker python SDK) or just from the training container documentation (e.g. https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md#huggingface-training-containers ).\n\nAny idea?\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-18T07:32:42.126Z",
                "Answer_upvote_count":0,
                "Answer_body":"Have not yet found a programmatic way to check the version.\n\nHowever, for each DLC (Deep Learning Container) available at https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md , we can look at the corresponding docker build files.\n\nE.g. for PyTorch 1.10.2 with HuggingFace transformers DLC, the corresponding dockerfile is here: https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/huggingface\/pytorch\/training\/docker\/1.10\/py3\/cu113\/Dockerfile.gpu\n\nAnd we can see that the version: smdistributed_modelparallel-1.8.1-cp38-cp38-linux_x86_64.whl.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"confusion about PIPE mode when using S3 shard key",
        "Question_creation_time":1589363811000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU31DdUqtuQziixKTkPasZKw\/confusion-about-pipe-mode-when-using-s-3-shard-key",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":25.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nI am a little confused about whether S3 Shard key would work when using PIPE mode, here is a example:\n\nAssume I have:\n\n2 instance, each instance have 4 worker;\n\ndata: total 8 files with total size 8GB, each file is 1GB. Put them into 4 different S3 path, that means, each path has 2 files (2GB in total)\n\nIf I use PIPE mode, and s3_input using distribution='ShardedByS3Key', and create 4 channel (each channel mapping a s3 path, 2 files)\n\ntrain_s3_input_1 = sagemaker.inputs.s3_input(channel_1, distribution='ShardedByS3Key')\n\nQuestion:\n\nHow much data of each worker get to train, 1 file or 2 files? thanks",
        "Answers":[
            {
                "Answer_creation_time":"2020-05-13T22:11:20.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, SageMaker will replicate a subset of data (1\/n ML compute instances) on each ML compute instance that is launched for model training when you specify ShardedByS3Key. If there are n ML compute instances launched for a training job, each instance gets approximately 1\/n of the number of S3 objects. This applies in both File and Pipe modes. Keep this in mind when developing algorithms.\n\nTo answer your question: How much data of each worker get to train, 1 file or 2 files? 1 file each from the training channel.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deploy SageMaker model to IoT Greengrass in different account?",
        "Question_creation_time":1556295446000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJha7KbxOTXuhMRGbMYGC0g\/deploy-sage-maker-model-to-io-t-greengrass-in-different-account",
        "Question_topic":[
            "Internet of Things (IoT)",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS IoT Greengrass",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":38.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Is it possible to deploy a model created by SageMaker in one account to an IoT Greengrass device in a different account?",
        "Answers":[
            {
                "Answer_creation_time":"2019-04-29T09:20:54.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"For IoT Greengrass 1.x, this is possible but not trivial. From the console this is not possible, as you can only select buckets or SageMaker jobs from the same account, but you can refer to resources in other accounts if you use the CLI or the API.\n\nYou have to create a new Resource Definition Version with the correct data specifying the model resource and then add it to your group definition. For permissions in the source account, you must set up the S3 bucket policy to allow access from the destination account. For permissions in the destination account, you must update the IoT Greengrass service role policy to access the model resource in the source account.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker PIPE Mode vs FSx ?",
        "Question_creation_time":1579692074000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyS6bjxG4R4qtnrXzA3uSeg\/sage-maker-pipe-mode-vs-f-sx",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon FSx for Lustre",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":124.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi, SageMaker supports training data streaming via PIPE mode, and also reading from FSx distributed file system. Those options seem to provide same value: low latency, high throughput.\n\nWhat are the reasons for using one or the other?\nDo we have any benchmark of PIPE vs FSx for SageMaker, in terms of costs and speed?",
        "Answers":[
            {
                "Answer_creation_time":"2020-01-22T22:29:08.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I can think of the following scenarios\n\nPipemode cons\n\n** UPDATED**\n\nData Shuffling - In pipe mode you are working with streaming data and hence you cannot perform data shuffle operations unless you are prepared to shuffle within batches (as in wait to read a batch of records and shuffle within the batch in Pipe mode). Of if your data is distributed across multiples files, then you could use Sagemaker data shuffle to perform file level shuffle\n\nData readers - There are default data readers for pipemode that come with Tensorflow for formats like csv, tfrecord etc. But if you have custom data formats or using a different deep leaning framework, yYou would have to use custom data readers to deal with the raw bytes and understand the logical end of record. You could also use ml-io to see if any of the built-in pipe mode readers work for your usecase\n\nPIPE mode streams the data for each epoch from S3 and hence will be slower than FSX when you run a few epochs\n\nFSX:\n\nFSX works by lazy loading the s3 file and hence it has a start up delay but gets faster during repeated training.\n\nThere is no dependency on the framework and your existing code will work as is..\n\nThe only con of using FSX is the additional storage costs, but I would almost prefer FSX to pipe mode in most cases.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to specify target feature in Sagemaker XGBoost?",
        "Question_creation_time":1661221322082,
        "Question_link":"https:\/\/repost.aws\/questions\/QUoW_FqSbIQKW0MqNJLlA2AA\/how-to-specify-target-feature-in-sagemaker-xg-boost",
        "Question_topic":[
            "Machine Learning & AI",
            "Database"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Extract Transform & Load Data"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":26.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I am considering migrating a data science project from Datarobot to Sagemaker. I am familiar with writing Python and have been going through one of the tutorial Jupyter notebooks to see how to explore the data and to build and deploy and estimator. But, I cannot see how to specify the target feature. I have entirely numerical data in a csv file. One of the fields in that file is the intended target for estimation, the rest are information from which the estimate is to be made.\n\nHow do I specify the column that is to be estimated? The code I expect should have this is ...\n\ncontainer = sm.image_uris.retrieve(\"xgboost\", session.boto_region_name, \"1.5-1\")\n\nxgb = sm.estimator.Estimator(\n    container,\n    role,\n    instance_count=1,\n    instance_type=\"ml.m4.xlarge\",\n    output_path=\"s3:\/\/xxxxxx001\/\",\n    sagemaker_session=session,\n)\n\nxgb.set_hyperparameters(\n    max_depth=5,\n    eta=0.2,\n    gamma=4,\n    min_child_weight=6,\n    subsample=0.8,\n    verbosity=0,\n    num_round=100,\n)\ns3_input_train = TrainingInput(\n    s3_data=\"s3:\/\/xxxxxx001\/data.csv\", content_type=\"csv\"\n)\nxgb.fit({\"train\": s3_input_train})",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-23T23:42:54.174Z",
                "Answer_upvote_count":0,
                "Answer_body":"On a badly formatted page on the AWS documentation, I found a statement that - the CSV file must have no headers and the target field must be the first field. So, apparently, it is not possible to specify the target. So primitive, yeah?",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Studio projects in VpcOnly mode without internet access",
        "Question_creation_time":1615480055000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcyhpq1pxRTmtjkDRAh_MDA\/sage-maker-studio-projects-in-vpc-only-mode-without-internet-access",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":323.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"A customer is using SageMaker Studio in VpcOnly mode (VPC, protected subnets without internet access, NO NAT gateways). The all functionality is fine. However, when I try create a SageMaker projects - as described here, SageMaker Studio is unable to list the project templates (timeout and unspecified error) resulting in empty list of the available project templates.\n\nProjects are enabled for the users - as described here. The problem is with project creation.\n\nIs internet access (e.g. via NAT gateways) is needed for SageMaker projects?",
        "Answers":[
            {
                "Answer_creation_time":"2021-04-10T20:10:40.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Figured it out. SageMaker Studio projects need Service Catalog access and VPCE for com.amazonaws.${AWS::Region}.servicecatalog",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to verify that checkpoints work for SageMaker Spot Training?",
        "Question_creation_time":1584346040000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbvA_lGXgQ3CdPuEoImWQVw\/how-to-verify-that-checkpoints-work-for-sage-maker-spot-training",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":51.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nHow can we know that checkpoint works before launching a sagemaker spot training job? Is there a way to force a regular checkpoint to s3 instead of waiting for the SIGTERM?\n\ncheers",
        "Answers":[
            {
                "Answer_creation_time":"2020-03-16T08:20:40.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi olivier, If you enable Sagemaker checkpointing , it periodically saves a copy of the artifacts into S3. I have used this in pytorch and it works by checkpointing periodically and the blog on Managed Spot Training: Save Up to 90% On Your Amazon SageMaker Training Jobs also mentions the same\n\nTo avoid restarting a training job from scratch should it be interrupted, we strongly recommend that you implement checkpointing, a technique that saves the model in training at periodic intervals",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Multi Model EndPoint and Inference Data Capture feature",
        "Question_creation_time":1649794559479,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlAvpGSsISyqu0MyebgRJDA\/sage-maker-multi-model-end-point-and-inference-data-capture-feature",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":175.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Does Data Capture feature used for model monitor and analytics work with the multi model endpoint (one container).. we ran into an error. See error \" An error occurred (ValidationException) when calling the CreateEndPointConfig operation: Data Capture Feature is not supported with MultiModel mode\" Theoretically, it should work because it is calling the DataCaptureConfig:\n\nfrom sagemaker.model_monitor import DataCaptureConfig\n\nendpoint_name = 'your-pred-model-monitor-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()) print(\"EndpointName={}\".format(endpoint_name))\n\ndata_capture_config=DataCaptureConfig( enable_capture = True, sampling_percentage=100, destination_s3_uri=s3_capture_upload_path)",
        "Answers":[
            {
                "Answer_creation_time":"2022-04-13T13:39:17.628Z",
                "Answer_upvote_count":0,
                "Answer_body":"SageMaker multi-model endpoints do not have support for SageMaker Model monitor as of writing this answer. So the error is pointing to exactly that.\n\nHowever, if you are looking to implement data drift using sagemaker model monitor then you can do that my mimicking data capture config functionality by capturing inference input and prediction output and storing it in the format supported by Model Monitor. And then setup a customer monitoring container using the instructions listed https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-byoc-containers.html",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker taking an unexpectedly long time to download training data",
        "Question_creation_time":1540384039000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPpqUS0ckRXCHW0BXgxV5wQ\/sagemaker-taking-an-unexpectedly-long-time-to-download-training-data",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":654.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"My customer's 220 Gb of training data took 54 minutes for Sagemaker to download. This is a rate of only 70 MB\/s, which is unexpectedly slow. He is accessing the data in S3 from his p3.8xlarge instance through a private VPC endpoint, so the theoretical maximum bandwidth is 25 Gbps. Is there anything that can be done to speed up the download?\n\nHe started the Sagemaker training with the following function:\n\nestimator = Estimator(image_name, role=role, output_path=output_location, train_instance_count=1, train_instance_type='ml.p3.8xlarge', train_volume_size=300, train_max_run = 52460*60 , security_group_ids='sg-00f1529adc4076841')\n\nThe output was: 2018-10-18 23:27:15 Starting - Starting the training job... Launching requested ML instances...... Preparing the instances for training... 2018-10-18 23:29:15 Downloading - Downloading input data............ .................................................................... .................................................................... .................................................................... 2018-10-19 00:23:50 Training - Downloading the training image..\n\nDataset download took ~54mins",
        "Answers":[
            {
                "Answer_creation_time":"2018-10-27T06:48:20.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"How are they connect to S3? are they using a VPC endpoint \/ NAT? If they are using a VPC endpoint, My recommendation will be the open a support ticket, it's possible that support will be able to look at the network logs.\n\nAnother option for the customer is to use pipe input, pipe mode is recommended for large datasets, and it'll shorter their startup time because the data is being streamed instead of being downloaded to your training instances.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Data Capture does not write files",
        "Question_creation_time":1660135320930,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKWPP4eXTTZe5qIUDJAXnsQ\/sagemaker-data-capture-does-not-write-files",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Lambda",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":52.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I want to enable data capture for a specific endpoint (so far, only via the console). The endpoint works fine and also logs & returns the desired results. However, no files are written to the specified S3 location.\n\nEndpoint Configuration\n\nThe endpoint is based on a training job with a scikit learn classifier. It has only one variant which is a ml.m4.xlarge instance type. Data Capture is enabled with a sampling percentage of 100%. As data capture storage locations I tried s3:\/\/<bucket-name> as well as s3:\/\/<bucket-name>\/<some-other-path>. With the \"Capture content type\" I tried leaving everything blank, setting text\/csv in \"CSV\/Text\" and application\/json in \"JSON\".\n\nEndpoint Invokation\n\nThe endpoint is invoked in a Lambda function with a client. Here's the call:\n\nsagemaker_body_source = {\n            \"segments\": segments,\n            \"language\": language\n        }\npayload = json.dumps(sagemaker_body_source).encode()\nresponse = self.client.invoke_endpoint(EndpointName=endpoint_name,\n                                       Body=payload,\n                                       ContentType='application\/json',\n                                       Accept='application\/json')\nresult = json.loads(response['Body'].read().decode())\nreturn result[\"predictions\"]\n\n\nInternally, the endpoint uses a Flask API with an \/invocation path that returns the result.\n\nLogs\n\nThe endpoint itself works fine and the Flask API is logging input and output:\n\nINFO:api:body: {'segments': [<strings...>], 'language': 'de'}\n\nINFO:api:output: {'predictions': [{'text': 'some text', 'label': 'some_label'}, ....]}",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-16T13:26:08.966Z",
                "Answer_upvote_count":0,
                "Answer_body":"So the issue seemed to be related to the IAM role. The default role (ModelEndpoint-Role) does not have access to write S3 files. It worked via the SDK since it uses another role in the sagemaker studio. I did not receive any error message about this.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Redshift ML \/ SageMaker - Deploy an existing model artifact to a Redshift Cluster",
        "Question_creation_time":1609954586000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCMYCx28qRe-MOCIfj91Y2g\/redshift-ml-sage-maker-deploy-an-existing-model-artifact-to-a-redshift-cluster",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Redshift"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":68.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Is it possible to deploy an existing model artifact from SageMaker to Redshift ML?\n\nFor example, with an Aurora ML you can reference a SageMaker endpoint and then use it as a UDF in a SELECT statement. Redshift ML works a bit differently - when you call CREATE MODEL - the model is trained with SageMaker Autopilot and then deployed to the Redshift Cluster.\n\nWhat if I already have a trained model, can i deploy it to a Redshift Cluster and then use a UDF for Inference?",
        "Answers":[
            {
                "Answer_creation_time":"2021-01-06T17:52:12.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"As of January 30 2021, you can't deploy an existing model artifact from SageMaker to Redshift ML directly with currently announced Redshift ML preview features. But you can reference sagemaker endpoint through a lambda function and use that lambda function as an user defined function in Redshift.\n\nBelow would be the steps:\n\nTrain and deploy your SageMaker model in a SageMaker Endpoint.\nUse Lambda function to reference sagemaker endpoint.\nCreate a Redshift Lambda UDF referring above lambda function to run predictions.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What is the cluster manager in SageMaker Spark Processing?",
        "Question_creation_time":1602770746000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUShPm0t4vR4S8XBKMiAcA6g\/what-is-the-cluster-manager-in-sage-maker-spark-processing",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":81.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"SageMaker Processing can launch multi-instance jobs. What is the underlying cluster manager? Yarn? Mesos? Something custom?",
        "Answers":[
            {
                "Answer_creation_time":"2020-10-15T14:12:23.000Z",
                "Answer_upvote_count":1,
                "Answer_body":"The Spark container uses YARN - for ref the bootstrap script on github: https:\/\/github.com\/aws\/sagemaker-spark-container\/blob\/master\/src\/smspark\/bootstrapper.py and the Dockerfile with hadoop-yarn dependencies",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to install Phyton package in Jupyter Notebook instance in SageMaker?",
        "Question_creation_time":1592823369000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4pvReJNZS6eDLxhd4pK-tQ\/how-to-install-phyton-package-in-jupyter-notebook-instance-in-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":636.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nI want to use awswrangler package in my Jupyter Notebook instance of SageMaker.\n\nI understand that we have to use Lifecycle configuration. I tried to do it using the following script:\n\n#!\/bin\/bash\n\npip install awswrangler==0.2.2\n\n\nBut when I import that package into my Notebook:\n\nimport boto3                                      # For executing native S3 APIs\nimport pandas as pd                               # For munging tabulara data\nimport numpy as np                                # For doing some calculation\nimport awswrangler as wr\nimport io\nfrom io import StringIO\n\n\nI still get the following error:\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n<ipython-input-1-f3d85c7dd0f6> in <module>()\n      2 import pandas as pd                               # For munging tabulara data\n      3 import numpy as np                                # For doing some calculation\n----> 4 import awswrangler as wr\n      5 import io\n      6 from io import StringIO\n\nModuleNotFoundError: No module named 'awswrangler'\n\n\nAny documentation or reference on how to install certain package for Jupyter Notebook in SageMaker?",
        "Answers":[
            {
                "Answer_creation_time":"2020-06-22T13:32:04.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nexample how to use lifecycle config to install python package in one environment : https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-pip-package-single-environment\/on-start.sh\n\nand to all conda env - https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-pip-package-all-environments\/on-start.sh",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ModuleNotFoundError when starting a training job on Sagemaker",
        "Question_creation_time":1598912648000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJMd_4s52RpWXDXITXFsQdw\/module-not-found-error-when-starting-a-training-job-on-sagemaker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":357.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I want to submit a training job on sagemaker. I tried it on notebook and it works. When I try the following I get ModuleNotFoundError: No module named 'nltk'\n\nMy code is\n\nimport sagemaker  \nfrom sagemaker.pytorch import PyTorch\n\nJOB_PREFIX   = 'pyt-ic'\nFRAMEWORK_VERSION = '1.3.1'\n\nestimator = PyTorch(entry_point='finetune-T5.py',\n                   source_dir='..\/src',\n                   train_instance_type='ml.p2.xlarge' ,\n                   train_instance_count=1,\n                   role=sagemaker.get_execution_role(),\n                   framework_version=FRAMEWORK_VERSION, \n                   debugger_hook_config=False,  \n                   py_version='py3',\n                   base_job_name=JOB_PREFIX)\n\nestimator.fit()\n\n\nfinetune-T5.py have many other libraries that are not installed. How can I install the missing library? Or is there a better way to run the training job?",
        "Answers":[
            {
                "Answer_creation_time":"2020-08-31T22:47:15.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Check out this link (Using third-party libraries section) on how to install third-party libraries for training jobs. You need to create requirement.txt file in the same directory as your training script to install other dependencies at runtime.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker GroundTruth Interface - option to skip a task and then return",
        "Question_creation_time":1596055479000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_S8ylg4UQdKh76o3zp3dWQ\/sage-maker-ground-truth-interface-option-to-skip-a-task-and-then-return",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth"
        ],
        "Question_upvote_count":1.0,
        "Question_view_count":68.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Customer wants to configure the SageMaker Ground Truth interface seen by the workers such that the labeler can navigate to previous or next tasks. For example, if one is labelling images, they could skip the current image, label the next one, and then return to the skipped image. The Ground Truth interface does not seem to have this capability. Is there an option for it that I missed? I could not find anything about it here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-labeling.html.",
        "Answers":[
            {
                "Answer_creation_time":"2020-07-29T21:28:39.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Currently, there is no functionality to skip a task and go back to it later. However, you could add a field like\n\n[ ] this task was skipped\n\nwhere the annotator could check the box for those items to be reviewed and processed at another time.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Should SageMaker Canvas region and S3 region be the same?",
        "Question_creation_time":1658221373656,
        "Question_link":"https:\/\/repost.aws\/questions\/QULHZtj6HwQReouXor72UuSg\/should-sage-maker-canvas-region-and-s-3-region-be-the-same",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Canvas"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":56.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi, I'm going to use the canvas by connecting to S3. When using sagemaker canvas, should the canvas region and S3 region be the same? Thank you.",
        "Answers":[
            {
                "Answer_creation_time":"2022-07-19T12:35:55.411Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi, S3 does not have to be in the same region as SageMaker Canvas, but make sure your user has the correct permissions to access the bucket!",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Why does my kernal keep dying when I try to import Hugging Face BERT models to Amazon SageMaker?",
        "Question_creation_time":1604517955000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsO3sfUGpTKeHiU8W9k1Kwg\/why-does-my-kernal-keep-dying-when-i-try-to-import-hugging-face-bert-models-to-amazon-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":458.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"When I try to import Hugging Face BERT models to the conda_pytorch_p36 kernal of my Amazon SageMaker Notebook instance using the following pip command, the kernal always dies:\n\n! pip install transformers\n\n\nThe result is the same for Hugging Face BERT, RoBERTa, and GPT2 models on ml.c5.2xlarge and ml.c5d.4xlarge Amazon SageMaker instances.\n\nWhy is this happening, and how do I resolve the issue?",
        "Answers":[
            {
                "Answer_creation_time":"2020-11-04T22:05:35.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"This issue occurs when the latest sentence piece breaks. The workaround is to force install sentencepiece==0.1.91.\n\npip install sentencepiece==0.1.91",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Pipe Mode",
        "Question_creation_time":1590161458000,
        "Question_link":"https:\/\/repost.aws\/questions\/QURbsBp9m5TsqKWWDdP8VJyw\/sage-maker-pipe-mode",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":33.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Does SageMaker pipe mode serve as a cost saving measure? Or is is just faster than file mode but generally not much cheaper? The cost savings of it might be 1. no need to copy data to training instances and 2. training instances need less space. Are these savings generally significant for customers?",
        "Answers":[
            {
                "Answer_creation_time":"2020-05-22T16:44:47.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"To the best of my understanding, pipe mode decreases startup times, but frequently increases the bill.\n\nThe SageMaker billing starts after the data has been copied onto the container in File mode and control is transferred to the user script.\n\nReading the data in pipe mode starts after control is transferred, so the data transfer happens during the billable time.\n\nFurther the data is, to the best of my knowledge, not hitting the disk (EBS). This is fast, but also means that if you pass over your data multiple times, you have to re-read it again, on your dime (S3 requests and container wait times).\n\nPipe mode is still a good idea. For example if you have only few passes over the data and the data is rather large, so that it would not fit on an EBS volume.\n\nAlso, in PyTorch for example, data loading can happen in parallel. So while the GPU is chucking away on one batch, the CPUs load and prepare the data for the next batch.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can I limit the type of instances that data scientists can launch for training jobs in SageMaker?",
        "Question_creation_time":1603454458000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUd77APmdHTx-2FZCvZfS6Qg\/can-i-limit-the-type-of-instances-that-data-scientists-can-launch-for-training-jobs-in-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":426.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"We want to limit the types of instances that our data scientists can launch for running training jobs and hyperparameter tuning jobs in SageMaker. Is it possible to limit the instance size options available through SageMaker by using IAM policies, or another method? For example: Could we remove the ability to launch ml.p3.16xlarge instances?",
        "Answers":[
            {
                "Answer_creation_time":"2020-10-23T12:15:48.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yes, you can limit the types of instances that are available for your data scientists to launch in SageMaker by using an IAM policy similar to the following one:\n\nNote: This example IAM policy allows SageMaker users to launch only Compute Optimized (ml.c5)-type training jobs and hyperparameter tuning jobs.\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"EnforceInstanceType\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:CreateTrainingJob\",\n                \"sagemaker:CreateHyperParameterTuningJob\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"ForAllValues:StringLike\": {\n                    \"sagemaker:InstanceTypes\": [\"ml.c5.*\"]\n                }\n            }\n        }\n\n     ]\n}",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AutoPilot for Forecasting",
        "Question_creation_time":1595154416000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUu_IodOxiQ6eTu010AYb8pQ\/auto-pilot-for-forecasting",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":40.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi there,\n\nIHAC who asked a question regarding the possible use of AutoPilot for solving Forecasting problems. They don't have the knowledge to play with DeepAR and they are running tests in parallel with Amazon Forecast. Their questions are:\n\nIs it possible to use AutoPilot for Forecasting problems? (my answer would be yes, since regression problems can be solved by XGBoost, which also won a bunch of competitions on Forecasting)\nWhich kind of pre-processing should the customer do and which pre-processing is done by AutoPilot which could simplify transformation of data for solving forecasting? In particular: are there any transformation to be done on the timestamp column? Should we introduce lagged entries - or is it done by AutoPilot?\n\nThanks to those taking the time to answer these questions :)\n\nBest, Davide Gallitelli",
        "Answers":[
            {
                "Answer_creation_time":"2020-07-20T08:24:30.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Although it is possible to model the forecasting as a regression problem in Autopilot, there is no time-series capability built in Autopilot. So, you need to do the preprocessing tasks such as time-series windowing, lag differencing, etc. in order to generate the training\/test datasets for the autopilot experiment.\n\nAdditionally, time series forecasting usually requires a model which can detect the pattern in a sequence of features. So, services such as DeepAR or Amazon forecast provide better capabilities to address this challenge.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Did the SageMaker PyTorch deployment process change?",
        "Question_creation_time":1594979513000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFIru4hJ2TcWLi7CYt3mnuw\/did-the-sage-maker-py-torch-deployment-process-change",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":134.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Did the SageMaker PyTorch deployment process change?\n\nIt use to be the case that people needed to have a model.tar.gz in s3, and an inference script locally or in git. Now, it seems that the inference script must also be part of the model.tar.gz. This is new, right?\n\nFrom the docs, https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#for-versions-1-2-and-higher:\n\n*For PyTorch versions 1.2 and higher, the contents of model.tar.gz should be organized as follows:\n\nModel files in the top-level directory\nInference script (and any other source files) in a directory named code\/ (for more about the inference script, see The SageMaker PyTorch Model Server)\nOptional requirements file located at code\/requirements.txt (for more about requirements files, see Using third-party libraries)*\n\nThis may be confusing, because this new mode of deployment means that people creating the model artifact need to know in advanced how the inference is going to look like. The previous design, with separation of artifact and inference code, was more agile.",
        "Answers":[
            {
                "Answer_creation_time":"2020-07-17T10:23:38.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"When AWS Sample - BERT sample using torch 1.4 was published, advance knowledge of the inference seems to be necessary. If you use the PyTorch SageMaker SDK to create or deploy the model after it is trained, it automatically re-packages the model.tar.gz to include the code files and the inference files. As an example, when you use the following script, the model.tar.gz is repackaged so the contents of the src directory is automatically added to the code directory model.tar.gz, which initially only contains model files. You don't need to know the inference code in advance.\n\nfrom sagemaker.pytorch import PyTorchModel\nfrom sagemaker import get_execution_role\nrole = get_execution_role()\n\nmodel_uri = estimator.model_data\n\nmodel = PyTorchModel(model_data=model_uri,\n                     role=role,\n                     framework_version='1.4.0',\n                     entry_point='serve.py',\n                     source_dir='src')\n\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge')\n\nFor the older versions, you couldn't include additional files \/dependencies during inference unless you built a custom container. The source.tar.gz was only used during training.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Ask AWS SageMaker",
        "Question_creation_time":1660876307100,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCKKplP7ES22DuZf8QJ38JA\/ask-aws-sage-maker",
        "Question_topic":[
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":35.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Can we make a new code through the sagemaker studio?\nIn my computer, GPU is GTX2080ti model, so if I use AWS sagemaker for paid service, can I get better performance?\nHow much GPU performance can you improve compared to before?\nI want to proceed with object segmentation through AWS sagemaker, can I use the code I used through sagemaker studio?",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-25T11:35:31.608Z",
                "Answer_upvote_count":0,
                "Answer_body":"My apologies, I am not fully sure on all the questions. But let me still make an attempt to respond to see if it helps.\n\nYes, you can write your own custom code through SageMaker studio.\n\nThis may not be an apple to apple comparison. The main advantage in this context, is your able to scale out your training to multiple nodes and cores (if your underlying model supports that). Likewise you can scale out the deployment as well. Typically the studio notebook is backed by a lightweight EC2 instance, but there are a large range of EC2 instances for training on SageMaker. Please refer to the following links for further assistance. 1. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks-available-instance-types.html 2. https:\/\/aws.amazon.com\/ec2\/instance-types\/\n\nPlease refer to the response above for question # 2.\n\nDid you mean semantic segmentation? If yes, the answer is yes too.\n\nHope that helps!\n\nRegards, Punya",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Notebook Instance Types for SageMaker Studio",
        "Question_creation_time":1593107198000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOd5vfn4FRjGvGjac4d00PQ\/notebook-instance-types-for-sage-maker-studio",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":343.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Within SageMaker Studio, you can change instance types (see screenshots here:https:\/\/aws.amazon.com\/blogs\/machine-learning\/learn-how-to-select-ml-instances-on-the-fly-in-amazon-sagemaker-studio\/). However, this seems to only support changing to: ml.t3.medium, ml.g4dn.xlarge, ml.m5.large, and ml.c5.large.\n\nIs there a way to change to other instance types for SageMaker Studio? For SageMaker Notebook Instances, I know you can change to many other types of instances, but I am not sure how to do it for SageMaker Studio.",
        "Answers":[
            {
                "Answer_creation_time":"2020-06-25T18:02:17.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"The instance types you are seeing are Fast Launch Instances ( which are instance types designed to launch in under two minutes).\n\nIn order to see all the types of instances, click on the switch on top of the instance type list that says \"Fast Launch\", that should display the rest of available instances.\n\nHere is additional info about fast launch instances: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks.html\n\nHope it helps!",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker batch transform",
        "Question_creation_time":1532619204000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlefH1ni4QOaulUT4870D5g\/sagemaker-batch-transform",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":254.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi, it seems that Sagemaker Batch Transform is limited to 100MB payloads I'd like to run preds against a 5GB csv file, what the recommended way to do so?",
        "Answers":[
            {
                "Answer_creation_time":"2018-07-26T18:02:46.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"SageMaker Batch Transform will automatically split your input file into whatever payload size is specified if you use \"SplitType\": \"Line\" and \"BatchStrategy\": \"MultiRecord\". There's no need to split files yourself or to use large payload sizes unless you have very large single records.\n\nHope that helps!",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to feed seed code to GitHub Repository from Sagemaker Projects Organization Template created with Service Catalog?",
        "Question_creation_time":1657259337697,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_Y4T-A3aQySFeRr3feBscA\/how-to-feed-seed-code-to-git-hub-repository-from-sagemaker-projects-organization-template-created-with-service-catalog",
        "Question_topic":[
            "Developer Tools",
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "AWS CodeBuild",
            "Amazon SageMaker",
            "AWS CloudFormation",
            "AWS Service Catalog"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":169.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"The objective is to replicate \"MLOps template for model building, training, and deployment with third-party Git repositories using Jenkins\" builtin Sagemaker Project template. I want to feed custom seed code to the Github repository each time a project is created using my organization custom template instead of the default seed code that the builtin template feeds.\n\nI am able to create the custom template using service catalog but I could not find a solution for feeding the seed code to github repo. So, I decided to see how the built in project template is doing this and it is using resources from this bucket \"s3:\/\/sagemaker-servicecatalog-seedcode-us-east-1\/bootstrap\/GitRepositorySeedCodeCheckinCodeBuildProject-v1.0.zip\" but I could not access it. I am not sure how to achieve the objective?",
        "Answers":[
            {
                "Answer_creation_time":"2022-07-11T17:23:15.771Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can download the seed package using awscli s3 cp <s3_uri> <target_path> or by using this URL: https:\/\/sagemaker-servicecatalog-seedcode-us-east-1.s3.amazonaws.com\/bootstrap\/GitRepositorySeedCodeCheckinCodeBuildProject-v1.0.zip\n\nThis .zip is used by CodeBuild that is called when the template is deployed (by a lambda mapped to a CFN custom component). If you take a look in the template you'll find a component named \"SageMakerModelBuildSeedCodeCheckinProjectTriggerLambdaInvoker\". You can find some env vars defined for this component like: SEEDCODE_BUCKET_NAME and SEEDCODE_BUCKET_KEY. These vars point to an S3 uri that has another .zip file with the content of the seed for the git repo. If you get the default values defined there you can re-create the URL and download the .zip file as well: https:\/\/sagemaker-servicecatalog-seedcode-us-east-1.s3.amazonaws.com\/toolchain\/model-building-workflow-jenkins-v1.0.zip\n\nSo, in the end, if you want to change the content that is pushed to the git repo, you can redefine these 2 vars and point to an S3 path that contains a .zip file you created.\n\nBonus: If you're a curious person, I recommend you to take a look at the .java file (src\/main\/java\/GitRepositorySeedCodeBootStrapper.java) inside the .zip of the CodeBuild .zip for you to understand what it does to prepare the git repo like: download a .zip, unpack it, commit\/push to the git repo.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"In SageMaker Studio, how to decide on which instance to open a terminal?",
        "Question_creation_time":1606945520000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUo5ycye8jQ7Cw8dgSAfE9RQ\/in-sage-maker-studio-how-to-decide-on-which-instance-to-open-a-terminal",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":495.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nI was writing some notebook on a t2.Medium Studio Notebook. Now I just switched to an m5.8xlarge. However, when I launch a terminal, it still shows up only 2 CPUs, not the 32 I expected. How to open a terminal on that m5.8xlarge instance?",
        "Answers":[
            {
                "Answer_creation_time":"2020-12-03T11:09:16.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Where do you launch the terminal from? If you use the launcher window, it would start on the t2.medium as you are experiencing.\n\nHowever, if you use the launch terminal button in the toolbar that is displayed at the top of your notebook, it will launch the image terminal on the new instance the notebook's kernel is running on (your m5.8xlarge instance).",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to access file system in Sagemaker notebook instance from outside of that instance (ie via Python Sagemaker Estimator training call)",
        "Question_creation_time":1638914293851,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3yXAL7d7Sl--kKO3TTZf1g\/how-to-access-file-system-in-sagemaker-notebook-instance-from-outside-of-that-instance-ie-via-python-sagemaker-estimator-training-call",
        "Question_topic":[
            "Machine Learning & AI",
            "Storage"
        ],
        "Question_tag":[
            "Build & Train ML Models",
            "Amazon SageMaker",
            "Storage"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":690.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nI have large image dataset stored in a Sagemaker notebook instance, in the file system. I was hoping to learn how I could access this data from outside of that particular notebook instance. I have done quite a bit of researching but can't seem to find much - I am relatively new to this.\n\nI want to be able to access the data in that notebook in a fast manner as I will be using the data to train an AI model. Is there any recommended way to do this?\n\nI originally uploaded the data within that notebook instance to train a model within that instance in exactly the same file system. Note that it is a reasonably large dataset which I had to do some preprocessing on within Sagemaker.\n\nWhat is the best way to store data when using the Sagemaker estimators from training AI models?\n\nMany thanks\n\nTim",
        "Answers":[
            {
                "Answer_creation_time":"2021-12-07T22:53:56.668Z",
                "Answer_upvote_count":2,
                "Answer_body":"Hi Tim, when you create a sagemaker training job using the estimator, the general best practice is to store your data on S3 and the training job will launch instances as requested by the training job configuration. As now we support fast file mode, which allows faster training job start compared to the file mode (which downloads the data from s3 to the training instance). But when you say you used sagemaker notebook instance to train the model, I assume you were not using SageMaker Training jobs but rather running the notebook (.ipynb) on the SageMaker notebook instance. Please note that as SageMaker is a fully managed service, the notebook instance (also training instances, hosting instances etc.) are launched in the service account, so you will not have directly access to those instance. The SageMaker notebook instance use EBS to store data and the EBS volume is mounted to the \/home\/ec2-user\/SageMaker. Please note that the EBS volume used by a SageMaker notebook instance can only be increased but not decrease. If you want to reduce the EBS volume, you need to create a new notebook instance with a smaller volume and move your data from the previous instance via s3. You will not be able to access that EBS volume from outside of the SageMaker notebook instance. The general best practice is to store large dataset on s3 and only use sample data on the SageMaker notebook instance (reduce the storage). Then use that small amount of sample data to test\/build your code. Then when you are ready to train on the whole dataset, you can launch a SageMaker training job and use the whole dataset stored on s3. Note that, running the training on the whole dataset on a SageMaker notebook instance will require you to use a big instance with enough computing power and also will not be able to perform distributed training with multiple instances. Comparatively, if you run the training job use SageMaker training instances, it gives you more flexibility of choosing the instance type and allow you to run on multiple instances for distributed training. Lastly, once the SageMaker training job is done, all the resources will be terminated which will save cost compared to continue using the big instance with a SageMaker notebook instance. Hope this has helped answer your question",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Experiments Deletion Help Needed",
        "Question_creation_time":1668037196721,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFMzl26gfQna8sAZcCDJw_Q\/sage-maker-experiments-deletion-help-needed",
        "Question_topic":[
            "Machine Learning & AI",
            "AWS Well-Architected Framework",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Security",
            "AWS Account Management"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":44.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi Friends,\n\nI have deleted everything in Sagemaker - but support is asking me to delete the experiments that are still in my account : they sent me a link to follow\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/experiments-cleanup.html\n\nbut I have no idea how to complete this task -- does anyone know in terms that someone who has no idea what this means - can follow and achieve this task\n\nyou have no idea how much it would mean to me for any assistance",
        "Answers":[
            {
                "Answer_creation_time":"2022-11-10T09:00:37.516Z",
                "Answer_upvote_count":1,
                "Answer_body":"You need to use a computer with Python, the SageMaker SDK installed, and AWS credentials with enough permissions for that account configured. If you are already using SageMaker Studio, that should work.\n\nUse the second method. Create a file (Menu File -> New -> Python File). Rename it as cleanup_experiments.py(right click on the file on top and select Rename Python File), then paste the code in the documentation (those three sections, one after another). Save the file and open a terminal (Menu File -> New -> Terminal). Navigate to the directory where you saved the file and execute the command python cleanup_experiments.py",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Closing up a Sagemaker user profile - intended behavior?",
        "Question_creation_time":1640092026826,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmSJa7T1nRm6PQkiXDxZJqA\/closing-up-a-sagemaker-user-profile-intended-behavior",
        "Question_topic":[
            "Machine Learning & AI",
            "Cloud Financial Management"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Billing"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":529.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I had a Sagemaker user I wasn't using, so I tried to delete it and initially came across this tutorial: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-delete-domain.html . As part of the larger process for how to delete a domain, it shows how to delete any user profiles within that domain. The steps are:\n\nChoose the user.\nOn the User Details page, for each non-failed app in the Apps list, choose Delete app.\nOn the Delete app dialog, choose Yes, delete app, type delete in the confirmation field, and then choose Delete.\nWhen the Status for all apps show as Deleted, choose Delete user.\n\nThe problem comes on the final step: I wasn't able to find a \"Delete user\" button. This feels like a bug, because without such a button the only way to stop charges on a Sagemaker user is to use the CLI, which I eventually did. You can only delete the domain if you have deleted all users, meaning it only works using the CLI for that as well. For every other AWS service I've used, there is an easy way to delete everything from the GUI.",
        "Answers":[
            {
                "Answer_creation_time":"2021-12-21T13:51:27.751Z",
                "Answer_upvote_count":0,
                "Answer_body":"Try clicking on the user, then Edit, and then Delete? I don't remember if that is the exact flow, but I do know that you can do it in the GUI. I've done it a few times.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Does Ground Truth Support Circles?",
        "Question_creation_time":1595625000000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqVY0A3PIQsuJpcce1tjJcA\/does-ground-truth-support-circles",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":27.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Customer has circular objects in their data. Does Ground Truth support drawing circles rather than boxes out of the box (no pun intended)? I know that it supports semantic segmentation, but that is overkill in this case.",
        "Answers":[
            {
                "Answer_creation_time":"2020-07-24T21:37:59.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"As of July 2020, we currently have Crowd HTML Element support for bounding box and polygons.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can Amazon SageMaker endpoints be fitted with multiple Amazon Elastic Inference accelerators?",
        "Question_creation_time":1604477936000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwU8IHcSVQ3eH9-fGx0KZCA\/can-amazon-sage-maker-endpoints-be-fitted-with-multiple-amazon-elastic-inference-accelerators",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Elastic Inference"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":39.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Can Amazon SageMaker endpoints be fitted with multiple Amazon Elastic Inference accelerators?\n\nI see that in EC2 it's possible, however I don't see it mentioned in Amazon SageMaker documentation.",
        "Answers":[
            {
                "Answer_creation_time":"2020-11-04T16:17:19.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"No, they cant be; multi-attach is only supported with EC2.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to automate sagemaker batch transform?",
        "Question_creation_time":1649085888036,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyENAstk3Q_--wYwScAIq-A\/how-to-automate-sagemaker-batch-transform",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS CloudFormation",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":546.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"does cloudformation support sagemaker batch transform? if yes, can the jobs be triggered\/run automatically once the stack is created?",
        "Answers":[
            {
                "Answer_creation_time":"2022-04-04T16:24:21.868Z",
                "Answer_upvote_count":1,
                "Answer_body":"While CloudFormation doesn't currently offer a resource for a SageMaker Batch Transform (resource list here in the docs), there are plenty of other integration points to automate running these jobs.\n\nCloudFormation\n\nI'd actually argue that CloudFormation is probably not a great fit for this anyway because CloudFormation defines resources which can be created, updated, and deleted. I could maybe see a correspondence between \"Create\" = \"Run a job\", maybe \"Delete\" = \"Delete job outputs\", and possibly \"Update\" = \"Re-run the job\"? But these are opinionated choices that might not make sense in every case.\n\nIf you really wanted, you could create a Custom CloudFormation resource backed by an AWS Lambda function using the CreateTransformJob API (via whatever language you prefer e.g. boto3 in Python).\n\nNote that:\n\nIf you wanted to use the SageMaker Python SDK (import sagemaker, Transformer, etc) instead of the low-level boto3 interface in Python - you'd need to install this extra library in your Lambda function. Tools like AWS SAM and CDK can help with this.\nThe maximum Lambda timeout is 15 minutes, you may not want to keep your Lambda function running (billable) just waiting for the transform to complete anyway, and even the overall Custom Resource will have a longer max timeout within which it must stabilize after a create\/update\/delete request... So additional orchestration may be required beyond a single synchronous Lambda function call.\nOther (better?) options\n\nAs mentioned above, you can create, describe and stop SageMaker Batch Transform jobs from any environment where you're able to call AWS APIs \/ use AWS SDKs... And you can even use the high-level open-source sagemaker SDK from anywhere you install it. Interesting options might include:\n\nAmazon SageMaker Pipelines: SageMaker Pipelines have native \"steps\" for a range of SageMaker processes, including transform jobs but also training, pre-processing and more. You can define a multi-step pipeline from the SageMaker Python SDK (in your notebook or elsewhere) and then start it running on-demand (with parameters) by calling the StartPipelineExecution API.\nAWS Step Functions: Step Functions provides general-purpose serverless orchestration so while the orchestration for SageMaker jobs in particular might be a little more complex (one step to start the job, then a polling check to check wait for completion) - the visual workflow editor and range of integrations to other services may be useful.\nAmazon S3 Lambda integrations can trigger an event automatically (to start your transform job) when new data is uploaded to Amazon S3.\nScheduled EventBridge Rules can run actions on a regular schedule (such as calling Lambda functions, kicking off these pipelines, etc) - in case you need a schedule-based execution rather than in response to some event.\n\nThe choice will depend on what the initial trigger for your workflow would be (schedule? Data upload? Some other AWS event? An API call from outside AWS?) and what other steps need to be orchestrated as well as your transform job in the overall flow.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"adjusting sagemaker xgboost project to tensorflow (or even just different folder name)",
        "Question_creation_time":1664391665708,
        "Question_link":"https:\/\/repost.aws\/questions\/QUAL9Vn9abQ6KKCs2ASwwmzg\/adjusting-sagemaker-xgboost-project-to-tensorflow-or-even-just-different-folder-name",
        "Question_topic":[
            "Developer Tools",
            "Machine Learning & AI",
            "Containers",
            "DevOps"
        ],
        "Question_tag":[
            "AWS CodePipeline",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Containers",
            "DevOps"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":46.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I have sagemaker xgboost project template \"build, train, deploy\" working, but I'd like to modify if to use tensorflow instead of xgboost. First up I was just trying to change the abalone folder to topic to reflect the data we are working with.\n\nI was experimenting with trying to change the topic\/pipeline.py file like so\n\n    image_uri = sagemaker.image_uris.retrieve(\n        framework=\"tensorflow\",\n        region=region,\n        version=\"1.0-1\",\n        py_version=\"py3\",\n        instance_type=training_instance_type,\n    )\n\n\ni.e. just changing the framework name from \"xgboost\" to \"tensorflow\", but then when I run the following from a notebook:\n\nfrom pipelines.topic.pipeline import get_pipeline\n\n\npipeline = get_pipeline(\n    region=region,\n    role=role,\n    default_bucket=default_bucket,\n    model_package_group_name=model_package_group_name,\n    pipeline_name=pipeline_name,\n)\n\n\nI get the following error\n\nValueError                                Traceback (most recent call last)\n<ipython-input-5-6343f00c3471> in <module>\n      7     default_bucket=default_bucket,\n      8     model_package_group_name=model_package_group_name,\n----> 9     pipeline_name=pipeline_name,\n     10 )\n\n~\/topic-models-no-monitoring-p-rboparx6tdeg\/sagemaker-topic-models-no-monitoring-p-rboparx6tdeg-modelbuild\/pipelines\/topic\/pipeline.py in get_pipeline(region, sagemaker_project_arn, role, default_bucket, model_package_group_name, pipeline_name, base_job_prefix, processing_instance_type, training_instance_type)\n    188         version=\"1.0-1\",\n    189         py_version=\"py3\",\n--> 190         instance_type=training_instance_type,\n    191     )\n    192     tf_train = Estimator(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/utilities.py in wrapper(*args, **kwargs)\n    197                 logger.warning(warning_msg_template, arg_name, func_name, type(value))\n    198                 kwargs[arg_name] = value.default_value\n--> 199         return func(*args, **kwargs)\n    200 \n    201     return wrapper\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/image_uris.py in retrieve(framework, region, version, py_version, instance_type, accelerator_type, image_scope, container_version, distribution, base_framework_version, training_compiler_config, model_id, model_version, tolerate_vulnerable_model, tolerate_deprecated_model, sdk_version, inference_tool, serverless_inference_config)\n    152             if inference_tool == \"neuron\":\n    153                 _framework = f\"{framework}-{inference_tool}\"\n--> 154         config = _config_for_framework_and_scope(_framework, image_scope, accelerator_type)\n    155 \n    156     original_version = version\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/image_uris.py in _config_for_framework_and_scope(framework, image_scope, accelerator_type)\n    277         image_scope = available_scopes[0]\n    278 \n--> 279     _validate_arg(image_scope, available_scopes, \"image scope\")\n    280     return config if \"scope\" in config else config[image_scope]\n    281 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/image_uris.py in _validate_arg(arg, available_options, arg_name)\n    443             \"Unsupported {arg_name}: {arg}. You may need to upgrade your SDK version \"\n    444             \"(pip install -U sagemaker) for newer {arg_name}s. Supported {arg_name}(s): \"\n--> 445             \"{options}.\".format(arg_name=arg_name, arg=arg, options=\", \".join(available_options))\n    446         )\n    447 \n\nValueError: Unsupported image scope: None. You may need to upgrade your SDK version (pip install -U sagemaker) for newer image scopes. Supported image scope(s): eia, inference, training.\n\n\nI was skeptical that the upgrade suggested by the error message would fix this, but gave it a try:\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npipelines 0.0.1 requires sagemaker==2.93.0, but you have sagemaker 2.110.0 which is incompatible.\n\n\nSo that seems like I can't upgrade sagemaker without changing pipelines, and it's not clear that's the right thing to do - like this project template may be all designed around those particular ealier libraries.\n\nBut so is it that the \"framework\" name should be different, e.g. \"tf\"? Or is there some other setting that needs changing in order to allow me to get a tensorflow pipeline ...?\n\nHowever I find that if I use the existing abalone\/pipeline.py file I can change the framework to \"tensorflow\" and there's no problem running that particular step in the notebook.\n\nI've searched all the files in the project to try and find any dependency on the abalone folder name, and the closest I came was in codebuild-buildspec.yml but that hasn't helped.\n\nHas anyone else successfully changed the folder name from abalone to something else, or am I stuck with abalone if I want to make progress?\n\nMany thanks in advance\n\np.s. is there a slack community for sagemaker studio anywhere?\n\np.p.s. I have tried changing all instances of the term \"Abalone\" to \"Topic\" within the topic\/pipeline.py file (matching case as appropriate) to no avail\n\np.p.p.s. I discovered that I can get an error free run of getting the pipeline from a unit test:\n\nimport pytest\n\nfrom pipelines.topic.pipeline import *\n\nregion = 'eu-west-1'\nrole = 'arn:aws:iam::398371982844:role\/SageMakerExecutionRole'\ndefault_bucket = 'sagemaker-eu-west-1-398371982844'\nmodel_package_group_name = 'TopicModelPackageGroup-Example'\npipeline_name = 'TopicPipeline-Example'\n\ndef test_pipeline():\n    pipeline = get_pipeline(\n        region=region,\n        role=role,\n        default_bucket=default_bucket,\n        model_package_group_name=model_package_group_name,\n        pipeline_name=pipeline_name,\n    )\n\n\nand strangely if I go to a different copy of the notebook, everything runs fine, there ... so I have two seemingly identical ipynb notebooks, and in one of them when I switch to trying to get a topic pipeline I get the above error, and in the other, I get no error at all, very strange\n\np.p.p.p.s. I also notice that conda list returns very different results depending on whether I run it in the notebook or the terminal ... but the conda list results are identical for the two notebooks ...",
        "Answers":[
            {
                "Answer_creation_time":"2022-09-29T11:24:21.266Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi! I see two parts in your question:\n\nHow to use Tensorflow in a SageMaker estimator to train and deploy a model\nHow to adapt a SageMaker MLOps template to your data and code\n\nTensorflow estimator is slightly different from XGBoost estimator, and the easiest way to work with it is not by using sagemaker.image_uris.retrieve(framework=\"tensorflow\",...), but to use sagemaker.tensorflow.TensorFlow estimator instead.\n\nThese are the two examples, which will be useful for you:\n\nTrain an MNIST model with TensorFlow\nDeploy a Trained TensorFlow V2 Model\n\nAs for updating the MLOps template, I recommend you to go through the comprehensive self-service lab on SageMaker Pipelines.\n\nIt shows you how to update the source directory from abalone to customer_churn. In your case it will be the topic.\n\nP. S. As for a Slack channel, to my best knowledge, this re:Post forum now is the best place to ask any questions on Amazon SageMaker, including SageMaker Studio.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"[AI\/ML] Data acquisition and preprocessing",
        "Question_creation_time":1607357917000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUebPx1UeWSGOb_3i0TXlBWA\/ai-ml-data-acquisition-and-preprocessing",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Machine Learning & AI",
            "Analytics",
            "Database"
        ],
        "Question_tag":[
            "AWS Lambda",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Glue",
            "Amazon DynamoDB",
            "Amazon Redshift"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":38.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nCustomer who loads the e-bike data to S3 wants to get AI\/ML insight from sensor data. The e-bike sensor data are size about 4KB files each and posted in S3 buckets. The sensor data is put into format like this\n\ntimestamp1, sensorA, sensorB, sensorC, ..., sensorZ timestamp2, sensorA, sensorB, sensorC, ..., sensorZ timestamp3, sensorA, sensorB, sensorC, ..., sensorZ ...\n\nThen these sensor data are put into one file about 4KB size.\n\nThe plan I have is to\n\nRead S3 objects\nParse S3 object with Lambda. I thought about Glue but wanted to put data in DynamoDB where Glue does not seem to support. Also, Glue seems to be more expensive.\nPut the data in DynamoDB with bike ID as primary key and timestamp as sort key.\nUse SageMaker to learn with the DynamoDB data. There will be separate discussion on choosing which model and making time-series inferencing.\nIf we need to re-learn, it will use the DynamoDB data, not from S3. I think it will be faster to get data from DynamoDB instead from the raw S3 data.\nAlso, I think we can filter out some bad input or apply little modification to DynamoDB data (shifting time stamps to the correct time, etc.)\nMake inferencing output based on the model.\n\nWhat do you think? Would you agree? Would you approach the problem differently? Would you rather learn from S3 directly via Athena or direct S3 access? Or would you rather use Glue and Redshift? But the data about 100MB would be sufficient to train the model we have in mind. Glue and Redshift maybe overkill. Currently, Korea region does not support Timestream database. So, time series database closest in Korea could be DynamoDB.\n\nPlease share your thoughts.\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_time":"2020-12-07T17:41:59.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thoughts about DynamoDB\n\nPer GB, DynamoDB is around 5X more cost per GB of data stored. On top of that, you have RCU\/WCU cost.\n\nI would recommend keeping data in S3. Not only is it more cost effective, but with S3, you do not have to worry about RCU\/WCU cost or throughput of DynamoDB.\n\nSageMaker notebooks and training instances can read directly from S3, and S3 has high-throughput. I don't think you will have a problem with 100 MB datasets.\n\nIf you need to prep\/transform your data, you can do the transformations \"in place\" in S3 using Glue, Athena, Glue DataBrew, GlueStudio, etc.\n\nGlue and DynamoDB\n\nI thought about Glue but wanted to put data in DynamoDB where Glue does not seem to support.\n\nGlue supports both Python and Spark jobs. If you use a Glue Python job, you can import the boto3 (AWS SDK) library and write to DynamoDB.\n\nOther strategies\n\nHow is your customer ingesting the sensor data \/ how is it being written to S3? Are they using AWS IoT Core?\n\nRegardless, the pattern you've described thus far is:\n\nDevice -> Sensor data in S3 -> Transform with Lambda -> store data in DynamoDB\n\nAn alternative approach you could consider is using Kinesis Firehose with Lambda transformations. This will allow you to do \"in-line\" parsing \/ transformation of your data before it is ever written to S3, this removing the need to re-read the data from S3 and apply transformations after the fact. Firehose also allows you to write the stored data in formats such as Parquet, which can help with cost and subsequent query performance.\n\nIf you want to store both raw data and transformed data, you can use a \"fanout\" pattern with Kinesis Streams\/Firehose, where one output is raw data to S3 and the other is a transformed stream.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker and Data on Databases",
        "Question_creation_time":1533317474000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUh_P30-iXTKmzZv0D4vtLOA\/sagemaker-and-data-on-databases",
        "Question_topic":[
            "Storage",
            "Analytics",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon Simple Storage Service",
            "AWS Data Pipeline",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":208.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"A customer has a question about data sources\n\n\u201cmost of our data is stored in SQL databases, while the SageMaker docs say that I have to put it all in S3. It\u2019s not obvious what the best way to do this is. I can think for example of splitting my analysis code in two; one pre-processing step to go from SQL queries to tabular data, and e.g. store that as Parquet files. For high-dimensional tensor data it\u2019s even less obvious.\u201d\n\nCan someone comment on that?",
        "Answers":[
            {
                "Answer_creation_time":"2018-08-03T17:52:46.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"We have an example notebook for interacting from Redshift data from a SageMaker managed notebook, which I believe is suitable for an Exploratory Data Analysis (EDA) use-case: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/working_with_redshift_data\/working_with_redshift_data.ipynb\n\nFor production purposes, the customer should consider separating the job of first extracting data from relational databases to S3 (to build out a data lake), and then using that for downstream processing\/machine learning (including SageMaker, EMR, Athena, Spectrum, etc.). Customers can build extraction pipelines from popular relational databases using AWS Glue, EMR, or their preferred ETL engines like those on the AWS Marketplace.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-02-12T01:50:00.007Z",
                "Answer_upvote_count":0,
                "Answer_body":"I'd recommend using SageMaker Data Wrangler to connects the dots of different SageMaker services. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/data-wrangler-import.html",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SparkR not working",
        "Question_creation_time":1591029652000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqyiKb_XvRhGxm1RxwjXhJQ\/spark-r-not-working",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon Elastic MapReduce"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":44.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I am trying to control a Spark cluster (using SparkR) from a Sagemaker notebook. I followed these instructions closely: https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/ and got it to work.\n\nToday when I try to run the SageMaker notebook (using the exact same code as before) I inexplicably get this error:\n\nAn error was encountered:\n[1] \"Error in callJMethod(sparkSession, \\\"read\\\"): Invalid jobj 1. If SparkR was restarted, Spark operations need to be re-executed.\"\n\n\nDoes anyone know why this is? I terminated the SparkR kernel and am still getting this error.",
        "Answers":[
            {
                "Answer_creation_time":"2020-06-01T19:56:15.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"You cannot have multiple SparkContexts in one JVM. The issue is resolved as WON'T FIX. You have to stop the spark session which spawned the sparkcontext (which you have already done).\n\nsparkR.session.stop()\n\nhttps:\/\/issues.apache.org\/jira\/browse\/SPARK-2243",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can I use SageMaker Autopilot for my time-series modeling?",
        "Question_creation_time":1601671292000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxaXaoPpqRyGdkh6aKK9uew\/can-i-use-sage-maker-autopilot-for-my-time-series-modeling",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":166.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I'm working on time-series modeling. I'm comparing battle-of-algorithms against the autopilot machine learning approach to identify the model that best fits my use case. I understand that Amazon SageMaker Autopilot doesn't work with time series. Is there an alternative library or algorithm in the AWS ecosystem that implements battle-of-algorithms for time series?",
        "Answers":[
            {
                "Answer_creation_time":"2020-10-05T07:31:28.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Amazon SageMaker Autopilot currently supports regression, binary classification, and multi-class classification. SageMaker supports only tabular data formatted in files with comma-separated values. For more information, see Automate model development with Amazon SageMaker Autopilot.\n\nFor your use case, you can use Amazon Forecast. Amazon Forecast includes the AutoML feature that trains different models with your target time series, related time series, and item metadata. Amazon Forecast then uses the model with the best accuracy metrics.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"ClientError: object_detection_augmented_manifest_training template",
        "Question_creation_time":1547563664000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjk2-AZ6VQl68Zdm1Owq-_A\/client-error-object-detection-augmented-manifest-training-template",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":52.0,
        "Question_answer_count":4,
        "Question_has_accepted_answer":true,
        "Question_body":"Hello,\n\nMy aim is to create a model for garden birds. I have 293 photos of birds that I have put through 2 custom labelling jobs in ground truth for training and validation. The issue I encountered was being able to have multiple labels on the bounding box which I managed to do via creating a custom labelling job with the following labels:\n\n<crowd-bounding-box\r\n    name=\"annotatedResult\"\r\n    labels=\"['Blackbird', 'Blue tit', 'Coal tit', 'Dunnock', 'Great tit', 'Long-tailed tit', 'Nuthatch', 'Pigeon', 'Robin']\" .....\n\n\nI now have 2 output manifest files with many lines of this:\n\n{\"source-ref\":\"s3:\/\/XXXXX\/Blackbird_1.JPG\",\"BirdLabel\":{\"workerId\":\"privateXXXXX\",\"imageSource\":{\"s3Uri\":\"s3:\/\/XXXXX\/Blackbird_1.JPG\"},\"boxesInfo\":{\"annotatedResult\":{\"boundingBoxes\":[{\"width\":1619,\"top\":840,\"label\":\"Blackbird\",\"left\":1287,\"height\":753}],\"inputImageProperties\":{\"width\":3872,\"height\":2592}}}},\"BirdLabel-metadata\":{\"type\":\"groundtruth\/custom\",\"job-name\":\"birdlabel\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-01-10T15:41:52+0000\"}}\n\n\nAfter this job was successful, I made an ml.p3.2xlarge instance, using the object_detection_augmented_manifest_training template.\n\nI have filled in the necessary sections, I then run it and received this error when I have the Content Type to 'application\/x-image' with Record wrapper type:RecordIO : 'ClientError: train channel is not specified.'\n\nI then changed the channel to train_annotation instead of train and I receive this error message: \"ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)\\n\\nCaused by: u'train' is a required property\n\nAdditional information can be provided if neccessary.\nAny help would be much apreciated! Thank you.\n\nEdited by: LuciA on Jan 16, 2019 1:12 PM\n\nEdited by: LuciA on Jan 16, 2019 1:18 PM\n\nEdited by: LuciA on Jan 16, 2019 1:19 PM",
        "Answers":[
            {
                "Answer_creation_time":"2019-02-07T18:20:27.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi LuciA - I'm an engineer at AWS. Thanks for continuing to try the service in the face of some difficulties. Can you please cross-reference your augmented manifest against the samples shown in https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-ground-truth-build-highly-accurate-datasets-and-reduce-labeling-costs-by-up-to-70\/, https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/object_detection_augmented_manifest_training\/object_detection_augmented_manifest_training.ipynb, and https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb?\n\nIt looks like your format is a little different, e.g., the algorithm expects to see keys called \"annotations\" and \"image_size\". Can you please check the syntax and let us know if your results change?",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2019-01-29T22:57:02.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi LuciA,\n\nThanks for trying out the SageMaker object detection algorithm. When using the Pipe mode with an AugmentedManifestFile, you need to specify the RecordWrapperType as RecordIO and ContentType as application\/x-recordio. Can you please retry with these suggestions and revert back if you continue to see issues.\n\nThanks!\nRegards,\nAmazon SageMaker Team",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-02-10T12:15:37.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi vrkhareataws\n\nThank you for getting back to me. I have tried the changes suggested and now recieve a new error message, 'InternalServerError: We encountered an internal error. Please try again.' I have added a snippet of code below for more insight:\n\ntraining_params = \\\r\n{\r\n    \"AlgorithmSpecification\": {\r\n        \"TrainingImage\": training_image, \r\n        \"TrainingInputMode\": \"Pipe\"\r\n    },\r\n    \"RoleArn\": role,\r\n    \"OutputDataConfig\": {\r\n        \"S3OutputPath\": s3_output_path\r\n    },\r\n    \"ResourceConfig\": {\r\n        \"InstanceCount\": 1,   \r\n        \"InstanceType\": \"ml.p3.2xlarge\",\r\n        \"VolumeSizeInGB\": 50\r\n    },\r\n    \"TrainingJobName\": job_name,\r\n    \"HyperParameters\": { \r\n         \"base_network\": \"resnet-50\",\r\n         \"use_pretrained_model\": \"1\",\r\n         \"num_classes\": \"1\",\r\n         \"mini_batch_size\": \"1\",\r\n         \"epochs\": \"5\",\r\n         \"learning_rate\": \"0.001\",\r\n         \"lr_scheduler_step\": \"3,6\",\r\n         \"lr_scheduler_factor\": \"0.1\",\r\n         \"optimizer\": \"rmsprop\",\r\n         \"momentum\": \"0.9\",\r\n         \"weight_decay\": \"0.0005\",\r\n         \"overlap_threshold\": \"0.5\",\r\n         \"nms_threshold\": \"0.45\",\r\n         \"image_shape\": \"300\",\r\n         \"label_width\": \"350\",\r\n         \"num_training_samples\": str(num_training_samples)\r\n    },\r\n    \"StoppingCondition\": {\r\n        \"MaxRuntimeInSeconds\": 86400\r\n    },\r\n    \"InputDataConfig\": [\r\n        {\r\n            \"ChannelName\": \"train\",\r\n            \"DataSource\": {\r\n                \"S3DataSource\": {\r\n                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\r\n                    \"S3Uri\": s3_train_data_path,\r\n                    \"S3DataDistributionType\": \"FullyReplicated\",\r\n                    \"AttributeNames\": [\"source-ref\",\"Bird-Label-Train\"] \r\n                }\r\n            },\r\n            \"ContentType\": \"application\/x-recordio\",\r\n            \"RecordWrapperType\": \"RecordIO\",\r\n            \"CompressionType\": \"None\"\r\n        },\r\n        {\r\n            \"ChannelName\": \"validation\",\r\n            \"DataSource\": {\r\n                \"S3DataSource\": {\r\n                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\r\n                    \"S3Uri\": s3_validation_data_path,\r\n                    \"S3DataDistributionType\": \"FullyReplicated\",\r\n                    \"AttributeNames\": [\"source-ref\",\"Bird-Label\"] \r\n                }\r\n            },\r\n            \"ContentType\": \"application\/x-recordio\",\r\n            \"RecordWrapperType\": \"RecordIO\",\r\n            \"CompressionType\": \"None\"\r\n        }\r\n    ]\r\n}\n\n\nI had run 2 seperate labelling jobs to get my training and validation set which have different names. \"Bird-Label\" for my validation and \"Bird-Label-Train\" for training. As you can see in the \"AttributeNames\" fields.\n\nI would also like to point out that my decision for using the image content type was due to this sentence, \"However you can also train in pipe mode using the image files (image\/png, image\/jpeg, and application\/x-image), without creating RecordIO files, by using the augmented manifest format.\" from this guide (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html).\n\nPlease let me know if you understand what is causing this error. Thank You!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-01-30T10:05:35.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi JonathanB-AWS\n\nThank you for sending me really useful links.\n\nHaving sorted through the information, I understood that my JSON was slightly different. Having gone back to basics, I created another labelling job, but used the 'Bouding Box' type as opposed to the 'Custom - Bounding box template'. My output matched what was expected. This ran with no errors!\n\nAs my purpose was to have multiple labels, I was able to edit the files and mapping of my output manifests, which also worked!\n\ni.e.\n\n{\"source-ref\":\"s3:\/\/xxxxx\/Blackbird_15.JPG\",\"ValidateBird\":{\"annotations\":[{\"class_id\":0,\"width\":2023,\"top\":665,\"height\":1421,\"left\":1312}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"ValidateBird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:23:51.174131\",\"type\":\"groundtruth\/object-detection\"}}\r\n{\"source-ref\":\"s3:\/\/xxxx\/Pigeon_19.JPG\",\"ValidateBird\":{\"annotations\":[{\"class_id\":2,\"width\":784,\"top\":634,\"height\":1657,\"left\":1306}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"ValidateBird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"2\":\"Pigeon\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:23:51.074809\",\"type\":\"groundtruth\/object-detection\"}}\n\n\nThe original mapping was 0:'Bird' for all images through the labelling job.\n\nThank you for your help!",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Studio - create domain error",
        "Question_creation_time":1586796156000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyWQfPusnSHG6Ujfzx27o1w\/sagemaker-studio-create-domain-error",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":1.0,
        "Question_view_count":792.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"A customer is trying to setup Sagemaker studio. He is following our published instructions to set up using IAM: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/onboard-iam.html\n\nBut is getting an error: User: arn:aws:iam:xxxx:user\/user1 is not authorized to perform: sagemaker:CreateDomain on resource: arn:aws:sagemaker: us-east-2:xxxx:domain\/yyyy\n\nHe has admin priviledges on the account and AmazonSageMakerFullAccess. We noticed that the AmazonSageMakerFullAccess policy actually has a limitation. You can perform all sagemaker actions, but not on a resource with arn \u201carn:aws:sagemaker:::domain\/*\u201d. We confirmed there are no other domains in that region with the CLI as you are only allowed one \u2013 so that isn\u2019t blocking. And aws sagemaker list-user-profiles returns no user profiles.\n\nHas anyone seen that error before or know the workaround? Should he create a custom policy to enable creating domains or would there be any implications of that? Are there specific permissions he should have so as to onboard using IAM?",
        "Answers":[
            {
                "Answer_creation_time":"2020-04-13T19:51:10.000Z",
                "Answer_upvote_count":1,
                "Answer_body":"A user with admin privileges would have access to \"iam:CreateServiceLinkedRole\" and \"sagemaker:CreateDomain\" actions, unless SCPs or permissions boundaries are involved. However, for the purpose of onboarding Amazon SageMaker Studio with limited permissions, I would grant the user least privilege by reviewing Control Access to the Amazon SageMaker API by Using Identity-based Policies and Actions, Resources, and Condition Keys for Amazon SageMaker documentation:\n\n{\n    \"Effect\": \"Allow\",\n    \"Action\": \"sagemaker:CreateDomain\",\n    \"Resource\": \"arn:aws:sagemaker:<REGION>:<ACCOUNT-ID>:domain\/*\"\n}\n\n\nNOTE: An AWS account is limited to one Domain, per region, see CreateDomain.\n\n{\n    \"Effect\": \"Allow\",\n    \"Action\": \"iam:CreateServiceLinkedRole\",\n    \"Resource\": \"*\",\n    \"Condition\": {\n        \"StringEquals\": {\n            \"iam:AWSServiceName\": \"sagemaker.amazonaws.com\"\n        }\n    }\n}\n\n\nCheers!",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker MXNet local mode not working",
        "Question_creation_time":1537534843000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQu1fDak6RL2wmivZ5UJwUw\/sage-maker-mx-net-local-mode-not-working",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":186.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi, I am trying to fit an MXNet model locally. I am adapting this https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/ and doing the following:\n\nbucket = 'XXXXXXXXXXX'\nprefix = 'sagemaker\/cifar-bench\/data'\n\ninputs = sagemaker_session.upload_data(\n    path='data',\n    bucket=bucket, \n    key_prefix=prefix)\n\nprint('data sent to ' + inputs)\n\n\nInception = MXNet('gluon_cifar_net.py', \n          role=role, \n          train_instance_count=1, \n          train_instance_type='local_gpu',\n          framework_version='1.2.1',\n          base_job_name='cifar10-inception-',\n          hyperparameters={'batch_size': 256, \n                           'optimizer': 'sgd',\n                           'epochs': 100, \n                           'learning_rate': 0.1, \n                           'momentum': 0.9})\n\n\nInception.fit(inputs)\n\n\nwhich returns an OSError: [Errno 2] No such file or directory\n\nIn the error log I can see that there seems to be error at self.latest_training_job = _TrainingJob.start_new(self, inputs) and self.sagemaker_client.create_training_job(**train_request)\n\nHow can I make the local mode work?",
        "Answers":[
            {
                "Answer_creation_time":"2018-09-21T17:24:55.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"It is very likely that you don't have docker-compose (or docker) installed in the box, that is why you are getting a No such file or directory.\n\nIf you want to use the GPU setup I would recommend running on a sagemaker notebook instance. Navigate to one of the example notebooks such as: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_cifar10\/mxnet_cifar10_local_mode.ipynb\n\nAnd run the setup.sh cell. This will install and configure all the docker dependencies correctly and then you should be able to use MXNet locally on GPU without any issue.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can we connect a Sagemaker Studio user to a gitlab repo within a private VPN?",
        "Question_creation_time":1643132865842,
        "Question_link":"https:\/\/repost.aws\/questions\/QURGs7VOVlTzKCG7H2AFLWww\/how-can-we-connect-a-sagemaker-studio-user-to-a-gitlab-repo-within-a-private-vpn",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":115.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"We have a gitlab repo within a private VPN and would like to setup Studio to clone that repo and to push and pull updates. Is that possible yet from within Studio?",
        "Answers":[
            {
                "Answer_creation_time":"2022-04-26T17:41:40.956Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thank you for your response. For those looking to do the same thing, according to AWS Support AWS SageMakers does NOT support GitLab yet and there is no ETA for that feature.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-02-24T18:06:03.844Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nThank you for using AWS services.\n\nAWS provides different VPN options like Client VPN and Site-to-Site VPN which might required different configuration and setup to get associated with other AWS resources like SageMaker Studio.\n\nTo answer your question in precise way and provide better assistance, we would like to understand the architecture at your end while implementing this configuration. The best way to understand the architecture is by opening a support case with networking team and discussed more about the options available to connect the Gitlab repo within private VPN to SageMaker Studio.\n\nhttps:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html\n\nFor more reference on AWS VPN:\n\nClient VPN: AWS Client VPN is a managed client-based VPN service that enables you to securely access your AWS resources and resources in your on-premises network. With Client VPN, you can access your resources from any location using an OpenVPN-based VPN client.\n\nSite-to-Site VPN: AWS Virtual Private Network solutions establish secure connections between your on-premises networks, remote offices, client devices, and the AWS global network.\n\nhttps:\/\/docs.aws.amazon.com\/vpn\/latest\/clientvpn-admin\/what-is.html\n\nhttps:\/\/docs.aws.amazon.com\/vpn\/latest\/s2svpn\/VPC_VPN.html\n\nhttps:\/\/docs.aws.amazon.com\/vpn\/index.html",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What is SageMaker Autopilot doing when in state \"InProgress - AnalyzingData\" ?",
        "Question_creation_time":1596036787000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU8QUiTTSMQ2W2uOgHXC7lqA\/what-is-sage-maker-autopilot-doing-when-in-state-in-progress-analyzing-data",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":24.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nI'm trying this nice SageMaker Autopilot demo https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/autopilot\/autopilot_customer_churn_high_level_with_evaluation.ipynb\n\nAt the beginning of the job, the status is \"InProgress - AnalyzingData\" for several minutes. This is long enough that I'd like to know more about it: what is Autopilot doing when at that status?",
        "Answers":[
            {
                "Answer_creation_time":"2020-08-20T01:25:08.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"There are some metrics begin collected in this stage. To understanding what is doing is the same as what happens when you're using tensorflow autoML. There's a deep explanation what is does in our Science page https:\/\/www.amazon.science\/publications\/amazon-sagemaker-autopilot-a-white-box-automl-solution-at-scale",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"S3 Dataset versioning with SageMaker?",
        "Question_creation_time":1549396058000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhYC1EJQuSWqpwTByAtB_fg\/s-3-dataset-versioning-with-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":386.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"Is there any standard for ML S3 dataset tracking or versioning? Basically, what setup allows to track a given model training execution to a given dataset? Interested to hear about proven or state-of-the-art ideas",
        "Answers":[
            {
                "Answer_creation_time":"2019-02-06T07:18:29.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Unfortunately, managing versions of datasets and which models used them is not embedded in SageMaker. But, you can use SageMaker search to manage the differences in data location between experiments. In that case, if your dataset isn't too big, my recommendation will be to create a standard for data structure in S3. i.e. for each new dataset, create a new prefix in S3 with your logic. Using SageMaker search you'll be able to find all your jobs and compare between datasets.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-09-28T12:51:12.007Z",
                "Answer_upvote_count":0,
                "Answer_body":"Nowadays, there are 3rd party tool that can be used alongside SageMaker. One example is Data Version Control (DVC), and we have discussed it how to integrate within SageMaker Processing jobs and SageMaker Training Jobs in this blogpost. As an alternative, you can leverage SageMaker Pipelines when your data preparation step is executed as a processing step within a pipeline execution. Pipelines allows you to achieve data versioning in a programmatic way by using execution-specific variables like ExecutionVariables.PIPELINE_EXECUTION_ID, which is the unique ID of a pipeline run. We can, for example, create a unique key for storing the output datasets in S3 that ties them to a specific pipeline run. We have also discussed this possibility as part of this blogpost.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to pass data to an endpoint",
        "Question_creation_time":1625081705000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUpamBayk2RT6c6KuNop0DQQ\/how-to-pass-data-to-an-endpoint",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":277.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":true,
        "Question_body":"Hello,\nI have followed the DeepAR Chicago Traffic violations notebook example. The Model and Endpoint has been created and the forecasting is working.\n\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/deepar_chicago_traffic_violations\/deepar_chicago_traffic_violations.ipynb\n\nHowevr, I haven't deleted the model nor the endpoint in order to use it externally. I have created a Python script on an EC2 that tries to load the endpoint and passes the data to it to get a prediction, and here is what I am doing:\n\nLoading the CSV exactly the way I did it on the notebook\nParsing the CSV the same way I did on the notebook for the \"predictor.predict\" command\nInstead of using the \"predictor.predict\", I am using \"invoke_endpoint\" to load the endpoint and passing the data from the previous point\nInstead of getting the same response I got on the notebook, I am getting the following message:\n\"type: <class 'list'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object\"\n\nNot sure what the issue is, seems that it requires a byte data... I guess I cannot send the data as a list to the endpoint and I need to serialize it or to encode it? convert to to JSON? to Bytes?\n\nAny help will be appreciated.\nRegards",
        "Answers":[
            {
                "Answer_creation_time":"2021-12-15T13:25:47.348Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nSo the issue here is the predictor.predict command converts the data to the format necessary for the endpoint to understand, thus you need to serialize or encode the payload by yourself. To do this you can work with something like json.dumps(payload) or for a byte array json.dumps(payload).encode().\n\nIf you want to use the predictor class this is taken care of by the serializer option. The serializer encodes\/decodes the data for us and lets you simply call the endpoint through the predictor class. An example of this is the following code snippet:\n\nfrom sagemaker.serializers import IdentitySerializer\nfrom sagemaker.deserializers import JSONDeserializer\nserializer=IdentitySerializer(content_type=\"application\/json\")\n\nHope this helps!\n\nTo check out the various serializer options that can work for your different use cases check the following link.\nSerializers: https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\n\nEdited by: rvegira-aws on Jul 22, 2021 9:22 AM\n\nEdited by: rvegira-aws on Jul 22, 2021 9:24 AM",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-07-22T16:21:57.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks rvegira-aws,\n\nI changed the approach, instead of using the \"invoke_endpoint\" method, I have used the predictor class as you suggested and this has fixed the issue.\n\nRegards.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-08-17T16:50:37.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I faced the exact problem when building models for my website, thanks for the question",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Amazon SageMaker Built-in algorithms and Spot checkpointing",
        "Question_creation_time":1593596016000,
        "Question_link":"https:\/\/repost.aws\/questions\/QURbWeXcwDT8i4dvXKE4HZXg\/amazon-sage-maker-built-in-algorithms-and-spot-checkpointing",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":32.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Which Amazon SageMaker built-in algorithms support checkpointing? In the documentation it says that:\n\nSageMaker built-in algorithms and marketplace algorithms that do not checkpoint are currently limited to a MaxWaitTimeInSeconds of 3600 seconds (60 minutes).\n\nHowever, in the algorithms I don't find any pointer to \"checkpoint\" or \"spot\". Can you help me out?",
        "Answers":[
            {
                "Answer_creation_time":"2020-07-01T13:49:48.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"This is the best resource that I've found to clarify this:\n\nhttps:\/\/aws.amazon.com\/blogs\/aws\/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs\/\n\nBuilt-in algorithms: computer vision algorithms support checkpointing (Object Detection, Semantic Segmentation, and very soon Image Classification). As they tend to train on large data sets and run for longer than other algorithms, they have a higher likelihood of being interrupted. Other built-in algorithms do not support checkpointing for now.\n\nAlso:\n\nPlease note that TensorFlow uses checkpoints by default. For other frameworks, you\u2019ll find examples in our sample notebooks and in the documentation.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Does Amazon SageMaker RL support heterogenous clusters?",
        "Question_creation_time":1593179712000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE59_Oro0SGKaOIdZNmySiw\/does-amazon-sage-maker-rl-support-heterogenous-clusters",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":27.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Does SageMaker RL support heterogenous clusters? I'd like to have our training to run on GPU and and SageMaker RL, and our inferences to run on CPUs.",
        "Answers":[
            {
                "Answer_creation_time":"2020-06-28T20:35:03.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yes, Amazon SageMaker RL allows you to define training workers separately from inference workers.\n\nFor more information, see Amazon SageMaker RL \u2013 Managed reinforcement learning with Amazon SageMaker on the AWS News Blog. Also, the Build and Train Reinforcement Models with Amazon SageMaker RL AWS online tech talk (see minute 26).",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using Hyperparameter Tuning Jobs over Training and Preprocessing",
        "Question_creation_time":1610658074000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3xcWKDPHR8ylWSaX83lNKQ\/using-hyperparameter-tuning-jobs-over-training-and-preprocessing",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":65.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Some data science teams want to tune the hyperparameters of their preprocessing jobs alongside ML model training jobs.\n\nDoes AWS have a recommended approach to establish this using Sagemaker Hyperparameter tuning?",
        "Answers":[
            {
                "Answer_creation_time":"2021-01-15T20:17:42.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"It depends on the dataset and the question for ML to answer.\n\nYes, it is feasible to do HPO with preprocessing. However, to run a HPO job, it is required to define to a specific target to achieve, e.g. maximize\/minimize certain values during the whole HPO process. Thus, it is important to understand what is the target during preprocessing. If the answer is yes, they should be able to leverage Hyperparameter Tuning Jobs.\n\nHere is how HPO works in SageMaker. Firstly, we define each training Job with output in a container and specify the hyperparameters in \/opt\/ml\/input\/config\/hyperparameters.json. When we run the pipeline using HyperparameterTuner in SageMaker, the initial Job can pass the hyperparameters to the Pipeline for HPO, and return the model with highest score.\n\nOption 1, if there is a clear defined target for preprocessing to achieve, we can also do HPO separately for data preprocessing through defining the function and outputs in a container and use HyperparameterTuner fit to tune the preprocessing.\n\nOption 2. include the preprocessing + training code in the whole SageMaker Training Job. But then you can't use separate infrastructure for training and preprocessing.\n\nSo it depends on what exactly they are looking for, but they can likely use SageMaker HPO.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Using R model in SageMaker ML pipelines",
        "Question_creation_time":1643230196748,
        "Question_link":"https:\/\/repost.aws\/questions\/QU17aS4s7uSRqmiLuveuchBw\/using-r-model-in-sage-maker-ml-pipelines",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":97.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi there,\n\nIs it possible to use R model training and serving in SageMaker ML Pipelines? Looked in examples here. And it doesn't look that R is fully supported currently by ML Pipelines. Any examples and success stories are very welcome.\n\nThanks.",
        "Answers":[
            {
                "Answer_creation_time":"2022-01-28T21:07:43.709Z",
                "Answer_upvote_count":1,
                "Answer_body":"In general it is possible to use the SageMaker python SDK and boto3 using the reticulate package in R, However do not have direct examples of SageMaker Pipelines using R.\n\nIt is possible to orchestrate the production pipeline using the R Containers for training and serving and setting up the DAG can be done with reticulate and SageMaker Python SDK and can be achieved using the AWS Step Functions. Please refer to the following example for reference.\n\nhttps:\/\/github.com\/aws-samples\/reinvent2020-aim404-productionize-r-using-amazon-sagemaker https:\/\/www.youtube.com\/watch?v=Zpp0nfvqDCA",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Where should I report to when encounter a trouble at SageMaker Canvas?",
        "Question_creation_time":1644763229468,
        "Question_link":"https:\/\/repost.aws\/questions\/QUpsGPPvV7SbuofE1fnwC5AA\/where-should-i-report-to-when-encounter-a-trouble-at-sage-maker-canvas",
        "Question_topic":[
            "Machine Learning & AI",
            "AWS Well-Architected Framework",
            "Microservices"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Well-Architected Framework",
            "Microservices",
            "Amazon SageMaker Canvas"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":63.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"When I was building model for analyzing in SageMaker Canvas, it just run for 1h 2m and then I got this notification:\n\nModel building failed: Failed to run Neo compilation or generate explainability report. client_request_id is f76d5bf7-6780-4257-9631-500101632b1e\n\nWhere should I contact the admin for this issue? Thank you so much!",
        "Answers":[
            {
                "Answer_creation_time":"2022-02-14T18:28:44.414Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Ailee, you can submit a ticket here and engineering will get back to you: https:\/\/t.corp.amazon.com\/create\/templates\/20b56bae-3fca-4281-9b94-69b6e50128cd. Thanks!",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"What value should I set for directory_path for the Amazon SageMaker SDK with FSx as data source?",
        "Question_creation_time":1605283057000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHaScKqcfRu-aZ1Cwza63NQ\/what-value-should-i-set-for-directory-path-for-the-amazon-sage-maker-sdk-with-f-sx-as-data-source",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon FSx for Lustre",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":1.0,
        "Question_view_count":120.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"What value should I set for the directory_path parameter in FileSystemInput for the Amazon SageMaker SDK?\n\nHere is some information about my Amazon FSx for Lustre file system:\n\nMy FSx ID is fs-0684xxxxxxxxxxx.\nMy FSx has the mount name lhskdbmv.\nThe FSx maps to an Amazon S3 bucket with files (without extra prefixes in their keys)\n\nMy attempts to describe the job and the results are the following:\n\nAttempt 1:\n\nfs = FileSystemInput(\n    file_system_id='fs-0684xxxxxxxxxxx',\n    file_system_type='FSxLustre',\n    directory_path='lhskdbmv',\n    file_system_access_mode='ro')\n\n\nResult:\n\nestimator.fit(fs) returns ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: FileSystem DirectoryPath 'lhskdbmv' for channel 'training' is not absolute or normalized. Please ensure you don't have a trailing \"\/\", and\/or \"..\", \".\", \"\/\/\" in the path.\n\nAttempt 2:\n\nfs = FileSystemInput(\n    file_system_id='fs-0684xxxxxxxxxxx',\n    file_system_type='FSxLustre',\n    directory_path='\/',\n    file_system_access_mode='ro')\n\n\nResult:\n\nClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: The directory path for FSx Lustre file system fs-068406952bf758bac is invalid. The directory path must begin with mount name of the file system.\n\nAttempt 3:\n\nfs = FileSystemInput(\n    file_system_id='fs-0684xxxxxxxxxxx',\n    file_system_type='FSxLustre',\n    directory_path='fsx',\n    file_system_access_mode='ro')\n\n\nResult:\n\nClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: FileSystem DirectoryPath 'fsx' for channel 'training' is not absolute or normalized. Please ensure you don't have a trailing \"\/\", and\/or \"..\", \".\", \"\/\/\" in the path.",
        "Answers":[
            {
                "Answer_creation_time":"2020-11-13T19:41:48.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"The directory_path parameter must point to \/mountname\/path\/to\/specific\/folder\/in-file-system. The value of mountname is returned in the CreateFileSystem API operation response. It is also returned in the response of the describe-file-systems AWS Command Line Interface (AWS CLI) command and the DescribeFileSystems API operation.\n\nFor your use case, the response might look similar to the following: mountName = lhskdbmv",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Glue Interactive vs SageMaker Processing?",
        "Question_creation_time":1663171688837,
        "Question_link":"https:\/\/repost.aws\/questions\/QU7J5WaZe3Qzi2giJaOmBFDQ\/glue-interactive-vs-sage-maker-processing",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics",
            "Database"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Glue",
            "Extract Transform & Load Data"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":63.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"Greetings! I'm a data scientist working in SageMaker notebooks. I'd appreciate an explanation about when should I use Glue Interactive and not SageMaker Processing jobs. To my understanding, they are very similar and I can't differentiate them.\n\nThank you!",
        "Answers":[
            {
                "Answer_creation_time":"2022-09-14T21:34:08.024Z",
                "Answer_upvote_count":2,
                "Answer_body":"Hello! It depends on what you are trying to achieve.\n\nLet us just talk about notebooks first - Sagemaker notebook (or even Glue notebook) is quite efficient for quick prototyping and analysis of data. For example, if you just want to make certain charts from a CSV or do quick data wrangling etc. then the Notebook is often the preferred choice. The notebook is also fantastic for documenting algorithms. The interactivity helps process step by step and to change your processing along based on the data that you would see. From a tooling perspective, the Glue notebooks provide the data engineer ability to run Jupyter notebok or Zeppelin notebook. SageMaker notebook is the tool preferred by data scientists and Machine learning engineers and provides the Jupyter notebook interface.\n\nSagemaker provides multiple computing options including ability to choose EC2 instances. in SageMaker Processing you can customize the execution environment, as you could provide a Docker image\n\nA Glue job is typically built for executing ETL jobs in a Spark based\/Python serverless job that executes in a cluster of nodes to parallel process data in multiple nodes. AWS Glue is a serverless data integration platform that makes combining, preparing, and finding data for application development, machine learning, and analytics a breeze. It delivers all of the features required for data integration, allowing you to begin analyzing and putting your data to use in minutes rather than months. To make data integration simpler, AWS Glue offers both code-based and visual interfaces. The AWS Glue Data Catalog allows users to quickly locate and retrieve data. With just limited clicks in AWS Glue Studio, ETL (extract, transform, and load) developers and data engineers can graphically construct, execute, and monitor ETL processes. AWS Glue DataBrew allows data analysts and scientists to visually enhance, clean, and standardize information without writing codes. AWS Glue scans your data sources, recognizes data types, and recommends schemas for storing your data. It produces the code needed to conduct your data transformations and loading operations automatically. AWS Glue makes it simple to perform and manage hundreds of ETL processes, as well as to mix and duplicate data across numerous data stores using SQL.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-15T13:21:35.278Z",
                "Answer_upvote_count":1,
                "Answer_body":"I would suggest that you use Sagemaker processing for the data cleansing and preparation. I have led projects where all the data cleansing, preparation and model build and testing have been done in Sagemaker and the data scientists love the tool.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker with multiple models",
        "Question_creation_time":1587366119000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfmnWJIIZQs6_2K1uIH9stQ\/sage-maker-with-multiple-models",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":254.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Customer wants to host multiple DNN models on same SageMaker container due to latency concerns. Customer does not want to spin-up different containers for each model due to network adding additional latency. Thus, my customer asked me a question below -\n\nCan one SageMaker host more than one model? Each model then share the same input and produce different outputs concatenated together?\n\nI answered as below -\n\nYes. Amazon SageMaker supports you hosting multiple models in several different ways \u2013\n\nUsing Multi-model Inference endpoints: Amazon SageMaker supports serving multiple models from same Inference endpoint. Details can be found here. The sample code can be found here. Currently, this feature do not support Elastic Inference or serial inference pipelines. Multi-model endpoints also enable time-sharing of memory resources across your models. This works best when the models are fairly similar in size and invocation latency. When this is the case, multi-model endpoints can effectively use instances across all models. If you have models that have significantly higher transactions per second (TPS) or latency requirements, we recommend hosting them on dedicated endpoints. Multi-model endpoints are also well suited to scenarios that can tolerate occasional cold-start-related latency penalties that occur when invoking infrequently used models\n\nUsing Bring your own algorithm on SageMaker You can also bring your own container with your own libs and runtime\/programming language for serving and training. See the example notebook on how you can bring your own algorithm\/container image on sagemaker here\n\nUsing Multi-model serving container by using multi-model archive file You can find a sample example here [4] for tensorflow serving\n\nIf models are called sequentially, the SageMaker inference pipeline allows you to chain up to 5 models called one after the other on the same endpoint Sagemaker endpoints include optimizations that will save costs, such as (1) 1-click deploy to pre-configured environments for popular ML frameworks with a managed serving stack, (2) autoscaling, (3) model compilation, (4) cost-effective hardware acceleration via Elastic Inference, (5) multi-variant model deployment for testing and overlapped model replacement, (6) multi-AZ backend. It is not necessarily a good idea to have multiple models on same endpoint (unless you have the reasons and requirements I mentioned in Option A above). Having one model per endpoint creates an isolation which has positive benefits on fault tolerance, security and scalability. Please keep in mind that SageMaker works on containers that runs on top of EC2.\n\n[1]https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\n\n[2]https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\n\n[3]https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\n\n[4]https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst#deploying-more-than-one-model-to-your-endpoint\n\n[5]https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\n\nAm I missing anything? Any other suggestions in terms of other approaches?",
        "Answers":[
            {
                "Answer_creation_time":"2020-07-02T08:12:08.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Customer does not want to spin-up different containers for each model due to network adding additional latency.\n\nI am assuming this is a pipeline scenario where different models need to be chained. If so, it's important to keep in mind that all containers in pipeline run on the same EC2 instance so that \"inferences run with low latency because the containers are co-located on the same EC2 instances.\"[1]\n\nHope this is useful.\n[1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How can make multi model endpoint with SageMaker?",
        "Question_creation_time":1660122368052,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJQBp6A_dSQm1RJ3f8AYMmg\/how-can-make-multi-model-endpoint-with-sage-maker",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":53.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"This is my code.\n\nfrom datetime import datetime\nfrom sagemaker.multidatamodel import MultiDataModel\nmme = MultiDataModel(\n    name=\"LV-multi-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"),\n    model_data_prefix=model_dir, # 2\uc5d0\uc11c \uad6c\ud55c \ubaa8\ub378\uc774 \ubaa8\uc5ec\uc788\ub294 \ud3f4\ub354(\uacbd\ub85c)!!,\n    model=sagemaker_model,  # \ubaa8\ub378 \uac1d\uccb4 1\uac1c \uc6b0\uc120 \ub123\uae30\n    sagemaker_session=sess\n)\n\npredictor = mme.deploy(\n    initial_instance_count=1,\n    instance_type=\"ml.g4dn.xlarge\"\n)\n\nAnd error message. How can I find Ecr Image(within multi-models=true)?\n\nClientError: An error occurred (ValidationException) when calling the CreateModel operation: Your Ecr Image 763104351884.dkr.ecr.ap-northeast-2.amazonaws.com\/pytorch-inference:1.8.1-gpu-py3 does not contain required com.amazonaws.sagemaker.capabilities.multi-models=true Docker label(s).",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-10T13:57:13.476Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi there - thanks for opening this thread. Multi-model endpoints are not supported on GPU instance types, see here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-model-endpoints.html#multi-model-endpoint-instance\n\nIn order to host a multi-model endpoint, choose a CPU instance type instead. The ECR image for CPUs will contain the required com.amazonaws.sagemaker.capabilities.multi-models=true label, see here: https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Model Spend",
        "Question_creation_time":1592312639000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlNS8ujYmQqePwWS-mgso3Q\/sage-maker-model-spend",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":71.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"If I deploy a SageMaker model, am I incurring hosting charges even while no one is accessing my model?",
        "Answers":[
            {
                "Answer_creation_time":"2020-06-16T13:24:24.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"When you deploy a SageMaker model, it deploys it behind a SageMaker endpoint for real-time inference. You are charged by the second for on-demand ML hosting. Check the model deployment section of each region on the SageMaker Pricing page. In some use cases, you can save on inference cost by hosting several models behind the same endpoint (check this blog post).",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to save a .html file to S3 that is created in a Sagemaker processing container",
        "Question_creation_time":1660653174738,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5abOieUyQZSFvyRwfApRVA\/how-to-save-a-html-file-to-s-3-that-is-created-in-a-sagemaker-processing-container",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon Simple Storage Service",
            "Amazon SageMaker",
            "Containers"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":115.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Error message: \"FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/processing\/output\/profile_case.html'\"\n\nBackground: I am working in Sagemaker using python trying to profile a dataframe that is saved in a S3 bucket with pandas profiling. The data is very large so instead of spinning up a large EC2 instance, I am using a SKLearn processor.\n\nEverything runs fine but when the job finishes it does not save the pandas profile (a .html file) in a S3 bucket or back in the instance Sagemaker is running in.\n\nWhen I try to export the .html file that is created from the pandas profile, I keep getting errors saying that the file cannot be found.\n\nDoes anyone know of a way to export the .html file out of the temporary 24xl instance that the SKLearn processor is running in to S3? Below is the exact code I am using:\n\nimport os\nimport sys\nimport subprocess\ndef install(package):\n    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\ninstall('awswrangler')\ninstall('tqdm')\ninstall('pandas')\ninstall('botocore==1.19.4')\ninstall('ruamel.yaml')\ninstall('pandas-profiling==2.13.0')\nimport awswrangler as wr\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom dateutil.relativedelta import relativedelta\nfrom string import Template\nimport gc\nimport boto3\n\nfrom pandas_profiling import ProfileReport\n\nclient = boto3.client('s3')\nsession = boto3.Session(region_name=\"eu-west-2\")\n\n%%writefile casetableprofile.py\n\nimport os\nimport sys\nimport subprocess\ndef install(package):\n    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\ninstall('awswrangler')\ninstall('tqdm')\ninstall('pandas')\ninstall('botocore')\ninstall('ruamel.yaml')\ninstall('pandas-profiling')\nimport awswrangler as wr\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom dateutil.relativedelta import relativedelta\nfrom string import Template\nimport gc\nimport boto3\n\nfrom pandas_profiling import ProfileReport\n\nclient = boto3.client('s3')\nsession = boto3.Session(region_name=\"eu-west-2\")\n\n\n\n\ndef run_profile():\n\n\n\n    query = \"\"\"\n    SELECT  * FROM \"healthcloud-refined\".\"case\"\n    ;\n    \"\"\"\n    tableforprofile = wr.athena.read_sql_query(query,\n                                            database=\"healthcloud-refined\",\n                                            boto3_session=session,\n                                            ctas_approach=False,\n                                            workgroup='DataScientists')\n    print(\"read in the table queried above\")\n\n    print(\"got rid of missing and added a new index\")\n\n    profile_tblforprofile = ProfileReport(tableforprofile, \n                                  title=\"Pandas Profiling Report\", \n                                  minimal=True)\n\n    print(\"Generated carerequest profile\")\n                                      \n    return profile_tblforprofile\n\n\nif __name__ == '__main__':\n\n    profile_tblforprofile = run_profile()\n    \n    print(\"Generated outputs\")\n\n    output_path_tblforprofile = ('profile_case.html')\n    print(output_path_tblforprofile)\n    \n    profile_tblforprofile.to_file(output_path_tblforprofile)\n\n    \n    #Below is the only part where I am getting errors\nimport boto3\nimport os   \ns3 = boto3.resource('s3')\ns3.meta.client.upload_file('\/opt\/ml\/processing\/output\/profile_case.html', 'intl-euro-uk-datascientist-prod','Mark\/healthclouddataprofiles\/{}'.format(output_path_tblforprofile))  \n\nimport sagemaker\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsession = boto3.Session(region_name=\"eu-west-2\")\n\nbucket = 'intl-euro-uk-datascientist-prod'\n\nprefix = 'Mark'\n\nsm_session = sagemaker.Session(boto_session=session, default_bucket=bucket)\nsm_session.upload_data(path='.\/casetableprofile.py',\n                                bucket=bucket,\n                                key_prefix=f'{prefix}\/source')\n\nimport boto3\n#import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nregion = boto3.session.Session().region_name\n\n\nS3_ROOT_PATH = \"s3:\/\/{}\/{}\".format(bucket, prefix)\n\nrole = get_execution_role()\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     sagemaker_session=sm_session,\n                                     instance_type='ml.m5.24xlarge',\n                                     instance_count=1)\n\nsklearn_processor.run(code='s3:\/\/{}\/{}\/source\/casetableprofile.py'.format(bucket, prefix),\n                      inputs=[],\n                      outputs=[ProcessingOutput(output_name='output',\n                                                source='\/opt\/ml\/processing\/output',\n                                                destination='s3:\/\/intl-euro-uk-datascientist-prod\/Mark\/')])\n\n\nThank you in advance!!!",
        "Answers":[
            {
                "Answer_creation_time":"2022-08-17T02:50:02.950Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi,\n\nFirstly, you should not (usually) need to directly interact with S3 from your processing script: The fact that you've configured your ProcessingOutput means that any files your script saves in \/opt\/ml\/processing\/output should automatically get uploaded to your s3:\/\/... destination URL. Of course there might be particular special cases where you want to directly access S3 from your script, but in general the processing job inputs and outputs should do it for you, to keep your code nice and simple.\n\nI'm no Pandas Profiler expert, but I think the error might be coming from here:\n\n    output_path_tblforprofile = ('profile_case.html')\n    print(output_path_tblforprofile)\n    \n    profile_tblforprofile.to_file(output_path_tblforprofile)\n\nDoesn't this just save the report to profile_case.html in your current working directory? That's not the \/opt\/ml\/processing\/output directory: It's usually the folder where the script is downloaded to the container I believe. The FileNotFound error is telling you that the HTML file is not getting created in the folder you expect, I think.\n\nSo I would suggest to make your output path explicit e.g. \/opt\/ml\/processing\/output\/profile_case.html, and also remove the boto3\/s3 section at the end - hope that helps!",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Outgoing mail for sagemaker labeling job",
        "Question_creation_time":1662016903588,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqbxtiU_kSe-GoPcj6g0pzg\/outgoing-mail-for-sagemaker-labeling-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":36.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"When having made a labeling job on Ground Truth, an outgoing mail should be sent to team member, but in my case, mail not be sent with no error message.\n\nin case no private team created (the first job creation) : mail can be sent. (set up a team during job creation)\nin case a private team already set up: mail cannot be sent. (select a existing team during job creation)\n\nI think policies of the job role might not be enough, for example, cognito policy. How can I make sure the cause of the error?",
        "Answers":[
            {
                "Answer_creation_time":"2022-09-02T14:53:23.761Z",
                "Answer_upvote_count":0,
                "Answer_body":"To Successfully create a SageMaker Labeling Job you will need the following Permission Policies applied within your account:\n\nThe IAM entity you have used to create the job will need permissions outlined in the \"Permissions Required to Use the Amazon SageMaker Ground Truth Console\" [1]\nYour Labelling Job Role will need SageMakerFullAccess [2]\n\nWith these permissions in place your job should create successfully.\n\nLinks to documentation provided by AWS:\n\n[1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security_iam_id-based-policy-examples.html#console-permissions\n\n[2] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security-iam-awsmanpol.html#security-iam-awsmanpol-AmazonSageMakerFullAccess",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"AWS AI\/ML integration with Power BI",
        "Question_creation_time":1607495476000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4VexAnfiSFi4Jf5i9RyO_A\/aws-ai-ml-integration-with-power-bi",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon QuickSight",
            "Amazon Comprehend"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":148.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"Customer wants to know if AWS AI\/ML services integrate with Power BI. The customer currently uses Power BI that integrates with Azure ML for sentiment analysis, opinion mining, etc. Customer is looking for a push button solution where the business analyst can do text analytics on the response from the model. Is there a way to do this on AWS or a marketplace solution?",
        "Answers":[
            {
                "Answer_creation_time":"2020-12-09T15:40:38.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"PowerBI can connect to Amazon Redshift and leverage the new SQL based ML capability in Redshift that uses Sagemaker under the hood.\n\nAs an alternative thought the customer can integrate Amazon Sagemaker Model with Amazon Quicksight to achieve functionality very similar to PowerBI with Azure ML. Quicksight does have some embedded ML capability like forecasting and anomaly detection but Opinion mining is not one of them yet.\n\nYou should be able to leverage Blazing Text Algorithm in Sagemaker or some market place solution like Twinword sentiment model in sagemaker for sentiment analysis for Text mining after the integration.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-03-21T18:11:56.219Z",
                "Answer_upvote_count":0,
                "Answer_body":"Please take a look at this AWS solution - https:\/\/aws.amazon.com\/solutions\/implementations\/text-analysis-with-amazon-opensearch-service-and-amazon-comprehend\/\n\nComprehend also has an integration with AWS Aurora - https:\/\/docs.aws.amazon.com\/AmazonRDS\/latest\/AuroraUserGuide\/mysql-ml.html",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How do I check my current SageMaker service quotas?",
        "Question_creation_time":1642480202619,
        "Question_link":"https:\/\/repost.aws\/questions\/QUweO83CSlTu-3Zn2RxdESWg\/how-do-i-check-my-current-sage-maker-service-quotas",
        "Question_topic":[
            "Management & Governance",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Management Console",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":533.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"How do I check my current service quotas for Amazon SageMaker?\n\nIn the case of Amazon EC2, service quotas can be checked here: https:\/\/console.aws.amazon.com\/servicequotas\/home\/services\/ec2\/quotas\n\nFor SageMaker, the default quotas are listed here: https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html but there isn't a link to where one can find the current region-specific quotas for an account, which could have changed after a request for a service quota increase.",
        "Answers":[
            {
                "Answer_creation_time":"2022-01-18T22:33:29.893Z",
                "Answer_upvote_count":1,
                "Answer_body":"Amazon SageMaker has now been integrated with Service Quotas. You should be able to find current SageMaker quotas for your account in the Service Quotas console. You can also request for a quota increase right from the Service Quotas console itself. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/regions-quotas.html#regions-quotas-quotas",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-06-15T18:32:09.862Z",
                "Answer_upvote_count":1,
                "Answer_body":"Unfortunately, AWS Sagemaker is not supported for direct visibility into the service quotas. We have an existing feature request for SageMaker Integration with the Service Quotas page as below. However, I currently done have an ETA for the same as it would depend on feasibility and further integration.\n\nHaving said that, the below documentations provides the default values as you mentioned: [+] https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html\n\nI would suggest to contact the AWS Support when ever you wish to get to get insights on current quotas currently.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Do Amazon SageMaker manifest files enable dataset versioning?",
        "Question_creation_time":1607681930000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq44kZCYWTiOnwXblHSQSTA\/do-amazon-sage-maker-manifest-files-enable-dataset-versioning",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":65.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Some Amazon SageMaker algorithms can train with a manifest JSON file that stores the mapping between images and their Amazon S3 ARNs and metadata, such as labels. This is a great option, because the manifest file is much smaller than the dataset itself. Because the manifest files are small, they can be used easily in versioning tools or saved as part of the model artifact. This appears to be the best construct enabling exact dataset versioning within SageMaker. i.e., if we exclude the creation of a unique training set hard copy per training job that can't be scaled to large datasets. Is my understanding accurate?",
        "Answers":[
            {
                "Answer_creation_time":"2020-12-11T10:56:42.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"If you create the conditions for immutability of the assets the manifest points to, then manifest enables exact dataset versioning with SageMaker. You can have a data store in Amazon S3 with all versions of the data assets and use the manifest files for creating and versioning datasets for specific usage.\n\nIf you don't guarantee immutability for the assets that the manifest points to, then your manifest becomes invalid.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Directory Error when running SageMaker backup-ebs lifecycle for Amazon Linux 2 transition",
        "Question_creation_time":1667961704440,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPsaLux6EQcqvHW1HtNlkIw\/directory-error-when-running-sage-maker-backup-ebs-lifecycle-for-amazon-linux-2-transition",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon CloudWatch Logs"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":28.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I'm following this guide for transitioning to Amazon Linux 2 provided by AWS\n\nI've set up the two needed lifecycle configurations and created a new S3 Bucket to store the backup. I've also ensured the IAM roles have the required S3 permissions and updated the notebook with the ebs-backup-bucket tag per the instructions.\n\nWhen I run the notebook with the new configuration I get the following error: \"Notebook Instance Lifecycle Config [LIFECYCLE ARN] for Notebook Instance [NOTEBOOK ARN] took longer than 5 minutes. Please check your CloudWatch logs for more details if your Notebook Instance has Internet access.\n\nLooking at the logs I get the error: \/bin\/bash: \/tmp\/OnStart_2022-11-09-01-51ontlqcqt: \/bin\/bash^M: bad interpreter: No such file or directory\n\nAny thoughts on how to resolve this issue? The code for the backup lifecycle configuration can be found here",
        "Answers":[
            {
                "Answer_creation_time":"2022-11-09T05:39:17.492Z",
                "Answer_upvote_count":1,
                "Answer_body":"The extra ^M symbol (i.e. Ctrl-M) stopped the whole scrip from being interpreted properly.\n\nThis issue is normally seen in scripts prepared in MSDOS\/Windows based system but used in Linux system due to difference of line endings.\n\nIn Unix based OS, lines end with \\n but MSDOS\/Win based system ends with \\r\\n\n\nIn Linux based system, you could show your prepared scripts by running\n\ncat -e some-script.sh \n\n\nThe results would be something similar to\n\n#!\/bin\/bash^M$\n... ...^M$\n\n\n$ is normal Unix end-of-line symbol. Windows uses an extra one ^M and this symbol is not recognized by Unix system. That's why, in SageMaker Notebook Lifecycle Configuration, which is running Linux, your script was interpreted as \/bin\/bash^M\n\nTo mitigate the issue, please convert the scripts to Unix based ending and update life cycle configuration. To achieve this, you could use Notepad++ in Windows. You can go to the Edit menu, select the EOL Conversion submenu, and from the options that come up select UNIX\/OSX Format. The next time you save the file, its line endings will, all going well, be saved with UNIX-style line endings.\n\nAlternatively, you could put the script in a Linux environment, e.g. EC2 instance with Amazon Linux 2 and install dos2unix via sudo yum install dos2unix. After installation, you could convert your files via\n\ndos2unix -n file.sh  output.sh \n\n\nAfter the conversion, please update LCC with the new scripts. You could verify that ^M has been removed via\n\ncat -e your_script.sh\n\n\nThe output will print all special characters directly without hiding.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"LightGBM on SageMaker",
        "Question_creation_time":1516632842000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPwkZcylKQR6-u0pghgrseA\/light-gbm-on-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":388.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I have a customer who wants to install LightGBM on SageMaker notebooks, as they are currently using it outside of SageMaker.\n\nRight now, they are interested in the ability to SSH into the instance, but it would be great if we could provide them a way to install LightGBM right now.\n\nCheers",
        "Answers":[
            {
                "Answer_creation_time":"2018-01-22T14:58:17.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"It's possible to do, I have used it myself for gradient boosting, from within Jupyter you can simply run:\n\n!conda install -y -c conda-forge lightgbm\n\n\nWithin a selected conda environment. No terminal access is needed, however it must be done, On the top right of the Jupyter notebook you can choose a terminal environment which will give you a shell to the backend instance and you can install there.\n\nHowever if you want the notebook to be immutable\/transferable you can do the install within the notebook .\n\nThanks",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker notebook instance in VPC failed to connect to local database",
        "Question_creation_time":1557130614000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUUm8LxKZTzixOCI_IovK1A\/sage-maker-notebook-instance-in-vpc-failed-to-connect-to-local-database",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":260.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"hi there,\n\nI am setting up a jupyter notebook in SageMaker within the VPC and using the jdbc jars to connect to the local database. But it shows the following error messages.\n\": com.microsoft.sqlserver.jdbc.SQLServerException: The TCP\/IP connection to the host xxx.xxx.xxx.xxx, port 21000 has failed. Error: \"connect timed out. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP\/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.\".\"\n\nI used the exactly the same VPC, subnet, security group as what i used in a glue job to extract the data from the local db. While the glue job works but the SageMaker notebook failed. I am sure the firewalls are opened.\nCould anyone tell me how to solve it?\nI also came across the following articles, but i am not sure if it is the root cause.\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options\/",
        "Answers":[
            {
                "Answer_creation_time":"2019-05-06T22:04:11.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\nThe principle here is that there much be network connectivity between the Notebook Instance and the DB Instance, and the security groups on the DB Instance should allow in-bound traffic from the Notebook Instance\n\nOne example of such as setup is\n\nRDS DB Instance is VPC vpc-a and Subnet subnet-b.\n\nSageMaker Notebook is launched in VPC vpc-a, Subnet subnet-b, with Security Group sg-c with DirectIntenetAccess \"Disabled\"\n\nIn the RDS DB Instance's Security Group rules, you can add an Inbound Rule to allow inbound traffic from the SageMaker Notebook security group \"sg-c\"\n-- Type - Protocol - Port Range - Source\n-- MYSQL\/Aurora - TCP - 3306 - sg-c\n\nSample Code:\n\n! pip install mysql-connector\r\n\r\nimport mysql.connector\r\nmydb = mysql.connector.connect(\r\nhost=\"$RDS_ENDPOINT\",\r\nuser=\"$RDS_USERNAME\",\r\npasswd=\"$RDS_PASSWORD\"\r\n)\r\ncursor = mydb.cursor()\r\ncursor.execute(\"SHOW DATABASES\") \n\n\nThanks for using Amazon SageMaker and let us know if there's anything else we can help with!\n\nEdited by: JaipreetS-AWS on May 6, 2019 3:04 PM",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Cost of autoscaling endpoint Amazon SageMaker endpoint to zero",
        "Question_creation_time":1666360814590,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0VGYdZe8TRivmtGHoiDDHw\/cost-of-autoscaling-endpoint-amazon-sage-maker-endpoint-to-zero",
        "Question_topic":[
            "Management & Governance",
            "Machine Learning & AI",
            "Cloud Financial Management"
        ],
        "Question_tag":[
            "AWS Auto Scaling",
            "Amazon SageMaker",
            "AWS Pricing Calculator",
            "Pricing"
        ],
        "Question_upvote_count":1.0,
        "Question_view_count":57.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I want to use an Amazon Sagemaker endpoint for a custom classification model. The endpoint should only handle sporadic input (say a few times a week). For this purpose I want to employ autoscaling that scales the number of instances down to 0 when the endpoint is not used.\n\nAre there any costs associated with having an endpoint with 0 instances?\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_time":"2022-10-21T16:50:29.704Z",
                "Answer_upvote_count":2,
                "Answer_body":"You dont pay any compute costs for the duration when the endpoint size scales down to 0. But i think you can design it better. There are few other options for you to use in SageMaker Endpoint(assuming you are using realtime endpoint)\n\nTry using SageMaker Serverless Inference instead. Its purely serverless in nature so you pay only when the endpoint is serving inference. i think that would fit your requirement better.\nYou can think of using Lambda as well which will reduce your hosting costs. but you have to do more work in setting up the inference stack all by yourself.\nThere is also an option of SageMaker asynchronous inference but its mostly useful for inference which require longer time to process each request. The reason i mention this is it also support scale to 0 when no traffic is coming.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"API definition for ModelBiasMonitor and ModelExplainabilityMonitor",
        "Question_creation_time":1611507553000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU9SPQKzelSUu3dr-D4zaXHQ\/api-definition-for-model-bias-monitor-and-model-explainability-monitor",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":25.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Where can I find the actual references to API definitions and descriptions for ModelBiasMonitor and ModelExplainabilityMonitor Classes?\n\nI can a find a few mentions in the Amazon SageMaker documentation in the following links. https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model_monitor.html https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_model_monitor\/fairness_and_explainability\/SageMaker-Model-Monitor-Fairness-and-Explainability.html\n\nWhere can I find the actual reference and the code implementation for these Classes?",
        "Answers":[
            {
                "Answer_creation_time":"2021-01-25T10:47:51.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"The actual reference to the classes can be found here: https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/model_monitor\/clarify_model_monitoring.py\nIt encapsulates the definitions and descriptions for all of SageMaker Clarify related monitoring classes.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Can you configure Amazon ECR containers to be immutable?",
        "Question_creation_time":1607710511000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUPiBylRCSe6_ax_u_4g-oA\/can-you-configure-amazon-ecr-containers-to-be-immutable",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":27.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Is there a way to configure Amazon ECR containers so that they can't be changed once they're created? Here are our requirements:\n\nContainers can't be changed after their built.\nContainers can't receive updates.\nChanges in the containerized application must require the building and deployment of a new container image.\nRuntime data and configurations must be stored outside of the container environment.",
        "Answers":[
            {
                "Answer_creation_time":"2020-12-18T15:46:52.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yes, you can configure Amazon ECR containers to be immutable. Amazon ECR uses resource-based permissions to control access to repositories. The resource-based permissions let you specify which IAM users or roles have access to a repository and what actions they can perform on it. By default, only the repository owner has access to a repository.\n\nFor more information, see Repository policies and Image tag mutability in the Amazon ECR user guide.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Jupyter notebook",
        "Question_creation_time":1606707121000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIzWlfNVTSIWIqkVsIaNv2A\/sagemaker-jupyter-notebook",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":62.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"What are the advantages of using SageMaker jupyter instance instead of running it locally? Is there a special integration with SageMaker that we lose it if we do not use Sagemaker jupyer instance?",
        "Answers":[
            {
                "Answer_creation_time":"2020-11-30T04:05:24.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Some useful points:\n\nThe typical arguments of cloud vs local will apply (as with e.g. Cloud9, Workspaces, etc): Can de-couple your work from the lifetime of your laptop, keep things running when your local machine is shut down, right-size the environment for what workloads you need to do on a given day, etc.\nSageMaker notebooks already run in an explicit IAM context (via assigned execution role) - so you don't need to log in e.g. as you would through the CLI on local machine... Can just run sagemaker.get_execution_role()\nPre-built environments for a range of use-cases (e.g. generic data science, TensorFlow, PyTorch, MXNet, etc) with libraries already installed, and easy wiping\/reset of the environment by stopping & starting the instance - no more \"environment soup\" on your local laptop.\nLinux-based environments, which typically makes for a shorter path to production code than Mac\/Windows.\nIf you started using SageMaker Studio, then yes there are some native integrations such as the UIs for experiment tracking and endpoint management\/monitoring; easy sharing of notebook snapshots; and whatever else might be announced over the next couple of weeks.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Glue + SageMaker Pip Packages",
        "Question_creation_time":1591210245000,
        "Question_link":"https:\/\/repost.aws\/questions\/QULN3fro-LQ1umpDN831VlZg\/glue-sage-maker-pip-packages",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Glue"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":89.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"My customer is looking to use Glue dev endpoints along with a SageMaker notebook. What I've noticed is that in Glue, a package, in this case scipy, will be listed as 1.4.1, but this will or won't match what you get in a sagemaker notebook dependent on kernel.\n\nconda_python3:\n\n!pip show scipy\nName: scipy\nVersion: 1.1.0\nSummary: SciPy: Scientific Library for Python\nHome-page: https:\/\/www.scipy.org\nAuthor: None\nAuthor-email: None\nLicense: BSD\nLocation: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\nRequires: \nRequired-by: seaborn, scikit-learn, sagemaker\n\n\nconda_tensorflow_p36:\n\n!pip show scipy\nName: scipy\nVersion: 1.4.1\nSummary: SciPy: Scientific Library for Python\nHome-page: https:\/\/www.scipy.org\nAuthor: None\nAuthor-email: None\nLicense: BSD\nLocation: \/home\/ec2-user\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\nRequires: numpy\nRequired-by: seaborn, scikit-learn, sagemaker, Keras\n\n\nIs there some sort of best practice to use a kernel that corresponds directly to what's installed on Glue?\n\nSeparate not very useful question. I wasn't able activate the venv that Jupyter notebooks do via shell. Is it using a venv? How come I can't find the right activate script?",
        "Answers":[
            {
                "Answer_creation_time":"2020-06-03T20:20:18.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"conda_python3 and conda_tensorflow_p36 are local kernels on the SageMaker notebook instance while the Spark kernels execute remotely in the Glue Spark environment.\n\nHence you are seeing different versions. The Glue Spark environment comes with 1.4.1 version of scipy. So when you use the PySpark (python) or Spark (scala) kernels and you will get the 1.4.1 version of scipy.\n\nIf you use the default LifeCycle script that Glue SageMaker notebooks already come with, the connectivity to the Glue Dev endpoint should be in place. Note that the Glue SageMaker notebooks has a tag called 'aws-glue-dev-endpoint' that is used to identify which Glue Dev endpoint that particular notebook instance communicates with.\n\nThe Spark kernels cannot be replicated via the python shell. Those kernels relay Spark commands via the Livy service to Spark on the Glue Dev endpoint using a Jupyter module called Sparkmagic.\n\nRef: https:\/\/github.com\/jupyter-incubator\/sparkmagic",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to export tresained models to ECR as container image",
        "Question_creation_time":1663258467464,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZHWz5-hpSc-80dEIkuxwQw\/how-to-export-tresained-models-to-ecr-as-container-image",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon Elastic Container Registry (ECR)",
            "Containers",
            "Amazon SageMaker Studio Lab"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":29.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I want to train and build the model in Sagemaker studio and then be able to export the model as a container image to ECR, so I can use the model in external platform by sharing the ECR image to another account where I Can create container with the image from ECR",
        "Answers":[
            {
                "Answer_creation_time":"2022-09-16T23:05:33.114Z",
                "Answer_upvote_count":0,
                "Answer_body":"The models you train in SageMaker are stored in S3 as .tar.gz files that you can use to deploy to an endpoint, or even test locally (extracting the model file from the tar file). If you are using a built-in algorithm, you can share the .tar.gz file to the second account and deploy the model in the second account, since built-in algorithm containers can be accessed from any AWS account.\n\nIf you are using a custom training image (docs here), you can push this image to ECR and allow a second account to pull the image and then use the image with the model that you have trained. However, note that Studio at this time does not support building Docker images out of the box. You can use SageMaker Notebook Instances instead.\n\nI would recommend keeping the model (.tar.gz) and the image (Docker) separate, since you can easily retrain and deploy the newer versions of models without updating the image every single time.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Training a classifier on parquet with SageMaker ?",
        "Question_creation_time":1588841008000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCqvDUq4hSQqRT97tBUvE8Q\/training-a-classifier-on-parquet-with-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":188.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nWhat parquet data loading logic is known to work well to train with SageMaker on parquet? ml-io? pyarrow? any examples? That would be to train a classifier, either logistic regression, XGBoost or custom TF.",
        "Answers":[
            {
                "Answer_creation_time":"2020-05-07T09:21:42.000Z",
                "Answer_upvote_count":1,
                "Answer_body":"XGBoost as a framework container (v0.90+) can read parquet for training (see example notebook).\nThe full list of valid content types are CSV, LIBSVM, PARQUET, RECORDIO_PROTOBUF (see source)\n\nAdditionally:\nUber Petastorm for reading parquet into Tensorflow, Pytorch, and PySpark inputs.\nAs XGBoost accepts numpy, you can convert from PySpark to numpy\/pandas using the mentioned PyArrow.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"SageMaker Batch Transform local mode?",
        "Question_creation_time":1571055107000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUtNtH0LyFSLCXc0xCV4hYkw\/sage-maker-batch-transform-local-mode",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":151.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nA customer is experimenting with SageMaker batch transform with parquet and is interested is some form of local development to speedup iteration. Does SageMaker Batch Transform support local mode?",
        "Answers":[
            {
                "Answer_creation_time":"2019-10-17T09:18:46.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can do local testing by running the container in serve mode as a docker. Then using Curl\/Postman to send an HTTP request and inspecting the response.\n\nThe request can be CSV\/JSON or binary (a parquet file in your case).\n\nIf you're able to run the Pytorch model in serve mode locally, then this local testing provides a lot of coverage before running in Batch Transform itself.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Unable to compile model to Neuron: no error message, no output",
        "Question_creation_time":1663166637969,
        "Question_link":"https:\/\/repost.aws\/questions\/QUA_oVwSPQReCt96QyX4cz-g\/unable-to-compile-model-to-neuron-no-error-message-no-output",
        "Question_topic":[
            "Machine Learning & AI",
            "Compute"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Neuron"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":71.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi. We are trying to convert all our in-house pytorch models to aws-neuron on inferentia. We successfully converted one, but the second model we tried did not compile. Unfortunately, compilation did not generate any error message nor log of any kind, so we are stuck. The model is rather simple, but large, U-Net, with partial convolutions instead of regular ones, but otherwise no fancy operators. Conversion of this model to torchscript is ok on the same instance. Could it be a memory problem ?",
        "Answers":[
            {
                "Answer_creation_time":"2022-09-15T09:25:42.678Z",
                "Answer_upvote_count":2,
                "Answer_body":"Hi, in order to see more information about the error, you can enable debugging during tracing by passing 'verbose' to the tracing command like this:\n\nimport torch\nimport torch.neuron\ntorch.neuron.trace(\n    model,\n    example_inputs=inp,\n    verbose=\"debug\",\n    compiler_workdir=\"logs\" # dir where debugging logs will be saved\n)\n\n\nYou'll see the error messages in the console and they will also be saved to the \"logs\" dir.\n\nIt is always good to run the NeuronSDK analyzer first to make sure the model is: 1\/ torch.jit traceable; 2\/ supported by the compiler\n\nimport torch\nimport torch.neuron\ntorch.neuron.analyze_model(model, example_inputs=inp)\n\n\nYou can also see a sample that shows how to compile an U-net Pytorch (3rd party implementation) to Inf1 instances here: https:\/\/github.com\/samir-souza\/laboratory\/blob\/master\/05_Inferentia\/03_UnetPytorch\/03_UnetPytorch.ipynb\n\nRef: https:\/\/awsdocs-neuron.readthedocs-hosted.com\/en\/latest\/neuron-guide\/neuron-frameworks\/pytorch-neuron\/api-compilation-python-api.html\n\nIf everything fails, try to look for something like this in the logs:\n\nINFO:Neuron:Compile command returned: -11\nWARNING:Neuron:torch.neuron.trace failed on _NeuronGraph$647; falling back to native python function call\nERROR:Neuron:neuron-cc failed with the following command line call:\n\n\nAnd paste here, please. With the \"Compile command returned:\" code it is possible to identify the error. You are suspecting that there is some issue related to memory, maybe Out of Memory. Normally when that is the case, you'll find the code: -9 in this part of the error.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-09-21T07:33:00.671Z",
                "Answer_upvote_count":0,
                "Answer_body":"Following your answer we were able to check the log and got\n\nINFO:Neuron:Compile command returned: -9\n\nwhich is apparently an out of memory error. Switching to a 6x instance solved the problem",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to pass environment variables in sagemaker tuner job",
        "Question_creation_time":1669725280762,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5aMFxhnLQeqMY39mlmYHjA\/how-to-pass-environment-variables-in-sagemaker-tuner-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":9.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Sagemaker training jobs support setting environment variables on-the-fly in the training job:\n\n \"Environment\": { \n      \"string\" : \"string\" \n   },\n\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html\n\nI did not find an equivalent for the tuner jobs:\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateHyperParameterTuningJob.html\n\nAccording to my testing, the SagemakerTuner in the python SDK simply ignores the environment variables set in the passed estimator.\n\nIs there any way to pass environment variables to the training jobs started by a tuner job programmatically, or is that currently unsupported?",
        "Answers":[
            {
                "Answer_creation_time":"2022-11-29T15:42:27.973Z",
                "Answer_upvote_count":1,
                "Answer_body":"Thanks for raising this. Yes, as you point out the Environment collection is not supported in the underlying CreateHyperparameterTuningJob API and therefore the SageMaker Python SDK can't make use of it when running a tuner.\n\nAs discussed on the SM Py SDK GitHub issue here, you might consider using hyperparameters instead to pass parameters through to the job?\n\nIf you specifically need environment variables for some other process\/library, you could also explore setting the variables from your Python script (perhaps to map from hyperparam to env var?).\n\nOr another option could be to customize your container image to bake in the variable via the ENV command? For example to customize an existing AWS Deep Learning Container (framework container), you could:\n\nUse sagemaker.image_uris.retrieve(...) to find the base image URI for your given framework, version, region, etc. You'll need to authenticate Docker to this registry as well as your own Amazon ECR account.\nCreate a Dockerfile that takes this base image URI as an arg and builds FROM it, something like this example\nAdd the required ENV commands to bake in the (static) environment variables you need\ndocker build your custom container (passing in the base image URI as a --build-arg), upload it to Amazon ECR, and use in your SageMaker training job.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Running a request against all variants in an endpoint",
        "Question_creation_time":1604486652000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6bm-EMtOQV6robgbTXClLQ\/running-a-request-against-all-variants-in-an-endpoint",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":14.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I have a customer asking me about the Rendezvous architecture. What I'm thinking is, we could implement this in a number of ways, all using endpoint variants:\n\nLambda (and probably SQS) around the endpoint;\nA custom monitoring job;\nStep Functions\n\nWithout going into details of the above options or of how the evaluation and SLA check will be done, it looks like the several models would fit very well as variants of an endpoint. The thing is, the architecture expects to call them all. Is there a way to directly call all variants of a model, or will a wrapper to identify the variants, call them all and process the results be needed?",
        "Answers":[
            {
                "Answer_creation_time":"2020-11-04T16:22:44.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"When I last looked into it, it was not possible to query all versions\/variants of the model automatically. You can specify what variant to use when using the invoke_endpoint method. I would therefore write a lambda function to invoke each of the endpoints one-by-one (see here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpoint.html). To be especially rigorous about it, you can add a function in your lambda code that first retrieves all the endpoint variants (see here: https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.describe_endpoint) then queries them one-by-one, and returns all the results.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"not able to add sagemaker dependencies as external dependencies to lambda",
        "Question_creation_time":1568641875000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUg5l3Jjl4SISDvnjIVYcqaA\/not-able-to-add-sagemaker-dependencies-as-external-dependencies-to-lambda",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":134.0,
        "Question_answer_count":3,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi Team,\nI'm trying to package sagemaker dependencies as external dependcies to upload to lambda.\nBut I'm getting the max size limit error. Package size is more than allowed size limit i.e.. deployment package size is 50 MB.\nAnd the reason I'm trying to do this is, 'get_image_uri' api is not accessible with boto3.\nsample code for this api :\n#Import the get_image_url utility function Amazon SageMaker Python SDK and get the location of the XGBoost container.\n\nimport sagemaker\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\ncontainer = get_image_uri(boto3.Session().region_name, 'xgboost')\n\nAny reference would be of great help. Thank you.",
        "Answers":[
            {
                "Answer_creation_time":"2019-09-16T13:56:24.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Could you explain in more detail why do you want to have sagemaker inside of a lambda please?",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2019-09-18T17:22:05.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Requirement is to train sagemaker model from lambda. So in the trainingconfiguration we have to pass the container for algorithm image.\nso container definition can either have hardcoded image uris and regions mapping, or, as below code snipper where you can get the latest image for specified region and algorithm.\n\ncontainer = get_image_uri(boto3.Session().region_name, 'xgboost')\n\nso to access the above api 'get_image_uri' I need to do below import.\nimport sagemaker\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\n\nThis is not supported by default in lambda.so I'm trying to package sagemaker dependencies as external dependencies and deploy in lambda.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-09-18T08:23:53.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"As s3 allows more size than direct upload, I even tried with uploading from s3 path.\nBut since sagemaker has numpy dependency and default numpy doesn't work with python containers, I had to add customized numpy packages for aws lambda, which raised the compressed file size to 105MB.\nNow that's also not working as it supports only upto 100MB and uncompressed size of less than 250MB.\nand reference or suggestion would be of great help.Thank you.",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"How to create a serverless endpoint configuration?",
        "Question_creation_time":1645067206226,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfmAxh_aDQiS2nk0gbDicsg\/how-to-create-a-serverless-endpoint-configuration",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":129.0,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"based on the sample code provided here , https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config\n\nI created a model via lambda, now when i try to create a serverless endpoint config (sample code below) , i keep getting -> parameter validation failed unknown parameter in ProductVariants [ 0 ]: \"ServerlessConfig\", must be one of : VairantName, ModelName, InitialInstanceCount , Instancetype...\n\nresponse = client.create_endpoint_config(\n   EndpointConfigName=\"endpoint-new\",\n   ProductionVariants=[\n        {\n            \"ModelName\": \"MyModel\",\n            \"VariantName\": \"AllTraffic\",\n            \"ServerlessConfig\": {\n                \"MemorySizeInMB\": 2048,\n                \"MaxConcurrency\": 10\n            }\n        } \n    ]\n)",
        "Answers":[
            {
                "Answer_creation_time":"2022-02-17T09:30:12.370Z",
                "Answer_upvote_count":1,
                "Answer_body":"The cause might be that your SageMaker Python SDK is not updated to the latest version. Please make sure you update it to the latest version as well as the AWS SDK for Python (boto3). You can use pip:\n\npip install --upgrade boto3\npip install --upgrade sagemaker\n\n\nFor a sample notebook you can have a look here. More information on the documentation page.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-02-17T09:37:24.602Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi,\n\nCan you confirm the version of boto3 that you are using? This error is likely to be caused because of an older version of boto3 that does not include capability for serverless inference.\n\nServerless inference was introduced in version 1.20.18\n\nHope this helps,\n\nGeorgios",
                "Answer_has_accepted":false
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Deploying multiple Comprehend Custom Classifiers (multi-label mode)",
        "Question_creation_time":1664215795645,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEQiFVzOhR5q1XYhjlltO7w\/deploying-multiple-comprehend-custom-classifiers-multi-label-mode",
        "Question_topic":[
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "DevOps",
            "Amazon Comprehend",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":36.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I want to train and deploy multiple comprehend custom classifiers (for example 50 models). I want to be able to classify my documents in near real-time (a couple of seconds are fine) 24\/7. The problem is that deploying one end-point for each classifier is very expensive, especially that one or two IU would be enough for all my models combined (I am expecting to process around 10 document a minute total\/length of one document is around 1000 characters ). Is there a way where I can deploy multiple models behind the same endpoint (similar to the multi-model endpoint in SageMaker)? Or maybe do an asynchronous approach and somewho make sure I get the response within seconds?",
        "Answers":[
            {
                "Answer_creation_time":"2022-10-24T19:34:11.311Z",
                "Answer_upvote_count":0,
                "Answer_body":"No , Comprehend don't support hosting multiple models with the same endpoint right now. Thanks for your suggestions . We will take them into consideration .",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"xgboost sagemaker batch transform job output in multiple lines",
        "Question_creation_time":1599771185000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYz7Bz_5sTmG0uBaqlt7J_g\/xgboost-sagemaker-batch-transform-job-output-in-multiple-lines",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":199.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hello,\n\nI've just trained a churn prediction model with XGBoost algorithm, based on the SageMaker example notebooks. I've created SageMaker batch transformation jobs using this model using input from CSV file with multiple records, however the output file is a single record CSV containing all the inferences in a single comma separated row. The result is that I'm not able to use the \"Join source\" feature with \"Input - Merge input data with job output\" since the input and output files must match the number of records. I've tried with different batch job configurations but I always get the same single line output file.\n\nDo you know if is there any configuration that allows me to merge input and output in order to have a direct association between an input column with its inference result? Is this a restriction from the XGBoost algorithm built-in implementation?",
        "Answers":[
            {
                "Answer_creation_time":"2020-09-11T02:38:30.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Sounds like a configuration issue, this algorithm should be able to output proper output CSVs.\n\nAre you using accept=\"text\/csv\" and assemble_with=\"Line\" on your Transformer? Is your strategy set to SingleRecord or MultiRecord?\n\nAnd split_type=\"Line\", content_type=\"text\/csv\" on the .transform() call?\n\nI have had custom algorithms accidentally output row vectors instead of column vectors for multi-record batches in the past (because they gave a 1D output which the default serializer interpreted as a row), but not built-in algorithms.\n\nDropping to SingleRecord could be a last resort (forcing Batch Transform itself to handle the serialization), but would decrease efficiency\/speed.",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    },
    {
        "Question_title":"Sagemaker Built-in Algorithms",
        "Question_creation_time":1652686627960,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDkYruiibS9S05bzFSkLaxg\/sagemaker-built-in-algorithms",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Service Catalog"
        ],
        "Question_upvote_count":0.0,
        "Question_view_count":76.0,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I am exploring the Sagemaker Built-in algorithms, and I am curious to learn more about the details of the algorithms. However, I am surprised that it is hard to find any references for the research background and implementation details in the numerous documents and tutorials for particular algorithms. If such information exists somewhere, I would highly appreciate a pointer. Thanks a lot in advance!",
        "Answers":[
            {
                "Answer_creation_time":"2022-05-16T08:11:20.111Z",
                "Answer_upvote_count":0,
                "Answer_body":"thanks for your interest in the built-in algorithms! You can find research papers in the documentation of many of them. And documentation page has a section \"how it works\" explaining the science of every algorithm. For example:\n\nBlazingText: BlazingText: Scaling and Accelerating Word2Vec using Multiple GPUs, Gupta et Khare\nDeepAR DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks, Salinas et al.\nFactorization Machines\nIP Insights\nKMeans\nKNN\nLDA\nLinear Learner\nNTM\nObject2Vec\nObject Detection (it's an SSD model)\nPCA\nRandom Cut Forest: Robust Random Cut Forest Based Anomaly Detection On Streams, Guha et al\nSemantic Segmentation\nSeq2seq\nXGBoost",
                "Answer_has_accepted":true
            }
        ],
        "Tool":"Amazon SageMaker",
        "Question_comment_count":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null
    }
]