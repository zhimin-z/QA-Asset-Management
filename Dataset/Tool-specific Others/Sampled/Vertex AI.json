[
    {
        "Question_id":71076952,
        "Question_title":"How to get a placeholder\/dummy step in SageMaker pipelines?",
        "Question_body":"<p>I'd like to sketch out some steps in a SageMaker pipeline, and only fill them in one at a time, but I don't think there's an EmptyStep option anywhere.<\/p>\n<p>I've considered using some vacuously true ConditionalSteps, or subclassing <code>sagemaker.workflow.steps.Step<\/code>, but the former can't be chained, and the latter seems likely to break things, given my implementation wouldn't necessarily conform to what the service is looking for.<\/p>\n<p>Is there a good way to go about this? An empty processor step?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-11 08:15:14.037 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-02-23 22:19:41.723 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":111,
        "Owner_creation_date":"2011-10-19 03:32:40.013 UTC",
        "Owner_last_access_date":"2022-09-24 15:14:11.2 UTC",
        "Owner_reputation":2966,
        "Owner_up_votes":1244,
        "Owner_down_votes":168,
        "Owner_views":257,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Seattle, WA",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":45351020,
        "Question_title":"how to add the column names to the input dataset using R script in Machine Learning model",
        "Question_body":"<p>I am trying to add the column names to the input dataset using below R script.<\/p>\n\n<pre><code>dataset1 &lt;- maml.mapInputPort(1)#class: data.frame\n# Sample operation\ncols &lt;- c(\"age\",\n    \"workclass\",\n    \"fnlwgt\",\n    \"education\",\n    \"education-num\",\n    \"marital-status\",\n    \"occupation\",\n    \"relationship\",\n    \"race\",\n    \"sex\",\n    \"capital-gain\",\n    \"capital-loss\",\n    \"hours-per-week\",\n    \"native-country\",\n    \"income\")\n colnames(data.frame) &lt;- cols\n data.set = dataset1;\n maml.mapOutputPort(\"data.set\");\n<\/code><\/pre>\n\n<p>But I am getting the error like below figure.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/m4vwp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/m4vwp.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Can you please tell me how to add the column names to the input dataset using R script in Machine Learning model?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2017-07-27 12:36:30.353 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"r|azure-machine-learning-studio",
        "Question_view_count":224,
        "Owner_creation_date":"2015-10-21 11:13:32.267 UTC",
        "Owner_last_access_date":"2022-09-07 05:24:44.26 UTC",
        "Owner_reputation":4594,
        "Owner_up_votes":240,
        "Owner_down_votes":22,
        "Owner_views":1089,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Bangalore, Karnataka, India",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":52819122,
        "Question_title":"What is the best practice on folder structure for Azure Machine Learning service (preview) projects",
        "Question_body":"<p>I'm very excited on the newly released Azure Machine Learning service (preview), which is a great step up from the previous (and deprecated) Machine Learning Workbench.<\/p>\n\n<p>However, I am thinking a lot about the best practice on structuring the folders and files in my project(s). I'll try to explain my thoughts.<\/p>\n\n<p>Looking at the documentation for the training of a model (e.g. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml#create-an-estimator\" rel=\"nofollow noreferrer\">Tutorial #1<\/a>), there seems to be good-practice to put all training scripts and necessary additional scripts inside a subfolder, so that it can be passed into the <code>Estimator<\/code> object without also passing all other files in the project. This is fine.<\/p>\n\n<p>But when working with the deployment of the service, specifically the deployment of the image, the documentation (e.g. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml#deploy-in-aci\" rel=\"nofollow noreferrer\">Tutorial #2<\/a>) seems to indicate that the scoring script need to be located in the root folder. If I try to refer to a script located in a subfolder, I get an error message saying<\/p>\n\n<p><code>WebserviceException: Unable to use a driver file not in current directory. Please navigate to the location of the driver file and try again.<\/code><\/p>\n\n<p>This may not be a big deal. Except, I have some additional scripts that I import both in the training script and in the scoring script, and I don't want to duplicate those additional scripts to be able to import them in both the training and the scoring scripts.<\/p>\n\n<p>I am working mainly in Jupyter Notebooks when executing the training and the deployment, and I could of course use some tricks to read the particular scripts from some other folder, save them to disk as a copy, execute the training or deployment while referring to the copies and finally delete the copies. This would be a decent workaround, but it seems to me that there should be a better way than just decent.<\/p>\n\n<p>What do you think?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-10-15 14:33:51.87 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":782,
        "Owner_creation_date":"2016-05-20 15:01:49.237 UTC",
        "Owner_last_access_date":"2022-09-05 14:52:13.07 UTC",
        "Owner_reputation":400,
        "Owner_up_votes":146,
        "Owner_down_votes":0,
        "Owner_views":43,
        "Answer_body":"<p>Currently, the score.py needs to be in current working directory, but dependency scripts - the <em>dependencies<\/em> argument to  <em>ContainerImage.image_configuration<\/em> - can be in a subfolder.<\/p>\n\n<p>Therefore, you should be able to use folder structure like this:<\/p>\n\n<pre><code>.\/score.py \n.\/myscripts\/train.py \n.\/myscripts\/common.py\n<\/code><\/pre>\n\n<p>Note that the relative folder structure is preserved during web service deployment; if you reference the common file in subfolder from your score.py, that reference should be valid within deployed image.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2018-10-23 15:41:31.403 UTC",
        "Answer_score":0.0,
        "Owner_location":"Uppsala, Sverige",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":66497968,
        "Question_title":"AWS Sagemaker ValueError: Unsupported dtype object on array when using strings and dates",
        "Question_body":"<p>I have a CSV file I'm trying to RCF on.  If I put a date or string in the CSV then I get an error like the one below.  If I limit it to just the integer and float fields the script runs fine.  Is there some way to process dates and string?  I see the taxi example from AWS and it has dates which appear the same as mine<\/p>\n<pre><code>eventData = pd.read_csv(data_location, delimiter=&quot;,&quot;, header=None, parse_dates=True)\n\nprint('Starting RCF Training')\n# specify general training job information\nrcf = RandomCutForest(role=sagemaker.get_execution_role(),\n                      instance_count=1,\n                      instance_type='ml.m4.xlarge',\n                      data_location=data_location,\n                      output_path='s3:\/\/{}\/{}\/output'.format(bucket, prefix),\n                      base_job_name=&quot;ad-rcf&quot;,\n                      num_samples_per_tree=512,\n                      num_trees=50)\n\nrcf.fit(rcf.record_set(eventData.values))\n<\/code><\/pre>\n<p>CSV Data that fails<\/p>\n<pre><code>392507,1613744,1\/2\/2020 19:11,1577238693,2469,3.30E+01,-9.67E+01\n691381,1888551,12\/10\/2019 9:22,1575641745,3460,2.37E+01,9.04E+01\n392507,1613744,1\/2\/2020 19:20,1577236815,1797,3.30E+01,-9.67E+01\n392507,1613744,1\/29\/2020 19:04,1577264188,1797,3.30E+01,-9.67E+01\n<\/code><\/pre>\n<p>Error output<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-35-ba19bf5d66a2&gt; in &lt;module&gt;\n---&gt; 21 rcf.fit(rcf.record_set(eventData.values))\n     22 \n     23 print('Done RCF Training')\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/amazon\/amazon_estimator.py in record_set(self, train, labels, channel, encrypt)\n    281         logger.debug(&quot;Uploading to bucket %s and key_prefix %s&quot;, bucket, key_prefix)\n    282         manifest_s3_file = upload_numpy_to_s3_shards(\n--&gt; 283             self.instance_count, s3, bucket, key_prefix, train, labels, encrypt\n    284         )\n    285         logger.debug(&quot;Created manifest file %s&quot;, manifest_s3_file)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/amazon\/amazon_estimator.py in upload_numpy_to_s3_shards(num_shards, s3, bucket, key_prefix, array, labels, encrypt)\n    443                 s3.Object(bucket, key_prefix + file).delete()\n    444         finally:\n--&gt; 445             raise ex\n    446 \n    447 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/amazon\/amazon_estimator.py in upload_numpy_to_s3_shards(num_shards, s3, bucket, key_prefix, array, labels, encrypt)\n    424                     write_numpy_to_dense_tensor(file, shard, label_shards[shard_index])\n    425                 else:\n--&gt; 426                     write_numpy_to_dense_tensor(file, shard)\n    427                 file.seek(0)\n    428                 shard_index_string = str(shard_index).zfill(len(str(len(shards))))\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/amazon\/common.py in write_numpy_to_dense_tensor(file, array, labels)\n    154             )\n    155         resolved_label_type = _resolve_type(labels.dtype)\n--&gt; 156     resolved_type = _resolve_type(array.dtype)\n    157 \n    158     # Write each vector in array into a Record in the file object\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/amazon\/common.py in _resolve_type(dtype)\n    288     if dtype == np.dtype(&quot;float32&quot;):\n    289         return &quot;Float32&quot;\n--&gt; 290     raise ValueError(&quot;Unsupported dtype {} on array&quot;.format(dtype))\n    291 \n    292 \n\nValueError: Unsupported dtype object on array\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-05 18:44:17.063 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":1395,
        "Owner_creation_date":"2012-09-06 15:44:12.34 UTC",
        "Owner_last_access_date":"2022-09-16 20:59:36.593 UTC",
        "Owner_reputation":136,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":39,
        "Answer_body":"<p>Figured out my issue, the RCF can't handle dates and strings.  There's this page for the Kenesis offering from AWS that covers the same Random Cut Forest algorithm <a href=\"https:\/\/docs.aws.amazon.com\/kinesisanalytics\/latest\/sqlref\/sqlrf-random-cut-forest.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/kinesisanalytics\/latest\/sqlref\/sqlrf-random-cut-forest.html<\/a>  It says the function only supports &quot;The algorithm accepts the DOUBLE, INTEGER, FLOAT, TINYINT, SMALLINT, REAL, and BIGINT data types.&quot;<\/p>\n<p>The gotcha part that AWS does with the NYC Taxi example is they use .value which is referring to only the value column of the data.  They are basically dropping the dates from the RCF as a feature.  It doesn't help that .values on the array does work and looks very similar to .value<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-03-10 04:05:15.253 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":73745159,
        "Question_title":"How to call a model's artifacts (pickeled vectorizer) when the model is on Production in databricks?",
        "Question_body":"<p>I am using databrick, machine learning view. I have successfully created and saved my model and also logged my pickled vectorizer as artifacts to it. I would like to load it in a different notebook; the model and the artifacts belong to the model which is currently in production.<\/p>\n<pre><code>   import mlflow.pyfunc\n\nmodel_name = &quot;Sentiment&quot;\nstage = 'Production'\n\nmodel = mlflow.pyfunc.load_model(\n    model_uri=f&quot;models:\/{model_name}\/{stage}&quot;\n)\n<\/code><\/pre>\n<p>So this code seems to be working but it does not load the artifacts or if it does, i do not know how to display them.<\/p>\n<p>I have found this code but not sure what to do with it to only get the artifacts from the model which is in Production.<\/p>\n<pre><code>  from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\nversion =1\nmodel_uri = MlflowClient.get_model_version_download_uri(name=model_name, version=version)\nModelsArtifactRepository(model_uri).download_artifacts(artifact_path=&quot;&quot;)\n<\/code><\/pre>\n<p>even when i run this i get an error :<\/p>\n<p>TypeError: get_model_version_download_uri() missing 1 required positional argument: 'self'<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-16 12:56:25.133 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-09-16 13:15:02.563 UTC",
        "Question_score":0,
        "Question_tags":"model|databricks|mlflow|artifacts",
        "Question_view_count":10,
        "Owner_creation_date":"2022-06-06 08:24:57.99 UTC",
        "Owner_last_access_date":"2022-09-19 13:50:16.62 UTC",
        "Owner_reputation":47,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":53280902,
        "Question_title":"AWS SageMaker pd.read_pickle() doesn't work but read_csv() does?",
        "Question_body":"<p>I've recently been trying to train some models on an AWS SageMaker jupyter notebook instance.<\/p>\n\n<p>Everything is worked very well until I tried to load in some custom dataset (REDD) through files.<\/p>\n\n<p>I have the dataframes stored in Pickle (.pkl) files on an S3 bucket. I couldn't manage to read them into sagemaker so I decided to convert them to csv's as this seemed to work but I ran into a problem. This data has an index of type datetime64 and when using <code>.to_csv()<\/code> this index gets converted to pure text and it loses it's data structure (and I need to keep this specific index for correct plotting.)<\/p>\n\n<p>So I decided to try the Pickle files again but I can't get it to work and have no idea why.<\/p>\n\n<p>The following code for csv's works but I can't use it due to the index problem:<\/p>\n\n<pre><code>bucket = 'sagemaker-peno'\nhouses_dfs = {}\ndata_key = 'compressed_data\/'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\nfor file in range(6):\n    houses_dfs[file+1] = pd.read_csv(data_location+'house_'+str(file+1)+'.csv', index_col='Unnamed: 0')\n<\/code><\/pre>\n\n<p>But this code does NOT work even though it uses almost the exact same syntax:<\/p>\n\n<pre><code>bucket = 'sagemaker-peno'\nhouses_dfs = {}\ndata_key = 'compressed_data\/'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\nfor file in range(6):\n    houses_dfs[file+1] =  pd.read_pickle(data_location+'house_'+str(file+1)+'.pkl')\n<\/code><\/pre>\n\n<p>Yes it's 100% the correct path, because the csv and pkl files are stored in the same directory (compressed_data).<\/p>\n\n<p>It throws me this error while using the Pickle method:<\/p>\n\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/sagemaker-peno\/compressed_data\/house_1.pkl'\n<\/code><\/pre>\n\n<p>I hope to find someone who has dealt with this before and can solve the <code>read_pickle()<\/code> issue or as an alternative fix my datetime64 type issue with csv's.<\/p>\n\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-11-13 12:20:48.037 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"pandas|csv|amazon-s3|pickle|amazon-sagemaker",
        "Question_view_count":788,
        "Owner_creation_date":"2015-08-09 12:51:41.797 UTC",
        "Owner_last_access_date":"2022-07-07 20:29:24.31 UTC",
        "Owner_reputation":87,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Answer_body":"<p>read_pickle() likes the full path more than a relative path from where it was run. This fixed my issue.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-11-26 20:32:53.167 UTC",
        "Answer_score":-1.0,
        "Owner_location":"Leuven, Belgi\u00eb",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":60975078,
        "Question_title":"How to only load one portion of an AzureML tabular dataset (linked to Azure Blob Storage)",
        "Question_body":"<p>I have a DataSet defined in my AzureML workspace that is linked to an Azure Blob Storage csv file of 1.6Gb.  This file contains timeseries information of around 10000 devices.  So, I could've also created 10000 smaller files (since I use ADF for the transmission pipeline).<\/p>\n\n<p>My question now is: is it possible to load a part of the AzureML DataSet in my python notebook or script instead of loading the entire file?<br>\nThe only code I have now load the full file:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>dataset = Dataset.get_by_name(workspace, name='devicetelemetry')\ndf = dataset.to_pandas_dataframe()\n<\/code><\/pre>\n\n<p>The only concept of partitions I found with regards to the AzureML datasets was around time series and partitioning of timestamps &amp; dates.  However, here I would love to partition per device, so I can very easily just do a load of all telemetry of a specific device.<\/p>\n\n<p>Any pointers to docs or any suggestions? (I couldn't find any so far)<\/p>\n\n<p>Thanks already<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-01 15:53:54.767 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|azure-machine-learning-service",
        "Question_view_count":560,
        "Owner_creation_date":"2013-02-12 07:50:30.743 UTC",
        "Owner_last_access_date":"2022-09-21 18:28:12.907 UTC",
        "Owner_reputation":2947,
        "Owner_up_votes":297,
        "Owner_down_votes":16,
        "Owner_views":355,
        "Answer_body":"<p>You're right there are the <code>.time_*()<\/code> filtering methods available with a <code>TabularDataset<\/code>.<\/p>\n\n<p>I'm not aware of anyway to do filtering as you suggest (but I agree it would be a useful feature). To get per-device partitioning, my recommendation would be to structure your container like so:<\/p>\n\n<pre><code>- device1\n    - 2020\n        - 2020-03-31.csv\n        - 2020-04-01.csv\n- device2\n   - 2020\n        - 2020-03-31.csv\n        - 2020-04-01.csv\n<\/code><\/pre>\n\n<p>In this way you can define an all-up Dataset, but also per-device Datasets by passing folder of the device to the DataPath<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># all up dataset\nds_all = Dataset.Tabular.from_delimited_files(\n    path=DataPath(datastore, '*')\n)\n# device 1 dataset\nds_d1 = Dataset.Tabular.from_delimited_files(\n    path=DataPath(datastore, 'device1\/*')\n)\n<\/code><\/pre>\n\n<p><strong>CAVEAT<\/strong><\/p>\n\n<p>dataprep SDK is optimized for blobs around 200MB in size. So you can work with many small files, but sometimes it can be slower than expected, especially considering the overhead of enumerating all blobs in a container.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-04-01 17:12:16.673 UTC",
        "Answer_score":1.0,
        "Owner_location":"Belgium",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":8995598,
        "Question_title":"Can I mark a branch as 'not going to push'?",
        "Question_body":"<p>I use named branches in Mercurial.<\/p>\n\n<p>In doing so I have created one branch called playground where I can try out various wacky experiments.  I never intend to merge this branch into any others and I never want to push it to our main repository.<\/p>\n\n<p>Since creating it, every time I do a push I am told I have added a new branch and I have to use the <code>--new-branch<\/code> flag. At this point <code>hg push -b default<\/code> (or whatever branch I'm pushing) works fine but it's annoying. Is there any way to suppress that message by letting Hg know that I am not interested in pushing that branch ever?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2012-01-24 23:04:39.733 UTC",
        "Question_favorite_count":6.0,
        "Question_last_edit_date":"2012-03-10 10:47:28.14 UTC",
        "Question_score":35,
        "Question_tags":"mercurial|branch|push|dvcs",
        "Question_view_count":7889,
        "Owner_creation_date":"2008-09-07 15:43:17.647 UTC",
        "Owner_last_access_date":"2022-09-24 03:20:58.157 UTC",
        "Owner_reputation":112797,
        "Owner_up_votes":3646,
        "Owner_down_votes":135,
        "Owner_views":9687,
        "Answer_body":"<p>Starting with Mercurial 2.1 (released in February 2012), you can mark your changesets <strong>secret<\/strong> to keep them from being pushed to another repository. You use the new <a href=\"https:\/\/www.mercurial-scm.org\/wiki\/Phases\" rel=\"nofollow noreferrer\"><code>hg phase<\/code> command<\/a> to do this:<\/p>\n\n<pre><code>$ hg phase --force --secret .\n<\/code><\/pre>\n\n<p>This mark the current working directory parent revision (<code>.<\/code>) as being in the <code>secret<\/code> phase. Secret changesets are local to your repository: they wont be pushed or pulled. Pushing now looks like this:<\/p>\n\n<pre><code>$ hg push\npushing to \/home\/mg\/tmp\/repo\nsearching for changes\nno changes to push but 2 secret changesets\n<\/code><\/pre>\n\n<p>There is no equivalent mechanism in older versions of Mercurial. There your best bet is to create a local clone for the changesets you don't want to push.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2012-01-25 09:54:53.55 UTC",
        "Answer_score":57.0,
        "Owner_location":"United States",
        "Answer_last_edit_date":"2018-06-02 13:22:04.577 UTC",
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":67213383,
        "Question_title":"Training on Sagemaker GPU is too slow",
        "Question_body":"<p>I've launched a training on CelebA dataset for a binary classification with PyTorch, in Sagemaker Studio.<\/p>\n<p>I've made sure all, model, tensors are sent to cuda().<\/p>\n<p>My image dataset is in S3, and I'm accessing it via this import and code:<\/p>\n<pre><code>from PIL import Image\nimport s3fs\n\nfs = s3fs.S3FileSystem()\n\n# example\nf = fs.open(f's3:\/\/aoha-bucket\/img_celeba\/dataset\/000001.jpg')\n<\/code><\/pre>\n<p>And of course my PyTorch DataLoader class, which is using of s3fs to load data into DataLoaders.<\/p>\n<pre><code>class myDataset(Dataset):\n    def __init__(self, csv_file, root_dir, target, length, adv = None, transform=None):\n        self.annotations = pd.read_csv(csv_file).iloc[:length,:]\n        self.root_dir = root_dir\n        self.transform = transform\n        self.target = target\n        self.length = length\n        self.adv = adv\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        img_path = fs.open(os.path.join(self.root_dir, self.annotations.loc[index, 'image_id']))\n        image = Image.open(img_path)\n        image = np.array(image)\n\n        if self.transform:\n            image = self.transform(image=image)[&quot;image&quot;]\n\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        image = torch.Tensor(image)\n\n        y_label = torch.tensor(int(self.annotations.loc[index, str(self.target)]))\n\n        if self.adv is None:\n            return image, y_label\n\n        if self.adv :\n            z_label = torch.tensor(int(self.annotations.loc[index, 'origin']))\n            return image, y_label, z_label\n<\/code><\/pre>\n<p>when I run this function, I get True:<\/p>\n<pre><code>next(model.parameters()).is_cuda\n<\/code><\/pre>\n<p>My issue is that, I don't know the training is too slow, even slower than my local CPU (not that powerful). It says for example, that one epoch is needing 1h45minutes, which is way too much.<\/p>\n<p>Im' using a GPU optimized PyTorch instance of Studio.<\/p>\n<p>Have you ever launched a training on GPU in Sagemaker using PyTorch ?\nCould you please help ?<\/p>\n<p>Thank you very much,\nHabib<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_date":"2021-04-22 12:32:48.503 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-04-22 22:10:13.807 UTC",
        "Question_score":0,
        "Question_tags":"computer-vision|pytorch|amazon-sagemaker",
        "Question_view_count":119,
        "Owner_creation_date":"2016-06-11 11:42:56.333 UTC",
        "Owner_last_access_date":"2022-09-12 10:44:20.033 UTC",
        "Owner_reputation":29,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Boulogne-Billancourt, France",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":61103291,
        "Question_title":"How to run tensor flow model locally into Mac",
        "Question_body":"<p>I have ML model deployed in Sagemaker. I copied the model ( tar.gz) to my Mac and trying to write the run that tar.gz model file locally . I need to pass input as an image which will go through this model and provide me output locally.How to write the python code to run this full setup locally.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2020-04-08 14:35:00.77 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":-1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":67,
        "Owner_creation_date":"2020-04-08 14:30:45.903 UTC",
        "Owner_last_access_date":"2020-08-15 22:13:00.76 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":66242546,
        "Question_title":"Time Series Forecasting in SageMaker - which instance should I choose?",
        "Question_body":"<p>I am currently trying to forecasting hourly values with a weekly seasonality. In the <code>SARIMAX<\/code> model that means that I have to set m = 168 (24 hours a day and 7 days a week -&gt; 24*7).<\/p>\n<p>My model looks like this:<\/p>\n<pre><code>model = SARIMAX(df['IdCount'],order=(4,1,3),seasonal_order=(2,2,1,168))\nresults = model.fit()\n<\/code><\/pre>\n<p>And I think because of the relatively big <code>m=168<\/code> it takes a very long time to fit the model. When doing this locally, the kernel died.<\/p>\n<p>Therefore, I chose to try it in SageMaker and I am currently using ml.t2.medium instance. In order to perform this task fast and without any issues which instance would you recommend and why? Which is the best one only in regards to performance and which is the best one regarding cost benefit ratio?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-02-17 13:07:25.233 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|performance|amazon-sagemaker|arima",
        "Question_view_count":148,
        "Owner_creation_date":"2019-05-29 11:58:00.073 UTC",
        "Owner_last_access_date":"2022-09-23 09:10:17.557 UTC",
        "Owner_reputation":1166,
        "Owner_up_votes":555,
        "Owner_down_votes":1,
        "Owner_views":248,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":9431809,
        "Question_title":"mercurial_keyring extension for hg - How can I change the keyring password?",
        "Question_body":"<p>I recently installed mercurial on my ubuntu server (running Ubuntu Server 11.10) and with it the oh so useful mercurial_keyring extension. In setting things up, I mistakenly entered a keyring password that I do not like...<\/p>\n\n<p>This bothers me a bit, because now every operation with my remote repository asks me for this password...<\/p>\n\n<p>Is there a way to change the keyring password easily?<\/p>\n\n<pre><code>mysuperusername@mysuperserver:\/var\/www\/...$ hg push\npushing to https:\/\/...@bitbucket.org\/...\/...\nPlease input your password for the keyring\nPassword:\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2012-02-24 13:49:26.35 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2012-02-27 13:30:11.477 UTC",
        "Question_score":0,
        "Question_tags":"python|mercurial|dvcs|mercurial-extension",
        "Question_view_count":759,
        "Owner_creation_date":"2012-02-24 13:44:39.143 UTC",
        "Owner_last_access_date":"2022-08-24 13:57:41.893 UTC",
        "Owner_reputation":2040,
        "Owner_up_votes":10,
        "Owner_down_votes":1,
        "Owner_views":329,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Quebec, Canada",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":72035831,
        "Question_title":"Mlflow \"load_model\" goes in deadlock",
        "Question_body":"<p>Trying to load a model from past run in mlflow, in jupyterlab, never finishes. After waiting for hours, interrupting the run throws the below state.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/NgxDL.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NgxDL.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<pre><code>---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nInput In [21], in &lt;cell line: 2&gt;()\n      1 logged_model = 'runs:\/7f6932baef144fa69847ba11ef66f8e6\/model\/'\n----&gt; 2 loaded_model = mlflow.tensorflow.load_model(logged_model)\n\nFile ~\/.conda\/envs\/tensorflow\/lib\/python3.8\/site-packages\/mlflow\/tensorflow\/__init__.py:397, in load_model(model_uri, dst_path)\n    360 def load_model(model_uri, dst_path=None):\n    361     &quot;&quot;&quot;\n    362     Load an MLflow model that contains the TensorFlow flavor from the specified path.\n    363 \n   (...)\n    395                                 for _, output_signature in signature_definition.outputs.items()]\n    396     &quot;&quot;&quot;\n--&gt; 397     local_model_path = _download_artifact_from_uri(artifact_uri=model_uri, output_path=dst_path)\n    398     flavor_conf = _get_flavor_configuration(local_model_path, FLAVOR_NAME)\n    399     _add_code_from_conf_to_system_path(local_model_path, flavor_conf)\n\nFile ~\/.conda\/envs\/tensorflow\/lib\/python3.8\/site-packages\/mlflow\/tracking\/artifact_utils.py:95, in _download_artifact_from_uri(artifact_uri, output_path)\n     92     parsed_uri = parsed_uri._replace(path=posixpath.dirname(parsed_uri.path))\n     93     root_uri = prefix + urllib.parse.urlunparse(parsed_uri)\n---&gt; 95 return get_artifact_repository(artifact_uri=root_uri).download_artifacts(\n     96     artifact_path=artifact_path, dst_path=output_path\n     97 )\n\nFile ~\/.conda\/envs\/tensorflow\/lib\/python3.8\/site-packages\/mlflow\/store\/artifact\/runs_artifact_repo.py:125, in RunsArtifactRepository.download_artifacts(self, artifact_path, dst_path)\n    110 def download_artifacts(self, artifact_path, dst_path=None):\n    111     &quot;&quot;&quot;\n    112     Download an artifact file or directory to a local directory if applicable, and return a\n    113     local path for it.\n   (...)\n    123     :return: Absolute path of the local filesystem location containing the desired artifacts.\n    124     &quot;&quot;&quot;\n--&gt; 125     return self.repo.download_artifacts(artifact_path, dst_path)\n\nFile ~\/.conda\/envs\/tensorflow\/lib\/python3.8\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py:242, in ArtifactRepository.download_artifacts(self, artifact_path, dst_path)\n    240 # Check if the artifacts points to a directory\n    241 if self._is_directory(artifact_path):\n--&gt; 242     dst_local_path, inflight_downloads = async_download_artifact_dir(\n    243         src_artifact_dir_path=artifact_path, dst_local_dir_path=dst_path\n    244     )\n    245 else:\n    246     inflight_downloads = async_download_artifact(\n    247         src_artifact_path=artifact_path, dst_local_dir_path=dst_path\n    248     )\n\nFile ~\/.conda\/envs\/tensorflow\/lib\/python3.8\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py:208, in ArtifactRepository.download_artifacts.&lt;locals&gt;.async_download_artifact_dir(src_artifact_dir_path, dst_local_dir_path)\n    206 for file_info in dir_content:\n    207     if file_info.is_dir:\n--&gt; 208         inflight_downloads += async_download_artifact_dir(\n    209             src_artifact_dir_path=file_info.path,\n    210             dst_local_dir_path=dst_local_dir_path,\n    211         )[2]\n    212     else:\n    213         inflight_downloads += async_download_artifact(\n    214             src_artifact_path=file_info.path,\n    215             dst_local_dir_path=dst_local_dir_path,\n    216         )\n\nFile ~\/.conda\/envs\/tensorflow\/lib\/python3.8\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py:199, in ArtifactRepository.download_artifacts.&lt;locals&gt;.async_download_artifact_dir(src_artifact_dir_path, dst_local_dir_path)\n    195 local_dir = os.path.join(dst_local_dir_path, src_artifact_dir_path)\n    196 inflight_downloads = []\n    197 dir_content = [  # prevent infinite loop, sometimes the dir is recursively included\n    198     file_info\n--&gt; 199     for file_info in self.list_artifacts(src_artifact_dir_path)\n    200     if file_info.path != &quot;.&quot; and file_info.path != src_artifact_dir_path\n    201 ]\n    202 if not dir_content:  # empty dir\n    203     if not os.path.exists(local_dir):\n\nFile ~\/.conda\/envs\/tensorflow\/lib\/python3.8\/site-packages\/mlflow\/store\/artifact\/sftp_artifact_repo.py:94, in SFTPArtifactRepository.list_artifacts(self, path)\n     92 artifact_dir = self.path\n     93 list_dir = posixpath.join(artifact_dir, path) if path else artifact_dir\n---&gt; 94 if not self.sftp.isdir(list_dir):\n     95     return []\n     96 artifact_files = self.sftp.listdir(list_dir)\n\nFile ~\/.conda\/envs\/tensorflow\/lib\/python3.8\/site-packages\/pysftp\/__init__.py:652, in Connection.isdir(self, remotepath)\n    650 self._sftp_connect()\n    651 try:\n--&gt; 652     result = S_ISDIR(self._sftp.stat(remotepath).st_mode)\n    653 except IOError:     # no such file\n    654     result = False\n\nFile ~\/.conda\/envs\/tensorflow\/lib\/python3.8\/site-packages\/paramiko\/sftp_client.py:493, in SFTPClient.stat(self, path)\n    491 path = self._adjust_cwd(path)\n    492 self._log(DEBUG, &quot;stat({!r})&quot;.format(path))\n--&gt; 493 t, msg = self._request(CMD_STAT, path)\n    494 if t != CMD_ATTRS:\n    495     raise SFTPError(&quot;Expected attributes&quot;)\n\nFile ~\/.conda\/envs\/tensorflow\/lib\/python3.8\/site-packages\/paramiko\/sftp_client.py:822, in SFTPClient._request(self, t, *arg)\n    820 def _request(self, t, *arg):\n    821     num = self._async_request(type(None), t, *arg)\n--&gt; 822     return self._read_response(num)\n\nFile ~\/.conda\/envs\/tensorflow\/lib\/python3.8\/site-packages\/paramiko\/sftp_client.py:852, in SFTPClient._read_response(self, waitfor)\n    850 while True:\n    851     try:\n--&gt; 852         t, data = self._read_packet()\n    853     except EOFError as e:\n    854         raise SSHException(&quot;Server connection dropped: {}&quot;.format(e))\n\nFile ~\/.conda\/envs\/tensorflow\/lib\/python3.8\/site-packages\/paramiko\/sftp.py:201, in BaseSFTP._read_packet(self)\n    200 def _read_packet(self):\n--&gt; 201     x = self._read_all(4)\n    202     # most sftp servers won't accept packets larger than about 32k, so\n    203     # anything with the high byte set (&gt; 16MB) is just garbage.\n    204     if byte_ord(x[0]):\n\nFile ~\/.conda\/envs\/tensorflow\/lib\/python3.8\/site-packages\/paramiko\/sftp.py:185, in BaseSFTP._read_all(self, n)\n    183             break\n    184 else:\n--&gt; 185     x = self.sock.recv(n)\n    187 if len(x) == 0:\n    188     raise EOFError()\n\nFile ~\/.conda\/envs\/tensorflow\/lib\/python3.8\/site-packages\/paramiko\/channel.py:699, in Channel.recv(self, nbytes)\n    686 &quot;&quot;&quot;\n    687 Receive data from the channel.  The return value is a string\n    688 representing the data received.  The maximum amount of data to be\n   (...)\n    696     if no data is ready before the timeout set by `settimeout`.\n    697 &quot;&quot;&quot;\n    698 try:\n--&gt; 699     out = self.in_buffer.read(nbytes, self.timeout)\n    700 except PipeTimeout:\n    701     raise socket.timeout()\n\nFile ~\/.conda\/envs\/tensorflow\/lib\/python3.8\/site-packages\/paramiko\/buffered_pipe.py:160, in BufferedPipe.read(self, nbytes, timeout)\n    158 while (len(self._buffer) == 0) and not self._closed:\n    159     then = time.time()\n--&gt; 160     self._cv.wait(timeout)\n    161     if timeout is not None:\n    162         timeout -= time.time() - then\n\nFile ~\/.conda\/envs\/tensorflow\/lib\/python3.8\/threading.py:302, in Condition.wait(self, timeout)\n    300 try:    # restore state no matter what (e.g., KeyboardInterrupt)\n    301     if timeout is None:\n--&gt; 302         waiter.acquire()\n    303         gotit = True\n    304     else:\n\nKeyboardInterrupt:\n<\/code><\/pre>\n<p>The mlflow tracking server is working properly for all the other operations. I am able to log params, metrics and artifacts. But I am not able to load a model or retrive any of the artifacts.<\/p>\n<p>Update:<\/p>\n<p>Looks like a bug as per <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/5656\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/5656<\/a>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-27 22:08:48.723 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-04-27 22:28:50.08 UTC",
        "Question_score":0,
        "Question_tags":"python|paramiko|mlflow",
        "Question_view_count":229,
        "Owner_creation_date":"2016-11-15 12:10:53.253 UTC",
        "Owner_last_access_date":"2022-09-23 07:04:48.82 UTC",
        "Owner_reputation":40,
        "Owner_up_votes":31,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":67671898,
        "Question_title":"Creating a Training Job using sagemaker estimator gives me \"error: unrecognized arguments: train\"",
        "Question_body":"<p>I am using a docker image which have all required files and then push it to aws ecr where i can use that image to pass to estimator. I have added the train.py file as entrypoint in dockerfile.<\/p>\n<pre><code>ENTRYPOINT [&quot;python3&quot;, &quot;-m&quot;,&quot;train&quot;]\n<\/code><\/pre>\n<p>This works as required in local with <code>docker run -it image<\/code> <strong>but when running training job i get the error.<\/strong><\/p>\n<pre><code>Training - Training image download completed. Training in progress...usage: train.py [-h] [--epochs EPOCHS] [--learning_rate LEARNING_RATE]\n                [--max_sequence_length MAX_SEQUENCE_LENGTH]\n                [--train_batch_size TRAIN_BATCH_SIZE]\n                [--valid_batch_size VALID_BATCH_SIZE]\ntrain.py: error: unrecognized arguments: train\n<\/code><\/pre>\n<p>The training job using sagemaker estimator:<\/p>\n<pre><code>estimator = sagemaker.estimator.Estimator(image, # docker image\n                                          role,\n                                          train_instance_count=1, \n                                          train_instance_type='ml.p2.xlarge', \n                                          output_path=output_path, \n                                          hyperparameters=hyperparameters,\n                                          sagemaker_session=session\n                                         )\n<\/code><\/pre>\n<p>Train.py main fun():<\/p>\n<pre><code>if __name__ == &quot;__main__&quot;:\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--epochs', type=int, default=2)\n    parser.add_argument('--learning_rate', type=float, default=2e-5)\n    parser.add_argument('--max_sequence_length', type=int, default=512)\n    parser.add_argument('--train_batch_size', type=int, default=12)\n    parser.add_argument('--valid_batch_size', type=int, default=8)\n    # SageMaker environment variables.\n    #parser.add_argument('--hosts', type=str, default=os.environ['SM_HOSTS'])\n    #parser.add_argument('--current_host', type=str, default=os.environ['SM_CURRENT_HOST'])\n    # Parse command-line args and run main.\n    args = parser.parse_args()\n    # Get SageMaker host information from runtime environment variables\n    #sm_hosts = json.loads(args.hosts)\n    #sm_current_host = args.current_host\n    train(args)\n<\/code><\/pre>\n<p>From <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">sagemaker doc<\/a> i was able to find that image in training job runs as <code>docker run image train<\/code> and when i tried to the same in local i got the same error.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2021-05-24 12:13:23.33 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-05-24 12:21:08.093 UTC",
        "Question_score":1,
        "Question_tags":"python|docker|dockerfile|amazon-sagemaker",
        "Question_view_count":652,
        "Owner_creation_date":"2020-07-17 15:56:30.437 UTC",
        "Owner_last_access_date":"2022-09-20 14:09:30.883 UTC",
        "Owner_reputation":193,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Gurgaon, Haryana, India",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":66566031,
        "Question_title":"MLFLow artifact logging and retrieve on remote server",
        "Question_body":"<p>I am trying to setup a MLFlow tracking server on a remote machine as a systemd service.\nI have a sftp server running and created a SSH key pair.<\/p>\n<p>Everything seems to work fine except the artifact logging. MLFlow seems to not have permissions to list the artifacts saved in the <code>mlruns<\/code> directory.<\/p>\n<p>I create an experiment and log artifacts in this way:<\/p>\n<pre><code>uri = 'http:\/\/192.XXX:8000' \nmlflow.set_tracking_uri(uri)\n\nmlflow.create_experiment('test', artifact_location='sftp:\/\/192.XXX:_path_to_mlruns_folder_')\n\nexperiment=mlflow.get_experiment_by_name('test')\nwith mlflow.start_run(experiment_id=experiment.experiment_id, run_name=run_name) as run:\n       mlflow.log_param(_parameter_name_, _parameter_value_)     \n       mlflow.log_artifact(_an_artifact_, _artifact_folder_name_)\n<\/code><\/pre>\n<p>I can see the metrics in the UI and the artifacts in the correct destination folder on the remote machine. However, in the UI I receive this message when trying to see the artifacts:<\/p>\n<blockquote>\n<p>Unable to list artifacts stored\nunder sftp:\/\/192.XXX:<em>path_to_mlruns_folder<\/em>\/<em>run_id<\/em>\/artifacts\nfor the current run. Please contact your tracking server administrator\nto notify them of this error, which can happen when the tracking\nserver lacks permission to list artifacts under the current run's root\nartifact directory.<\/p>\n<\/blockquote>\n<p>I cannot figure out why as the <code>mlruns<\/code> folder has <code>drwxrwxrwx<\/code> permissions and all the subfolders have <code>drwxrwxr-x<\/code>. What am I missing?<\/p>\n<hr \/>\n<p>UPDATE\nLooking at it with fresh eyes, it seems weird that it tries to list files through <code>sftp:\/\/192.XXX:<\/code>, it should just look in the folder <code>_path_to_mlruns_folder_\/_run_id_\/artifacts<\/code>. However, I still do not know how to circumvent that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-10 13:45:56.893 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-03-11 08:34:46.85 UTC",
        "Question_score":3,
        "Question_tags":"python|mlflow",
        "Question_view_count":2283,
        "Owner_creation_date":"2014-06-18 09:47:32.693 UTC",
        "Owner_last_access_date":"2022-09-25 05:06:29.037 UTC",
        "Owner_reputation":2210,
        "Owner_up_votes":1124,
        "Owner_down_votes":117,
        "Owner_views":262,
        "Answer_body":"<p>The problem seems to be that by default the systemd service is run by root.\nSpecifying a user and creating a ssh key pair for that user to access the same remote machine worked.<\/p>\n<pre><code>[Unit]\n\nDescription=MLflow server\n\nAfter=network.target \n\n[Service]\n\nRestart=on-failure\n\nRestartSec=20\n\nUser=_user_\n\nGroup=_group_\n\nExecStart=\/bin\/bash -c 'PATH=_yourpath_\/anaconda3\/envs\/mlflow_server\/bin\/:$PATH exec mlflow server --backend-store-uri postgresql:\/\/mlflow:mlflow@localhost\/mlflow --default-artifact-root sftp:\/\/_user_@192.168.1.245:_yourotherpath_\/MLFLOW_SERVER\/mlruns -h 0.0.0.0 -p 8000' \n\n[Install]\n\nWantedBy=multi-user.target\n<\/code><\/pre>\n<p><code>_user_<\/code> and <code>_group_<\/code> should be the same listed by <code>ls -la<\/code> in the <code>mlruns<\/code> directory.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-03-12 10:16:46.663 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":73318372,
        "Question_title":"several dependency errors causing in Azure AutoML while running model",
        "Question_body":"<p>I am trying to work on different models on a small piece of ML project which needs to work on azure platform and get the score.py with all the values. It is getting not a single library issue, but getting multiple <strong>Module errors<\/strong> and <strong>Attribute errors<\/strong>. I am using latest SDK version only, but I am not sure, where I am going side path.<\/p>\n<p>Any previous observations on this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-11 09:27:26.063 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":29,
        "Owner_creation_date":"2022-04-27 21:21:09.217 UTC",
        "Owner_last_access_date":"2022-08-17 18:50:07.473 UTC",
        "Owner_reputation":41,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Answer_body":"<p>The compatibility break is there for the newer version of the packages based on the current version of <strong>SDK<\/strong>. If the current SDK version is <strong>1.13.0<\/strong> and above, previous versions of packages are not in working stage. The compatibility issue is raising because of support of packages from SDK for different versions. It differs from version-to-version package support from <strong>SDK<\/strong>.<\/p>\n<p>Because of this we are getting Module not found,  <code>ImportError and AttributeError<\/code>.<\/p>\n<p>This solution depends on the AutoML SDK training version.<\/p>\n<ul>\n<li>If you are using 1.13.0 above version of SDK, update the versions of pandas to 0.25.1 and scikit-learn to 0.22.1<\/li>\n<\/ul>\n<p>Using the following command in  <code>BASH<\/code>  to upgrade the versions.<\/p>\n<pre><code>pip install \u2013upgrade pandas==0.25.1\n\npip install \u2013upgrade sickit-learn==0.22.1\n\n<\/code><\/pre>\n<p>The generic syntax for upgrading is:<\/p>\n<pre><code>pip install \u2013upgrade package_name==version\n\n<\/code><\/pre>\n<ul>\n<li>If the error occurs in AutoML Configuration file, then need to upgrade that also.<\/li>\n<li>But it is suggestable to uninstall and reinstall  <code>AutoMLConfig<\/code>.<\/li>\n<\/ul>\n<pre><code>pip uninstall azureml-train automl\n\n<\/code><\/pre>\n<p>Then reinstall using the below code,<\/p>\n<pre><code>pip install azureml-train automl\n\n<\/code><\/pre>\n<p>If you are using windows operating system, then install  <a href=\"https:\/\/docs.conda.io\/en\/latest\/miniconda.html\" rel=\"nofollow noreferrer\">Miniconda<\/a>.<\/p>\n<p>If you are a linux user, then using sudo or conda syntaxes for the same operation.<\/p>\n<p>Some of the advanced libraries of computer vision supportive like TensorFlow will be installed by default. Then we need to install them from dependencies.<\/p>\n<blockquote>\n<pre><code>azureml.core.runconfig import RunConfiguration from\nazureml.core.conda_dependencies import CondaDependencies run_config =\nRunConfiguration() run_config.environment.python.conda_dependencies =\nCondaDependencies.create(conda_packages=['tensorflow==1.12.0']) \n\n<\/code><\/pre>\n<\/blockquote>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-auto-ml#tensorflow\" rel=\"nofollow noreferrer\">Documentation<\/a>  credit to @Larry Franks.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-11 10:18:37.33 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":68434877,
        "Question_title":"AWS Forecast - What does the parameter \"backtest windows\" really mean?",
        "Question_body":"<p>I am new to &quot;AWS Forecast&quot; but I am training a model on retail data using the AWS UI. Two of the fields to setup the model, have optional input for: &quot;Number of backtest windows&quot; and &quot;Backtest window offset&quot;.<\/p>\n<p>Although, these are optional I want to understand what they mean and the AWS description does not help me understand it. Would anyone be able to provide an example with explicit numbers and dates, so that it makes more sense in how it is applied?<\/p>\n<p>Thanks so much.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-19 03:42:18.367 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-07-19 04:25:10.63 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":117,
        "Owner_creation_date":"2021-06-18 16:32:04.133 UTC",
        "Owner_last_access_date":"2022-08-04 07:57:36.273 UTC",
        "Owner_reputation":105,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":56266967,
        "Question_title":"How to solve programming error when storing pandas data frame to snowflake",
        "Question_body":"<p>I'm trying to use SQLAlchemy to store a data frame I created in sagemaker to snowflake. The code only works with certain columns. When I add other columns it gives me an error even though they have the same data type. In the following example, if I only upload TA_ID it works, yet if I upload Cluster_ID, the code throws me an error. <\/p>\n\n<p>I checked SQLAlchemy website but didn't find much information on programming error. <\/p>\n\n<h2>SQL codes used to create table<\/h2>\n\n<pre><code>CREATE OR REPLACE TABLE test.m (\n    TA_ID string,\n     Cluster_ID string\n)\n<\/code><\/pre>\n\n<h2>Python code<\/h2>\n\n<pre><code>master2.to_sql(name='m', con=engine2, if_exists='append',  schema='test',index=False, index_label=None, chunksize=2000 )\n<\/code><\/pre>\n\n<p>ProgrammingError: <\/p>\n\n<pre><code>(snowflake.connector.errors.ProgrammingError) 000904 (42000): SQL compilation error: error line 1 at position 29\ninvalid identifier '\"Cluster_ID\"' [SQL: 'INSERT INTO test.m (\"TA_ID\", \"Cluster_ID\") VALUES (%(TA_ID)s, %(Cluster_ID)s)'] [parameters: ({'TA_ID': 'TA007', 'Cluster_ID': '0'}, {'TA_ID': 'TA007', 'Cluster_ID': '16'}, {'TA_ID': 'TA007', 'Cluster_ID': '40'}, {'TA_ID': 'TA007', 'Cluster_ID': '15'}, {'TA_ID': 'TA007', 'Cluster_ID': '29'}, {'TA_ID': 'TA007', 'Cluster_ID': '23'}, {'TA_ID': 'TA007', 'Cluster_ID': '9'}, {'TA_ID': 'TA007', 'Cluster_ID': '25'}, {'TA_ID': 'TA007', 'Cluster_ID': '42'}, {'TA_ID': 'TA007', 'Cluster_ID': '28'})] (Background on this error at: http:\/\/sqlalche.me\/e\/f405)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-05-23 01:30:22.99 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-05-23 04:54:03.9 UTC",
        "Question_score":0,
        "Question_tags":"sql|pandas|sqlalchemy|amazon-sagemaker|snowflake-cloud-data-platform",
        "Question_view_count":934,
        "Owner_creation_date":"2015-01-23 20:35:22.283 UTC",
        "Owner_last_access_date":"2019-10-22 15:28:10.93 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":71340893,
        "Question_title":"When I get a prediction from sagemaker endpoint, what does the endpoint do?",
        "Question_body":"<p>In sagemaker, the docs talk about inference scripts requiring to have 4 specific functions. When we get a prediction, the python SDK sends a request to the endpoint.<\/p>\n<p>Then the inference script runs. But I cannot find where in the SDK the inference script is run.<\/p>\n<p>When I navigate through the sdk code the <code>Predictor.predict()<\/code> method calls the sagemaker session to post a request to the endpoint and get a response. That is the final step in the sdk. Sagemaker is obviously doing something when it receives that request.<\/p>\n<p>What is the code that it runs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-03 16:58:59.29 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":405,
        "Owner_creation_date":"2020-01-13 16:18:39.743 UTC",
        "Owner_last_access_date":"2022-09-23 12:14:46.19 UTC",
        "Owner_reputation":1012,
        "Owner_up_votes":91,
        "Owner_down_votes":96,
        "Owner_views":66,
        "Answer_body":"<p>The endpoint is essentially a Flask web server running in a Docker container<\/p>\n<p>If it's a scikit-learn image, when you invoke the endpoint, it loads your script from S3, then...<\/p>\n<p>It calls <code>input_fn(request_body: bytearray, content_type) -&gt; np.ndarray<\/code> to parse the <code>request_body<\/code> into a numpy array<\/p>\n<p>Then it calls your <code>model_fn(model_dir: str) -&gt; object<\/code> function to load the model from <code>model_dir<\/code> and return the model<\/p>\n<p>Then it calls <code>predict_fn(input_object: np.ndarray, model: object) -&gt; np.array<\/code>, which calls your <code>model.predict()<\/code> function and returns the prediction<\/p>\n<p>Then it calls <code>output_fn(prediction: np.array, accept: str)<\/code> to take the result from <code>predict_fn<\/code> and encode it to the <code>accept<\/code> type<\/p>\n<p>You don't need to implement all of these functions yourself, as there are defaults<\/p>\n<p>You <strong>do<\/strong> need to implement <code>model_fn<\/code><\/p>\n<p>You only need to implement <code>input_fn<\/code> if you have non numeric data<\/p>\n<p>You only need to implement <code>predict_fn<\/code> if your model uses something other than <code>.predict()<\/code><\/p>\n<p>You can see how the default function implementations work <a href=\"https:\/\/github.com\/aws\/sagemaker-scikit-learn-container\/blob\/master\/src\/sagemaker_sklearn_container\/serving.py\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-03-03 17:38:04.32 UTC",
        "Answer_score":2.0,
        "Owner_location":"Ireland",
        "Answer_last_edit_date":"2022-03-08 17:40:23.947 UTC",
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":56495962,
        "Question_title":"Compare the performance of the models and add annotations to the results",
        "Question_body":"<p>I'm preparing Azure Machine Learning exam and have a question shown below:<\/p>\n\n<p>You are working on an Azure Machine Learning Experiment.<\/p>\n\n<p>You have the dataset configured as shown in the following table:<a href=\"https:\/\/i.stack.imgur.com\/1ckOj.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1ckOj.jpg\" alt=\"table\"><\/a><\/p>\n\n<p>You need to ensure that you can compare the performance of the models and add annotations to the results.<\/p>\n\n<p>A. You consolidate the output of the Score Model modules by using the Add Rows module and then use the Execute R Script module.<\/p>\n\n<p>B. You connect the Score Model modules from each trained model as inputs for the Evaluate Model module and then use the Execute R Script Module.<\/p>\n\n<p>C. You save the output of the Score Model modules as a combined set, and then use the Project Columns modules to select the MAE.<\/p>\n\n<p>D. You connect the Score Model modules from each trained model as inputs for the Evaluate Model module and then save the results as a dataset.<\/p>\n\n<p>I think all of the above are correct but what confuses me is there are different answers from the internet. Some are the same with me, but others are not. I need someone to confirm my answer or explain to me the correct answer.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2019-06-07 14:11:21.893 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"machine-learning|azure-machine-learning-studio",
        "Question_view_count":30,
        "Owner_creation_date":"2015-12-07 02:00:13.69 UTC",
        "Owner_last_access_date":"2022-06-01 02:54:45.017 UTC",
        "Owner_reputation":388,
        "Owner_up_votes":84,
        "Owner_down_votes":2,
        "Owner_views":322,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":62583856,
        "Question_title":"how to log hydra's multi-run in mlflow",
        "Question_body":"<p>I am trying to manage the results of machine learning with mlflow and hydra.\nSo I tried to run it using the multi-run feature of hydra.\nI used the following code as a test.<\/p>\n<pre><code>import mlflow\nimport hydra\nfrom hydra import utils\nfrom pathlib import Path\nimport time\n\n\n@hydra.main('config.yaml')\ndef main(cfg):\n    print(cfg)\n\n\n    mlflow.set_tracking_uri('file:\/\/' + utils.get_original_cwd() + '\/mlruns')\n    mlflow.set_experiment(cfg.experiment_name)\n\n\n    mlflow.log_param('param1',5)\n    # mlflow.log_param('param1',5)\n    # mlflow.log_param('param1',5)\n\n    with mlflow.start_run() :\n        mlflow.log_artifact(Path.cwd() \/ '.hydra\/config.yaml')\n\n\nif __name__ == '__main__':\n    main()\n<\/code><\/pre>\n<p>This code will not work.\nI got the following error<\/p>\n<pre><code>Exception: Run with UUID [RUNID] is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True\n<\/code><\/pre>\n<p>So I modified the code as follows<\/p>\n<pre><code>import mlflow\nimport hydra\nfrom hydra import utils\nfrom pathlib import Path\nimport time\n\n\n@hydra.main('config.yaml')\ndef main(cfg):\n    print(cfg)\n\n\n    mlflow.set_tracking_uri('file:\/\/' + utils.get_original_cwd() + '\/mlruns')\n    mlflow.set_experiment(cfg.experiment_name)\n\n\n    mlflow.log_param('param1',5)\n    # mlflow.log_param('param1',5)\n    # mlflow.log_param('param1',5)\n\n    with mlflow.start_run(nested=True) :\n        mlflow.log_artifact(Path.cwd() \/ '.hydra\/config.yaml')\n\n\nif __name__ == '__main__':\n    main()\n\n<\/code><\/pre>\n<p>This code works, but the artifact is not saved.\nThe following corrections were made to save the artifacts.<\/p>\n<pre><code>import mlflow\nimport hydra\nfrom hydra import utils\nfrom pathlib import Path\nimport time\n\n\n@hydra.main('config.yaml')\ndef main(cfg):\n    print(cfg)\n\n\n    mlflow.set_tracking_uri('file:\/\/' + utils.get_original_cwd() + '\/mlruns')\n    mlflow.set_experiment(cfg.experiment_name)\n\n\n    mlflow.log_param('param1',5)\n    # mlflow.log_param('param1',5)\n    # mlflow.log_param('param1',5)\n\n    \n    mlflow.log_artifact(Path.cwd() \/ '.hydra\/config.yaml')\n\n\nif __name__ == '__main__':\n    main()\n<\/code><\/pre>\n<p>As a result, the artifacts are now saved.\nHowever, when I run the following command<\/p>\n<pre><code>python test.py model=A,B hidden=12,212,31 -m\n<\/code><\/pre>\n<p>Only the artifact of the last execution condition was saved.<\/p>\n<p>How can I modify mlflow to manage the parameters of the experiment by taking advantage of the multirun feature of hydra?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-25 20:30:55.417 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-06-26 07:08:01.413 UTC",
        "Question_score":1,
        "Question_tags":"python|machine-learning|data-science|mlflow|fb-hydra",
        "Question_view_count":1109,
        "Owner_creation_date":"2019-03-21 12:30:34.14 UTC",
        "Owner_last_access_date":"2022-09-23 12:41:46.963 UTC",
        "Owner_reputation":581,
        "Owner_up_votes":26,
        "Owner_down_votes":2,
        "Owner_views":37,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Japan",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":70538098,
        "Question_title":"Databricks MLFlow AutoML XGBoost can't predict_proba()",
        "Question_body":"<p>I used AutoML in Databricks Notebooks for a binary classification problem and the winning model flavor was XGBoost (big surprise).<\/p>\n<p>The outputted model is of this variety:<\/p>\n<pre><code>mlflow.pyfunc.loaded_model:\n      artifact_path: model\n      flavor: mlflow.sklearn\n      run_id: 123456789\n<\/code><\/pre>\n<p>Any idea why when I use <code>model.predict_proba(X)<\/code>, I get this response?<\/p>\n<p><code>AttributeError: 'PyFuncModel' object has no attribute 'predict_proba'<\/code><\/p>\n<p>I know it is possible to get the probabilities because ROC\/AUC is a metric used for tuning the model. Any help would be amazing!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-31 01:02:03.883 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"pandas|scikit-learn|databricks|xgboost|mlflow",
        "Question_view_count":451,
        "Owner_creation_date":"2019-07-11 07:02:37.217 UTC",
        "Owner_last_access_date":"2022-09-21 21:43:57.89 UTC",
        "Owner_reputation":77,
        "Owner_up_votes":35,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":"<p>I had the same issue with catboost model.\nThe way I solved it was by saving the artifacts in a local dir<\/p>\n<pre><code>import os\nfrom mlflow.tracking import MlflowClient\nclient = MlflowClient()\nlocal_dir = &quot;\/dbfs\/FileStore\/user\/models&quot;\nlocal_path = client.download_artifacts('run_id', &quot;model&quot;, local_dir)```\n\n```model_path = '\/dbfs\/FileStore\/user\/models\/model\/model.cb'\nmodel = CatBoostClassifier()\nmodel = model.load_model(model_path)\nmodel.predict_proba(test_set)```\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-03-08 12:56:27.86 UTC",
        "Answer_score":2.0,
        "Owner_location":"San Francisco, CA, USA",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":59000779,
        "Question_title":"Error in converting SageMaker XGBoost model to ONNX model",
        "Question_body":"<p>I'm trying to convert a SageMaker XGBoost model to ONNX, in order to use the ONNX model in .Net application using ML.NET. I've tried to convert the model using <code>winmltools<\/code> and <code>onnxmltools<\/code> but both tools are returned similar error.<\/p>\n\n<p>There is a good resource to use machine learning in business area. I've tried <a href=\"http:\/\/fasttrackteam.com\/using-machine-learning-to-improve-sales-a-simple-example.aspx\" rel=\"noreferrer\">Using Machine Learning to Improve Sales<\/a> in SageMaker to create the model and then convert the model to ONNX model. The example is working well in SageMaker.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/4ftF1.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4ftF1.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>After running the example, I got a model and the type of the model is <code>sagemaker.estimator.Estimator<\/code>. I've tried to convert the model by using <code>winmltools<\/code> and <code>onnxmltools<\/code>. But both are returned same error. <\/p>\n\n<p><code>ValueError: No proper operator name found for '&lt;class 'sagemaker.estimator.Estimator'&gt;'<\/code><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/vQ7kP.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vQ7kP.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I've tried to follow <a href=\"https:\/\/docs.microsoft.com\/en-us\/windows\/ai\/windows-ml\/convert-model-winmltools\" rel=\"noreferrer\">Convert ML models to ONNX with WinMLTools<\/a> and <a href=\"https:\/\/github.com\/onnx\/onnxmltools\" rel=\"noreferrer\">ONNXMLTools enables conversion of models to ONNX<\/a> to convert the SageMaker model to ONNX model.<\/p>\n\n<p>After that, I used <code>xgb.create_model()<\/code> command to create SageMaker model. Then used the tools to convert the model to ONNX. but no luck. I got same error this time. Just the model is different.<\/p>\n\n<p><code>ValueError: No proper operator name found for '&lt;class 'sagemaker.model.Model'&gt;'<\/code><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/QuAiX.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QuAiX.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Then I load the model using <code>pickle<\/code> and tried to convert the model. I got same error, just the model is different.<\/p>\n\n<p><code>ValueError: No proper operator name found for '&lt;class 'xgboost.core.Booster'&gt;'<\/code><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WV2Xy.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WV2Xy.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>At this moment, I have no idea about the issues. How should I solve the issues. I've attached the <a href=\"https:\/\/1drv.ms\/u\/s!An4yDE4xkdIjhOMgu_vBDiYa8VPrgQ?e=r6muMO\" rel=\"noreferrer\">Improve Sales Classification to ONNX notebook file<\/a> for reference.\nCould you please take a look at the issues and let me know a way to solve the issues?\nThanks in advance!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_date":"2019-11-22 19:42:59.947 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":5,
        "Question_tags":"python|machine-learning|xgboost|amazon-sagemaker|onnx",
        "Question_view_count":489,
        "Owner_creation_date":"2012-06-21 18:23:52.6 UTC",
        "Owner_last_access_date":"2022-09-24 23:28:00.993 UTC",
        "Owner_reputation":3426,
        "Owner_up_votes":1775,
        "Owner_down_votes":134,
        "Owner_views":593,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Bangladesh",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":66861409,
        "Question_title":"delete_model() error when cleaning up AWS sagemaker",
        "Question_body":"<p>I followed the tutorial on <a href=\"https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/<\/a><\/p>\n<p>I got an error when trying to clean up with the following code.<\/p>\n<pre><code>xgb_predictor.delete_endpoint()\nxgb_predictor.delete_model()\n<\/code><\/pre>\n<p>ClientError: An error occurred (ValidationException) when calling the DescribeEndpointConfig operation: Could not find the endpoint configuration.<\/p>\n<p>Does it mean I need to delete the model first instead?<\/p>\n<p>I checked on the console and deleted the model manually.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-29 20:45:40.983 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker|resource-cleanup",
        "Question_view_count":336,
        "Owner_creation_date":"2019-08-23 20:16:54.837 UTC",
        "Owner_last_access_date":"2021-05-13 23:08:45.31 UTC",
        "Owner_reputation":37,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":51332898,
        "Question_title":"Is it possible to have SageMaker output Objective Metrics during a training job?",
        "Question_body":"<p>In the SageMaker hyper parameter tuning jobs, you can use a RegEx expression to parse your logs and output a objective metric to the web console. Is it possible to do this during a normal training job?<\/p>\n\n<p>It would be great to have this feature so I don't need to look through all the logs to find the metric.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-07-13 21:15:17.53 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-07-13 21:18:21.89 UTC",
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":485,
        "Owner_creation_date":"2015-09-25 17:16:18.36 UTC",
        "Owner_last_access_date":"2022-05-26 17:59:10.97 UTC",
        "Owner_reputation":749,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":68892815,
        "Question_title":"Loading custom conda envs not working in SageMaker",
        "Question_body":"<p>I have installed <code>miniconda<\/code> on my AWS SageMaker persistent EBS instance. Here is my starting script:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>#!\/bin\/bash\n\nset -e\n\n# OVERVIEW\n# This script installs a custom, persistent installation of conda on the Notebook Instance's EBS volume, and ensures\n# that these custom environments are available as kernels in Jupyter.\n# \n# The on-start script uses the custom conda environment created in the on-create script and uses the ipykernel package\n# to add that as a kernel in Jupyter.\n#\n# For another example, see:\n# https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html#nbi-isolated-environment\n\nsudo -u ec2-user -i &lt;&lt;'EOF'\nunset SUDO_UID\nWORKING_DIR=\/home\/ec2-user\/SageMaker\/\n\nfor env in $WORKING_DIR\/miniconda\/envs\/*; do\n    BASENAME=$(basename &quot;$env&quot;)\n    source &quot;$WORKING_DIR\/miniconda\/bin\/activate&quot;\n    source activate &quot;$BASENAME&quot;\n    pip install ipykernel boto3\n    python -m ipykernel install --user --name &quot;$BASENAME&quot; --display-name &quot;Custom ($BASENAME)&quot;\ndone\n# Optionally, uncomment these lines to disable SageMaker-provided Conda functionality.\n# echo &quot;c.EnvironmentKernelSpecManager.use_conda_directly = False&quot; &gt;&gt; \/home\/ec2-user\/.jupyter\/jupyter_notebook_config.py\n# rm \/home\/ec2-user\/.condarc\nEOF\n\necho &quot;Restarting the Jupyter server..&quot;\nrestart jupyter-server\n<\/code><\/pre>\n<p>I use this in order to load my custom envs. However, when I access the JupyterLab interface, even if I see that the activated kernel is the Custom one, the only version of python running on my notebook kernel is <code>\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/bin\/python<\/code>:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/yHOnG.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/yHOnG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I also inspected the CloudWatch logs, and I see this error log: <code>Could not find conda environment: [custom_env]<\/code>.<\/p>\n<p>But, when I run the commands of the starting script within the JupyterLab terminal, conda succeeds in finding those envs. So the question is: what am I missing?<\/p>\n<p>Thanks a lot.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":5,
        "Question_creation_date":"2021-08-23 12:34:50.767 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":10,
        "Question_tags":"python|amazon-web-services|conda|jupyter-lab|amazon-sagemaker",
        "Question_view_count":3163,
        "Owner_creation_date":"2019-04-11 19:48:28.437 UTC",
        "Owner_last_access_date":"2022-09-23 14:27:17.82 UTC",
        "Owner_reputation":309,
        "Owner_up_votes":1360,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Remote",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":64992013,
        "Question_title":"gremlin convert string propery to numeric property",
        "Question_body":"<p>Currently I have a graph having timestamp as string property<\/p>\n<pre><code>g.V().order().by('timestamp', '10')\n<\/code><\/pre>\n<p>This causes issues while sorting as string sorting is different from numeric sorting.<\/p>\n<pre><code>String sorting  : 1, 10, 2\nNumeric sorting : 1, 2, 10\n<\/code><\/pre>\n<p>Is there any way to convert all the timestamp properties to Long(Numeric) in gremlin or how to query gremlin to use numeric sort. Open for suggesstions.<\/p>\n<p>If i use Order.desc with timestamp property it is throwing null pointer exception but Order.decr works fine. Any idea?\nThanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-11-24 17:45:06.37 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-11-27 11:19:32.103 UTC",
        "Question_score":0,
        "Question_tags":"java|gremlin|tinkerpop3|amazon-neptune",
        "Question_view_count":328,
        "Owner_creation_date":"2016-07-23 14:00:46.067 UTC",
        "Owner_last_access_date":"2022-07-01 07:23:25.847 UTC",
        "Owner_reputation":91,
        "Owner_up_votes":55,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Answer_body":"<p>If you want to convert all of the strings to integers the simplest way is going to be to do it in the application along the lines of:<\/p>\n<ol>\n<li>Get the property using Gremlin<\/li>\n<li>Convert it to an integer in your application<\/li>\n<li>Use Gremlin to write the value back and replace the prior one (be sure to use the Cardinality.single keyword).<\/li>\n<\/ol>\n<p>An alternative way would be to export your graph to CSV, update the CSV and reload it. Depending on the size of your graph that could be a better option.<\/p>\n<p>The other way would be to use in-line code (lambdas) but if you are using Amazon Neptune that is not an option as they are not permitted.<\/p>\n<p>As to Oder.decr and Order.desc both should work unless your Neptune engine version or Gremlin Client version are back level by quite a long way.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-11-25 00:29:55.31 UTC",
        "Answer_score":1.0,
        "Owner_location":"India",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":70750834,
        "Question_title":"Azure ML static Endpoint with dynamic model",
        "Question_body":"<p>Is it possible to deploy an endpoint with the latest model on Azure ML? So, I have a CICD Pipeline on Azure DevOps that will generate and evaluate a new model with the latest model and it will Register a model with a better one. But, the problem is when I try to deploy the endpoint, it only can attach a specific model, not the best model. So, if I want to deploy a new one, it will generate a new endpoint link.<\/p>\n<p>So my question is, is it possible to deploy an endpoint with the newest model without changing its URL REST endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-18 05:46:52.98 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-01-18 06:07:06.82 UTC",
        "Question_score":1,
        "Question_tags":"azure|azure-devops|azure-machine-learning-studio|azure-machine-learning-service|mlops",
        "Question_view_count":117,
        "Owner_creation_date":"2018-11-16 03:30:26.92 UTC",
        "Owner_last_access_date":"2022-09-23 03:35:59.12 UTC",
        "Owner_reputation":683,
        "Owner_up_votes":28,
        "Owner_down_votes":5,
        "Owner_views":166,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":62614143,
        "Question_title":"AWS SageMaker: Create an endpoint using a trained model hosted in S3",
        "Question_body":"<p>I have following this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/semantic_segmentation_pascalvoc\/semantic_segmentation_pascalvoc.ipynb\" rel=\"nofollow noreferrer\">tutorial<\/a>, which is mainly for jupyter notebook, and made some minimal modification for external processing. I've created a project that could prepare my dataset locally, upload it to S3, train, and finally deploy the model predictor to the same bucket. Perfect!<\/p>\n<p>So, after to train and saved it in S3 bucket:<\/p>\n<pre><code> ss_model.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n<p>it failed while deploying as an endpoint. So, I have found tricks to host an endpoint in many ways, but not from a model already saved in S3. Because in order to host, you probably need to get the estimator, which in normal way is something like:<\/p>\n<pre><code> self.estimator = sagemaker.estimator.Estimator(self.training_image,\n                                                role,\n                                                train_instance_count=1,\n                                                train_instance_type='ml.p3.2xlarge',\n                                                train_volume_size=50,\n                                                train_max_run=360000,\n                                                output_path=output,\n                                                base_job_name='ss-training',\n                                                sagemaker_session=sess)\n<\/code><\/pre>\n<p>My question is: is there a way to load an estimator from a model saved in S3 (.tar)? Or, anyway, to create an endpoint without train it again?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2020-06-27 18:42:34.33 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|deep-learning|amazon-sagemaker|semantic-segmentation",
        "Question_view_count":1360,
        "Owner_creation_date":"2015-04-17 11:36:43.9 UTC",
        "Owner_last_access_date":"2022-09-24 14:52:52.453 UTC",
        "Owner_reputation":138,
        "Owner_up_votes":40,
        "Owner_down_votes":0,
        "Owner_views":38,
        "Answer_body":"<p>So, after to run on many pages, just found a clue <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/blazingtext_hosting_pretrained_fasttext\/blazingtext_hosting_pretrained_fasttext.ipynb\" rel=\"nofollow noreferrer\">here<\/a>. And I finally found out how to load the model and create the endpoint:<\/p>\n<pre><code>def create_endpoint(self):\n    sess = sagemaker.Session()\n    training_image = get_image_uri(sess.boto_region_name, 'semantic-segmentation', repo_version=&quot;latest&quot;)        \n    role = &quot;YOUR_ROLE_ARN_WITH_SAGEMAKER_EXECUTION&quot;\n    model = &quot;s3:\/\/BUCKET\/PREFIX\/...\/output\/model.tar.gz&quot;\n\n    sm_model = sagemaker.Model(model_data=model, image=training_image, role=role, sagemaker_session=sess)\n    sm_model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge')\n<\/code><\/pre>\n<p><strong>Please, do not forget to disable your endpoint after using. This is really important! Endpoints are charged by &quot;running&quot; not only by the use<\/strong><\/p>\n<p>I hope it also can help you out!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-06-29 20:41:20.863 UTC",
        "Answer_score":3.0,
        "Owner_location":"S\u00e3o Jos\u00e9 dos Campos, Sao Jose dos Campos - State of S\u00e3o Paulo, Brazil",
        "Answer_last_edit_date":"2020-07-27 13:14:15.04 UTC",
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":60036916,
        "Question_title":"Sagemaker lifecycle configuration for installing pandas not working",
        "Question_body":"<p>I am trying to update pandas within a lifecycle configuration, and following the example of AWS I have the next code:<\/p>\n\n<pre><code>#!\/bin\/bash\n\nset -e\n\n# OVERVIEW\n# This script installs a single pip package in a single SageMaker conda environments.\n\nsudo -u ec2-user -i &lt;&lt;EOF\n# PARAMETERS\nPACKAGE=pandas\nENVIRONMENT=python3\nsource \/home\/ec2-user\/anaconda3\/bin\/activate \"$ENVIRONMENT\"\npip install --upgrade \"$PACKAGE\"==0.25.3\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\nEOF\n<\/code><\/pre>\n\n<p>Then I attach it to a notebook and when I enter the notebook and open a notebook file, I see that pandas have not been updated. Using <code>!pip show pandas<\/code> I get:<\/p>\n\n<pre><code>Name: pandas\nVersion: 0.24.2\nSummary: Powerful data structures for data analysis, time series, and statistics\nHome-page: http:\/\/pandas.pydata.org\nAuthor: None\nAuthor-email: None\nLicense: BSD\nLocation: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\nRequires: pytz, python-dateutil, numpy\nRequired-by: sparkmagic, seaborn, odo, hdijupyterutils, autovizwidget\n<\/code><\/pre>\n\n<p>So we can see that I am indeed in the python3 env although the version is 0.24. <\/p>\n\n<p>However, the log in cloudwatch shows that it has been installed:<\/p>\n\n<pre><code>Collecting pandas==0.25.3 Downloading https:\/\/files.pythonhosted.org\/packages\/52\/3f\/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d\/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n2020-02-03T12:33:09.065+01:00\nRequirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in .\/anaconda3\/lib\/python3.6\/site-packages (from pandas==0.25.3) (2018.4)\n2020-02-03T12:33:09.065+01:00\nRequirement already satisfied, skipping upgrade: python-dateutil&gt;=2.6.1 in .\/anaconda3\/lib\/python3.6\/site-packages (from pandas==0.25.3) (2.7.3)\n2020-02-03T12:33:09.065+01:00\nRequirement already satisfied, skipping upgrade: numpy&gt;=1.13.3 in .\/anaconda3\/lib\/python3.6\/site-packages (from pandas==0.25.3) (1.16.4)\n2020-02-03T12:33:09.065+01:00\nRequirement already satisfied, skipping upgrade: six&gt;=1.5 in .\/anaconda3\/lib\/python3.6\/site-packages (from python-dateutil&gt;=2.6.1-&gt;pandas==0.25.3) (1.13.0)\n2020-02-03T12:33:09.065+01:00\nInstalling collected packages: pandas Found existing installation: pandas 0.24.2 Uninstalling pandas-0.24.2: Successfully uninstalled pandas-0.24.2\n2020-02-03T12:33:12.066+01:00\nSuccessfully installed pandas-0.25.3\n<\/code><\/pre>\n\n<p>What could be the problem? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-03 10:02:36.233 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-02-03 16:14:58.477 UTC",
        "Question_score":2,
        "Question_tags":"python|pandas|amazon-web-services|pip|amazon-sagemaker",
        "Question_view_count":1493,
        "Owner_creation_date":"2018-04-09 18:36:08.403 UTC",
        "Owner_last_access_date":"2022-09-23 12:00:52.963 UTC",
        "Owner_reputation":1754,
        "Owner_up_votes":396,
        "Owner_down_votes":76,
        "Owner_views":197,
        "Answer_body":"<p>if you want to install the  packages only in for the python3 environment, use the following script in your <strong>Create Sagemaker Lifecycle<\/strong> configurations. <\/p>\n\n<pre><code>#!\/bin\/bash\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\n# This will affect only the Jupyter kernel called \"conda_python3\".\nsource activate python3\n\n# Replace myPackage with the name of the package you want to install.\npip install pandas==0.25.3\n# You can also perform \"conda install\" here as well.\nsource deactivate\nEOF\n<\/code><\/pre>\n\n<p>Reference : \"<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">Lifecycle Configuration Best Practices<\/a>\" <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-02-10 16:30:36.82 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":63554434,
        "Question_title":"How do you assign ressourses when using PyTorch on AWS Sagemaker?",
        "Question_body":"<p>I wish to use AWS Sagemaker in to train a PyTorch model. I am wondering how do I assign resources to the task? if I had my own computer I would use:<\/p>\n<pre><code>device = torch.cuda(&quot;cuda0&quot;)  \n<\/code><\/pre>\n<p>Is it the same for AWS Sagemaker<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-24 03:42:27.647 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-08-24 16:04:53.127 UTC",
        "Question_score":0,
        "Question_tags":"deep-learning|pytorch|amazon-sagemaker",
        "Question_view_count":51,
        "Owner_creation_date":"2020-08-12 00:09:17.843 UTC",
        "Owner_last_access_date":"2022-01-11 23:47:25.687 UTC",
        "Owner_reputation":71,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Cornwall, ON, Canada",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":71663330,
        "Question_title":"What is the advantage of DVC, git-annex, git-lfs for large or binary files over git?",
        "Question_body":"<p>If I have different versions of a file, e.g., in different branches, and I try to reconcile those, git will has great mechanisms for that. However, in order to do the reconciliations, e.g., in a merge, git requires access to the &quot;inside&quot; of the file. Thus files should be text files.<\/p>\n<p>If I change a version controlled file, git does not save the delta between those files, but safes and entire snapshot of the file. If one makes a change, even a small change, to a large file, the entire files will be stored twice by git. Thus files should be small.<\/p>\n<p>Files that are either large or binary (or both), they should not be tracked by Git. If I still need them in my project, I should use something like DVC, git-annex, git-lfs.<\/p>\n<p>As far as I understand, all three of those keep the those other files outside of git, and keep a reference, which is tracked by git. I will use DVC as a stand-in, as I know even less about the other two.<\/p>\n<ol>\n<li><p>In DVC, the reference is a text file and thus, git will not get confused. However, since it is only a reference, there is not much merging to be done by git anyways. So, git's reconciliation-capabilities are not really required. What is the advantage of using DVC then regarding this aspect? Can't I just use git and just not use those mechanisms?<\/p>\n<\/li>\n<li><p>In DVC, it seems that if I change a large file, just like in git, a snapshot of that file is created (not a delta saved). So, how does this improve the situation compared to git? I still get lots of (near) copies of this big file.<\/p>\n<\/li>\n<\/ol>\n<p>I understand from <a href=\"https:\/\/stackoverflow.com\/a\/35578715\/4533188\">here<\/a> that git-lfs keeps most of the (near) copies of my file in the remote storage. Only if I checkout the respective version of the large file, the files is downloaded. In that case, while I would be correct about my point 2, at least it is only a &quot;problem&quot; of the server (in terms of space), but not on my local disk space and also not for the internet bandwidth usage. This might be the same for DVC.<\/p>\n<p>Are my &quot;objections&quot; or &quot;caveats&quot; of the points 1 and 2 valid?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_date":"2022-03-29 13:52:24.137 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"git|git-lfs|dvc|git-annex",
        "Question_view_count":370,
        "Owner_creation_date":"2015-02-05 13:50:19.917 UTC",
        "Owner_last_access_date":"2022-09-23 12:45:06.05 UTC",
        "Owner_reputation":11374,
        "Owner_up_votes":415,
        "Owner_down_votes":2,
        "Owner_views":845,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "dvc"
        ]
    },
    {
        "Question_id":54797698,
        "Question_title":"SageMaker delete Models and Endpoint configurations with python API",
        "Question_body":"<p>I've tried deleting\/recreating endpoints with the same name, and wasted a lot of time before I realized that changes do not get applied unless you also delete the corresponding Model and Endpoint configuration so that new ones can be created with that name. <\/p>\n\n<p>Is there a way with the sagemaker python api to delete all three instead of just the endpoint?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-02-21 01:07:49.467 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":4760,
        "Owner_creation_date":"2013-02-20 05:47:52.693 UTC",
        "Owner_last_access_date":"2022-09-23 20:45:28.4 UTC",
        "Owner_reputation":6281,
        "Owner_up_votes":430,
        "Owner_down_votes":17,
        "Owner_views":958,
        "Answer_body":"<p>It looks like AWS is currently in the process of supporting model deletion via API with <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/pull\/647\" rel=\"nofollow noreferrer\" title=\"sagemaker-python-sdk\/pull\/647\">this<\/a> pull request. <\/p>\n\n<p>For the time being Amazon's only <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-cleanup.html\" rel=\"nofollow noreferrer\" title=\"docs.aws.amazon.com\/sagemaker\">recommendation<\/a> is to delete everything via the console. <\/p>\n\n<p>If this is critical to your system you can probably manage everything via Cloud Formation and create\/delete services containing your Sagemaker models and endpoints.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-02-21 01:54:42.127 UTC",
        "Answer_score":2.0,
        "Owner_location":"NYC",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":54120240,
        "Question_title":"How can I load and deploy a pre-trained AWS Sagemaker XGBoost model on local machine?",
        "Question_body":"<p>I've trained a Sagemaker XGBoost model and downloaded the model.tar.gz file from S3 onto my local machine. How can I load this model for deploying it using flask?<\/p>\n\n<p>I've tried using pickle to load the unzipped model file but it doesn't seem to work.<\/p>\n\n<pre><code>import sagemaker\nimport boto3\nimport os\nimport pickle\n\nwith open('xgboost-model', 'r') as inp:\n   cls.model = pkl.load(inp)\n<\/code><\/pre>\n\n<p>Traceback (most recent call last):\n  File \"\", line 2, in \n  File \"C:\\Anaconda3\\lib\\encodings\\cp1252.py\", line 23, in decode\n    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\nUnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 969: character maps to <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-10 00:03:40.773 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"machine-learning|xgboost|amazon-sagemaker",
        "Question_view_count":3577,
        "Owner_creation_date":"2012-06-04 21:07:02.563 UTC",
        "Owner_last_access_date":"2021-01-22 16:23:41.947 UTC",
        "Owner_reputation":355,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":44,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":54135744,
        "Question_title":"Updating a vertex property based on another",
        "Question_body":"<p>I'd like to update a <code>highScore<\/code> based on a users <code>score<\/code> when it is incremented. If the newly incremented <code>score<\/code> is greater than <code>highScore<\/code>, set <code>highScore<\/code> = <code>score<\/code>.<\/p>\n\n<pre><code>\/\/ initial data\ng.addV(\"player\")\n  .property(id, 1)\n  .property(single, \"score\", 0)\n  .property(single, \"highScore\", 0)\n\n\/\/ increment score by 1 and set highScore if required\ng.V(1)\n  .sack(assign)\n  .by(\"score\")\n  .sack(sum)\n  .by(__.constant(1))\n  .property(single, \"score\", sack())\n  .choose(\n    __.values(\"highScore\").is(lt(__.values(\"score\"))),\n    __.property(single, \"highScore\", __.values(\"score\")))\n  )\n<\/code><\/pre>\n\n<p>It seems to error on <code>lt(__.values(\"score\"))<\/code>. It's parsing that as a traversal instead of a value.<\/p>\n\n<blockquote>\n  <p>com.amazon.neptune.tinkerpop.structure.NeptuneGraph$NeptuneGraphTraversal\n  cannot be cast to java.lang.Integer<\/p>\n<\/blockquote>\n\n<p>How can I pass the current value of <code>score<\/code> into that predicate? I've tried adding <code>.value()<\/code>, <code>.iterate()<\/code>, and <code>.next()<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-10 19:31:57.933 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"gremlin|tinkerpop|amazon-neptune",
        "Question_view_count":417,
        "Owner_creation_date":"2010-08-23 23:03:20.887 UTC",
        "Owner_last_access_date":"2022-09-23 22:06:30.213 UTC",
        "Owner_reputation":4949,
        "Owner_up_votes":730,
        "Owner_down_votes":15,
        "Owner_views":218,
        "Answer_body":"<p>This approach using <code>where()<\/code> seems to work:<\/p>\n\n<pre><code>gremlin&gt; g.V(1).as('a').\n......1&gt;   sack(assign).\n......2&gt;     by(\"score\").\n......3&gt;   sack(sum).\n......4&gt;     by(__.constant(1)).\n......5&gt;   property(single, \"score\", sack()).\n......6&gt;   choose(where('a', lt('a')).by('highScore').by('score'),\n......7&gt;          __.property(single, \"highScore\", sack()))\n==&gt;v[1]\ngremlin&gt; g.V().valueMap()\n==&gt;[score:[1],highScore:[1]]\ngremlin&gt; g.V(1).as('a').\n......1&gt;   sack(assign).\n......2&gt;     by(\"score\").\n......3&gt;   sack(sum).\n......4&gt;     by(__.constant(1)).\n......5&gt;   property(single, \"score\", sack()).\n......6&gt;   choose(where('a', lt('a')).by('highScore').by('score'),\n......7&gt;          __.property(single, \"highScore\", sack()))\n==&gt;v[1]\ngremlin&gt; g.V().valueMap()\n==&gt;[score:[2],highScore:[2]]\ngremlin&gt; g.V().property('highScore',10)\n==&gt;v[1]\ngremlin&gt; g.V().valueMap()\n==&gt;[score:[2],highScore:[10]]\ngremlin&gt; g.V(1).as('a').\n......1&gt;   sack(assign).\n......2&gt;     by(\"score\").\n......3&gt;   sack(sum).\n......4&gt;     by(__.constant(1)).\n......5&gt;   property(single, \"score\", sack()).\n......6&gt;   choose(where('a', lt('a')).by('highScore').by('score'),\n......7&gt;          __.property(single, \"highScore\", sack()))\n==&gt;v[1]\ngremlin&gt; g.V().valueMap()\n==&gt;[score:[3],highScore:[10]]\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-01-11 01:24:52.437 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":67993635,
        "Question_title":"AWS Neptune reader endpoint fails to work",
        "Question_body":"<p>I have a Kubernetes pod communicating to a neptune cluster over a transit gateway. Querying and storing data over the primary cluster endpoint works without any issues at all, however, swapping to a reader endpoint just times out. This has been happening for over 24 hours now.<\/p>\n<p>What could be the root cause of this? This is a java-based application running gremlin 3.4.10 on the latest Neptune engine.<\/p>\n<p>I'm getting same results locally with credentials and on remote server. Again, the primary endpoint works in both environments, but the read-only does not work at all.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-15 21:19:47.76 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|gremlin|amazon-neptune",
        "Question_view_count":64,
        "Owner_creation_date":"2020-08-20 21:22:57.01 UTC",
        "Owner_last_access_date":"2022-08-10 03:15:28.033 UTC",
        "Owner_reputation":832,
        "Owner_up_votes":65,
        "Owner_down_votes":6,
        "Owner_views":127,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Seattle, WA, USA",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":54134805,
        "Question_title":"VIM (or other Plugin) installation in SageMaker Jupyter(Lab)",
        "Question_body":"<p>Can one install Jupyter\/JupyterLab plugins on SageMaker? I don't see any options to add plugins either in JupyterLab or the SageMaker interface. Would love to have at least the VIM plugin installed.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-10 18:24:17.967 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2019-01-11 09:14:19.713 UTC",
        "Question_score":2,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1635,
        "Owner_creation_date":"2014-07-14 03:33:39.73 UTC",
        "Owner_last_access_date":"2022-09-23 14:53:07.687 UTC",
        "Owner_reputation":724,
        "Owner_up_votes":362,
        "Owner_down_votes":1,
        "Owner_views":44,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Modena, Province of Modena, Italy",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":3846656,
        "Question_title":"How does the 3 way merge in Mercurial\/Meld work?",
        "Question_body":"<p>I'm working on a project where I have a commit that introduced a feature with major problems that weren't discovered immediately. Now I want to completely remove that revision while keeping the work following it but i'm having a hard time wrapping my head around this 3 way merge. Here is a simplified graph of my project.<\/p>\n\n<pre>\no  changeset:   134:7f81764aa03a\n|  tag:         tip\n|  parent:      128:451d8a19edea\n|  summary:     Backed out changeset 451d8a19edea\n|\n| @  changeset:   133:5eefa40e2a29\n| |  summary:     (Change I need to keep keep)\n| |\n*snip 3 commits*\n| o  changeset:   129:5f6182a97d40\n|\/   summary:     (Change I need to keep keep)\n|\no  changeset:   128:451d8a19edea\n|  summary:     (Change that introduced a major problem)\n|\no  changeset:   127:4f26dc55455d\n|  summary:     (summary doesn't matter for this question)\n<\/pre>\n\n<p>If I understand this correctly, r127 and r134 are exactly the same. When I <code>hg up -C -r 133<\/code> and then run <code>hg merge<\/code>, Meld pops up with three forms of one of my files: local, base, and other. local seems to be r133 but i'm having a hard time wrapping my head around what \"base\" and \"other\" mean.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2010-10-02 16:52:51.763 UTC",
        "Question_favorite_count":11.0,
        "Question_last_edit_date":"2012-11-02 14:42:02.53 UTC",
        "Question_score":33,
        "Question_tags":"mercurial|merge|dvcs|3-way-merge",
        "Question_view_count":15819,
        "Owner_creation_date":"2008-11-06 20:05:44.813 UTC",
        "Owner_last_access_date":"2022-09-17 16:40:42.81 UTC",
        "Owner_reputation":4806,
        "Owner_up_votes":179,
        "Owner_down_votes":13,
        "Owner_views":327,
        "Answer_body":"<p><strong>Local<\/strong> is r133<\/p>\n\n<p><strong>Other<\/strong> is r134<\/p>\n\n<p><strong>Base<\/strong> is r128 (the common ancestor to both r133 and r 134)<\/p>\n\n<p>When you perform a 3 way merge it compares all three of those together to help you decide what to take and from where.  By seeing what change is in the other revision and what the common ancestor looked like you are able to make a much more informed decision about what to keep and what to change.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2010-10-02 16:58:23.49 UTC",
        "Answer_score":35.0,
        "Owner_location":"Las Vegas, NV, United States",
        "Answer_last_edit_date":"2010-10-02 20:30:59.77 UTC",
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":63361229,
        "Question_title":"How do you write lifecycle configurations for SageMaker on windows?",
        "Question_body":"<p>I'm trying to set up a startup lifecycle configuration for a SageMaker sketchbook (which just ends up being a .sh file), and it seems like, regardless of what I do, my notebooks timeout on startup. I simplified everything as much as possible, to the point of commenting out all but <code>#!\/bin\/bash<\/code>, and I still get a timeout. Checking cloudwatch this shows up in the log:<\/p>\n<pre><code>\/bin\/bash: \/tmp\/OnStart_2020-08-11-07-01jgfhhkwa: \/bin\/bash^M: bad interpreter: No such file or directory\n<\/code><\/pre>\n<p>through testing, I also found that if I add a carriage return before <code>#!\/bin\/bash<\/code> I get this in the log:<\/p>\n<pre><code>\/tmp\/OnStart_2020-08-11-06-444y3fobzp: line 1: $'\\r': command not found\n<\/code><\/pre>\n<p>based on <a href=\"https:\/\/askubuntu.com\/questions\/966488\/how-do-i-fix-r-command-not-found-errors-running-bash-scripts-in-wsl\">this on the \\r error<\/a>, and <a href=\"https:\/\/stackoverflow.com\/questions\/14219092\/bash-script-and-bin-bashm-bad-interpreter-no-such-file-or-directory\">this on the ^M error<\/a>, this seems to be an incompatibility between windows and unix formatted text. However, I'm editing the lifecycle configuration through aws on my windows machine:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/A5oiU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/A5oiU.png\" alt=\"screenshot of editing the lifecycle config\" \/><\/a><\/p>\n<p>is there some way that I can edit this field on my windows machine through AWS, but it be properly written in unix on the other end?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-11 15:16:43.89 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"linux|windows|bash|amazon-web-services|amazon-sagemaker",
        "Question_view_count":608,
        "Owner_creation_date":"2018-12-21 02:51:36.8 UTC",
        "Owner_last_access_date":"2022-09-25 01:54:35.743 UTC",
        "Owner_reputation":1011,
        "Owner_up_votes":218,
        "Owner_down_votes":5,
        "Owner_views":93,
        "Answer_body":"<p>This is, indeed, to do with special character representation in different os' based on <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/issues\/8\" rel=\"nofollow noreferrer\">this<\/a> you can use notepad++ to easily convert the dos representation a unix representation, then just &quot;paste as plain text&quot;, and it works fine<\/p>\n<ul>\n<li>copy to notepad++ view<\/li>\n<li>show symbol<\/li>\n<li>show all symbols<\/li>\n<li>replace &quot;\/r&quot; with nothing CRLF should become LF which is valid in unix<\/li>\n<li>copy and paste as plain text<\/li>\n<\/ul>\n<p>Doing this fixed the problem<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-08-11 16:55:41.183 UTC",
        "Answer_score":2.0,
        "Owner_location":"Earth",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":70500840,
        "Question_title":"How can I improve order().by() performance in Neptune?",
        "Question_body":"<p>I am trying to solve a performance issue with a traversal and have tracked it down to the <code>order().by()<\/code> step. It seems that <code>order().by()<\/code> greatly increases the number of statement index ops required (per the profiler) and dramatically slows down execution.<\/p>\n<p>A non-ordered traversal is very fast:<\/p>\n<pre><code>g.V().hasLabel(&quot;post&quot;).limit(40)\n<\/code><\/pre>\n<p>execution time: 2 ms<\/p>\n<p>index ops: 1<\/p>\n<p>Adding a single ordering step adds thousands of index ops and runs much slower.<\/p>\n<pre><code>g.V().hasLabel(&quot;post&quot;).order().by(&quot;createdDate&quot;, desc).limit(40)\n<\/code><\/pre>\n<p>execution time: 62 ms<\/p>\n<p>index ops: 3909<\/p>\n<p>Adding a single filtering step adds thousands more index ops and runs even slower:<\/p>\n<pre><code>g.V().hasLabel(&quot;post&quot;).has(&quot;isActive&quot;, true).order().by(&quot;createdDate&quot;, desc).limit(40)\n<\/code><\/pre>\n<p>execution time: 113 ms<\/p>\n<p>index ops: 7575<\/p>\n<p>However the same filtered traversal without ordering runs just as fast as the original unfiltered traversal:<\/p>\n<pre><code>g.V().hasLabel(&quot;post&quot;).has(&quot;isActive&quot;, true).limit(40)\n<\/code><\/pre>\n<p>execution time: 1 ms<\/p>\n<p>index ops: 49<\/p>\n<p>By the time we build out the actual traversal we run in production there are around 12 filtering steps and 4 <code>by()<\/code> step-modulators causing the traversal to take over 6000 ms to complete with over 33000 index ops. Removing the <code>order().by()<\/code> steps causes the same traversal to run fast (500 ms).<\/p>\n<p>The issue seems to be with <code>order().by()<\/code> and the number of index ops required to sort. I have seen the performance issue noted <a href=\"https:\/\/docs.aws.amazon.com\/neptune\/latest\/userguide\/gremlin-traversal-tuning.html\" rel=\"nofollow noreferrer\">here<\/a> but adding <code>barrier()<\/code> did not help. The traversal is also fully optimized requiring no Tinkerpop conversion.<\/p>\n<p>I am running engine version 1.1.0.0 R1. There are about 5000 <code>post<\/code> vertices.<\/p>\n<p>How can I improve the performance of this traversal?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-27 21:58:36.25 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-12-28 15:02:56.27 UTC",
        "Question_score":0,
        "Question_tags":"gremlin|amazon-neptune",
        "Question_view_count":166,
        "Owner_creation_date":"2010-08-23 23:03:20.887 UTC",
        "Owner_last_access_date":"2022-09-23 22:06:30.213 UTC",
        "Owner_reputation":4949,
        "Owner_up_votes":730,
        "Owner_down_votes":15,
        "Owner_views":218,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":70644991,
        "Question_title":"How to download large files from SageMaker Studio Lab?",
        "Question_body":"<p>In Colab one can move large files to Google Drive and then download them easy to local computer but how does one do it in SageMaker Studio Lab? The Download option doesn't work for large files. Do one need to use a script or special application?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":7,
        "Question_creation_date":"2022-01-09 19:57:35.7 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"download|amazon-sagemaker|large-files",
        "Question_view_count":1056,
        "Owner_creation_date":"2022-01-09 19:52:07.493 UTC",
        "Owner_last_access_date":"2022-03-29 10:46:27.72 UTC",
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":64630198,
        "Question_title":"AWS SageMaker, describe a specific training job using python SDK",
        "Question_body":"<p>Using SageMaker python SDK I've created an hyper-param tuning job, which runs many jobs in parallel to search for the optimal HP values.<\/p>\n<p>The jobs complete and I get the best training job name as a string &quot;Job...&quot;.\nI've found the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeTrainingJob.html\" rel=\"nofollow noreferrer\">following article<\/a> about how to describe a job using the AWS-CLI or http request.<\/p>\n<p>Is there a way of doing it using the python SageMaker SDK, in order to avoid the complexity of an authenticated request to AWS?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-11-01 09:01:31.31 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker|hyperparameters",
        "Question_view_count":686,
        "Owner_creation_date":"2011-12-25 10:19:41.143 UTC",
        "Owner_last_access_date":"2022-09-22 10:21:32.26 UTC",
        "Owner_reputation":9050,
        "Owner_up_votes":1458,
        "Owner_down_votes":21,
        "Owner_views":1750,
        "Answer_body":"<p>With a <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/session.py#L70\" rel=\"nofollow noreferrer\"><code>sagemaker.session.Session<\/code><\/a> instance, you can <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/session.py#L1519\" rel=\"nofollow noreferrer\">describe training jobs<\/a>:<\/p>\n<pre><code>import sagemaker\n\n\nsagemaker_session = sagemaker.session.Session()\nsagemaker_session.describe_training_job(&quot;Job...&quot;)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-12-22 16:57:22.863 UTC",
        "Answer_score":4.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":61351024,
        "Question_title":"Kubernetes MLflow Service Pod Connection",
        "Question_body":"<p>I have deployed a build of mlflow to a pod in my kubernetes cluster. I'm able to port forward to the mlflow ui, and now I'm attempting to test it. To do this, I am running the following test on a jupyter notebook that is running on another pod in the same cluster.<\/p>\n<pre><code>import mlflow\n\nprint(&quot;Setting Tracking Server&quot;)\ntracking_uri = &quot;http:\/\/mlflow-tracking-server.default.svc.cluster.local:5000&quot;\n\nmlflow.set_tracking_uri(tracking_uri)\n\nprint(&quot;Logging Artifact&quot;)\nmlflow.log_artifact('\/home\/test\/mlflow-example-artifact.png')\n\nprint(&quot;DONE&quot;)\n<\/code><\/pre>\n<p>When I run this though, I get<\/p>\n<pre><code>ConnectionError: HTTPConnectionPool(host='mlflow-tracking-server.default.svc.cluster.local', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get? (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n<\/code><\/pre>\n<p>The way I have deployed the mlflow pod is shown below in the yaml and docker:<\/p>\n<p>Yaml:<\/p>\n<pre><code>---\napiVersion: apps\/v1\nkind: Deployment\nmetadata:\n  name: mlflow-tracking-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: mlflow-tracking-server\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mlflow-tracking-server\n    spec:\n      containers:\n      - name: mlflow-tracking-server\n        image: &lt;ECR_IMAGE&gt;\n        ports:\n        - containerPort: 5000\n        env:\n        - name: AWS_MLFLOW_BUCKET\n          value: &lt;S3_BUCKET&gt;\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: AWS_SECRET_ACCESS_KEY\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mlflow-tracking-server\n  namespace: default\n  labels:\n    app: mlflow-tracking-server\n  annotations:\n    service.beta.kubernetes.io\/aws-load-balancer-type: nlb\nspec:\n  externalTrafficPolicy: Local\n  type: LoadBalancer\n  selector:\n    app: mlflow-tracking-server\n  ports:\n    - name: http\n      port: 5000\n      targetPort: http\n<\/code><\/pre>\n<p>While the dockerfile calls a script that executes the mlflow server command: <code>mlflow server --default-artifact-root ${AWS_MLFLOW_BUCKET} --host 0.0.0.0 --port 5000<\/code>, I cannot connect to the service I have created using that mlflow pod.<\/p>\n<p>I have tried using the tracking uri <code>http:\/\/mlflow-tracking-server.default.svc.cluster.local:5000<\/code>, I've tried using the service EXTERNAL-IP:5000, but everything I tried cannot connect and log using the service. Is there anything that I have missed in deploying my mlflow server pod to my kubernetes cluster?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":6,
        "Question_creation_date":"2020-04-21 18:54:46.493 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2020-09-07 11:16:43.817 UTC",
        "Question_score":2,
        "Question_tags":"kubernetes|kubernetes-service|mlflow",
        "Question_view_count":855,
        "Owner_creation_date":"2014-11-26 14:40:35.813 UTC",
        "Owner_last_access_date":"2021-08-12 15:37:44.573 UTC",
        "Owner_reputation":945,
        "Owner_up_votes":35,
        "Owner_down_votes":1,
        "Owner_views":148,
        "Answer_body":"<p>Your <strong>mlflow-tracking-server<\/strong> service should have <em>ClusterIP<\/em> type, not <em>LoadBalancer<\/em>. <\/p>\n\n<p>Both pods are inside the same Kubernetes cluster, therefore, there is no reason to use <em>LoadBalancer<\/em> Service type.<\/p>\n\n<blockquote>\n  <p>For some parts of your application (for example, frontends) you may want to expose a Service onto an external IP address, that\u2019s outside of your cluster.\n  Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default is ClusterIP.<\/p>\n  \n  <p>Type values and their behaviors are:<\/p>\n  \n  <ul>\n  <li><p><strong>ClusterIP<\/strong>: Exposes the Service on a cluster-internal IP. Choosing this\n  value makes the Service only reachable from within the cluster. This\n  is the default ServiceType. <\/p><\/li>\n  <li><p><strong>NodePort<\/strong>: Exposes the Service on each Node\u2019s IP at a static port (the NodePort). A > ClusterIP Service, to which the NodePort Service routes, is automatically created. You\u2019ll > be able to contact the NodePort Service, from outside the cluster, by\n  requesting :. <\/p><\/li>\n  <li><strong>LoadBalancer<\/strong>: Exposes the Service\n  externally using a cloud provider\u2019s load balancer. NodePort and\n  ClusterIP Services, to which the external load balancer routes, are\n  automatically created. <\/li>\n  <li><strong>ExternalName<\/strong>: Maps the Service to the contents\n  of the externalName field (e.g. foo.bar.example.com), by returning a\n  CNAME record with its value. No proxying of any kind is set up.<\/li>\n  <\/ul>\n  \n  <p><a href=\"https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#publishing-services-service-types\" rel=\"nofollow noreferrer\">kubernetes.io<\/a><\/p>\n<\/blockquote>",
        "Answer_comment_count":6.0,
        "Answer_creation_date":"2020-04-21 19:52:17.657 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2020-04-21 20:02:33.943 UTC",
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":37519307,
        "Question_title":"Unable to integrate Azure ML API with PHP",
        "Question_body":"<p>I tried integrating Azure ML API with PHP but unfortunately getting an error in response.<\/p>\n\n<p>Updated: I have used request response API sending through json response<\/p>\n\n<p>Below is the response obtained on executing PHP script:<\/p>\n\n<pre><code>array(1) { [\"error\"]=&gt; array(3) { [\"code\"]=&gt; string(11) \"BadArgument\" \n    [\"message\"]=&gt; string(26) \"Invalid argument provided.\" [\"details\"]=&gt; array(1)\n    {[0]=&gt; array(2) { [\"code\"]=&gt; string(18) \"RequestBodyInvalid\" [\"message\"]=&gt;\n    string(68) \"No request body provided or error in deserializing the request\n    body.\" } } } }\n<\/code><\/pre>\n\n<p>PHP Script:<\/p>\n\n<pre><code>$url = 'URL';\n$api_key = 'API';\n$data = array(\n    'Inputs'=&gt; array(\n        'My Experiment Name'=&gt; array(\n            \"ColumnNames\" =&gt; [['Column1'],\n                              ['Column2'],\n                              ['Column3'],\n                              ['Column4'],\n                              ['Column5'],\n                              ['Column6'],\n                              ['Column7']],\n            \"Values\" =&gt; [ ['Value1'],\n                          ['Value2'],\n                          ['Value3'],\n                          ['Value4'],\n                          ['Value5'],\n                          ['Value6'],\n                          ['Value7']]\n            ),\n        ),\n        'GlobalParameters' =&gt; new StdClass(),\n    );\n\n$body = json_encode($data);\n\n$ch = curl_init();\ncurl_setopt($ch, CURLOPT_URL, $url);\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Content-Type: application\/json', 'Authorization: Bearer '.$api_key, 'Accept: application\/json'));\ncurl_setopt($ch, CURLOPT_POST, 1);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, $body);\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\ncurl_setopt($ch, CURLOPT_SSL_VERIFYPEER, false);\n\n$response  = json_decode(curl_exec($ch), true);\n\/\/echo 'Curl error: ' . curl_error($ch);\ncurl_close($ch);\n\nvar_dump ($response);\n<\/code><\/pre>\n\n<p>I have followed few examples, still unable to crack it. Please let me know the solution for this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2016-05-30 06:51:11.917 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2016-06-02 13:05:08.577 UTC",
        "Question_score":2,
        "Question_tags":"php|azure|curl|machine-learning|azure-machine-learning-studio",
        "Question_view_count":463,
        "Owner_creation_date":"2015-10-30 11:54:36.917 UTC",
        "Owner_last_access_date":"2016-08-01 12:11:32.813 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":73554522,
        "Question_title":"How to solve below pyspark error in Amazon SageMaker Studio Lab",
        "Question_body":"<p>When I try to start a Pyspark session in Amazon SageMaker studio Lab in a new notebook,\nBelow error is appeared<\/p>\n<pre><code>spark = SparkSession.builder.appName(&quot;Practice&quot;).getOrCreate()\n\n<\/code><\/pre>\n<p>Error:\n<a href=\"https:\/\/i.stack.imgur.com\/qp5yA.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-08-31 10:28:50.16 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":-1,
        "Question_tags":"pyspark|amazon-sagemaker",
        "Question_view_count":15,
        "Owner_creation_date":"2021-05-08 13:21:47.46 UTC",
        "Owner_last_access_date":"2022-09-02 18:41:19.64 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":26109102,
        "Question_title":"Is Feature Branching still (or ever) considered a bad practice?",
        "Question_body":"<p>Coming from the TFS world and having just gotten comfortable enough with Git, I am about to propose to my team that we should incorporate the <a href=\"https:\/\/www.atlassian.com\/git\/tutorials\/comparing-workflows\/gitflow-workflow\">Gitflow workflow<\/a> as pointed out by the <a href=\"http:\/\/nvie.com\/posts\/a-successful-git-branching-model\/\">famous article by Vincent Dressen<\/a> going forward.<\/p>\n\n<p>Almost all modern-day literature surrounding branching strategies voice the effectiveness of the Gitflow workflow, which is an extended version of feature branching, but dated articles from influential engineers, such as <a href=\"http:\/\/martinfowler.com\/bliki\/FeatureBranch.html\">Martin Fowler's Feature Branch article (2009)<\/a>, discredit feature branching in general in favor of continuous integration.<\/p>\n\n<p><a href=\"http:\/\/arialdomartini.wordpress.com\/2011\/11\/02\/help-me-because-i-think-martin-fowler-has-a-merge-paranoia\/\">Some of his critics<\/a> stated that Fowler's opposition to feature branching was in part because he was using SVN as his VCS, which was an ineffective tool for merging and therefore led Fowler to recommend a branching anti-pattern \"merge paranoia\".<\/p>\n\n<p>Fowler then responded in 2011 <a href=\"http:\/\/martinfowler.com\/bliki\/SemanticConflict.html\">by saying DVCS systems may make merging easier, but they still don't solve semantic conflicts<\/a>. Now in 2014, we have language aware merge tools such as <a href=\"http:\/\/www.semanticmerge.com\/\">Semantic Merge<\/a>, which might solve this problem altogether.<\/p>\n\n<p>My questions are<\/p>\n\n<ol>\n<li><p><strong>Is feature branching and continuous integration mutually exclusive?<\/strong><\/p><\/li>\n<li><p><strong>How relevant is Fowler's article in modern day development, especially with our accessibility to tools like SourceTree, Git, Jenkins, and other code review software that make feature branching and the like much easier?<\/strong><\/p><\/li>\n<\/ol>",
        "Question_answer_count":4,
        "Question_comment_count":8,
        "Question_creation_date":"2014-09-29 21:19:41.457 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2014-09-29 22:02:26.363 UTC",
        "Question_score":8,
        "Question_tags":"git|continuous-integration|branch|dvcs",
        "Question_view_count":2596,
        "Owner_creation_date":"2010-05-26 13:32:38.323 UTC",
        "Owner_last_access_date":"2022-09-05 05:55:57.757 UTC",
        "Owner_reputation":1255,
        "Owner_up_votes":824,
        "Owner_down_votes":19,
        "Owner_views":214,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"San Francisco, CA, United States",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":2476356,
        "Question_title":"Distributed version control for HUGE projects - is it feasible?",
        "Question_body":"<p>We're pretty happy with SVN right now, but <a href=\"http:\/\/hginit.com\/\" rel=\"nofollow noreferrer\">Joel's tutorial<\/a> intrigued me. So I was wondering - would it be feasible in our situation too?<\/p>\n\n<p>The thing is - our SVN repository is HUGE. The software itself has a 15 years old legacy and has survived several different source control systems already. There are over 68,000 revisions (changesets), the source itself takes up over 100MB and I cant even begin to guess how many GB the whole repository consumes.<\/p>\n\n<p>The problem then is simple - a clone of the whole repository would probably take ages to make, and would consume far more space on the drive that is remotely sane. And since the very point of distributed version control is to have a as many repositories as needed, I'm starting to get doubts.<\/p>\n\n<p>How does Mercurial (or any other distributed version control) deal with this? Or are they unusable for such huge projects?<\/p>\n\n<p><strong>Added:<\/strong> To clarify - the whole thing is one monolithic beast of a project which compiles to a single .EXE and cannot be split up.<\/p>\n\n<p><strong>Added 2:<\/strong> Second thought - The Linux kernel repository uses git and is probably an order of magnitude or two bigger than mine. So how do they make it work?<\/p>",
        "Question_answer_count":10,
        "Question_comment_count":3,
        "Question_creation_date":"2010-03-19 10:07:37.907 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2010-03-19 10:53:44.183 UTC",
        "Question_score":11,
        "Question_tags":"svn|mercurial|scalability|dvcs",
        "Question_view_count":2051,
        "Owner_creation_date":"2008-11-27 12:59:33.427 UTC",
        "Owner_last_access_date":"2022-09-23 13:40:16.713 UTC",
        "Owner_reputation":102217,
        "Owner_up_votes":675,
        "Owner_down_votes":74,
        "Owner_views":5509,
        "Answer_body":"<p>100MB of source code is less than the Linux kernel. Changelog between Linux kernel 2.6.33 and 2.6.34-rc1 has 6604 commits. Your repository scale doesn't sound intimidating to me.<\/p>\n\n<ul>\n<li>Linux kernel 2.6.34-rc1 uncompressed from .tar.bz2 archive: 445MB<\/li>\n<li>Linux kernel 2.6 head checked out from main Linus tree: 827MB<\/li>\n<\/ul>\n\n<p>Twice as much, but still peanuts with the big hard drives we all have.<\/p>",
        "Answer_comment_count":8.0,
        "Answer_creation_date":"2010-03-19 10:20:23.017 UTC",
        "Answer_score":11.0,
        "Owner_location":"Latvia",
        "Answer_last_edit_date":"2010-03-19 10:33:48.217 UTC",
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":69533933,
        "Question_title":"How to pass environment variables while deploying sagemaker endpoint?",
        "Question_body":"<p>I'm trying to send a parameter as an environment variable for my deployed model. I ran a hyperparameter tuning run and I want to pass the string for one of the model parameters into the deployed endpoint.<\/p>\n<p>I'm loading my trained PyTorch model with:<\/p>\n<pre><code>inference_model = PyTorchModel(\nentry_point=&quot;inference.py&quot;,\nsource_dir=&quot;serve&quot;,\nrole=role,\nmodel_data=model_data_s3_path,\nenv={'MODEL_ARCHITECTURE': best_architecture_name}, \nframework_version=&quot;1.8.1&quot;,\npy_version=&quot;py36&quot;,\n)\n<\/code><\/pre>\n<p>I'm using &quot;env&quot; to try and grab it during inference, but it doesn't seem to work when I use <code>os.environ['MODEL_ARCHITECTURE']<\/code>. I get an error that there is no such environment variable? What am I missing? Should I even be passing the string this way?<\/p>\n<p>I'm also using <code>os.environ['MODEL_ARCHITECTURE']<\/code> in the <code>model.py<\/code> script, which is called in <code>inference.py<\/code>.<\/p>\n<p>How can I pass the string along? I need to do it this way (it needs to be automated, I can't change the model name by hand) since I'll be handing it off to some people to run it straight through and they can't manually change the architecture name.<\/p>\n<p>EDIT: I tried SM_ARCHITECTURE_NAME since SageMaker often adds &quot;SM_&quot; to the environment variables \/ hyperparameters during training, but that didn't work either.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-10-12 00:56:33.163 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-10-12 20:58:31.723 UTC",
        "Question_score":3,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":586,
        "Owner_creation_date":"2016-07-19 00:48:21.237 UTC",
        "Owner_last_access_date":"2022-09-13 07:31:55.037 UTC",
        "Owner_reputation":819,
        "Owner_up_votes":42,
        "Owner_down_votes":0,
        "Owner_views":87,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":62462790,
        "Question_title":"How can I get current job-name in SageMaker training job script?",
        "Question_body":"<p>I write some training job on AWS-SageMaker framework.<\/p>\n\n<p>For some it's requirements, it needs know the job-name of which current running on.<\/p>\n\n<p>I know this code works for it ...<\/p>\n\n<pre><code>import sagemaker_containers\nenv = sagemaker_containers.training_env()\njob_name = env['job_name']\n<\/code><\/pre>\n\n<p>But <code>sagemaker_containers<\/code> package has been deprecated. (I read that on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"nofollow noreferrer\">it's GitHub<\/a>)<\/p>\n\n<p>What should i do?<\/p>\n\n<p>I just started learning about this platform last month. I would appreciate any advice. Thank you.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-19 03:28:46.547 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":753,
        "Owner_creation_date":"2014-06-04 23:37:08.59 UTC",
        "Owner_last_access_date":"2022-03-10 01:47:52.843 UTC",
        "Owner_reputation":113,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":"<p>For older containers using the deprecated <code>sagemaker_containers<\/code>, the approach you described is correct.<\/p>\n<p>For newer containers that use <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\" rel=\"nofollow noreferrer\"><code>sagemaker-training-toolkit<\/code><\/a>, this is how you retrieve information about the environment: <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit#get-information-about-the-container-environment\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit#get-information-about-the-container-environment<\/a><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker_training import environment\n\nenv = environment.Environment()\n\njob_name = env[&quot;job_name&quot;]\n<\/code><\/pre>\n<p>You can check the <a href=\"https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/dlc-release-notes.html\" rel=\"nofollow noreferrer\">DLC Release Notes<\/a> to see what's installed in each version.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-06-30 18:19:11.55 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":56308560,
        "Question_title":"Nodejs example for pachyderm",
        "Question_body":"<p>I am new to Pachyderm.<\/p>\n\n<p>I have a pipeline to extract, transform and then save in the db.\nEverything is already written in nodejs, docekrized.\nNow, I would like to move and use pachyderm.<\/p>\n\n<p>I tried following the python examples they provided, but creating this new pipeline always fails and the job never starts.<\/p>\n\n<p>All my code does is take the <code>\/pfs\/data<\/code> and copy it to <code>\/pfs\/out<\/code>. <\/p>\n\n<p>Here is my pipeline definition<\/p>\n\n<pre><code>{\n    \"pipeline\": {\n        \"name\": \"copy\"\n    },\n    \"transform\": {\n        \"cmd\": [\"npm\", \"start\"],\n        \"image\": \"simple-node-docker\"\n    },\n    \"input\": {\n        \"pfs\": {\n            \"repo\": \"data\",\n            \"glob\": \"\/*\"\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>All that happens is that the pipeline fails and the job never starts.<\/p>\n\n<p>Is there a way to debug on why the pipeline is failing?\nIs there something special about my docker image that needs to happen?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-05-25 20:45:13.66 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"node.js|pachyderm",
        "Question_view_count":80,
        "Owner_creation_date":"2014-06-01 13:18:45.057 UTC",
        "Owner_last_access_date":"2022-09-24 21:07:11.22 UTC",
        "Owner_reputation":71,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "pachyderm"
        ]
    },
    {
        "Question_id":61212875,
        "Question_title":"Access Crowd HTML output results",
        "Question_body":"<p>I'm creating a website using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-ui-template-reference.html\" rel=\"nofollow noreferrer\">Crowd HTML Elements<\/a> that let users\/workers annotate images with the bounding box format.  The form looks like this:<\/p>\n\n<pre><code>&lt;crowd-form&gt;\n  &lt;crowd-bounding-box\n    name=\"annotatedResult\"\n    labels=\"['Referee', 'Player']\"\n    src=\"https:\/\/s3.amazonaws.com\/cv-demo-images\/basketball-outdoor.jpg\"\n    header=\"Draw boxes around each basketball player and referee in this image\"\n  &gt;\n    &lt;full-instructions header=\"Bounding Box Instructions\" &gt;\n      &lt;p&gt;Use the bounding box tool to draw boxes around the requested target of interest:&lt;\/p&gt;\n      &lt;ol&gt;\n        &lt;li&gt;Draw a rectangle using your mouse over each instance of the target.&lt;\/li&gt;\n        &lt;li&gt;Make sure the box does not cut into the target, leave a 2 - 3 pixel margin&lt;\/li&gt;\n        &lt;li&gt;\n          When targets are overlapping, draw a box around each object,\n          include all contiguous parts of the target in the box.\n          Do not include parts that are completely overlapped by another object.\n        &lt;\/li&gt;\n        &lt;li&gt;\n          Do not include parts of the target that cannot be seen,\n          even though you think you can interpolate the whole shape of the target.\n        &lt;\/li&gt;\n        &lt;li&gt;Avoid shadows, they're not considered as a part of the target.&lt;\/li&gt;\n        &lt;li&gt;If the target goes off the screen, label up to the edge of the image.&lt;\/li&gt;\n      &lt;\/ol&gt;\n    &lt;\/full-instructions&gt;\n\n    &lt;short-instructions&gt;\n      Draw boxes around each basketball player and referee in this image.\n    &lt;\/short-instructions&gt;\n  &lt;\/crowd-bounding-box&gt;\n&lt;\/crowd-form&gt;\n<\/code><\/pre>\n\n<p>The results of a worker's submission looks like the following:<\/p>\n\n<pre><code>  {\n    \"annotatedResult\": {\n      \"boundingBoxes\": [\n        {\n          \"height\": 3300,\n          \"label\": \"Dog\",\n          \"left\": 536,\n          \"top\": 154,\n          \"width\": 4361\n        }\n      ],\n      \"inputImageProperties\": {\n        \"height\": 3456,\n        \"width\": 5184\n      }\n    }\n  }\n]\n<\/code><\/pre>\n\n<p>I'd like to take this output and write it to a database, pass it to AWS Lambda, store it as metadata, etc. but I don't know how to access the results.  Is the JSON output a property of some HTML DOM property I can grab?<\/p>\n\n<p>I can attach a javascript function to the submit action of the crowd-form portion...<\/p>\n\n<pre><code>&lt;script&gt;\n  document.querySelector('crowd-form').onsubmit = function() {\n      ???\n  };\n&lt;\/script&gt;\n<\/code><\/pre>\n\n<p>...but I'm not sure what object I need to grab to get the results.  Thanks for your help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-14 16:45:19.607 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|mechanicalturk",
        "Question_view_count":371,
        "Owner_creation_date":"2017-01-03 11:49:04.907 UTC",
        "Owner_last_access_date":"2022-02-04 13:53:41.673 UTC",
        "Owner_reputation":312,
        "Owner_up_votes":28,
        "Owner_down_votes":1,
        "Owner_views":65,
        "Answer_body":"<p>You can access the bounding boxes during the onsubmit event like this:<\/p>\n\n<pre><code>&lt;script&gt;\n    document.querySelector('crowd-form').onsubmit = function(e) {\n      const boundingBoxes = document.querySelector('crowd-bounding-box').value.boundingBoxes || document.querySelector('crowd-bounding-box')._submittableValue.boundingBoxes;\n    }\n&lt;\/script&gt;\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/jsfiddle.net\/ap56djgq\/\" rel=\"nofollow noreferrer\">Here's<\/a> a working jsfiddle.<\/p>\n\n<p>Your use case sounds interesting. If you don't mind sharing, please email me at samhenry@amazon.com and I may be able to help further.<\/p>\n\n<p>Thank you,<\/p>\n\n<p>Amazon Mechanical Turk <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-04-14 17:21:05.26 UTC",
        "Answer_score":1.0,
        "Owner_location":"Hoth",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":56351452,
        "Question_title":"Connect on-prem jypyter notebook to mlflow tracking server in Azure",
        "Question_body":"<p>Is it possible to connect a notebook running in premises to an mlflow Tracking server that is part of an Azure Databricks workspace? Have all the local logging and tracking saved in Azure?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-05-29 00:00:53.237 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"azure-databricks|mlflow",
        "Question_view_count":291,
        "Owner_creation_date":"2012-08-02 23:01:34.333 UTC",
        "Owner_last_access_date":"2022-09-02 23:21:30.913 UTC",
        "Owner_reputation":1390,
        "Owner_up_votes":122,
        "Owner_down_votes":2,
        "Owner_views":121,
        "Answer_body":"<p>I had a similar problem, used python and solved it with the following steps:<\/p>\n<ol>\n<li>Install mlflow and datbricks-cli libraries.<\/li>\n<li>Define the following env variables : DATABRICKS_HOST (databricks workspace url: <a href=\"https:\/\/region.azuredatabricks.net\" rel=\"nofollow noreferrer\">https:\/\/region.azuredatabricks.net<\/a>) and DATABRICKS_TOKEN<\/li>\n<li>Define mlflow client:<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow_client = mlflow.tracking.MlflowClient(tracking_uri='databricks')\n<\/code><\/pre>\n<ol start=\"5\">\n<li>Use mlflow_client client for logging, saving and etc..<\/li>\n<\/ol>\n<p>for more reference you can look at the &quot;Log to a tracking server from a notebook&quot; section <a href=\"https:\/\/docs.azuredatabricks.net\/applications\/mlflow\/tracking.html#log-to-a-tracking-server-from-a-notebook\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-05-29 08:23:47.527 UTC",
        "Answer_score":1.0,
        "Owner_location":"Seattle, WA, USA",
        "Answer_last_edit_date":"2020-06-20 09:12:55.06 UTC",
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":38978361,
        "Question_title":"Error 1000: AFx Library library exception: Type Decimal is not supported",
        "Question_body":"<p>I am trying to query a MS Access Web App (SQL Azure) using the Azure ML platform. The field I'm trying to capture is type <code>Fixed-point number (6 decimal places)<\/code>, the default numeric field type in Azure SQL. When I try to query this field, I get the error:<\/p>\n\n<p><code>Error 1000: AFx Library library exception: Type Decimal is not supported<\/code><\/p>\n\n<p>I have tried casting it to another form like follows:<\/p>\n\n<p><code>select cast(a) FROM b<\/code><\/p>\n\n<p>And got the error:<\/p>\n\n<p><code>Error 0069: SQL query \"select cast(\"a\" as float) from \"b\"\" is not correct:\nColumn names cannot be null or empty.<\/code><\/p>\n\n<p>What gives?<\/p>\n\n<p>Furthermore, how isn't the default on Azure SQL supported in Azure ML???<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2016-08-16 15:03:11.487 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"azure|azure-sql-database|azure-machine-learning-studio",
        "Question_view_count":2105,
        "Owner_creation_date":"2013-10-15 17:33:57.317 UTC",
        "Owner_last_access_date":"2022-09-15 13:31:55.58 UTC",
        "Owner_reputation":2720,
        "Owner_up_votes":938,
        "Owner_down_votes":5,
        "Owner_views":482,
        "Answer_body":"<p>As per serhiyb's answer, the win was to assign it to another variable:<\/p>\n\n<p><code>Select cast(\"field\" as float) as 'someAlias' FROM \"Table\"<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2016-08-16 15:52:25.297 UTC",
        "Answer_score":4.0,
        "Owner_location":"London, UK",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":52630208,
        "Question_title":"AWS: connect to Neptune from lambda",
        "Question_body":"<p>I'm trying to connect to Neptune from my lambda. \nLambda configuration contains the same VPC, subnets and security groups as my neptune instance. <\/p>\n\n<p>Also execution role of lambda  has this policies : AmazonRDSFullAccess, AmazonRDSDirectoryServiceAccess, NeptuneFullAccess and \nAWSLambdaENIManagementAccess.<\/p>\n\n<p>Anyway I have this error: Unable to connect to the remote server ---> System.Net.Http.HttpRequestException: No such device or address ---> <\/p>\n\n<p>Did I miss something?<\/p>\n\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2018-10-03 15:09:22.477 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"c#|aws-lambda|amazon-neptune",
        "Question_view_count":1191,
        "Owner_creation_date":"2015-04-28 12:52:13.18 UTC",
        "Owner_last_access_date":"2022-08-29 14:38:22.883 UTC",
        "Owner_reputation":195,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":70,
        "Answer_body":"<p>This definitely looks like a connectivity issue. The recommended way to manage such connections is 2 have 2 security groups:<\/p>\n\n<ol>\n<li><code>client<\/code> - A security group that you attach to all clients, like Lambdas, EC2 instances etc. The default outbound rule gives you outbound access to every resource in the VPC. You can tighten that if you'd like.<\/li>\n<li><code>db<\/code> - A security group that you should attach to your Neptune cluster. In this security group, edit hte inbound rules, and explicitly add a TCP rule that allows inbound connections to your database port (8182 is the default port). <\/li>\n<\/ol>\n\n<p>You can attach the <code>db<\/code> security group to your cluster either during creation or by modifying existing clusters. <\/p>\n\n<p>P.S. As a side note, your Lambda doesn't really need <code>AmazonRDSFullAccess<\/code> or <code>NeptuneFullAccess<\/code> roles unless you plan on making management API calls from the lambda, like issuing a <code>CreateDBCluster<\/code> request for example. Those IAM roles don't really have anything to do with a client being able to talk to a running DB cluster\/instance.<\/p>\n\n<p>Hope this helps.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2018-10-04 08:02:15.37 UTC",
        "Answer_score":2.0,
        "Owner_location":"London, UK",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":68219312,
        "Question_title":"AWS Rekognition vs AWS visual search",
        "Question_body":"<p>Currently, I need to implement searching products by image on my app. As doing research, I wanna go for aws rekognition. So when the model predicts the image, I can pass the predicted label to query products by my api. This is what I plan to do. However, I also come across aws visual search (using aws sageMaker) which is way beyond my understanding. So, am I on the right way to implement it by using the first option (aws rekognition ) ???<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-02 03:38:37.023 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-rekognition",
        "Question_view_count":201,
        "Owner_creation_date":"2015-12-22 06:45:09.14 UTC",
        "Owner_last_access_date":"2021-12-08 07:06:33.67 UTC",
        "Owner_reputation":280,
        "Owner_up_votes":30,
        "Owner_down_votes":1,
        "Owner_views":78,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Phnom Penh, Cambodia",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":68118209,
        "Question_title":"install python package in Azure ml",
        "Question_body":"<p>i am trying to install python package called delta-sharing. I was able to install (<strong>pip install delta-sharing<\/strong>) successfully in computer terminal. But when i try to <strong>import delta_sharing<\/strong> in the notebook, it is not found. I am wondering if i miss anything here?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-24 15:00:36.587 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|azure-machine-learning-service",
        "Question_view_count":73,
        "Owner_creation_date":"2017-10-16 03:07:21.893 UTC",
        "Owner_last_access_date":"2022-04-21 14:22:50.677 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Toronto",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":35434371,
        "Question_title":"Azure Machine Learning Data Transformation",
        "Question_body":"<p>Can machine learning be used to transform\/modifiy a list of numbers.<\/p>\n\n<p>I have many pairs of binary files read from vehicle ECUs, an original or stock file before the vehicle was tuned, and a modified file which has the engine parameters altered.  The files are basically lists of little or big endian 16 bit numbers.<\/p>\n\n<p>I was wondering if it is at all possible to feed these pairs of files into machine learning, and for it to take a new stock file and attempt to transform or tune that stock file.<\/p>\n\n<p>I would appreciate it if somebody could tell me if this is something which is at all possible.  All of the examples I've found appear to make decisions on data rather than do any sort of a transformation.<\/p>\n\n<p>Also I'm hoping to use azure for this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2016-02-16 13:49:52.703 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2016-03-01 16:33:25.583 UTC",
        "Question_score":0,
        "Question_tags":"cortana-intelligence|azure-machine-learning-studio",
        "Question_view_count":113,
        "Owner_creation_date":"2012-03-02 15:03:45.703 UTC",
        "Owner_last_access_date":"2022-06-01 11:22:51.52 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":72490682,
        "Question_title":"How to create an aws sagemaker project using terraform?",
        "Question_body":"<p>This is the terraform shown in the docs:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.example.id\n  }\n}\n<\/code><\/pre>\n<p>I created a service catalog product with id: &quot;prod-xxxxxxxxxxxxx&quot;.\nWhen I substitute the service catalog product id into the above template,\nto get the following:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.prod-xxxxxxxxxxxxx\n  }\n}\n<\/code><\/pre>\n<p>I run terraform plan, but the following error occurs:<\/p>\n<pre><code>A managed resource &quot;aws_servicecatalog_product&quot; &quot;prod-xxxxxxxxxxxxx&quot; has not been declared in the root module.\n\n<\/code><\/pre>\n<p>What do I need to do to fix this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-06-03 13:54:49.18 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-06-03 14:20:14.543 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|terraform|amazon-sagemaker",
        "Question_view_count":298,
        "Owner_creation_date":"2022-05-25 20:48:45.307 UTC",
        "Owner_last_access_date":"2022-09-15 14:23:04.53 UTC",
        "Owner_reputation":35,
        "Owner_up_votes":25,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Answer_body":"<p>Since the documentation is lacking a bit of clarity, in order to have this work as in the example, you would first have to create the Service Catalog product in Terraform as well, e.g.:<\/p>\n<pre><code>resource &quot;aws_servicecatalog_product&quot; &quot;example&quot; {\n  name  = &quot;example&quot;\n  owner = [aws_security_group.example.id] # &lt;---- This would need to be created first\n  type  = aws_subnet.main.id # &lt;---- This would need to be created first\n\n  provisioning_artifact_parameters {\n    template_url = &quot;https:\/\/s3.amazonaws.com\/cf-templates-ozkq9d3hgiq2-us-east-1\/temp1.json&quot;\n  }\n\n  tags = {\n    foo = &quot;bar&quot;\n  }\n}\n<\/code><\/pre>\n<p>You can reference it then in the SageMaker project the same way as in the example:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.example.id\n  }\n}\n<\/code><\/pre>\n<p>Each of the resources that gets created has a set of attributes that can be accessed as needed by other resources, data sources or outputs. In order to understand how this works, I strongly suggest reading the documentation about referencing values [1]. Since you already created the Service Catalog product, the only thing you need to do is provide the string value for the product ID:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = &quot;prod-xxxxxxxxxxxxx&quot;\n  }\n}\n<\/code><\/pre>\n<p>When I can't understand what value is expected by an argument (e.g., <code>product_id<\/code> in this case), I usually read the docs and look for examples like in [2]. Note: That example is CloudFormation, but it can help you understand what type of a value is expected (e.g., string, number, bool).<\/p>\n<p>You could also import the created Service Catalog product into Terraform so you can manage it with IaC [3]. You should understand all the implications of <code>terraform import<\/code> though before trying it [4].<\/p>\n<hr \/>\n<p>[1] <a href=\"https:\/\/www.terraform.io\/language\/expressions\/references\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/expressions\/references<\/a><\/p>\n<p>[2] <a href=\"https:\/\/docs.amazonaws.cn\/en_us\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-project.html#aws-resource-sagemaker-project--examples--SageMaker_Project_Example\" rel=\"nofollow noreferrer\">https:\/\/docs.amazonaws.cn\/en_us\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-project.html#aws-resource-sagemaker-project--examples--SageMaker_Project_Example<\/a><\/p>\n<p>[3] <a href=\"https:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs\/resources\/servicecatalog_product#import\" rel=\"nofollow noreferrer\">https:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs\/resources\/servicecatalog_product#import<\/a><\/p>\n<p>[4] <a href=\"https:\/\/www.terraform.io\/cli\/commands\/import\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/cli\/commands\/import<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-03 15:14:54.487 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":68714123,
        "Question_title":"How do I deal with problems during training testing with container that I supply scripts in folder train?",
        "Question_body":"<p>I am working on updating existing working example of sentiment pytorch sagemaker. I trained the model OK with this:<\/p>\n<pre><code>from sagemaker.pytorch import PyTorch\n\nestimator = PyTorch(entry_point=&quot;train.py&quot;,\n                source_dir=&quot;.\/train&quot;,\n                role=role,\n                framework_version='1.6.0',\n                py_version='py3',                    \n                instance_count=1,\n                instance_type='ml.p2.xlarge',\n                hyperparameters={\n                    'epochs': 10,\n                    'hidden_dim': 200,\n                })\n\nestimator.fit({'training': input_data})\n<\/code><\/pre>\n<p>I got this success:<\/p>\n<pre><code>Epoch: 1, BCELoss: 0.6740543088134454\nEpoch: 2, BCELoss: 0.6022386648217026\nEpoch: 3, BCELoss: 0.4997656838018067\nEpoch: 4, BCELoss: 0.4238818397327345\nEpoch: 5, BCELoss: 0.38591035227386317\nEpoch: 6, BCELoss: 0.3637635330764615\nEpoch: 7, BCELoss: 0.3529281938562588\nEpoch: 8, BCELoss: 0.30158030925964824\nEpoch: 9, BCELoss: 0.30744082191768957\n\n2021-08-09 00:38:19 Uploading - Uploading generated training modelEpoch: 10, BCELoss: \n0.28836183219539874\n2021-08-09 00:38:16,558 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n\n2021-08-09 00:38:39 Completed - Training job completed\nProfilerReport-1628468954: NoIssuesFound\nTraining seconds: 352\n<\/code><\/pre>\n<p>Then when I deploy it in the training container with my train.py script which imported your sagemaker_training library, I was able to do it:\npredictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\nresult:\n-------------------!<\/p>\n<h2>Then when I tried to batch test using my test dataset in the training container:<\/h2>\n<pre><code>ModelError                                Traceback (most recent call last)\n&lt;ipython-input-134-46108efe51ea&gt; in &lt;module&gt;\n----&gt; 1 predictions = predict(test_X.values)\n  2 predictions = [round(num) for num in predictions]\n\n&lt;ipython-input-133-215b27c43941&gt; in predict(data, rows)\n  5     predictions = np.array([])\n  6     for array in split_array:\n----&gt; 7         predictions = np.append(predictions, predictor.predict(array))\n  8 \n  9     return predictions\n\n~\/anaconda3\/envs\/amazonei_pytorch_latest_p36\/lib\/python3.6\/site- \npackages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model, \ntarget_variant, inference_id)\n134             data, initial_args, target_model, target_variant, inference_id\n135         )\n--&gt; 136         response = \nself.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n137         return self._handle_response(response)\n138 \n\n~\/anaconda3\/envs\/amazonei_pytorch_latest_p36\/lib\/python3.6\/site- \npackages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n384                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n385             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 386             return self._make_api_call(operation_name, kwargs)\n387 \n388         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/amazonei_pytorch_latest_p36\/lib\/python3.6\/site- \npackages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n703             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)\n704             error_class = self.exceptions.from_code(error_code)\n--&gt; 705             raise error_class(parsed_response, operation_name)\n706         else:\n707             return parsed_response\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: \nReceived server error (0) from model with message &quot;Your invocation timed out while \nwaiting for a response from container model. Review the latency metrics for each \ncontainer in Amazon CloudWatch, resolve the issue, and try again.&quot;. See https:\/\/us-east- \n1.console.aws.amazon.com\/cloudwatch\/home?region=us-east- \n1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/pytorch-training-2021-08-09-00-47-00-025 \nin account 804604702169 for more information.\n<\/code><\/pre>\n<p>How can void this error? is there anything wrong with updating from &quot;import\nsagemaker_container&quot; to &quot;import sagemaker_training&quot;?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-09 14:50:17.74 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|machine-learning|pytorch|amazon-sagemaker",
        "Question_view_count":39,
        "Owner_creation_date":"2020-03-19 14:26:54.16 UTC",
        "Owner_last_access_date":"2022-04-23 15:46:57.05 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Delaware, USA",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":58655148,
        "Question_title":"Wrong input data to a TensforFlow model",
        "Question_body":"<p>I have built a TensorFlow model on Sagemaker and it works fine with real time inference, however I want to use Batch transform functionality and I started to look to input data. I started to debug my model locally with saved_model_cli:<\/p>\n\n<pre><code>saved_model_cli show \\\n--dir . \\\n--tag_set serve \\\n--signature_def serving_default\n\nThe given SavedModel SignatureDef contains the following input(s):\ninputs['inputs'] tensor_info:\n  dtype: DT_FLOAT\n  shape: (-1, 50, 11)\n  name: lstm_input:0\nThe given SavedModel SignatureDef contains the following output(s):\noutputs['dense\/BiasAdd:0'] tensor_info:\n  dtype: DT_FLOAT\n  shape: (-1, 1)\n  name: dense\/BiasAdd:0\nMethod name is: tensorflow\/serving\/predict\n<\/code><\/pre>\n\n<p>I assume, that my input data is called <code>inputs<\/code> as per output above, however, when I run the following code, I get an error <\/p>\n\n<pre><code>saved_model_cli run \\\n--dir . \\\n--tag_set serve \\\n--signature_def predict \\\n--input_examples 'inputs=[{\"\":[1.2]}]'\n<\/code><\/pre>\n\n<p><strong>ValueError: \"inputs\" is not a valid input key. Please choose from \"\", or use --show option.<\/strong><\/p>\n\n<p>I tried to supply a npy file (<code>--inputs inputs=batch_transform.npy<\/code>), different representations of data, but always the same error. <\/p>\n\n<p>My model is saved with the following code:<\/p>\n\n<pre><code>tf.saved_model.simple_save(  \n   tf.keras.backend.get_session(),  \n   os.path.join(model_dir, 'model\/1'),  \n   inputs={'inputs': model.input},  \n   outputs={t.name: t for t in model.outputs})\n<\/code><\/pre>\n\n<p>I tried TF 1.12 and 1.14 versions, but the outcome is the same.<\/p>\n\n<p>Any advice?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-11-01 06:42:57.833 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"tensorflow|keras|amazon-sagemaker",
        "Question_view_count":376,
        "Owner_creation_date":"2011-03-01 15:11:31.03 UTC",
        "Owner_last_access_date":"2022-09-24 08:39:28.467 UTC",
        "Owner_reputation":175,
        "Owner_up_votes":150,
        "Owner_down_votes":1,
        "Owner_views":38,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Luxembourg City, Luxembourg",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":54233746,
        "Question_title":"Filter mlflow runs by commit ID",
        "Question_body":"<p>When using the UI of MlFlow, is it possible to filter\/search the runs using the (git) commit ID? I manage to search by parameters but it doesn't seem like there's a way to filter by the commit ID.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/npkFO.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/npkFO.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-17 10:22:40.897 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-01-18 07:23:50.323 UTC",
        "Question_score":4,
        "Question_tags":"machine-learning|version-control|mlflow",
        "Question_view_count":982,
        "Owner_creation_date":"2011-03-22 10:28:37.227 UTC",
        "Owner_last_access_date":"2022-09-23 13:51:41.56 UTC",
        "Owner_reputation":11410,
        "Owner_up_votes":2846,
        "Owner_down_votes":6,
        "Owner_views":1782,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":72833918,
        "Question_title":"Eval_Metrics not recognized\/invalid in AWS XGBoost model",
        "Question_body":"<pre><code>xgb.set_hyperparameters(objective='binary:logistic',num_round=100)\nxgb.fit({'train': s3_input_train})\n\n...\n\n\nfrom sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\nhyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n                         'min_child_weight': ContinuousParameter(1, 10),\n                         'alpha': ContinuousParameter(0, 2),\n                         'max_depth': IntegerParameter(1, 10),\n                         'num_round': IntegerParameter(1, 300),\n                        'gamma': ContinuousParameter(0, 5),\n                        'lambda': ContinuousParameter(0, 1000),\n                        'max_delta_step':IntegerParameter(1, 10),\n                        'colsample_bylevel':ContinuousParameter(0.1, 1),\n                        'colsample_bytree':ContinuousParameter(0.5, 1),\n                        'subsample':ContinuousParameter(0.5, 1)}\n\n\nobjective_metric_name = 'validation:aucpr'\n\ntuner = HyperparameterTuner(xgb,\n                            objective_metric_name,\n                            hyperparameter_ranges,\n                            max_jobs=50,\n                            max_parallel_jobs=3)\n\ntuner.fit({'train': s3_input_train, 'validation': s3_input_val}, include_cls_metadata=False, wait=False)\n<\/code><\/pre>\n<p>Returns the error:<\/p>\n<pre><code>\nAn error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: The objective metric for the hyperparameter tuning job, [validation:aucpr], isn\u2019t valid for the [811284229777.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest] algorithm. Choose a valid objective metric.\n<\/code><\/pre>\n<p>The same applies when replacing aucpr with f1 and logloss. They are clearly defined as evaluation metrics in the documentation for classification purposes. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html<\/a><\/p>\n<p>What can I do to allow the f1, aucpr and logloss evaluation metrics?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-01 19:35:52.423 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|xgboost|amazon-sagemaker",
        "Question_view_count":77,
        "Owner_creation_date":"2021-10-06 17:54:05.253 UTC",
        "Owner_last_access_date":"2022-09-20 18:00:16.443 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":56565218,
        "Question_title":"How to extract username of Sagemaker Notebook instance user to replace EC2-Default-User",
        "Question_body":"<p>I am in the process of setting up notebook instances for several users in a team. These users will be able to use version control. The issue is that when a user commits the Author of the commit is ec2-default-user. I want the author to be \"username\" eg john doe. <\/p>\n\n<p>I know that one solution to this is to tell the users to write in the command line \"git config --global user.name \"John Doe\"\".<\/p>\n\n<p>This does not work for me as I want this to be automated for the users. Therefore I am looking for a way to extract the user's username in the lifecycle config and then pass this username to some code which will set the author of any commits to the user's username. <\/p>\n\n<p>This is what I have currently<\/p>\n\n<pre><code>set -e\ncat &lt;&lt; EOF &gt;&gt; \/home\/ec2-user\/.gitconfig\n[user]\n        name = John Doe\n\nEOF\n<\/code><\/pre>\n\n<p>but where there is John Doe I would like it to be the username of the current user who has opened up their sagemaker notebook instance.<\/p>\n\n<p>I would like a way to extract the username in the lifecycle config and pas this value to the code that sets the username. I'm relatively new to all this hence the limited knowledge. I've looked on documentation but can't seem to find much.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2019-06-12 15:01:14.703 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|jupyter-notebook|lifecycle|amazon-sagemaker",
        "Question_view_count":1135,
        "Owner_creation_date":"2014-09-27 17:28:16.9 UTC",
        "Owner_last_access_date":"2020-02-19 09:16:01.12 UTC",
        "Owner_reputation":105,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":40,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":3515248,
        "Question_title":"How can I tell git (or some other dvcs) to track a file privately?",
        "Question_body":"<p>My use case starts from something a lot like <a href=\"https:\/\/stackoverflow.com\/questions\/1406892\/elegantly-handle-site-specific-settings-configuration-in-svn-hg-git-etc\" title=\"Django local settings\">this<\/a>; a team uses a central repository (in my case it is subversion, but I believe if it were git the issue would be the same), and some of the files are member-private (Django local settings files, IDE private project preferences etc). While the private file should remain private -- that is, I don't want changes I make to it to be pushed or dcommitted -- I do want the file tracked and version controlled.<\/p>\n\n<p>The best option would be a way to keep the file private by default; a workaround would be a way to keep private commits -- having to remember to commit the private file separately would be a nuisance, but still better than not being able to track it at all.<\/p>\n\n<p>In comparison to suggested solutions:\n<a href=\"https:\/\/stackoverflow.com\/questions\/2562523\/using-git-to-work-with-subversion-ignoring-modifications-to-tracked-files\">this<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/3354482\/with-git-temporary-exclude-a-changed-tracked-file-from-commit-in-command-line\">this<\/a> aren't good because they prevent the file from being committed at all; this is not what I want. I want it committed locally; I just don't want it published.<\/p>\n\n<p>BTW -- while I love DVCSs, and git has been a sort-of default, I don't feel particularly committed to it (pun unintended); if only hg or bzr can do this, it may be reason enough for me to switch.<\/p>",
        "Question_answer_count":7,
        "Question_comment_count":0,
        "Question_creation_date":"2010-08-18 18:15:37.723 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2017-05-23 12:29:50.933 UTC",
        "Question_score":5,
        "Question_tags":"git|dvcs",
        "Question_view_count":215,
        "Owner_creation_date":"2009-06-21 14:44:14.853 UTC",
        "Owner_last_access_date":"2021-04-29 06:47:01.633 UTC",
        "Owner_reputation":2895,
        "Owner_up_votes":17,
        "Owner_down_votes":1,
        "Owner_views":126,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":65235553,
        "Question_title":"Problem importing Amazon SageMaker machine learning algorithm container",
        "Question_body":"<p>I'm new to AWS platform. I am following up a data science tutorial on AWS platform to build a classifier and deploy. However, while trying to import the machine learning algorithm container (XGBOOST), the following code doesn't seem to work:<\/p>\n<pre><code>from sagemaker.amazon.amazon_estimator import get_image_uri\ncontainer = get_image_uri(boto3.Session().region_name, 'xgboost')\n<\/code><\/pre>\n<p>I get the following error saying the file doesn't work:<\/p>\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/image_uri_config\/xgboost.json'\n<\/code><\/pre>\n<p>I also used a 'linear-learner' in place of the 'xgboost' but I still got the same error. So, is there something I'm missing?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2020-12-10 13:34:18.657 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-12-13 11:46:21.243 UTC",
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|machine-learning|data-science|amazon-sagemaker",
        "Question_view_count":100,
        "Owner_creation_date":"2018-05-16 10:57:59.153 UTC",
        "Owner_last_access_date":"2021-07-28 21:37:10.707 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":2561712,
        "Question_title":"Will Distributed Version Control Systems survive?",
        "Question_body":"<p>I am personally a SVN lover, but am starting to fall prey to the buzz that is surrounding DVCS.<\/p>\n\n<p>SVN is free, and time tested, is DVCS the new SVN?<\/p>\n\n<p>I am also looking for which DVCS server will win out GIT or Mercurial?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":5,
        "Question_creation_date":"2010-04-01 15:51:44.68 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"svn|version-control|dvcs",
        "Question_view_count":555,
        "Owner_creation_date":"2010-01-22 14:24:27.963 UTC",
        "Owner_last_access_date":"2022-09-23 17:20:51.207 UTC",
        "Owner_reputation":55364,
        "Owner_up_votes":1689,
        "Owner_down_votes":123,
        "Owner_views":3268,
        "Answer_body":"<p>Well you can look at the advantage and disadvantages on wiki<\/p>\n\n<ul>\n<li><p>Differences<\/p>\n\n<ul>\n<li>There may be many \"central\" repositories.<\/li>\n<li>Code from disparate repositories are merged based on a web of trust, i.e., historical merit or quality of changes.<\/li>\n<li>Lieutenants are project members who have the power to dynamically decide which branches to merge.<\/li>\n<li>Network is not involved in most operations.<\/li>\n<li>A separate set of \"sync\" operations are available for committing or receiving changes with remote repositories.<\/li>\n<\/ul><\/li>\n<\/ul>\n\n<p>[edit] <\/p>\n\n<ul>\n<li><p>Advantages<\/p>\n\n<ul>\n<li>Allows users to work productively even when not connected to a network<\/li>\n<li>Makes most operations much faster since no network is involved<\/li>\n<li>Allows participation in projects without requiring permissions from project authorities, and thus arguably better fosters culture of meritocracy[citation needed] instead of requiring \"committer\" status<\/li>\n<li>Allows private work, so you can use your revision control system even for early drafts you don't want to publish<\/li>\n<li>Avoids relying on a single physical machine. A server disk crash is a non-event with distributed revision control<\/li>\n<li>Still permits centralized control of the \"release version\" of the project<\/li>\n<\/ul><\/li>\n<\/ul>\n\n<p>[edit] <\/p>\n\n<ul>\n<li><p>Disadvantages<\/p>\n\n<ul>\n<li>Concepts of DVCSs are more difficult for developers to grasp as they are required to know more about infrastructure.<\/li>\n<\/ul><\/li>\n<\/ul>\n\n<p>however in the end I believe what it would come down to is what companies use what. Look at COBOL, it is still used in a lot of places even though it's even taught that much anymore. Companies that already have this implemented will most likely stay with what they have instead of changing everything to accommodate the new hype. IMO.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2010-04-01 15:55:33.563 UTC",
        "Answer_score":2.0,
        "Owner_location":"Richmond, Virginia United States",
        "Answer_last_edit_date":"2010-04-01 16:14:06.583 UTC",
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":72092617,
        "Question_title":"Vertex AI invoke endpoint failed. ERROR: Prediction failed. Please contact cloudml-feedback@google.com",
        "Question_body":"<p>I have deployed my App on Vertex AI endpoint. The endpoint is created successfully and I am getting &quot;Active&quot; status. But when I try to invoke the endpoint, I am getting the following error:<\/p>\n<pre><code>{\n    &quot;error&quot;: {\n        &quot;code&quot;: 500,\n        &quot;message&quot;: &quot;Prediction failed. Please contact cloudml-feedback@google.com&quot;,\n        &quot;status&quot;: &quot;INTERNAL&quot;\n    }\n}\n<\/code><\/pre>\n<p>POST Request URL: <code>https:\/\/us-central1-aiplatform.googleapis.com\/v1\/projects\/${PROJECT_ID}\/locations\/us-central1\/endpoints\/${ENDPOINT_ID}:predict<\/code><\/p>\n<p>Header: I am passing the required header i.e., Content-Type: application\/json and Authorization token.<\/p>\n<p>Body:<\/p>\n<pre><code>{\n  &quot;instances&quot;: [{\n      &quot;image_path&quot;: &quot;https:\/\/github.com\/naqishah13\/multilabelimages\/blob\/main\/movie-poster-1.jpg?raw=true&quot;\n      }\n  ]\n}\n<\/code><\/pre>\n<p>Here is my flask code that I am using for prediction:<\/p>\n<pre><code>@app.route('\/get_movie_genres\/', methods=['POST'])\ndef main():\n    request_json = request.get_json()\n    request_instances = request_json['instances']\n    image_path = request_instances[0]['image_path']\n    predicted_genres = predict_(image_path)\n    output = {'predictions':\n               [\n                   {\n                       'predicted_genres' : prediction\n                   }\n               ]\n           }\n    return jsonify(output)\n<\/code><\/pre>\n<p>I have saved some images on github and I am using those images (via github path) in order to do the movie genre prediction.<\/p>\n<p>Am I doing something wrong? what might be causing the issue?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-05-02 21:31:19.767 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-05-03 04:40:01.557 UTC",
        "Question_score":0,
        "Question_tags":"python|flask|google-cloud-vertex-ai",
        "Question_view_count":143,
        "Owner_creation_date":"2022-05-02 21:09:40.543 UTC",
        "Owner_last_access_date":"2022-06-21 11:13:56.757 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":53432985,
        "Question_title":"gremlin-python drop multiple vertices and edges in one transaction",
        "Question_body":"<p>My context:<\/p>\n\n<ul>\n<li>gremlin-python <\/li>\n<li>AWS Neptune<\/li>\n<\/ul>\n\n<p>My question is:<\/p>\n\n<p>1) Can I drop vertices and edges recursively in one transaction given that I have all the specific ids of the vertices and edges I want to drop? The aim is to write python function that evaluates each edge, and determine whether it needs to be dropped, and chain a gremlin query to drop it. <\/p>\n\n<p>For instance:<\/p>\n\n<p>vertex ids to delete:<\/p>\n\n<pre><code>'vertex1', 'vertex2', 'vertex3'\n<\/code><\/pre>\n\n<p>edge ids to delete:<\/p>\n\n<pre><code>'edge1', 'edge2', 'edge3'\n<\/code><\/pre>\n\n<p>An example of the python function to chain on to g would be like:<\/p>\n\n<pre><code>def chain_drop(g, vertex_id):\n    g = g.V(vertex_id).drop()\n    return g\n<\/code><\/pre>\n\n<p>The chained query i would like to execute as one transaction would ideally look something like:<\/p>\n\n<pre><code>g.E('edge1').drop()\n .V('vertex1').drop()\n .E('edge3').drop()\n .V('vertex3').drop()\n .iterate() # execute all these queries as one transaction\n<\/code><\/pre>\n\n<p>The above doesn't work... And it seems I cannot .E('someid') in the middle of my gremlin query.<\/p>\n\n<p>Slightly off topic but my best try (non-recursive) would be something like below:<\/p>\n\n<pre><code>g.E('edge1', 'edge2', 'edge3').as_('edges')\n .V('vertex1', 'vertex2', 'vertex3').as_('vertices')\n .union(__.select('edges'),\n        __.select('vertices'))\n .drop()\n .iterate()\n<\/code><\/pre>\n\n<p>Any help much appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-11-22 14:21:29.457 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|gremlin|amazon-neptune",
        "Question_view_count":1852,
        "Owner_creation_date":"2017-12-09 17:33:59.173 UTC",
        "Owner_last_access_date":"2019-07-31 08:23:07.02 UTC",
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>You can't do what you want to do exactly for two reasons:<\/p>\n\n<ol>\n<li>There is no mid-traversal <code>E()<\/code> as it is a start step only.<\/li>\n<li><code>drop()<\/code> is kills all traversers so the mid-traversal <code>V()<\/code> following it won't ever get executed.<\/li>\n<\/ol>\n\n<p>You can take a different approach however and simply gather all of your elements to remove and then then drop those...something like:<\/p>\n\n<pre><code>g.V(1).aggregate('x').V(2).aggregate('x').select('x').unfold().drop()\n<\/code><\/pre>\n\n<p>Note that dropping vertices will also drop edges so if those edges you want to drop are connected to the vertices you want to drop you don't need to drop them explicitly.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2018-11-22 14:45:12.95 UTC",
        "Answer_score":5.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":26265181,
        "Question_title":"Matching using the new Azure Machine Learning",
        "Question_body":"<p>Does anyone have an example of doing matching using the new <a href=\"http:\/\/azure.microsoft.com\/en-us\/services\/machine-learning\/\" rel=\"nofollow\">Machine Learning functionality<\/a> in Microsoft Azure? <\/p>\n\n<p>The examples of doing classification make sense, and I was wondering if there was an example of doing matching using the built in tools. This would be instead of using classification and comparing those with my own custom codes. Either way an example would be nice. <\/p>\n\n<p>I want to match two different entities based on location, demographic data, etc.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2014-10-08 19:47:28.427 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2016-02-01 04:25:47.15 UTC",
        "Question_score":3,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":954,
        "Owner_creation_date":"2013-12-11 18:44:27.693 UTC",
        "Owner_last_access_date":"2018-09-26 19:41:04.177 UTC",
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":70435505,
        "Question_title":"Vertex AI AutoML getting data about Model, Dataset, Training Job",
        "Question_body":"<p>I am using Vertex AI for AutoML Video classification and I would like to get some data that I'm seeing in Web UI (Cloud Console) (Model\/Dataset detail).\nI'm using AI platform Python SDK or REST API.<\/p>\n<p>For example Model API returns 'training videos' but not test videos (web Model detail, tab EVALUATE)<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Y7Um9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Y7Um9.png\" alt=\"Vertex AI Model evaluation\" \/><\/a><\/p>\n<p>then for example in tab Model Properties on the web I can't obtain Training time, Total items, Algorithm, Objective, Total Items<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/5Pi96.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5Pi96.png\" alt=\"Vertex AI Model properties\" \/><\/a><\/p>\n<p>For Dataset detail, I would like to get number of labeled\/unlabeled videos, labels and correspoding number<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xr8Gw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xr8Gw.png\" alt=\"Dataset detail, labels\" \/><\/a><\/p>\n<p>This is code I'm using to get the data (as component in Vertex AI Pipeline):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_metadata(project_id, region, model_id):\n    import requests\n\n    import google.auth\n    import google.cloud.aiplatform as aip\n    from google.cloud import aiplatform_v1\n    from google.protobuf import json_format\n    from google.auth.transport import requests as grequests\n\n    aip.init(project=project_id, location=region)\n    API_ENDPOINT = &quot;{}-aiplatform.googleapis.com&quot;.format(region)\n    model = aip.Model(model_id)\n    model_dict = model.to_dict()\n    model_metadata = model_dict['metadata']\n\n    model_name = model_dict['displayName']\n    model_creation_date = model_dict['createTime']\n    model_type = model_metadata['modelType']\n    number_training = model_metadata['trainingDataItemsCount']\n\n    client_options = {\n        &quot;api_endpoint&quot;: API_ENDPOINT\n    }\n    model_path = model.resource_name\n    client_model = aiplatform_v1.services.model_service.ModelServiceClient(client_options=client_options)\n    list_eval_request = aiplatform_v1.types.ListModelEvaluationsRequest(parent=model_path)\n    list_eval = client_model.list_model_evaluations(request=list_eval_request)\n\n    eval_name = ''\n    for val in list_eval:\n        eval_name = val.name\n    get_eval_request = aiplatform_v1.types.GetModelEvaluationRequest(name=eval_name)\n    model_eval = client_model.get_model_evaluation(request=get_eval_request)\n    model_eval_data = json_format.MessageToDict(model_eval._pb)\n\n    model_metrics = model_eval_data['metrics']\n    average_precision = model_metrics.get('auPrc')\n    confidence_metrics = model_metrics['confidenceMetrics']\n    confidence_threshold = -1\n    f1_score = -1\n    precision = -1\n    recall = -1\n\n    for item in confidence_metrics:\n        confidence_threshold_temp = item['confidenceThreshold']\n        if confidence_threshold_temp &gt;= 0.5:\n            confidence_threshold = confidence_threshold_temp\n            f1_score = item['f1Score']\n            precision = item['precision']\n            recall = item['recall']\n            break\n    # auc_precision = precision\n    # auc_recall = recall\n\n    credentials, _ = google.auth.default()\n    r = grequests.Request()\n    credentials.refresh(r)\n    training_pipeline_resource_name = model_dict['trainingPipeline']\n\n    training_pipeline_url = f'https:\/\/{API_ENDPOINT}\/v1beta1\/{training_pipeline_resource_name}'\n    headers = {\n        'Authorization': f'Bearer {credentials.token}'\n    }\n    r = requests.get(training_pipeline_url, headers=headers)\n    training_pipeline_detail = r.json()\n    input_data_config = training_pipeline_detail.get('inputDataConfig', {})\n    dataset_id = input_data_config.get('datasetId', '')\n    fraction_split = input_data_config.get('fractionSplit', {})\n    test_fraction = fraction_split.get('testFraction')\n    training_fraction = fraction_split.get('trainingFraction')\n    data_split = f'{training_fraction}\/{test_fraction}'\n\n    dataset = aip.VideoDataset(dataset_id)\n    dataset_resource = json_format.MessageToDict(dataset.gca_resource._pb)\n    dataset_name = dataset_resource.get('displayName')\n    dataset_creation_date = dataset_resource.get('createTime')\n    labels = dataset_resource['labels']\n    dataset_type = labels.get('aiplatform.googleapis.com\/dataset_metadata_schema')\n\n    data = {\n        'model_id': model_id,\n        'model_name': model_name,\n        'model_creation_date': model_creation_date,\n        'model_type': model_type,\n        'number_training': number_training,\n        'average_precision': average_precision,\n        'precision': precision,\n        'recall': recall,\n        'data_split': data_split,\n        'dataset_name': dataset_name,\n        'dataset_type': dataset_type,\n        'dataset_id': dataset_id,\n        'dataset_creation_date': dataset_creation_date,        \n    }\n\n<\/code><\/pre>\n<p>Also for example what I found is that on training job when I created dataset, training model via WebUI I can obtain data split (training\/testing ratio), but when I'm doing this in Vertex AI Pipelines, I'm not explicitly setting data split for AutoMLVideoTrainingJobRunOp, I can't get data split from Training job detail, so it seems that it saved only when it's explicitly set.<\/p>\n<p>Other thing I noticed is when API requests are made for Cloud Console (inspecting Chrome Dev Tools) it returns more (richer) data (items) then for public Vertex AI APIs.<\/p>\n<p>I'm not sure if this is temporary or intentional\/permanent behaviour.<\/p>\n<p>I would appreciate thoughts\/comments\/help with this.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2021-12-21 12:29:50.567 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-automl|google-cloud-vertex-ai|google-cloud-ai|google-cloud-ai-platform-pipelines",
        "Question_view_count":395,
        "Owner_creation_date":"2013-06-25 21:45:24.86 UTC",
        "Owner_last_access_date":"2022-09-24 17:35:26.44 UTC",
        "Owner_reputation":276,
        "Owner_up_votes":46,
        "Owner_down_votes":0,
        "Owner_views":47,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Prague, Czech Republic",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":69040163,
        "Question_title":"Vertex ai custom model training for pyspark ml model",
        "Question_body":"<p>Is it possible to train a spark\/pyspark ML lib model using VertexAI custom container model building? I couldn't find any reference in the vertex ai documents regarding spark model training. For distributed processing model building only options available are PyTorch or TensorFlow.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-03 06:00:46.963 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"apache-spark|pyspark|apache-spark-mllib|machine-learning-model|google-cloud-vertex-ai",
        "Question_view_count":432,
        "Owner_creation_date":"2021-09-03 05:52:21.403 UTC",
        "Owner_last_access_date":"2022-09-23 09:40:44.083 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":56797782,
        "Question_title":"Error while storing vertex and edge using gremlin with spring-boot",
        "Question_body":"<p>I am using spring-boot to create a rest api. So I am sending data over api which will run 24\/7.<\/p>\n\n<p>I configured cluster, client and GraphTraversalSource with remote connection once. using @Inject annotation.<\/p>\n\n<p>When the method is called from the controller, I am sending two objects. <\/p>\n\n<p>User and Movie and craeted pipeline like this.<\/p>\n\n<p>g.addV(\"USER\").property(T.id,\"userid1\").addV(\"MOVIE\").property(T.id,\"movie1\").next()<\/p>\n\n<p>this stores both vertexes.<\/p>\n\n<p>Now I call the api again with different user and different movie.<\/p>\n\n<p>g.addV(\"USER\").property(T.id,\"userid2\").addV(\"MOVIE\").property(T.id,\"movie2\").next()<\/p>\n\n<p>even both vertexes ids are different, I still get error for the \"userid1\". I don't understand, why I am getting the error for \"userid\"<\/p>\n\n<p>org.apache.tinkerpop.gremlin.driver.exception.ResponseException: {\"requestId\":\"965ffcdc-204f-4c2d-989b-108f4f2fd53c\",\"detailedMessage\":\"Vertex with id already exists: userid1\",\"code\":\"ConstraintViolationException\"}<\/p>\n\n<pre><code>at org.apache.tinkerpop.gremlin.driver.Handler$GremlinResponseHandler.channelRead0(Handler.java:259) ~[gremlin-driver-3.4.2.jar:3.4.2]\n\nat org.apache.tinkerpop.gremlin.driver.Handler$GremlinResponseHandler.channelRead0(Handler.java:198) ~[gremlin-driver-3.4.2.jar:3.4.2]\n\nat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]\n\nat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]\n\nat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]\n\nat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]\n\nat org.apache.tinkerpop.gremlin.driver.Handler$GremlinSaslAuthenticationHandler.channelRead0(Handler.java:124) ~[gremlin-driver-3.4.2.jar:3.4.2]\n\nat org.apache.tinkerpop.gremlin.driver.Handler$GremlinSaslAuthenticationHandler.channelRead0(Handler.java:68) ~[gremlin-driver-3.4.2.jar:3.4.2]\n\nat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]\n\nat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]\n\nat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]\n\nat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]\n\nat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]\n\nat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]\n\nat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]\n\nat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352) ~[netty-all-4.1.36.Final.jar:4.1.36.Final]\n\nat org.apach\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":3,
        "Question_creation_date":"2019-06-27 19:55:00.763 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"gremlin|tinkerpop3|amazon-neptune",
        "Question_view_count":233,
        "Owner_creation_date":"2013-07-02 22:49:57.27 UTC",
        "Owner_last_access_date":"2020-07-07 21:00:44.237 UTC",
        "Owner_reputation":1247,
        "Owner_up_votes":121,
        "Owner_down_votes":1,
        "Owner_views":61,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":56522685,
        "Question_title":"ValueError: Cannot Convert String to Float With Pandas and Amazon Sagemaker",
        "Question_body":"<p>I'm trying to deploy a simple ML model on SageMaker to get the hang of it, and I am not having any luck because I get the following error:  <\/p>\n\n<pre><code>ValueError: could not convert string to float: '6.320000000000000097e-03 1.800000000000000000e+01 2.310000000000000053e+00 0.000000000000000000e+00 5.380000000000000338e-01 6.575000000000000178e+00 6.520000000000000284e+01 4.089999999999999858e+00 1.000000000000000000e+00 2.960000000000000000e+02 1.530000000000000071e+01 3.968999999999999773e+02 4.980000000000000426e+00 2.400000000000000000e+01'\n<\/code><\/pre>\n\n<p>This is the first row of my dataframe.  <\/p>\n\n<p>This is the code in my notebook that I'm using right now:<\/p>\n\n<pre><code>from sagemaker import get_execution_role, Session\nfrom sagemaker.sklearn.estimator import SKLearn\nwork_dir = 'data'\nsession  = Session()\nrole     = get_execution_role()\ntrain_input = session.upload_data('data')\nscript      = 'boston_housing_prep.py'\n\nmodel = SKLearn(\nentry_point         = script,\ntrain_instance_type = 'ml.c4.xlarge',\nrole                = role,\nsagemaker_session   = session,\nhyperparameters     = {'alpha': 10}\n)\n\nmodel.fit({'train': train_input})\n<\/code><\/pre>\n\n<p>My script for boston_housing_prep.py looks like this:<\/p>\n\n<pre><code>import argparse\nimport pandas as pd\nimport os\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.externals import joblib\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--alpha', type=int, default=1)\n\n    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n\n    args = parser.parse_args()\n    input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]\n    if len(input_files) == 0:\n        raise ValueError(('There are no files in {}.\\n' +\n                      'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n                      'the data specification in S3 was incorrectly specified or the role specified\\n' +\n                      'does not have permission to access the data.').format(args.train, \"train\"))\n    raw_data = [ pd.read_csv(file, header=None, engine=\"python\") for file in input_files ]\n    df       = pd.concat(raw_data)\n\n    y_train = df.iloc[:, -1]\n    X_train = df.iloc[:, :5]\n\n    scaler  = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n\n    alpha = args.alpha\n\n    clf = Ridge(alpha=alpha)\n    clf = clf.fit(X_train, y_train)\n\n    joblib.dump(clf, os.path.join(args.model_dir, \"model.joblib\"))\n\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n    return clf\n<\/code><\/pre>\n\n<p>The line that's giving the problem is this one: <\/p>\n\n<pre><code>X_train = scaler.fit_transform(X_train)\n<\/code><\/pre>\n\n<p>I tried <code>df = df.astype(np.float) <\/code> after I loaded in the df, but that didn't work either.<\/p>\n\n<p>This file loads in without a problem when I'm not in SageMaker.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4,
        "Question_creation_date":"2019-06-10 08:05:10.92 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-06-10 10:13:03.33 UTC",
        "Question_score":0,
        "Question_tags":"pandas|numpy|scikit-learn|amazon-sagemaker",
        "Question_view_count":336,
        "Owner_creation_date":"2014-09-15 23:32:32.337 UTC",
        "Owner_last_access_date":"2022-09-23 22:20:44.79 UTC",
        "Owner_reputation":3257,
        "Owner_up_votes":451,
        "Owner_down_votes":0,
        "Owner_views":319,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"New York, NY, United States",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":60638587,
        "Question_title":"How to get insights in exceptions and logging of AzureML endpoint deployment",
        "Question_body":"<p>Because of a faulty score.py file in my InferenceConfig, a Model.Deploy failed to Azure Machine Learning, using ACI.  I wanted to create the endpoint in the cloud, but the only state I can see in the portal is Unhealthy.  My local script to deploy the model (using ) keeps running, until it times out. (using the <code>service.wait_for_deployment(show_output=True)<\/code>statement).<\/p>\n\n<p>Is there an option to get more insights in the actual reason\/error message of the deployment turning \"Unhealthy\"?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-11 14:39:32.18 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-03-14 19:54:48.01 UTC",
        "Question_score":3,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":364,
        "Owner_creation_date":"2013-02-12 07:50:30.743 UTC",
        "Owner_last_access_date":"2022-09-21 18:28:12.907 UTC",
        "Owner_reputation":2947,
        "Owner_up_votes":297,
        "Owner_down_votes":16,
        "Owner_views":355,
        "Answer_body":"<p>Usually the timeout is caused by an error in init() function in scoring script. You can get the detailed logs using <code>print(service.get_logs())<\/code> to find the Python error.<\/p>\n\n<p>For more comprehensive troubleshooting guide, see:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-03-13 21:02:44.177 UTC",
        "Answer_score":2.0,
        "Owner_location":"Belgium",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":66655967,
        "Question_title":"Monitoring the performance of ML model on EC2 Instance",
        "Question_body":"<p>If we go back and use dockerized ML models on EC2 Instances - is there any native way to check the model metrics (for classification, for example, accuracy, precision, recall and f1-score)? For sure, Cloudwatch can be used but it will just gives the overall info regarding endpoint, disk utilisation, etc., but not ML model metrics.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-16 13:14:39 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-03-16 19:07:31.6 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|amazon-ec2|amazon-sagemaker|production",
        "Question_view_count":67,
        "Owner_creation_date":"2013-12-08 08:33:34.717 UTC",
        "Owner_last_access_date":"2022-09-21 18:08:28.293 UTC",
        "Owner_reputation":2778,
        "Owner_up_votes":138,
        "Owner_down_votes":1,
        "Owner_views":352,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":53045141,
        "Question_title":"What does \"git clone \/path\/to\/repository\" do?",
        "Question_body":"<p>I came across <code>git clone \/path\/to\/repository<\/code> in my Git <a href=\"https:\/\/confluence.atlassian.com\/bitbucketserver\/basic-git-commands-776639767.html\" rel=\"nofollow noreferrer\">tutorial<\/a>. What does this command do and when will you use it?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_date":"2018-10-29 12:05:05.587 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":-5,
        "Question_tags":"git|github|version-control|dvcs",
        "Question_view_count":1893,
        "Owner_creation_date":"2016-04-04 15:38:55.887 UTC",
        "Owner_last_access_date":"2022-09-14 19:59:33.07 UTC",
        "Owner_reputation":51,
        "Owner_up_votes":507,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Answer_body":"<p>When there exists a repository at <code>\/path\/to\/repository<\/code>, or in Windows-speak, <code>C:\\path\\to\\repository<\/code>, then the commands<\/p>\n\n<pre><code>git clone \/path\/to\/repository     # Linux, Unix, Cygwin\ngit clone C:\\path\\to\\repository   # Windows CMD\ngit clone C:\/path\/to\/repository   # Git-for-Windows's shell\n<\/code><\/pre>\n\n<p>create a copy of the repository in the new directory <code>repository<\/code> (in the current directory where you issued the <code>git<\/code> command). This copy treats the original at <code>\/path\/to\/repository<\/code> as the \"origin\" repository, from which you can pull changes and to which you can push changes.<\/p>\n\n<p>It is the same as if you had cloned a repository from some remote hosting site; it just so happens that the origin is located on your local disk.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2018-10-29 13:12:48.743 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2018-10-29 19:26:25.58 UTC",
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":58325923,
        "Question_title":"Deepar Prediction Quantiles Explained",
        "Question_body":"<p>I am working with Deepar and trying to get a better understanding of the quantile values returned. From the documentation, the likelihood hyperparameter explains that: <code>...provide quantiles of the distribution and return samples<\/code>. <\/p>\n\n<p>If I look at a single data point the quantiles returned are linear. E.g. the 0.1 quantile has the lowest predicted value and 0.9 quantile has the highest predicted value. I am having trouble understanding this. If these are samples from the distribution, shouldn't they look similar to the distribution selected with the likelihood hyperparameter (negative-binomial in my case)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-10 15:01:44.473 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":568,
        "Owner_creation_date":"2016-03-04 20:36:10.467 UTC",
        "Owner_last_access_date":"2022-09-22 17:34:48.727 UTC",
        "Owner_reputation":246,
        "Owner_up_votes":39,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Answer_body":"<p>DeepAR returns probabilistic forecasts in terms of quantiles: by default, the 0.1, 0.2, 0.3, ..., 0.9 quantiles are returned. This means that, according to the model, in each future time step you have 10% chance of observing something lower than the 0.1 quantile, 20% chance of observing something lower than the 0.2 quantile, and so on. Quantiles are in fact in order, and they must be by definition of quantile. Hope this clarifies is a bit!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-11-07 12:59:51.227 UTC",
        "Answer_score":4.0,
        "Owner_location":"Denver",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":2048566,
        "Question_title":"Emacs VCS interface commits only one file",
        "Question_body":"<p>When I commit changes with Emacs' built-in VCS interface (I use it with Bazaar) it commits only one file - that's open in current buffer.\nSo when I press C-c v v, enter message and C-c C-c, it does something like<\/p>\n\n<pre><code>bzr commit -m \"my message\" file\/open\/in.buffer\n<\/code><\/pre>\n\n<p>instead of<\/p>\n\n<pre><code>bzr commit -m \"my message\"\n<\/code><\/pre>\n\n<p>How to commit all changes with Emacs?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2010-01-12 11:38:54.843 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2010-03-29 02:59:06.36 UTC",
        "Question_score":3,
        "Question_tags":"version-control|emacs|dvcs|commit|bazaar",
        "Question_view_count":1124,
        "Owner_creation_date":"2009-12-27 11:40:17.207 UTC",
        "Owner_last_access_date":"2018-12-11 20:50:44.997 UTC",
        "Owner_reputation":1014,
        "Owner_up_votes":523,
        "Owner_down_votes":18,
        "Owner_views":428,
        "Answer_body":"<p>Got an answer from identi.ca user <a href=\"https:\/\/identi.ca\/mjog\" rel=\"nofollow noreferrer\">https:\/\/identi.ca\/mjog<\/a>.<\/p>\n\n<p><a href=\"http:\/\/www.xsteve.at\/prg\/emacs_dvc\/dvc.html\" rel=\"nofollow noreferrer\">http:\/\/www.xsteve.at\/prg\/emacs_dvc\/dvc.html<\/a> - better frontend for DVCS.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2010-01-12 12:11:41.36 UTC",
        "Answer_score":2.0,
        "Owner_location":"Moscow, Russia",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":70367195,
        "Question_title":"Getting the Azure ML environment build status",
        "Question_body":"<p>I am trying to set up a ML pipeline on Azure ML using the Python SDK.\nI have scripted the creation of a custom environment from a DockerFile as follows<\/p>\n<pre><code>from azureml.core import Environment\nfrom azureml.core.environment import ImageBuildDetails\nfrom other_modules import workspace, env_name, dockerfile\n\ncustom_env : Environment = Environment.from_dockerfile(name=env_name, dockerfile=dockerfile)\n                      \ncustom_env.register(workspace=workspace)\n\nbuild : ImageBuildDetails = custom_env.build(workspace=workspace)\n\nbuild.wait_for_completion()\n<\/code><\/pre>\n<p>However, the <code>ImageBuildDetails<\/code> object that the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment.environment?view=azure-ml-py#build-workspace--image-build-compute-none-\" rel=\"nofollow noreferrer\"><code>build<\/code><\/a> method returns invariably times out while executing the last <code>wait_for_completion()<\/code> line, ... likely due to network constraints that I cannot change.<\/p>\n<p>So, how can I possibly check the build status via the SDK in a way that doesn't exclusively depend on the returned <code>ImageBuildDetails<\/code> object?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-15 16:35:16.65 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python-3.x|azure-machine-learning-service|azureml-python-sdk|azuremlsdk",
        "Question_view_count":167,
        "Owner_creation_date":"2015-12-21 00:13:47.22 UTC",
        "Owner_last_access_date":"2022-09-23 09:09:23.043 UTC",
        "Owner_reputation":372,
        "Owner_up_votes":188,
        "Owner_down_votes":1,
        "Owner_views":35,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Oslo, Norway",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":40253448,
        "Question_title":"How Can I use gensim package in Azure ML?",
        "Question_body":"<p>I am using text analysis with Azure ML. So in my python script I want to create a bag of word model and then calculate TFIDF of each words. For that I am using gensim model, It's not working on Azure ML. So is there any options for me? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2016-10-26 03:51:57.853 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"machine-learning|text-analysis|azure-machine-learning-studio",
        "Question_view_count":1024,
        "Owner_creation_date":"2016-02-11 19:00:50.737 UTC",
        "Owner_last_access_date":"2022-09-23 07:23:55.703 UTC",
        "Owner_reputation":3811,
        "Owner_up_votes":118,
        "Owner_down_votes":13,
        "Owner_views":703,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Auckland, New Zealand",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":63601068,
        "Question_title":"Sagemaker Neo compilation failed for TFLite object detection model",
        "Question_body":"<p>I am trying to compile a TFLite object detection model for imx8qm using Sagemaker Neo. I supplied a .tflite file and specified the input config like this: <code>{&quot;input&quot;:[1, 300, 300, 3]}<\/code><\/p>\n<p>But the compilation failed with the following error:\n<code>ClientError: InputConfiguration: Framework cannot load TFLite model. Unable to infer tensor data type for all inputs\/outputs: Please specify all input layers in data_shape.<\/code><\/p>\n<p>How to fix this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-26 15:40:38.747 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":359,
        "Owner_creation_date":"2011-10-20 00:07:00.893 UTC",
        "Owner_last_access_date":"2022-06-27 17:08:57.15 UTC",
        "Owner_reputation":349,
        "Owner_up_votes":98,
        "Owner_down_votes":0,
        "Owner_views":58,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":56860955,
        "Question_title":"Problem running Dask on AWS Sagemaker and AWS Fargate",
        "Question_body":"<p>I am trying to setup a cluster on AWS to run distributed sklearn model training with dask. To get started, I was trying to follow this tutorial which I hope to tweak: <a href=\"https:\/\/towardsdatascience.com\/serverless-distributed-data-pre-processing-using-dask-amazon-ecs-and-python-part-1-a6108c728cc4\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/serverless-distributed-data-pre-processing-using-dask-amazon-ecs-and-python-part-1-a6108c728cc4<\/a><\/p>\n\n<p>I have managed to push the docker container to AWS ECR and then launch a  CloudFormation template to build a cluster on AWS Fargate. The next step in the tutorial is to launch an AWS Sagemaker Notebook. I have tried this but something is not working because when I run the commands I get errors (see image). <strong>What might the problem be? Could it be related to the VPC\/subnets? Is it related to AWS Sagemaker internet access?<\/strong> (I have tried enabling and disabling this).<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/LFtvS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LFtvS.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Expected Results: dask to update, scaling up of the Fargate cluster to work.<\/p>\n\n<p>Actual Results: none of the above.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-02 22:55:34.793 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-07-02 23:01:31.117 UTC",
        "Question_score":0,
        "Question_tags":"dask|amazon-sagemaker|dask-distributed|aws-fargate",
        "Question_view_count":1239,
        "Owner_creation_date":"2018-04-06 12:57:57.347 UTC",
        "Owner_last_access_date":"2022-08-21 20:25:36.183 UTC",
        "Owner_reputation":192,
        "Owner_up_votes":175,
        "Owner_down_votes":1,
        "Owner_views":80,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":68448005,
        "Question_title":"Trying to deploy machine learning model on kubernettes, getting failed with ModuleNotFoundError: No module named 'Cython' or 'setuptools_rust'",
        "Question_body":"<p>Here is my environment yml file :-<\/p>\n<pre><code># Conda environment specification. The dependencies defined in this file will\n# be automatically provisioned for runs with userManagedDependencies=False.\n# Details about the Conda environment file format:\n# https:\/\/conda.io\/docs\/user-guide\/tasks\/manage-environments.html#create-env-file-manually\n\n\nname: project_environment\ndependencies:\n  # The python interpreter version.\n\n  # Currently Azure ML only supports 3.5.2 and later.\n\n- python\n- pip\n\n- pip:\n    # Required packages for AzureML execution, history, and data preparation.\n\n  - azureml-defaults\n  - scikit-learn\n\n  - numpy\n  - azureml-monitoring\n  - cython\n  - setuptools_rust\n<\/code><\/pre>\n<p>And it is failing on below code:<\/p>\n<pre><code>-\ndeployment_config = AksWebservice.deploy_configuration(auth_enabled=False, collect_model_data=True, enable_app_insights=True, cpu_cores = 2, memory_gb = 2)\naks_target = AksCompute(ws,aks_name)\n(On below line getting error)\nservice = Model.deploy(ws, service_name, [model], inference_config, deployment_config, aks_target)\nservice.wait_for_deployment(show_output = True)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-07-19 23:27:10.41 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-07-22 19:27:52.303 UTC",
        "Question_score":-2,
        "Question_tags":"azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":151,
        "Owner_creation_date":"2021-07-19 23:18:48.023 UTC",
        "Owner_last_access_date":"2021-09-08 18:37:11.09 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":72986981,
        "Question_title":"Porting custom job from GCP AI Platform to Vertex AI - how to get state and logs of job?",
        "Question_body":"<p>I am porting custom job training from gcp AI Platform to Vertex AI.\nI am able to start a job, but can't find how to to get the status and how to stream the logs to my local client.<\/p>\n<p>For AI Platform I was using this to get the state:<\/p>\n<pre><code>from google.oauth2 import service_account\nfrom googleapiclient import discovery\nscopes = ['https:\/\/www.googleapis.com\/auth\/cloud-platform']\ncredentials = service_account.Credentials.from_service_account_file(keyFile, scopes=scopes)\nml_apis = discovery.build(&quot;ml&quot;,&quot;v1&quot;, credentials=credentials, cache_discovery=False)\nx = ml_apis.projects().jobs().get(name=&quot;projects\/%myproject%\/jobs\/&quot;+job_id).execute()  # execute http request\nreturn x['state']\n<\/code><\/pre>\n<p>And this to stream the logs:<\/p>\n<pre><code>cmd = 'gcloud ai-platform jobs stream-logs ' + job_id\n<\/code><\/pre>\n<p>This does not work for Vertex AI job. What is the replacement code?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-07-14 21:47:19.54 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|google-ai-platform",
        "Question_view_count":62,
        "Owner_creation_date":"2009-10-06 11:50:17.773 UTC",
        "Owner_last_access_date":"2022-09-23 20:34:01.687 UTC",
        "Owner_reputation":2595,
        "Owner_up_votes":462,
        "Owner_down_votes":5,
        "Owner_views":357,
        "Answer_body":"<p>Can you try this command for streaming logs :<\/p>\n<pre><code>gcloud ai custom-jobs stream-logs 123 --region=europe-west4\n<\/code><\/pre>\n<p>123 is the <strong>ID<\/strong> of the custom job for this case, you can add glcoud wide flags such as --format as well.<\/p>\n<p>You can visit this <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ai\/custom-jobs\/stream-logs\" rel=\"nofollow noreferrer\">link<\/a> for more details about this command and additional flags available.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-15 03:12:46.997 UTC",
        "Answer_score":1.0,
        "Owner_location":"Germany",
        "Answer_last_edit_date":"2022-07-15 05:34:19.57 UTC",
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":63637178,
        "Question_title":"Spark is running python 3.7.6 but spark context is showing python 2.7. How to fix using the spark context?",
        "Question_body":"<p>I have installed sagemaker using <code>sc.install_pypi_package(&quot;sagemaker==2.5.1&quot;)<\/code>. However, I get the following error when I try to import sagemaker and it is pointing to python2.7.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Yfln3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yfln3.png\" alt=\"cannot import name git_utils\" \/><\/a><\/p>\n<p>I checked my EMR master node running pyspark and the version there is pyspark 2.4.5 running python 3.7.6.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hyctk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hyctk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So then I tried to upgrade the python version of my spark context but it says<\/p>\n<blockquote>\n<p>&quot;ValueError: Package already installed for current Spark context!&quot;<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/XBg3E.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XBg3E.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So I thought lemme try uninstalling python2.7 from spark context and that does not let me do it, saying<\/p>\n<blockquote>\n<p>&quot;Not uninstalling python at \/usr\/lib64\/python2.7\/lib-dynload, outside\nenvironment \/tmp\/1598628537004-0&quot;<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ktlfO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ktlfO.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What am I doing wrong? I believe the sagemaker import is failing due to spark context referring python2.7. How do I fix this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-28 15:55:22.48 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-08-28 16:13:30.847 UTC",
        "Question_score":0,
        "Question_tags":"python|apache-spark|version|amazon-emr|amazon-sagemaker",
        "Question_view_count":855,
        "Owner_creation_date":"2015-06-12 14:03:56.363 UTC",
        "Owner_last_access_date":"2022-04-06 17:39:44.81 UTC",
        "Owner_reputation":78,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":"<p>Referred to this <a href=\"https:\/\/aws.amazon.com\/blogs\/big-data\/install-python-libraries-on-a-running-cluster-with-emr-notebooks\/\" rel=\"nofollow noreferrer\">link<\/a> and updated the python version of spark context to python3. This fixes the issue:<\/p>\n<pre><code>%%configure -f\n{ &quot;conf&quot;:{\n          &quot;spark.pyspark.python&quot;: &quot;python3&quot;,\n          &quot;spark.pyspark.virtualenv.enabled&quot;: &quot;true&quot;,\n          &quot;spark.pyspark.virtualenv.type&quot;:&quot;native&quot;,\n          &quot;spark.pyspark.virtualenv.bin.path&quot;:&quot;\/usr\/bin\/virtualenv&quot;\n         }\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-08-28 16:28:48.583 UTC",
        "Answer_score":1.0,
        "Owner_location":"Toronto, ON, Canada",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":65349712,
        "Question_title":"How to install Jupyter extensions on Azure Machine Learning Notebooks?",
        "Question_body":"<p>I created a virtual machine on Azure Machine Learning and I'm running a simple jupyter notebook. I would like to install the jupyter extensions since I really need the collapsible titles but it seem it isn't working. I tried with pip install and it's already installed but the menu does not appear...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-12-17 23:17:51.16 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"azure|jupyter-notebook|azure-machine-learning-studio",
        "Question_view_count":320,
        "Owner_creation_date":"2020-03-17 01:48:16.707 UTC",
        "Owner_last_access_date":"2022-09-15 01:18:42.977 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Switzerland",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":60343353,
        "Question_title":"Clear data in AWS Neptune",
        "Question_body":"<p>I am trying to run simple CLEAR and DELETE SPARQL queries and keep getting back:<\/p>\n\n<pre><code>    Malformed query: Encountered \\\" \\\"clear\\\" \\\"CLEAR \\\"\\\" at line\n    1, column 1.\\n Was expecting one of:\\n \\\"base\\\" ...\\n \\\"prefix\\\" \n...\\n \\\"select\\\" ...\\n \\\"construct\\\" ...\\n \\ \"describe\\\" ...\\n \\\"ask\\\" ...\\n\n<\/code><\/pre>\n\n<p>Are the CLEAR and DELETE queries not supported? or does Neptune have another way of clearing the graph in the instance.<\/p>\n\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_date":"2020-02-21 17:07:20.403 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"curl|postman|sparql|rdf|amazon-neptune",
        "Question_view_count":888,
        "Owner_creation_date":"2018-12-03 19:04:22.41 UTC",
        "Owner_last_access_date":"2022-09-12 11:17:13.42 UTC",
        "Owner_reputation":23,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Answer_body":"<blockquote>\n  <p>Word of Caution: The answer contains examples to DELETE all your data,\n  so be extra careful when you execute these queries in your database.<\/p>\n<\/blockquote>\n\n<p>Neptune does support CLEAR and DELETE. CLEAR and DELETE are UPDATE operations, so you can do them in two ways: <\/p>\n\n<p>1) Use \"update=\" in the request params<\/p>\n\n<pre><code>curl http:\/\/endpoint:8182\/sparql -d \"update=DELETE DATA { &lt;http:\/\/x&gt; &lt;http:\/\/y&gt; &lt;http:\/\/z&gt; }\u201d \n\nOR\n\ncurl http:\/\/localhost:8182\/sparql -d \"update=DELETE WHERE { ... }\u201d \n\nYou can use a similar one for CLEAR.\n<\/code><\/pre>\n\n<p>2) Use Content-Type Header (application\/sparql-update) and use the query directly in request params. <\/p>\n\n<pre><code>curl http:\/\/endpoint:8182\/sparql -H \"Content-Type: application\/sparql-update\" -d \"DELETE DATA { &lt;http:\/\/x&gt; &lt;http:\/\/y&gt; &lt;http:\/\/z&gt; }\u201d \n\n<\/code><\/pre>\n\n<p>It looks like you tried a mix of both, and possibly got the combination incorrect. Neptune is fully SPARQL 1.1 compliant, so if you see something to not be working, do let us know. In almost all cases, the request would not have adhered to the SPARQL HTTP spec.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-02-21 23:47:04.923 UTC",
        "Answer_score":6.0,
        "Owner_location":"Hoboken, NJ, USA",
        "Answer_last_edit_date":"2020-02-23 03:01:45.733 UTC",
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":66865031,
        "Question_title":"Sagemaker Regression - ValueError: Cannot format input",
        "Question_body":"<p>I am new to SageMaker &amp; Python<\/p>\n<p>I am trying to get a simple regression model going on AWS using Jupyter Notebooks.\nI am using the Abalone date from the  UCI data repository.\nI would greatly appreciate some assistance or a link to help me in what to do.<\/p>\n<p>Everything looks fine until I try to run:<\/p>\n<pre><code>\nregression_linear = sagemaker.estimator.Estimator(\n    container,\n    role=sagemaker.get_execution_role(),\n    input_mode = &quot;File&quot;,\n    instance_count = 1,\n    instance_type='ml.m4.xlarge',\n    output_path=output_location,\n    sagemaker_session=sess\n    )\n\nregression_linear.set_hyperparameters(\n    feature_dim=8,\n    epochs=16,\n    wd=0.01,\n    loss=&quot;absolute_loss&quot;,\n    predictor_type=&quot;regressor&quot;,\n    normalize_data=True,\n    optimizer=&quot;adam&quot;,\n    mini_batch_size=100,\n    lr_scheduler_step=100,\n    lr_scheduler_factor=0.99,\n    lr_scheduler_minimum_lr=0.0001,\n    learning_rate=0.1,\n    )\n\nfrom time import gmtime, strftime\njob_name = &quot;DEMO-linear-learner-abalone-regression-&quot; + strftime(&quot;%H-%M-%S&quot;, gmtime())\nprint(&quot;Training job: &quot;, job_name)\n\nregression_linear.fit(inputs={&quot;train&quot;: train_data}, job_name=job_name)\n<\/code><\/pre>\n<p>Then I am getting the following error:<\/p>\n<pre><code>ValueError                                Traceback (most recent call last)\n&lt;ipython-input-101-82bd2950b590&gt; in &lt;module&gt;\n----&gt; 1 regression_linear.fit(inputs={&quot;train&quot;: train_data}, job_name=job_name)\n      2 \n      3 # , &quot;validation&quot;: test_data\n\nValueError: Cannot format input       age  sex  length  diameter  height  whole_weight  shucked_weight  \\\n449    18    0   0.565     0.455   0.150        0.8205          0.3650   \n1080    7    1   0.430     0.335   0.120        0.3970          0.1985   \n2310   13    0   0.435     0.350   0.110        0.3840          0.1430   \n3790   10    0   0.650     0.505   0.175        1.2075          0.5105   \n3609    9    0   0.555     0.405   0.120        0.9130          0.4585   \n...   ...  ...     ...       ...     ...           ...             ...   \n2145    9    0   0.415     0.325   0.115        0.3455          0.1405   \n3815    8   -1   0.460     0.340   0.100        0.3860          0.1805   \n3534    6   -1   0.400     0.315   0.090        0.3300          0.1510   \n2217   13    0   0.515     0.415   0.130        0.7640          0.2760   \n3041    9    1   0.575     0.470   0.150        0.9785          0.4505   \n\n      vicera_weight  shell_weight  \n449          0.1590        0.2600  \n1080         0.0865        0.1035  \n2310         0.1005        0.1250  \n3790         0.2620        0.3900  \n3609         0.1960        0.2065  \n...             ...           ...  \n2145         0.0765        0.1100  \n3815         0.0875        0.0965  \n3534         0.0680        0.0800  \n2217         0.1960        0.2500  \n3041         0.1960        0.2760  \n\n[2923 rows x 9 columns]. Expecting one of str, TrainingInput, file_input or FileSystemInput\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-30 05:00:01.673 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|linear-regression|amazon-sagemaker",
        "Question_view_count":163,
        "Owner_creation_date":"2019-03-11 22:58:34.29 UTC",
        "Owner_last_access_date":"2022-08-19 02:02:31.103 UTC",
        "Owner_reputation":672,
        "Owner_up_votes":69,
        "Owner_down_votes":18,
        "Owner_views":111,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Melbourne VIC, Australia",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":58285038,
        "Question_title":"Trying to query Azure SQL Database with Azure ML \/ Docker Image",
        "Question_body":"<p>I wanted to do a realtime deployment of my model on azure, so I plan to create an image which firsts queries an ID in azure SQL db to get the required features, then predicts using my model and returns the predictions. The error I get from PyODBC library is that drivers are not installed<\/p>\n\n<p>I tried it on the azure ML jupyter notebook to establish the connection and found that no drivers are being installed in the environment itself. After some research i found that i should create a docker image and deploy it there,  but i still met with the same results<\/p>\n\n<pre><code>    driver= '{ODBC Driver 13 for SQL Server}'\n    cnxn = pyodbc.connect('DRIVER='+driver+';SERVER='+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password+';Encrypt=yes'+';TrustServerCertificate=no'+';Connection Timeout=30;')\n<\/code><\/pre>\n\n<blockquote>\n  <p>('01000', \"[01000] [unixODBC][Driver Manager]Can't open lib 'ODBC\n  Driver 13 for SQL Server' : file not found (0) (SQLDriverConnect)\")<\/p>\n<\/blockquote>\n\n<p>i want a result to the query instead i get this message<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_date":"2019-10-08 10:59:56.097 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-10-08 11:55:47.687 UTC",
        "Question_score":1,
        "Question_tags":"sql-server|azure|azure-machine-learning-service",
        "Question_view_count":1319,
        "Owner_creation_date":"2018-05-10 14:51:27.537 UTC",
        "Owner_last_access_date":"2019-12-10 09:09:54.133 UTC",
        "Owner_reputation":55,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Mumbai, Maharashtra, India",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":73334181,
        "Question_title":"How to save and access pickle\/hdf5 files in azure machine learning studio",
        "Question_body":"<p>I have a pickle file parameters.pkl containing some parameters and their values of a model. The pickle file has been created through the following process:<\/p>\n<pre><code>dict={'scaler': scaler,\n'features': z_tags,\n'Z_reconstruction_loss': Z_reconstruction_loss} \npickle.dump(dict, open('parameters.pkl', 'wb'))\n\nmodel_V2.hdf5\n<\/code><\/pre>\n<p>I am new to azure machine learning studio.It will be helpful to know, how the pickle file and hdf5 files can be stored in Azure machine Learning Studio and an API endpoint be created, so that the the pickle file can be accessed through API. Objective is to access the pickle file and its contents through API.. I have tried the following:<\/p>\n<pre><code>pip install azureml , azureml-core\nfrom azureml.core import Workspace\nfrom azureml.core.webservice import AciWebservice\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.conda_dependencies import CondaDependencies\n\n\nws = Workspace.create(\n               name='myworkspace',            \n               subscription_id='&lt;azure-subscription-id&gt;',           \n               resource_group='myresourcegroup',                 \n               create_resource_group=True,                 \n               location='eastus2'                \n               )\n\nws.write_config()\n\nws = Workspace.from_config()\n\nmodel = Model.register(workspace = ws,\n              model_path =&quot;model\/parameters.pkl&quot;,\n              model_name = &quot;parameters&quot;,\n              tags = {&quot;version&quot;: &quot;1&quot;},\n              description = &quot;parameters&quot;,\n              )\n\n\n# to install required packages\nenv = Environment('env')\ncd = CondaDependencies.create(pip_packages=['pandas==1.1.5', 'azureml-defaults','joblib==0.17.0'], conda_packages = ['scikit-learn==0.23.2'])\nenv.python.conda_dependencies = cd\n# Register environment to re-use later\nenv.register(workspace = ws)\nprint(&quot;Registered Environment&quot;)\n\nmyenv = Environment.get(workspace=ws, name=&quot;env&quot;)\n\nmyenv.save_to_directory('.\/environ', overwrite=True)\n\naciconfig = AciWebservice.deploy_configuration(\n            cpu_cores=1,\n            memory_gb=1,\n            tags={&quot;data&quot;:&quot;parameters&quot;},\n            description='parameters MODEL',\n            )\n\ninference_config = InferenceConfig(entry_script=&quot;score.py&quot;, environment=myenv)\n<\/code><\/pre>\n<p>What to modify in following score script, as I don't want to predict anything but access the parameter values stored in the pickle file.<\/p>\n<pre><code>def init():\n    global modelmodel_path = Model.get_model_path(&quot;parameters&quot;)\n    print(&quot;Model Path is  &quot;, model_path)\n    model = joblib.load(model_path)\n\ndef run(data):\n   try:\n     data = json.loads(data)\n     result = model.predict(data['data'])\n     return {'data' : result.tolist() , 'message' : &quot;Successfully \n            accessed&quot;}\n   except Exception as e:\n      error = str(e)\n      return {'data' : error , 'message' : 'Failed to access'}\n\nDeploy the Model\nservice = Model.deploy(workspace=ws,\n                name='iris-model',\n                models=[model],\n                inference_config=inference_config,\n                deployment_config=aciconfig, \n                overwrite = True)\nservice.wait_for_deployment(show_output=True)\nurl = service.scoring_uri\nprint(url)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-12 12:27:08.527 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python-3.x|azure|pickle|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":130,
        "Owner_creation_date":"2015-05-15 10:32:06.62 UTC",
        "Owner_last_access_date":"2022-09-21 10:30:16.813 UTC",
        "Owner_reputation":543,
        "Owner_up_votes":17,
        "Owner_down_votes":2,
        "Owner_views":125,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":64634812,
        "Question_title":"Passing multiple lines of input for SageMaker prediction",
        "Question_body":"<p>I have built a SageMaker pipeline which uses a combination of Custom Transformer (using SKLearn Transformer and an XGBoost model). A sample pipeline is shown below:<\/p>\n<pre><code>from sagemaker.model import Model\nfrom sagemaker.pipeline import PipelineModel\nimport boto3\nfrom time import gmtime, strftime\n\ntimestamp_prefix = strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\n\nscikit_learn_inferencee_model = sklearn_preprocessor.create_model()\nxg_model = xg_estimator.create_model()\n\nmodel_name = 'inference-pipeline-' + timestamp_prefix\nendpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\nsm_model = PipelineModel(\n    name=model_name, \n    role=role, \n    models=[\n        scikit_learn_inference_model, \n        xg_model])\n\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)\n<\/code><\/pre>\n<p>Tbe sklearn preprocessor takes 100 rows of input and generates a single line that is passed as input to the XGBoost model for prediction. So for every 100 rows of input, I get only one prediction.<\/p>\n<p>However, all the examples for SageMaker prediction point to only one row of input as follows:<\/p>\n<pre><code>from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\nfrom sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n\npayload = 'M, 0.44, 0.365, 0.125, 0.516, 0.2155, 0.114, 0.155'\nactual_rings = 10\n\npredictor = RealTimePredictor(\n    endpoint=endpoint_name,\n    sagemaker_session=sagemaker_session,\n    serializer=csv_serializer,\n    content_type=CONTENT_TYPE_CSV,\n    accept=CONTENT_TYPE_JSON)\n\nprint(predictor.predict(payload))\n<\/code><\/pre>\n<p>How can I pass multiple of rows of input to a SageMaker endpoint and get 1 prediction out?<\/p>\n<p>Thank you.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2020-11-01 17:29:28.273 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|scikit-learn|amazon-sagemaker",
        "Question_view_count":383,
        "Owner_creation_date":"2011-10-19 10:12:30.6 UTC",
        "Owner_last_access_date":"2022-09-09 09:12:03.143 UTC",
        "Owner_reputation":3073,
        "Owner_up_votes":238,
        "Owner_down_votes":5,
        "Owner_views":341,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":1391241,
        "Question_title":"Is there a free private Bazaar online source host?",
        "Question_body":"<p>I am looking for a free private online Bazaar source host. Does anybody know of any? I had a quick look and it seems there are lots of Git, Mercurial, and SVN options, but no Bazaar?<\/p>\n\n<p>Just to clarify, when I say \"private\" I mean in the sense that you get your own private repository where you can grant access to people you choose.<\/p>",
        "Question_answer_count":7,
        "Question_comment_count":4,
        "Question_creation_date":"2009-09-07 22:55:25.157 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2015-07-21 16:21:09.963 UTC",
        "Question_score":17,
        "Question_tags":"version-control|hosting|dvcs|bazaar",
        "Question_view_count":7627,
        "Owner_creation_date":"2009-08-17 06:30:42.25 UTC",
        "Owner_last_access_date":"2022-09-17 09:26:15.707 UTC",
        "Owner_reputation":10738,
        "Owner_up_votes":3048,
        "Owner_down_votes":0,
        "Owner_views":363,
        "Answer_body":"<p><a href=\"http:\/\/launchpad.net\" rel=\"nofollow noreferrer\">http:\/\/launchpad.net<\/a> - free bazaar hosting... but I don't know what do you mean by access - I think that is has same functionalisty as github (we are using it working with Moovida) but I am not quite sure.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2009-09-08 21:52:31.603 UTC",
        "Answer_score":3.0,
        "Owner_location":"Melbourne, Australia",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":60334889,
        "Question_title":"\"No Kernel!\" error Azure ML compute JupyterLab",
        "Question_body":"<p>When using the JupyterLab found within the azure ML compute instance, every now and then, I run into an issue where it will say that network connection is lost. <\/p>\n\n<p>I have confirmed that the computer is still running.\nthe notebook itself can be edited and saved, so the computer\/VM is definitely running\nOf course, the internet is fully functional<\/p>\n\n<p>On the top right corner <em>next to the now blank circle<\/em> it will say \"No Kernel!\"<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-21 08:39:54.26 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio|azure-machine-learning-service|azure-machine-learning-workbench",
        "Question_view_count":1026,
        "Owner_creation_date":"2015-09-15 16:27:17.953 UTC",
        "Owner_last_access_date":"2022-09-24 06:49:58.907 UTC",
        "Owner_reputation":2272,
        "Owner_up_votes":1340,
        "Owner_down_votes":67,
        "Owner_views":516,
        "Answer_body":"<p>We can't repro the issue, can you help gives us more details? One possibility is that the kernel has bugs and hangs (could be due to extensions, widgets installed) or the resources on the machine are exhausted and kernel dies. What VM type are you using? If it's a small VM you may ran out of resources.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-02-21 22:12:37.643 UTC",
        "Answer_score":1.0,
        "Owner_location":"Bangalore, Karnataka, India",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-workbench",
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":72919002,
        "Question_title":"How to automate cleaning of data in AWS using Jupyter Notebook",
        "Question_body":"<p>I have a Jupyter Notebook file that cleans the data file (.csv) in S3. The cleaning process is taken care of...<\/p>\n<p>However, I want to be able to automatically apply this cleaning process to every file that is uploaded to the S3 bucket. Each file will have the exact same data format. I am thinking maybe of using AWS Glue, but not sure where to start. If we can skip the upload to S3 and go straight into Glue that would be interesting to explore...<\/p>\n<p>The end goal is to load the clean data in Quick Sight and also AWS Sage Maker for ML applications.<\/p>\n<p>Any advice on how to approach this?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-07-09 04:46:41.077 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":-2,
        "Question_tags":"amazon-web-services|jupyter-notebook|aws-glue|amazon-sagemaker",
        "Question_view_count":38,
        "Owner_creation_date":"2022-06-24 04:36:44.433 UTC",
        "Owner_last_access_date":"2022-09-24 21:40:40.873 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":71278400,
        "Question_title":"Can't start Sagemaker Notebook Instance with a CodeCommit repo",
        "Question_body":"<p>I linked a repo in CodeCommit to Sagemaker. However when I try to start an instance with that repo it fails and I get a message:<\/p>\n<pre><code>fatal: unable to access 'https:\/\/git-codecommit.us-east-1.amazonaws.com\/v1\/repos\/MyRepo\/': The requested URL returned error: 403\n<\/code><\/pre>\n<p>I think maybe it has something to do with the IAM role. Is there some policy I should add to the AmazonSageMaker-ExecutionRole. I am completely new to this so please excuse any incorrect usage of terms here.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-26 16:31:14.433 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-iam|amazon-sagemaker|aws-codecommit",
        "Question_view_count":264,
        "Owner_creation_date":"2019-07-16 22:44:53.577 UTC",
        "Owner_last_access_date":"2022-06-30 19:48:40.413 UTC",
        "Owner_reputation":349,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":40,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":60650556,
        "Question_title":"From the cluster, cannot download a Python Wheel from the storage account",
        "Question_body":"<p>1) We upload a python wheel to the storage account associated with the workspace successfully.\n2) In the second step we submit an experiment which runs in the cluster and needs to download and run the package from step 1.<\/p>\n\n<p>The experiment is able to download the package and run when the storage account is not associated with any VNet. However, when we associate the storage account in Vnets the experiment hangs and eventually fails. This storage account is in two Vnet\u2019s, a, and b. The cluster is also in the Vnet, a.<\/p>\n\n<p>I don\u2019t know why the cluster cannot download the wheel package when the storage account is in a Vnet. It is our policy to have storage accounts in Vnet\u2019s.<\/p>\n\n<p>It there something else we are missing? I also checked the container registry setting and its set to \u2018allow from everywhere\u2019 (we are using std SKU).<\/p>\n\n<p>Let me know if any further information is required. Thanks.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2020-03-12 08:56:53.54 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-studio|azure-machine-learning-service|azure-machine-learning-workbench",
        "Question_view_count":81,
        "Owner_creation_date":"2011-02-02 10:11:55.12 UTC",
        "Owner_last_access_date":"2022-08-19 08:34:49.06 UTC",
        "Owner_reputation":389,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":137,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-workbench",
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":70955450,
        "Question_title":"How to return all labels and scores in SageMaker Inference?",
        "Question_body":"<p>I am using the <code>HuggingFacePredictor<\/code> from <code>sagemaker.huggingface<\/code> to inference some text and I would like to get all label scores.<\/p>\n<p>Is there any way of getting, as response from the endpoint:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;labels&quot;: [&quot;help&quot;, &quot;Greeting&quot;, &quot;Farewell&quot;] ,\n    &quot;score&quot;: [0.81, 0.1, 0.09],\n}\n<\/code><\/pre>\n<p>(or similar)<\/p>\n<p>Instead of:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;label&quot;: &quot;help&quot;,\n    &quot;score&quot;: 0.81,\n}\n<\/code><\/pre>\n<p>Here is some example code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nfrom sagemaker.huggingface import HuggingFacePredictor\nfrom sagemaker.session import Session\n\nsagemaker_session = Session(boto_session=boto3.session.Session())\n\npredictor = HuggingFacePredictor(\n    endpoint_name=project, sagemaker_session=sagemaker_session\n)\nprediciton = predictor.predict({&quot;inputs&quot;: text})[0]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-02-02 12:14:43.13 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-02-02 12:32:12.913 UTC",
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|nlp|amazon-sagemaker|huggingface-transformers",
        "Question_view_count":193,
        "Owner_creation_date":"2021-09-30 08:42:37.333 UTC",
        "Owner_last_access_date":"2022-09-23 14:16:21.167 UTC",
        "Owner_reputation":373,
        "Owner_up_votes":36,
        "Owner_down_votes":4,
        "Owner_views":17,
        "Answer_body":"<p>With your current code sample, it is not quite clear what specific task you are performing, but for the sake of this answer, I'll assume you're doing text classification.<\/p>\n<p>Most importantly, though, we can read the following in <a href=\"https:\/\/huggingface.co\/docs\/sagemaker\/reference#inference-toolkit-api\" rel=\"nofollow noreferrer\">Huggingface's Sagemaker reference document<\/a> (bold highlight by me):<\/p>\n<blockquote>\n<p>The Inference Toolkit accepts inputs in the inputs key, and <strong>supports additional <code>pipelines<\/code> parameters in the <code>parameters<\/code> key<\/strong>. You can provide any of the supported <code>kwargs<\/code> from <code>pipelines<\/code> as parameters.<\/p>\n<\/blockquote>\n<p>If we check out the <a href=\"https:\/\/huggingface.co\/docs\/transformers\/v4.16.2\/en\/main_classes\/pipelines#transformers.TextClassificationPipeline.__call__\" rel=\"nofollow noreferrer\">accepted arguments by the <code>TextClassificationPipeline<\/code><\/a>, we can see that there is indeed one that returns all samples:<\/p>\n<blockquote>\n<p><code>return_all_scores<\/code> (bool, optional, defaults to False) \u2014 Whether to return scores for all labels.<\/p>\n<\/blockquote>\n<p>While I unfortunately don't have access to Sagemaker inference, I can run a sample to illustrate the output with a local pipeline:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import pipeline\n# uses 2-way sentiment classification model per default\npipe = pipeline(&quot;text-classification&quot;) \n\npipe(&quot;I am really angry right now &gt;:(&quot;, return_all_scores=True)\n# Output: [[{'label': 'NEGATIVE', 'score': 0.9989138841629028},\n#           {'label': 'POSITIVE', 'score': 0.0010860705515369773}]]\n<\/code><\/pre>\n<p>Based on the slightly different input format expected by Sagemaker, coupled with the example given in <a href=\"https:\/\/github.com\/huggingface\/notebooks\/blob\/master\/sagemaker\/10_deploy_model_from_s3\/deploy_transformer_model_from_s3.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a>, I would assume that a corrected input in your own example code should look like this:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;inputs&quot;: text,\n    &quot;parameters&quot;: {&quot;return_all_scores&quot;: True}\n}\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2022-02-02 13:40:35.227 UTC",
        "Answer_score":1.0,
        "Owner_location":"Barcelona",
        "Answer_last_edit_date":"2022-02-02 14:56:13.01 UTC",
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":55515277,
        "Question_title":"Ho do I call a webservice deployed in Azure Machine Learning Service?",
        "Question_body":"<p>I'm trying Azure Machine Learning Service for the first time. I worked on examples given by MS. I was able to develop in Python and deploy as web services. But I'm not able to use it outside the notebook or any application. Do I need API of that web service? If yes, where can I find it?<\/p>\n\n<p>I have not got anything to try on. I googled for different methods, but couldn't find relevant link. So I didn't try much there.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-04-04 11:48:47.737 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-12-23 01:23:12.4 UTC",
        "Question_score":0,
        "Question_tags":"python-3.x|azure-machine-learning-service",
        "Question_view_count":163,
        "Owner_creation_date":"2018-12-20 07:13:19.703 UTC",
        "Owner_last_access_date":"2021-02-01 05:49:25.62 UTC",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":3065105,
        "Question_title":"Creating a Bazaar branch from an offline SVN working copy?",
        "Question_body":"<p>I'm doing some offline development on my SVN working copy. Since I won't have access to the SVN repository for a while, I wanted to use Bazaar as a helper version control to keep the intermediate commit history before I commit everything back to the SVN repository. Is this possible?<\/p>\n\n<p>When I try to create a branch using TortoiseBZR from the SVN working copy, it wants to access the SVN repository, which is a problem.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2010-06-17 19:49:58.53 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"svn|dvcs|bazaar|tortoisebzr",
        "Question_view_count":234,
        "Owner_creation_date":"2009-01-15 12:49:22.153 UTC",
        "Owner_last_access_date":"2022-09-23 05:58:43.833 UTC",
        "Owner_reputation":18245,
        "Owner_up_votes":2261,
        "Owner_down_votes":41,
        "Owner_views":2139,
        "Answer_body":"<p>You can just disable\/uninstall bzr-svn plugin if you don't need to work with svn servers from bzr.<\/p>\n\n<p>Or, in the command-line execute following command:<\/p>\n\n<pre><code>bzr --no-plugins init\n<\/code><\/pre>\n\n<p>It will create bzr branch in your directory, and after that bzr and TortoiseBzr won't try to open svn working copy. <\/p>\n\n<p>But you probably still will have problems when running bzr commands from subfolders.\nSo, you can add all required files in your svn copy under bzr version control, then commit them:<\/p>\n\n<pre><code>bzr add\nbzr commit -m initial\n<\/code><\/pre>\n\n<p>Now you can re-create this state of files in different (empty) directory with <\/p>\n\n<pre><code>bzr branch path\/to\/bzr\/branch\/in\/svn\/copy new\/path\n<\/code><\/pre>\n\n<p>And do all work in <code>new\/path<\/code>. When you'll be ready to update your svn working copy with latest committed revision from new\/path then just push your changes back:<\/p>\n\n<pre><code>bzr push path\/to\/bzr\/branch\/in\/svn\/copy\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2010-06-18 04:32:53.827 UTC",
        "Answer_score":2.0,
        "Owner_location":"Maribor, Maribor Slovenia",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":73216926,
        "Question_title":"Load estimator from model artifact in s3 bucket aws",
        "Question_body":"<p>I have used estimator for a pytorch model and have saved the artifacts in s3 bucket. using below code<\/p>\n<pre><code>estimator = PyTorch(\n    entry_point=&quot;train_deploy.py&quot;,\n    source_dir=&quot;code&quot;,\n    role=role,\n    framework_version=&quot;1.3.1&quot;,\n    py_version=&quot;py3&quot;,\n    instance_count=1,  # this script only support distributed training for GPU instances.\n    instance_type=&quot;ml.m5.12xlarge&quot;,\n    output_path=output_path,\n    hyperparameters={\n        &quot;epochs&quot;: 1,\n        &quot;num_labels&quot;: 7,\n        &quot;backend&quot;: &quot;gloo&quot;,\n    },\n    disable_profiler=False, # disable debugger\n)\nestimator.fit({&quot;training&quot;: inputs_train, &quot;testing&quot;: inputs_test})\n<\/code><\/pre>\n<p>The model works well and there are no issues with it. However i would like to re use this model later for inference, how do i do that. i am looking for something like below<\/p>\n<pre><code>estimator = PyTorch.load(input_path = &quot;&lt;xyz&gt;&quot;)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-03 06:06:27.267 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|pytorch|amazon-sagemaker|bert-language-model",
        "Question_view_count":28,
        "Owner_creation_date":"2013-05-05 08:07:44.647 UTC",
        "Owner_last_access_date":"2022-09-19 08:48:50.963 UTC",
        "Owner_reputation":169,
        "Owner_up_votes":23,
        "Owner_down_votes":2,
        "Owner_views":30,
        "Answer_body":"<p>I was able to solve this by the following steps<\/p>\n<pre><code>model_data=output_path\nfrom sagemaker.pytorch.model import PyTorchModel \n\npytorch_model = PyTorchModel(model_data=model_data,\n                             role=role,\n                             framework_version=&quot;1.3.1&quot;,\n                             source_dir=&quot;code&quot;,\n                             py_version=&quot;py3&quot;,\n                             entry_point=&quot;train_deploy.py&quot;)\n\npredictor = pytorch_model.deploy(initial_instance_count=1, instance_type=&quot;ml.m4.2xlarge&quot;)\npredictor.serializer = sagemaker.serializers.JSONSerializer()\npredictor.deserializer = sagemaker.deserializers.JSONDeserializer()\nresult = predictor.predict(&quot;&lt;text that needs to be predicted&gt;&quot;)\nprint(&quot;predicted class: &quot;, np.argmax(result, axis=1))\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-08-03 07:29:00.763 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":70070421,
        "Question_title":"Return confidence score with custom model for Vertex AI batch predictions",
        "Question_body":"<p>I uploaded a pretrained scikit learn classification model to Vertex AI and ran a batch prediction on 5 samples. It just returned a list of false predictions with no confidence score. I don't see anywhere in the SDK documentation or Google console for how to get batch predictions to include the confidence scores. Is that something Vertex AI can do?<\/p>\n<p>My intent is to automate a batch prediction pipeline using the following code.<\/p>\n<pre><code># Predict\n# &quot;csv&quot;, &quot;&quot;bigquery&quot;, &quot;tf-record&quot;, &quot;tf-record-gzip&quot;, or &quot;file-list&quot;\nbatch_prediction_job = model.batch_predict(\n    job_display_name = job_display_name,\n    gcs_source = input_path,\n    instances_format = &quot;&quot;, # jsonl, csv, bigquery, \n    gcs_destination_prefix = output_path,\n    starting_replica_count = 1,\n    max_replica_count = 10,\n    sync = True,\n)\n\nbatch_prediction_job.wait()\n\nreturn batch_prediction_job.resource_name\n<\/code><\/pre>\n<p>I tried it out in google console as a test to make sure my input data was properly formatted.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2021-11-22 18:04:35.697 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-11-23 16:51:56.647 UTC",
        "Question_score":0,
        "Question_tags":"python|scikit-learn|google-cloud-vertex-ai",
        "Question_view_count":319,
        "Owner_creation_date":"2014-11-26 14:46:22.68 UTC",
        "Owner_last_access_date":"2022-09-23 13:33:34.89 UTC",
        "Owner_reputation":1256,
        "Owner_up_votes":391,
        "Owner_down_votes":2,
        "Owner_views":245,
        "Answer_body":"<p>I don't think so; the stock sklearn container provided by vertex doesn't provide such a score I guess. You might need to write a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/use-custom-container\" rel=\"nofollow noreferrer\">custom container<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-11-23 06:05:01.513 UTC",
        "Answer_score":1.0,
        "Owner_location":"Boston, MA",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":62092647,
        "Question_title":"Lambda timeout when adding an edge using nodejs to aws neptune using gremlin",
        "Question_body":"<p>We are trying to add an edge between two vertex using nodejs running in Lambda and aws neptune. \nWe can easily add a vertex, but when we try to add an edge, our Lambda timesout. Our timeout is set to 20 seconds. <\/p>\n\n<p>Here is the code.<\/p>\n\n<pre><code>async function updateDoc (db, dataTmp, now, id, callback, logData, errorData) {\n  try {\n    console.log('inside updateDoc function')   \n\n    console.log(dataTmp)        \n\n    var user1 = await g.V().hasLabel('user').has('userId', dataTmp.userId).valueMap().unfold().toList()\nconsole.log('user1', user1)\nvar user2 = await g.V().hasLabel('user').has('userId', dataTmp.trustedUserId).valueMap().unfold().toList()\nconsole.log('user2', user2)\nconst edgeAdded =  await g.V().hasLabel('user').has('userId', dataTmp.userId).as('u').V().hasLabel('user').has('userId', dataTmp.trustedUserId).as('tu').addE('trust').from('u').to('tu').next()\nconsole.log('edgeAdded making asynch', edgeAdded)\n    \/\/ dc.close()        \n    return callback(null)\n  } catch (error) {\n    return callback(null)\n  }    \n\n}\n<\/code><\/pre>\n\n<p>Here is the output from lambda <\/p>\n\n<p>{8 items\n\"type\":\"AddTrustedUser\"<\/p>\n\n<p>\"userId\":\"5ed1cd97ee7ac30008b86a8c\"<\/p>\n\n<p>\"trustedUserId\":\"5ed1cd98ee7ac30008b86a8d\"<\/p>\n\n<p>\"dupId\":\"r54sw17ND\"<\/p>\n\n<p>\"time\":\"2020-05-30T03:06:02.632Z\"<\/p>\n\n<p>\"initTime\":\"2020-05-30T03:06:02.632Z\"<\/p>\n\n<p>\"lastTime\":\"2020-05-30T03:06:02.632Z\"<\/p>\n\n<p>\"logsGroupName\":\"AddTrustedUser\"\n}<\/p>\n\n<p>user1 [ <\/p>\n\n<p>Map { 'firstName' => [ 'jaat' ] },<\/p>\n\n<p>Map { 'userId' => [ '5ed1cd97ee7ac30008b86a8c' ] }<\/p>\n\n<p>]<\/p>\n\n<p>user2 [ <\/p>\n\n<p>Map { 'firstName' => [ 'maat' ] },<\/p>\n\n<p>Map { 'userId' => [ '5ed1cd98ee7ac30008b86a8d' ] }<\/p>\n\n<p>]<\/p>\n\n<p>REPORT  Duration: 20020.15 ms   Billed Duration: 20000 ms   Memory Size: 1024 MB    Max Memory Used: 103 MB Init Duration: 745.55 ms     [+20021ms]\n3 minutes ago cafbd19c-7b7a-4acb-b3fb-134bb51f054f Task timed out after 20.02 seconds [+20021ms]<\/p>\n\n<p>Any pointers on what we are doing wrong. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2020-05-29 18:38:59.47 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-05-30 03:14:02.093 UTC",
        "Question_score":0,
        "Question_tags":"node.js|aws-lambda|gremlin|amazon-neptune",
        "Question_view_count":357,
        "Owner_creation_date":"2014-06-30 22:12:58.957 UTC",
        "Owner_last_access_date":"2022-09-20 03:57:57.493 UTC",
        "Owner_reputation":591,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":50,
        "Answer_body":"<p>Changing from() to from_() fixed the issue. \ntinkerpop.apache.org\/docs\/current\/reference\/#from-step<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-06-07 03:52:07.29 UTC",
        "Answer_score":0.0,
        "Owner_location":"Massachusetts",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":67517072,
        "Question_title":"Deploying YOLOV5 on Azure Machine Learning",
        "Question_body":"<p>I have a YOLOV5 model trained on a custom dataset and I try to deploy it to Azure with a pipeline.<\/p>\n<p>First I tried it with a notebook instance and everything is fine but since I need to automatize it I am try to create a &quot;dataset&quot; on Azure but when I upload the dataset it changes the dataset type (Normally in YOLO it must be like this -images(folder) -labels(folder))<\/p>\n<p>Later tried it with method below:<\/p>\n<pre><code>run = Experiment(ws, name='try').submit(src)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>but when I run it I am having the following error<\/p>\n<pre><code>TypeError: '&gt;' not supported between instances of 'int' and 'str'\n<\/code><\/pre>\n<p>I read several guides from Microsoft but none of them includes deploying an object detection model with a custom dataset.<\/p>\n<p>So I am a bit lost, If anybody can guide me I would appreciate it<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-05-13 09:41:58.617 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-05-14 07:17:25.207 UTC",
        "Question_score":1,
        "Question_tags":"azure|object-detection|yolo|azure-machine-learning-service|yolov5",
        "Question_view_count":1035,
        "Owner_creation_date":"2021-03-24 10:00:57.473 UTC",
        "Owner_last_access_date":"2022-09-10 14:40:32.643 UTC",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":70422687,
        "Question_title":"How add raster library for R to Sagemaker's AMI linux instance?",
        "Question_body":"<p>This is my lifecycle configuration for the Notebook but raster fails in the install step....<\/p>\n<p>I have taken some recommendations from the following thread where they install dependencies related to geospatial processing but without much success<\/p>\n<p><a href=\"https:\/\/github.com\/datacarpentry\/r-raster-vector-geospatial\/issues\/135\" rel=\"nofollow noreferrer\">github thead<\/a><\/p>\n<p>I am also incorporating EPEL, which aws recommends for these cases.<\/p>\n<pre><code>sudo -b  yum --enablerepo=epel\n<\/code><\/pre>\n<p>I install all the libraries in the conda R environment<\/p>\n<pre><code>set -e\n# OVERVIEW\n# This script installs a single conda R package (bigmemory) in SageMaker R environment.\n# To install an R package with conda, the package needs to be prefixed with 'r-'. For example, to install the package `shiny`, run 'conda install -c r r-shiny'.\n \nnohup sudo -b -u ec2-user -i &lt;&lt;'EOF'\n\nnohup sudo -b  yum --enablerepo=epel\n\n\n# First, we need to install the lustre-client libraries\n# https:\/\/stackoverflow.com\/questions\/20923209\/problems-installing-the-devtools-package\nnohup sudo -b yum install -y \\\n    libgeos \\\n    libgdal-dev \\\n    libgeos-dev \\\n    libproj-dev \\\n    libgdal-devel \\\n    libgeos-devel \\\n    libproj-devel \\\n    libcurl \\\n    libcurl-devel \\\n    openssl-devel \\\n    libxml2-devel  \n\necho &quot;PASS STAGE 1 &quot;\n\n# libgdal1 libgdal1-dev libgeos libgeos-dev\nsource activate R\n\nnohup conda install -y -c r r-binutils &amp;\nnohup conda install -y -c r r-libgit2 &amp; \nnohup conda install -y -c r r-libxml2-devel &amp;  \n\nnohup conda install -y -c r r-devtools   &amp;\n\nnohup conda install -y -c r r-remotes   &amp;\nnohup conda install -y -c r r-units   &amp;\nnohup conda install -y -c r r-sf    &amp;\nnohup conda install -y -c r r-terra   &amp;\nnohup conda install -y -c r r-spData  &amp;\nnohup conda install -y -c r r-spdep  &amp; \nnohup conda install -y -c r r-raster  &amp;  ##this can not install\nnohup conda install -y -c r r-reshape2  &amp;\nnohup conda install -y -c r r-DescTools  &amp;\nnohup conda install -y -c r r-spdep  &amp;\nnohup conda install -y -c r r-xgboost  &amp;\nnohup conda install -y -c r r-dplyr  &amp;\nnohup conda install -y -c r r-readr   &amp;\nnohup conda install -y -c r r-readxl   &amp;\nnohup conda install -y -c r r-paws   &amp; \nnohup conda install -y -c r r-botor   &amp;\n\n\necho &quot;PASS STAGE 2 &quot;\n#outForest\nnohup conda install -y -c r r-RcppEigen &amp;\nnohup conda install -y -c r r-FNN &amp;\nnohup conda install -y -c r r-ranger &amp;\nnohup conda install -y -c r r-missRanger &amp;\n\nnohup conda install -y -c r r-outForest &amp;\n \necho &quot;PASS STAGE 3 &quot;\nconda deactivate\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-12-20 13:38:41.513 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"r|linux|bash|amazon-sagemaker|amazon-ami",
        "Question_view_count":76,
        "Owner_creation_date":"2015-01-13 20:57:58.35 UTC",
        "Owner_last_access_date":"2022-09-11 04:30:15.527 UTC",
        "Owner_reputation":406,
        "Owner_up_votes":662,
        "Owner_down_votes":27,
        "Owner_views":80,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Valpara\u00edso, Chile",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":29840532,
        "Question_title":"If we have installed nltk zip file and then we want to install the setup of the same using the python script",
        "Question_body":"<p>If we have installed nltk zip file and then we want to install the setup of the same using the python script then how can we do it?<\/p>\n\n<p>I have created  the zip file and kept all the packages in it and then importing it from python script <\/p>\n\n<pre><code>from mynltk.corpus import *\nfrom mynltk.corpus.reader import *\nbrown.words()[0:10]\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2015-04-24 06:47:30.847 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2015-04-24 07:20:49.86 UTC",
        "Question_score":1,
        "Question_tags":"python|subprocess|azure-machine-learning-studio",
        "Question_view_count":44,
        "Owner_creation_date":"2015-01-22 10:00:09.5 UTC",
        "Owner_last_access_date":"2022-03-16 20:44:48.503 UTC",
        "Owner_reputation":547,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":62320331,
        "Question_title":"Sagemaker API to list Hyperparameters",
        "Question_body":"<p>I'm currently trying to implement MLFlow Tracking into my training pipeline and would like to log the hyperparameters of my hyperparameter Tuning of each training job.<\/p>\n\n<p>Does anyone know, how to pull the list of hyperparameters that can be seen on the sagemaker training job interface (on the AWS console)? Is there any other smarter way to list how models perform in comparison in Sagemaker (and displayed)?<\/p>\n\n<p>I would assume there must be an easy and Pythonic way to do this (either boto3 or the sagemaker api) to get this data. I wasn't able to find it in Cloudwatch.<\/p>\n\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-11 08:38:22.897 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|mlflow",
        "Question_view_count":484,
        "Owner_creation_date":"2016-07-28 15:35:17.217 UTC",
        "Owner_last_access_date":"2022-03-28 15:52:24.487 UTC",
        "Owner_reputation":43,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<p>there is indeed a rather pythonic way in the SageMaker python SDK:<\/p>\n\n<pre><code>tuner = sagemaker.tuner.HyperparameterTuner.attach('&lt; your tuning jobname&gt;')\n\nresults = tuner.analytics().dataframe()  # all your tuning metadata, in pandas!\n<\/code><\/pre>\n\n<p>See full example here <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-06-14 22:27:30.467 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker",
            "mlflow"
        ]
    },
    {
        "Question_id":42844593,
        "Question_title":"Comparing brier score for Azure ML classifier",
        "Question_body":"<p>I'm trying to compare the brier score for two classifiers in Azure ML studio:<\/p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import brier_score_loss\n\ndef azureml_main(dataframe1, dataframe2):\n    colnames_1 = dataframe1.columns\n    y_true_1 = np.array(dataframe1[colnames_1[1]])\n    y_prob_1 = np.array(dataframe1[colnames_1[-1]])\n    brier_score_1 = brier_score_loss(y_true_1, y_prob_1)\n\n    colnames_2 = dataframe2.columns\n    y_true_2 = np.array(dataframe2[colnames_2[1]])\n    y_prob_2 = np.array(dataframe2[colnames_2[-1]])\n    brier_score_2 = brier_score_loss(y_true_2, y_prob_2)\n\n    data = {'brier_score': [brier_score_1, brier_score_2]}\n    result = pd.DataFrame(data, columns=['brier_score'])\n\n    return result\n<\/code><\/pre>\n\n<p>My problem is that the script only outputs a value in the first row with the brier score of the first dataset. The second row is empty. This is how I have connected the script: \n<img src=\"https:\/\/anonimag.es\/i\/azure0f4ae.png\" alt=\"azure\"><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2017-03-16 20:55:07.193 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2017-03-17 02:01:37.823 UTC",
        "Question_score":0,
        "Question_tags":"python|python-3.x|azure|azure-machine-learning-studio",
        "Question_view_count":97,
        "Owner_creation_date":"2015-02-22 16:00:48.827 UTC",
        "Owner_last_access_date":"2022-09-23 13:13:21.623 UTC",
        "Owner_reputation":2026,
        "Owner_up_votes":599,
        "Owner_down_votes":15,
        "Owner_views":130,
        "Answer_body":"<p>I turned out that the problem was caused by a few NaN values in the second dataframe.\nAdding <code>dataframe2 = dataframe2.dropna()<\/code> to the top of the script solved the problem.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2017-03-30 18:58:43.347 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":71708046,
        "Question_title":"Api call- Translate Python Post to R",
        "Question_body":"<p>I am currently trying to translate this API call over from R to python. I have seen many of these sort of questions on the forum but still can't seem to figure it out. Any help is greatly appreciated!<\/p>\n<p>Here's the request that works in python:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import requests\n\nrest_endpoint = &quot;some url&quot;\nresponse = requests.post ( rest_endpoint , \n                           headers = auth_header , \n                           json = { 'ExperimentName' : 'experiment_name' ,\n                                    'ParameterAssignments' : { 'input_path' : 'this_is_a_test2.csv' } } )\n<\/code><\/pre>\n<p>I greatly appreciate any help<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-01 13:54:56.843 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|r|azure-machine-learning-service",
        "Question_view_count":50,
        "Owner_creation_date":"2020-04-01 18:26:29.497 UTC",
        "Owner_last_access_date":"2022-06-10 13:50:15.997 UTC",
        "Owner_reputation":382,
        "Owner_up_votes":32,
        "Owner_down_votes":3,
        "Owner_views":32,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":72827239,
        "Question_title":"amazon-sagemaker-lab:: libXrender.so.1 package",
        "Question_body":"<p>I am trying to use the amazon sagemaker lab environment and the package libXrender is not installed.<\/p>\n<p>sudo privileges are removed and it's not possible to install it with:<\/p>\n<p><code>apt-get install libxrender1<\/code><\/p>\n<p>Is there an easy fix or do I have to contact their support to install the package in their docker container?<\/p>\n<p>Thanks in advance!<\/p>\n<p>Error results from this piece of code:<\/p>\n<pre><code>from rdkit.Chem.Draw import rdMolDraw2D\nfrom rdkit.Chem.Draw.rdMolDraw2D import *\n\nImportError: libXrender.so.1: cannot open shared object file: No such file or directory\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-01 09:32:28.847 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker|rdkit",
        "Question_view_count":77,
        "Owner_creation_date":"2016-06-14 13:45:51.74 UTC",
        "Owner_last_access_date":"2022-09-22 10:07:51.313 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Cambridge, UK",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":69828187,
        "Question_title":"sagemaker inference container ModuleNotFoundError: No module named 'model_handler'",
        "Question_body":"<p>I am trying to deploy a model using my own custom inference container on sagemaker. I am following the documentation here <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/adapt-inference-container.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/adapt-inference-container.html<\/a><\/p>\n<p>I have an entrypoint file:<\/p>\n<pre><code>from sagemaker_inference import model_server\n#HANDLER_SERVICE = &quot;\/home\/model-server\/model_handler.py:handle&quot;\nHANDLER_SERVICE = &quot;model_handler.py&quot;\nmodel_server.start_model_server(handler_service=HANDLER_SERVICE)\n<\/code><\/pre>\n<p>I have a model_handler.py file:<\/p>\n<pre><code>from sagemaker_inference.default_handler_service import DefaultHandlerService\nfrom sagemaker_inference.transformer import Transformer\nfrom CustomHandler import CustomHandler\n\n\nclass ModelHandler(DefaultHandlerService):\n    def __init__(self):\n        transformer = Transformer(default_inference_handler=CustomHandler())\n        super(HandlerService, self).__init__(transformer=transformer)\n<\/code><\/pre>\n<p>And I have my CustomHandler.py file:<\/p>\n<pre><code>import os\nimport json\nimport pandas as pd\nfrom joblib import dump, load\nfrom sagemaker_inference import default_inference_handler, decoder, encoder, errors, utils, content_types\n\n\nclass CustomHandler(default_inference_handler.DefaultInferenceHandler):\n\n    def model_fn(self, model_dir: str) -&gt; str:\n        clf = load(os.path.join(model_dir, &quot;model.joblib&quot;))\n        return clf\n\n    def input_fn(self, request_body: str, content_type: str) -&gt; pd.DataFrame:\n        if content_type == &quot;application\/json&quot;:\n            items = json.loads(request_body)\n\n            for item in items:\n                processed_item1 = process_item1(items[&quot;item1&quot;])\n                processed_item2 = process_item2(items[&quot;item2])\n                all_item1 += [processed_item1]\n                all_item2 += [processed_item2]\n            return pd.DataFrame({&quot;item1&quot;: all_item1, &quot;comments&quot;: all_item2})\n\n    def predict_fn(self, input_data, model):\n        return model.predict(input_data)\n<\/code><\/pre>\n<p>Once I deploy the model to an endpoint with these files in the image, I get the following error: <code>ml.mms.wlm.WorkerLifeCycle - ModuleNotFoundError: No module named 'model_handler'<\/code>.<\/p>\n<p>I am really stuck what to do here. I wish there was an example of how to do this in the above way end to end but I don't think there is. Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-03 16:10:37.33 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-11-03 16:56:00.743 UTC",
        "Question_score":2,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":728,
        "Owner_creation_date":"2018-03-22 16:45:24.657 UTC",
        "Owner_last_access_date":"2022-09-23 14:29:34.747 UTC",
        "Owner_reputation":738,
        "Owner_up_votes":22,
        "Owner_down_votes":7,
        "Owner_views":69,
        "Answer_body":"<p>This is because of the path mismatch. The entrypoint is trying to look for &quot;model_handler.py&quot; in <code>WORKDIR<\/code> directory of the container.\nTo avoid this, always specify absolute path when working with containers.<\/p>\n<p>Moreover your code looks confusing. Please use this sample code as the reference:<\/p>\n<pre><code>import subprocess\nfrom subprocess import CalledProcessError\nimport model_handler\nfrom retrying import retry\nfrom sagemaker_inference import model_server\nimport os\n\n\ndef _retry_if_error(exception):\n    return isinstance(exception, CalledProcessError or OSError)\n\n\n@retry(stop_max_delay=1000 * 50, retry_on_exception=_retry_if_error)\ndef _start_mms():\n    # by default the number of workers per model is 1, but we can configure it through the\n    # environment variable below if desired.\n    # os.environ['SAGEMAKER_MODEL_SERVER_WORKERS'] = '2'\n    print(&quot;Starting MMS -&gt; running &quot;, model_handler.__file__)\n    model_server.start_model_server(handler_service=model_handler.__file__ + &quot;:handle&quot;)\n\n\ndef main():\n    _start_mms()\n    # prevent docker exit\n    subprocess.call([&quot;tail&quot;, &quot;-f&quot;, &quot;\/dev\/null&quot;])\n\nmain()\n<\/code><\/pre>\n<p>Further, notice this line - <code>model_server.start_model_server(handler_service=model_handler.__file__ + &quot;:handle&quot;) <\/code>\nHere we are starting the server, and telling it to call <code>handle()<\/code> function in model_handler.py to invoke your custom logic for all incoming requests.<\/p>\n<p>Also remember that Sagemaker BYOC requires model_handler.py to implement another function <code>ping()<\/code><\/p>\n<p>So your &quot;model_handler.py&quot; should look like this -<\/p>\n<pre><code>custom_handler = CustomHandler()\n\n# define your own health check for the model over here\ndef ping():\n    return &quot;healthy&quot;\n\n\ndef handle(request, context): # context is necessary input otherwise Sagemaker will throw exception\n    if request is None:\n        return &quot;SOME DEFAULT OUTPUT&quot;\n    try:\n        response = custom_handler.predict_fn(request)\n        return [response] # Response must be a list otherwise Sagemaker will throw exception\n\n    except Exception as e:\n        logger.error('Prediction failed for request: {}. \\n'\n                     .format(request) + 'Error trace :: {} \\n'.format(str(e)))\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-11-11 10:55:50.22 UTC",
        "Answer_score":1.0,
        "Owner_location":"Milton Keynes",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":70105315,
        "Question_title":"Vertex properties order in valuemap",
        "Question_body":"<pre><code>addV('src').\nproperty(id,'sales__src').\nproperty(&quot;index1&quot;,&quot;brand&quot;).\nproperty(&quot;index2&quot;,&quot;time&quot;).\nproperty(&quot;index3&quot;,&quot;city1&quot;).\nproperty(&quot;index4&quot;,&quot;city2&quot;).\nproperty(&quot;index5&quot;,&quot;city3&quot;).   \nproperty(&quot;index6&quot;,&quot;city4&quot;).\nproperty(&quot;index7&quot;,&quot;city5&quot;).\nproperty(&quot;index8&quot;,&quot;city6&quot;).\nproperty(&quot;index9&quot;,&quot;city7&quot;).\nproperty(&quot;index10&quot;,&quot;city8&quot;).\nproperty(&quot;index11&quot;,&quot;city9&quot;).\nproperty(&quot;index12&quot;,&quot;city10&quot;).\nas(&quot;sales_src&quot;).\n<\/code><\/pre>\n<p>When I plot in a graph<\/p>\n<pre><code>    %%gremlin -p v,inE,outV,inE,outV\ng.V('city_brand').inE().outV().inE().outV().path().\nby(valueMap().with(WithOptions.tokens))\n<\/code><\/pre>\n<p>The order of the properties index became<\/p>\n<p>index1<\/p>\n<p>index11<\/p>\n<p>index12<\/p>\n<p>How to force the order to be in 1,2,3,4 order?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-11-25 03:01:43.77 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"gremlin|amazon-neptune",
        "Question_view_count":48,
        "Owner_creation_date":"2021-04-05 13:22:39.74 UTC",
        "Owner_last_access_date":"2021-12-24 08:46:30.397 UTC",
        "Owner_reputation":15,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":51982177,
        "Question_title":"Increase the number of rows selected in Azure ML",
        "Question_body":"<p>I have created a predictive model with AML. The thing is when i execute r component only 1567 rows(result dataset) are selected from the dataset input (it contains around 85000 rows). I want to include at least 7000 for the training and scoring but i am not sure how to do it. <\/p>\n\n<p>Input Dataset:\n<a href=\"https:\/\/i.stack.imgur.com\/bmonN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bmonN.png\" alt=\"Input Dataset\"><\/a><\/p>\n\n<p>Result Dataset\n<a href=\"https:\/\/i.stack.imgur.com\/MWofY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/MWofY.png\" alt=\"Result Dataset\"><\/a><\/p>\n\n<p>Model<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Rbr0k.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Rbr0k.png\" alt=\"AML model\"><\/a><\/p>\n\n<p>Thanks in Advance.<\/p>\n\n<pre><code># Map 1-based optional input ports to variables\ndataset &lt;- maml.mapInputPort(1) # class: data.frame\n\ndataset$Capacidad &lt;- as.numeric(dataset$Capacidad)\n\nresource &lt;- list()\noutput_forecast &lt;- data.frame()\n\n\ndatasource &lt;- data.frame(Fecha = character(0), Delegacion = character(0), Grupo_recurso = character(0), IDRecurso = character(0), Numero_proyectos = numeric(0), Cantidad = numeric(0), Capacidad = numeric(0), Productividad = numeric(0))\n\n\naddToDatasource &lt;- function(datasource_data_frame, fecha, delegacion, grupo_recurso, id_recurso, numero_proyectos, cantidad, capacidad, productividad){\n\n  new_data_frame &lt;- data.frame(Fecha = fecha, Delegacion = delegacion, Grupo_recurso = grupo_recurso, IDRecurso = id_recurso, Numero_proyectos = numero_proyectos, Cantidad = cantidad, Capacidad = capacidad, Productividad = productividad)\n\n  return(rbind(datasource_data_frame, new_data_frame))\n}\n\n resource_data &lt;- dataset[!(dataset$Grupo_recurso %in% c(\"ADMIN\", \n \"DIRECTOR\", \"EXTERNO\", \"GERENTE\", \"RESP OPERACIONES\", \n\n \"MARKETING\", \"TELEMARKETING\")), ]\n\nresource_list &lt;- unique(resource_data[(resource_data$Activo == 1), \n\"IDRecurso\"])\nresource_list &lt;- resource_list[!(resource_list %in% c(\"AFH\", \"BSS\", \"EDC\",\"GLM\", \"GJ\", \"GPV\"))]\n\n\n\nfor(i in 1:length(resource_list)){\n\n  dataset_res &lt;- dataset[(dataset$IDRecurso %in% resource_list[i]),]\n\n  dataset_res$Fecha &lt;- format(as.Date(dataset_res$Fecha), \"%m-%Y\")\n\n  date_list &lt;- unique(dataset_res$Fecha)\n    for(j in 1:length(date_list)){\n\n    dataset_date &lt;- dataset_res[(dataset_res$Fecha == date_list[j]),]\n\n    #Number of projects calculation\n    number_projects &lt;- length(unique(dataset_date$Proyecto))\n\n    if(sum(dataset_date$Capacidad) &gt; 0){\n\n  #Productivity calculation\n  productivity &lt;- sum(dataset_date$Cantidad_productiva)\/sum(dataset_date$Capacidad)\n\n  datasource &lt;- addToDatasource(datasource, date_list[j], unique(dataset_date$Delegacion), unique(dataset_res$Grupo_recurso), resource_list[i], number_projects, sum(dataset_date$Cantidad), sum(dataset_date$Capacidad), productivity)\n}\n<\/code><\/pre>\n\n<p>}\n}<\/p>\n\n<h1>Select data.frame to be sent to the output Dataset port<\/h1>\n\n<p>maml.mapOutputPort(\"datasource\");<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":8,
        "Question_creation_date":"2018-08-23 09:10:56.747 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-08-23 10:32:30.713 UTC",
        "Question_score":0,
        "Question_tags":"r|azure|prediction|azure-machine-learning-studio",
        "Question_view_count":60,
        "Owner_creation_date":"2016-10-11 21:15:32.123 UTC",
        "Owner_last_access_date":"2019-01-10 10:40:30.757 UTC",
        "Owner_reputation":49,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":21,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Barcelona, Espa\u00f1a",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":73666897,
        "Question_title":"Why does SageMaker PyTorch DDP init times out on SageMaker?",
        "Question_body":"<p>I'm using PyTorch DDP on SageMaker PyTorch Training DLC 1.8.1 The code seems properly DDP-formatted. I'm using instance_count = 2, and launching\u00a0<code>torch.distributed.launch<\/code>\u00a0and I believe the ranks and world size are properly set however the\u00a0<code>dist.init_process_group<\/code>\u00a0waits and times out<\/p>\n<pre><code>RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)\n<\/code><\/pre>\n<p>What could go wrong? machines not networked together?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-09 19:51:40.293 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"pytorch|amazon-sagemaker|distributed-training",
        "Question_view_count":27,
        "Owner_creation_date":"2022-09-08 07:14:26.503 UTC",
        "Owner_last_access_date":"2022-09-23 21:03:19.637 UTC",
        "Owner_reputation":48,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":55020390,
        "Question_title":"How do I start an AWS Sagemaker training job with GPU access in my docker container?",
        "Question_body":"<p>I have some python code that trains a Neural Network using tensorflow. <\/p>\n\n<p>I've created a docker image based on a tensorflow\/tensorflow:latest-gpu-py3 image that runs my python script.\nWhen I start an EC2 p2.xlarge instance I can run my docker container using the command<\/p>\n\n<pre><code>docker run --runtime=nvidia cnn-userpattern train\n<\/code><\/pre>\n\n<p>and the container with my code runs with no errors and uses the host GPU. <\/p>\n\n<p>The problem is, when I try to run the same container in an AWS Sagemaker training job with instance ml.p2.xlarge (I also tried with ml.p3.2xlarge), the algorithm fails with error code:<\/p>\n\n<blockquote>\n  <p>ImportError: libcuda.so.1: cannot open shared object file: No such file or directory<\/p>\n<\/blockquote>\n\n<p>Now I know what that error code means. It means that the runtime environment of the docker host is not set to \"nvidia\". The AWS documentation says that the command used to run the docker image is always<\/p>\n\n<pre><code>docker run image train\n<\/code><\/pre>\n\n<p>which would work if the default runtime is set to \"nvidia\" in the docker\/deamon.json. Is there any way to edit the host deamon.json or tell docker in the Dockerfile to use \"--runtime=nvidia\"?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-03-06 10:05:52.687 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|docker|tensorflow|gpu|amazon-sagemaker",
        "Question_view_count":1858,
        "Owner_creation_date":"2013-06-01 11:17:36.943 UTC",
        "Owner_last_access_date":"2022-06-24 11:54:43.47 UTC",
        "Owner_reputation":344,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Answer_body":"<p>With some help of the AWS support service we were able to find the problem.\nThe docker image I used to run my code on was, as I said tensorflow\/tensorflow:latest-gpu-py3 (available on <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a>)<\/p>\n\n<p>the \"latest\" tag refers to version 1.12.0 at this time. The problem was not my own, but with this version of the docker image. <\/p>\n\n<p>If I base my docker image on tensorflow\/tensorflow:1.10.1-gpu-py3, it runs as it should and uses the GPU fully. <\/p>\n\n<p>Apparently the default runtime is set to \"nvidia\" in the docker\/deamon.json on all GPU instances of AWS sagemaker.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-03-07 13:17:35.197 UTC",
        "Answer_score":0.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":55425199,
        "Question_title":"How to input data from S3 Bucket to Amazon Sagemaker",
        "Question_body":"<p>I would like to input data(Caltech 256 dataset) from Amazon s3 to sagemaker. I am doing this because I would like to modify the dataset if I can get this to work. Any ideas?<\/p>\n\n<p>Tried using the pandas code from 'Load S3 Data into AWS SageMaker Notebook'<\/p>\n\n<p>I hope the data from the S3 bucket will work just like downloading straight from the url. Obviously it isn't working.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2019-03-29 20:39:45.25 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-02-14 07:12:17.433 UTC",
        "Question_score":0,
        "Question_tags":"amazon-s3|deep-learning|artificial-intelligence|amazon-sagemaker|aws-deeplens",
        "Question_view_count":194,
        "Owner_creation_date":"2019-03-29 15:27:32.897 UTC",
        "Owner_last_access_date":"2020-06-29 22:55:47.223 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":46523924,
        "Question_title":"Adding python modules to AzureML workspace",
        "Question_body":"<p>I've been working recently on deploying a machine learning model as a web service. I used Azure Machine Learning Studio for creating my own Workspace ID and Authorization Token. Then, I trained LogisticRegressionCV model from <strong>sklearn.linear_model<\/strong> locally on my machine (using python 2.7.13) and with the usage of below code snippet I wanted to publish my model as web service:<\/p>\n\n<pre><code>from azureml import services\n\n@services.publish('workspaceID','authorization_token')\n@services.types(var_1= float, var_2= float)\n@services.returns(int)\n\ndef predicting(var_1, var_2):\n    input = np.array([var_1, var_2].reshape(1,-1)\nreturn model.predict_proba(input)[0][1]\n<\/code><\/pre>\n\n<p>where <em>input<\/em> variable is a list with data to be scored and <em>model<\/em> variable contains trained classifier. Then after defining above function I want to make a prediction on sample input vector:<\/p>\n\n<pre><code>predicting.service(1.21, 1.34)\n<\/code><\/pre>\n\n<p>However following error occurs:<\/p>\n\n<pre><code>RuntimeError: Error 0085: The following error occurred during script \nevaluation, please view the output log for more information:\n<\/code><\/pre>\n\n<p>And the most important message in log is: <\/p>\n\n<pre><code>AttributeError: 'module' object has no attribute 'LogisticRegressionCV'\n<\/code><\/pre>\n\n<p>The error is strange to me because when I was using normal <em>sklearn.linear_model.LogisticRegression<\/em> everything was fine. I was able to make predictions sending POST requests to created endpoint, so I guess <strong>sklearn<\/strong> worked correctly. \nAfter changing to <em>LogisticRegressionCV<\/em> it does not. <\/p>\n\n<p>Therefore I wanted to update sklearn on my workspace.<\/p>\n\n<p>Do you have any ideas how to do it? Or even more general question: how to install any python module on azure machine learning studio in a way to use predict functions of any model I develpoed locally?<\/p>\n\n<p>Thanks<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2017-10-02 10:37:09.737 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|azure|scikit-learn|python-module|azure-machine-learning-studio",
        "Question_view_count":2578,
        "Owner_creation_date":"2016-03-23 16:31:44.64 UTC",
        "Owner_last_access_date":"2022-09-08 12:42:19.317 UTC",
        "Owner_reputation":186,
        "Owner_up_votes":108,
        "Owner_down_votes":1,
        "Owner_views":55,
        "Answer_body":"<p>For installing python module on Azure ML Studio, there is a section <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/cdb56f95-7f4c-404d-bde7-5bb972e6f232\/#Anchor_3\" rel=\"nofollow noreferrer\"><code>Technical Notes<\/code><\/a> of the offical document <code>Execute Python Script<\/code> which introduces it.<\/p>\n\n<p>The general steps as below.<\/p>\n\n<ol>\n<li>Create a Python project via <code>virtualenv<\/code> and active it.<\/li>\n<li>Install all packages you want via <code>pip<\/code> on the virtual Python environment, and then<\/li>\n<li>Package all files and directorys under the path <code>Lib\\site-packages<\/code> of your project as a zip file.<\/li>\n<li>Upload the zip package into your Azure ML WorkSpace as a dataSet.<\/li>\n<li>Follow the offical <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/execute-python-scripts#importing-existing-python-script-modules\" rel=\"nofollow noreferrer\">document<\/a> to import Python Module for your <code>Execute Python Script<\/code>.<\/li>\n<\/ol>\n\n<p>For more details, you can refer to the other similar SO thread <a href=\"https:\/\/stackoverflow.com\/questions\/46222606\/updating-pandas-to-version-0-19-in-azure-ml-studio\/46232963#46232963\">Updating pandas to version 0.19 in Azure ML Studio<\/a>, it even introduced how to update the version of Python packages installed by Azure.<\/p>\n\n<p>Hope it helps.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2017-10-03 07:03:59.333 UTC",
        "Answer_score":2.0,
        "Owner_location":"Warszawa, Polska",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":61480421,
        "Question_title":"Azure ML Designer - Unable to connect dataset (any directory type) to clean missing data module (dataframedirectory type) in designer",
        "Question_body":"<p>Unable to connect dataset (any directory type) to clean missing data module (dataframedirectory type) in designer. Please advise. Screenshot of trying to connect is below where the clean missing module connection point is not highlighted to connect both modules, which i am assuming is because of type mismatch. Dataset output is of type \"AnyDirectory\" type where as clean missing module type is \"dataframedirectory\". How to cast in designer?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Qs3uV.png\" rel=\"nofollow noreferrer\">https:\/\/i.stack.imgur.com\/Qs3uV.png<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-28 12:41:31.757 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":241,
        "Owner_creation_date":"2014-04-15 12:18:57.88 UTC",
        "Owner_last_access_date":"2021-05-11 16:14:42.53 UTC",
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":64394661,
        "Question_title":"Erro InvalidInputDatatype: Input of type 'Unknown' is not supported in azure (azureml.train.automl)",
        "Question_body":"<p>I have a pandas's DataFrame created by:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>TB_HISTORICO_MODELO = pd.read_sql(&quot;&quot;&quot;select DAT_INICIO_SEMANA_PLAN\n,COD_NEGOCIO\n,VENDA\n,LUCRO\n,MODULADO\n,RUPTURA\n,QTD_ESTOQUE_MEDIO\n,PECAS from TB&quot;&quot;&quot;, cursor)\n\nTB_HISTORICO_MODELO[&quot;DAT_INICIO_SEMANA_PLAN&quot;] = pd.to_datetime(TB_HISTORICO_MODELO[&quot;DAT_INICIO_SEMANA_PLAN&quot;])\n\ndataset = TB_HISTORICO_MODELO[TB_HISTORICO_MODELO['COD_NEGOCIO']=='A101'].drop(columns=['COD_NEGOCIO']) .reset_index(drop=True)\n<\/code><\/pre>\n<p>Everything look like right.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; dataset.dtypes\nDAT_INICIO_SEMANA_PLAN    datetime64[ns]\nVENDA                            float64\nLUCRO                            float64\nMODULADO                           int64\nRUPTURA                            int64\nQTD_ESTOQUE_MEDIO                  int64\nPECAS                            float64\ndtype: object\n<\/code><\/pre>\n<p>But when I rum this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>#%% Create the AutoML Config file and run the experiment on Azure\n\nfrom azureml.train.automl import AutoMLConfig\n\ntime_series_settings = {\n   'time_column_name': 'DAT_INICIO_SEMANA_PLAN',\n   'max_horizon': 14,\n   'country_or_region': 'BR',\n   'target_lags': 'auto'\n}\n\nautoml_config = AutoMLConfig(task='forecasting',\n                            primary_metric='normalized_root_mean_squared_error',\n                            blocked_models=['ExtremeRandomTrees'],\n                            experiment_timeout_minutes=30,\n                            training_data=dataset,\n                            label_column_name='VENDA',\n                            compute_target = compute_cluster,\n                            enable_early_stopping=True,\n                            n_cross_validations=3,\n                            # max_concurrent_iterations=4,\n                            # max_cores_per_iteration=-1,\n                            verbosity=logging.INFO,\n                            **time_series_settings)\n\nremote_run = Experimento.submit(automl_config, show_output=True)\n<\/code><\/pre>\n<p>I get the message<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; remote_run = Experimento.submit(automl_config, show_output=True)\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\n  File &quot;\/home\/fnord\/venv\/lib64\/python3.6\/site-packages\/azureml\/core\/experiment.py&quot;, line 219, in submit\n    run = submit_func(config, self.workspace, self.name, **kwargs)\n  File &quot;\/home\/fnord\/venv\/lib64\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py&quot;, line 92, in _automl_static_submit\n    automl_config_object._validate_config_settings(workspace)\n  File &quot;\/home\/fnord\/venv\/lib64\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py&quot;, line 1775, in _validate_config_settings\n    supported_types=&quot;, &quot;.join(SupportedInputDatatypes.REMOTE_RUN_SCENARIO)\nazureml.train.automl.exceptions.ConfigException: ConfigException:\n        Message: Input of type 'Unknown' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset, azureml.pipeline.core.pipeline_output_dataset.PipelineOutputTabularDataset]\n        InnerException: None\n        ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;Input of type 'Unknown' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset, azureml.pipeline.core.pipeline_output_dataset.PipelineOutputTabularDataset]&quot;,\n        &quot;details_uri&quot;: &quot;https:\/\/aka.ms\/AutoMLConfig&quot;,\n        &quot;target&quot;: &quot;training_data&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;BadArgument&quot;,\n            &quot;inner_error&quot;: {\n                &quot;code&quot;: &quot;ArgumentInvalid&quot;,\n                &quot;inner_error&quot;: {\n                    &quot;code&quot;: &quot;InvalidInputDatatype&quot;\n                }\n            }\n        }\n    }\n}\n\n<\/code><\/pre>\n<p>Where is wrong?<\/p>\n<p>documentation:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train<\/a>\n<a href=\"https:\/\/docs.microsoft.com\/pt-br\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/pt-br\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-16 18:36:27.197 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|pandas|azure|azure-machine-learning-service",
        "Question_view_count":382,
        "Owner_creation_date":"2018-04-02 02:01:36.793 UTC",
        "Owner_last_access_date":"2022-09-23 14:36:15.803 UTC",
        "Owner_reputation":264,
        "Owner_up_votes":276,
        "Owner_down_votes":2,
        "Owner_views":23,
        "Answer_body":"<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train#data-source-and-format?WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">Configure AutoML Doc<\/a> says:<\/p>\n<blockquote>\n<p>For remote experiments, training data must be accessible from the remote compute. AutoML only accepts Azure Machine Learning TabularDatasets when working on a remote compute.<\/p>\n<\/blockquote>\n<p>It looks as if your <code>dataset<\/code> object is a Pandas DataFrame, when it should really be an Azure ML <code>Dataset<\/code>. Check out <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-datasets?WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">this doc<\/a> on creating Datasets.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-10-18 07:07:46.533 UTC",
        "Answer_score":3.0,
        "Owner_location":"Rio de Janeiro, RJ, Brasil",
        "Answer_last_edit_date":"2020-10-18 07:14:19.15 UTC",
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":60523435,
        "Question_title":"How do I version control Azure ML workspaces with custom environments and pipelines?",
        "Question_body":"<p>I'm trying to figure out how viable Azure ML in production; I would like to accomplish the following:<\/p>\n\n<ol>\n<li>Specify <em>custom environments<\/em> for my pipelines using a <em>pip file<\/em> and use them in a pipeline<\/li>\n<li><em>Declaratively<\/em> specify my workspace, environments and pipelines in an <em>Azure DevOps repo<\/em><\/li>\n<li><em>Reproducibly<\/em> deploy my Azure ML workspace to my subscription using an <em>Azure DevOps pipeline<\/em><\/li>\n<\/ol>\n\n<p>I found an <a href=\"https:\/\/stackoverflow.com\/questions\/60506398\/how-do-i-use-an-environment-in-an-ml-azure-pipeline\">explanation of how to specify environments using notebooks<\/a> but this seems ill-suited for the second and third requirements I have.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-04 10:00:22.363 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-03-04 18:58:12.107 UTC",
        "Question_score":1,
        "Question_tags":"azure|azure-devops|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":659,
        "Owner_creation_date":"2011-09-30 16:07:01.727 UTC",
        "Owner_last_access_date":"2022-08-14 20:09:24.847 UTC",
        "Owner_reputation":1263,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":44,
        "Answer_body":"<p>Currently, we have a python script, <code>pipeline.py<\/code> that uses the <code>azureml-sdk<\/code>to create, register and run all of our ML artifacts (envs, pipelines, models). We call this script in our Azure DevOps CI pipeline with a Python Script task after building the right pip env from the requirements file in our repo.<\/p>\n\n<p>However, it is worth noting there is YAML support for ML artifact definition. Though I don't know if the existing support will cover all of your bases (though that is the plan).<\/p>\n\n<p>Here's some great docs from MSFT to get you started:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/microsoft\/MLOpsPython\" rel=\"nofollow noreferrer\">GitHub Template repo of an end-to-end example of ML pipeline + deployment<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments\" rel=\"nofollow noreferrer\">How to define\/create an environment (using Pip or Conda) and use it in a remote compute context<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/targets\/azure-machine-learning?context=azure%2Fmachine-learning%2Fservice%2Fcontext%2Fml-context&amp;view=azure-devops&amp;tabs=yaml\" rel=\"nofollow noreferrer\">Azure Pipelines guidance on CI\/CD for ML Service<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-pipeline-yaml\" rel=\"nofollow noreferrer\">Defining ML pipelines in YAML<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-03-04 16:49:44.893 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2020-03-04 16:53:44.233 UTC",
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":69042524,
        "Question_title":"SageMaker Estimator use_spot_instances causes Invalid MaxWaitTimeInSeconds",
        "Question_body":"<p><a href=\"https:\/\/aws.amazon.com\/blogs\/aws\/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs\/\" rel=\"nofollow noreferrer\">Managed Spot Training: Save Up to 90% On Your Amazon SageMaker Training Jobs<\/a> says:<\/p>\n<blockquote>\n<p>Setting it up is extremely simple, as it should be when working with a fully-managed service:<\/p>\n<ul>\n<li>If you\u2019re using the console, just switch the feature on.<\/li>\n<li>If you\u2019re working with the Amazon SageMaker SDK, <strong>just set the train_use_spot_instances to true<\/strong> in the Estimator constructor.<\/li>\n<\/ul>\n<\/blockquote>\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html#sagemaker.estimator.Estimator\" rel=\"nofollow noreferrer\">SageMaker SDK sagemaker.estimator.Estimator<\/a> says:<\/p>\n<blockquote>\n<ul>\n<li>use_spot_instances (bool) \u2013\nSpecifies whether to use SageMaker Managed Spot instances for training. If enabled then the max_wait arg should also be set.<\/li>\n<li>max_wait (int) \u2013\nTimeout in seconds waiting for spot training instances (default: None). After this amount of time Amazon SageMaker will stop waiting for Spot instances to become available (default: None).<\/li>\n<\/ul>\n<\/blockquote>\n<p>As per the documentations, run below.<\/p>\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\n\nestimator = TensorFlow(\n    entry_point=&quot;fashion_mnist_training.py&quot;,\n    source_dir=&quot;src&quot;,\n    metric_definitions=metric_definitions,\n    hyperparameters=hyperparameters,\n    role=role,\n    input_mode='File',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    use_spot_instances=True,\n    max_wait= 23 * 60 * 60, \n    base_job_name=base_job_name,\n    checkpoint_s3_uri=checkpoint_s3_uri,\n    model_dir=False  # To avoid duplicate 'model_dir' command line argument\n)\n<\/code><\/pre>\n<p>However, error is caused.<\/p>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: Invalid MaxWaitTimeInSeconds. It must be present and be greater than or equal to MaxRuntimeInSeconds\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-03 09:21:17.28 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|spot-instances",
        "Question_view_count":269,
        "Owner_creation_date":"2014-11-22 09:22:35.47 UTC",
        "Owner_last_access_date":"2022-09-24 22:13:03.237 UTC",
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":65099376,
        "Question_title":"Segmentation fault error in importing sentence_transformers in Azure Machine Learning Service Nvidia Compute",
        "Question_body":"<p>I would like to use sentence_transformers in AML to run XLM-Roberta model for sentence embedding. I have a script in which I import sentence_transformers:<\/p>\n<pre><code>from sentence_transformers import SentenceTransformer\n<\/code><\/pre>\n<p>Once I run my AML pipeline, the run fails on this script with the following error:<\/p>\n<pre><code>AzureMLCompute job failed.\nUserProcessKilledBySystemSignal: Job failed since the user script received system termination signal usually due to out-of-memory or segfault.\n    Cause: segmentation fault\n    TaskIndex: \n    NodeIp: #####\n    NodeId: #####\n<\/code><\/pre>\n<p>I'm pretty sure that this import is causing this error, because if I comment out this import, the rest of the script will run.\nThis is weird because the installation of the sentence_transformers succeed.<\/p>\n<p>This is the details of my compute:<\/p>\n<pre><code>Virtual machine size\nSTANDARD_NV24 (24 Cores, 224 GB RAM, 1440 GB Disk)\nProcessing Unit\nGPU - 4 x NVIDIA Tesla M60\n<\/code><\/pre>\n<p>Agent Pool:<\/p>\n<pre><code>Azure Pipelines\n<\/code><\/pre>\n<p>Agent Specification:<\/p>\n<pre><code>ubuntu-16.04\n<\/code><\/pre>\n<p>requirements.txt file:<\/p>\n<pre><code>torch==1.4.0\nsentence-transformers\n<\/code><\/pre>\n<p>Does anyone have a solution for this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-12-01 22:17:00.137 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-12-01 23:52:47.497 UTC",
        "Question_score":1,
        "Question_tags":"azure|nvidia|azure-machine-learning-service|roberta-language-model|sentence-transformers",
        "Question_view_count":530,
        "Owner_creation_date":"2015-12-23 16:48:13.15 UTC",
        "Owner_last_access_date":"2022-09-17 08:32:34.823 UTC",
        "Owner_reputation":398,
        "Owner_up_votes":21,
        "Owner_down_votes":1,
        "Owner_views":28,
        "Answer_body":"<p>I fixed the issue by changing the pytorch version from 1.4.0 to 1.6.0.\nSo the requirements.txt looks like this:<\/p>\n<pre><code>torch==1.6.0\nsentence-transformers\n<\/code><\/pre>\n<p>At first I tried one of the older versions of sentence-transformers which was compatible with pytorch 1.4.0. But the older version doesn't support &quot;xml-roberta-base&quot; model, so I tried to upgrade the pytorch version.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-12-01 23:48:58.21 UTC",
        "Answer_score":2.0,
        "Owner_location":"Finland",
        "Answer_last_edit_date":"2020-12-01 23:57:49.223 UTC",
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":30999879,
        "Question_title":"git garbage-size out of control, need understanding",
        "Question_body":"<p>we are using git as our DVCS for a very large project (yes, I know git it's not always pointed at as the best for these situations), and there's something I don't quite understand about my repo. <\/p>\n\n<p>This is my count-objects output:<\/p>\n\n<pre><code>count: 53\nsize: 1.57 MiB\nin-pack: 26444\npacks: 2\nsize-pack: 42.49 GiB\nprune-packable: 0\ngarbage: 8\nsize-garbage: 32.22 GiB\n<\/code><\/pre>\n\n<p>as you can see the size is less than 2Mb, the size-pack is 43Gb (what is this, exactly?), but the size-garbage is 32Gb! What is that? Can I remove it? How?<\/p>\n\n<p>I tried many options found on the internet with very poor understanding of what they do on a separate repository with basically no gains or major changes. Like:<\/p>\n\n<pre><code>git reflog expire --all --expire=now\ngit gc --prune=now --aggressive\ngit gc\ngit repack -a -d --depth=250 --window=250\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2015-06-23 10:09:45.727 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2015-06-23 10:23:45.83 UTC",
        "Question_score":0,
        "Question_tags":"git|garbage-collection|dvcs|unreal-engine4",
        "Question_view_count":1792,
        "Owner_creation_date":"2012-06-11 13:42:34.28 UTC",
        "Owner_last_access_date":"2020-10-04 13:16:52.683 UTC",
        "Owner_reputation":129,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":43,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Torino",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":66471547,
        "Question_title":"Using \"cv_splits_indices\" in AutoMLConfig (azureml)",
        "Question_body":"<p>When training an regression model with AutoMLConfig with n_cross_validations being a normal int, I'm facing no problems.<\/p>\n<p>Now I want to use TimeSeriesSplit as the cross validation method for training a model with AutoMLConfig. For this there is a &quot;cv_splits_indices&quot; argument where I put in a list of lists of indicis like the following when n_splits=5 in TimeSeriesSplit :<\/p>\n<pre><code>array([[array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n        array([11, 12, 13, 14])],\n       [array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),\n        array([15, 16, 17, 18])],\n       [array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18]),\n        array([19, 20, 21, 22])],\n       [array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22]),\n        array([23, 24, 25, 26])],\n       [array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26]),\n        array([27, 28, 29, 30])]], dtype=object)\n<\/code><\/pre>\n<p>Unfortunately when running the following cell:<\/p>\n<pre><code>automl_settings = {\n    &quot;iteration_timeout_minutes&quot;: 15,\n    &quot;experiment_timeout_hours&quot;: 0.3,\n    &quot;max_cores_per_iteration&quot; : -1,\n    &quot;enable_early_stopping&quot;: True,\n    &quot;primary_metric&quot;: 'normalized_root_mean_squared_error',\n    &quot;featurization&quot;: 'auto',\n    &quot;verbosity&quot;: logging.INFO,\n    &quot;cv_splits_indices&quot;: idxs\n}\n\nautoml_config = AutoMLConfig(task='regression',\n                             debug_log=f'automated_ml_errors_.log',\n                             training_data=train,\n                             validation_data=train,\n                             label_column_name=y_var,\n                             **automl_settings)\n<\/code><\/pre>\n<p>I receive the following error:<\/p>\n<pre><code>ConfigException: ConfigException:\n Message: cv_splits_indices should be a List of List[numpy.ndarray]. Each List[numpy.ndarray] corresponds to a CV fold and should have just 2 elements: The indices for training set and for the validation set.\n InnerException: None\n ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;cv_splits_indices should be a List of List[numpy.ndarray]. Each List[numpy.ndarray] corresponds to a CV fold and should have just 2 elements: The indices for training set and for the validation set.&quot;,\n        &quot;details_uri&quot;: &quot;https:\/\/aka.ms\/AutoMLConfig&quot;,\n        &quot;target&quot;: &quot;cv_splits_indices&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;BadArgument&quot;,\n            &quot;inner_error&quot;: {\n                &quot;code&quot;: &quot;ArgumentInvalid&quot;\n            }\n        },\n        &quot;reference_code&quot;: &quot;XXXXXXREDACTEDXXXX&quot;\n    }\n}\n<\/code><\/pre>\n<p>What is going wrong here? Does my input look correct?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-04 08:50:05.423 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-03-04 11:39:22.623 UTC",
        "Question_score":1,
        "Question_tags":"azure|scikit-learn|azure-machine-learning-studio|azureml-python-sdk",
        "Question_view_count":37,
        "Owner_creation_date":"2017-10-01 09:44:48.053 UTC",
        "Owner_last_access_date":"2022-09-24 22:10:05.09 UTC",
        "Owner_reputation":175,
        "Owner_up_votes":36,
        "Owner_down_votes":13,
        "Owner_views":56,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Vlaardingen, Netherlands",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":58193491,
        "Question_title":"How can I add a new Vertex only if it is possible to add an Edge too?",
        "Question_body":"<p>I need to add a new Vertex with an Edge. \nMy code is something like:<\/p>\n\n<pre><code>g.V().addV(\"Vertex\").addE(\"MyEdge\").to(V().has(\"OtherVertex\", \"name\", \"test\"))\n<\/code><\/pre>\n\n<p>If <code>V().has(\"OtherVertex\", \"name\", \"test\")<\/code> return a Vertex, everything works fine. My problem is if the <code>OtherVertex<\/code> doesn't exist, Gremlin add the new Vertex without edges. I would like to add the new Vertex only if I can create the Edge.\nI am using Gremlin-server for developing. My guess is, I could try to use <code>Transactions<\/code>, but I am not sure if AWS Neptune support it now.<\/p>\n\n<p>Any suggestion?\nThanks.<\/p>\n\n<hr>\n\n<p>I think, avoiding transactions, I realize that I can select <code>OtherVertex<\/code> first. If it doesn't exist, the query will not create a new Vertex:<\/p>\n\n<pre><code>g.V().has(\"OtherVertex\", \"name\", \"test\").as('t').addV(\"Vertex\").as('v').addE(\"MyEdge\").from('v').to('t')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-01 23:29:43.367 UTC",
        "Question_favorite_count":0.0,
        "Question_last_edit_date":"2019-10-03 17:29:32.957 UTC",
        "Question_score":0,
        "Question_tags":"tinkerpop3|amazon-neptune|tinkergraph",
        "Question_view_count":188,
        "Owner_creation_date":"2012-01-16 15:20:07.14 UTC",
        "Owner_last_access_date":"2022-09-23 18:06:17.137 UTC",
        "Owner_reputation":1829,
        "Owner_up_votes":21,
        "Owner_down_votes":1,
        "Owner_views":71,
        "Answer_body":"<p>As you wrote, this is the correct approach:<\/p>\n\n<pre><code>g.V().has(\"OtherVertex\", \"name\", \"test\").as('t').\n  addV(\"Vertex\").as('v').addE(\"MyEdge\").from('v').to('t')\n<\/code><\/pre>\n\n<p>I would just add something in relation to your initial attempt that showed:<\/p>\n\n<pre><code>g.V().addV(\"Vertex\")\n<\/code><\/pre>\n\n<p>I think you simply meant to start with:<\/p>\n\n<pre><code>g.addV(\"Vertex\")\n<\/code><\/pre>\n\n<p>If you go with the former you create some unintended problems:<\/p>\n\n<pre><code>gremlin&gt; g = TinkerGraph.open().traversal()\n==&gt;graphtraversalsource[tinkergraph[vertices:0 edges:0], standard]\ngremlin&gt; g.V().addV('person')\ngremlin&gt; g.V().addV('person')\ngremlin&gt; g\n==&gt;graphtraversalsource[tinkergraph[vertices:0 edges:0], standard]\n<\/code><\/pre>\n\n<p>Note that nothing gets added for an empty graph. That is because <code>V()<\/code> returns no vertices and therefore there is nothing in the pipeline to trigger <code>addV()<\/code> with. Let's assume you have some data though:<\/p>\n\n<pre><code>gremlin&gt; g = TinkerFactory.createModern().traversal()\n==&gt;graphtraversalsource[tinkergraph[vertices:6 edges:6], standard]\ngremlin&gt; g.V().addV('person')\n==&gt;v[13]\n==&gt;v[14]\n==&gt;v[15]\n==&gt;v[16]\n==&gt;v[17]\n==&gt;v[18]\ngremlin&gt; g.V().addV('person')\n==&gt;v[19]\n==&gt;v[20]\n==&gt;v[21]\n==&gt;v[22]\n==&gt;v[23]\n==&gt;v[24]\n==&gt;v[25]\n==&gt;v[26]\n==&gt;v[27]\n==&gt;v[28]\n==&gt;v[29]\n==&gt;v[30]\n<\/code><\/pre>\n\n<p>Now, there's an even worse problem in that we're adding one vertex for every existing vertex as <code>V()<\/code> now returns all the vertices in the graph on each execution. <\/p>\n\n<p>As a final note, please see this <a href=\"http:\/\/tinkerpop.apache.org\/docs\/current\/recipes\/#element-existence\" rel=\"nofollow noreferrer\">Gremlin Recipe<\/a> for ways to do get-or-create type operations as it's related to this conditional sort of mutation that you're doing now.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-10-04 12:22:51.223 UTC",
        "Answer_score":1.0,
        "Owner_location":"Santiago, Chile",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":68678587,
        "Question_title":"AzureML experiment pipeline not using CUDA with PyTorch",
        "Question_body":"<p>I am running an experiment pipeline to train my model with PyTorch and CUDA.\nI created the environment as follow:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>    env = Environment.from_conda_specification(model, join(model, 'conda_dependencies.yml'))\n    env.docker.enabled = True\n    env.environment_variables = {'MODEL_NAME': model, 'BRANCH': branch, 'COMMIT': commit}\n    env.docker.base_image = DEFAULT_GPU_IMAGE\n\n    run_config = RunConfiguration()\n    run_config.environment = env\n    run_config.docker = DockerConfiguration(use_docker=True)\n<\/code><\/pre>\n<p>And here is the training step:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>train_step = PythonScriptStep(\n      name='Model Train',\n      source_directory=training_dir,\n      compute_target=cluster,\n      runconfig=run_config,\n      script_name='train_aml.py',\n      arguments=[\n        '--model', model,\n        '--model_output_dir', model_output_dir,\n      ],\n      inputs=[train_dataset.as_mount()],\n      outputs=[model, model_output_dir]\n    )\n<\/code><\/pre>\n<p>Even though I am using a <code>Standard_NC12_Promo<\/code> machine when I run my training script, the GPU is not picked up by PyTorch <code>device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)<\/code>.<br \/>\nIf I try running my script on the same machine but not in an experiment then the GPU is used.<br \/>\nDo you know any potential solutions to this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-06 08:34:00.413 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-08-06 15:12:32.71 UTC",
        "Question_score":1,
        "Question_tags":"azure|pytorch|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":219,
        "Owner_creation_date":"2015-07-04 10:48:10.427 UTC",
        "Owner_last_access_date":"2022-09-23 15:10:29.127 UTC",
        "Owner_reputation":959,
        "Owner_up_votes":136,
        "Owner_down_votes":31,
        "Owner_views":204,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Amsterdam, Paesi Bassi",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":62711259,
        "Question_title":"Customize metric visualization in MLFlow UI when using mlflow.tensorflow.autolog()",
        "Question_body":"<p>I'm trying to integrate MLFlow to my project. Because I'm using <code>tf.keras.fit_generator()<\/code> for my training so I take advantage of <code>mlflow.tensorflow.autolog()<\/code>(<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.tensorflow.html#mlflow.tensorflow.autolog\" rel=\"nofollow noreferrer\">docs<\/a> here) to enable automatic logging of metrics and parameters:<\/p>\n<pre><code>    model = Unet()\n    optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n\n    metrics = [IOUScore(threshold=0.5), FScore(threshold=0.5)]\n    model.compile(optimizer, customized_loss, metrics)\n\n    callbacks = [\n        tf.keras.callbacks.ModelCheckpoint(&quot;model.h5&quot;, save_weights_only=True, save_best_only=True, mode='min'),\n        tf.keras.callbacks.TensorBoard(log_dir='.\/logs', profile_batch=0, update_freq='batch'),\n    ]\n\n\n    train_dataset = Dataset(src_dir=SOURCE_DIR)\n\n    train_data_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n\n   \n    with mlflow.start_run():\n        mlflow.tensorflow.autolog()\n        mlflow.log_param(&quot;batch_size&quot;, BATCH_SIZE)\n\n        model.fit_generator(\n            train_data_loader,\n            steps_per_epoch=len(train_data_loader),\n            epochs=EPOCHS,\n            callbacks=callbacks   \n            )\n<\/code><\/pre>\n<p>I expected something like this (just a demonstration taken from the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#visualizing-metrics\" rel=\"nofollow noreferrer\">docs<\/a>):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/eG56Z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eG56Z.png\" alt=\"Visualization on the docs\" \/><\/a><\/p>\n<p>However, after the training finished, this is what I got:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fS1JD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fS1JD.png\" alt=\"f1_score visualization\" \/><\/a><\/p>\n<p>How can I configure so that the metric plot will update and display its value at each epoch instead of just showing the latest value?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-03 08:15:46.63 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-07-06 04:10:26.393 UTC",
        "Question_score":2,
        "Question_tags":"tf.keras|mlflow",
        "Question_view_count":1035,
        "Owner_creation_date":"2016-07-08 02:05:15.393 UTC",
        "Owner_last_access_date":"2022-09-25 05:18:34.54 UTC",
        "Owner_reputation":173,
        "Owner_up_votes":97,
        "Owner_down_votes":0,
        "Owner_views":28,
        "Answer_body":"<p>After searching around, I found <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/2390\" rel=\"nofollow noreferrer\">this issue<\/a> related to my problem above. Actually, all my metrics just logged once each training (instead of each epoch as my intuitive thought). The reason is I didn't specify the <code>every_n_iter<\/code> parameter in <code>mlflow.tensorflow.autolog()<\/code>, which indicates how many 'iterations' must pass before MLflow logs metric executed (see the <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tensorflow.html#mlflow.tensorflow.autolog\" rel=\"nofollow noreferrer\">docs<\/a>). So, changing my code to:<\/p>\n<p><code>mlflow.tensorflow.autolog(every_n_iter=1)<\/code><\/p>\n<p>fixed the problem.<\/p>\n<p>P\/s: Remember that in TF 2.x, an 'iteration' is an epoch (in TF 1.x it's a batch).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-06 04:08:45.28 UTC",
        "Answer_score":1.0,
        "Owner_location":"Danang, H\u1ea3i Ch\u00e2u District, Da Nang, Vietnam",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":73407651,
        "Question_title":"Where can i find some good azure mlops examples?",
        "Question_body":"<p>I am working on an azure mlops end to end implementation and I am looking for some recent frameworks that can help me on my journey. So far, I am seeing things that are 2 years old. The mlops examples have got to show the training, registering and inferencing components.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-08-18 18:09:53.837 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service|mlops|azureml-python-sdk|azuremlsdk|azure-ml-pipelines",
        "Question_view_count":55,
        "Owner_creation_date":"2022-08-18 18:06:40.81 UTC",
        "Owner_last_access_date":"2022-09-20 08:42:59.847 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":59150100,
        "Question_title":"Sagemaker and Tensorflow model not saved",
        "Question_body":"<p>I am learning Sagemaker and I have this entry point:<\/p>\n\n<pre><code>import os\nimport tensorflow as tf\nfrom tensorflow.python.estimator.model_fn import ModeKeys as Modes\n\nINPUT_TENSOR_NAME = 'inputs'\nSIGNATURE_NAME = 'predictions'\n\nLEARNING_RATE = 0.001\n\n\ndef model_fn(features, labels, mode, params):\n    # Input Layer\n    input_layer = tf.reshape(features[INPUT_TENSOR_NAME], [-1, 28, 28, 1])\n\n    # Convolutional Layer #1\n    conv1 = tf.layers.conv2d(\n        inputs=input_layer,\n        filters=32,\n        kernel_size=[5, 5],\n        padding='same',\n        activation=tf.nn.relu)\n\n    # Pooling Layer #1\n    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n    # Convolutional Layer #2 and Pooling Layer #2\n    conv2 = tf.layers.conv2d(\n        inputs=pool1,\n        filters=64,\n        kernel_size=[5, 5],\n        padding='same',\n        activation=tf.nn.relu)\n    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n    # Dense Layer\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n    dropout = tf.layers.dropout(\n        inputs=dense, rate=0.4, training=(mode == Modes.TRAIN))\n\n    # Logits Layer\n    logits = tf.layers.dense(inputs=dropout, units=10)\n\n    # Define operations\n    if mode in (Modes.PREDICT, Modes.EVAL):\n        predicted_indices = tf.argmax(input=logits, axis=1)\n        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\n\n    if mode in (Modes.TRAIN, Modes.EVAL):\n        global_step = tf.train.get_or_create_global_step()\n        label_indices = tf.cast(labels, tf.int32)\n        loss = tf.losses.softmax_cross_entropy(\n            onehot_labels=tf.one_hot(label_indices, depth=10), logits=logits)\n        tf.summary.scalar('OptimizeLoss', loss)\n\n    if mode == Modes.PREDICT:\n        predictions = {\n            'classes': predicted_indices,\n            'probabilities': probabilities\n        }\n        export_outputs = {\n            SIGNATURE_NAME: tf.estimator.export.PredictOutput(predictions)\n        }\n        return tf.estimator.EstimatorSpec(\n            mode, predictions=predictions, export_outputs=export_outputs)\n\n    if mode == Modes.TRAIN:\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n        train_op = optimizer.minimize(loss, global_step=global_step)\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n\n    if mode == Modes.EVAL:\n        eval_metric_ops = {\n            'accuracy': tf.metrics.accuracy(label_indices, predicted_indices)\n        }\n        return tf.estimator.EstimatorSpec(\n            mode, loss=loss, eval_metric_ops=eval_metric_ops)\n\n\ndef serving_input_fn(params):\n    inputs = {INPUT_TENSOR_NAME: tf.placeholder(tf.float32, [None, 784])}\n    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n\n\ndef read_and_decode(filename_queue):\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n\n    features = tf.parse_single_example(\n        serialized_example,\n        features={\n            'image_raw': tf.FixedLenFeature([], tf.string),\n            'label': tf.FixedLenFeature([], tf.int64),\n        })\n\n    image = tf.decode_raw(features['image_raw'], tf.uint8)\n    image.set_shape([784])\n    image = tf.cast(image, tf.float32) * (1. \/ 255)\n    label = tf.cast(features['label'], tf.int32)\n\n    return image, label\n\n\ndef train_input_fn(training_dir, params):\n    return _input_fn(training_dir, 'train.tfrecords', batch_size=100)\n\n\ndef eval_input_fn(training_dir, params):\n    return _input_fn(training_dir, 'test.tfrecords', batch_size=100)\n\n\ndef _input_fn(training_dir, training_filename, batch_size=100):\n    test_file = os.path.join(training_dir, training_filename)\n    filename_queue = tf.train.string_input_producer([test_file])\n\n    image, label = read_and_decode(filename_queue)\n    images, labels = tf.train.batch(\n        [image, label], batch_size=batch_size,\n        capacity=1000 + 3 * batch_size)\n\n    return {INPUT_TENSOR_NAME: images}, labels\n\ndef neo_preprocess(payload, content_type):\n    import logging\n    import numpy as np\n    import io\n\n    logging.info('Invoking user-defined pre-processing function')\n\n    if content_type != 'application\/x-image' and content_type != 'application\/vnd+python.numpy+binary':\n        raise RuntimeError('Content type must be application\/x-image or application\/vnd+python.numpy+binary')\n\n    f = io.BytesIO(payload)\n    image = np.load(f)*255\n\n    return image\n\n### NOTE: this function cannot use MXNet\ndef neo_postprocess(result):\n    import logging\n    import numpy as np\n    import json\n\n    logging.info('Invoking user-defined post-processing function')\n\n    # Softmax (assumes batch size 1)\n    result = np.squeeze(result)\n    result_exp = np.exp(result - np.max(result))\n    result = result_exp \/ np.sum(result_exp)\n\n    response_body = json.dumps(result.tolist())\n    content_type = 'application\/json'\n\n    return response_body, content_type\n<\/code><\/pre>\n\n<p>And I am training it <\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='cnn_fashion_mnist.py',\n                       role=role,\n                       input_mode='Pipe',\n                       training_steps=1, \n                       evaluation_steps=1,\n                       train_instance_count=1,\n                       output_path=output_path,\n                       train_instance_type='ml.c5.2xlarge',\n                       base_job_name='mnist')\n<\/code><\/pre>\n\n<p>so far it is trying correctly and it tells me that everything when well, but when I check the output there is nothing there or if I try to deploy it I get the error saying it couldn't find the model because there is nothing in the bucker, any ideas or extra configurations? Thank you<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-12-03 04:14:10.973 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":658,
        "Owner_creation_date":"2016-05-09 05:03:09.773 UTC",
        "Owner_last_access_date":"2021-07-16 12:54:52.163 UTC",
        "Owner_reputation":45,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":"<p>Looks like you are using one of the older Tensorflow versions.\nWe would recommend switching to a newer more straight-forward way of running Tensorflow in SageMaker (script mode) by switching to a more recent Tensorflow version.<\/p>\n\n<p>You can read more about it in our documentation:\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html<\/a><\/p>\n\n<p>Here is an example that might help:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/tensorflow_script_mode_training_and_serving.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/tensorflow_script_mode_training_and_serving.ipynb<\/a> <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-12-17 23:52:25.67 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":2980510,
        "Question_title":"Using Git or Mercurial, how would you know when you do a clone or a pull, no one is checking in files (pushing it)?",
        "Question_body":"<p>Using Git or Mercurial, how would you know when you do a clone or a pull, no one is checking in files (pushing it)?  It can be important that:<\/p>\n\n<p>1) You never know it is in an inconsistent state, so you try for 2 hours trying to debug the code when your code is in a inconsistent state.<\/p>\n\n<p>2) With all the framework code (such as Ruby on Rails) -- potentially hundreds of files -- if some files are inconsistent with the other, can't the <code>rake db:migrate<\/code> or <code>script\/generate controller<\/code> cause some damage or inconsistencies to the code base?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_date":"2010-06-05 13:26:11.373 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2010-08-16 20:20:18.453 UTC",
        "Question_score":6,
        "Question_tags":"git|frameworks|mercurial|dvcs",
        "Question_view_count":284,
        "Owner_creation_date":"2009-05-09 15:50:29.477 UTC",
        "Owner_last_access_date":"2022-09-18 06:42:24.453 UTC",
        "Owner_reputation":141372,
        "Owner_up_votes":1451,
        "Owner_down_votes":39,
        "Owner_views":12992,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":63440881,
        "Question_title":"Use SageMaker Pytorch image for training",
        "Question_body":"<p>I am trying to containerize the training process for a fine tuned BERT model and run it on SageMaker. I was planning to use the pre-built SageMaker Pytorch GPU containers (<a href=\"https:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images\/<\/a>) as my starting point but I am having issues pulling the images during my build process.<\/p>\n<p>My Dockerfile looks like this:<\/p>\n<pre><code># SageMaker PyTorch image\nFROM 763104351884.dkr.ecr.us-east-1.amazonaws.com\/pytorch-training:1.5.0-gpu-py36-cu101-ubuntu16.04\n\n\nENV PATH=&quot;\/opt\/ml\/code:${PATH}&quot;\n\n# \/opt\/ml and all subdirectories are utilized by SageMaker, we use the \/code subdirectory to store our user code.\nCOPY \/bert \/opt\/ml\/code\n\n# this environment variable is used by the SageMaker PyTorch container to determine our user code directory.\nENV SAGEMAKER_SUBMIT_DIRECTORY \/opt\/ml\/code\n\n# this environment variable is used by the SageMaker PyTorch container to determine our program entry point\n# for training and serving.\n# For more information: https:\/\/github.com\/aws\/sagemaker-pytorch-container\nENV SAGEMAKER_PROGRAM bert\/train\n<\/code><\/pre>\n<p>My build_and_push script:<\/p>\n<pre><code>#!\/usr\/bin\/env bash\n\n# This script shows how to build the Docker image and push it to ECR to be ready for use\n# by SageMaker.\n\n# The argument to this script is the image name. This will be used as the image on the local\n# machine and combined with the account and region to form the repository name for ECR.\nIMAGE=&quot;my-bert&quot;\n\n# parameters\nPY_VERSION=&quot;py36&quot;\n\n# Get the account number associated with the current IAM credentials\naccount=$(aws sts get-caller-identity --query Account --output text)\n\nif [ $? -ne 0 ]\nthen\n    exit 255\nfi\n\nchmod +x bert\/train\n\n# Get the region defined in the current configuration (default to us-west-2 if none defined)\nregion=$(aws configure get region)\nregion=${region:-us-east-2}\n\n# If the repository doesn't exist in ECR, create it.\naws ecr describe-repositories --repository-names ${IMAGE} || aws ecr create-repository --repository-name ${IMAGE}\n\necho &quot;---&gt; repository done..&quot;\n# Get the login command from ECR and execute it directly\naws ecr get-login-password --region $region | docker login --username AWS --password-stdin $account.dkr.ecr.$region.amazonaws.com\necho &quot;---&gt; logged in to account ecr..&quot;\n\n# Get the login command from ECR in order to pull down the SageMaker PyTorch image\n# aws ecr get-login-password --region $region | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com\n# echo &quot;---&gt; logged in to pytorch ecr..&quot;\n\necho &quot;Building image with arch=gpu, region=${region}&quot;\nTAG=&quot;gpu-${PY_VERSION}&quot;\nFULLNAME=&quot;${account}.dkr.ecr.${region}.amazonaws.com\/${IMAGE}:${TAG}&quot;\ndocker build -t ${IMAGE}:${TAG} --build-arg ARCH=&quot;$arch&quot; -f &quot;Dockerfile&quot; .\ndocker tag ${IMAGE}:${TAG} ${FULLNAME}\ndocker push ${FULLNAME}\n\n<\/code><\/pre>\n<p>I get the following message during the push and the sagemaker pytorch image is not pulled:<\/p>\n<pre><code>Get https:\/\/763104351884.dkr.ecr.us-east-1.amazonaws.com\/v2\/pytorch-training\/manifests\/1.5.0-gpu-py36-cu101-ubuntu16.04: no basic auth credentials\n<\/code><\/pre>\n<p>Please let me know if this is the correct way to use a pre-built SageMaker image and what I could do to fix this error.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-16 19:00:27.587 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|pytorch|amazon-sagemaker",
        "Question_view_count":922,
        "Owner_creation_date":"2014-10-11 23:58:16.727 UTC",
        "Owner_last_access_date":"2020-12-20 00:03:34.543 UTC",
        "Owner_reputation":205,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":34,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":71412319,
        "Question_title":"Register Trained Model in Azure Machine Learning",
        "Question_body":"<p>I'm training a Azure Machine learning model using script via python SDK. I'm able to see the environment creation and the model getting trained in std_log in output&amp;logs folder. After the Model training I try to dump the model, but I don't see the model in any folder.<\/p>\n<p>If possible I want to register the model directly into the Model section in Azure ML rather than dumping it in the pickle file.<\/p>\n<p>I'm using the following link for reference <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-sdk-train\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-sdk-train<\/a><\/p>\n<p>Below is the output log snapshot for the model training run<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/OdkyF.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OdkyF.jpg\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-09 16:02:13.913 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-03-09 16:30:02.513 UTC",
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio|azureportal|azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":156,
        "Owner_creation_date":"2021-08-15 08:29:14.667 UTC",
        "Owner_last_access_date":"2022-09-23 03:04:02.127 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Chennai, Tamil Nadu, India",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":73214887,
        "Question_title":"Gremlin `elementMap() step` returns less elements than actually present",
        "Question_body":"<p>I have an application with more than 3000 vertices having the same label , let's say <code>ABC<\/code>. It is required for my application to get the list of all the vertices and their properties for the user to choose the entity and interact with it. For that I am writing a <code>GetAllVertices<\/code> query for label <code>ABC<\/code>.<\/p>\n<p>The id's of the vertices are numbers\nEx: 1,2,3,..<\/p>\n<p>The following query returns the correct amount of vertices ~ 3000<\/p>\n<pre><code>g.V().hasLabel('ABC').dedup().count()\n<\/code><\/pre>\n<p>The following query however only returns around 1600 entries<\/p>\n<pre><code>g.V().hasLabel('ABC').elementMap()\n<\/code><\/pre>\n<p>I am trying to understand what is happening and how can I get the elementMap for all the vertices that I am interested in. I think it might be because of the hash function elementMap() might be using that is causing the collision of the keys and thus resulting in overwriting some of the keys with different entries.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2022-08-02 23:50:20.167 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-08-03 11:37:54.1 UTC",
        "Question_score":0,
        "Question_tags":"graph|gremlin|amazon-neptune",
        "Question_view_count":34,
        "Owner_creation_date":"2016-04-13 18:43:19.21 UTC",
        "Owner_last_access_date":"2022-09-23 23:13:20.557 UTC",
        "Owner_reputation":41,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":63403985,
        "Question_title":"\"Failure Exception: OSError: [Errno 30] Read-only file system\" when using AzureML in Python Azure Function",
        "Question_body":"<h2>Issue<\/h2>\n<p>I am trying prepare and then submit a new experiment to Azure Machine Learning from an Azure Function in Python. I therefore register a new dataset for my Azure ML workspace, which contains the training data for my ML model using <code>dataset.register(...<\/code>. However, when I try to create this dataset with the following line of code<\/p>\n<pre><code>dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)\n<\/code><\/pre>\n<p>then I get a <code>Failure Exception: OSError: [Errno 30] Read-only file system ...<\/code>.<\/p>\n<h2>Ideas<\/h2>\n<ol>\n<li>I know that I shouldn't write to the file system from within an Azure function if possible. But I actually don't want to write anything to the local file system. I only want to create the dataset as a reference to my blob storage under <code>datastore_path<\/code> and then register this to my Azure Machine Learning workspace. But it seems that the method <code>from_delimited_files<\/code> is trying to write to the file system anyway (maybe some caching?).<\/li>\n<li>I also know that there is a temp folder in which writing temporary files is permitted. However, I belive I cannot really control where this method is writing data. I already tried changing the current working directory to this temp folder just before the function call using <code>os.chdir(tempfile.gettempdir())<\/code>, but that didn't help.<\/li>\n<\/ol>\n<p>Any other ideas? I don't think I am doing something particularly unusually...<\/p>\n<h2>Details<\/h2>\n<p>I am using python 3.7 and azureml-sdk 1.9.0 and I can run the python script locally without problems. I currently deploy from VSCode using the Azure Functions extension version 0.23.0 (and an Azure DevOps pipeline for CI\/CD).<\/p>\n<p>Here is my full stack trace:<\/p>\n<pre><code>Microsoft.Azure.WebJobs.Host.FunctionInvocationException: Exception while executing function: Functions.HttpTrigger_Train\n ---&gt; Microsoft.Azure.WebJobs.Script.Workers.Rpc.RpcException: Result: Failure\nException: OSError: [Errno 30] Read-only file system: '\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/bin\/deps.lock'\nStack:   File &quot;\/azure-functions-host\/workers\/python\/3.7\/LINUX\/X64\/azure_functions_worker\/dispatcher.py&quot;, line 345, in _handle__invocation_request\n    self.__run_sync_func, invocation_id, fi.func, args)\n  File &quot;\/usr\/local\/lib\/python3.7\/concurrent\/futures\/thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;\/azure-functions-host\/workers\/python\/3.7\/LINUX\/X64\/azure_functions_worker\/dispatcher.py&quot;, line 480, in __run_sync_func\n    return func(**params)\n  File &quot;\/home\/site\/wwwroot\/HttpTrigger_Train\/__init__.py&quot;, line 11, in main\n    train()\n  File &quot;\/home\/site\/wwwroot\/shared_code\/train.py&quot;, line 70, in train\n    dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/data\/_loggerfactory.py&quot;, line 126, in wrapper\n    return func(*args, **kwargs)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/data\/dataset_factory.py&quot;, line 308, in from_delimited_files\n    quoting=support_multi_line)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/readers.py&quot;, line 100, in read_csv\n    df = Dataflow._path_to_get_files_block(path, archive_options)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/dataflow.py&quot;, line 2387, in _path_to_get_files_block\n    return datastore_to_dataflow(path)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py&quot;, line 41, in datastore_to_dataflow\n    datastore, datastore_value = get_datastore_value(source)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py&quot;, line 83, in get_datastore_value\n    _set_auth_type(workspace)\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py&quot;, line 134, in _set_auth_type\n    get_engine_api().set_aml_auth(SetAmlAuthMessageArgument(AuthType.SERVICEPRINCIPAL, json.dumps(auth)))\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py&quot;, line 18, in get_engine_api\n    _engine_api = EngineAPI()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py&quot;, line 55, in __init__\n    self._message_channel = launch_engine()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/engine.py&quot;, line 300, in launch_engine\n    dependencies_path = runtime.ensure_dependencies()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py&quot;, line 141, in ensure_dependencies\n    with _FileLock(deps_lock_path, raise_on_timeout=timeout_exception):\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py&quot;, line 113, in __enter__\n    self.acquire()\n  File &quot;\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py&quot;, line 72, in acquire\n    self.lockfile = os.open(self.lockfile_path, os.O_CREAT | os.O_EXCL | os.O_RDWR)\n\n   at Microsoft.Azure.WebJobs.Script.Description.WorkerFunctionInvoker.InvokeCore(Object[] parameters, FunctionInvocationContext context) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/Workers\/WorkerFunctionInvoker.cs:line 85\n   at Microsoft.Azure.WebJobs.Script.Description.FunctionInvokerBase.Invoke(Object[] parameters) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/FunctionInvokerBase.cs:line 85\n   at Microsoft.Azure.WebJobs.Script.Description.FunctionGenerator.Coerce[T](Task`1 src) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/FunctionGenerator.cs:line 225\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionInvoker`2.InvokeAsync(Object instance, Object[] arguments) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionInvoker.cs:line 52\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.InvokeAsync(IFunctionInvoker invoker, ParameterHelper parameterHelper, CancellationTokenSource timeoutTokenSource, CancellationTokenSource functionCancellationTokenSource, Boolean throwOnTimeout, TimeSpan timerInterval, IFunctionInstance instance) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 587\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithWatchersAsync(IFunctionInstanceEx instance, ParameterHelper parameterHelper, ILogger logger, CancellationTokenSource functionCancellationTokenSource) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 532\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, ParameterHelper parameterHelper, IFunctionOutputDefinition outputDefinition, ILogger logger, CancellationTokenSource functionCancellationTokenSource) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 470\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, FunctionStartedMessage message, FunctionInstanceLogEntry instanceLogEntry, ParameterHelper parameterHelper, ILogger logger, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 278\n   --- End of inner exception stack trace ---\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, FunctionStartedMessage message, FunctionInstanceLogEntry instanceLogEntry, ParameterHelper parameterHelper, ILogger logger, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 325\n   at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.TryExecuteAsyncCore(IFunctionInstanceEx functionInstance, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 117\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-13 22:20:36.433 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-02-15 16:41:46.92 UTC",
        "Question_score":2,
        "Question_tags":"python|azure-functions|readonly|azure-machine-learning-service|oserror",
        "Question_view_count":1092,
        "Owner_creation_date":"2020-08-13 19:15:32.177 UTC",
        "Owner_last_access_date":"2022-06-30 19:57:08.65 UTC",
        "Owner_reputation":61,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":"<p>The issue was an incompatible OS version in my virtual environment.<\/p>\n<p>A huge thanks goes to <a href=\"https:\/\/docs.microsoft.com\/answers\/users\/111253\/pramodvalavala-msft.html\" rel=\"nofollow noreferrer\">PramodValavala-MSFT<\/a> for his idea to create a docker container! Following his suggestion, I suddenly got the following error message for the  <code>dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)<\/code> command:<\/p>\n<blockquote>\n<p>Exception: NotImplementedError: Unsupported Linux distribution debian 10.<\/p>\n<\/blockquote>\n<p>which reminded me of the following warning in the azure machine learning documentation:<\/p>\n<blockquote>\n<p>Some dataset classes have dependencies on the azureml-dataprep\npackage, which is only compatible with 64-bit Python. For Linux users,\nthese classes are supported only on the following distributions: Red\nHat Enterprise Linux (7, 8), Ubuntu (14.04, 16.04, 18.04), Fedora (27,\n28), Debian (8, 9), and CentOS (7).<\/p>\n<\/blockquote>\n<p>Choosing the predefined docker image <code>2.0-python3.7<\/code> (running Debian 9) instead of  <code>3.0-python3.7<\/code> (running Debian 10) solved the issue (see <a href=\"https:\/\/hub.docker.com\/_\/microsoft-azure-functions-python\" rel=\"nofollow noreferrer\">https:\/\/hub.docker.com\/_\/microsoft-azure-functions-python<\/a>).<\/p>\n<p>I suspect that the default virtual environment, which I was using originally, also ran on an incompatible OS.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-08-16 22:27:27.4 UTC",
        "Answer_score":3.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":72994988,
        "Question_title":"How to mlflow-autolog a sklearn ConfusionMatrixDisplay?",
        "Question_body":"<p>I'm trying to log the plot of a <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_estimator\" rel=\"nofollow noreferrer\">confusion matrix generated with scikit-learn<\/a> for a <em>test<\/em> set using <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.sklearn.html\" rel=\"nofollow noreferrer\">mlflow's support for scikit-learn<\/a>.<\/p>\n<p>For this, I tried something that resemble the code below (I'm using mlflow hosted on Databricks, and <code>sklearn==1.0.1<\/code>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sklearn.datasets\nimport pandas as pd\nimport numpy as np\nimport mlflow\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nmlflow.set_tracking_uri(&quot;databricks&quot;)\nmlflow.set_experiment(&quot;\/Users\/name.surname\/plotcm&quot;)\n\ndata = sklearn.datasets.fetch_20newsgroups(categories=['alt.atheism', 'sci.space'])\n\ndf = pd.DataFrame(data = np.c_[data['data'], data['target']])\\\n       .rename({0:'text', 1:'class'}, axis = 'columns')\n\ntrain, test = train_test_split(df)\n\nmy_pipeline = Pipeline([\n    ('vectorizer', TfidfVectorizer()),\n    ('classifier', SGDClassifier(loss='modified_huber')),\n])\n\nmlflow.sklearn.autolog()\n\nfrom sklearn.metrics import ConfusionMatrixDisplay # should I import this after the call to `.autolog()`?\n\nmy_pipeline.fit(train['text'].values, train['class'].values)\n\ncm = ConfusionMatrixDisplay.from_predictions(\n      y_true=test[&quot;class&quot;], y_pred=my_pipeline.predict(test[&quot;text&quot;])\n  )\n<\/code><\/pre>\n<p>while the confusion matrix for the training set is saved in my mlflow run, no png file is created in the mlflow frontend for the <code>test<\/code> set.<\/p>\n<p>If I try to add<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>cm.figure_.savefig('test_confusion_matrix.png')\nmlflow.log_artifact('test_confusion_matrix.png')\n<\/code><\/pre>\n<p>that does the job, but requires explicitly logging the artifact.<\/p>\n<p>Is there an idiomatic\/proper way to autolog the confusion matrix computed using a test set after <code>my_pipeline.fit()<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-15 13:44:41.357 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-07-17 18:51:20.967 UTC",
        "Question_score":0,
        "Question_tags":"python|scikit-learn|confusion-matrix|mlflow",
        "Question_view_count":157,
        "Owner_creation_date":"2014-11-11 16:17:30.717 UTC",
        "Owner_last_access_date":"2022-09-24 20:31:18.173 UTC",
        "Owner_reputation":4811,
        "Owner_up_votes":376,
        "Owner_down_votes":73,
        "Owner_views":713,
        "Answer_body":"<p>The proper way to do this is to use <code>mlflow.log_figure<\/code> as a fluent API announced in <code>MLflow 1.13.0<\/code>. You can read the documentation <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_figure\" rel=\"nofollow noreferrer\">here<\/a>. This code will do the job.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.log_figure(cm.figure_, 'test_confusion_matrix.png')\n<\/code><\/pre>\n<p>This function implicitly store the image, and then calls <code>log_artifact<\/code> against that path, something like you did.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-20 08:15:34.1 UTC",
        "Answer_score":1.0,
        "Owner_location":"Verona, VR, Italy",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":53797757,
        "Question_title":"Amazon SageMaker kMeans won't take sparse matrix (csr_matrix) as input, any alternatives before using a dense matrix?",
        "Question_body":"<p>I want to apply sagemaker's kMeans algorithm to a sparse matrix, obtained with <code>TfidfVectorizer<\/code> from <code>sklearn<\/code>'s library.<\/p>\n\n<p>Ideally I would like to provide the input data to Sagemaker's kMeans implementation as sparse matrix <code>scipy.sparse.csr.csr_matrix<\/code>, but when I this (<code>kmeans.fit(kmeans.record_set(train_data))<\/code>) I get the following error:<\/p>\n\n<p><code>TypeError: must be real number, not csr_matrix<\/code><\/p>\n\n<p>Of course, if I pass a dense matrix the algorithm will work (<code>train_data.toarray()<\/code>) but the amount of memory it would need is ginormous. Any possible alternatives before I incur into using supersized amazon instances?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2018-12-15 22:22:12.777 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-12-17 17:24:26.16 UTC",
        "Question_score":0,
        "Question_tags":"sparse-matrix|k-means|amazon-sagemaker",
        "Question_view_count":315,
        "Owner_creation_date":"2011-04-14 16:43:27.107 UTC",
        "Owner_last_access_date":"2022-09-16 17:33:55.68 UTC",
        "Owner_reputation":2618,
        "Owner_up_votes":155,
        "Owner_down_votes":12,
        "Owner_views":337,
        "Answer_body":"<p>The key was in the SageMaker python SDK. There you can find a function that transforms a scipy sparse matrix to a sparse tensor (<code>write_spmatrix_to_sparse_tensor<\/code>).<\/p>\n\n<p>The complete code that solved the problem without having to incur into a dense matrix is the following:<\/p>\n\n<pre><code>from sagemaker.amazon.common import write_spmatrix_to_sparse_tensor\n\ntfidf_matrix = tfidf_vectorizer.fit_transform('your_train_data') # output: sparse scipy matrix\nsagemaker_bucket = 'your-bucket' \ndata_key = 'kmeans_lowlevel\/data'\ndata_location = f\"s3:\/\/{sagemaker_bucket}\/{data_key}\"\nbuf = io.BytesIO()\nwrite_spmatrix_to_sparse_tensor(buf, tfidf_matrix)\nbuf.seek(0)\nboto3.resource('s3').Bucket(sagemaker_bucket).Object(data_key).upload_fileobj(buf)\n<\/code><\/pre>\n\n<p>After doing this, in the <code>create_training_params<\/code> configuration you'll have to feed the S3Uri field with the data location you have provided to store the sparse matrix in S3:<\/p>\n\n<pre><code>create_training_params = \\\n{\n    ... # all other params\n\n    \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": data_location, # YOUR_DATA_LOCATION_GOES_HERE\n                    \"S3DataDistributionType\": \"FullyReplicated\"\n                }\n            },\n            \"CompressionType\": \"None\",\n            \"RecordWrapperType\": \"None\"\n        }\n    ]\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-12-19 15:02:37.47 UTC",
        "Answer_score":1.0,
        "Owner_location":"Menorca",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":1940103,
        "Question_title":"How suitable is a DVCS for the corporate environment?",
        "Question_body":"<p>I've been using SVN for some time now, and am pretty happy with how it works (but I can't say I'm an expert, and I haven't really done much with branches and merging). However an opportunity has arisen to put in some new practises on a new team and so I thought I'd take a look at DVCSs to see if it's worth making the jump.<\/p>\n\n<p>The company I work for is a pretty standard company where we all work in the same location (or sometimes at home) and we want to keep a central store of all code.<\/p>\n\n<p>My question is: if all you are doing with a DVCS is creating a central hub that everyone pushes their changes to, is there really any benefit to moving to a DVCS and its extra overheads in this sort of environment? <\/p>",
        "Question_answer_count":11,
        "Question_comment_count":0,
        "Question_creation_date":"2009-12-21 13:27:42.327 UTC",
        "Question_favorite_count":4.0,
        "Question_last_edit_date":"2010-03-29 04:36:12.517 UTC",
        "Question_score":9,
        "Question_tags":"version-control|dvcs",
        "Question_view_count":836,
        "Owner_creation_date":"2009-11-21 21:24:44.097 UTC",
        "Owner_last_access_date":"2022-09-13 13:34:55.14 UTC",
        "Owner_reputation":2189,
        "Owner_up_votes":4,
        "Owner_down_votes":2,
        "Owner_views":229,
        "Answer_body":"<p>With DVCS's people can maintain their own local branches without making any changes in the central repository, and push their changes to the master repository when they think it's cooked up. Our project is stored in an SVN repository but personally I use git-svn to manage my local changes and find it quite useful, because we are not allowed to submit all the changes immediately(they have to be approved by the integrator first).<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2009-12-21 13:37:05.467 UTC",
        "Answer_score":4.0,
        "Owner_location":"Huddersfield, United Kingdom",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":73734047,
        "Question_title":"Deploying a databricks model as a scoring webservice failed in Azure Machine Learning",
        "Question_body":"<p>I am doing an Azure Databricks lab 04. Integrating Azure Databricks and Azure Machine Learning -&gt; 2. Deploying Models in Azure Machine Learning. The idea is to 1. train a  model 2) deploy that model in an Azure Container Instance (ACI) in AML and 3) make predictions via HTTPS. However, I get an error when deploying the model.<\/p>\n<p>The full code from the notebook is displayed at the bottom or can be found here: <a href=\"https:\/\/adb-4934989010098757.17.azuredatabricks.net\/?o=4934989010098757#notebook\/4364513836468644\/command\/4364513836468645\" rel=\"nofollow noreferrer\">https:\/\/adb-4934989010098757.17.azuredatabricks.net\/?o=4934989010098757#notebook\/4364513836468644\/command\/4364513836468645<\/a> .<\/p>\n<p>I run the actual model deployment in the following way:<\/p>\n<pre><code>aci_service_name='nyc-taxi-service'\n\nservice = Model.deploy(workspace=ws,\n                       name=aci_service_name,\n                       models=[registered_model],\n                       inference_config=inference_config,\n                       deployment_config= aci_config, \n                       overwrite=True)\n\nservice.wait_for_deployment(show_output=True)\nprint(service.state)\n<\/code><\/pre>\n<p>After running the model deployment, the cell runs for over 25 minutes and breaks when checking the status of the inference endpoint. It gives the following error: &quot;<\/p>\n<pre><code>&quot;Service deployment polling reached non-successful terminal state, current service state: Failed \ncode&quot;: &quot;AciDeploymentFailed&quot;,\n  &quot;statusCode&quot;: 400,\n  &quot;message&quot;: &quot;Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\n\n<\/code><\/pre>\n<p>The scoring script looks like this:<\/p>\n<pre><code>script_dir = 'scripts'\ndbutils.fs.mkdirs(script_dir)\nscript_dir_path = os.path.join('\/dbfs', script_dir)\nprint(&quot;Script directory path:&quot;, script_dir_path)\n\n%%writefile $script_dir_path\/score.py\nimport json\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport joblib\nfrom azureml.core.model import Model\n\ncolumns = ['passengerCount', 'tripDistance', 'hour_of_day', 'day_of_week', \n           'month_num', 'normalizeHolidayName', 'isPaidTimeOff', 'snowDepth', \n           'precipTime', 'precipDepth', 'temperature']\n\ndef init():\n    global model\n    model_path = Model.get_model_path('nyc-taxi-fare')\n    model = joblib.load(model_path)\n    print('model loaded')\n\ndef run(input_json):\n    # Get predictions and explanations for each data point\n    inputs = json.loads(input_json)\n    data_df = pd.DataFrame(np.array(inputs).reshape(-1, len(columns)), columns = columns)\n    # Make prediction\n    predictions = model.predict(data_df)\n    # You can return any data type as long as it is JSON-serializable\n    return {'predictions': predictions.tolist()}\n\n<\/code><\/pre>\n<p>Does someone know how I could fix this potentially? Thanks in advance!<\/p>\n<p>The full code is displayed below:<\/p>\n<pre><code>\n**Required Libraries**: \n* `azureml-sdk[databricks]` via PyPI\n* `sklearn-pandas==2.1.0` via PyPI\n* `azureml-mlflow` via PyPI\nimport os\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport sklearn\nimport joblib\nimport math\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport azureml\nfrom azureml.core import Workspace, Experiment, Run\nfrom azureml.core.model import Model\n\nprint('The azureml.core version is {}'.format(azureml.core.VERSION))\n%md\n\n### Connect to the AML workspace\n%md\n\nIn the following cell, be sure to set the values for `subscription_id`, `resource_group`, and `workspace_name` as directed by the comments. Please note, you can copy the subscription ID and resource group name from the **Overview** page on the blade for the Azure ML workspace in the Azure portal.\n#Provide the Subscription ID of your existing Azure subscription\nsubscription_id = &quot; ..... &quot;\n\n#Replace the name below with the name of your resource group\nresource_group = &quot;RG_1&quot;\n\n#Replace the name below with the name of your Azure Machine Learning workspace\nworkspace_name = &quot;aml-ws&quot;\n\nprint(&quot;subscription_id:&quot;, subscription_id)\nprint(&quot;resource_group:&quot;, resource_group)\nprint(&quot;workspace_name:&quot;, workspace_name)\n%md\n\n**Important Note**: You will be prompted to login in the text that is output below the cell. Be sure to navigate to the URL displayed and enter the code that is provided. Once you have entered the code, return to this notebook and wait for the output to read `Workspace configuration succeeded`.\n\n*Also note that the sign-on link and code only appear the first time in a session. If an authenticated session is already established, you won't be prompted to enter the code and authenticate when creating an instance of the Workspace.*\nws = Workspace(subscription_id, resource_group, workspace_name)\nprint(ws)\nprint('Workspace region:', ws.location)\nprint('Workspace configuration succeeded')\n%md\n### Load the training data\n\nIn this notebook, we will be using a subset of NYC Taxi &amp; Limousine Commission - green taxi trip records available from [Azure Open Datasets]( https:\/\/azure.microsoft.com\/en-us\/services\/open-datasets\/). The data is enriched with holiday and weather data. Each row of the table represents a taxi ride that includes columns such as number of passengers, trip distance, datetime information, holiday and weather information, and the taxi fare for the trip.\n\nRun the following cell to load the table into a Spark dataframe and reivew the dataframe.\ndataset = spark.sql(&quot;select * from nyc_taxi_1_csv&quot;).toPandas()\ndisplay(dataset)\n%md \n\n### Use MLflow with Azure Machine Learning for Model Training\n\nIn the subsequent cells you will learn to do the following:\n- Set up MLflow tracking URI so as to use Azure ML\n- Create MLflow experiment \u2013 this will create a corresponding experiment in Azure ML Workspace\n- Train a model on Azure Databricks cluster while logging metrics and artifacts using MLflow\n- Save the trained model to Databricks File System (DBFS)\nimport mlflow\nmlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\nexperiment_name = 'MLflow-AML-Exercise'\nmlflow.set_experiment(experiment_name)\n\nprint(&quot;Training model...&quot;)\noutput_folder = 'outputs'\nmodel_file_name = 'nyc-taxi.pkl'\ndbutils.fs.mkdirs(output_folder)\nmodel_file_path = os.path.join('\/dbfs', output_folder, model_file_name)\n\nwith mlflow.start_run() as run:\n  df = dataset.dropna(subset=['totalAmount'])\n  x_df = df.drop(['totalAmount'], axis=1)\n  y_df = df['totalAmount']\n\n  X_train, X_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=0)\n\n  numerical = ['passengerCount', 'tripDistance', 'snowDepth', 'precipTime', 'precipDepth', 'temperature']\n  categorical = ['hour_of_day', 'day_of_week', 'month_num', 'normalizeHolidayName', 'isPaidTimeOff']\n\n  numeric_transformations = [([f], Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])) for f in numerical]\n    \n  categorical_transformations = [([f], OneHotEncoder(handle_unknown='ignore', sparse=False)) for f in categorical]\n\n  transformations = numeric_transformations + categorical_transformations\n\n  clf = Pipeline(steps=[('preprocessor', DataFrameMapper(transformations, df_out=True)), \n                        ('regressor', GradientBoostingRegressor())])\n\n  clf.fit(X_train, y_train)\n  \n  y_predict = clf.predict(X_test)\n  y_actual = y_test.values.flatten().tolist()\n  \n  rmse = math.sqrt(mean_squared_error(y_actual, y_predict))\n  mlflow.log_metric('rmse', rmse)\n  mae = mean_absolute_error(y_actual, y_predict)\n  mlflow.log_metric('mae', mae)\n  r2 = r2_score(y_actual, y_predict)\n  mlflow.log_metric('R2 score', r2)\n  \n  plt.figure(figsize=(10,10))\n  plt.scatter(y_actual, y_predict, c='crimson')\n  plt.yscale('log')\n  plt.xscale('log')\n\n  p1 = max(max(y_predict), max(y_actual))\n  p2 = min(min(y_predict), min(y_actual))\n  plt.plot([p1, p2], [p1, p2], 'b-')\n  plt.xlabel('True Values', fontsize=15)\n  plt.ylabel('Predictions', fontsize=15)\n  plt.axis('equal')\n  \n  results_graph = os.path.join('\/dbfs', output_folder, 'results.png')\n  plt.savefig(results_graph)\n  mlflow.log_artifact(results_graph)\n  \n  joblib.dump(clf, open(model_file_path,'wb'))\n  mlflow.log_artifact(model_file_path)\n%md \n\nRun the cell below to list the experiment run in Azure Machine Learning Workspace that you just completed.\naml_run = list(ws.experiments[experiment_name].get_runs())[0]\naml_run\n%md \n\n## Exercise 1: Register a databricks-trained model in AML\n\nAzure Machine Learning provides a Model Registry that acts like a version controlled repository for each of your trained models. To version a model, you use the SDK as follows. Run the following cell to register the model with Azure Machine Learning.\nmodel_name = 'nyc-taxi-fare'\nmodel_description = 'Model to predict taxi fares in NYC.'\nmodel_tags = {&quot;Type&quot;: &quot;GradientBoostingRegressor&quot;, \n              &quot;Run ID&quot;: aml_run.id, \n              &quot;Metrics&quot;: aml_run.get_metrics()}\n\nregistered_model = Model.register(model_path=model_file_path, #Path to the saved model file\n                                  model_name=model_name, \n                                  tags=model_tags, \n                                  description=model_description, \n                                  workspace=ws)\n\nprint(registered_model)\n%md\n\n## Exercise 2: Deploy a service that uses the model\n%md\n\n### Create the scoring script\nscript_dir = 'scripts'\ndbutils.fs.mkdirs(script_dir)\nscript_dir_path = os.path.join('\/dbfs', script_dir)\nprint(&quot;Script directory path:&quot;, script_dir_path)\n%%writefile $script_dir_path\/score.py\nimport json\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport joblib\nfrom azureml.core.model import Model\n\ncolumns = ['passengerCount', 'tripDistance', 'hour_of_day', 'day_of_week', \n           'month_num', 'normalizeHolidayName', 'isPaidTimeOff', 'snowDepth', \n           'precipTime', 'precipDepth', 'temperature']\n\ndef init():\n    global model\n    model_path = Model.get_model_path('nyc-taxi-fare')\n    model = joblib.load(model_path)\n    print('model loaded')\n\ndef run(input_json):\n    # Get predictions and explanations for each data point\n    inputs = json.loads(input_json)\n    data_df = pd.DataFrame(np.array(inputs).reshape(-1, len(columns)), columns = columns)\n    # Make prediction\n    predictions = model.predict(data_df)\n    # You can return any data type as long as it is JSON-serializable\n    return {'predictions': predictions.tolist()}\n%md\n\n### Create the deployment environment\nfrom azureml.core import Environment\nfrom azureml.core.environment import CondaDependencies\n\nmy_env_name=&quot;nyc-taxi-env&quot;\nmyenv = Environment.get(workspace=ws, name='AzureML-Minimal').clone(my_env_name)\nconda_dep = CondaDependencies()\nconda_dep.add_pip_package(&quot;numpy==1.18.1&quot;)\nconda_dep.add_pip_package(&quot;pandas==1.1.5&quot;)\nconda_dep.add_pip_package(&quot;joblib==0.14.1&quot;)\nconda_dep.add_pip_package(&quot;scikit-learn==0.24.1&quot;)\nconda_dep.add_pip_package(&quot;sklearn-pandas==2.1.0&quot;)\nconda_dep.add_pip_package(&quot;azure-ml-api-sdk&quot;)\n\nmyenv.python.conda_dependencies=conda_dep\n\nprint(&quot;Review the deployment environment.&quot;)\nmyenv\n%md\n\n### Create the inference configuration\nfrom azureml.core.model import InferenceConfig\ninference_config = InferenceConfig(entry_script='score.py', source_directory=script_dir_path, environment=myenv)\nprint(&quot;InferenceConfig created.&quot;)\n%md\n\n### Create the deployment configuration\n\nIn this exercise we will use the Azure Container Instance (ACI) to deploy the model\nfrom azureml.core.webservice import AciWebservice, Webservice\n\ndescription = 'NYC Taxi Fare Predictor Service'\n\naci_config = AciWebservice.deploy_configuration(\n                        cpu_cores=3, \n                        memory_gb=15, \n                        location='eastus', \n                        description=description, \n                        auth_enabled=True, \n                        tags = {'name': 'ACI container', \n                                'model_name': registered_model.name, \n                                'model_version': registered_model.version\n                                }\n                        )\n\nprint(&quot;AciWebservice deployment configuration created.&quot;)\n%md\n\n### Deploy the model as a scoring webservice\n\nPlease note that it can take **10-15 minutes** for the deployment to complete.\naci_service_name='nyc-taxi-service'\n\nservice = Model.deploy(workspace=ws,\n                       name=aci_service_name,\n                       models=[registered_model],\n                       inference_config=inference_config,\n                       deployment_config= aci_config, \n                       overwrite=True)\n\nservice.wait_for_deployment(show_output=True)\nprint(service.state)\n%md\n\n## Exercise 3: Consume the deployed service\n%md\n\n**Review the webservice endpoint URL and API key**\napi_key, _ = service.get_keys()\nprint(&quot;Deployed ACI test Webservice: {} \\nWebservice Uri: {} \\nWebservice API Key: {}&quot;.\n      format(service.name, service.scoring_uri, api_key))\n%md\n\n**Prepare test data**\n#['passengerCount', 'tripDistance', 'hour_of_day', 'day_of_week', 'month_num', \n# 'normalizeHolidayName', 'isPaidTimeOff', 'snowDepth', 'precipTime', 'precipDepth', 'temperature']\n\ndata1 = [2, 5, 9, 4, 5, 'Memorial Day', True, 0, 0.0, 0.0, 65]\ndata2 = [[3, 10, 15, 4, 7, 'None', False, 0, 2.0, 1.0, 80], \n         [2, 5, 9, 4, 5, 'Memorial Day', True, 0, 0.0, 0.0, 65]]\n\nprint(&quot;Test data prepared.&quot;)\ndataset.head()\n%md\n\n### Consume the deployed webservice over HTTP\nimport requests\nimport json\n\nheaders = {'Content-Type':'application\/json', 'Authorization':('Bearer '+ api_key)}\nresponse = requests.post(service.scoring_uri, json.dumps(data1), headers=headers)\nprint('Predictions for data1')\nprint(response.text)\nprint(&quot;&quot;)\nresponse = requests.post(service.scoring_uri, json.dumps(data2), headers=headers)\nprint('Predictions for data2')\nprint(response.text)\n%md\n\n### Clean-up\n\nWhen you are done with the exercise, delete the deployed webservice by running the cell below.\nservice.delete()\nprint(&quot;Deployed webservice deleted.&quot;)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-09-15 15:47:30.727 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-09-15 15:48:42.017 UTC",
        "Question_score":0,
        "Question_tags":"python|azure|deployment|databricks|azure-machine-learning-studio",
        "Question_view_count":45,
        "Owner_creation_date":"2021-06-11 13:17:50.14 UTC",
        "Owner_last_access_date":"2022-09-21 19:40:01.61 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":70657497,
        "Question_title":"GCP's Vertex AI - Restarting notebook <NAME>: <ZONE> does not have enough resources available to fulfill the request",
        "Question_body":"<p>I'm having issues starting \/ creating new user-managed notebooks in Vertex AI &gt; Workbench, where I'll end up with this error and can't even open JupyterLabs:<\/p>\n<blockquote>\n<p>[Vertex AI] - Restarting notebook NAME: ZONE does not have enough\nresources available to fulfill the request. Retry later or try another zone in your configurations.<\/p>\n<\/blockquote>\n<p>I didn't have this problem last month when I was still under the free trial. I also noticed this error only come up when I try to install the GPU in the notebook.<\/p>\n<p>So far, I've done the following:<\/p>\n<ul>\n<li>Change payment method (got an email that asked me to update payment settings at the end of the trial last month)<\/li>\n<li>Create new notebook in different zones and cluster over the past week, all give the same error when I attach a GPU.<\/li>\n<li>In Quotas, changed 'GPU (all regions)' from 1 to 0, and then back to 1.<\/li>\n<li>In Quotas, changed 'VM instances (default)' from 24 to 48, and then back to 24.<\/li>\n<li>Created several new projects, didn't work either.<\/li>\n<\/ul>\n<p>I also created a snapshot of both the data and boot of the notebook, but don't know how to use it to recreate it in Vertex AI again.<\/p>\n<p>Have you faced a similar issue before, how did you fix it? Thanks<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2,
        "Question_creation_date":"2022-01-10 18:49:48.547 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-01-12 12:41:32.413 UTC",
        "Question_score":0,
        "Question_tags":"google-cloud-platform|gcp-ai-platform-notebook|google-cloud-vertex-ai",
        "Question_view_count":480,
        "Owner_creation_date":"2021-06-29 03:57:22.35 UTC",
        "Owner_last_access_date":"2022-06-08 12:36:55.403 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":73761872,
        "Question_title":"Azure ML Studio Docker Endpoint Deployment Error 400 Client Error",
        "Question_body":"<p>I would like to deploy an ML endpoint (first locally to see the error messages) from custom environment. But I am getting the following error message. Can anybody clarify what does this stands for?<\/p>\n<p>The environment deployment is fine, I can pull and play with the environment image, the the ML model also works as it expected, but in the endpoint development I fail.<\/p>\n<pre><code>from azureml.core.model import InferenceConfig, Model\nfrom azureml.core.webservice import AciWebservice, LocalWebservice\n\nocr_model = Model(name='OCR_model', workspace=workspace)\nquality_model = Model(name='Quality_model', workspace=workspace)\ninference_config = InferenceConfig(entry_script=&quot;ml_scores.py&quot;, environment=python_env)\ndeployment_config = LocalWebservice.deploy_configuration(port=6789)\n\nservice = Model.deploy(workspace=workspace,\n                       name=&quot;ml-service-v2&quot;,\n                       overwrite=True,\n                       models=[ocr_model, quality_model],\n                       inference_config=inference_config,\n                       deployment_config=deployment_config)\n\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n<p>The error message:<\/p>\n<pre><code>Warning, custom base image or base dockerfile detected without a specified `inferencing_stack_version`. Please set environment.inferencing_stack_version='latest'\nWarning, custom base image or base dockerfile detected without a specified `inferencing_stack_version`. Please set environment.inferencing_stack_version='latest'\nDocker container start has failed:\n400 Client Error for http+docker:\/\/localhost\/v1.41\/containers\/f4ee0f109af60a21a11683dd3d69db87b6371b2c3b038ea0227a8cc8422c4200\/start: Bad Request (&quot;failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: &quot;runsvdir&quot;: executable file not found in $PATH: unknown&quot;)\n\nDownloading model OCR_model:42 to \/tmp\/azureml_25emn1le\/OCR_model\/42\nDownloading model Quality_model:40 to \/tmp\/azureml_25emn1le\/Quality_model\/40\nGenerating Docker build context.\nPackage creation Succeeded\nLogging into Docker registry 088e52be0d7942c4a6e11258fa37a140.azurecr.io\nLogging into Docker registry 088e52be0d7942c4a6e11258fa37a140.azurecr.io\nBuilding Docker image from Dockerfile...\nStep 1\/5 : FROM 088e52be0d7942c4a6e11258fa37a140.azurecr.io\/azureml\/azureml_152ffef9bfdf00715d63c8352ff2cc66\n ---&gt; 31b97aac1e18\nStep 2\/5 : COPY azureml-app \/var\/azureml-app\n ---&gt; a31c2d80deb2\nStep 3\/5 : RUN mkdir -p '\/var\/azureml-app' &amp;&amp; echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjIwMjQ0MjYwLWQyY2ItNDNkMy05NjZlLWQwZjcyMDE3ZWJhMCIsInJlc291cmNlR3JvdXBOYW1lIjoibWxfYmV0YV9yZyIsImFjY291bnROYW1lIjoibWFjaGluZS1sZWFybmluZy1iZXRhLXdzIiwid29ya3NwYWNlSWQiOiIwODhlNTJiZS0wZDc5LTQyYzQtYTZlMS0xMjU4ZmEzN2ExNDAifSwibW9kZWxzIjp7fSwibW9kZWxzSW5mbyI6e319 | base64 --decode &gt; \/var\/azureml-app\/model_config_map.json\n ---&gt; Running in 783f521ea037\n ---&gt; 8900434882b2\nStep 4\/5 : RUN mv '\/var\/azureml-app\/tmpdswbb3xk.py' \/var\/azureml-app\/main.py\n ---&gt; Running in 8b61d3f35a90\n ---&gt; 05890493cdd1\nStep 5\/5 : CMD [&quot;runsvdir&quot;,&quot;\/var\/runit&quot;]\n ---&gt; Running in 052ae96b5d75\n ---&gt; b15b8dfb9deb\nSuccessfully built b15b8dfb9deb\nSuccessfully tagged ml-service-v2:latest\nContainer (name:xenodochial_goldwasser, id:2757f295fa1836dd83d2dcb162ccbb566c04cba1f3fb0c35943f5d4d101d5479) cannot be killed.\nContainer has been successfully cleaned up.\nImage sha256:ed2b4e50c397597d857883a76e5d53a5c27ce700894d3b2f7167d477f7b82be2 successfully removed.\nStarting Docker container...\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/docker\/api\/client.py:268, in APIClient._raise_for_status(self, response)\n    267 try:\n--&gt; 268     response.raise_for_status()\n    269 except requests.exceptions.HTTPError as e:\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/requests\/models.py:1021, in Response.raise_for_status(self)\n   1020 if http_error_msg:\n-&gt; 1021     raise HTTPError(http_error_msg, response=self)\n\nHTTPError: 400 Client Error: Bad Request for url: http+docker:\/\/localhost\/v1.41\/containers\/f4ee0f109af60a21a11683dd3d69db87b6371b2c3b038ea0227a8cc8422c4200\/start\n\nDuring handling of the above exception, another exception occurred:\n\nAPIError                                  Traceback (most recent call last)\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/_model_management\/_util.py:495, in start_docker_container(container)\n    493 print(&quot;Starting Docker container...&quot;)\n--&gt; 495 container.start()\n    497 print('Docker container running.')\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/docker\/models\/containers.py:404, in Container.start(self, **kwargs)\n    396 &quot;&quot;&quot;\n    397 Start this container. Similar to the ``docker start`` command, but\n    398 doesn't support attach options.\n   (...)\n    402         If the server returns an error.\n    403 &quot;&quot;&quot;\n--&gt; 404 return self.client.api.start(self.id, **kwargs)\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/docker\/utils\/decorators.py:19, in check_resource.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapped(self, resource_id, *args, **kwargs)\n     16     raise errors.NullResource(\n     17         'Resource ID was not provided'\n     18     )\n---&gt; 19 return f(self, resource_id, *args, **kwargs)\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/docker\/api\/container.py:1109, in ContainerApiMixin.start(self, container, *args, **kwargs)\n   1108 res = self._post(url)\n-&gt; 1109 self._raise_for_status(res)\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/docker\/api\/client.py:270, in APIClient._raise_for_status(self, response)\n    269 except requests.exceptions.HTTPError as e:\n--&gt; 270     raise create_api_error_from_http_exception(e)\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/docker\/errors.py:31, in create_api_error_from_http_exception(e)\n     30         cls = NotFound\n---&gt; 31 raise cls(e, response=response, explanation=explanation)\n\nAPIError: 400 Client Error for http+docker:\/\/localhost\/v1.41\/containers\/f4ee0f109af60a21a11683dd3d69db87b6371b2c3b038ea0227a8cc8422c4200\/start: Bad Request (&quot;failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: &quot;runsvdir&quot;: executable file not found in $PATH: unknown&quot;)\n\nDuring handling of the above exception, another exception occurred:\n\nWebserviceException                       Traceback (most recent call last)\nInput In [20], in &lt;cell line: 12&gt;()\n      9 # deployment_config = AciWebservice.deploy_configuration(cpu_cores=2, memory_gb=4)\n     10 deployment_config = LocalWebservice.deploy_configuration(port=6789)\n---&gt; 12 service = Model.deploy(workspace=workspace,\n     13                        name=&quot;ml-service-v2&quot;,\n     14                        overwrite=True,\n     15                        models=[ocr_model, quality_model],\n     16                        inference_config=inference_config,\n     17                        deployment_config=deployment_config)\n     19 service.wait_for_deployment(show_output=True)\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/core\/model.py:1649, in Model.deploy(workspace, name, models, inference_config, deployment_config, deployment_target, overwrite, show_output)\n   1647 # Local webservice.\n   1648 if deployment_config and isinstance(deployment_config, LocalWebserviceDeploymentConfiguration):\n-&gt; 1649     return deployment_config._webservice_type._deploy(workspace, name, models,\n   1650                                                       inference_config=inference_config,\n   1651                                                       deployment_config=deployment_config)\n   1653 # IotWebservice does not support environment-style deployment,\n   1654 # so make sure we don't deploy IotWebservice with environment;\n   1655 # We only support ACI, AKS, AKS endpoint, and MIR for now.\n   1656 from azureml._model_management._constants import IOT_WEBSERVICE_TYPE\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/core\/webservice\/local.py:739, in LocalWebservice._deploy(workspace, name, models, image_config, deployment_config, wait, inference_config)\n    735     raise WebserviceException('Error, provided inference configuration must be of type InferenceConfig '\n    736                               'in order to deploy a local service.', logger=module_logger)\n    738 service = LocalWebservice(workspace, name, must_exist=False)\n--&gt; 739 service.update(models=models,\n    740                image_config=image_config,\n    741                inference_config=inference_config,\n    742                deployment_config=deployment_config,\n    743                wait=wait)\n    744 return service\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/core\/webservice\/local.py:72, in _in_state.&lt;locals&gt;.decorator.&lt;locals&gt;.decorated(self, *args, **kwargs)\n     69 if self.state not in states:\n     70     raise WebserviceException('Cannot call {}() when service is {}.'.format(func.__name__, self.state),\n     71                               logger=module_logger)\n---&gt; 72 return func(self, *args, **kwargs)\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/core\/webservice\/local.py:549, in LocalWebservice.update(self, models, image_config, deployment_config, wait, inference_config)\n    547 self._generate_docker_context()\n    548 self._build_image()\n--&gt; 549 self._run_container(wait=wait)\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/core\/webservice\/local.py:1021, in LocalWebservice._run_container(self, wait)\n   1018         LocalWebservice._copy_local_asset(container, self._inference_config.entry_script)\n   1020 # Engage!\n-&gt; 1021 start_docker_container(container)\n   1023 # Record the state of the deployment.\n   1024 self._container = container\n\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/_model_management\/_util.py:503, in start_docker_container(container)\n    500     raise WebserviceException('Docker container start has failed, the port you are attempting to use '\n    501                               'is already in use:\\n{}'.format(e), logger=module_logger)\n    502 else:\n--&gt; 503     raise WebserviceException('Docker container start has failed:\\n{}'.format(e), logger=module_logger)\n\nWebserviceException: WebserviceException:\n    Message: Docker container start has failed:\n400 Client Error for http+docker:\/\/localhost\/v1.41\/containers\/f4ee0f109af60a21a11683dd3d69db87b6371b2c3b038ea0227a8cc8422c4200\/start: Bad Request (&quot;failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: &quot;runsvdir&quot;: executable file not found in $PATH: unknown&quot;)\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;Docker container start has failed:\\n400 Client Error for http+docker:\/\/localhost\/v1.41\/containers\/f4ee0f109af60a21a11683dd3d69db87b6371b2c3b038ea0227a8cc8422c4200\/start: Bad Request (\\&quot;failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: \\&quot;runsvdir\\&quot;: executable file not found in $PATH: unknown\\&quot;)&quot;\n    }\n}\n<\/code><\/pre>\n<p>Thanks for any kind of guidance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-18 10:17:23.223 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|azure-machine-learning-service",
        "Question_view_count":53,
        "Owner_creation_date":"2022-09-18 10:09:28.96 UTC",
        "Owner_last_access_date":"2022-09-22 23:18:37.353 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":1178325,
        "Question_title":"DVCS and data loss?",
        "Question_body":"<p>After almost two years of using DVCS, it seems that one inherent \"flaw\" is accidental data loss: I have lost code which isn't pushed, and I know other people who have as well.<\/p>\n\n<p>I can see a few reasons for this: off-site data duplication (ie, \"commits have to go to a remote host\") is not built in, the repository lives in the same directory as the code and the notion of \"hack 'till you've got something to release\" is prevalent... But that's beside the point.<\/p>\n\n<p>I'm curious to know: have you experienced DVCS-related data loss? Or have you been using DVCS without trouble? And, related, apart from \"remember to push often\", is there anything which can be done to minimize the risk?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2009-07-24 15:13:10.697 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-09-06 15:04:24.09 UTC",
        "Question_score":5,
        "Question_tags":"version-control|dvcs",
        "Question_view_count":226,
        "Owner_creation_date":"2009-02-26 18:36:33.13 UTC",
        "Owner_last_access_date":"2022-09-24 15:39:24.27 UTC",
        "Owner_reputation":141929,
        "Owner_up_votes":2285,
        "Owner_down_votes":352,
        "Owner_views":8748,
        "Answer_body":"<p>I've lost more data from clobbering uncommitted changes in a centralized VCS, and then deciding that I actually wanted them, than from anything I've done with a DVCS.  Part of that is that I've been using CVS for almost a decade and git for under a year, so I've had a lot more opportunities to get into trouble with the centralized model, but differences in the properties of the workflow between the two models are also major contributing factors.<\/p>\n\n<p>Interestingly, most of the reasons for this boil down to \"BECAUSE it's easier to discard data, I'm more likely to keep it until I'm sure I don't want it\".  (The only difference between discarding data and losing it is that you meant to discard it.)  The biggest contributing factor is probably a quirk of my workflow habits - my \"working copy\" when I'm using a DVCS is often several different copies spread out over multiple computers, so corruption or loss in a single repo or even catastrophic data loss on the computer I've been working on is less likely to destroy the only copy of the data.  (Being able to do this is a big win of the distributed model over centralized ones - when every commit becomes a permanent part of the repository, the psychological barrier to copying tentative changes around is a lot higher.)<\/p>\n\n<p>As far as minimizing the risks, it's possible to develop habits that minimize them, but you have to develop those habits.  Two general principles there:<\/p>\n\n<ul>\n<li>Data doesn't exist until there are\nmultiple copies of it in different\nplaces.  There are workflow habits\nthat will give you multiple copies\nfor free - f'rexample, if you work\nin two different places, you'll have\na reason to push to a common location\nat the end of every work session,\neven if it's not ready to publish.<\/li>\n<li>Don't try to do anything clever,\nstupid, or beyond your comfort zone\nwith the only reference to a commit\nyou might want to keep.  Create a\ntemporary tag that you can revert to,\nor create a temporary branch to do\nthe operations on. (git's reflog lets\nyou recover old references after the\nfact; I'd be unsurprised if other\nDVCSs have similar functionality.\nSo manual tagging may not be\nnecessary, but it's often more\nconvenient anyways.)<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2009-07-25 02:04:28.6 UTC",
        "Answer_score":2.0,
        "Owner_location":"Montreal, QC, Canada",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":69859398,
        "Question_title":"Azure ML Studio Designer - Is it possible to copy pipeline or pipeline drafts from one workspace to another?",
        "Question_body":"<p>Is it possible to export or copy Pipelines created in Azure ML Studio <em>Designer<\/em> from one Workspace to another, using the UI, python sdk, and\/or azure CLI?  If so, how?<\/p>\n<p>EDIT:  My Designer does not appear to have the 'Export To Code' option that DeepDave-MT shows below.  How do I enable this ability?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xhpCf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xhpCf.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-05 21:24:48.9 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-11-09 16:20:55.253 UTC",
        "Question_score":3,
        "Question_tags":"azure-machine-learning-studio|azureml-python-sdk|azuremlsdk",
        "Question_view_count":267,
        "Owner_creation_date":"2012-06-27 21:51:16.13 UTC",
        "Owner_last_access_date":"2022-09-21 21:19:20.11 UTC",
        "Owner_reputation":751,
        "Owner_up_votes":68,
        "Owner_down_votes":5,
        "Owner_views":73,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":72606870,
        "Question_title":"Pass NEPTUNE_API_TOKEN environment variable via docker run command",
        "Question_body":"<p>Using the <code>docker run<\/code> command, I'm trying to pass my NEPTUNE_API_TOKEN to my container.<\/p>\n<p>My understanding is that I should use the <code>-e<\/code> flag as follows: <code>-e ENV_VAR='env_var_value'<\/code> and that might work.\nI wish, however, to use the value existing in the already-running session, as follows:<\/p>\n<pre><code>docker run -e NEPTUNE_API_TOKEN=$(NEPTUNE_API_TOKEN) &lt;my_image&gt;\n<\/code><\/pre>\n<p>However, after doing so, NEPTUNE_API_TOKEN is set to empty when checking the value inside the container.\nMy question is whether I'm doing something wrong or if this is not possible and I must provide an explicit Neptune API token as a string.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-13 17:30:31.553 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"docker|neptune",
        "Question_view_count":42,
        "Owner_creation_date":"2018-10-01 16:18:21.937 UTC",
        "Owner_last_access_date":"2022-09-22 07:04:08.157 UTC",
        "Owner_reputation":735,
        "Owner_up_votes":211,
        "Owner_down_votes":5,
        "Owner_views":80,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Israel",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "neptune"
        ]
    },
    {
        "Question_id":71145168,
        "Question_title":"Problems with Image Label Adjustment Job in Amazon Sagemaker Ground Truth",
        "Question_body":"<p>I'm trying to create a Image Label Adjustment Job in Ground Truth and I'm having some trouble. The thing is that I have a dataset of images, in which there are pre-made bounding boxes. I have an external python script that creates the &quot;dataset.manifest&quot; file with the json's of each image. Here are the first four lines of that manifest file:<\/p>\n<pre><code>{&quot;source-ref&quot;: &quot;s3:\/\/automatic-defect-detection\/LM-WNB1-M-0000126254-camera_2_0022.jpg&quot;, &quot;bounding-box&quot;: {&quot;image_size&quot;: [{&quot;width&quot;: 2048, &quot;height&quot;: 1536, &quot;depth&quot;: 3}], &quot;annotations&quot;: [{&quot;class_id&quot;: 0, &quot;width&quot;: 80, &quot;height&quot;: 80, &quot;top&quot;: 747, &quot;left&quot;: 840}]}, &quot;bounding-box-metadata&quot;: {&quot;class-map&quot;: {&quot;0&quot;: &quot;KK&quot;}, &quot;type&quot;: &quot;groundtruth\/object-detection&quot;, &quot;human-annotated&quot;: &quot;yes&quot;}}\n{&quot;source-ref&quot;: &quot;s3:\/\/automatic-defect-detection\/LM-WNB1-M-0000126259-camera_2_0028.jpg&quot;, &quot;bounding-box&quot;: {&quot;image_size&quot;: [{&quot;width&quot;: 2048, &quot;height&quot;: 1536, &quot;depth&quot;: 3}], &quot;annotations&quot;: [{&quot;class_id&quot;: 0, &quot;width&quot;: 80, &quot;height&quot;: 80, &quot;top&quot;: 1359, &quot;left&quot;: 527}]}, &quot;bounding-box-metadata&quot;: {&quot;class-map&quot;: {&quot;0&quot;: &quot;KK&quot;}, &quot;type&quot;: &quot;groundtruth\/object-detection&quot;, &quot;human-annotated&quot;: &quot;yes&quot;}}\n{&quot;source-ref&quot;: &quot;s3:\/\/automatic-defect-detection\/LM-WNB1-M-0000126256-camera_3_0006.jpg&quot;, &quot;bounding-box&quot;: {&quot;image_size&quot;: [{&quot;width&quot;: 2048, &quot;height&quot;: 1536, &quot;depth&quot;: 3}], &quot;annotations&quot;: [{&quot;class_id&quot;: 3, &quot;width&quot;: 80, &quot;height&quot;: 80, &quot;top&quot;: 322, &quot;left&quot;: 1154}, {&quot;class_id&quot;: 3, &quot;width&quot;: 80, &quot;height&quot;: 80, &quot;top&quot;: 633, &quot;left&quot;: 968}]}, &quot;bounding-box-metadata&quot;: {&quot;class-map&quot;: {&quot;3&quot;: &quot;FF&quot;}, &quot;type&quot;: &quot;groundtruth\/object-detection&quot;, &quot;human-annotated&quot;: &quot;yes&quot;}}\n{&quot;source-ref&quot;: &quot;s3:\/\/automatic-defect-detection\/LM-WNB1-M-0000126253-camera_2_0019.jpg&quot;, &quot;bounding-box&quot;: {&quot;image_size&quot;: [{&quot;width&quot;: 2048, &quot;height&quot;: 1536, &quot;depth&quot;: 3}], &quot;annotations&quot;: [{&quot;class_id&quot;: 2, &quot;width&quot;: 80, &quot;height&quot;: 80, &quot;top&quot;: 428, &quot;left&quot;: 1058}]}, &quot;bounding-box-metadata&quot;: {&quot;class-map&quot;: {&quot;2&quot;: &quot;DD&quot;}, &quot;type&quot;: &quot;groundtruth\/object-detection&quot;, &quot;human-annotated&quot;: &quot;yes&quot;}}\n<\/code><\/pre>\n<p>Now the problem is that I'm creating private jobs in Amazon Sagemaker to try it out. I have the manifest file and the images in a S3 bucket, and it actually kinda works. So I select the input manifest, activate the &quot;Existing-labels display options&quot;. The existing labels for the bounding boxes do not appear automatically, so I have to enter them manually (don't know why), but if I do that and try the preview before creating the adjustment job, the bounding boxes appear perfectly and I can adjust them. The thing is that, me being the only worker invited for the job, the job never apears to start working on it, and it just auto-completes. I can see later that the images are there with my pre-made bounding boxes, but the job never appears to adjust those boxes. I don't have the &quot;Automated data labeling&quot; option activated. Is there something missing in my manifest file?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-16 16:00:22.603 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-02-16 20:43:33.643 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker|bounding-box",
        "Question_view_count":183,
        "Owner_creation_date":"2022-01-24 17:36:34.9 UTC",
        "Owner_last_access_date":"2022-09-24 04:13:29.37 UTC",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Chile",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":72309936,
        "Question_title":"AWS sagemaker with fbprophet algorithm",
        "Question_body":"<p>I am trying to generate some realtime predictions using fbProhet,  AWS sagemaker .\nHere what I am trying.<\/p>\n<ol>\n<li><p>created a dockerfile which will install fbprohet library and copy the myfile.py file and added\nENTRYPOINT [&quot;python&quot;, &quot;.\/myfile.py&quot;], where myfile.py contains the code to analyze the sample and upload the result as .csv to s3.<\/p>\n<\/li>\n<li><p>created docker image using the above file and pushed to ECR<\/p>\n<\/li>\n<li><p>created a training job in sagemaker by referring the above docker image.<\/p>\n<\/li>\n<li><p>created model from the training job<\/p>\n<\/li>\n<li><p>created endpoint using the model<\/p>\n<\/li>\n<\/ol>\n<p>But it is getting failed as it couldn't find and model artifact in S3.  and to test I have created a .zip file and uploaded to s3 bucket. but still the endpoint is getting failed as there is a ping error &quot;the default variant did not pass the ping health check&quot;.<\/p>\n<p>I am not sure what I am missing.<\/p>\n<p>the dockerfile is:<\/p>\n<pre><code>FROM python:3-slim\nSHELL [&quot;\/bin\/bash&quot;, &quot;-c&quot;]\n\nRUN apt-get update &amp;&amp; apt-get install -y wget &amp;&amp; apt-get install -y curl &amp;&amp; apt-get install -y git &amp;&amp; apt-get clean &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\nRUN curl -L https:\/\/github.com\/pyenv\/pyenv-installer\/raw\/master\/bin\/pyenv-installer | bash\nENV PYENV_ROOT $HOME\/.pyenv\nENV PATH $PYENV_ROOT\/shims:$PYENV_ROOT\/bin:$PATH\nRUN wget --quiet https:\/\/repo.anaconda.com\/miniconda\/Miniconda3-latest-Linux-x86_64.sh -O \/tmp\/install_miniconda.sh &amp;&amp; \\\n \/bin\/bash \/tmp\/install_miniconda.sh -b -p \/opt\/conda\nENV PATH=\/opt\/conda\/bin:$PATH\nRUN pip install --no-cache-dir --upgrade \\\n    pip \\\n    setuptools \\\n    wheel\n\nRUN conda install -c conda-forge Prophet\n\nCOPY myfile.py .\nRUN chmod +x myfile.py\nCOPY requirement.txt .\nRUN python -m pip install -r requirement.txt\n\nENTRYPOINT [&quot;python&quot;, &quot;.\/myfile.py&quot;\n<\/code><\/pre>\n<p>and myfile is :<\/p>\n<pre><code>import boto3\nimport logging\nimport json\nimport base64\nimport pandas as pd\nimport plotly.express as px\nimport numpy as np\nimport sqlite3\nfrom sqlite3 import Error\nfrom time import time\nfrom datetime import datetime\nfrom datetime import timedelta\n\nfrom configparser import ConfigParser\nfrom sqlalchemy.engine.url import URL\nfrom sqlalchemy import create_engine\n\nfrom botocore.exceptions import ClientError\nimport configparser\nfrom prophet import Prophet\n\ndef test():\n    bucket = 'bucket-test'\n    file = 'test.xlsx'\n    df = pd.read_excel(f&quot;s3:\/\/{bucket}\/{file}&quot;)\n    df.columns.values\n    column1=(df.columns.values[1])\n    column2=(df.columns.values[10])\n    parsedTimestampColumn=(df[column1])\n    parsedMetricsColumn=(df[column2])\n    data={'ps':parsedTimestampColumn , 'pd':parsedMetricsColumn}\n    df1 = pd.DataFrame(data)\n    \n    m = Prophet(yearly_seasonality=True)\n    m.fit(df1)\n    future = m.make_future_dataframe(periods=500,freq='H')\n    forecast = m.predict(future)\n    forecast[['ps', 'w', 'w_l', 'w_u']].tail()\n    fig1 = m.plot(forecast)\n    fig2 = m.plot_components(forecast)\n    \n    results=pd.concat([df1[['pd']],forecast[['ps', 'w', 'w_l', 'w_u']]],axis=1)\n    results['error']=results['y']-results['w']\n    results[&quot;uncertainity&quot;]=results['w_u']-results['w_l']\n    results[results['error'].abs()&gt; 1.2*results['uncertainity']]\n    results['Anomalies']=results.apply(lambda x:'Yes' if (np.abs(x['error'])&gt;1.2*x['uncertainity'] )else 'No',axis=1)\n    result_Dataframe = pd.DataFrame(results)\n    print(result_Dataframe.tail())\n    return result_Dataframe\n     \n\nif __name__==&quot;__main__&quot;:\n  test()\n\n<\/code><\/pre>\n<p>can any one guide me what I am missing here to create a successful model and endpoint ?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":6,
        "Question_creation_date":"2022-05-19 19:09:49.657 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2022-05-19 20:28:46.387 UTC",
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|docker|amazon-sagemaker",
        "Question_view_count":178,
        "Owner_creation_date":"2020-07-26 17:36:21.563 UTC",
        "Owner_last_access_date":"2022-09-15 14:20:33.707 UTC",
        "Owner_reputation":51,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":71467176,
        "Question_title":"How can I verify that my training job is reading the augmented manifest file?",
        "Question_body":"<p>Apologies for the long post.<\/p>\n<p>Originally, I had data in one location on an S3 bucket and used to train deep learning image classification models on this data using the typical 'File' mode and passing the S3 uri where the data is stored as training input. To try and accelerate training, I wanted to switch to using:<\/p>\n<ol>\n<li>Pipe mode, to stream data and not download all the data at the beginning of the training, starting training faster and saving disk space.<\/li>\n<li>Augmented Manifest File coupled with 1., so that I don't have to place my data in a single location on S3, so I avoid moving data around when I train models.<\/li>\n<\/ol>\n<p>I was making my script similar to <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=934156#934156\" rel=\"nofollow noreferrer\">the one in this example<\/a>. I printed the steps done when parsing the data, however I noticed that the data might not have been read because when printing it shows the following:<\/p>\n<pre><code>step 1 Tensor(&quot;ParseSingleExample\/ParseExample\/ParseExampleV2:0&quot;, shape=(), dtype=string)\nstep 2 Tensor(&quot;DecodePng:0&quot;, shape=(None, None, 3), dtype=uint8)\nstep 3 Tensor(&quot;Cast:0&quot;, shape=(None, None, 3), dtype=float32)\n<\/code><\/pre>\n<p>I guess the image is not being read\/found since the shape is <code>[None, None, 3]<\/code> when it should be <code>[224, 224, 3]<\/code>, so maybe the problem is from the Augmented Manifest file?<\/p>\n<p>Here's an example of how my Augmented Manifest file is written:<\/p>\n<pre><code>{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image1.png&quot;, &quot;label&quot;: 1}\n{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image2.png&quot;, &quot;label&quot;: 2}\n{&quot;image-ref&quot;: &quot;s3:\/\/path\/to\/my\/image\/image3.png&quot;, &quot;label&quot;: 3}\n<\/code><\/pre>\n<p>Some other details I should probably mention:<\/p>\n<ol>\n<li>When I create the Training Input I pass <code>'content_type': 'application\/x-recordio', 'record_wrapping': 'RecordIO'<\/code>, even though my data are in .png format, but I assumed that as the augmented manifest file is read the data get wrapped in the RecordIO format.<\/li>\n<li>Following my first point, I pass <code>PipeModeDataset(channel=channel, record_format='RecordIO')<\/code>, so also not sure about the RecordIO thing.<\/li>\n<\/ol>\n<p>There isn't an actual error that is raised, just when I start fitting the model nothing happens, it keeps on running but nothing actually runs so I'm trying to find the issue.<\/p>\n<hr \/>\n<p>EDIT: It now reads the shape correctly, but there's still the issue where it enters the .fit method and does nothing, just keeps running without doing anything. Find part of the script below.<\/p>\n<pre><code>def train_input_fn(train_channel):\n    &quot;&quot;&quot;Returns input function that feeds the model during training&quot;&quot;&quot;\n    return _input_fn(train_channel)\n\ndef _input_fn(channel):\n    &quot;&quot;&quot;\n        Returns a Dataset which reads from a SageMaker PipeMode channel.\n    &quot;&quot;&quot;\n    \n    features = {\n        'image-ref': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([3], tf.int64),\n    }\n \n    def combine(records):\n        return records[0], records[1]\n \n    def parse(record):\n        \n        parsed = tf.io.parse_single_example(record, features)\n        \n                 \n\n        image = tf.io.decode_png(parsed[&quot;image-ref&quot;], channels=3, dtype=tf.uint8)\n        image = tf.reshape(image, [224, 224, 3])\n        \n        lbl = parsed['label']\n        print(image, lbl)\n        return (image, lbl)\n \n    ds = PipeModeDataset(channel=channel, record_format='RecordIO')\n    ds = ds.map(parse, num_parallel_calls=AUTOTUNE)\n    ds = ds.prefetch(AUTOTUNE)\n \n    return ds\n\ndef model(dataset):\n    &quot;&quot;&quot;Generate a simple model&quot;&quot;&quot;\n    inputs = Input(shape=(224, 224, 3))\n    prediction_layer = Dense(2, activation = 'softmax')\n\n\n    x = inputs\n    x = tf.keras.applications.mobilenet.MobileNet(include_top=False, input_shape=(224,224,3), weights='imagenet')(x)\n    outputs = prediction_layer(x)\n    rec_model = tf.keras.Model(inputs, outputs)    \n    \n    rec_model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n        metrics=['accuracy']\n    )\n    \n    \n    rec_model.fit(\n        dataset\n    )\n\n    return rec_model\n\ndef main(params):\n    \n    epochs = params['epochs']\n    train_channel = params['train_channel']\n    record_format = params['record_format']\n    batch_size = params['batch_size']\n        \n    train_spec = train_input_fn(train_channel)\n    model_classifier = model(train_spec)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-14 11:41:25.31 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2022-03-14 14:41:37.627 UTC",
        "Question_score":0,
        "Question_tags":"python|tensorflow|amazon-s3|manifest|amazon-sagemaker",
        "Question_view_count":127,
        "Owner_creation_date":"2019-12-10 22:23:16.283 UTC",
        "Owner_last_access_date":"2022-08-11 09:37:11.827 UTC",
        "Owner_reputation":15,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>From <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions#using-the-pipemodedataset\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<blockquote>\n<p>A PipeModeDataset can read TFRecord, RecordIO, or text line records.<\/p>\n<\/blockquote>\n<p>While your'e trying to read binary (PNG) files. I don't see a relevant <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions\/tree\/tf-2\/src\/pipemode_op\/RecordReader\" rel=\"nofollow noreferrer\">record reader here<\/a> to help you do that.<br \/>\nYou could build your own format pipe implementation like shown <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">here<\/a>, but it's considerably more effort.<\/p>\n<p>Alternatively, you mentioned your files are scattered in different folders, but if your files common path contains less than 2M files, you could use <a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/10\/amazon-sagemaker-fast-file-mode\/\" rel=\"nofollow noreferrer\">FastFile mode<\/a> to <strong>stream<\/strong> data. Currently, FastFile only supports an S3 Prefix, so you won't be able to use a manifest.<\/p>\n<p>Also see this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/choose-the-best-data-source-for-your-amazon-sagemaker-training-job\/\" rel=\"nofollow noreferrer\">general pros\/cons discussion of the different available storage and input types available in SageMaker<\/a>.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2022-03-18 12:38:20.133 UTC",
        "Answer_score":3.0,
        "Owner_location":"Beirut, Lebanon",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":71404719,
        "Question_title":"numpy.core._exceptions.MemoryError: Unable to allocate 75.2 GiB for an array with shape (1, 10086709500) and data type float64",
        "Question_body":"<p>I am training IsolationForest for anomaly detection in amazon SageMaker using the following code:-<\/p>\n<pre><code>def build_model(df, no_of_estimators, maximum_samples, error_percent, maximum_features):\n        encoder = MultiColumnLabelEncoder(columns=columns_to_encode)\n        df = df.dropna()\n        df = encoder.fit_transform(df)\n        print(df.head())\n        print(df.tail())\n        print(df.shape)\n        isf = IsolationForest(n_estimators=no_of_estimators, max_samples=maximum_samples, contamination=error_percent, max_features=df.shape[1],\n                              bootstrap=False, n_jobs=6,verbose=2, random_state=42)\n        isf.fit(df)\n        df['outlier'] = isf.predict(df)\n        df['score'] = isf.decision_function(df.drop(['outlier'], axis=1))\n        df = encoder.inverse_transform(df)\n        return isf, df\n<\/code><\/pre>\n<p>But I am getting the following error:-<\/p>\n<pre><code>Traceback (most recent call last):\nFile &quot;train_script_new.py&quot;, line 127, in &lt;module&gt;\nisf, prediction_df = build_model(dataset, no_of_estimators, maximum_samples, \nerror_percent, maximum_features)\nFile &quot;train_script_new.py&quot;, line 102, in build_model\nisf.fit(df)\nFile &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sklearn\/ensemble\/iforest.py&quot;, line 274, \nin fit\nself._threshold_ = np.percentile(self.decision_function(X),\nFile &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sklearn\/ensemble\/iforest.py&quot;, line 345, \nin decision_function\nreturn self.score_samples(X) - self.offset_\nFile &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sklearn\/ensemble\/iforest.py&quot;, line 403, \nin score_samples\ndepths += _average_path_length(n_samples_leaf)\nFile &quot;\/miniconda3\/lib\/python3.7\/site-packages\/sklearn\/ensemble\/iforest.py&quot;, line 448, \nin _average_path_length\naverage_path_length = np.zeros(n_samples_leaf.shape)\nnumpy.core._exceptions.MemoryError: Unable to allocate 75.2 GiB for an array with \nshape (1, 10086709500) and data type float64\n<\/code><\/pre>\n<p>The shape of the dataset i am passing is:- df.shape: (20226508, 5) but in the error it says (1, 10086709500) is this because something is happening internally in the algorithm which is creating this array?<\/p>\n<p>These are the last 3 lines of the logs printed by the model:-<\/p>\n<pre><code> Building estimator 500 of 500 for this parallel run (total 500)...\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  6.0min remaining: 0.0s\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  6.0min finished\n<\/code><\/pre>\n<p>How could I come out of this error? I have tried to train model with larger instance which didn't work. currently i am training with ml.m4.16xlarge instance. Please help.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-03-09 05:28:42.243 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|machine-learning|amazon-sagemaker|isolation-forest",
        "Question_view_count":619,
        "Owner_creation_date":"2018-07-15 07:40:34.317 UTC",
        "Owner_last_access_date":"2022-06-20 05:07:35.513 UTC",
        "Owner_reputation":205,
        "Owner_up_votes":33,
        "Owner_down_votes":0,
        "Owner_views":42,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Bangalore, Karnataka, India",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":45582412,
        "Question_title":"How to build a Convolution Neural Net in Azure Machine Learning?",
        "Question_body":"<p>Someone should add \"net#\" as a tag. I'm trying to improve my neural network in Azure Machine Learning Studio by turning it into a convolution neural net using this tutorial:<\/p>\n\n<p><a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Neural-Network-Convolution-and-pooling-deep-net-2\" rel=\"noreferrer\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Neural-Network-Convolution-and-pooling-deep-net-2<\/a><\/p>\n\n<p>The differences between mine and the tutorial is I'm doing regression with 35 features and 1 label and they're doing classification with 28x28 features and 10 labels. <\/p>\n\n<p>I start with the basic and 2nd example and get them working with:<\/p>\n\n<pre><code>input Data [35];\n\nhidden H1 [100]\n    from Data all;\n\nhidden H2 [100]\n    from H1 all;\n\noutput Result [1] linear\n    from H2 all;\n<\/code><\/pre>\n\n<p>Now the transformation to convolution I misunderstand. In the tutorial and documentation here: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-azure-ml-netsharp-reference-guide\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-azure-ml-netsharp-reference-guide<\/a> it doesn't mention how the node tuple values are calculated for the hidden layers. The tutorial says:<\/p>\n\n<pre><code>hidden C1 [5, 12, 12]\n  from Picture convolve {\n    InputShape  = [28, 28];\n    KernelShape = [ 5,  5];\n    Stride      = [ 2,  2];\n    MapCount = 5;\n  }\n\nhidden C2 [50, 4, 4]\n   from C1 convolve {\n     InputShape  = [ 5, 12, 12];\n     KernelShape = [ 1,  5,  5];\n     Stride      = [ 1,  2,  2];\n     Sharing     = [ F,  T,  T];\n     MapCount = 10;\n  }\n<\/code><\/pre>\n\n<p>Seems like the [5, 12, 12] and [50,4,4] pop out of no where along with the KernalShape, Stride, and MapCount. How do I know what values are valid for my example? I tried using the same values, but it didn't work and I have a feeling since he has a [28,28] input and I have a [35], I need tuples with 2 integers not 3. <\/p>\n\n<p>I just tried with random values that seem to correlate with the tutorial:<\/p>\n\n<pre><code>const { T = true; F = false; }\n\ninput Data [35];\n\nhidden C1 [7, 23]\n  from Data convolve {\n    InputShape  = [35];\n    KernelShape = [7];\n    Stride      = [2];\n    MapCount = 7;\n  }\n\nhidden C2 [200, 6]\n   from C1 convolve {\n     InputShape  = [ 7, 23];\n     KernelShape = [ 1,  7];\n     Stride      = [ 1,  2];\n     Sharing     = [ F,  T];\n     MapCount = 14;\n  }\n\nhidden H3 [100]\n  from C2 all;\n\noutput Result [1] linear\n  from H3 all;\n<\/code><\/pre>\n\n<p>Right now it seems impossible to debug because the only error code Azure Machine Learning Studio ever gives is:<\/p>\n\n<pre><code>Exception\":{\"ErrorId\":\"LibraryException\",\"ErrorCode\":\"1000\",\"ExceptionType\":\"ModuleException\",\"Message\":\"Error 1000: TLC library exception: Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown.\",\"Exception\":{\"Library\":\"TLC\",\"ExceptionType\":\"LibraryException\",\"Message\":\"Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown.\"}}}Error: Error 1000: TLC library exception: Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown. Process exited with error code -2\n<\/code><\/pre>\n\n<p>Lastly my setup is <a href=\"https:\/\/i.stack.imgur.com\/PBN9L.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PBN9L.png\" alt=\"Azure Machine Learning Setup\"><\/a> <\/p>\n\n<p>Thanks for the help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2017-08-09 05:37:28.253 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2017-08-14 04:53:38.057 UTC",
        "Question_score":9,
        "Question_tags":"machine-learning|convolution|azure-machine-learning-studio|net#",
        "Question_view_count":1268,
        "Owner_creation_date":"2014-12-13 21:25:26.277 UTC",
        "Owner_last_access_date":"2022-09-24 17:32:50.893 UTC",
        "Owner_reputation":1454,
        "Owner_up_votes":117,
        "Owner_down_votes":15,
        "Owner_views":328,
        "Answer_body":"<p>The correct network definition for 35-column length input with given kernels and strides would be following:<\/p>\n\n<pre><code>const { T = true; F = false; }\n\ninput Data [35];\n\nhidden C1 [7, 15]\n  from Data convolve {\n    InputShape  = [35];\n    KernelShape = [7];\n    Stride      = [2];\n    MapCount = 7;\n  }\n\nhidden C2 [14, 7, 5]\n   from C1 convolve {\n     InputShape  = [ 7, 15];\n     KernelShape = [ 1,  7];\n     Stride      = [ 1,  2];\n     Sharing     = [ F,  T];\n     MapCount = 14;\n  }\n\nhidden H3 [100]\n  from C2 all;\n\noutput Result [1] linear\n  from H3 all;\n<\/code><\/pre>\n\n<p>First, the C1 = [7,15]. The first dimension is simply the MapCount. For the second dimension, the kernel shape defines the length of the \"window\" that's used to scan the input columns, and the stride defines how much it moves at each step. So the kernel windows would cover columns 1-7, 3-9, 5-11,...,29-35, yielding the second dimension of 15 when you tally the windows.<\/p>\n\n<p>Next, the C2 = [14,7,5]. The first dimension is again the MapCount. For the second and third dimension, the 1-by-7 kernel \"window\" has to cover the input size of 7-by-15, using steps of 1 and 2 along corresponding dimensions. <\/p>\n\n<p>Note that you could specify C2 hidden layer shape of [98,5] or even [490], if you wanted to flatten the outputs. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2017-08-21 19:04:16.37 UTC",
        "Answer_score":1.0,
        "Owner_location":"Missouri",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":929508,
        "Question_title":"Mercurial with multiple projects",
        "Question_body":"<p>I have a couple of projects with different release cycles sitting in my SVN repository. Releases are created by using the classic tags structure in SVN. When there are bugs to fix in releases a branch is created from a tag, the bug is fixed and then merged from there into trunk.<\/p>\n\n<p>Now, for multiple reasons, I want to switch from SVN to mercurial with a central push site.<\/p>\n\n<p>Question: Which is the best way in mercurial to organize multiple projects that share little code between them? Should I create multiple push sites, one for each project?<\/p>\n\n<p>Please include in the answer a description on how to recreate my release-tag, bugfix branch, ... with your preferred version of repository design.<\/p>\n\n<p>Edit: I would like to install as little extensions as possible.<\/p>\n\n<p>Edit2:<\/p>\n\n<p>Given this SVN layout:<\/p>\n\n<pre><code>.\n|-- project-a\n|   |-- branches\n|   |   |-- 1.x\n|   |   `-- feature-1\n|   |-- tags\n|   `-- trunk\n`-- project-b\n    |-- branches\n    |-- tags\n    |   |-- 1.0\n    |   `-- 1.1\n    `-- trunk\n<\/code><\/pre>\n\n<p>(thanks @bendin! :) )<\/p>\n\n<p>Is it better to work with multiple hg push repositories<\/p>\n\n<pre><code>project_a-trunk\nproject_a-1.x\nproject_a-feature-1\nproject_b-trunk\n<\/code><\/pre>\n\n<p>for the branches. Tags are folded into the appropriate branch.<\/p>\n\n<p>Or would you rather go with two push repositories in this example<\/p>\n\n<pre><code>project_a\nproject_b\n<\/code><\/pre>\n\n<p>with named branches and therefore multiple heads within one repo.<\/p>\n\n<p>The advantage I see with the multiple heads repos is that I don't have to go hunt for a tag in multiple repos. The disadvantage I see is that the hg book seems to discourage multiple head repos. What would\/do you do?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_date":"2009-05-30 10:35:06.46 UTC",
        "Question_favorite_count":12.0,
        "Question_last_edit_date":"2015-07-03 10:20:00.393 UTC",
        "Question_score":32,
        "Question_tags":"version-control|mercurial|dvcs|repository-design",
        "Question_view_count":8124,
        "Owner_creation_date":"2008-11-04 00:25:25.99 UTC",
        "Owner_last_access_date":"2022-09-22 08:10:44.19 UTC",
        "Owner_reputation":2742,
        "Owner_up_votes":79,
        "Owner_down_votes":5,
        "Owner_views":417,
        "Answer_body":"<p>Some subversion repositories will group logically unrelated things (i.e. projects with different version numbers and release cycles) under one trunk:<\/p>\n\n<pre><code>.\n|-- branches\n|   |-- project-a-1.x\n|   `-- project-a-feature-1\n|-- tags\n|   |-- project-a-1.0\n|   |-- project-b-1.0\n|   `-- project-b-1.1\n`-- trunk\n    |-- project-a\n    `-- project-b\n<\/code><\/pre>\n\n<p>This kind of layout has no direct analog in mercurial. Each project which has its own release cycle and own version numbers should have its own repository. <\/p>\n\n<p>Some subversion repositories are structured to do this by giving each project its own trunk, tags and branches: <\/p>\n\n<pre><code>.\n|-- project-a\n|   |-- branches\n|   |   |-- 1.x\n|   |   `-- feature-1\n|   |-- tags\n|   `-- trunk\n`-- project-b\n    |-- branches\n    |-- tags\n    |   |-- 1.0\n    |   `-- 1.1\n    `-- trunk\n<\/code><\/pre>\n\n<p>You can think of each project as a logical repository within your physical subversion repository. Each project has its own trunk, tags and branches.  This also has the benefit that you can keep tag and branch names shorter because you already know which project they belong to.<\/p>\n\n<p>This layout is also trivial to express with a tool like mercurial. Each \"project\" becomes a mercurial repository. Tags and branches within that repository are tags and branches of that project.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2009-05-30 10:59:57.393 UTC",
        "Answer_score":12.0,
        "Owner_location":"Unterschleissheim, Bavaria, Germany",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":32752659,
        "Question_title":"unable to install R library in azure ml",
        "Question_body":"<p>I have been trying to install a machine learning package that I can use in my R script.<\/p>\n\n<p>I have done placed the tarball of the installer inside a zip file and am doing <\/p>\n\n<pre><code>install.packages(\"src\/packagename_2.0-3.tar.gz\", repos = NULL, type=\"source\") \n<\/code><\/pre>\n\n<p>from within the R script. However, the progress indicator just circles indefinitely, and it's not installed in environment.<\/p>\n\n<p>How can I install this package?<\/p>\n\n<p><code>ada<\/code> is the package I'm trying to install and <code>ada_2.0-3.tar.gz<\/code> is the file I'm using.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2015-09-24 03:07:33.39 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2015-09-24 05:03:33.853 UTC",
        "Question_score":0,
        "Question_tags":"r|azure|cran|adaboost|azure-machine-learning-studio",
        "Question_view_count":982,
        "Owner_creation_date":"2015-03-21 00:57:21.17 UTC",
        "Owner_last_access_date":"2021-03-18 18:32:12.667 UTC",
        "Owner_reputation":2024,
        "Owner_up_votes":76,
        "Owner_down_votes":1,
        "Owner_views":298,
        "Answer_body":"<p>You cannot use the tarball packages. If you are on windows you need to do the following:<\/p>\n\n<p>Once you install a package (+ it's dependencies) it will download the packages in a directory <\/p>\n\n<blockquote>\n  <p>C:\\Users\\xxxxx\\AppData\\Local\\Temp\\some directory\n  name\\downloaded_packages<\/p>\n<\/blockquote>\n\n<p>These will be in a zip format. These are the packages you need. <\/p>\n\n<p>Or download the windows binaries from cran.<\/p>\n\n<p>Next you need to put all the needed packages in one total zip-file and upload this to AzureML as a new dataset.<\/p>\n\n<p>in AzureML load the data package connected to a r-script<\/p>\n\n<pre><code>install.packages(\"src\/ada.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nlibrary(ada, lib.loc=\".\", verbose=TRUE)\n<\/code><\/pre>\n\n<p>Be sure to check that all dependent packages are available in Azure. Rpart is available.<\/p>\n\n<p>For a complete overview, look at this <a href=\"http:\/\/blogs.msdn.com\/b\/benjguin\/archive\/2014\/09\/24\/how-to-upload-an-r-package-to-azure-machine-learning.aspx\" rel=\"nofollow\">msdn blog<\/a> explaining it a bit better with some visuals.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2015-09-24 11:59:56.317 UTC",
        "Answer_score":3.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":73499320,
        "Question_title":"Error loading model from mlflow: java.io.StreamCorruptedException: invalid type code: 00",
        "Question_body":"<p>I'm using Databricks Connect version 9.1.16 to connect to a databricks external cluster with spark version 3.1 and download a Pyspark ML model that's been trained and saved using mlflow.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.set_tracking_uri(&quot;databricks&quot;)\nmodel_h = mlflow.spark.load_model(model_uri=&quot;models:\/model_name\/model_version&quot;)\n<\/code><\/pre>\n<p>I get the following output and error:<\/p>\n<pre><code>2022\/08\/26 11:54:18 INFO mlflow.spark: 'models:\/model_name\/model_version' resolved as 'dbfs:\/\/databricks\/databricks\/mlflow-registry\/model_id\/models\/model'\n2022\/08\/26 11:54:25 INFO mlflow.spark: URI 'dbfs:\/\/databricks\/databricks\/mlflow-registry\/model_id\/models\/model\/sparkml' does not point to the current DFS.\n2022\/08\/26 11:54:25 INFO mlflow.spark: File 'dbfs:\/\/databricks\/databricks\/mlflow-registry\/model_id\/models\/model\/sparkml' not found on DFS. Will attempt to upload the file.\n2022\/08\/26 11:55:06 INFO mlflow.spark: Copied SparkML model to \/tmp\/mlflow\/model_id\n---------------------------------------------------------------------------\nPy4JJavaError                             Traceback (most recent call last)\nc:\\Users\\carlafernandez\\Documents\\my_notebook.ipynb Celda 5 in &lt;cell line: 2&gt;()\n      1 mlflow.set_tracking_uri(&quot;databricks&quot;)\n----&gt; 2 model_h = mlflow.spark.load_model(model_uri=&quot;models:\/model_name\/model_version&quot;)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\mlflow\\spark.py:711, in load_model(model_uri, dfs_tmpdir)\n    708 local_model_path = _download_artifact_from_uri(model_uri)\n    709 _add_code_from_conf_to_system_path(local_model_path, flavor_conf)\n--&gt; 711 return _load_model(model_uri=model_uri, dfs_tmpdir_base=dfs_tmpdir)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\mlflow\\spark.py:660, in _load_model(model_uri, dfs_tmpdir_base)\n    658     return _load_model_databricks(model_uri, dfs_tmpdir)\n    659 model_uri = _HadoopFileSystem.maybe_copy_from_uri(model_uri, dfs_tmpdir)\n--&gt; 660 return PipelineModel.load(model_uri)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\pyspark\\ml\\util.py:463, in MLReadable.load(cls, path)\n    460 @classmethod\n    461 def load(cls, path):\n    462     &quot;&quot;&quot;Reads an ML instance from the input path, a shortcut of `read().load(path)`.&quot;&quot;&quot;\n--&gt; 463     return cls.read().load(path)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\pyspark\\ml\\pipeline.py:258, in PipelineModelReader.load(self, path)\n    256 metadata = DefaultParamsReader.loadMetadata(path, self.sc)\n    257 if 'language' not in metadata['paramMap'] or metadata['paramMap']['language'] != 'Python':\n--&gt; 258     return JavaMLReader(self.cls).load(path)\n    259 else:\n    260     uid, stages = PipelineSharedReadWrite.load(metadata, self.sc, path)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\pyspark\\ml\\util.py:413, in JavaMLReader.load(self, path)\n    411 if not isinstance(path, str):\n    412     raise TypeError(&quot;path should be a string, got type %s&quot; % type(path))\n--&gt; 413 java_obj = self._jread.load(path)\n    414 if not hasattr(self._clazz, &quot;_from_java&quot;):\n    415     raise NotImplementedError(&quot;This Java ML type cannot be loaded into Python currently: %r&quot;\n    416                               % self._clazz)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\py4j\\java_gateway.py:1304, in JavaMember.__call__(self, *args)\n   1298 command = proto.CALL_COMMAND_NAME +\\\n   1299     self.command_header +\\\n   1300     args_command +\\\n   1301     proto.END_COMMAND_PART\n   1303 answer = self.gateway_client.send_command(command)\n-&gt; 1304 return_value = get_return_value(\n   1305     answer, self.gateway_client, self.target_id, self.name)\n   1307 for temp_arg in temp_args:\n   1308     temp_arg._detach()\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\pyspark\\sql\\utils.py:117, in capture_sql_exception.&lt;locals&gt;.deco(*a, **kw)\n    115 def deco(*a, **kw):\n    116     try:\n--&gt; 117         return f(*a, **kw)\n    118     except py4j.protocol.Py4JJavaError as e:\n    119         converted = convert_exception(e.java_exception)\n\nFile c:\\Users\\carlafernandez\\miniconda3\\envs\\prueba_databricks_connect\\lib\\site-packages\\py4j\\protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)\n    324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n    325 if answer[1] == REFERENCE_TYPE:\n--&gt; 326     raise Py4JJavaError(\n    327         &quot;An error occurred while calling {0}{1}{2}.\\n&quot;.\n    328         format(target_id, &quot;.&quot;, name), value)\n    329 else:\n    330     raise Py4JError(\n    331         &quot;An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n&quot;.\n    332         format(target_id, &quot;.&quot;, name, value))\n\nPy4JJavaError: An error occurred while calling o645.load.\n: java.io.StreamCorruptedException: invalid type code: 00\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1698)\n    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n    at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n    at sun.reflect.GeneratedMethodAccessor311.invoke(Unknown Source)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184)\n    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2296)\n    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n    at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)\n    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n    at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)\n    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n    at org.apache.spark.sql.util.ProtoSerializer.$anonfun$deserializeObject$1(ProtoSerializer.scala:6631)\n    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n    at org.apache.spark.sql.util.ProtoSerializer.deserializeObject(ProtoSerializer.scala:6616)\n    at com.databricks.service.SparkServiceRPCHandler.execute0(SparkServiceRPCHandler.scala:728)\n    at com.databricks.service.SparkServiceRPCHandler.$anonfun$executeRPC0$1(SparkServiceRPCHandler.scala:477)\n    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n    at com.databricks.service.SparkServiceRPCHandler.executeRPC0(SparkServiceRPCHandler.scala:372)\n    at com.databricks.service.SparkServiceRPCHandler$$anon$2.call(SparkServiceRPCHandler.scala:323)\n    at com.databricks.service.SparkServiceRPCHandler$$anon$2.call(SparkServiceRPCHandler.scala:309)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at com.databricks.service.SparkServiceRPCHandler.$anonfun$executeRPC$1(SparkServiceRPCHandler.scala:359)\n    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n    at com.databricks.service.SparkServiceRPCHandler.executeRPC(SparkServiceRPCHandler.scala:336)\n    at com.databricks.service.SparkServiceRPCServlet.doPost(SparkServiceRPCServer.scala:167)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n    at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)\n    at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n    at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n    at org.eclipse.jetty.server.Server.handle(Server.java:516)\n    at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n    at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n    at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n    at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n    at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n    at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n    at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n    at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n    at java.lang.Thread.run(Thread.java:748)\n<\/code><\/pre>\n<p>So it seems like it's able to find a copy the model, but then somehow it cannot read it. It's worth noting that the same <strong>works in a databricks notebook<\/strong>, the problem only occurs using databricks connect.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-26 10:05:08.487 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"pyspark|databricks|mlflow|databricks-connect",
        "Question_view_count":23,
        "Owner_creation_date":"2017-09-24 08:15:07.953 UTC",
        "Owner_last_access_date":"2022-09-23 11:38:23.76 UTC",
        "Owner_reputation":56,
        "Owner_up_votes":47,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Madrid, Espa\u00f1a",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":41032108,
        "Question_title":"How to install TensorFlow in jupyter notebook on Azure Machine Learning Studio",
        "Question_body":"<p>I'm trying to test Azure Machine Learning Studio. <\/p>\n\n<p>I want to use TensorFlow, but it is not installed on Jupyter notebook.<\/p>\n\n<p>How can I use some machine learning libraries like TensorFlow, Theano, Keras,... on the notebook?<\/p>\n\n<p>I tried this:<\/p>\n\n<pre><code>!pip install tensorflow \n<\/code><\/pre>\n\n<p>But, I got error as below:<\/p>\n\n<pre><code>Collecting tensorflow\n  Downloading tensorflow-0.12.0rc0-cp34-cp34m-manylinux1_x86_64.whl (43.1MB)\n    100% |################################| 43.1MB 27kB\/s \nCollecting protobuf==3.1.0 (from tensorflow)\n  Downloading protobuf-3.1.0-py2.py3-none-any.whl (339kB)\n    100% |################################| 348kB 3.7MB\/s \nCollecting six&gt;=1.10.0 (from tensorflow)\n  Downloading six-1.10.0-py2.py3-none-any.whl\nRequirement already satisfied: numpy&gt;=1.11.0 in \/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages (from tensorflow)\nRequirement already satisfied: wheel&gt;=0.26 in \/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages (from tensorflow)\nRequirement already satisfied: setuptools in \/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/setuptools-27.2.0-py3.4.egg (from protobuf==3.1.0-&gt;tensorflow)\nInstalling collected packages: six, protobuf, tensorflow\n  Found existing installation: six 1.9.0\n    DEPRECATION: Uninstalling a distutils installed project (six) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\n    Uninstalling six-1.9.0:\n      Successfully uninstalled six-1.9.0\n  Rolling back uninstall of six\nException:\nTraceback (most recent call last):\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/basecommand.py\", line 215, in main\n    status = self.run(options, args)\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/commands\/install.py\", line 342, in run\n    prefix=options.prefix_path,\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/req\/req_set.py\", line 784, in install\n    **kwargs\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/req\/req_install.py\", line 851, in install\n    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/req\/req_install.py\", line 1064, in move_wheel_files\n    isolated=self.isolated,\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/wheel.py\", line 345, in move_wheel_files\n    clobber(source, lib_dir, True)\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/wheel.py\", line 329, in clobber\n    os.utime(destfile, (st.st_atime, st.st_mtime))\nPermissionError: [Errno 1] Operation not permitted\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2016-12-08 05:03:06.563 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":7,
        "Question_tags":"tensorflow|theano|jupyter-notebook|keras|azure-machine-learning-studio",
        "Question_view_count":2886,
        "Owner_creation_date":"2010-07-20 14:41:43.497 UTC",
        "Owner_last_access_date":"2022-09-23 17:56:36.287 UTC",
        "Owner_reputation":1540,
        "Owner_up_votes":1643,
        "Owner_down_votes":3,
        "Owner_views":404,
        "Answer_body":"<p>As you noticed, the active user doesn't have permissions to write to the <code>site-packages<\/code> directory in Azure Machine Learning Studio notebooks. You could try installing the package to another directory where you do have write permissions (like the default working directory) and importing from there, but I recommend the following lower-hassle option.<\/p>\n\n<p><a href=\"https:\/\/notebooks.azure.com\" rel=\"nofollow noreferrer\">Azure Notebooks<\/a> is a separate Jupyter Notebook service that will allow you to install tensorflow, theano, and keras. Like the notebooks in AML Studio, these notebooks will persist in your account. The primary downside is that if you want to access your workspace through e.g. the Python <code>azureml<\/code> package, you'll need to <a href=\"https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python\" rel=\"nofollow noreferrer\">provide your workspace id\/authorization token<\/a> to set up the connection. (In Azure ML Studio, those values are loaded automatically from the current workspace.) Otherwise I believe Azure Notebooks can do everything you are used to doing inside AML Studio only.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2017-02-01 13:41:08.903 UTC",
        "Answer_score":3.0,
        "Owner_location":"Toronto, ON, Canada",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":51343746,
        "Question_title":"How to check if gremlin is properly connected to aws neptune instance",
        "Question_body":"<p>I have launched an aws neptune instance and installed <em>apache-tinkerpop-gremlin-console<\/em> version 3.3.3 on windows 10 machine.<\/p>\n\n<p>neptune-remote.yml looks like:<\/p>\n\n<pre><code>hosts: [abc-nept.XXXXXX.us-XXXX-1.neptune.amazonaws.com]\nport: 8182\nserializer: { className: org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV3d0, config: { serializeResultToString: true }}\n<\/code><\/pre>\n\n<p>after running <em>gremlin.bat<\/em> next command is:<\/p>\n\n<blockquote>\n  <p>:remote connect tinkerpop.server conf\/neptune-remote.yaml<\/p>\n<\/blockquote>\n\n<p>Now at this stage I am able to make queries and those are working! So question is how can I check whether I am actually <strong>connected to aws neptune instance or not?<\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2018-07-14 22:05:38.23 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2018-07-14 22:14:18.723 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|gremlin|amazon-vpc|tinkerpop3|amazon-neptune",
        "Question_view_count":757,
        "Owner_creation_date":"2014-04-16 06:07:57.69 UTC",
        "Owner_last_access_date":"2022-09-24 19:08:37.22 UTC",
        "Owner_reputation":7865,
        "Owner_up_votes":2392,
        "Owner_down_votes":1,
        "Owner_views":1629,
        "Answer_body":"<p>I assume your question is related to having multiple <code>:remote<\/code> instances configured. Obviously, if you've simply created:<\/p>\n\n<pre><code>:remote connect tinkerpop.server conf\/neptune-remote.yaml\n<\/code><\/pre>\n\n<p>then the only place your data could be going to or coming from is Neptune. The Console does allow multiple <code>:remote<\/code> instance that you can switch between so if you also had one for a local Gremlin Server then you might want to confirm which one you're sending requests. You just do this:<\/p>\n\n<pre><code>gremlin&gt; :remote\n==&gt;Remote - Gremlin Server - [localhost\/127.0.0.1:8182]\n<\/code><\/pre>\n\n<p>You'll be able to see the \"current\" <code>:remote<\/code> and thus know whether it is for Neptune or your local Gremlin Server instance.<\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_date":"2018-07-15 10:30:10.077 UTC",
        "Answer_score":3.0,
        "Owner_location":"Karachi, Pakistan",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":66390251,
        "Question_title":"Output Filter for CSV files in AWS Batch Transform",
        "Question_body":"<p>I have been trying to do a batch transform job and I have been getting some issues when joining the input and output into a single file (Input\/Output filtering and data joins), but first allow me to provide some context:<\/p>\n<ol>\n<li>The model used is XGBoost with output multi:softprob, let's say with N classes.<\/li>\n<li>The input and output types are &quot;text\/csv&quot;, split and assembled with option &quot;Line&quot; respectively.<\/li>\n<\/ol>\n<p>First, I filter the input data like this, extracting the first 2 items of the csv row.\nInput filter:\n$[2:]<\/p>\n<p>Then I select that I want to merge the input and output data.<\/p>\n<p>Now, assume that I have N = 20 classes.<\/p>\n<p>I could use an output filter like this, to get the 2 results filtered of the input and the last\n20 of the output.<\/p>\n<p>$[0,1,-20,-19,-18,...,-3,-2,-1]<\/p>\n<p>Unfortunately this previous filter expression is longer than 63 characters, and therefore it isn't allowed.<\/p>\n<p>Then I tried using slicing, based on jsonPath and Batch Transform documentation, like this:<\/p>\n<p>$[-20:2]<\/p>\n<p>But, apparently an &quot;Internal server error&quot; occurs when I do this. Nothing is shown in the logs, other than that.<\/p>\n<p>Yes, that's literally the error shown in the logs of the job, like this:<\/p>\n<p>s3:\/\/bucket\/file1.csv: Internal Server Error<\/p>\n<p>When I try a smaller filter, like $[0,1,-20,-1], it works fine, meaning that the model works great and that every other configuration is correct. But of course what I actually want to obtain are ALL the probabilities for the 20 (N) classes.<\/p>\n<p><strong>Any suggestion for the output filter in a way that works for N classes and with csv files?<\/strong><\/p>\n<p>Thank you in advance.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_date":"2021-02-26 17:24:17.663 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|batch-processing|amazon-sagemaker",
        "Question_view_count":220,
        "Owner_creation_date":"2020-12-15 17:44:45.16 UTC",
        "Owner_last_access_date":"2021-07-22 16:27:06.457 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":3562161,
        "Question_title":"Assignment of mercurial global changeset id",
        "Question_body":"<p>Apparently Mercurial assigns a global changeset id to each change. How do they ensure that this is unique?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2010-08-25 01:04:20.307 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2012-03-30 12:27:11.287 UTC",
        "Question_score":3,
        "Question_tags":"mercurial|dvcs|sha1|changeset",
        "Question_view_count":655,
        "Owner_creation_date":"2009-08-30 02:12:42.007 UTC",
        "Owner_last_access_date":"2022-03-30 02:31:02.987 UTC",
        "Owner_reputation":109688,
        "Owner_up_votes":2961,
        "Owner_down_votes":288,
        "Owner_views":7115,
        "Answer_body":"<p>As Zach says, the changeset ID is computed using the <a href=\"http:\/\/en.wikipedia.org\/wiki\/SHA-1\" rel=\"noreferrer\">SHA-1 hash function<\/a>. This is an example of a cryptographically secure hash function. Cryptographic hash functions take an input string of arbitrary length and produces a fixed-length digest from this string. In the case of SHA-1, the output length is fixed to 160 bit, of which Mercurial by default only shows you the first 48 bit (12 hexadecimal digits).<\/p>\n\n<p>Cryptographic hash functions have the property that it is extremely difficult to find two different inputs that produce the same output, that is, it is hard to find strings <code>x != y<\/code> such that <code>H(x) == H(y)<\/code>. This is called collision resistance.<\/p>\n\n<p>Since Mercurial uses the SHA-1 function to compute the changeset ID, you get the same changeset ID for identical inputs (identical changes, identical committer names and dates). However, if you use different inputs (<code>x != y<\/code>) when you will get different outputs (changeset IDs) because of the collision resistance.<\/p>\n\n<p>Put differently, if you do not get different changeset IDs for different input, then you have found a collision for SHA-1! So far, nobody has ever found a collision for SHA-1, so this will be a major discovery.<\/p>\n\n<hr>\n\n<p>In more detail, the SHA-1 hash function is used in a recursive way in Mercurial. Each changeset hash is computed by concatenating:<\/p>\n\n<ul>\n<li>manifest ID<\/li>\n<li>commit username<\/li>\n<li>commit date<\/li>\n<li>affected files<\/li>\n<li>commit message<\/li>\n<li>first parent <em>changeset ID<\/em><\/li>\n<li>second parent <em>changeset ID<\/em><\/li>\n<\/ul>\n\n<p>and then running SHA-1 on all this (see <a href=\"http:\/\/selenic.com\/hg\/file\/c00f03a4982e\/mercurial\/changelog.py#l231\" rel=\"noreferrer\">changelog.py<\/a> and <a href=\"http:\/\/selenic.com\/hg\/file\/c00f03a4982e\/mercurial\/revlog.py#l1107\" rel=\"noreferrer\">revlog.py<\/a>). Because the hash function is used recursively, the changeset hash will fix the entire history all the way back to the root in the changeset graph.<\/p>\n\n<p>This also means that you wont get the same changeset ID if you add the line <code>Hello World!<\/code> to two different projects at the same time with the same commit message -- when their histories are different (different parent changesets), the two new changesets will get different IDs.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2010-08-25 08:02:05.18 UTC",
        "Answer_score":8.0,
        "Owner_location":"Sydney, Australia",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":71030280,
        "Question_title":"Error : cc.fr.300.bin cannot be opened for loading",
        "Question_body":"<p>I am using Azure Machine Learning and Azure Databricks.\nIn Azure Databricks I have a script.py written by %% command (%%write script.py).<\/p>\n<p>In this script I tried to load cc.fr.300.bin that is saved as a model in Azure Machine Learning.<\/p>\n<p>I did this:<\/p>\n<pre><code>import fasttext\nfr_model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'cc.fr.300.bin')\nfr_model = fasttext.load_model(fr_model_path)\n<\/code><\/pre>\n<p>But I have the error :<\/p>\n<pre><code>File &quot;\/structure\/azureml-app\/script.py&quot;, line 134, in init\n    fr_model = fasttext.load_model(fr_model_path)\n  File &quot;\/azureml-envs\/azureml_d7...\/lib\/python3.6\/site-packages\/fasttext\/FastText.py&quot;, line 441, in load_model\n    return _FastText(model_path=path)\n  File &quot;\/azureml-envs\/azureml_d7...\/lib\/python3.6\/site-packages\/fasttext\/FastText.py&quot;, line 98, in __init__\n    self.f.loadModel(model_path)\nValueError: \/var\/azureml-app\/azureml-models\/test\/1\/cc.fr.300.bin cannot be opened for loading!\n<\/code><\/pre>\n<p>What can I do ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-08 07:57:11.253 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":-2,
        "Question_tags":"azure-machine-learning-service|fasttext",
        "Question_view_count":548,
        "Owner_creation_date":"2021-11-29 12:42:01.32 UTC",
        "Owner_last_access_date":"2022-09-21 11:05:53.383 UTC",
        "Owner_reputation":151,
        "Owner_up_votes":54,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":67814409,
        "Question_title":"How to run different Jupyter notebooks conditionally in a AWS Sagemaker notebook instance",
        "Question_body":"<p>I have a AWS sagemaker notebook instance which have 2 different jupyter notebooks. There are certain conditions in which each of it should work.<\/p>\n<p>So, if the consition A exist, the Jupyter Notebook 1 should run and if Condition B exist, the Jupyter Notebook 2 should run.<\/p>\n<p>I have tried this code so far, but it doesnt work:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>if condition A:\n\n    sm_client = boto3.client('sagemaker')\n    notebook_instance_name = NotebookInstanceName\n    notebook_instance_name = 'Calorie-1615128222'\n    url = sm_client.create_presigned_notebook_instance_url(NotebookInstanceName=notebook_instance_name)['AuthorizedUrl']\n    \n    print(url)\n\n    url_tokens = url.split('\/')\n    http_proto = url_tokens[0]\n    http_hn = url_tokens[2].split('?')[0].split('#')[0]\n\n    s = requests.Session()\n    r = s.get(url)\n    cookies = &quot;; &quot;.join(key + &quot;=&quot; + value for key, value in s.cookies.items())\n    print(cookies)\n\n    ws = websocket.create_connection(\n        &quot;wss:\/\/{}\/terminals\/websocket\/1&quot;.format(http_hn),\n        cookie=cookies,\n        host=http_hn,\n        origin=http_proto + &quot;\/\/&quot; + http_hn\n    )\n    \n    print(ws)\n    \n    # ws = websockets.connect(&quot;wss:\/\/{}\/terminals\/websocket\/1&quot;.format(http_hn))\n    \n    ws.send(&quot;&quot;&quot;[ &quot;stdin&quot;, &quot;jupyter nbconvert --execute --to notebook --inplace \/home\/ec2-user\/SageMaker\/Calorie\/Notebook1.ipynb \n--ExecutePreprocessor.kernel_name=conda_tensorflow2_p36 --ExecutePreprocessor.timeout=1500\\\\r&quot; ]&quot;&quot;&quot;)\n    #ws.send(&quot;&quot;&quot;[ &quot;stdin&quot;, &quot;jupyter nbconvert --execute Notebook1.ipynb --ExecutePreprocessor.kernel_name=conda_tensorflow2_p36 --ExecutePreprocessor.timeout=1500\\\\r&quot; ]&quot;&quot;&quot;)\n    \n    time.sleep(5)\n    ws.close()\n    print(&quot;websocket client created&quot;)\n    #return None\n    \n<\/code><\/pre>\n<p>Please help. Many Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-03 01:44:08.107 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|jupyter-notebook|amazon-sagemaker|aws-sam",
        "Question_view_count":413,
        "Owner_creation_date":"2018-04-01 04:41:44.627 UTC",
        "Owner_last_access_date":"2022-08-24 00:30:14.56 UTC",
        "Owner_reputation":813,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":172,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Australia",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":59363969,
        "Question_title":"mlflow: problems with pip installation",
        "Question_body":"<p>I read through many threads regarding installation issues using pip. However, I could find a solution to help me fix my problem.\nI installed mlflow with :<\/p>\n\n<pre><code>    pip3 install mlflow\n<\/code><\/pre>\n\n<p>so mlflow is installed in \/usr\/local\/bin\/mlflow<\/p>\n\n<p>Since it is not in \/Users\/xxxx\/opt\/anaconda3\/lib\/python3.7\/site-packages, I get \"ModuleNotFoundError: No module named 'mlflow' error when I try to run code that imports mlflow module. How should I fix this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-12-16 20:37:11.793 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"pip|python-import|python-3.7|importerror|mlflow",
        "Question_view_count":9843,
        "Owner_creation_date":"2018-04-26 22:59:32.553 UTC",
        "Owner_last_access_date":"2021-12-14 18:54:28.437 UTC",
        "Owner_reputation":87,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":20,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"San Francisco, CA, USA",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":53165953,
        "Question_title":"ValueError: Tensor is not an element of this graph, when hosting a model in Sagemaker with Gunicorn and Flask and Keras",
        "Question_body":"<p>I created a time series predictor with Keras and  Dockerized the model with with Flash and Gunicorn as per AWS docs. I am loading the serialized model with this code.<\/p>\n\n<pre><code>@classmethod\ndef get_model(cls):\n    if cls.model == None:\n        cls.model = load_model('\/opt\/ml\/bitcoin_model.h5')\n    return cls.model\n<\/code><\/pre>\n\n<p>Then I used the predict method to produce the results , the dockerized container is working perfectly in the local environment , but when I try to host the model in sagemaker it produces this error.<\/p>\n\n<pre><code>ValueError: Tensor Tensor(\"dense_1\/BiasAdd:0\", shape=(?, 1), dtype=float32) is not an element of this graph.\n<\/code><\/pre>\n\n<p>So how can I resolve this issue ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-11-06 04:57:19.33 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-11-06 05:07:40.787 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|keras|amazon-sagemaker",
        "Question_view_count":223,
        "Owner_creation_date":"2016-04-06 04:30:54.45 UTC",
        "Owner_last_access_date":"2022-09-25 05:42:06.587 UTC",
        "Owner_reputation":9168,
        "Owner_up_votes":1086,
        "Owner_down_votes":5,
        "Owner_views":675,
        "Answer_body":"<p>The issue was resolved by calling _make_predict_function() method in the model load phase.<\/p>\n\n<pre><code>@classmethod\ndef get_model(cls):\n    if cls.model == None:\n        cls.model = load_model('\/opt\/ml\/bitcoin_model.h5')\n        cls.model._make_predict_function()\n    return cls.model\n<\/code><\/pre>\n\n<p>Bug Reference : <a href=\"https:\/\/github.com\/keras-team\/keras\/issues\/6462\" rel=\"nofollow noreferrer\">https:\/\/github.com\/keras-team\/keras\/issues\/6462<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-11-06 04:57:19.33 UTC",
        "Answer_score":1.0,
        "Owner_location":"Frankfurt, Germany",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":11273260,
        "Question_title":"Advantages of GitHub over Bitbucket for Git Repositories",
        "Question_body":"<p>Now that <a href=\"https:\/\/bitbucket.org\/\" rel=\"noreferrer\">Bitbucket<\/a> also supports Git repositories, it seams to me that it is a good alternative to <a href=\"https:\/\/github.com\/\" rel=\"noreferrer\">GitHub<\/a>, especially since its free plan includes unlimited private repositories, which is not available on GitHub. Yet, GitHub seams much more popular.<\/p>\n\n<p>Are there any major reasons to choose GitHub as the hosting site for Git repositories instead of Bitbucket?<\/p>\n\n<p>(Although I have no problems with making my personal projects publicly available in general, I like the idea of being able to make the switch from public to private or vice versa any time I want. But if there are some good reasons to use GitHub, I would be willing to give up this freedom.)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2012-06-30 10:10:23.11 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2012-07-01 09:10:53.48 UTC",
        "Question_score":11,
        "Question_tags":"git|version-control|github|dvcs|bitbucket",
        "Question_view_count":3524,
        "Owner_creation_date":"2012-05-04 10:08:03.777 UTC",
        "Owner_last_access_date":"2022-09-24 10:23:13.187 UTC",
        "Owner_reputation":9517,
        "Owner_up_votes":1768,
        "Owner_down_votes":185,
        "Owner_views":747,
        "Answer_body":"<p>This should probably be community wiki, since it's subjective but ultimately I think of use to the community.<\/p>\n\n<p>GitHub's greatest strength is that it's widely in use, and also supported by more 3rd parties. For example, continuous integration services like <a href=\"http:\/\/travis-ci.org\" rel=\"nofollow\">Travis CI<\/a> or <a href=\"https:\/\/buildhive.cloudbees.com\/\" rel=\"nofollow\">BuildHive<\/a> feature transparent integration with GitHub.<\/p>\n\n<p>Personally, I use GitHub for public code because it's pretty widely used and supported and use <a href=\"http:\/\/codeplane.com\" rel=\"nofollow\">Codeplane<\/a> for my private code, because $9 a month for unlimited repos is pretty good.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2012-06-30 10:39:52.3 UTC",
        "Answer_score":7.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2012-10-19 13:34:41.797 UTC",
        "Question_valid_tags":[

        ]
    },
    {
        "Question_id":68703802,
        "Question_title":"Simplest GUI frontend to demo GCP Vertex AutomML Image Classification",
        "Question_body":"<p>I've built a GCP Vertex AutoML Image Classification model and deployed to endpoint. It works great from the Deploy and Test tab. What's the simplest way to let others without access to the project try it via a GUI? The required functionality is to upload an image from your computer and let the model output the predicted class.<\/p>\n<p>Is there an existing tool I can use (has to be a GUI, not command line)? If not what's the simplest way to build such frontend?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-08 18:49:05.48 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"vertex|google-cloud-automl|automl|google-cloud-vertex-ai|gpc",
        "Question_view_count":79,
        "Owner_creation_date":"2016-01-09 01:34:07.89 UTC",
        "Owner_last_access_date":"2022-02-26 19:24:23.073 UTC",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":41451123,
        "Question_title":"An example to create a training model on real data in AzureML",
        "Question_body":"<p>Can you introduce a real sample for azure ML and show how can it be possible to see the result of the training?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2017-01-03 19:40:53.41 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"machine-learning|azure-machine-learning-studio",
        "Question_view_count":76,
        "Owner_creation_date":"2014-06-23 20:02:17.94 UTC",
        "Owner_last_access_date":"2022-09-24 13:03:15.667 UTC",
        "Owner_reputation":17835,
        "Owner_up_votes":636,
        "Owner_down_votes":1612,
        "Owner_views":2203,
        "Answer_body":"<p><a href=\"http:\/\/blog.learningtree.com\/how-to-build-a-predictive-model-using-azure-machine-learning\/\" rel=\"nofollow noreferrer\">Here<\/a> is a good sample to create your first model.\nI should notice that I can't load data from url, as there is a forbidden error to load from url, and I don't know why! \nAnyhow, you can import data manually by copy the data from <a href=\"http:\/\/blog.learningtree.com\/wp-content\/uploads\/2015\/01\/breast-cancer-wisconsin.data_.arff_.txt\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Also, you can find the created model which is published here: \n<a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Cancer-Model-1\" rel=\"nofollow noreferrer\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Cancer-Model-1<\/a><\/p>\n\n<p>About see the result of the training model, you can right click on the tick (highlighted by a red circle in the following picture) of Evaluation Model. Then, in the opened menu, go to \"Evaluation Result -> Visualization\".\n<a href=\"https:\/\/i.stack.imgur.com\/vhJWE.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vhJWE.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>After that you can see a window like the following (which shows ROC curve and some related result such as accuracy of the training model):<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/FAuzU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FAuzU.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/B39lI.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/B39lI.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Besides, you can see <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-azure-ml-customer-churn-scenario\" rel=\"nofollow noreferrer\">this example<\/a> as an another sample.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2017-01-03 19:40:53.41 UTC",
        "Answer_score":0.0,
        "Owner_location":"Belgium",
        "Answer_last_edit_date":"2017-01-04 13:50:37.233 UTC",
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":55731954,
        "Question_title":"read json file with Python from S3 into sagemaker notebook",
        "Question_body":"<p>I want to read a json file from S3 into a sagemaker notebook.<\/p>\n\n<p>I can do this with pandas with this code, and this works without error :<\/p>\n\n<pre><code>import json\nimport pandas as pd\nimport boto3\n\n\nprefix_source = 'folder'\n\ns3 = boto3.resource('s3')\nmy_bucket_source = s3.Bucket('bucket_source')\n\nfor obj in my_bucket_source.objects.filter(Prefix=prefix_source):\n        data_location = 's3:\/\/{}\/{}'.format(obj.bucket_name, obj.key)\n        data = pd.read_json(data_location, lines = True )\n        display(data.head())\n<\/code><\/pre>\n\n<p>but I don't want to use pandas, I want to use Python <\/p>\n\n<p>I tried this code<\/p>\n\n<pre><code>for obj in my_bucket_source.objects.filter(Prefix=prefix_source):\n        data_location = 's3:\/\/{}\/{}'.format(obj.bucket_name, obj.key)\n        with open(data_location, 'r') as f:\n            array = json.load(f)\n            display(array) \n<\/code><\/pre>\n\n<p>I got this error :<\/p>\n\n<p><strong>IOError: [Errno 2] No such file or directory<\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-04-17 16:12:27.827 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|json|amazon-s3|amazon-sagemaker",
        "Question_view_count":7094,
        "Owner_creation_date":"2015-10-25 11:08:01.667 UTC",
        "Owner_last_access_date":"2022-09-22 16:18:18.21 UTC",
        "Owner_reputation":737,
        "Owner_up_votes":101,
        "Owner_down_votes":0,
        "Owner_views":63,
        "Answer_body":"<p>Json.load() expect a local file system path \"\/...\", not an \"s3:\/\/\" URI.<br>\nSee answer here: <a href=\"https:\/\/stackoverflow.com\/a\/47121263\">https:\/\/stackoverflow.com\/a\/47121263<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-04-17 18:44:51.137 UTC",
        "Answer_score":2.0,
        "Owner_location":null,
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":64558347,
        "Question_title":"Not able to read Sagemaker Semantic Segmentation Model Batch Transformation Output file",
        "Question_body":"<p>Currently I have deployed a <strong>Semantic Segmentation model<\/strong> and an endpoint with which I am able to invoke and get inferences. Now, I am getting inferences for each image at a time.<\/p>\n<p>Now I want to try batch of images at a time using <strong>Batch Transform job<\/strong>. It worked perfectly fine but the images that is created is an <strong>.out file<\/strong> and I'm <strong>not able to open that file<\/strong> using any of the viz library like matplotlib imread, PIL Image and openCV imread. It all says not an image.<\/p>\n<p>Just wanted to understand <strong>what is the .out file ?<\/strong> and if it is an segmented mask image which is typically the output of a semantic segmentation model then how can I read that file.<\/p>\n<p><strong>My code for Batch Transformation:<\/strong><\/p>\n<pre><code>from sagemaker.predictor import RealTimePredictor, csv_serializer, csv_deserializer\n\nclass Predictor(RealTimePredictor):\n    \n    def __init__(self, endpoint_name, sagemaker_session=None):\n        super(Predictor, self).__init__(\n            endpoint_name, sagemaker_session, csv_serializer, csv_deserializer\n        )\n\nss_model = sagemaker.model.Model(role =role, image=training_image, model_data = model, predictor_cls=Predictor, sagemaker_session=sess)\n\ntransformer = ss_model.transformer(instance_count=1, instance_type='ml.c4.xlarge', output_path=batch_output_data)\n\ntransformer.transform(data=batch_input_data, data_type='S3Prefix', content_type='image\/png', split_type='None')\n\ntransformer.wait()\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_date":"2020-10-27 16:03:23.127 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"amazon-sagemaker|semantic-segmentation",
        "Question_view_count":206,
        "Owner_creation_date":"2014-06-03 18:52:59.517 UTC",
        "Owner_last_access_date":"2022-08-21 06:55:48.4 UTC",
        "Owner_reputation":4221,
        "Owner_up_votes":84,
        "Owner_down_votes":13,
        "Owner_views":441,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"India",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":73570230,
        "Question_title":"How to access Mlflow running on fargate (ECS) with only VPN in\/outbound rules from sagemaker notebook instance?",
        "Question_body":"<p><strong>Context:<\/strong><\/p>\n<p>I have deployed Mlflow on ECS(Fargate) using terraform using this public <a href=\"https:\/\/github.com\/Glovo\/terraform-aws-mlflow\" rel=\"nofollow noreferrer\">git-repo<\/a>. After deploying Mlflow which was publicly accessible using the link, I made some changes in the security group and changed in\/outbound rule to the only company VPN ips, now that link is only accessible under the VPN.<\/p>\n<p><strong>Question:<\/strong><\/p>\n<p>Now I have Sagemake notebook instance and want to access that link inside the notebook and the notebook is running on AWS internet(outside Company-VPN) and I'm not able to access that link. What could be the possible solution?<\/p>\n<p>I don't want to open access of Mlflow-link publicaly to accessible form anywhere on the internet.<\/p>\n<p><strong>Running this code on notebook:<\/strong><\/p>\n<pre><code>!pip install mlflow\nimport mlflow\nmlflow.set_tracking_uri(&quot;http:\/\/mlflow-mlp-xyz-xyz.eu-west-1.elb.amazonaws.com\/&quot;)\nmlflow.get_experiment_by_name('mlpmlflowlogger')\ncurrent_experiment=dict(mlflow.get_experiment_by_name('mlpmlflowlogger'))\nprint(current_experiment)\nexperiment_id=current_experiment['experiment_id']\nprint(experiment_id)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-09-01 13:31:23.46 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-ecs|amazon-vpc|aws-security-group|mlflow",
        "Question_view_count":31,
        "Owner_creation_date":"2017-11-29 13:03:57.98 UTC",
        "Owner_last_access_date":"2022-09-20 13:40:47.487 UTC",
        "Owner_reputation":813,
        "Owner_up_votes":87,
        "Owner_down_votes":1,
        "Owner_views":115,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Deggendorf, Germany",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":50219664,
        "Question_title":"Azure ML Experiment Batch Webservice Call Fails with Invalid Output Extension",
        "Question_body":"<p>I have an Azure webjob that is calling a ML training experiment via HttpRequests, leveraging the code generated in the ML webportal:<\/p>\n\n<pre><code>var request = new BatchExecutionRequest()\n            {\n                Inputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"input1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{trainingDataFileName}\"\n                        }\n                    },\n                },\n\n                Outputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"output1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = \"azureStorageConnectionString\",\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/Model_2018421.ilearner\"\n                        }\n                    },\n                },\n\n                GlobalParameters = new Dictionary&lt;string, string&gt;()\n                {\n                }\n            };\n<\/code><\/pre>\n\n<p>However, the request fails with the following message:<\/p>\n\n<blockquote>\n  <p>The blob reference:\n  experiments\/experimentId\/TenantId\/Model_2018421.ilearner\n  has an invalid or missing file extension. Supported file extensions\n  for this output type are: \\\\\".csv, .tsv, .arff\\\\\"<\/p>\n<\/blockquote>\n\n<p>I'm pretty confused about this, since it's written right the documentation all over the place that if I'm expecting a trained model to use \".ilearner\" as the file extension for the model.<\/p>\n\n<p>I've seen <a href=\"https:\/\/stackoverflow.com\/questions\/47920098\/use-azure-data-factory-updating-azure-machine-learning-models\">this question<\/a> asking about the same error leveraging the DataFactory, and also <a href=\"https:\/\/datascience.stackexchange.com\/questions\/27397\/azure-machine-learning-model-retraining-problem\">this question on datascience.stackexchange<\/a>. Neither one had any clues, answers, or other follow up.<\/p>\n\n<p>Any insight on what I'm missing would be greatly appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-05-07 17:37:17.43 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"c#|rest|azure|azure-machine-learning-studio",
        "Question_view_count":83,
        "Owner_creation_date":"2016-10-21 13:46:29.223 UTC",
        "Owner_last_access_date":"2022-09-21 14:30:39.16 UTC",
        "Owner_reputation":547,
        "Owner_up_votes":1387,
        "Owner_down_votes":1,
        "Owner_views":45,
        "Answer_body":"<p>For anyone looking for your \"Don't Overthink It\" moment of the day:<\/p>\n\n<p>I needed to provide TWO output blob file references:<\/p>\n\n<pre><code>var request = new BatchExecutionRequest()\n            {\n                Inputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"input1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{trainingDataFileName}.csv\"\n                        }\n                    },\n                },\n\n                Outputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"output1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{outputFileNameCsv}.csv\"\n                        }\n                    },\n                    {\n                        \"output2\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{outputFileNameIlearner}.ilearner\"\n                        }\n                    },\n                },\n\n                GlobalParameters = new Dictionary&lt;string, string&gt;()\n                {\n                }\n            };\n<\/code><\/pre>\n\n<p>There's an old saying in American English about not making assumptions, and I assumed the second output was an optional parameter used in batch operations. Since I'm not actually looking for more than one result from each call, I thought I was safe to remove the second output parameter.<\/p>\n\n<p>TL\/DR: Keep all the parameters the webservice portal's \"Consume\" tab generates, and make sure the first one is a .csv file reference.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-05-08 13:59:56.893 UTC",
        "Answer_score":0.0,
        "Owner_location":"Columbus, OH, United States",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":73030972,
        "Question_title":"Azure Machine Learning Service writing to AzureDataLakeGen2Datastore",
        "Question_body":"<p>I registered an azure storage account gen2 where datalake filesystem enabled as a datastore as<\/p>\n<pre><code>    _ = Datastore.register_azure_data_lake_gen2(workspace='xxx',\n                                                datastore_name='store-identifier',\n                                                filesystem= 'container',\n                                                account_name= 'xxx',\n                                                subscription_id= 'xxx',\n                                                resource_group= 'xxx',\n                                                overwrite=1)\n<\/code><\/pre>\n<p>I tried uploading files to this using<\/p>\n<pre><code> destination_store = Datastore.get(workspace, 'store-identifier');\n destination_store.upload(\n     src='data\/', target_path=&quot;data_science\/&quot;, overwrite=True\n )\n<\/code><\/pre>\n<blockquote>\n<p>AzureDataLakeGen2Datastore' object has no attribute 'upload'<\/p>\n<\/blockquote>\n<p>When i opened the underlying <code>AzureDataLakeGen2Datastore<\/code> it mentions that the class does not support an upload and asks users to use a <code>Dataset<\/code> instance to upload data. However  I tried both <code>Dataset.Tabular.upload<\/code> and <code>Dataset.Files.upload<\/code> however neither of them support support this functionality<\/p>\n<p>How should i proceed further. Is this functionality not yet supported via azure machine learning ?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1,
        "Question_creation_date":"2022-07-19 04:00:07.487 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-07-19 04:05:21.25 UTC",
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service|azureml-python-sdk",
        "Question_view_count":500,
        "Owner_creation_date":"2010-04-12 17:27:26.887 UTC",
        "Owner_last_access_date":"2022-09-24 02:56:17.55 UTC",
        "Owner_reputation":9826,
        "Owner_up_votes":1723,
        "Owner_down_votes":15,
        "Owner_views":1238,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"United States",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":63045395,
        "Question_title":"Machine learning in Azure: How do I publish a pipeline to the workspace once I've already built it in Python using the SDK?",
        "Question_body":"<p>I don't know where else to ask this question so would appreciate any help or feedback. I've been reading the SDK documentation for azure machine learning service (in particular <code>azureml.core<\/code>). There's a class called <code>Pipeline<\/code> that has methdods <code>validate()<\/code> and <code>publish()<\/code>. Here are the docs for this:<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py<\/a><\/p>\n<p>When I call <code>validate()<\/code>, everything validates and I call publish but it seems to only create an API endpoint in the workspace, it doesn't register my pipeline under Pipelines and there's obviously nothing in the designer.<\/p>\n<p>My question: I want to publish my pipeline so I just have to launch from the workspace with one click. I've built it already using the SDK (Python code). I don't want to work with an API. Is there any way to do this or would I have to rebuild the entire pipeline using the designer (drag and drop)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2020-07-23 01:14:16.67 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-07-23 22:41:42.14 UTC",
        "Question_score":2,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio|azure-machine-learning-service|azure-machine-learning-workbench",
        "Question_view_count":739,
        "Owner_creation_date":"2020-07-21 00:40:20.127 UTC",
        "Owner_last_access_date":"2021-05-28 02:17:28.527 UTC",
        "Owner_reputation":65,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":"<p>Totally empathize with your confusion. Our team has been working with Azure ML pipelines for quite some time but <code>PublishedPipelines<\/code> still confused me initially because:<\/p>\n<ul>\n<li>what the SDK calls a <code>PublishedPipeline<\/code> is called as a <code>Pipeline Endpoint<\/code> in the Studio UI, and<\/li>\n<li>it is semi-related to <code>Dataset<\/code> and <code>Model<\/code>'s <code>.register()<\/code> method, but fundamentally different.<\/li>\n<\/ul>\n<p><code>TL;DR<\/code>: all <code>Pipeline.publish()<\/code> does is create an endpoint that you can use to:<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb\" rel=\"nofollow noreferrer\">schedule<\/a> and <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-setup-versioned-pipeline-endpoints.ipynb\" rel=\"nofollow noreferrer\">version<\/a> Pipelines, and<\/li>\n<li>re-run the pipeline from other services via a REST API call (e.g. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/data-factory\/transform-data-machine-learning-service\" rel=\"nofollow noreferrer\">via Azure Data Factory<\/a>).<\/li>\n<\/ul>\n<p>You can see <code>PublishedPipelines<\/code> in the Studio UI in two places:<\/p>\n<ul>\n<li>Pipelines page :: Pipeline Endpoints tab<\/li>\n<li>Endpoints page :: Pipeline Endpoints tab<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/UQ6RS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/UQ6RS.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-23 01:35:57.44 UTC",
        "Answer_score":1.0,
        "Owner_location":null,
        "Answer_last_edit_date":"2020-07-23 01:45:50.207 UTC",
        "Question_valid_tags":[
            "azure-machine-learning-workbench",
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":36547545,
        "Question_title":"Sales prediction in Azure ML",
        "Question_body":"<p>I am very new to Azure Machine Learning things, one of our client use to sell some fresh products to business people. They have a 'suggested buy' system, a feature will suggest some quantities to buy based on customer's sales history.<\/p>\n\n<p>After client came to know about Microsoft's Azure ML, they want to use that prediction system to suggest quantities to customers.<\/p>\n\n<p>We have sales data with these columns,<\/p>\n\n<ul>\n<li>CustomerName <\/li>\n<li>ItemName <\/li>\n<li>OrderDate <\/li>\n<li>QuantityPurchased <\/li>\n<li>QuantitySold<\/li>\n<\/ul>\n\n<p>We would like customers have suggested quantity should come from Azure ML using the Sales Data.<\/p>\n\n<p>Can some one please suggest me how can I do this?<\/p>\n\n<p>Thanks much in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2016-04-11 11:33:27.933 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"azure|prediction|azure-machine-learning-studio",
        "Question_view_count":2649,
        "Owner_creation_date":"2014-08-25 04:55:03.18 UTC",
        "Owner_last_access_date":"2021-06-03 01:18:34.777 UTC",
        "Owner_reputation":722,
        "Owner_up_votes":35,
        "Owner_down_votes":5,
        "Owner_views":178,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_date":null,
        "Answer_score":null,
        "Owner_location":"Online",
        "Answer_last_edit_date":null,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    }
]