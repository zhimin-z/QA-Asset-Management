[
    {
        "Questiont_title":"How to pass environment variables in sagemaker tuner job",
        "Question_creation_time":1669725280762,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5aMFxhnLQeqMY39mlmYHjA\/how-to-pass-environment-variables-in-sagemaker-tuner-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":9,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Sagemaker training jobs support setting environment variables on-the-fly in the training job:\n\n \"Environment\": { \n      \"string\" : \"string\" \n   },\n\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html\n\nI did not find an equivalent for the tuner jobs:\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateHyperParameterTuningJob.html\n\nAccording to my testing, the SagemakerTuner in the python SDK simply ignores the environment variables set in the passed estimator.\n\nIs there any way to pass environment variables to the training jobs started by a tuner job programmatically, or is that currently unsupported?",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-29T15:42:27.973Z",
                "Answer_upvote_count":1,
                "Answer_body":"Thanks for raising this. Yes, as you point out the Environment collection is not supported in the underlying CreateHyperparameterTuningJob API and therefore the SageMaker Python SDK can't make use of it when running a tuner.\n\nAs discussed on the SM Py SDK GitHub issue here, you might consider using hyperparameters instead to pass parameters through to the job?\n\nIf you specifically need environment variables for some other process\/library, you could also explore setting the variables from your Python script (perhaps to map from hyperparam to env var?).\n\nOr another option could be to customize your container image to bake in the variable via the ENV command? For example to customize an existing AWS Deep Learning Container (framework container), you could:\n\nUse sagemaker.image_uris.retrieve(...) to find the base image URI for your given framework, version, region, etc. You'll need to authenticate Docker to this registry as well as your own Amazon ECR account.\nCreate a Dockerfile that takes this base image URI as an arg and builds FROM it, something like this example\nAdd the required ENV commands to bake in the (static) environment variables you need\ndocker build your custom container (passing in the base image URI as a --build-arg), upload it to Amazon ECR, and use in your SageMaker training job.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Prevent boto3.client('sagemaker').create_auto_ml_job() from deploying endpoint",
        "Question_creation_time":1669662766261,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGEnbfLBgQJKrbdKfizHkEw\/prevent-boto-3-client-sagemaker-create-auto-ml-job-from-deploying-endpoint",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":18,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"When I invoke the .create_auto_ml_job() method both with and without the optional ModelDeployConfig kwarg, the autopilot job deploys an endpoint using the best model. Is there a way to prevent the .create_auto_ml_job() method from behaving this way? I do not wish to deploy the best model to an endpoint, and do not wish to have to delete this endpoint.",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"Sagemaker Model Monitor - endPointInput:contentType is of type application\/octet-stream",
        "Question_creation_time":1669654298426,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0i6A69kwRlelvy3hhpum3w\/sagemaker-model-monitor-end-point-input-content-type-is-of-type-application-octet-stream",
        "Question_topic":[
            "Machine Learning & AI",
            "Developer Tools",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Monitoring & Logging",
            "Amazon SageMaker Clarify",
            "ML Ops with Amazon SageMaker and Kubernetes"
        ],
        "Question_upvote_count":0,
        "Question_view_count":13,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Calling the sagemaker model endpoint with contentType application\/octet-stream which is also being captured in Data Capture Logs. What would be the ideal way to transform the data such that model monitor can work properly? I understand that model monitor works with csv or json content types.\n\nAble to use a preprocessing script to transform the input and output to csv\/json from protobuf format. Can I change the contentType field also as part of pre-processing?",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"Sagemaker Pipeline strange warning message",
        "Question_creation_time":1669623728809,
        "Question_link":"https:\/\/repost.aws\/questions\/QUm9Ml6PX2QdOA5VIaDMblQg\/sagemaker-pipeline-strange-warning-message",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0,
        "Question_view_count":16,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Using Sagemaker's Python SDK 2.11 when I run my pipeline, I see this strange warning message:\n\n\/personal_dir\/lib\/python3.8\/site-packages\/sagemaker\/workflow\/pipeline_context.py:233: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n\n\nBefore, I ran the exact same pipeline script with LocalPipelineSession without any problems and without any kind of weird warning messages.\n\nThis is how I am creating the PipelineSession object:\n\ndef get_session(region, default_bucket):\n    boto_session = boto3.Session(region_name=region)\n    sagemaker_client = boto_session.client(\"sagemaker\")\n\n    return PipelineSession(\n        boto_session=boto_session,\n        sagemaker_client=sagemaker_client,\n        default_bucket=default_bucket\n    )\n\n\nIm getting the region in the following way:\n\nimport boto3\n\nregion = boto3.Session().region_name\n\n\nI have tried to search the web for the meaning of that warning message, but could not find anything. What does that warning message means?? Am I doing something wrong and what can I do to make that warning disapear",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-29T16:59:13.535Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi, this warning is simply to clarify that running in a pipeline session will defer execution of jobs - generating pipeline step definitions instead of kicking off the jobs straight away.\n\nFor example calls such as Estimator.fit() or Processor.run() when using a pipeline session won't start a job (or wait for it to complete, or stream logs from CloudWatch), just prepare a definition to build up a pipeline that can be started later.\n\nIf you're already familiar with how PipelineSession works, I would say you can ignore it :-) If not, can refer to the SDK docs here for more details.\n\nCould be that there's an inconsistency between LocalPipelineSession versus PipelineSession in showing the message? Or that you disagree this message should be at warning level... Either way I'd suggest raising an issue on the SageMaker Python SDK GitHub might be a good way to log that feedback with the team!",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Using Lambda for data processing - Sagemaker",
        "Question_creation_time":1669522752249,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-EUVNgsoRViEFMwlZbcIAg\/using-lambda-for-data-processing-sagemaker",
        "Question_topic":[
            "Storage",
            "Serverless",
            "Compute",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon Simple Storage Service",
            "AWS Lambda",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_upvote_count":0,
        "Question_view_count":20,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I have created a docker image which has Entrypoint as processing.py. This script is taking data from \/opt\/ml\/processing\/input and after processing putting it \/opt\/ml\/processing\/output folder.\n\nFor processing the data I should put the file in \/opt\/ml\/processing\/input from s3 and then pick processed file from \/opt\/ml\/processing\/output into S3.\n\nFollowing script in sagemaker is doing it properly:\n\nfrom sagemaker.processing import Processor, ProcessingInput, ProcessingOutput import sagemaker\n\ninput_data = 's3:\/\/sagemaker-ap-south-1-057036842446\/sagemaker\/Data\/Training\/Churn_Modelling.csv' output_dir = 's3:\/\/sagemaker-ap-south-1-057036842446\/sagemaker\/Outputs\/' image_uri = '057036842446.dkr.ecr.ap-south-1.amazonaws.com\/aws-docker-repo:latest' aws_role = sagemaker.get_execution_role()\n\nprocessor = Processor(image_uri= image_uri, role=aws_role, instance_count=1, instance_type=\"ml.m5.xlarge\")\n\nprocessor.run(inputs=[ProcessingInput( source=input_data, destination='\/opt\/ml\/processing\/input')], outputs=[ProcessingOutput(source='\/opt\/ml\/processing\/output', destination=output_dir)] )\n\nCould someone please guide how this can be executed with lambda function? It is not recognizing sagemaker package, second there is a challenge in placing file before the script execution and pick processed files.\n\nI am trying codepipeline to automate this operation.",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-27T22:22:23.442Z",
                "Answer_upvote_count":0,
                "Answer_body":"SageMaker Python SDK, as a third party Python package, is not naturally installed Lambda's default runtime. You need to install this package.\n\nYou can either follow steps in https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/python-package.html to form a zip archive or similarly install sagemaker Python SDK as a layer as in https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/configuration-layers.html\n\nThis should provide a similar runtime env as you container.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-11-28T09:53:48.959Z",
                "Answer_upvote_count":0,
                "Answer_body":"I have build a codepipeline where after build I am executing lambda in the deploy stage.\n\nIn Lamdba I am executing the docker image.\n\nTo execute docker image I need to first provide it file from s3 and after script processing file generated need to move to S3.\n\nIs it actually possible in Lambda limitations?",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"sagemakee endpoint failing with \"\"An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body\"\"",
        "Question_creation_time":1669407059790,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbzR_PqclRoK5dmfEzR53wg\/sagemakee-endpoint-failing-with-an-error-occurred-model-error-when-calling-the-invoke-endpoint-operation-received-client-error-413-from-primary-and-could-not-load-the-entire-response-body",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":64,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello, I have created sagemaker endpoint by following https:\/\/github.com\/huggingface\/notebooks\/blob\/main\/sagemaker\/20_automatic_speech_recognition_inference\/sagemaker-notebook.ipynb and this is failing with error \"\"An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body\"\".\n\nThe predict function returning me following error but CW log does not have any error details for the endpoint.\n\nModelError Traceback (most recent call last)\n\/tmp\/ipykernel_16248\/2846183179.py in\n2 # audio_path = \"s3:\/\/ml-backend-sales-call-audio\/sales-call-audio\/1279881599154831602.playback.mp3\"\n3 audio_path = \"\/home\/ec2-user\/SageMaker\/finetune-deploy-bert-with-amazon-sagemaker-for-hugging-face\/1279881599154831602.playback.mp3\" ## AS OF NOW have stored locally in notebook instance\n----> 4 res = predictor.predict(data=audio_path)\n5 print(res)\n\n~\/anaconda3\/envs\/amazonei_pytorch_latest_p37\/lib\/python3.7\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model, target_variant, inference_id)\n159 data, initial_args, target_model, target_variant, inference_id\n160 )\n--> 161 response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n162 return self._handle_response(response)\n163\n\n~\/anaconda3\/envs\/amazonei_pytorch_latest_p37\/lib\/python3.7\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n493 )\n494 # The \"self\" in this scope is referring to the BaseClient.\n--> 495 return self._make_api_call(operation_name, kwargs)\n496\n497 _api_call.name = str(py_operation_name)\n\n~\/anaconda3\/envs\/amazonei_pytorch_latest_p37\/lib\/python3.7\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n912 error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n913 error_class = self.exceptions.from_code(error_code)\n--> 914 raise error_class(parsed_response, operation_name)\n915 else:\n916 return parsed_response\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body. See https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/asr-facebook-wav2vec2-base-960h-2022-11-25-19-27-19 in account xxxx for more information.\n\n`",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-26T15:49:44.270Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello AmitKayal,\n\nI understand that you have successfully created an Endpoint. However, when you try to invoke this Endpoint you get the following error:\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body. See https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/asr-facebook-wav2vec2-base-960h-2022-11-25-19-27-19 in account xxxx for more information.\n\n\n\nAnd when you looked at your CloudWatch logs there was nothing related to this error. Let me know if I have misunderstood anything.\n\nThe ClientError 413 usually occurs when the payload size for the endpoint invocation exceeds the limit of 6 MB [1,2], this could be the reason why your Endpoint is throwing the error 413.\n\nIf your payload size is more than 6MB you can work around this by using either Batch Transform [3] or Asynchronous Inference [4]. Batch Transform can be used if you would like to process the request to your model in batches. With Batch Transform you have an option to define your own maximum payload size [5]. Otherwise, if you want to receive inference for each request to your model you can use Asynchronous Inference which takes up to 1 GB of payload size with the runtime of about 15 minutes [6]. Asynchronous Inference queues requests to your model and processes them asynchronously, this is ideal for payloads that are greater than 6MB but not more than 1GB.\n\nShould the suggested work around not work, I recommend that you open a Support case with AWS Technical Support [7]. The Technical Support team will be able to help you to further troubleshoot this issue.\n\nI trust this information finds you well. Should you have any further questions, please feel free to reach out.\n\nReferences:\n\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/245\nhttps:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html#limits_sagemaker\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-batch.html\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/async-inference.html\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/hosting-faqs.html#hosting-faqs-general\nhttps:\/\/support.console.aws.amazon.com\/support\/home?region=us-east-1#\/case\/create",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Getting TrainingJobName from Training step of Sagemaker Pipeline",
        "Question_creation_time":1669298009010,
        "Question_link":"https:\/\/repost.aws\/questions\/QU21YXYHCuRhip83ELnFlYHg\/getting-training-job-name-from-training-step-of-sagemaker-pipeline",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Lambda",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":77,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I am running a sagemaker pipeline with a training step. This whole setup runs from a Lambda function while I pass a few parameters to the pipeline.py file. To get the TrainingJobName from the training step, my code is step_train.__dict__['step_args']['TrainingJobName'] This works fine while I running it in sagemaker notebook but when I execute the same code to get the train job name from lambda, I get this error [ERROR] TypeError: '_StepArguments' object is not subscriptable [ERROR] TypeError: '_StepArguments' object is not subscriptable Traceback (most recent call last): File \"\/var\/task\/lambda_function.py\", line 46, in lambda_handler pipeline = create_pipeline(validated_api_input) File \"\/tmp\/pipeline.py\", line 105, in create_pipeline \"job_name\" : training_step['step_args']['TrainingJobName']\n\nHow do I resolve this?",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-27T12:32:12.557Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello there,\nThank you for contacting. The error specifically comes up when you are trying to use a subscript for a data-type which isn't subscript-able, in your case this is coming when you are treating the object _StepArguments as a dictionary. A workaround for the same will be to implement a __get_item___ function in your code. A sample for the same is as follows:\n\ndef __getitem__(self, key):\n        return self.__dict__[key]\n\n\nHowever, as this was working on Sage-maker instance by default, also share the Python version that you are using and if you are using the same version of library in both Lambda function and SageMaker.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-11-28T23:16:55.719Z",
                "Answer_upvote_count":0,
                "Answer_body":"_StepArguments is a new concept introduced in newer version of SageMaker Python SDK. If you want to keep you original code, you need to pin the same SDK version in Lambda as in your notebook instance. Normally, I guess you will be installing the latest version but there has been a compatibility-broken change since then.\n\nIf you are OK to code changes, you could follow https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-and-manage-steps.html#build-and-manage-properties to use properties of each step, which maps to Describe* response. You will still be able to retrieve any job info. This is actually the recommended way.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Column header is not showing when reading data from redshift to jupyter on Sagemaker",
        "Question_creation_time":1669209147429,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxjkMWSCuS2ezRSYihJWHnA\/column-header-is-not-showing-when-reading-data-from-redshift-to-jupyter-on-sagemaker",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon Redshift"
        ],
        "Question_upvote_count":0,
        "Question_view_count":32,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello,\n\nI am reading a specific table from a redshift database using redshift connector. when i am viewing the dataframe it does not show the column headers. It shows only numbers as the column headers.\n\nCan anyone help whats wrong here? we need to view the table headers",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"How to keep sagemaker inference warm-up",
        "Question_creation_time":1669198631982,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVZqbuLPeSLWOe3TobeZHtQ\/how-to-keep-sagemaker-inference-warm-up",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":28,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Calling sagemaker inference frequently (3-5 calls in a minute) reduces runtime duration from ~200ms to ~50ms, so it seems there is similar warm-up behaviour like in Lambda. Do you have any suggestions how to keep sagemaker inference responsive always fast?",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-23T11:39:43.954Z",
                "Answer_upvote_count":0,
                "Answer_body":"You may need to check where this acceleration comes from to determine the warm up process. In CloudWatch metrics, you have ModelLatency and OverheadLatency.\n\nSageMaker Endpoint has a front-end router which maintains some caches for meta data and credentials. If the requests are frequent enough, the cache will be retained and auto renewed. This will reduce the OverheadLatency.\n\nIf you see a big drop in ModeLatency with warm-up requests, this may mean your algorithm container could have been configured to retrain some temporary data longer.\n\nNormally, you could schedule an invocation Lambda with CloudWatch Alarms to target tracking the metricInvokationPerInstance. This will make sure you always maintain a certain invocation rate when idle and those fake requests could settle down when real requests are picking up.\n\nThe issue with warm-up is that we stops the normal auto-scaling process of endpoints. The endpoint may not scale down properly.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Error when saving custom metrics in SageMaker Experiments through SageMaker Pipelines Training Job",
        "Question_creation_time":1669166447689,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwKF_EXaJQH2kimV1TIxI9g\/error-when-saving-custom-metrics-in-sage-maker-experiments-through-sage-maker-pipelines-training-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0,
        "Question_view_count":33,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"IHAC that I am working on enabling sagemaker experiments through a training job using SageMaker Pipelines. The below is the logic inserted into the train script which was working fine a few days ago tracking custom metrics into the trial component created by SageMaker Pipelines.\n\n    try:\n        print('>>> Loading an existing trial component')\n        my_tracker = Tracker.load()\n        \n    except ValueError:\n        print('>>> Creating a new trial component')\n        my_tracker = Tracker.create()\n        \n    my_tracker.log_metric(\"mse:mse error\", mean_squared_error(valid_y, preds))\n    \n    my_tracker.close()\n\n\nHowever, since yesterday I am facing an error with running the same code with the following error:\n\nLoading an existing trial component Traceback (most recent call last): File \"training.py\", line 82, in <module> my_tracker = Tracker.load() File \"\/miniconda3\/lib\/python3.7\/site-packages\/smexperiments\/tracker.py\", line 161, in load _ArtifactUploader(tc.trial_component_name, artifact_bucket, artifact_prefix, boto3_session), AttributeError: 'NoneType' object has no attribute 'trial_component_name'\n\nI tried to change the versions of sagemaker and sagemaker-experiments to an older version but still see the same issue. This code works when I trigger just the training job out of SageMaker Pipelines but shows the above error when running through SageMaker Pipelines. Any pointers how to fix this?",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-23T04:36:15.726Z",
                "Answer_upvote_count":0,
                "Answer_body":"SageMaker Python SDK is using Boto3 as the backend. You may also want to roll back & pin the Boto3 version.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker g4 and g5 instances do not have working nvidia-drivers",
        "Question_creation_time":1669082188275,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBqYWuFr7SyC6P6Uae9LOww\/sagemaker-g-4-and-g-5-instances-do-not-have-working-nvidia-drivers",
        "Question_topic":[
            "Machine Learning & AI",
            "Compute"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "GPU Development"
        ],
        "Question_upvote_count":3,
        "Question_view_count":80,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am a heavy user of g4 and g5 instances on Sagemaker (notebook instances). Today when I tried to use the same instances as I always do I was met with the following when running nvidia-smi\n\nNVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n\nThese are all the exact same instances and workloads I have used before. The same message was found when trying to run on ec2 natively as well.",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-23T04:32:10.418Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nThis is a NVIDIA driver issue which affect all NVIDA functions.\n\nCould you please try using the following cmds to unblock\n\nsudo dkms remove nvidia\/510.47.03 --all\n\nsudo dkms install nvidia\/510.47.03 -k $(uname -r)\n\n\nPlease let me know if this would work.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"What should be suggested way to do object classification in a streamed video using pre-trained model with custom label?",
        "Question_creation_time":1669043896491,
        "Question_link":"https:\/\/repost.aws\/questions\/QUNQj9L8OfQOuWzjmEIhEFqQ\/what-should-be-suggested-way-to-do-object-classification-in-a-streamed-video-using-pre-trained-model-with-custom-label",
        "Question_topic":[
            "Analytics",
            "Media Services",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon Kinesis Video Streams",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Rekognition"
        ],
        "Question_upvote_count":0,
        "Question_view_count":48,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello Everyone,\n\nI'm trying to create new model or with the help of pre-trained model for classification which can support video streaming data.\n\nI've tried custom label in AWS Rekognition for classification but I could evaluate that model only with image data (not in a streaming mode). Is there a way to evaluate a model for object detection or classification in a streamed video data?\n\nI've used AWS kinesis video stream to stream my laptop cam to AWS kinesis video stream. When I try to integrate Kinesis video stream with AWS Rekognition, I understood that it supports only face detection and specific object detection like pet, person or package labels with the pre-trained model. Is my understanding correct? or is there a way to use pre-trained model for a custom labels in a video streaming data.\n\nCan anyone suggest me the better solution for this requirement?\n\nAny help would be appreciated.",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-28T23:46:42.457Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nIt seems from your question that that Rekognition Custom Labels is what you're looking for, but the only feature that is lacking is the ability to stream video. One option is to extract images from Kinesis Video Streams (here) and pass those images to Rekognition Custom Labels.\n\nIf you want to use off-the-shelf Rekognition models and directly stream video you are limited to either face detection or person, pet, package detection. Please let me know if you have anymore questions.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Pipelines - Is it possible to use a TransformStep with the Catboost Estimator ?",
        "Question_creation_time":1668677758132,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdkeWBFI3SXSA8QznUYgT1Q\/sagemaker-pipelines-is-it-possible-to-use-a-transform-step-with-the-catboost-estimator",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0,
        "Question_view_count":45,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi! I am trying to implement a Sagemaker Pipeline including the following steps (among other things):\n\nProcessingStep: processing script (PySparkProcessor) generating a train , validation and test dataset (csv)\nTrainingStep: model training, CatBoost Estimator (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/catboost.html)\nTransformStep: batch inference using the model on the test dataset (csv)\n\nThe TransformStep returns the following error: python3: can't open file 'serve': [Errno 2] No such file or directory\n\nI wonder if I'm using TransformStep in the wrong way or if, at the moment, the use of TransformStep with the CatBoost model has not been implemented yet.\n\nCode:\n\n[...]\npyspark_processor = PySparkProcessor(\n    base_job_name=\"sm-spark\",\n    framework_version=\"3.1\",\n    role=role_arn,\n    instance_type=\"ml.m5.xlarge\",\n    instance_count=12,\n    sagemaker_session=pipeline_session,\n    max_runtime_in_seconds=2400,\n)\n\nstep_process_args = pyspark_processor.run(\n    submit_app=os.path.join(\n        s3_preprocess_script_dir, \"preprocess.py\"\n    ),  # Hack to fix cache hit\n    submit_py_files=[os.path.join(\n        s3_preprocess_script_dir, \"preprocess_utils.py\"\n    ), os.path.join(\n        s3_preprocess_script_dir, \"spark_utils.py\"\n    )],\n    outputs=[\n        ProcessingOutput(\n            output_name=\"datasets\",\n            source=\"\/opt\/ml\/processing\/output\",\n            destination=s3_preprocess_output_path,\n        )\n    ],\n    arguments=[\"--aws_account\", AWS_ACCOUNT, \"--aws_env\", AWS_ENV, \"--project_name\", PROJECT_NAME, \"--mode\", \"training\"],\n)\n\nstep_process = ProcessingStep(\n    name=\"PySparkPreprocessing\",\n    step_args=step_process_args,\n    cache_config=cache_config,\n)\n\ntrain_model_id = \"catboost-classification-model\"\ntrain_model_version = \"*\"\ntrain_scope = \"training\"\ntraining_instance_type = \"ml.m5.xlarge\"\n\n# Retrieve the docker image\ntrain_image_uri = image_uris.retrieve(\n    region=None,\n    framework=None,\n    model_id=train_model_id,\n    model_version=train_model_version,\n    image_scope=train_scope,\n    instance_type=training_instance_type,\n)\n\n# Retrieve the training script\ntrain_source_uri = script_uris.retrieve(\n    model_id=train_model_id, model_version=train_model_version, script_scope=train_scope\n)\n\n# Retrieve the pre-trained model tarball to further fine-tune\ntrain_model_uri = model_uris.retrieve(\n    model_id=train_model_id, model_version=train_model_version, model_scope=train_scope\n)\n\ntraining_job_name = name_from_base(f\"jumpstart-{train_model_id}-training\")\n\n# Create SageMaker Estimator instance\ntabular_estimator = Estimator(\n    role=role_arn,\n    image_uri=train_image_uri,\n    source_dir=train_source_uri,\n    model_uri=train_model_uri,\n    entry_point=\"transfer_learning.py\",\n    instance_count=1,\n    instance_type=\"ml.m5.xlarge\",\n    max_run=360000,\n    hyperparameters=hyperparameters,\n    sagemaker_session=pipeline_session,\n    output_path=s3_training_output_path,\n    disable_profiler=True,  # The default profiler rule includes a timestamp which will change each time the pipeline is upserted, causing cache misses. If profiling is not needed, set disable_profiler to True on the estimator.\n)\n\n# Launch a SageMaker Training job by passing s3 path of the training data\nstep_train_args = tabular_estimator.fit(\n    {\n        \"training\": TrainingInput(\n            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n                \"datasets\"\n            ].S3Output.S3Uri\n        )\n    },\n    logs=True,\n    job_name=training_job_name,\n)\n\nstep_train = TrainingStep(\n    name=\"CatBoostTraining\",\n    step_args=step_train_args,\n    cache_config=cache_config,\n)\n\nscript_eval = ScriptProcessor(\n    image_uri=[MASKED],\n    command=[\"python3\"],\n    instance_type=\"ml.m5.xlarge\",\n    instance_count=1,\n    base_job_name=\"script-evaluation\",\n    role=role_arn,\n    sagemaker_session=pipeline_session,\n)\n\neval_args = script_eval.run(\n    inputs=[\n        ProcessingInput(\n            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n            destination=\"\/opt\/ml\/processing\/model\",\n        ),\n        ProcessingInput(\n            source=step_process.properties.ProcessingOutputConfig.Outputs[\n                \"datasets\"\n            ].S3Output.S3Uri,\n            destination=\"\/opt\/ml\/processing\/input\",\n        ),\n    ],\n    outputs=[\n        ProcessingOutput(\n            output_name=\"evaluation\",\n            source=\"\/opt\/ml\/processing\/evaluation\",\n            destination=s3_evaluation_output_path,\n        ),\n    ],\n    code=\"common\/evaluation.py\",\n)\n\nevaluation_report = PropertyFile(\n    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n)\n\nstep_eval = ProcessingStep(\n    name=\"Evaluation\",\n    step_args=eval_args,\n    property_files=[evaluation_report],\n    cache_config=cache_config,\n)\n\nmodel = Model(\n    image_uri=\"467855596088.dkr.ecr.eu-west-3.amazonaws.com\/sagemaker-catboost-image:latest\",\n    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n    sagemaker_session=pipeline_session,\n    role=role_arn,\n)\n\nevaluation_s3_uri = \"{}\/evaluation.json\".format(\n    step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n)\n\nmodel_step_args = model.create(\n    instance_type=\"ml.m5.large\",\n)\ncreate_model = ModelStep(name=\"CatBoostModel\", step_args=model_step_args)\n\nstep_fail = FailStep(\n    name=\"FailBranch\",\n    error_message=Join(\n        on=\" \", values=[\"Execution failed due to F1-score <\", 0.8]\n    ),\n)\n\ncond_lte = ConditionGreaterThanOrEqualTo(\n    left=JsonGet(\n        step_name=step_eval.name,\n        property_file=evaluation_report,\n        json_path=\"classification_metrics.f1-score.value\",\n    ),\n    right=f1_threshold,\n)\n\nstep_cond = ConditionStep(\n    name=\"F1ScoreCondition\",\n    conditions=[cond_lte],\n    if_steps=[create_model],\n    else_steps=[step_fail],\n)\n\n# Transform Job\ns3_test_transform_input = os.path.join(step_process.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"], \"test\")\n\ntransformer = Transformer(model_name=create_model.properties.ModelName,\n                          instance_count=1,\n                          instance_type=\"ml.m5.xlarge\",\n                          assemble_with=\"Line\",\n                          accept=\"text\/csv\",\n                          output_path=s3_test_transform_output_path,\n                          sagemaker_session=pipeline_session)\n\ntransform_step_args = transformer.transform(\n    data=s3_test_transform_input,\n    content_type=\"text\/csv\",\n    split_type=\"Line\",\n)\n\nstep_transform = TransformStep(\n    name=\"InferenceTransform\",\n    step_args=transform_step_args,\n)\n\n\n# Create and execute pipeline\nstep_transform.add_depends_on([step_process, create_model])\n\npipeline = Pipeline(\n    name=pipeline_name,\n    steps=[step_process, step_train, step_eval, step_cond, step_transform],\n    sagemaker_session=pipeline_session,\n)\n\npipeline.upsert(role_arn=role_arn, description=[MASKED])\nexecution = pipeline.start()\nexecution.wait(delay=60, max_attempts=120)",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-17T09:48:18.264Z",
                "Answer_upvote_count":1,
                "Answer_body":"I'd suggest to start out by debugging whether the model created by your pipeline actually deploys or transforms OK (just from notebook), because I think that's where your problem might be.\n\nAs shown in the sample notebooks for classification and regression, deploy(...) and similar calls for CatBoost (and other new JumpStart-based algorithms) require some extra parameters including inference image_uri and source_dir. Unlike, say, the XGBoost algorithm where only one image URI needs to be specified across training and inference - and no source scripts need to be bundled in at either training or inference time.\n\nI haven't been able to test for myself yet, but think you might be able to fix this by adding image_uri and source_dir (specifying the inference container and script bundle as shown in the example notebooks, which are different from the training ones) to your create_model(...) call.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-11-25T11:12:15.448Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks for your answer. I managed to build and run apipeline with the CatBoost model (jumpstart version) which includes:\n\npreprocessing\ntraining\nmodel registration\nbatch inference\n\nI encounter two difficulties that I wanted to bring up:\n\nUnexpected behavior when registering the model\n\nWhat I tried based on the documentation:\n\n[...]\n\n# Retrieve the inference docker container uri\ndeploy_image_uri = image_uris.retrieve(\n    region=None,\n    framework=None,\n    image_scope=\"inference\",\n    model_id=train_model_id,\n    model_version=train_model_version,\n    instance_type=inference_instance_type,\n)\n\n# Retrieve the inference script uri\ndeploy_source_uri = script_uris.retrieve(\n    model_id=train_model_id, model_version=train_model_version, script_scope=\"inference\"\n)\n\nmodel = Model(\n    image_uri=deploy_image_uri,\n    model_data=\"s3:\/\/[MASKED]\/output\/model.tar.gz\",\n    source_dir=deploy_source_uri ,\n    sagemaker_session=pipeline_session,\n    entry_point=\"inference.py\",\n    role=role_arn,\n)\n\n[...]\n\nregister_model_step_args = model.register(\n    content_types=[\"text\/csv\"],\n    response_types=[\"text\/csv\"],\n    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n    transform_instances=[\"ml.m5.xlarge\"],\n    model_package_group_name=model_package_group_name,\n    approval_status=\"Approved\",\n    model_metrics=model_metrics,\n)\n\n\nBy doing so, the execution of the pipeline returns the following error:\n\nboto3.exceptions.S3UploadFailedError: Failed to upload \/tmp\/tmp5jzb0338\/new.tar.gz to jumpstart-cache-prod-eu-west-3\/source-directory-tarballs\/catboost\/inference\/classification\/v1.1.1\/sourcedir.tar.gz: An error occurred (AccessDenied) when calling the CreateMultipartUpload operation: Access Denied\n\n\nAccording to the documentation (see source_dir in https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html?highlight=model#sagemaker.model.Model), the same s3 path is used to save the model as the one used as source of the files. This is a problem when using the jumpstart inference scripts because you obviously can't upload to this reserved bucket. The workaround I used is to download locally the tarball of the Catboost inference scripts and then specify a local path as the origin of the scripts.\n\nos.makedirs(\"tmp\/deploy_source_uri\", exist_ok=True)\nS3Downloader.download(deploy_source_uri, \"tmp\")\nos.system(\"tar -xf tmp\/sourcedir.tar.gz --directory tmp\/deploy_source_uri\")\n\nmodel = Model(\n    image_uri=deploy_image_uri,\n    model_data=\"s3:\/\/[MASKED]\/output\/model.tar.gz\",\n    source_dir=\"tmp\/deploy_source_uri\",\n    sagemaker_session=pipeline_session,\n    entry_point=\"inference.py\",\n    role=role_arn,\n)\n\n\nIs there a better way ?\n\nFailed to output csv files when using batch transform\n\nI managed to perform a batch transformation step with the previous catboost model. However, I was not able to produce files in csv format, only json format seems to be compatible with catboost inference scripts.\n\nWhat I wanted to do but does not work:\n\ntransformer = Transformer(\n    model_name=model_step.properties.ModelName,\n    instance_count=1,\n    instance_type=\"ml.m5.xlarge\",\n    strategy=\"MultiRecord\",\n    assemble_with=\"Line\",\n    output_path=s3_test_transform_output_path,\n    accept=\"text\/csv\",\n    max_concurrent_transforms=1,\n    max_payload=5,\n    sagemaker_session=pipeline_session,\n)\n\nstep_transform = TransformStep(\n    name=\"InferenceTransform\",\n    transformer=transformer,\n    inputs=TransformInput(\n        data=s3_test_transform_input,\n        content_type=\"text\/csv\",              \n        split_type=\"Line\",\n        input_filter=\"$[1:]\",\n        join_source=\"Input\"                         # Wanted to join input data to prediction in csv format\n    ),\n    depends_on=[model_step]\n)\n\n\nBy doing so, the execution of the pipeline returns the following error:\n\n2022-11-23 13:48:12,273 [INFO ] W-9000-model_1-stdout MODEL_LOG - Failed to do transform\n2022-11-23 13:48:12,273 [INFO ] W-9000-model_1-stdout MODEL_LOG - Traceback (most recent call last):\n2022-11-23 13:48:12,273 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File \"\/opt\/ml\/model\/code\/inference.py\", line 55, in transform_fn\n2022-11-23 13:48:12,273 [INFO ] W-9000-model_1-stdout MODEL_LOG -     return encoder.encode(output, accept)\n2022-11-23 13:48:12,273 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker_inference\/encoder.py\", line 108, in encode\n2022-11-23 13:48:12,274 [INFO ] W-9000-model_1-stdout MODEL_LOG -     return encoder(array_like)\n2022-11-23 13:48:12,274 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker_inference\/encoder.py\", line 79, in _array_to_csv\n2022-11-23 13:48:12,274 [INFO ] W-9000-model_1-stdout MODEL_LOG -     np.savetxt(stream, array_like, delimiter=\",\", fmt=\"%s\")\n2022-11-23 13:48:12,274 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File \"<__array_function__ internals>\", line 5, in savetxt\n2022-11-23 13:48:12,275 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/numpy\/lib\/npyio.py\", line 1380, in savetxt\n2022-11-23 13:48:12,274 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 272\n2022-11-23 13:48:12,275 [INFO ] W-9000-model_1 ACCESS_LOG - \/169.254.255.130:42106 \"POST \/invocations HTTP\/1.1\" 500 288\n2022-11-23 13:48:12,275 [INFO ] W-9000-model_1 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:379f03461a27,timestamp:null\n2022-11-23 13:48:12,276 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:379f03461a27,timestamp:null\n2022-11-23 13:48:12,276 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:11|#Level:Host|#hostname:379f03461a27,timestamp:null\n2022-11-23 13:48:12,276 [INFO ] W-9000-model_1-stdout MODEL_LOG -     raise ValueError(\n2022-11-23 13:48:12,276 [INFO ] W-9000-model_1-stdout MODEL_LOG - ValueError: Expected 1D or 2D array, got 0D array instead\n\n\nAfter analysis of the inference.py script of the jumpstart model, it seems that the implementation of transform_fn is not compatible with the generation of output in csv format (transform_fn in inference.py provides a dict (output variable) to encoder.encode(output, accept) which call np.savetxt(stream, array_like, delimiter=\",\", fmt=\"%s\"), array_like variable is thus a dict which is not compatible with np.savetxt).\n\nThe workaround I used is to output a json file like so:\n\ntransformer = Transformer(\n    model_name=model_step.properties.ModelName,\n    instance_count=1,\n    instance_type=\"ml.m5.xlarge\",\n    strategy=\"MultiRecord\",\n    assemble_with=\"Line\",\n    output_path=s3_test_transform_output_path,\n    accept=\"application\/json\",                                                 # JSON\n    max_concurrent_transforms=1,\n    max_payload=5,\n    sagemaker_session=pipeline_session,\n)\n\nstep_transform = TransformStep(\n    name=\"InferenceTransform\",\n    transformer=transformer,\n    inputs=TransformInput(\n        data=s3_test_transform_input,\n        content_type=\"text\/csv\",\n        split_type=\"Line\",\n        input_filter=\"$[1:]\",\n    ),                                                                # No more join_source :(\n    depends_on=[model_step]\n)\n\n\nIs what I'm trying to do with the CatBoost Jumpstart model not yet implemented or have I misused the pipeline objects?",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker studio performance issues - why so slow, or am I missing a trick?",
        "Question_creation_time":1668593899489,
        "Question_link":"https:\/\/repost.aws\/questions\/QULsv-qoydT_6tkC2cbdt_0w\/sagemaker-studio-performance-issues-why-so-slow-or-am-i-missing-a-trick",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Studio Lab"
        ],
        "Question_upvote_count":0,
        "Question_view_count":43,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi all, I am fairly new to sagemaker studio, but I am concerned with the speed of the model pipeline. When I run preprocessing (Tfidf), training and evaluating models locally they take under a minute, however when using sagemaker studio it has taken at least 13 minutes.\n\nI have been running an XgBoost model based on the Abalone pipeline example and running this with my data which is around 5500 rows of text data (the text data ~1.5k characters on average per example).\n\nThe below example shows multiple pipeline runs with a HyperparameterTuner and even with the ml.c5.18xlarge and the below hyperparameter ranges, I was only able to complete the pipeline in 14 min run time (this included pre-processing step, hp-tuning, model register and model evaluation). FYI even without the tuning it still took around 14 min.\n\nI was wondering if I am missing a trick here or is it just Sagemaker takes awhile to start? Any help would be much appreciated!\n\n xgb_train.set_hyperparameters(\n        objective='multi:softmax',\n        num_round=50,\n        max_depth=3,\n        eta=0.2,\n        gamma=4,\n        min_child_weight=6,\n        subsample=0.7,\n        silent=0,\n        num_class=18,\n        alpha=0.1904424349464699\n    )\n\n   hyperparameter_ranges = {\n                            \"lambda\": ContinuousParameter(0.01, 10, scaling_type=\"Logarithmic\"),\n                        }",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-17T10:35:57.596Z",
                "Answer_upvote_count":0,
                "Answer_body":"If I understand correctly:\n\nYou're using a multi-step SageMaker Pipeline based on the Abalone example, something like shown on the screenshot here in the docs... With a small dataset of 5,500 examples (but I guess ~8MB since lots of characters per example)\nYour comparison point is (pre-processing the TF-IDF), training and evaluating the models on local machine\n\n...right?\n\nSageMaker jobs (e.g. processing, training, transform) generally run on on-demand compute infrastructure: Meaning you can select the number and size of instances for each step, and only pay for the resources you use without having to manage clusters.\n\nOn the other hand, this means when a SageMaker job starts it needs to provision your compute and download your container image + scripts. In my experience, job start-up can take ~2-4 minutes or even longer, and depends on several factors (including instance type, container image size, data size and mode, etc). Reducing that time continues to be a priority for our engineering teams and a lot of progress has already been made since SageMaker was first launched.\n\nSo if your workload takes ~a minute on a modestly sized local machine, the majority of the pipeline run time you're seeing is likely dominated by infrastructure provisioning: Especially if you have a set of serial steps e.g. a pre-processing job, a training job, a transform job, etc. Switching to a larger instance type will typically be little help if your actual training\/processing workload is not the bottleneck: But some instance types might be faster to provision than others.\n\nThere are many benefits to running your ML steps through SageMaker jobs rather than locally, and chaining those jobs through automated pipelines. For example:\n\nAutomated tracking of parameters, artifacts, and lineage\nConfidence that each job is running in a clean, reproducible, containerised environment\nIntegration of logging and metrics\nOther useful features like SageMaker Distributed for large workloads, SageMaker Debugger, and so on\nMulti-step pipeline automation and the ability to e.g. run your pipelines on a regular schedule, or version control the pipeline definition.\n\nIn my experience this extra governance and tooling is useful for production-ready workflows, even if infrastructure introduces some extra end-to-end latency... But of course every situation is different.\n\nIf the trade-off is particularly painful in your particular phase of your current project, and interactivity+speed is more important, some tips I could suggest:\n\nIt is still possible to prepare data, train and evaluate models directly in a SageMaker Studio notebook just like any other notebook environment... And you can switch your notebook's compute using the toolbar menu... Just be aware that the you won't be benefiting from the SageMaker features like job lineage\/experiment tracking.\nIf you wanted to still use SageMaker jobs but optimize your pipeline, you could probably squeeze everything into an XGBoostProcessor job so that your data pre-processing, model training, and evaluation happens all in one script... You could even register the model from that same script via boto3\/SageMaker Python SDK. But again you're losing the tracking benefits of SageMaker jobs.\nIf you're particularly interested in running multiple training jobs and re-using the infrastructure, check out the recently launched warm pooling feature which lets you keep a training cluster alive and re-use it for faster start-up in a follow-on job.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Clone Failed SageMaker MLOps Project Using Third-party Git Repos",
        "Question_creation_time":1668586359113,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhIvfTpxzRtWRE-dO20bHmQ\/clone-failed-sage-maker-ml-ops-project-using-third-party-git-repos",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0,
        "Question_view_count":37,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm trying to use MLOps template for model building, training, and deployment with third-party Git repositories using CodePipeline in my ML project. I created the project successfully using the template and all the seed code is available in the GitHub repos I specified. But when I try to clone the repo, I am getting the below error\n\nI see that the local path that has been specified in\n\nNo such file or directory: '\/home\/sagemaker-user\/home\/sagemaker-user\/cat-ml-test-1-p-mtd5ofsbdgva\/sagemaker-p-mtd5ofsbdgva-modeldeploy'\n\n\ndoes not sound quite right. But there's is no option to change the local path by myself also.\n\nHow can I solve this? Any leads are welcome TIA",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"AWS Ground Truth Plus Available Medical Expertise",
        "Question_creation_time":1668530819224,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1vINii7DRkGK-4klJOq7wA\/aws-ground-truth-plus-available-medical-expertise",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth Plus"
        ],
        "Question_upvote_count":0,
        "Question_view_count":28,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"On Dec 1, 2021, AWS put out a press release regarding SageMaker Ground Truth Plus that contained the statement:\n\nTo get started, customers simply point Amazon SageMaker Ground Truth Plus to their data source in Amazon Simple Storage Service (Amazon S3) and provide their specific labeling requirements (e.g. instructions for how medical experts should label anomalies in radiology images of lungs).\n\nCan AWS provide medical experts for labeling medical data? Or am I misinterpreting this statement and the services included in this \"turnkey\" solution. (BTW, I've already built and tested a custom segmentation task for SageMaker Ground Truth and am looking for \"expert\" labeling.)",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-15T19:01:33.076Z",
                "Answer_upvote_count":0,
                "Answer_body":"That's a poor choice of example on their part; the FAQ for the service explicitly states: \"Currently, Amazon SageMaker Ground Truth Plus is not a HIPAA eligible service.\"",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Is it possible SageMaker HyperParameter Tuning Job without sagemaker-training tool kit",
        "Question_creation_time":1668501531881,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrrvbV_wJRuqG9TX1KyoZnQ\/is-it-possible-sage-maker-hyper-parameter-tuning-job-without-sagemaker-training-tool-kit",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Model Training"
        ],
        "Question_upvote_count":0,
        "Question_view_count":22,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I had made my custom training image so It can be conducted through CreateTrainingJob, not sagemaker training took kit (requiring \"ContainerEntrypoint\" option).\n\nBut when I'm trying to run HyperParameter Tuning Job, it is not allowed to add \"ContainerEntrypoint\" option in \"AlgorithmSpecification\" field.\n\nIs it impossible to run HyperParameter Tuning Job with training images that can not be run without sagemaker training toolkit?\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-16T01:11:51.672Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi there!\n\nFor custom training image, you can specify the entrypoint in your Dockerfile.\n\nBelow are some code snippet as well as links you can use as reference:\n\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/hyperparameter_tuning\/rapids_bring_your_own\/code\/Dockerfile\n\nENTRYPOINT [\".\/entrypoint.sh\"]\n\n\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/hyperparameter_tuning\/keras_bring_your_own\/Dockerfile\n\nENTRYPOINT [\"python\", \"-m\", \"trainer.start\"]\n\n\nFurthermore, SageMaker Training Toolkit is a nicely wrapped up python package for you to use and eases the process of creating a custom training image, it's no different from implementing the logic yourself.\n\nSo it is definitely possible to run HyperParameter Tuning Job using custom containers without using our SageMaker Training Toolkit.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Is any component of Sagemaker still involved in production inferencing",
        "Question_creation_time":1668489528537,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVpIbYCjMTzOPTHJFfh9rnQ\/is-any-component-of-sagemaker-still-involved-in-production-inferencing",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Test & Optimize ML Models"
        ],
        "Question_upvote_count":0,
        "Question_view_count":36,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Are any components of the SageMaker Engine involved still during production ( inference) , I had a discussion with someone saying Sagemaker has nothing to do with production anymore because everything is deployed , containerized and executed somewhere in a VM or cluster., I think there are some instances the main engine is working in production and please correct me if i am wrong 1. Billing records - have to be generated -so something is connected to the engine ? does the model help here or is it just pure EC2 etc compute measured ? Who manages the endpoints ( server less, and asynch) ? if we need to use Sage maker monitor this is also alive during production. Question is What main components are active in production outside model or does mode communicate to engine still? As an addendum someone mentioned to me if it is deployed to EC2 or K8s yes it is disconnected from the main services in production you CANNOT use Model monitor in this scenario? --- but if you deploy to SAGEMAKER INFERENCE? you can use model monitor in prod. You deploy to an endpoint for real time... What is SageMAker Inference - as a concept or as some physical thing because there are 71 options in Sagemaker inference ?",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"how to access\/set up a model registry ?",
        "Question_creation_time":1668477799381,
        "Question_link":"https:\/\/repost.aws\/questions\/QUW-r2Vv8-RC-qtY34c3WobQ\/how-to-access-set-up-a-model-registry",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Build & Train ML Models",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "ML Ops with Amazon SageMaker and Kubernetes"
        ],
        "Question_upvote_count":0,
        "Question_view_count":25,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"based on aws docs\/examples (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-registry-version.html), one can create\/register model that is generated by your training pipeline. first we need to create a model package group ( sample code below). is model registry already set up by default in our account or do we have to create it as well? can you have more than one model registry in an account. I don't see , in the code below, where we are passing the model registry info. I am assuming , we simply create multiple training jobs and save the models in its own model group. is there a sample code\/repo, where there are multiple training steps that calls same model registry with a different group name ( i can use it as a sample). also, once the model is registered, how do i set up such that, a model can be accepted or declined, before promoting. can we set this up via code , please point me to any examples.\n\nimport time\ngroup_name = \"mygroup\"\ninput_dict = { \"ModelPackageGroupName\" : group_name}\n\nresponse =  client.create_model_package_group(**input_dict)\nprint('result :  {}'.format(response['ModelPackageGroupArn']))",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-15T12:54:41.012Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nModel package group and model registry are 2 different things. Model registry is already in your account but you have to create a model package group where you will have model packages (different versions of the model). see this notebook about using model registry. Your code above is basically creating a model group\/model package group in model registry, and in that model group you will have the different versions of your model.\n\nI don't have an example in mind, but you can easily have a code where you create different models and add them in the model groups accordingly. Following the notebook shared above , you would just add the same code but adapt it to another model group assuming that you have to two trainings happening each with a different purpose.\n\nTo summarize: model registry = N model groups and each model group = N model versions (model packages). Suppose you are doing a sentiment analysis task where you have different models per customer. You could create a Model Package Group per customer and in each group register Model Package Versions for each model created.\n\nYou can automate the testing of your model before registering into the model group by for example checking the loss metric. You can use SageMaker projects for an example that includes a pipeline. Check this example. You could basically have a conditional step in your pipeline.\n\nHope this helps and if I answered your question, please accept it.\n\nThank you",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Help with Inference Script for Amazon Sagemaker Neo Compiled Models",
        "Question_creation_time":1668430718144,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJvbkzp91TGSZO1GwW-r90w\/help-with-inference-script-for-amazon-sagemaker-neo-compiled-models",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":32,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hello everyone, I was trying to execute the example mentioned in the docs - https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_neo_compilation_jobs\/pytorch_torchvision\/pytorch_torchvision_neo.html. I was able to successfully run this example but as soon as I changed the target_device to jetson_tx2, after which I ran the entire script again, keeping the rest of the code as it is, the model stopped working. I was not getting any inferences from the deployed model and it always errors out with the message:\n\nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from <users-sagemaker-endpoint> with message \"Your invocation timed out while waiting for a response from container model. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\"                \n\n\nAccording to the troubleshoot docs https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo-troubleshooting-inference.html, this seems to be an issue of model_fn() function. The inference script used by this example is mentioned here https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_neo_compilation_jobs\/pytorch_torchvision\/code\/resnet18.py , which itself doesn't contain any model_fn() definition but it still worked for target device ml_c5. So could anyone please help me with the following questions:\n\nWhat changes does SageMaker Neo do to the model depending on target_device type? Since it seems the same model is loaded in a different way for different target device.\nIs there any way to determine how the model is expected to load for a certain target_device type so that I could define the model_fn() function myself in the same inference script mentioned above?\nAt-last, can anyone please help with the inference script for this very same model as mentioned in docs above which works for jetson_tx2 device as well.\n\nAny suggestions or links on how to resolve this issue would be really helpful.",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-18T10:08:12.280Z",
                "Answer_upvote_count":1,
                "Answer_body":"As you mentioned, you changed the Neo compiling target from ml_c5 to jetson_tx2, the compiled model will require runtime from jetson_tx2. If you kept other code unchanged, the model will be deployed to a ml.c5.9xlarge EC2 instance, which doesn't provide Nvida Jeston.\n\nThe model can't be loaded and will error out since Jestion is a device Nvidia GPU structure while c5 is only equipped with CPU. No CUDA environment.\n\nIf you compile the model with jeston_tx2 as target, you should download the model and run the compiled model in a real Nvidia Jeston device.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"AWS Sagemaker model endpoint data capture",
        "Question_creation_time":1668105022579,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJfA77iAoT2-EkVff7sc7LA\/aws-sagemaker-model-endpoint-data-capture",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":35,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have deployed a sagemaker endpoint, with data capture enabled with a 100% sampling rate. But after running it multiple times, it is not storing all the inputs to the model,\n\nFor instance, after running 3 times, it saves only one time.",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-14T12:08:34.106Z",
                "Answer_upvote_count":0,
                "Answer_body":"CLOSED,\n\nSolved something else was going wrong",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Ground Truth label - a word at the end of the sentence getting split into 2 parts",
        "Question_creation_time":1668086625248,
        "Question_link":"https:\/\/repost.aws\/questions\/QU50Od2mJZTjyEcGuGds1-qQ\/ground-truth-label-a-word-at-the-end-of-the-sentence-getting-split-into-2-parts",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth"
        ],
        "Question_upvote_count":0,
        "Question_view_count":15,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"We are annotating the pdf for extrating NER, The targetted entity word at the end of the sentence getting split into 2 parts. First part stay at the end of the first line and second part coming in second line. While annotating the tool doesn't allow the dragging to next line.\n\nAre there any work arounds available ?",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"Ground Truth Job Validation post completion",
        "Question_creation_time":1668077364552,
        "Question_link":"https:\/\/repost.aws\/questions\/QURI4l_IFoR9SZyc-6wNPqgQ\/ground-truth-job-validation-post-completion",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Ground Truth"
        ],
        "Question_upvote_count":1,
        "Question_view_count":21,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"We annotated 10 pdf files in Ground Truth, how do I validate the annotations done by the team ? Do we have any metrics ? Ex - How many annotations done in one pdf ? What is the confidence score for each annotation ?\n\nMy idea is that if i get this metrics, I will review the doc with less number of annotations and doc with low confidence score.\n\nCan Ground truth expers provide some insights in this ?",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-21T11:54:18.048Z",
                "Answer_upvote_count":0,
                "Answer_body":"This is a machine learning problem and goes beyond AWS Ground Truth.\n\nUsually you do not get to measure how confident each annotation is, unless you asked the annotators to say how confident they are on each annotation.\n\nUsually you have to provide some gold standard annotations that you think are correct. You do this on some sample PDFs that you annotated yourself. Then you check the performance of each annotator on your set. You can compute metrics such as Cohen's Kappa to assess agreement between annotators: https:\/\/en.wikipedia.org\/wiki\/Cohen%27s_kappa",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Ground Truth Label Jobs - Same document repeating in the labeling job",
        "Question_creation_time":1668073221576,
        "Question_link":"https:\/\/repost.aws\/questions\/QUu20cb3GSTPKBsnJsRDjbYg\/ground-truth-label-jobs-same-document-repeating-in-the-labeling-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Ground Truth"
        ],
        "Question_upvote_count":0,
        "Question_view_count":21,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"We want to extract the entities from pdf documents. We are exploring labeling the existing document using Safemaker GroundTruth. The documents are keep repeating for some reasons which we are not able to identify the reason ? When it repeats some pages coming as read only and other pages will us to annotate. When it repeats the previously read only pages are annotable .\n\nCan some expert from this help us to understand why it is repeating ?",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"SageMaker Experiments Deletion Help Needed",
        "Question_creation_time":1668037196721,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFMzl26gfQna8sAZcCDJw_Q\/sage-maker-experiments-deletion-help-needed",
        "Question_topic":[
            "Machine Learning & AI",
            "AWS Well-Architected Framework",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Security",
            "AWS Account Management"
        ],
        "Question_upvote_count":0,
        "Question_view_count":44,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi Friends,\n\nI have deleted everything in Sagemaker - but support is asking me to delete the experiments that are still in my account : they sent me a link to follow\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/experiments-cleanup.html\n\nbut I have no idea how to complete this task -- does anyone know in terms that someone who has no idea what this means - can follow and achieve this task\n\nyou have no idea how much it would mean to me for any assistance",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-10T09:00:37.516Z",
                "Answer_upvote_count":1,
                "Answer_body":"You need to use a computer with Python, the SageMaker SDK installed, and AWS credentials with enough permissions for that account configured. If you are already using SageMaker Studio, that should work.\n\nUse the second method. Create a file (Menu File -> New -> Python File). Rename it as cleanup_experiments.py(right click on the file on top and select Rename Python File), then paste the code in the documentation (those three sections, one after another). Save the file and open a terminal (Menu File -> New -> Terminal). Navigate to the directory where you saved the file and execute the command python cleanup_experiments.py",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"AsyncInferenceConfig takes different parameters",
        "Question_creation_time":1668012887011,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUrJbuMmWTni5RT-e-GCYbA\/async-inference-config-takes-different-parameters",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0,
        "Question_view_count":22,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I would like to deploy an async endpoint in SageMaker. However when trying to deploy it I get the following error: ParamValidationError: Parameter validation failed: Unknown parameter in input: \"AsyncInferenceConfig\"\n\nThis is the code I tried for deploying the endpoint\n\nfrom sagemaker.async_inference.async_inference_config import AsyncInferenceConfig\n\nasync_config = AsyncInferenceConfig(\n    output_path=\"s3:\/\/poembucketus\/async_inference\/output\",\n    max_concurrent_invocations_per_instance=4,\n)\n\nasync_predictor = huggingface_estimator.deploy(\n    initial_instance_count=1,\n    instance_type=\"ml.m5.xlarge\",\n    async_inference_config=async_config,\n)\n\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-09T21:23:53.989Z",
                "Answer_upvote_count":0,
                "Answer_body":"From the format of parameter, this error should be thrown from Boto3. If AsyncInferenceConfig is not recognized as a valid parameter, this may mean the version of boto3 is too old and released before the async feature.\n\nCould you please confirm the version of boto3 and try updating it to the latest if possible ?",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Access async endpoint created in console from notebook instance",
        "Question_creation_time":1668007391842,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwLeSd5B8RHS7RD7KQIl5TQ\/access-async-endpoint-created-in-console-from-notebook-instance",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0,
        "Question_view_count":15,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"In SageMaker, I've created an async endpoint from a model. How does one access this endpoint from a notebook instance so that it can be used to make predictions?\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-09T21:26:59.952Z",
                "Answer_upvote_count":0,
                "Answer_body":"You may consider using invoke_endpoint_async function from sagemakerruntime in boto3 SDK. Doc here Just make sure your notebook's execution role has proper IAM permissions and fill in required parameters, boto3 will handle the rest.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Directory Error when running SageMaker backup-ebs lifecycle for Amazon Linux 2 transition",
        "Question_creation_time":1667961704440,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPsaLux6EQcqvHW1HtNlkIw\/directory-error-when-running-sage-maker-backup-ebs-lifecycle-for-amazon-linux-2-transition",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon CloudWatch Logs"
        ],
        "Question_upvote_count":0,
        "Question_view_count":28,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I'm following this guide for transitioning to Amazon Linux 2 provided by AWS\n\nI've set up the two needed lifecycle configurations and created a new S3 Bucket to store the backup. I've also ensured the IAM roles have the required S3 permissions and updated the notebook with the ebs-backup-bucket tag per the instructions.\n\nWhen I run the notebook with the new configuration I get the following error: \"Notebook Instance Lifecycle Config [LIFECYCLE ARN] for Notebook Instance [NOTEBOOK ARN] took longer than 5 minutes. Please check your CloudWatch logs for more details if your Notebook Instance has Internet access.\n\nLooking at the logs I get the error: \/bin\/bash: \/tmp\/OnStart_2022-11-09-01-51ontlqcqt: \/bin\/bash^M: bad interpreter: No such file or directory\n\nAny thoughts on how to resolve this issue? The code for the backup lifecycle configuration can be found here",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-09T05:39:17.492Z",
                "Answer_upvote_count":1,
                "Answer_body":"The extra ^M symbol (i.e. Ctrl-M) stopped the whole scrip from being interpreted properly.\n\nThis issue is normally seen in scripts prepared in MSDOS\/Windows based system but used in Linux system due to difference of line endings.\n\nIn Unix based OS, lines end with \\n but MSDOS\/Win based system ends with \\r\\n\n\nIn Linux based system, you could show your prepared scripts by running\n\ncat -e some-script.sh \n\n\nThe results would be something similar to\n\n#!\/bin\/bash^M$\n... ...^M$\n\n\n$ is normal Unix end-of-line symbol. Windows uses an extra one ^M and this symbol is not recognized by Unix system. That's why, in SageMaker Notebook Lifecycle Configuration, which is running Linux, your script was interpreted as \/bin\/bash^M\n\nTo mitigate the issue, please convert the scripts to Unix based ending and update life cycle configuration. To achieve this, you could use Notepad++ in Windows. You can go to the Edit menu, select the EOL Conversion submenu, and from the options that come up select UNIX\/OSX Format. The next time you save the file, its line endings will, all going well, be saved with UNIX-style line endings.\n\nAlternatively, you could put the script in a Linux environment, e.g. EC2 instance with Amazon Linux 2 and install dos2unix via sudo yum install dos2unix. After installation, you could convert your files via\n\ndos2unix -n file.sh  output.sh \n\n\nAfter the conversion, please update LCC with the new scripts. You could verify that ^M has been removed via\n\ncat -e your_script.sh\n\n\nThe output will print all special characters directly without hiding.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Batch Transformation Mismatch -",
        "Question_creation_time":1667925703950,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJo7fF1tiRQiljqizFaC-jg\/batch-transformation-mismatch",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Pipelines",
            "Monitoring"
        ],
        "Question_upvote_count":0,
        "Question_view_count":43,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I performing batch transformation in a pipeline and joining it with the ground truth labels to create a ModelQuality monitoring step as per the guide here: https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-pipelines\/tabular\/model-monitor-clarify-pipelines\/sagemaker-pipeline-model-monitor-clarify-steps.ipynb\n\nOverview:\n\nUtilizing Sklearn pipeline tools for data handling and prediction (i.e. the sklearn pipeline module)\nUsing my own inference script (based on the build your own example provided by AWS)\nReading in csv file of data, which successfully loads and predicts labels (output ex: ['an01' 'hxn2' 'sv4' ... 'ngn' 'ssv' 'ssv'])\nTransformation step is unsuccessful and provides following message:\n\n2022-11-08T16:22:59.531:[sagemaker logs]: sagemaker-us-east-1-766029086407\/TEST-monitor-steps\/cipm6ea9musp\/projectKW-V3-Tune-TrainRegister-process\/output\/test\/test.csv: Fail to join data: mismatched line count between the input and the output\n\nI am having trouble understanding if the configurations of TransformInput provided to the step are wrong or if there is an error with the inference script itself.\n\nBelow is the code for the transformer step:\n\ntransformer = Transformer(\n    model_name=step_create_model.properties.ModelName,\n    instance_type=\"ml.m5.xlarge\",\n    instance_count=1,\n    accept=\"text\/csv\",\n    assemble_with=\"Line\",\n    output_path=f\"s3:\/\/{bucket}\/Transform\",\n    sagemaker_session=sagemaker_session\n)\n\nstep_transform = TransformStep(\n    name=\"TransformStep\",\n    transformer=transformer,\n    inputs=TransformInput(\n        data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n        join_source=\"Input\",\n        content_type=\"text\/csv\",\n        split_type=\"Line\",\n    ),\n)\n\n\nBelow is the code for the output_fn in the inference script:\n\ndef output_fn(prediction, accept):\n    if accept == \"application\/json\":\n        instances = []\n        for row in prediction.tolist():\n            instances.append({\"features\": row})\n\n        json_output = {\"instances\": instances}\n\n        return worker.Response(json.dumps(json_output), mimetype=accept)\n    elif accept == 'application\/x-npy':\n        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n    elif accept == 'text\/csv':\n        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n    else:\n        raise RuntimeException(\"{} accept type is not supported by this script.\".format(accept))\n\n\nThanks for the help!",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-10T03:04:49.533Z",
                "Answer_upvote_count":0,
                "Answer_body":"Due to the fact that this is an account specific issue, that is also related to your unique use case involving a custom inference script, to answer your question with great depth and to investigate the error you have provided, we require details that are non-public information. Please kindly open a support case with AWS using the following link and our engineers will be pleased to assist you further.\n\nIn your support case, kindly include the following details:\n\nBatchTransform Job ARN\nThe overview you have included in your question for context.\nThe inference script.\nCloudWatch BatchTransform job logs (say 10 lines before error and 5 lines after the error). These can be found in the following log stream \"\/aws\/sagemaker\/TransformJobs\" , as seen the doc in [1].\n\n[1] Log Amazon SageMaker Events with Amazon CloudWatch - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/logging-cloudwatch.html",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-11-09T16:40:46.007Z",
                "Answer_upvote_count":0,
                "Answer_body":"Agree that this sounds like it'd need some specific troubleshooting and good to raise with support.\n\nHowever my initial guesses would be:\n\nIf your model output array is 1D, might be that it's getting serialized to 1,2,3,4,... instead of 1\\n2\\n3\\n4\\n... - in which case batch transform might be seeing the output as one record with many fields, instead of many records with a single field each. Try expanding the extra dimension \/ ensuring the CSV output format for a batched request is as you expect\nIf you're trying to accept\/ignore header rows in input batches, remember you need to return the same number of output rows. Generally the model should return same number of records out as the batch had going in.\nIf urgently struggling with this, you could try setting strategy=\"SingleRecord\" which will ensure each request sends only a single record so less to go wrong. However note that it'll be less resource-efficient (more HTTP\/request overhead vs actual payload, than the default batched approach)",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Groundtruth Named entity recognition Labeling Job Failure",
        "Question_creation_time":1667860132622,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwmj-aPT9StmbAjEsfAp0dA\/sage-maker-groundtruth-named-entity-recognition-labeling-job-failure",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Machine Learning & AI",
            "Security, Identity, & Compliance"
        ],
        "Question_tag":[
            "AWS Lambda",
            "Amazon SageMaker",
            "IAM Policies"
        ],
        "Question_upvote_count":0,
        "Question_view_count":52,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi! I'm unable to create a SageMaker Groundtruth Labeling Job for NER.\n\nSteps:\n\nCreated a new bucket.\n\nCreated folder for documents.\n\nCreated folder for outputs.\n\nAdded private work team\n\nCreated labeling job - Created new Role for Labeling job.\n\nHave tried multiple times, always receiving the same error:\n\n{\n    \"labeling-job-name\": \"Labelling-job-18\",\n    \"event-name\": \"PRE_HUMAN_LAMBDA_FAILED\",\n    \"event-log-message\": \"ERROR: Pre-human task Lambda failed for line 1.ClientError: The label categories file located at s3:\/\/<my-bucket>\/20221107\/output\/Labelling-job-18\/annotation-tool\/data.json can't be accessed. Make sure it exists in the us-east-1 region, that the role arn:aws:iam::<acct-id>:role\/service-role\/AmazonSageMaker-ExecutionRole-20221028T185408 has read permissions to it and try your request again.\"\n}\n\n\nHow can I fix this? Is the role created in the labelling job missing some kind of lambda policy?",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-09T05:48:03.836Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nThank you for using sagemaker Ground-Truth. Usually this exception occurs when the labeling job is not able to access the S3 bucket, Assuming that the bucket is located in the same region and sagemaker full access policy[1] is attached to the role. Please verify below points.\n\nPlease check if any bucket policy is attached, which is blocking or restricting access[2]\nVerify if any organizational SCP(service control policy) is setting a permission boundary on this account to prevent S3 access.[3]\n\nPost checking these policies, please try to re-run the job. If you have any difficulty in verifying any of the above mentioned points or if you still run into issue, Please reach out to AWS Support[4] (Sagemaker), along with your issue\/use case in detail and share relevant AWS resource names. We will troubleshoot accordingly by verifying relevant job logs.\n\n[1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security-iam-awsmanpol.html#security-iam-awsmanpol-AmazonSageMakerFullAccess [2] https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/userguide\/example-bucket-policies.html [3] https:\/\/docs.aws.amazon.com\/organizations\/latest\/userguide\/orgs_manage_policies_scps.html [4] https:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html#creating-a-support-casehttps:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html#creating-a-support-case",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to Resolve \"ERROR execute(301) Failed to execute model:\"",
        "Question_creation_time":1667853055854,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwmKbCBpXSym2Vh_4Z0cW0g\/how-to-resolve-error-execute-301-failed-to-execute-model",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Panorama"
        ],
        "Question_upvote_count":0,
        "Question_view_count":17,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"We have two applications working on the same AWS Panorama Appliance and processing different video streams. Unfortunately, we are catching the following error.\n\n2022-10-09 21:25:32.360 ERROR executionThread(358) Model 'model': 2022-10-09 21:25:32.359 ERROR execute(301) Failed to execute model:\nTVMError: \n'\"---------------------------------------------------------------\"\nAn error occurred during the execution of TVM.\nFor more information, please see: https:\/\/tvm.apache.org\/docs\/errors.html\n'\"---------------------------------------------------------------\n  Check failed: (context->execute(batch_size\n\"Stack trace:\n  File \"\/home\/nvidia\/neo-ai-dlr\/3rdparty\/tvm\/src\/runtime\/contrib\/tensorrt\/tensorrt_runtime.cc\", line 177\n  [bt] (0) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(+0x381358) [0x7f81e66358]\n  [bt] (1) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(tvm::runtime::detail::LogFatal::Entry::Finalize()+0x88) [0x7f81bb64a0]\n  [bt] (2) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(tvm::runtime::contrib::TensorRTRuntime::Run()+0x12b8) [0x7f81e243b0]\n  [bt] (3) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::json::JSONRuntimeBase::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)+0x5c) [0x7f81e1bfc4]\n  [bt] (4) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(+0x3c0dc4) [0x7f81ea5dc4]\n  [bt] (5) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(+0x3c0e4c) [0x7f81ea5e4c]\n  [bt] (6) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(dlr::TVMModel::Run()+0xc0) [0x7f81c258e0]\n  [bt] (7) \/data\/cloud\/assets\/applicationInstance-6ta4fxv6hatsk62pf7aigge36e\/a9adc18d31f58ce11dab117a31b7f47e7ee2ab83e04b52c2952ac8cd47b51f72\/model\/libdlr.so(RunDLRModel+0x1c) [0x7f81bea304]\n  [bt] (8) \/usr\/lib\/libAwsOmniInferLib.so(awsomniinfer::CNeoModel::SNeoModel::execute()+0x3c) [0x7f887db978]\"\n2022-10-09 21:25:32.437 ERROR executionThread(358) Model 'model': 2022-10-09 21:25:32.437 ERROR setData(279) Failed to set model input 'data':\n\n\nThe error isn't persistent. It may happen once in 2-3 weeks, and I need to know which place to investigate. The application logs are in the attachment. I am trying to avoid this issue.\n\nHowever, I would appreciate it if somebody knew how to cook this properly.",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"List all running Sagemaker or other instances in AWS CLI",
        "Question_creation_time":1667851269800,
        "Question_link":"https:\/\/repost.aws\/questions\/QUV7j_pRiGSBC4WFBAPGuY_g\/list-all-running-sagemaker-or-other-instances-in-aws-cli",
        "Question_topic":[
            "Machine Learning & AI",
            "Cloud Financial Management",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Billing",
            "AWS Account Management"
        ],
        "Question_upvote_count":0,
        "Question_view_count":42,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Recently with another office account, I have seen hidden instances were running and creating unwanted cost in sagemaker, is there any way to check all the running processes\/instances using AWS CLI. I got many answers which suggest to go to Billing page, but that is not my objective. I want to immediately find out which instances are running so that I can stop them immediately.\n\nAre there any best practices for this?",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-08T08:03:23.462Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can either use CLI or AWS Config advance queries. With CLI you can use list-notebook-instances which Returns a list of the SageMaker notebook instances in the requester\u2019s account in an Amazon Web Services Region. Reference : https:\/\/awscli.amazonaws.com\/v2\/documentation\/api\/latest\/reference\/sagemaker\/list-notebook-instances.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Studio is not opening after deleting lifecycle configuration",
        "Question_creation_time":1667756083758,
        "Question_link":"https:\/\/repost.aws\/questions\/QU91ywEwTsRRqmHKZJ1yVrrA\/sagemaker-studio-is-not-opening-after-deleting-lifecycle-configuration",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Studio Lab"
        ],
        "Question_upvote_count":0,
        "Question_view_count":23,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I was working on a sagemaker studio for ML work, I attached Lifecycle Configuration with it, which was creating problem. Then I deleted the lifecycle configuration without detaching it, and this problem is happening. Can't start sagemaker studio notebook and this is shown.\n\nAny suggestion to fix this ?",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-07T01:49:47.445Z",
                "Answer_upvote_count":1,
                "Answer_body":"You can try detaching the LCC script using the CLI. You can use the CloudShell from console, since your console role is able to perform updates on the domain.\n\nUse the update-domain CLI call, and provide an empty configuration for the default user settings, something like-\n\naws sagemaker update-domain --domain-id d-abc123 \\\n--default-user-settings '{\n\"JupyterServerAppSettings\": {\n  \"DefaultResourceSpec\": {\n    \"InstanceType\": \"system\"\n   },\n}}'",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Tips for managing several ML project with similar framework",
        "Question_creation_time":1667407640163,
        "Question_link":"https:\/\/repost.aws\/questions\/QUt1SmhJXaTri_0TMwsy3wcg\/tips-for-managing-several-ml-project-with-similar-framework",
        "Question_topic":[
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "DevOps",
            "ML Ops with Amazon SageMaker and Kubernetes"
        ],
        "Question_upvote_count":0,
        "Question_view_count":17,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\n\nI'm working on an end-to-end ml project which, for the moment, goes from training (it takes already processed train\/val\/test data from an S3 bucket) to deploy, passing through hyperparameter tunning. This project has been developed on SageMaker Studio and in the beginning, I decided to keep track on the project with a github's repository.\n\nSo, my work has certain degree of maturity: I was able to successfully train, tune, deploy and infer over a dataset. But the troubles came when I try to replicate this work for another dataset (a new project with a similar framework).\n\nLet's say that I had a client \"A\" for which I developed this project and now I have a new client \"B\" with similar requirements than client A. I'm looking for the best way of \"copy & paste\" the project considering the following:\n\na) I would like to keep working on the repository. The idea here is the repo been like a project's template ir order to clone the repository, make some few corrections (changing model's name, working bucket, etc) and then execute tuning, evaluation and deploy. b) There's a lot of changes and improvements that I should make in the future. So, I'd like those changes been reflected on both projects.\n\nIf anyone could give me some tips, guidelines, share his experience with something like this I would be very grateful.\n\nRegards! :D",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"Experiment tracking with Sagemaker Pipelines",
        "Question_creation_time":1667379630510,
        "Question_link":"https:\/\/repost.aws\/questions\/QUttCrJyfVQYyAE-AO0vg-EA\/experiment-tracking-with-sagemaker-pipelines",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0,
        "Question_view_count":21,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Is it possible to track only TrainingSteps in a Sagemaker Pipeline that contains multiple Processing & other steps? I don't really see big benefit of creating Trial Components for Processing Jobs or Model Repacking jobs into the experiments as they just overflow the UI.\n\nBasically could the pipeline_experiment_config parameter be used for defining which steps of the Pipeline should be tracked or should I disable automatic experiment creation and just try to create a manual experiment tracker during the Training Job.",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-02T13:40:54.790Z",
                "Answer_upvote_count":0,
                "Answer_body":"Sure, if you don't want an experiment and trial created for the pipeline, you could easily achieve this by setting the pipeline_experiment_config to None.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"problem in installing library in sagemaker studio notebook",
        "Question_creation_time":1667296098553,
        "Question_link":"https:\/\/repost.aws\/questions\/QUtopbA5VQSqaAI0BGv5mGsA\/problem-in-installing-library-in-sagemaker-studio-notebook",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":24,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a problem while installing librosa this error message comes:\n\nOSError: cannot load library 'libsndfile.so': libsndfile.so: cannot open shared object file: No such file or directory\n\n\nI tried many solutions and didn't work, please help",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"Damage detection on mobile",
        "Question_creation_time":1667293813732,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjpP7mZ07RSmhLNC0lXA3YQ\/damage-detection-on-mobile",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Rekognition"
        ],
        "Question_upvote_count":0,
        "Question_view_count":18,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi, I am new to ML\/AI, Can someone help me create model for detection damage on mobile i.e. screen damage, back panel damaged etc, How sagemaker can help",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-01T21:02:17.749Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hope this helps to get started on - https:\/\/awslabs.github.io\/sagemaker-defect-detection\/",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SQL Server driver issue on notebook instance running on AWS SageMaker",
        "Question_creation_time":1667244851129,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeC8hq97nSzKHHmMKbCPxww\/sql-server-driver-issue-on-notebook-instance-running-on-aws-sage-maker",
        "Question_topic":[
            "Database",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Microsoft SQL Server",
            "Amazon SageMaker",
            "Database"
        ],
        "Question_upvote_count":0,
        "Question_view_count":49,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am using aws sagemaker notebook instance to run jupyter notebook. I have a MS SQL Server DB 2019 db that I am trying to connect to from the notebook. Notebook instance is running on Amazon Linux 2, Jupyter Lab 1 platform.\n\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport json\nimport os\n\n\n# sql database\nimport pyodbc\nconnection = pyodbc.connect(\n                              'Driver={SQL Server};'\n                              'Server=sname;'\n                              'Database=dbname;'\n                              'Trusted_Connection=True;'\n                           )\n\ncursor = connection.cursor()\n\n\nI get an error, likely because the driver is not installed on the instance.\n\nError                                     Traceback (most recent call last)\n\/tmp\/ipykernel_20407\/3026941781.py in <cell line: 12>()\n     10 # sql database\n     11 import pyodbc\n---> 12 connection = pyodbc.connect(\n     13                               'Driver={SQL Server};'\n     14                               'Server=sname;'\n\nError: ('01000', \"[01000] [unixODBC][Driver Manager]Can't open lib 'SQL Server' : file not found (0) (SQLDriverConnect)\")\n\n\nhow do I install the driver on sagemaker instance and resolve this issue?",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-01T10:44:32.142Z",
                "Answer_upvote_count":0,
                "Answer_body":"You're correct in assuming that the problem is due to the driver not being installed. Microsoft have recently published a useful guide to installing various versions of the SQL Server drivers on different editions of Linux which should help you to get the driver installed: https:\/\/learn.microsoft.com\/en-us\/sql\/connect\/odbc\/linux-mac\/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-ver16\n\nOnce you've done that, you also need to check that you have connectivity to your SQL Server instance from your EC2 instance. You can check basic network connectivity with a ping or traceroute, but you also need to check that you can establish a connection between the two. Personally, I'd start by installing the command line tools - again, Microsoft publish a good tutorial for installing them at this link: https:\/\/learn.microsoft.com\/en-us\/sql\/linux\/sql-server-linux-setup-tools?view=sql-server-ver16\n\nIf you can't connect to your SQL Server by name, try instead to use the IP address as the server name. This will work if it's a \"base\" instance of SQL Server, I think you can use it with a secondary instance too, by using something like 192.168.0.1\/instancename (subsitute the IP address of the SQL Server).\n\nOnce you've done that, you just need to sort out authentication. You don't say if you've got SQL Server installed on Linux or Windows, but these two articles should help you to fix any LDAP\/Kerberos configuration issues that you might have in getting the two to talk to each other:\n\nAD Authentication with SQL Server & Linux: https:\/\/learn.microsoft.com\/en-us\/sql\/linux\/sql-server-linux-active-directory-authentication?view=sql-server-ver15\n\nThis may also be of use, which describes how to join a SQL Server that's running on a Linux box to AD. Worth a read to understand the mechanism, but I think the previous link is what you need: https:\/\/learn.microsoft.com\/en-us\/sql\/linux\/sql-server-linux-active-directory-join-domain?view=sql-server-ver15\n\nAlternatively, you could (temporarily) bypass that step, which can be tricky to the uninitiated, and simply use SQL Server authentication instead, by creating a SQL Server login\/password and corresponding user in the database you're trying to access. This isn't as secure as using Kerberos authentication, but it may suffice for your needs, or be useful to get you to connect your notebook to SageMaker temporarily to prove basic connectivity.\n\nPlease vote up my answer if it's useful to you, hope that helps. Jon.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"What classifier is used in SageMakers BlazingText algorithm?",
        "Question_creation_time":1666879393628,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhUSE7oIGTt2ZUt8bQS-SdQ\/what-classifier-is-used-in-sage-makers-blazing-text-algorithm",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Text processing & Analytics",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":16,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I would like to use the BlazingText algorithm in SageMaker for text classification: As I understand it we first represent the text with a word2vec algorithm to get the word embeddings, and then use these embeddings as input to a classifier. But is it known what classifier is being used and is it possible to modify it?\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-28T08:31:03.340Z",
                "Answer_upvote_count":0,
                "Answer_body":"The BlazingText algorithm uses fasttext classifiers under the file model.bin and thus I believe by modifying\/replacing that file with your own fasttext classifier you should be able to modify the classifier used by BlazingText.\n\nRegards NN",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to delete Pipeline, Trial and Experiment in Sagemaker",
        "Question_creation_time":1666851304944,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfeScyHhkScmH7_SndJaQng\/how-to-delete-pipeline-trial-and-experiment-in-sagemaker",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Account Management"
        ],
        "Question_upvote_count":0,
        "Question_view_count":25,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I already deleted Domain, Users, Models, Endpoints, Endpoint configurations, Notebook instances, S3 Bucket instance and log groups but left with Pipeline, Trial and Experiment. Can anyone please help me with steps to followed to delete Pipeline, Trial and Experiment.",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-27T09:05:42.742Z",
                "Answer_upvote_count":0,
                "Answer_body":"Delete Pipeline (https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.delete_pipeline):\n\nclient = boto3.client('sagemaker')\n\nresponse = client.delete_pipeline(\n    PipelineName='string',\n    ClientRequestToken='string'\n)\n\n\nDelete Trial (https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.delete_trial_component):\n\nresponse = client.delete_trial_component(\n    TrialComponentName='string'\n)",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Deleting SageMaker training jobs",
        "Question_creation_time":1666792726682,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwhvF6pP-TR-fTMXIcDfCMg\/deleting-sage-maker-training-jobs",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Model Training"
        ],
        "Question_upvote_count":0,
        "Question_view_count":69,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Today we can't delete SageMaker training jobs.\n\nAs noted in AWS docs: \"Training jobs and logs cannot be deleted and are retained indefinitely.\", and confirmed in this Github issue: amazon-sagemaker-examples#633\n\nIs it planned to add this functionality ?",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-26T14:02:48.092Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi there\n\nI don't know whether this functionality will be added (I would be surprised, tho, if it would) , but I'm curious to learn what your concerns are with not being able to delete training jobs. Could you elaborate?\n\nThanks Heiko",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-10-26T14:36:30.972Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thank you for your response.\n\nIn our case, these old training jobs are impacting our security KPIs. Our organisation requires enabling parameters NetworkIsolation and EnableInterContainerTrafficEncryption on all training jobs. But old jobs cannot be modified or deleted, so they keep showing up in the KPIs.\n\nAlso, it would be useful to clean up unused resources.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker batch transform not loading CSV correctly",
        "Question_creation_time":1666732676105,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEci4LOFuRdeRj40V-koySw\/sage-maker-batch-transform-not-loading-csv-correctly",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":96,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am running a batch transform job that us uploading data from a CSV. The CSV is formatted as such\n\n\"joe annes rifle accesories discount\"\n\"cute puppies for sale\"\n\"Two dudes talk about sports\"\n\"Smith & Wesson M&P 500 review\"\n\"Glock vs 1911 handgun\"\n\n\nMy code for creating the batch transform is below\n\nelec_model = PyTorchModel(model_data='s3:\/\/some_path\/binary-models\/tar_models\/14_10_2022__19_54_23_arms_ammunition.tar.gz',\n                         role=role,\n                         entry_point='torchserve_.py',\n                         source_dir='source_dir',\n                          framework_version='1.12.0',\n                          py_version='py38')\n\nnl_detector = elec_model.transformer(\n                     instance_count = 1,\n                     instance_type = 'ml.g4dn.xlarge', strategy=\"MultiRecord\", assemble_with=\"Line\", output_path = \"s3:\/\/some_path\/trash_output\")\n\nnl_detector.transform(\"s3:\/\/brand-safety-training-data\/trash\", content_type=\"text\/csv\", split_type=\"Line\")\n\n\nWhen I run this code instead of the batch job taking the CSV and breaking up the examples with every space, which is what\n\nsplit_type=\"Line\" \n\n\nis telling the algorithm to do, but instead it just ingests all of the sentences in the above CSV, and outputs 1 probability. Also, if I do the same thing with the same code, but switch\n\nstrategy=\"MultiRecord\"\n\n\nto\n\nstrategy=\"SingleRecord\"\n\n\nso the one code block would look like this\n\nnl_detector = elec_model.transformer(\n                     instance_count = 1,\n                     instance_type = 'ml.g4dn.xlarge', strategy=\"SingleRecord\", assemble_with=\"Line\", output_path = \"s3:\/\/some_path\/trash_output\")\n\n\nThe algorithm works correctly, and performs inference on all of the above sentences in the CSV correctly. Any reason why this is happening?\n\nEDIT 1: When I print the input payload it looks like this\n\n\"joe annes rifle accesories discount\"\n\n2022-10-26T21:03:04,265 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n\n\"cute puppies for sale\"\n\n2022-10-26T21:03:04,265 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n\n\"Two dudes talk about sports\"\n\n2022-10-26T21:03:04,265 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\n\n\nWhere each sentence is a inference example, and is separated by this statement\n\n2022-10-26T21:03:04,265 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\n\n\nSo it seems like sagemaker is separating the inference examples. But when I try and pass these sentences into a huggingface tokenizer, the tokenizer tokenizes them like they are one inference example, when they should be 3 distinct inference examples.",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-30T04:11:00.002Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nThis issue is not quite related to SageMaker but how you pass the data to transformer tokenizer.\n\nYou are right on split_type=\"Line\" which splits your CSV files by lines. However, MultiRecord will ask SageMaker to pack as many lines as possible, up to MaxPayloadInMB as described in this doc. The default value is 6 MB. SingleRecord, on the other hand, will pass lines one by one.\n\nThe text will be from Byte\/IO stream, which essentially something like a string as follows, \"joe annes rifle accesories discount\\ncute puppies for sale\\nTwo dudes talk about sports\\nSmith & Wesson M&P 500 review\\nGlock vs 1911 handgun\" If we pass this directly to tokenizer, it will be treated as a single string.\n\nYou can firstly parse the string to a list like\n\n[\"joe annes rifle accesories discount\",  \"cute puppies for sale\", \"Two dudes talk about sports\", \"Smith & Wesson M&P 500 review\", \"Glock vs 1911 handgun\"]\n\n\nbefore passing data to tokenizer, which will result in a (5, xxx) tensor. This could ensure transformer understand sentences individually.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"how to add autoscaling policy to an sagemaker endpoint via terraform?",
        "Question_creation_time":1666661848109,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvlNBp88AT7GtMkUbWIIZCw\/how-to-add-autoscaling-policy-to-an-sagemaker-endpoint-via-terraform",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":29,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"based on the documentation here, https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/async-inference\/Async-Inference-Walkthrough.ipynb, an autoscaling policy could be added to an sagemaker async endpoint . I want to set this via terraform and have this so far (sample below) , but i'm not how to set Dimensions attribute in TargetTrackingScalingPolicyConfiguration in terraform?\n\n  TargetTrackingScalingPolicyConfiguration={\n        \"TargetValue\": 5.0, SageMakerVariantInvocationsPerInstance\n        \"CustomizedMetricSpecification\": {\n            \"MetricName\": \"ApproximateBacklogSizePerInstance\",\n            \"Namespace\": \"AWS\/SageMaker\",\n            \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": endpoint_name}],\n            \"Statistic\": \"Average\",\n        },\n\nresource \"aws_appautoscaling_target\" \"sagemaker_target\" {\n  max_capacity       =  3\n  min_capacity       = 1\n  resource_id        = \"myendpoint\"\n  scalable_dimension = \"sagemaker:variant:DesiredInstanceCount\"\n  service_namespace  = \"sagemaker\"\n}\n\nresource \"aws_appautoscaling_policy\" \"sagemaker_policy\" {\n  name               = \"somepolicy\"\n  policy_type        = \"TargetTrackingScaling\"\n  resource_id        = aws_appautoscaling_target.sagemaker_target.resource_id\n  scalable_dimension = aws_appautoscaling_target.sagemaker_target.scalable_dimension\n  service_namespace  = aws_appautoscaling_target.sagemaker_target.service_namespace\n\n  target_tracking_scaling_policy_configuration {\n    customized_metric_specification{\n      metric_name = \"ApproximateBacklogSizePerInstance\"\n      namespace =  \"AWS\/SageMaker\"\n      Dimensions = ....?????\n      statistic =  \"Average\"\n    }\n    target_value       = 3\n    scale_in_cooldown  =300\n    scale_out_cooldown = 600\n  }\n}",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-30T05:14:51.808Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nThis Dimensions refer to the CloudWatch dimensions of metric ApproximateBacklogSizePerInstance. It's the same dimension when you first define the metric as described in doc here",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"how to add\/update autoscaling policy for an async endpoint in sagemaker?",
        "Question_creation_time":1666657149491,
        "Question_link":"https:\/\/repost.aws\/questions\/QUj2WySokdQBaLNX601V6lbg\/how-to-add-update-autoscaling-policy-for-an-async-endpoint-in-sagemaker",
        "Question_topic":[
            "Management & Governance",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Auto Scaling",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":19,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have an async sagemaker endpoint, with an auto scaling policy (sample code below) . Everytime I update the model.tar.gz file , i delete the old endpoint and create a new one, with the same name and same setting. do i have to delete and re-create the autoscaling as well?\n\nclient = boto3.client(\"application-autoscaling\") \nresponse = client.register_scalable_target(\n    ServiceNamespace=\"sagemaker\",\n    ResourceId=resource_id,\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n    MinCapacity=0,\n    MaxCapacity=5,\n)\nresponse = client.put_scaling_policy(\n    PolicyName=\"Invocations-ScalingPolicy\",\n    ServiceNamespace=\"sagemaker\", \n    ResourceId=  \"endpoint\/myendpoint\/variant\/test\"\n    ...",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-30T06:53:38.984Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nAccording to our doc, if the role used includes permissions to deregister the model, Application Auto Scaling deregisters those models as scalable targets without notifying you. If the permission is not strong enough, the process fails and you must delete automatic scaling policies and deregister scalable targets before deleting the endpoint.\n\nIn other words, once you see an endpoint deleted, the corresponding auto scaling policy should have been deleted and targets de-registered. Even if you create a new endpoint with the same, you still need to register the target and attach policies.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to deploy lifecicle configuration to Sagemaker Studio via Cloudformation",
        "Question_creation_time":1666655893373,
        "Question_link":"https:\/\/repost.aws\/questions\/QUb1jvjl80RCClOggimxYaTQ\/how-to-deploy-lifecicle-configuration-to-sagemaker-studio-via-cloudformation",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS CloudFormation"
        ],
        "Question_upvote_count":0,
        "Question_view_count":23,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hello,\n\nI need to deploy a new lifecicle configuration to Sagemaker Studio. I was searching in the Cloudformation Documentation and found nothing but this service.\n\nhttps:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-notebookinstancelifecycleconfig.html\n\nAs the name of the service says, it was deployed as a Lifecicle config for a Notebook Instance.\n\nIs there any way to deploy a Lifecicle configuration for Sagemaker studio via Cloudformation ?\n\nThanks. Anderson",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-25T07:12:22.487Z",
                "Answer_upvote_count":1,
                "Answer_body":"CloudFormation currently does not support SageMaker Studio lifecycle configurations out-of-the-box. See also: https:\/\/github.com\/aws-cloudformation\/cloudformation-coverage-roadmap\/issues\/1132\n\nI assume it would be possible to create a custom resource for that purpose.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"How to pass credentials in Glue Notebooks - Interactive Session using Magic Commands to override the 1 hour temporary token expiration",
        "Question_creation_time":1666635956843,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0wxONrToTd2vvUzwsmt3cw\/how-to-pass-credentials-in-glue-notebooks-interactive-session-using-magic-commands-to-override-the-1-hour-temporary-token-expiration",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Glue",
            "Session Management"
        ],
        "Question_upvote_count":0,
        "Question_view_count":28,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi, If I start the notebook via the console the token\/credentials expire after one hour, Gives the following error \"Exception encountered while retrieving session: An error occurred (ExpiredTokenException) when calling the GetSession operation: The security token included in the request is expired \" .\n\nI am guessing this is happening since its using temporary credentials by default. How does one pass the credentials using the magic commands such that credentials do not expire or workaround?\n\nI can run notebooks locally using the profile in local .aws folder, but can't use TAGS for the sessions to account for costs.",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-01T17:00:31.794Z",
                "Answer_upvote_count":0,
                "Answer_body":"From the console Glue notebook, I tried running %iam_role my-glue-role magic to reset the IAM role and that seemed to do the trick.\n\nYou are already connected to session axxxxxxx-2xxxx-4xxx-bxxx-0xxxxxxx. Your change will not reflect in the current session, but it will affect future new sessions. \n\nCurrent iam_role is arn:aws:iam::xxxxxxxxx:role\/my-glue-role\niam_role has been set to my-glue-role.\n\n\nIf you are doing it using an API in your computer, you can increase the maximum session duration expiration for temporary credentials for IAM roles using the DurationSeconds parameter for your use case.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"HELP!!!! Amazon SageMaker not writing best optimal route based on Genetic Algorithm to 2nd Output Dynamo Database(am stucked here&incurring dollar charges with no progress -Error Screenshot available)",
        "Question_creation_time":1666611225424,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDsciSjamQ5WZR_FHC1u0vQ\/help-amazon-sage-maker-not-writing-best-optimal-route-based-on-genetic-algorithm-to-2nd-output-dynamo-database-am-stucked-here-incurring-dollar-charges-with-no-progress-error-screenshot-available",
        "Question_topic":[
            "Machine Learning & AI",
            "Database",
            "Containers",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon DynamoDB",
            "Containers",
            "DevOps"
        ],
        "Question_upvote_count":0,
        "Question_view_count":26,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"My Challenges is this: I used CloudFormation template to deploy 2 Dynamo DB (Input and Output) and 1 IAM role to use AWS-managed Lamba Function for Genetic Algorithm, so Amazon SageMaker (using Jupyter Notebook on AWS) is meant to write the locations (X and Y coordinates into the input Dynamo DB (Successful), while the Docker file to Docker Image to Docker Container is also to be run by Amazon SageMaker to write the best Optimal Route based on Genetic Algorithm (Mutation, genomes and generation transfer mode of operation) to the 2nd Dynamo DB (Unsuccessful) and this is where I am stucked, have read a lot of materials and research a lot and even reached out to some Amazon AWS Community but they could not resolve it, Please will be glad if repost.aws can help please (Error Screenshot Available)",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"What is Validation set adjustment while the system is auto labeling",
        "Question_creation_time":1666580175899,
        "Question_link":"https:\/\/repost.aws\/questions\/QUtppe1uHQTK-zEzkN4xDn4Q\/what-is-validation-set-adjustment-while-the-system-is-auto-labeling",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth",
            "Amazon SageMaker Model Training"
        ],
        "Question_upvote_count":0,
        "Question_view_count":45,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"How is it works, and what is the purpose?\n\nINFO:samurai_science_object_detection.cli:Running validation set adjustment.",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-25T16:55:57.485Z",
                "Answer_upvote_count":0,
                "Answer_body":"When an auto labeling job is initiated by Ground Truth, a random sample of input data is selected and sent to Human workers for labeling. Upon the return of this data, a training set and a validation set are created. Ground Truth uses these datasets to train and validate the model used for auto labeling.\n\nMuch like with ML models, cross-validation is done by using a complementary subset of the data from the input data to evaluate the model. In Ground Truth auto labeling, this Validation set of data is periodically adjusted ( at every iteration of the labeling job) to improve the accuracy of the automated labels.\n\nIf you have further specific questions around your workflows or require a deep dive on your logs in this regard, you may open a support case using this link , as we may require details that are non-public information, and we will be happy to assist you further.\n\nHow it works - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html#sms-automated-labeling-how-it-works\n\nCross-Validation - https:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/cross-validation.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"I want to use the model monitor in the sagemaker pipeline to get the results of statistics.json and constraints.json",
        "Question_creation_time":1666395801466,
        "Question_link":"https:\/\/repost.aws\/questions\/QUW13HHW8LT8O80p_ozuHk1A\/i-want-to-use-the-model-monitor-in-the-sagemaker-pipeline-to-get-the-results-of-statistics-json-and-constraints-json",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0,
        "Question_view_count":37,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Summary.\nI would like to know what the CSV format is for getting aws sagemaker model quality metrics (regression metrics).\nContents\nThe model monitor in sagemaker is used to evaluate models (own models, batch processing). At this time, I believe that using the AWS mechanism, I can get statistics.json and constraints.json as a result.\nIn the model monitor mechanism, there are three parameters for \"problem_type\" in the following script. I think there are three parameters for \"regression\", \"BinaryClassification\" and \"MulticlassClassification\", but I don't know what CSV format to send the content in for \"regression\", and I'm not sure if AWS will handle it appropriately. Question.\nIncidentally, in the case of classification, 'BinaryClassification' and 'MulticlassClassification' results from the creation of appropriate CSVs.\n\nThe CSV in the case of classification is created below.\n\nprobability,\tprediction,\tlabel\n[0.99,0.88], 0, 0\n[0.34,0.77], 1, 0\n\u30fb\u30fb\u30fb\nI don't know what kind of CSV content I should send to get the 'Regression' results.\nHere are some excerpts from pipeline.py and the CSV content we are currently testing.\n\nHere are some excerpts from 'pipeline.py\n\n    model_quality_check_config = ModelQualityCheckConfig(\n        baseline_dataset=step_transform.properties.TransformOutput.S3OutputPath,\n        dataset_format=DatasetFormat.csv(header=False),\n        output_s3_uri= f\"sagemaker\/monitor\/\",\n        problem_type='Regression',\n        inference_attribute= \"_c1\",\n        ground_truth_attribute= \"_c0\"\n    )\n\n    model_quality_check_step = QualityCheckStep(\n        name=\"ModelQualityCheckStep\",\n        depends_on = [\"lambdacsvtransform\"],\n        skip_check=skip_check_model_quality,\n        register_new_baseline=register_new_baseline_model_quality,\n        quality_check_config=model_quality_check_config,\n        check_job_config=check_job_config,\n        supplied_baseline_statistics=supplied_baseline_statistics_model_quality,\n        supplied_baseline_constraints=supplied_baseline_constraints_model_quality,\n        model_package_group_name=\"group\"\n    )\n\nCSV\uff08Regression\uff09\n\n\"_c0\", \"_c0\"\n0.88, 0.99\n0.66, 0.87\n\u30fb\u30fb\u30fb",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-23T12:21:53.451Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nWhen you say \"you do not know what CSV content I should send to get the 'Regression' results\", are you referring to the ContentType for your dataset?\n\nFirstly, the line below means that the features\/column names in the training dataset are not provided as the first row:\n\ndataset_format=DatasetFormat.csv(header=False)\n\nPlease see the link [1] for more information on the above parameter. Basically, your dataset content is a comma-separated value file but for this particular scenario header = False as there is no column names provided for the training dataset.\n\nI believe your overall question has to do with MetricsSource [2] object that would be defined as part of the ModelMetrics module, as in what will be the ContentType value used for your use case.\n\nWhen it comes to MetricsSource object, if you consider the example \"SageMaker Pipelines integration with Model Monitor and Clarify\" from https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-pipelines\/tabular\/model-monitor-clarify-pipelines\/sagemaker-pipeline-model-monitor-clarify-steps.ipynb, you will see that there is code that looks like the following:\n\n    model_statistics=MetricsSource(\n        s3_uri=model_quality_check_step.properties.CalculatedBaselineStatistics,\n        content_type=\"application\/json\",\n    ),\n    model_constraints=MetricsSource(\n        s3_uri=model_quality_check_step.properties.CalculatedBaselineConstraints,\n        content_type=\"application\/json\",\n    ),\n\n\nAs you can see from the above that content_type is \"application\/json\"\n\nThe data you shared below suggest that the Pipeline has been deployed as an endpoint [3], as there is a deploy module that is supported.\n\nBut from running the example \"SageMaker Pipelines integration with Model Monitor and Clarify\", I was able to get the files\n\ns3:\/\/S3-BUCKET-NAME-HERE\/some-prefix\/modelqualitycheckstep\/statistics.json \ns3:\/\/S3-BUCKET-NAME-HERE\/some-prefix\/modelqualitycheckstep\/constraints.json\n\n\nwhere statistics.json contains the following:\n\n{\n  \"version\": 0,\n  \"dataset\": {\n    \"item_count\": 627,\n    \"evaluation_time\": \"2022-10-23T10:49:40.638Z\"\n  },\n  \"regression_metrics\": {\n    \"mae\": {\n      \"value\": 1.4107242246563925,\n      \"standard_deviation\": 0.025615074935394368\n    },\n    \"mse\": {\n      \"value\": 3.9022604063585753,\n      \"standard_deviation\": 0.23140761659194883\n    },\n    \"rmse\": {\n      \"value\": 1.9754139835382798,\n      \"standard_deviation\": 0.05901487899216817\n    },\n    \"r2\": {\n      \"value\": 0.40614751710172436,\n      \"standard_deviation\": 0.03121704707239033\n    }\n  }\n}\n\n\nFrom the above, one can see that if the MetricsSource Object is declared then the metrics published for a regression problem type.\n\nHope I answered the question properly, if not please reach out to AWS Support[4] (SageMaker), explain your issue\/use case in detail and share relevant AWS resource names (plus CloudWatch logs).\n\nReferences:\n\n[1] https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model_monitor.html#sagemaker.model_monitor.dataset_format.DatasetFormat\n\n[2] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_MetricsSource.html\n\n[3] https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/pipeline.html\n\n[4] https:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html#creating-a-support-case",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How does sagemaker creates a model?",
        "Question_creation_time":1666386495979,
        "Question_link":"https:\/\/repost.aws\/questions\/QUTsasf-AKRriQkvxUPXGlxg\/how-does-sagemaker-creates-a-model",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":1,
        "Question_view_count":31,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I am creating sagemaker resources such as model, endpoint configuration and real time endpoint via cloudformation ( sample below) . in the template below, we provide the s3 bucket URI for the model artifact in the ModelDataUrl argument. if we update the model artifact , or delete a older version and upload a new model.tar file in the same bucket. will that work, instead of creating a new model resource everytime there is a new version of model.tar file ? when making a inference, I understand , sagemaker downloads the model.tar file in the container specified , unpack the model.tar file and call the binary file for inference ,so it doesn't matter if we update the model.tar file , right? sagemaker will simply download whatever tar file is present in the s3 URI and works with that.\n\nSageMakerModel:\n    Type: AWS::SageMaker::Model\n    Properties: \n      Containers: \n        -\n          Image: !Ref ImageURI\n          ModelDataUrl: 's3:\/\/some-bucket\/model.tar'\n          Mode: SingleModel\n      ExecutionRoleArn: !Ref RoleArn",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-22T12:21:49.235Z",
                "Answer_upvote_count":1,
                "Answer_body":"If you trained your model in SageMaker, the model artifacts are saved as a single compressed tar file in Amazon S3. If you trained your model outside SageMaker, you need to create this single compressed tar file and save it in a S3 location. SageMaker decompresses this tar file into \/opt\/ml\/model directory before your container starts.\n\nReference : https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-10-22T03:56:46.545Z",
                "Answer_upvote_count":1,
                "Answer_body":"Just to add a bit more detail to Nitin's answer since you raised a couple of interesting secondary questions:\n\nFirst, to be picky, your artifact does need to be a compressed .tar.gz file as mentioned here. Plain .tar files (which you mentioned a couple of times) likely won't have this compression and won't work. From my recent tests the filename extension itself in S3 doesn't matter though - so if your object is in the correct format but just happens to be called .tar, this should be fine.\n\nSecond, yes today you can simply update the saved model artifact in S3 and continue referencing the same SageMaker model - but when can you do it and should you?\n\nOne important caveat to bear in mind is that if you're deploying your models to online inference endpoints, there's no API available to force running instances to re-fetch an updated model tarball. If you update your artifact while an endpoint is live (especially if auto-scaling is enabled or some endpoint update is in progress) you could end up with endpoint instances running a mixture of old vs new model.\n\nThe second is that, even if you only run defined batch jobs and can guarantee the timing of artifact updates won't overlap with job start-up, keeping track of your model versions is a pretty important best practice to help keep your operations in order.\n\nSo even though it's possible to just update the artifact in S3, I'd usually suggest users track their historical models: For example by treating the S3 prefix as immutable, re-creating Models for new artifact versions, and using SageMaker Model Registry to track the history of versions. It's worth mentioning that storing your history of versions (\"ModelPackages\") in Model Registry doesn't necessarily have to balloon the number of \"Models\" in SageMaker: As discussed here, a ModelPackage isn't necessarily tied to a Model, and you need to create a Model from it to be able to use in deployment\/inference.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Cost of autoscaling endpoint Amazon SageMaker endpoint to zero",
        "Question_creation_time":1666360814590,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0VGYdZe8TRivmtGHoiDDHw\/cost-of-autoscaling-endpoint-amazon-sage-maker-endpoint-to-zero",
        "Question_topic":[
            "Management & Governance",
            "Machine Learning & AI",
            "Cloud Financial Management"
        ],
        "Question_tag":[
            "AWS Auto Scaling",
            "Amazon SageMaker",
            "AWS Pricing Calculator",
            "Pricing"
        ],
        "Question_upvote_count":1,
        "Question_view_count":57,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I want to use an Amazon Sagemaker endpoint for a custom classification model. The endpoint should only handle sporadic input (say a few times a week). For this purpose I want to employ autoscaling that scales the number of instances down to 0 when the endpoint is not used.\n\nAre there any costs associated with having an endpoint with 0 instances?\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-21T16:50:29.704Z",
                "Answer_upvote_count":2,
                "Answer_body":"You dont pay any compute costs for the duration when the endpoint size scales down to 0. But i think you can design it better. There are few other options for you to use in SageMaker Endpoint(assuming you are using realtime endpoint)\n\nTry using SageMaker Serverless Inference instead. Its purely serverless in nature so you pay only when the endpoint is serving inference. i think that would fit your requirement better.\nYou can think of using Lambda as well which will reduce your hosting costs. but you have to do more work in setting up the inference stack all by yourself.\nThere is also an option of SageMaker asynchronous inference but its mostly useful for inference which require longer time to process each request. The reason i mention this is it also support scale to 0 when no traffic is coming.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"greengrass ml component",
        "Question_creation_time":1666267168133,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4YAAy55MQQChcPc0JhyWWA\/greengrass-ml-component",
        "Question_topic":[
            "Internet of Things (IoT)",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS IoT Greengrass",
            "AWS IoT Core",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Internet of Things (IoT)"
        ],
        "Question_upvote_count":0,
        "Question_view_count":38,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello I am studying Greengrass machine learning components. I have a question. I'm going to distribute Greengrass custom machine learning components to Raspberry Pi now. The artifacts associated with the recipe in this component contain inference codes uploaded to S3. And this inference code has a function of determining who is by photographing the user's face in real time. I'm curious. If the components are distributed well in Raspberry Pi, will the inference code continue to run? Or, do I have to deploy components every time I try to run an inference? Thank you.",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"fail to run guide demo of DeepRacer SageMaker",
        "Question_creation_time":1666058970622,
        "Question_link":"https:\/\/repost.aws\/questions\/QUWyDmIyDjQu2l8OJyKBFHNw\/fail-to-run-guide-demo-of-deep-racer-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":16,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I try to follow the developer guide of training model of DeepRacer on SageMaker, but fail in \"traning_job = sagemaker.create_training_job\" part:",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"Amazon SageMaker - Training Job \/ Data Wrangler",
        "Question_creation_time":1665982852548,
        "Question_link":"https:\/\/repost.aws\/questions\/QUg1kenySVSc2Eq9mW7Mwy1A\/amazon-sage-maker-training-job-data-wrangler",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Data Wrangler"
        ],
        "Question_upvote_count":0,
        "Question_view_count":47,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a customer who is interested in testing Amazon Sagemaker and would like to consult the following questions:\n\nQ1. While submitting training job in Amazon Sagemaker, if there is insufficient capacity occurred, would there be any auto-retry mechanism? How to set up? Q2. Is the underlying SQL \/ MySQL infrastructure in Data Wrangler from AWS serverless DB backend? Q3. What is the backend database to support Sagemaker \/ Sagemkaer Data Wrangler ?\n\nUse case: Vision ML - Object detection (self-built algorithm) Framework: Tensorflow 2.4.4",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-19T12:27:17.163Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nQ1\/ this is not a built in feature for training job API. You'd need to implement on your side with some try:catch mechanism. If instead you are using SageMaker Pipelines to start the jobs, then that has such functionality (see: Retry mechanism)\n\nQ2\/Q3\/ SageMaker Data Wrangler does NOT implement a database. It does offer the option to connect to a number of data sources, including databases. Is this what you meant? Can you elaborate a bit more on these two points on what you are looking for?\n\nThank you, G",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to use custom functions for a model in a Sagemaker pipeline?",
        "Question_creation_time":1665782248751,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjjEHGriWRBCoNnc8luz-6Q\/how-to-use-custom-functions-for-a-model-in-a-sagemaker-pipeline",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":33,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"If I want to use a custom function transformer in preprocessing, how do I ensure that it's detected at both pipeline building and deployment?\n\nI'm building a sklearn pipeline, and in preprocessing I use a custom FunctionTransformer. In Sagemaker, I am able to train, evaluate, and register the model, but get the below error when I try to deploy it: AttributeError: Can't get attribute 'truncate_function' on <module '__main__' from '\/miniconda3\/bin\/gunicorn'>\n\nI've tried putting the functions into a helper.py file, and including it as a dependency during training, but then get the following error when evaluating in a ProcessingStep: \"ModuleNotFoundError: No module named 'helper'.",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"AWS Sagemaker Notebook Not working ,how can i solve the issue?",
        "Question_creation_time":1665720123752,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhUbNweWRR0-oROL-jwKIRQ\/aws-sagemaker-notebook-not-working-how-can-i-solve-the-issue",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":39,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"The code failed because of a fatal error: Error sending http request and maximum retry encountered..\n\nSome things to try: a) Make sure Spark has enough available resources for Jupyter to create a Spark context. b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly. c) Restart the kernel.\n\nNote: There are no such logs on cloudwatch to figureout the issue.",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-14T15:26:34.864Z",
                "Answer_upvote_count":0,
                "Answer_body":"Try using a Python 3 kernel instead of the PySpark kernel. Found that suggestion here: https:\/\/stackoverflow.com\/a\/68663849",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Data Wrangler: Data Flow: Export to S3 using Jupyter Notebook",
        "Question_creation_time":1665658565074,
        "Question_link":"https:\/\/repost.aws\/questions\/QUskLbkfD8RW2YzO9Vr7XggA\/data-wrangler-data-flow-export-to-s-3-using-jupyter-notebook",
        "Question_topic":[
            "Machine Learning & AI",
            "Compute"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon EC2",
            "Amazon SageMaker Data Wrangler"
        ],
        "Question_upvote_count":0,
        "Question_view_count":25,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"When I have created a Data flow using data wrangler and when I am trying to Export to export to S3 using Jupyter Notebook and when I am running the notebook, I am getting the below mentioned error every time when creating a processing job:\n\nError: An error occurred (ResourceLimitExceeded) when calling the CreateProcessingJob operation: The account-level service limit 'ml.m5.4xlarge for processing job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 2 Instances. Please contact AWS support to request an increase for this limit.\n\nPlease provide me with the solution for this. I have increased the service quota for running apps and Notebook instance also but then also same issue arises.",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-17T15:05:05.558Z",
                "Answer_upvote_count":1,
                "Answer_body":"When you export a data flow to S3, you're starting a SageMaker processing job that will run the script and store data in S3.\n\nSo, increase your account limit for Processing Job instances, for the instance type ml.m5.4xlarge (instead of running apps\/notebook instances).",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"aws textract human review flow, failed to load image",
        "Question_creation_time":1665415509779,
        "Question_link":"https:\/\/repost.aws\/questions\/QUo48ev4bTTvO-GjsezfAmuQ\/aws-textract-human-review-flow-failed-to-load-image",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon Textract",
            "Amazon Augmented AI",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":48,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nI`m using aws textract to extract key-value pairs from an pdf. Because sometimes the accucary is low i use augmented AI (human review worflows) to involve a human worker. That works fine with png files, but when I use pdf files (which textract supports), I get an \"Failed to load image\". How do I get around this? I tried using a custom template, but can't find a way to insert the file type.\n\nBest regards,\n\nPaul",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-11T04:31:12.324Z",
                "Answer_upvote_count":1,
                "Answer_body":"The underlying challenge here is that, while modern browsers can natively render PDFs, they require different embedding methods for PDFs vs images. To my knowledge there's no built-in SageMaker Crowd HTML Element that's capable of handling both types interchangeably - and your experience with the pre-built UI seems to confirm this.\n\nDisplaying PDFs in A2I\/SMGT\n\nThis simple sample suggests to use an <iframe type=\"application\/pdf\"> to display PDFs via the browser's native renderer. You could try this approach... but as of ~March 2022, I found support was patchy because some browsers' default security policies didn't like loading a cross-origin iframe with interactive content.\n\nIf relying on the browser native renderer won't work for your users, you can use the open-source PDF.js renderer instead. Here is a more complex sample template that does that. PDF.js is powerful, but can be pretty tricky to get started with from my experience... Note that the basic process in this sample is:\n\nTag the <script>s and stylesheets for PDF.js in from a CDN\nInclude a PDF viewer structure in your HTML\nPass your A2I object URL in through JavaScript and set up your viewer there - including any interactivity you need\n(The second inline script tag there you can probably ignore: It's specific to what that data that template collects)\nScaling template complexity\n\nAlthough the situation has improved a lot in recent years, writing direct-to-browser inline JavaScript in HTML can be tricky due to browser diversity and developer tooling limitations. If you want to build more advanced, interactive task templates, you might want to explore using front-end frameworks like React\/Angular\/Vue within A2I\/Ground Truth.\n\nThe above-mentioned PDF.js template is actually a legacy that's since been replaced by this VueJS app in the sample that uses it. In that case, the switch was made because we wanted to customize the PDF viewer (rendering detection boxes over the document), and the complexity of the app justified setting up a proper toolchain. You can find discussion there about using frameworks in general and VueJS in particular with A2I, and could use the app as a starting point for building your own complex template in advance. Note if I was re-building that from scratch, I'd probably use much less liquid templating, and implement more within the JS framework itself as discussed here.\n\nYou can see the complex template being built\/deployed from (SageMaker) Python notebook here, and a screenshot of it in action here. This end-to-end sample is discussed further in an AWS ML blog post.\n\nHandling mixed PDF\/Image content\n\nIf you need your template to handle both PDFs and images, this will add extra complexity. Could your JavaScript infer from the object URL (filename) which category the input object falls into, and dynamically set up either an <img> tag or a PDF viewer? Could you fetch the object from JS and check the Content-Type response header? Might it be simpler to add the file type as an input to your A2I loop, and pass it in that way? (e.g. using conditional liquid template to either render an <img> or not?)\n\nDepending on what points in the flow you know the file type, there are multiple different ways you might tackle this. Ultimately though, you'll probably be switching between either generating an img or a PDF viewer: Whether those HTML elements are created by static Liquid templating or by dynamic JS.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2022-10-11T03:30:51.984Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Paul, have you tried this pdf using normal AnalyzeDocument API? And also could you check if the IAM role for your human review workflow has sufficient permission to access your s3 bucket?",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Can Sagemaker RStudio mount an NFS share?",
        "Question_creation_time":1665409392840,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqjjos-zZQhCU4vxTSFSDLQ\/can-sagemaker-r-studio-mount-an-nfs-share",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":25,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Our existing Python code currently accesses data files residing on a file system. It navigates through folders to reach individual files and reads them.\n\nWe would now like to migrate the code to notebooks hosted in Sagemaker Studio. The data files will be hosted in S3 buckets. However, we do not want to modify the code to use S3 API to access files. Instead, we would like to continue using a filesystem view of the files. What is the best way to make the S3 objects visible to the Sagemaker Studio as a filesystem? Please note that we would like to mount in Sagemaker studio and not on Sagemaker notebook.",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-12T16:58:30.223Z",
                "Answer_upvote_count":0,
                "Answer_body":"SageMaker Studio uses EFS for storage. So you could potentially do an aws s3 sync and sync your S3 files\/folders to EFS and use them as a filesystem. I'd also recommend to be cautious of the file sizes since EFS is more expensive.\n\nStudio now supports JupyterLab3 and you can now install popular extensions. So if you're just looking at viewing the S3 files from Studio, you can use the S3 file browser extension.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Annotation of PDF document using Bounding Box",
        "Question_creation_time":1665169244843,
        "Question_link":"https:\/\/repost.aws\/questions\/QUK0c3Zp1jQ8SFMF3Mfp7SmA\/annotation-of-pdf-document-using-bounding-box",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon Augmented AI",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":25,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi! Is it possible to develop some kind of template for SageMaker that uses bounding boxes to annotate a PDF document? I was looking for something similar to the crowd-bounding-box HTML tag, but instead of only capturing portions of the PDF's image, I also wanted to extract data regarding the text located inside that portion, along with it's coordinates and stuff like that, so I could give context to my annotation.",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-17T17:20:33.352Z",
                "Answer_upvote_count":0,
                "Answer_body":"Your usecase sounds very close to the core Textract workflow - check out this AWS ML blog post that provides a solution that generates searchable PDF's, including bounding boxes for the text and such\n\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/generating-searchable-pdfs-from-scanned-documents-automatically-with-amazon-textract\/",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Amazon CloudWatch Metric ModelSetupTime not available",
        "Question_creation_time":1665007033890,
        "Question_link":"https:\/\/repost.aws\/questions\/QUu2KU0TNvTu-2V6sOylZJsg\/amazon-cloud-watch-metric-model-setup-time-not-available",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Amazon CloudWatch",
            "Monitoring"
        ],
        "Question_upvote_count":0,
        "Question_view_count":23,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"The ModelSetupTime metric is not available for monitoring the serverless endpoint(https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html) in Amazon CloudWatch.Able to see only the below mentioned metrics Invocations Invocation5XXErrors Invocation4XXErrors ModelLatency OverheadLatency\n\nIs there any role\/configuration changes I have to do to view the ModelSetupTime ?",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-17T16:57:04.142Z",
                "Answer_upvote_count":0,
                "Answer_body":"Currently the metric is not published to CW.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Submit EMR serverless jobs from SageMaker notebook",
        "Question_creation_time":1664942921237,
        "Question_link":"https:\/\/repost.aws\/questions\/QUaaYMax_hTIi0UNn-7wGPIQ\/submit-emr-serverless-jobs-from-sage-maker-notebook",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon EMR",
            "Amazon EMR Serverless"
        ],
        "Question_upvote_count":0,
        "Question_view_count":45,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I am processing a dataset and need to submit a job to EMR serverless for the dataset to be processed in a distributed way. I have created an application in EMR studio. I would like to submit jobs to that application. I found the command to submit jobs\n\naws emr-serverless start-job-run \\\n    --application-id application-id \\\n    --execution-role-arn job-role-arn \\\n    --job-driver '{\n        \"sparkSubmit\": {\n            \"entryPoint\": \"s3:\/\/us-east-1.elasticmapreduce\/emr-containers\/samples\/wordcount\/scripts\/wordcount.py\",\n            \"entryPointArguments\": [\"s3:\/\/DOC-EXAMPLE-BUCKET-OUTPUT\/wordcount_output\"],\n            \"sparkSubmitParameters\": \"--conf spark.executor.cores=1 --conf spark.executor.memory=4g --conf spark.driver.cores=1 --conf spark.driver.memory=4g --conf spark.executor.instances=1\"\n        }\n    }'\n\n\nBut how can I run the above command from a Python 3 Data Science Notebook in SageMaker studio. Basically what endpoint do I need to use to submit the job.",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-05T18:25:33.441Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nInstead of using the CLI to submit your job, have you tried using the boto3 Python library? https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/emr-serverless.html . All of the configuration parameters you've shared can be passed in EMRServerless boto3.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-10-06T07:59:43.539Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nYou can use below method to submit job for EMR serverless.\n\n=>Running jobs from the EMR Studio console\n\n=>Running jobs from the AWS CLI\n\nhttps:\/\/docs.aws.amazon.com\/emr\/latest\/EMR-Serverless-UserGuide\/jobs.html\n\nSubmittion of EMR serverless jobs from SageMaker notebook is not supported yet.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Using HuggingFace in Sagemaker Studio as part of a project",
        "Question_creation_time":1664885918543,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6Ahf5zWZRZq63k1TOQ_48w\/using-hugging-face-in-sagemaker-studio-as-part-of-a-project",
        "Question_topic":[
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_upvote_count":0,
        "Question_view_count":47,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"TLDR: if we are trying to use a HuggingFaceProcessor\/Estimator in a Sagemaker Studio project, what are the requirements for the train.py file in terms of how it refers to the assembled training data, and where it should store the results of the operations it performs( e.g. compiled model, datae etc.)\n\nFULL DETAILS\n\nSo our high level goal is to be able to deploy some kind of non-XGB model from a sagemaker studio project, given that the templates provided are all XGB. As outlined in an earlier question we'd started with TensorFlow, but since our TensorFlow model wraps a HuggingFace model we thought let's try something even simpler, just a HuggingFace model using the HuggingFaceProcessor.\n\nSo following docs on HuggingFaceProcessor and a HuggingFace Estimator example we started to adjust the abalone (project template) pipeline.py to look like this (full code can be provided on request):\n\n    # processing step for feature engineering\n    hf_processor = HuggingFaceProcessor(\n        role=role, \n        instance_count=processing_instance_count,\n        instance_type=processing_instance_type,\n        transformers_version='4.4.2',\n        pytorch_version='1.6.0', \n        base_job_name=f\"{base_job_prefix}\/frameworkprocessor-hf\",\n        sagemaker_session=pipeline_session,\n    )\n    step_args = hf_processor.run(\n        outputs=[\n            ProcessingOutput(output_name=\"train\", source=\"\/opt\/ml\/processing\/train\"),\n            ProcessingOutput(output_name=\"validation\", source=\"\/opt\/ml\/processing\/validation\"),\n            ProcessingOutput(output_name=\"test\", source=\"\/opt\/ml\/processing\/test\"),\n        ],\n        code=os.path.join(BASE_DIR, \"preprocess.py\"),\n        arguments=[\"--input-data\", input_data],\n    )\n    step_process = ProcessingStep(\n        name=\"PreprocessTopicData\",\n        step_args=step_args,\n    )\n\n    # training step for generating model artifacts\n    model_path = f\"s3:\/\/{sagemaker_session.default_bucket()}\/{base_job_prefix}\/TopicTrain\"\n\n    hf_train = HuggingFace(entry_point='train.py',\n                            source_dir=BASE_DIR,\n                            base_job_name='huggingface-sdk-extension',\n                            instance_type=processing_instance_type,\n                            instance_count=processing_instance_count,\n                            transformers_version='4.4',\n                            pytorch_version='1.6',\n                            py_version='py36',\n                            role=role,\n                          )\n  \n    hf_train.set_hyperparameters(\n       epochs=3,\n       train_batch_size=16,\n       learning_rate=1.0e-5,\n       model_name='distilbert-base-uncased',\n    )\n                           \n    step_args = hf_train.fit(\n        inputs={\n            \"train\": TrainingInput(\n                s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n                    \"train\"\n                ].S3Output.S3Uri,\n                content_type=\"text\/csv\",\n            ),\n            \"validation\": TrainingInput(\n                s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n                    \"validation\"\n                ].S3Output.S3Uri,\n                content_type=\"text\/csv\",\n            ),\n        },\n    )\n\n\nFinding that pushing to master doesn't provide any feedback on issues arising from pipeline.py, we realised that trying to get the pipeline from a notebook was a better way of debugging these sorts of changes, assuming one remembered to restart the kernel each time to ensure changes to the pipeline.py file was available to the notebook.\n\nSo using the following code in the notebook we worked through a series of issues trying to bash the code into shape such that it would compile:\n\nfrom pipelines.topic.pipeline import get_pipeline\n\n\npipeline = get_pipeline(\n    region=region,\n    role=role,\n    default_bucket=default_bucket,\n    model_package_group_name=model_package_group_name,\n    pipeline_name=pipeline_name,\n)\n\n\nWe needed to change the default processing and training instance types to avoid a \"cpu\" unsupported issue:\n\n    processing_instance_type=\"ml.p3.xlarge\",\n    training_instance_type=\"ml.p3.xlarge\",\n\n\nand add a train.py script:\n\nfrom transformers import AutoTokenizer\nfrom transformers import TFAutoModelForSequenceClassification\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=18)\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom transformers import (\n    DistilBertTokenizerFast,\n    TFDistilBertForSequenceClassification,\n)\nDATA_COLUMN = 'text'\nLABEL_COLUMN = 'label'\nMAX_SEQUENCE_LENGTH = 512\nLEARNING_RATE = 5e-5\nBATCH_SIZE = 16\nNUM_EPOCHS = 3\nNUM_LABELS = 15\n\nif __name__ == \"__main__\":\n\n    # --------------------------------------------------------------------------------\n    # Tokenizer\n    # --------------------------------------------------------------------------------\n    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n    def tokenize(sentences, max_length=MAX_SEQUENCE_LENGTH, padding='max_length'):\n        \"\"\"Tokenize using the Huggingface tokenizer\n        Args:\n            sentences: String or list of string to tokenize\n            padding: Padding method ['do_not_pad'|'longest'|'max_length']\n        \"\"\"\n        return tokenizer(\n            sentences,\n            truncation=True,\n            padding=padding,\n            max_length=max_length,\n            return_tensors=\"tf\"\n        )\n    # --------------------------------------------------------------------------------\n    # Load data\n    # --------------------------------------------------------------------------------\n    from keras.utils import to_categorical\n    from sklearn.preprocessing import LabelEncoder\n    labelencoder_Y_1 = LabelEncoder()\n    yy = labelencoder_Y_1.fit_transform(train_data[LABEL_COLUMN].tolist())\n    yy = to_categorical(yy)\n    print(len(yy))\n    print(yy.shape)\n    train_dat, validation_dat, train_label, validation_label = train_test_split(\n        train_data[DATA_COLUMN].tolist(),\n        yy,\n        test_size=0.2,\n        shuffle=True\n    )\n    # --------------------------------------------------------------------------------\n    # Prepare TF dataset\n    # --------------------------------------------------------------------------------\n    train_dataset = tf.data.Dataset.from_tensor_slices((\n        dict(tokenize(train_dat)),  # Convert BatchEncoding instance to dictionary\n        train_label\n    )).shuffle(1000).batch(BATCH_SIZE).prefetch(1)\n    validation_dataset = tf.data.Dataset.from_tensor_slices((\n        dict(tokenize(validation_dat)),\n        validation_label\n    )).batch(BATCH_SIZE).prefetch(1)\n    # --------------------------------------------------------------------------------\n    # training\n    # --------------------------------------------------------------------------------\n    model = TFDistilBertForSequenceClassification.from_pretrained(\n        'distilbert-base-uncased',\n        num_labels=NUM_LABELS\n    )\n    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n    model.compile(\n        optimizer=optimizer,\n        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n    )\n\n\nHowever we are now stuck on this error when trying to get the pipeline from a notebook.\n\n<ipython-input-3-be38b3dda75f> in <module>\n      7     default_bucket=default_bucket,\n      8     model_package_group_name=model_package_group_name,\n----> 9     pipeline_name=pipeline_name,\n     10 )\n     11 # !conda list\n\n~\/topic-models-no-monitoring-p-rboparx6tdeg\/sagemaker-topic-models-no-monitoring-p-rboparx6tdeg-modelbuild\/pipelines\/topic\/pipeline.py in get_pipeline(region, sagemaker_project_arn, role, default_bucket, model_package_group_name, pipeline_name, base_job_prefix, processing_instance_type, training_instance_type)\n    228                     \"validation\"\n    229                 ].S3Output.S3Uri,\n--> 230                 content_type=\"text\/csv\",\n    231             ),\n    232         },\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline_context.py in wrapper(*args, **kwargs)\n    246             return self_instance.sagemaker_session.context\n    247 \n--> 248         return run_func(*args, **kwargs)\n    249 \n    250     return wrapper\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)\n   1059         self._prepare_for_training(job_name=job_name)\n   1060 \n-> 1061         self.latest_training_job = _TrainingJob.start_new(self, inputs, experiment_config)\n   1062         self.jobs.append(self.latest_training_job)\n   1063         if wait:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in start_new(cls, estimator, inputs, experiment_config)\n   1956         train_args = cls._get_train_args(estimator, inputs, experiment_config)\n   1957 \n-> 1958         estimator.sagemaker_session.train(**train_args)\n   1959 \n   1960         return cls(estimator.sagemaker_session, estimator._current_job_name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in train(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\n    611             self.sagemaker_client.create_training_job(**request)\n    612 \n--> 613         self._intercept_create_request(train_request, submit, self.train.__name__)\n    614 \n    615     def _get_train_request(  # noqa: C901\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in _intercept_create_request(self, request, create, func_name)\n   4303             func_name (str): the name of the function needed intercepting\n   4304         \"\"\"\n-> 4305         return create(request)\n   4306 \n   4307 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in submit(request)\n    608         def submit(request):\n    609             LOGGER.info(\"Creating training-job with name: %s\", job_name)\n--> 610             LOGGER.debug(\"train request: %s\", json.dumps(request, indent=4))\n    611             self.sagemaker_client.create_training_job(**request)\n    612 \n\n\/opt\/conda\/lib\/python3.7\/json\/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\n    236         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n    237         separators=separators, default=default, sort_keys=sort_keys,\n--> 238         **kw).encode(obj)\n    239 \n    240 \n\n\/opt\/conda\/lib\/python3.7\/json\/encoder.py in encode(self, o)\n    199         chunks = self.iterencode(o, _one_shot=True)\n    200         if not isinstance(chunks, (list, tuple)):\n--> 201             chunks = list(chunks)\n    202         return ''.join(chunks)\n    203 \n\n\/opt\/conda\/lib\/python3.7\/json\/encoder.py in _iterencode(o, _current_indent_level)\n    429             yield from _iterencode_list(o, _current_indent_level)\n    430         elif isinstance(o, dict):\n--> 431             yield from _iterencode_dict(o, _current_indent_level)\n    432         else:\n    433             if markers is not None:\n\n\/opt\/conda\/lib\/python3.7\/json\/encoder.py in _iterencode_dict(dct, _current_indent_level)\n    403                 else:\n    404                     chunks = _iterencode(value, _current_indent_level)\n--> 405                 yield from chunks\n    406         if newline_indent is not None:\n    407             _current_indent_level -= 1\n\n\/opt\/conda\/lib\/python3.7\/json\/encoder.py in _iterencode_dict(dct, _current_indent_level)\n    403                 else:\n    404                     chunks = _iterencode(value, _current_indent_level)\n--> 405                 yield from chunks\n    406         if newline_indent is not None:\n    407             _current_indent_level -= 1\n\n\/opt\/conda\/lib\/python3.7\/json\/encoder.py in _iterencode(o, _current_indent_level)\n    436                     raise ValueError(\"Circular reference detected\")\n    437                 markers[markerid] = o\n--> 438             o = _default(o)\n    439             yield from _iterencode(o, _current_indent_level)\n    440             if markers is not None:\n\n\/opt\/conda\/lib\/python3.7\/json\/encoder.py in default(self, o)\n    177 \n    178         \"\"\"\n--> 179         raise TypeError(f'Object of type {o.__class__.__name__} '\n    180                         f'is not JSON serializable')\n    181 \n\nTypeError: Object of type ParameterInteger is not JSON serializable\n\nWhich is telling us that some aspect of the training job (?) is not serializable, and it's not clear how to debug further.\n\nWhat would be enormously helpful is project templates for sagemaker studio showing the use of all the Processors, e.g. HuggingFace, TensorFlow and so on, but failing that we'd be most grateful is anyone could point us to documentation on what the requirements are for the train.py file that we need to specifiy for the HuggingFace Estimator.\n\nmany thanks in advance",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-08T11:27:02.372Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, Not sure I can get you all the way to solution either but I think here are some more useful tips:\n\nInstance type: To my knowledge ml.p3.xlarge does not exist - you'll probably want to look at ml.g4dn.xlarge or ml.p3.2xlarge\n\nAvoiding kernel restarts:\n\nIt is possible to make Jupyter pick up changes you make to local files on-the-fly instead of having to restart the kernel each time. Just add the following lines to the top of your notebook before you run any imports: The autoreload extension will then reload modules each time before running your code.\n\n%load_ext autoreload\n%autoreload 2\n\n\nFramework Processors with Pipelines:\n\nAs you might already be aware, there were some issues using Pipelines when FrameworkProcessor (which HuggingFaceProcessor, TensorFlowProcessor, etc are built on) was first launched. I believe these should now be fixed, but do require you to be using the pipeline_session syntax - I see you already are, so that's great.\n\nJust in case you're seeing any echoes of this, would maybe recommend to try demonstrating pipeline creation first without the processing job (don't necessarily need to have it working properly end-to-end), and then adding the processing job in. Do be aware you might come across some older samples that haven't been updated yet to use PipelineSession, and these may not work properly with your DL Framework Processors. (I think Heiko's sample above might pre-date this).\n\nPipeline JSON error:\n\nIt's hard to tell what's up for sure because seems like there are some gaps in your code (e.g. is processing_instance_count a plain number or a pipeline parameter? Either way, seems like something is going wrong at the actual pipeline definition stage rather than trying to execute it. I'd suggest to simplify and gradually build up: E.g. hard-coding pipeline parameters to plain values, building the pipeline with just a subset of steps, etc.\n\nHF processing and training scripts:\n\nGetting your pipeline working should hopefully be largely separate from (and parallelizable with) getting your individual training\/processing\/inference jobs running as expected. You'll find more information about the requirements for your script in the Hugging Face section of the SageMaker Python SDK doc.\n\nI'd maybe point to this example training script for sequence classification, which shows the standard pattern of using argparse and SM_MODEL_DIR to find what local folder your script should save the trained model to. That same repository has many examples showing other features and use cases too... But from a quick check most\/all of them don't seem to take data channel inputs. This one shows how input data (local folders) are also passed through the CLI\/environment variables to your script.\n\nTo summarize:\n\nYour script receives input and output locations (local folders) through CLI variables with environment variable fallbacks - e.g. --foo and SM_CHANNEL_FOO if you run a job like estimator.fit({\"foo\": \"s3:\/\/...\/...\"})\nHyperparameters are also received through CLI and\/or the SM_HPS JSON environment variable\nMake sure to save your model to the SM_MODEL_DIR folder\nif you want to output metrics, just use print() or logging on the script side and then define regular expressions on the pipeline side, to tell SageMaker how to scrape them from the logs\n\nHere is a quick overview from HF, another introductory example, and a really over-engineered sample if you want to see what a very complex use case might look like \ud83e\udd72 Hope this helps!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-10-11T07:24:28.177Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi there\n\nI'm not 100% sure what caused the error that you're seeing, but since you mentioned that a Hugging Face (HF) Pipeline example could be useful, I wanted to share this project I developed a while ago where we use all the HF component in a Sagemaker Pipeline: https:\/\/github.com\/marshmellow77\/ade-pipeline\/tree\/main\/ade-modelbuild\/pipelines\n\nThe original pipeline definition is in teh abalone folder, and the new one in the ade folder (ade = Adverse Drug Event).\n\nCheers Heiko",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"AWS SageMaker - Extending Pre-built Container, Deploy Endpoint Failed. No such file or directory: 'serve'\"",
        "Question_creation_time":1664542013756,
        "Question_link":"https:\/\/repost.aws\/questions\/QUR-uTDaDsQBGjMoAUcsi2sQ\/aws-sage-maker-extending-pre-built-container-deploy-endpoint-failed-no-such-file-or-directory-serve",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":97,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"I am trying to deploy the SageMaker Inference Endpoint by extending the Pre-built image. However, it failed with \"FileNotFoundError: [Errno 2] No such file or directory: 'serve'\"\n\nMy Dockerfile\n\nARG REGION=us-west-2\n\n# SageMaker PyTorch image\nFROM 763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-ec2\n\nRUN apt-get update\n\nENV PATH=\"\/opt\/ml\/code:${PATH}\"\n\n# this environment variable is used by the SageMaker PyTorch container to determine our user code directory.\nENV SAGEMAKER_SUBMIT_DIRECTORY \/opt\/ml\/code\n\n# \/opt\/ml and all subdirectories are utilized by SageMaker, use the \/code subdirectory to store your user code.\nCOPY inference.py \/opt\/ml\/code\/inference.py\n\n# Defines inference.py as script entrypoint \nENV SAGEMAKER_PROGRAM inference.py\n\n\nCloudWatch Log From \/aws\/sagemaker\/Endpoints\/mytestEndpoint\n\n2022-09-30T04:47:09.178-07:00\nTraceback (most recent call last):\n  File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 20, in <module>\n    subprocess.check_call(shlex.split(' '.join(sys.argv[1:])))\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 359, in check_call\n    retcode = call(*popenargs, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 340, in call\n    with Popen(*popenargs, **kwargs) as p:\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 858, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 1704, in _execute_child\n    raise child_exception_type(errno_num, err_msg, err_filename)\nTraceback (most recent call last): File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 20, in <module> subprocess.check_call(shlex.split(' '.join(sys.argv[1:]))) File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 359, in check_call retcode = call(*popenargs, **kwargs) File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 340, in call with Popen(*popenargs, **kwargs) as p: File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 858, in __init__ self._execute_child(args, executable, preexec_fn, close_fds, File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 1704, in _execute_child raise child_exception_type(errno_num, err_msg, err_filename)\n\n2022-09-30T04:47:13.409-07:00\nFileNotFoundError: [Errno 2] No such file or directory: 'serve'",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-01T09:28:00.773Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi, @holopekochan!\n\nThe serve script is installed by SageMaker PyTorch Inference Toolkit when you pip-install it in the Dockerfile.\n\nHowever, it's hard to say why it's not found in your container. Are you sure that you use the inference container, not training container, for your endpoint? If you go to the AWS Console > Amazon SageMaker > Models > your model, what ECR image it shows in Container 1 - Image?\n\nIt will be useful if you can share the code that you used to setup the SageMaker PyTorch estimator (if any) how you define your PyTorchModel and how you deploy() it.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-09-30T15:14:58.846Z",
                "Answer_upvote_count":0,
                "Answer_body":"Should use the Sagemaker image\n\n763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-sagemaker\n\n\ninstead of ec2\n\n763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-ec2",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Jumpstart in Sagemaker does not work",
        "Question_creation_time":1664537965056,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOGpNbAjoRASDDw1pQ5F6Fg\/jumpstart-in-sagemaker-does-not-work",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":32,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello,\n\nI am new to SageMaker, I created a domain, and I have an execution role, although when I use Sagemaker studio I cannot see all functionalities, such as the resources button, jumpstart button, etc. I have enabled 'SageMaker Projects and JumpStart '. I am also attaching a photo of what my UI in the studio looks like.\nAny help would be appreciated. Thanks in advance",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"using transformers module with sagemaker studio project: ModuleNotFoundError: No module named 'transformers'",
        "Question_creation_time":1664396753855,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdd2zOBY0Q4CEG1ZdbgNsgA\/using-transformers-module-with-sagemaker-studio-project-module-not-found-error-no-module-named-transformers",
        "Question_topic":[
            "Developer Tools",
            "Machine Learning & AI",
            "Management & Governance",
            "DevOps"
        ],
        "Question_tag":[
            "AWS CodePipeline",
            "Amazon SageMaker",
            "AWS Command Line Interface",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_upvote_count":0,
        "Question_view_count":81,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"So as mentioned in my other recent post, I'm trying to modify the sagemaker example abalone xgboost template to use tensorfow.\n\nMy current problem is that running the pipeline I get a failure and in the logs I see:\n\nModuleNotFoundError: No module named 'transformers'\n\n\nNOTE: I am importing 'transformers' in preprocess.py not in pipeline.py\n\nNow I have 'transformers' listed in various places as a dependency including:\n\nsetup.py - required_packages = [\"sagemaker==2.93.0\", \"sklearn\", \"transformers\", \"openpyxl\"]\npipelines.egg-info\/requires.txt - transformers (auto-generated from setup.py?)\n\nbut so I'm keen to understand, how can I ensure that additional dependencies are available in the pipline itself?\n\nMany thanks in advance\n\nADDITIONAL DETAILS ON HOW I ENCOUNTERED THE ERROR\n\nFrom one particular notebook (see previous post for more details) I have succesfully constructed the new topic\/tensorflow pipeline and run the following steps:\n\npipeline.upsert(role_arn=role)\nexecution = pipeline.start()\nexecution.describe()\n\n\nthe describe() method gives this output:\n\n{'PipelineArn': 'arn:aws:sagemaker:eu-west-1:398371982844:pipeline\/topicpipeline-example',\n 'PipelineExecutionArn': 'arn:aws:sagemaker:eu-west-1:398371982844:pipeline\/topicpipeline-example\/execution\/0aiczulkjoaw',\n 'PipelineExecutionDisplayName': 'execution-1664394415255',\n 'PipelineExecutionStatus': 'Executing',\n 'PipelineExperimentConfig': {'ExperimentName': 'topicpipeline-example',\n  'TrialName': '0aiczulkjoaw'},\n 'CreationTime': datetime.datetime(2022, 9, 28, 19, 46, 55, 147000, tzinfo=tzlocal()),\n 'LastModifiedTime': datetime.datetime(2022, 9, 28, 19, 46, 55, 147000, tzinfo=tzlocal()),\n 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:eu-west-1:398371982844:user-profile\/d-5qgy6ubxlbdq\/sjoseph-reg-genome-com-273',\n  'UserProfileName': 'sjoseph-reg-genome-com-273',\n  'DomainId': 'd-5qgy6ubxlbdq'},\n 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:eu-west-1:398371982844:user-profile\/d-5qgy6ubxlbdq\/sjoseph-reg-genome-com-273',\n  'UserProfileName': 'sjoseph-reg-genome-com-273',\n  'DomainId': 'd-5qgy6ubxlbdq'},\n 'ResponseMetadata': {'RequestId': 'f949d6f4-1865-4a01-b7a2-a96c42304071',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': 'f949d6f4-1865-4a01-b7a2-a96c42304071',\n   'content-type': 'application\/x-amz-json-1.1',\n   'content-length': '882',\n   'date': 'Wed, 28 Sep 2022 19:47:02 GMT'},\n  'RetryAttempts': 0}}\n\n\nWaiting for the execution I get:\n\n---------------------------------------------------------------------------\nWaiterError                               Traceback (most recent call last)\n<ipython-input-14-72be0c8b7085> in <module>\n----> 1 execution.wait()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in wait(self, delay, max_attempts)\n    581             waiter_id, model, self.sagemaker_session.sagemaker_client\n    582         )\n--> 583         waiter.wait(PipelineExecutionArn=self.arn)\n    584 \n    585 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/waiter.py in wait(self, **kwargs)\n     53     # method.\n     54     def wait(self, **kwargs):\n---> 55         Waiter.wait(self, **kwargs)\n     56 \n     57     wait.__doc__ = WaiterDocstring(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/waiter.py in wait(self, **kwargs)\n    376                     name=self.name,\n    377                     reason=reason,\n--> 378                     last_response=response,\n    379                 )\n    380             if num_attempts >= max_attempts:\n\nWaiterError: Waiter PipelineExecutionComplete failed: Waiter encountered a terminal failure state: For expression \"PipelineExecutionStatus\" we matched expected path: \"Failed\"\n\n\nWhich I assume is corresponding to the failure I see in the logs:\n\nI did also run python setup.py build to ensure my build directory was up to date ... here's the terminal output of that command:\n\nsagemaker-user@studio$ python setup.py build\n\/opt\/conda\/lib\/python3.9\/site-packages\/setuptools\/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n  warnings.warn(\n\/opt\/conda\/lib\/python3.9\/site-packages\/setuptools\/config\/setupcfg.py:508: SetuptoolsDeprecationWarning: The license_file parameter is deprecated, use license_files instead.\n  warnings.warn(msg, warning_class)\nrunning build\nrunning build_py\ncopying pipelines\/topic\/pipeline.py -> build\/lib\/pipelines\/topic\nrunning egg_info\nwriting pipelines.egg-info\/PKG-INFO\nwriting dependency_links to pipelines.egg-info\/dependency_links.txt\nwriting entry points to pipelines.egg-info\/entry_points.txt\nwriting requirements to pipelines.egg-info\/requires.txt\nwriting top-level names to pipelines.egg-info\/top_level.txt\nreading manifest file 'pipelines.egg-info\/SOURCES.txt'\nadding license file 'LICENSE'\nwriting manifest file 'pipelines.egg-info\/SOURCES.txt'\n\n\nIt seems like the dependencies are being written to pipelines.egg-info\/requires.txt but are these not being picked up by the pipeline?",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-29T11:42:35.510Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi! There are two places where you need to install the dependencies \/ requirements:\n\nIn your environment where you execute pipeline.start() \u2013 can be Amazon SageMaker Studio, your local machine or CI\/CD pipeline executor, e. g. AWS CodeBuild. These dependencies are installed in setup.py.\nInside the SageMaker processing and training jobs as well as in inference endpoints. This is usually done via requirements.txt file that you submit as part of your source_dir.\n\nIn your example, I recommend you to use the TensorFlowProcessor. The way how to install dependencies into it is described in the corresponding section of the documentation, in particular:\n\nSageMaker Processing installs the dependencies in requirements.txt in the container for you.\n\nSame applies to your model training and to the TensorFlow estimator. See the section Use third-party libraries in the TensorFlow documentation of the SageMaker Python SDK, in particular:\n\nIf there are other packages you want to use with your script, you can use a requirements.txt to install other dependencies at runtime.\n\nHope it helps!",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"adjusting sagemaker xgboost project to tensorflow (or even just different folder name)",
        "Question_creation_time":1664391665708,
        "Question_link":"https:\/\/repost.aws\/questions\/QUAL9Vn9abQ6KKCs2ASwwmzg\/adjusting-sagemaker-xgboost-project-to-tensorflow-or-even-just-different-folder-name",
        "Question_topic":[
            "Developer Tools",
            "Machine Learning & AI",
            "Containers",
            "DevOps"
        ],
        "Question_tag":[
            "AWS CodePipeline",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Containers",
            "DevOps"
        ],
        "Question_upvote_count":0,
        "Question_view_count":46,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I have sagemaker xgboost project template \"build, train, deploy\" working, but I'd like to modify if to use tensorflow instead of xgboost. First up I was just trying to change the abalone folder to topic to reflect the data we are working with.\n\nI was experimenting with trying to change the topic\/pipeline.py file like so\n\n    image_uri = sagemaker.image_uris.retrieve(\n        framework=\"tensorflow\",\n        region=region,\n        version=\"1.0-1\",\n        py_version=\"py3\",\n        instance_type=training_instance_type,\n    )\n\n\ni.e. just changing the framework name from \"xgboost\" to \"tensorflow\", but then when I run the following from a notebook:\n\nfrom pipelines.topic.pipeline import get_pipeline\n\n\npipeline = get_pipeline(\n    region=region,\n    role=role,\n    default_bucket=default_bucket,\n    model_package_group_name=model_package_group_name,\n    pipeline_name=pipeline_name,\n)\n\n\nI get the following error\n\nValueError                                Traceback (most recent call last)\n<ipython-input-5-6343f00c3471> in <module>\n      7     default_bucket=default_bucket,\n      8     model_package_group_name=model_package_group_name,\n----> 9     pipeline_name=pipeline_name,\n     10 )\n\n~\/topic-models-no-monitoring-p-rboparx6tdeg\/sagemaker-topic-models-no-monitoring-p-rboparx6tdeg-modelbuild\/pipelines\/topic\/pipeline.py in get_pipeline(region, sagemaker_project_arn, role, default_bucket, model_package_group_name, pipeline_name, base_job_prefix, processing_instance_type, training_instance_type)\n    188         version=\"1.0-1\",\n    189         py_version=\"py3\",\n--> 190         instance_type=training_instance_type,\n    191     )\n    192     tf_train = Estimator(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/utilities.py in wrapper(*args, **kwargs)\n    197                 logger.warning(warning_msg_template, arg_name, func_name, type(value))\n    198                 kwargs[arg_name] = value.default_value\n--> 199         return func(*args, **kwargs)\n    200 \n    201     return wrapper\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/image_uris.py in retrieve(framework, region, version, py_version, instance_type, accelerator_type, image_scope, container_version, distribution, base_framework_version, training_compiler_config, model_id, model_version, tolerate_vulnerable_model, tolerate_deprecated_model, sdk_version, inference_tool, serverless_inference_config)\n    152             if inference_tool == \"neuron\":\n    153                 _framework = f\"{framework}-{inference_tool}\"\n--> 154         config = _config_for_framework_and_scope(_framework, image_scope, accelerator_type)\n    155 \n    156     original_version = version\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/image_uris.py in _config_for_framework_and_scope(framework, image_scope, accelerator_type)\n    277         image_scope = available_scopes[0]\n    278 \n--> 279     _validate_arg(image_scope, available_scopes, \"image scope\")\n    280     return config if \"scope\" in config else config[image_scope]\n    281 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/image_uris.py in _validate_arg(arg, available_options, arg_name)\n    443             \"Unsupported {arg_name}: {arg}. You may need to upgrade your SDK version \"\n    444             \"(pip install -U sagemaker) for newer {arg_name}s. Supported {arg_name}(s): \"\n--> 445             \"{options}.\".format(arg_name=arg_name, arg=arg, options=\", \".join(available_options))\n    446         )\n    447 \n\nValueError: Unsupported image scope: None. You may need to upgrade your SDK version (pip install -U sagemaker) for newer image scopes. Supported image scope(s): eia, inference, training.\n\n\nI was skeptical that the upgrade suggested by the error message would fix this, but gave it a try:\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npipelines 0.0.1 requires sagemaker==2.93.0, but you have sagemaker 2.110.0 which is incompatible.\n\n\nSo that seems like I can't upgrade sagemaker without changing pipelines, and it's not clear that's the right thing to do - like this project template may be all designed around those particular ealier libraries.\n\nBut so is it that the \"framework\" name should be different, e.g. \"tf\"? Or is there some other setting that needs changing in order to allow me to get a tensorflow pipeline ...?\n\nHowever I find that if I use the existing abalone\/pipeline.py file I can change the framework to \"tensorflow\" and there's no problem running that particular step in the notebook.\n\nI've searched all the files in the project to try and find any dependency on the abalone folder name, and the closest I came was in codebuild-buildspec.yml but that hasn't helped.\n\nHas anyone else successfully changed the folder name from abalone to something else, or am I stuck with abalone if I want to make progress?\n\nMany thanks in advance\n\np.s. is there a slack community for sagemaker studio anywhere?\n\np.p.s. I have tried changing all instances of the term \"Abalone\" to \"Topic\" within the topic\/pipeline.py file (matching case as appropriate) to no avail\n\np.p.p.s. I discovered that I can get an error free run of getting the pipeline from a unit test:\n\nimport pytest\n\nfrom pipelines.topic.pipeline import *\n\nregion = 'eu-west-1'\nrole = 'arn:aws:iam::398371982844:role\/SageMakerExecutionRole'\ndefault_bucket = 'sagemaker-eu-west-1-398371982844'\nmodel_package_group_name = 'TopicModelPackageGroup-Example'\npipeline_name = 'TopicPipeline-Example'\n\ndef test_pipeline():\n    pipeline = get_pipeline(\n        region=region,\n        role=role,\n        default_bucket=default_bucket,\n        model_package_group_name=model_package_group_name,\n        pipeline_name=pipeline_name,\n    )\n\n\nand strangely if I go to a different copy of the notebook, everything runs fine, there ... so I have two seemingly identical ipynb notebooks, and in one of them when I switch to trying to get a topic pipeline I get the above error, and in the other, I get no error at all, very strange\n\np.p.p.p.s. I also notice that conda list returns very different results depending on whether I run it in the notebook or the terminal ... but the conda list results are identical for the two notebooks ...",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-29T11:24:21.266Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi! I see two parts in your question:\n\nHow to use Tensorflow in a SageMaker estimator to train and deploy a model\nHow to adapt a SageMaker MLOps template to your data and code\n\nTensorflow estimator is slightly different from XGBoost estimator, and the easiest way to work with it is not by using sagemaker.image_uris.retrieve(framework=\"tensorflow\",...), but to use sagemaker.tensorflow.TensorFlow estimator instead.\n\nThese are the two examples, which will be useful for you:\n\nTrain an MNIST model with TensorFlow\nDeploy a Trained TensorFlow V2 Model\n\nAs for updating the MLOps template, I recommend you to go through the comprehensive self-service lab on SageMaker Pipelines.\n\nIt shows you how to update the source directory from abalone to customer_churn. In your case it will be the topic.\n\nP. S. As for a Slack channel, to my best knowledge, this re:Post forum now is the best place to ask any questions on Amazon SageMaker, including SageMaker Studio.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"How do you find out input features required for a sagemaker model to do batch inference?",
        "Question_creation_time":1664371572064,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6-AhM27-RxqsbVO9bOhyMg\/how-do-you-find-out-input-features-required-for-a-sagemaker-model-to-do-batch-inference",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":26,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Say for example I have a trained sagemaker model artefact or model in a model registry. Now I need to prepare the input dataset to be used in the model for batch inference. How do I know what input features the model is expecting so that I can prepare the data accordingly ? Is there a way to find out from the model artefact?",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-11T06:40:49.861Z",
                "Answer_upvote_count":0,
                "Answer_body":"In short, this is not possible unless you manually associate some additional data or can trace back to somewhere the information is available.\n\nI'd suggest to check out SageMaker Lineage Tracking to help with tracking connections like these, but the tools there are generally at the dataset level rather than feature-level. Since SageMaker serves a very broad variety of ML domains (e.g. from tabular to image, voice, video, text, and many more), the concept of \"a feature\" is tricky to scope without being overly restrictive: Is it a reference to SageMaker Feature Store? What about customers using alternative feature stores or plain CSV data?\n\nIf you're working in domains that support it (e.g. especially tabular), I might recommend SageMaker data quality profiling as a nice way to track this. A data quality baseline report will contain schema and also feature distribution information, and you can attach this report to your model package in Model Registry. It won't fully describe the source of your data of course, but will document the properties of it and also enable you to run data drift monitoring on your deployed models.\n\nYou should see that supported request\/response content types are also available as fields in Model Registry, and you can even attach a sample payload URL as used by Inference Recommender. If you need to store additional information that doesn't have a clear place in Model Registry, you could of course resort to Tags.\n\nSo there are multiple options to associate data information with your model packages - but if you have an existing model package without this information, there may not be an automatic way to trace the data \"source\" in your particular context.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"aws.amazon.commachine-learning\/pricing link is broken",
        "Question_creation_time":1664336034992,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvybuFyXJRnW4Q_LvXxbEaA\/aws-amazon-commachine-learning-pricing-link-is-broken",
        "Question_topic":[
            "Machine Learning & AI",
            "Cloud Financial Management"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Pricing Calculator",
            "Pricing"
        ],
        "Question_upvote_count":0,
        "Question_view_count":40,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"https:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/requesting-real-time-predictions.html article\n\nlink => http:\/\/aws.amazon.commachine-learning\/pricing\/",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-28T08:25:45.712Z",
                "Answer_upvote_count":2,
                "Answer_body":"'Amazon Machine Learning service' is now deprecated and is replaced by Amazon SageMaker. You can see the warning on the top of the page you shared in the post or this is from the documentation page - \"We are no longer updating the Amazon Machine Learning (Amazon ML) service or accepting new users for it. This documentation is available for existing users, but we are no longer updating it.\"\n\nAnd for SageMaker, you can see the pricing here. Real time predictions (equivalent to what your posted link was suggesting) on SageMaker documentation can be found here.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-09-28T03:42:21.132Z",
                "Answer_upvote_count":1,
                "Answer_body":"Feedback on AWS official documentation can be provided via the feedback link in the upper right corner of the document, for example.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Killing a hung Sagemaker Job",
        "Question_creation_time":1664321471237,
        "Question_link":"https:\/\/repost.aws\/questions\/QU9DABK2CjTpKk4-kT_KVdsQ\/killing-a-hung-sagemaker-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":21,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"After running 'stop_training_job', the job is not stopping. Happy to provide the full processing name and job arn if needed. Thanks!",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"How to create a sagemaker serverless endpoint via cloudformation?",
        "Question_creation_time":1664305906330,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1p5lKUqcTl6s6e-Oh7mKYg\/how-to-create-a-sagemaker-serverless-endpoint-via-cloudformation",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS CloudFormation",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":1,
        "Question_view_count":35,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"based on the documentation, i am creating a model and trying to create an sagemaker endpoint configuration for a serverless endpoint (sample below) , I added a ServerlessConfig attribute in my endpoint configuration resource below, but i'm get an error = \"Property validation failure: [Encountered unsupported properties in {\/} [ServerlessConfig]]. any ideas?\n\nSageMakerModel:\n    Type: AWS::SageMaker::Model\n    Properties: \n      Containers: \n        -\n          Image: !Ref ImageURI\n          ModelDataUrl: !Ref ModelData\n          ExecutionRoleArn: !Ref RoleArn\n\nSageMakerEndpointConfig:\n    Type: \"AWS::SageMaker::EndpointConfig\"\n    Properties:\n      ServerlessConfig: \n        -\n          MaxConcurrency: 5",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-27T21:15:09.542Z",
                "Answer_upvote_count":0,
                "Answer_body":"ServerlessConfig is a property of the ProductionVariant object. So, something similar to this would work -\n\nType: \"AWS::SageMaker::EndpointConfig\"\nProperties:\n  ProductionVariants:\n    -  \n      VariantName: serverless-variant\n      ServerlessConfig: \n        MaxConcurrency: 5\n        ...",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"can a sagemaker endpoint be made public?",
        "Question_creation_time":1664239929738,
        "Question_link":"https:\/\/repost.aws\/questions\/QU96EIw32SSxmx3plPtUUcYA\/can-a-sagemaker-endpoint-be-made-public",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":1,
        "Question_view_count":40,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"is there a way to make a sagemaker endpoint be accessible publicly ?",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-27T09:46:53.193Z",
                "Answer_upvote_count":3,
                "Answer_body":"I believe the way to make a sagemaker inference endpoint public is to use api-gw infront of it. Check out this solution https:\/\/docs.aws.amazon.com\/solutions\/latest\/constructs\/aws-apigateway-sagemakerendpoint.html",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-09-27T02:32:12.501Z",
                "Answer_upvote_count":2,
                "Answer_body":"Hello. In order to make an Amazon SageMaker Real-Time Endpoint public, you can create and manage APIs through an API Gateway. This is an official blog that is showing you a possible solution:\n\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Deploying multiple Comprehend Custom Classifiers (multi-label mode)",
        "Question_creation_time":1664215795645,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEQiFVzOhR5q1XYhjlltO7w\/deploying-multiple-comprehend-custom-classifiers-multi-label-mode",
        "Question_topic":[
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "DevOps",
            "Amazon Comprehend",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0,
        "Question_view_count":36,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I want to train and deploy multiple comprehend custom classifiers (for example 50 models). I want to be able to classify my documents in near real-time (a couple of seconds are fine) 24\/7. The problem is that deploying one end-point for each classifier is very expensive, especially that one or two IU would be enough for all my models combined (I am expecting to process around 10 document a minute total\/length of one document is around 1000 characters ). Is there a way where I can deploy multiple models behind the same endpoint (similar to the multi-model endpoint in SageMaker)? Or maybe do an asynchronous approach and somewho make sure I get the response within seconds?",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-24T19:34:11.311Z",
                "Answer_upvote_count":0,
                "Answer_body":"No , Comprehend don't support hosting multiple models with the same endpoint right now. Thanks for your suggestions . We will take them into consideration .",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"SageMaker MultiDataModel deployment error during inference. ValueError: Exactly one .pth or .pt file is required for PyTorch models: []",
        "Question_creation_time":1664208836417,
        "Question_link":"https:\/\/repost.aws\/questions\/QUe7ia8vHWRmukMsV_i5SzpA\/sage-maker-multi-data-model-deployment-error-during-inference-value-error-exactly-one-pth-or-pt-file-is-required-for-py-torch-models",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "AWS Deep Learning Containers",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon CloudWatch Logs"
        ],
        "Question_upvote_count":0,
        "Question_view_count":112,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello, I've been trying to deploy multiple PyTorch models on one endpoint on SageMaker from a SageMaker Notebook. First I tested deployment of single models on single endpoints, to check if everything works smoothly and it did. I would create a PyTorchModel first:\n\nimport sagemaker\nfrom sagemaker.pytorch import PyTorchModel\nfrom sagemaker import get_execution_role\nfrom sagemaker.multidatamodel import MultiDataModel\nfrom sagemaker.serializers import JSONSerializer\nfrom sagemaker.deserializers import JSONDeserializer\nimport boto3\n\nrole = get_execution_role()\nsagemaker_session = sagemaker.Session()\n\npytorch_model = PyTorchModel(\n            entry_point='inference.py',\n            source_dir='code',\n            role=role,\n            model_data='s3:\/\/***\/model\/model.tar.gz',\n            framework_version='1.11.0',\n            py_version='py38',\n            name='***-model',\n            sagemaker_session=sagemaker_session\n        )\n\n\nMultiDataModel inherits properties from Model classes, so I used the same PyTorch model that I used for single model deployment. Then I would define the MultiDataModel the following way:\n\nmodels = MultiDataModel(name='***-multi-model',\n                       model_data_prefix='s3:\/\/***-sagemaker\/model\/',\n                       model=pytorch_model,\n                       sagemaker_session=sagemaker_session\n                       )\n\n\nAll it should need is the prefix to the S3 bucket of the model artifacts saved as tar.gz files (the same files used for single model deployment), the previously defined PyTorch model, a name and a sagemaker_session.\n\nTo deploy it:\n\nmodels.deploy(initial_instance_count =1,\n             instance_type='ml.m4.xlarge',\n             serializer=JSONSerializer(),\n             deserializer=JSONDeserializer(),\n             endpoint_name='***-multi-model-deployment',\n             )\n\n\nThe deployment goes well, as there are no failures and the endpoint is InService by the end of this step. However the error occurs when I try to run inference on one of the models:\n\nimport json\nbody = {\"url\":\"https:\/\/***image.jpg\"} #url to an image online\npayload = json.dumps(body)\nclient = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(\n    EndpointName = \"***-multi-model-deployment\",\n    ContentType  = \"application\/json\",\n    TargetModel  = \"\/model.tar.gz\",\n    Body         = payload)\n\n\nThis prompts an error message:\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"{\n  \"code\": 500,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Failed to start workers for model ec1cd509c40ca81ffc3fb09deb4599e2 version: 1.0\"\n}\n\". See https:\/\/***.console.aws.amazon.com\/cloudwatch\/home?region=***#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/***-multi-model-deployment in account ***** for more information.\n\n\nThe Cloudwatch logs show this error in particular:\n\n22-09-26T15:51:40,494 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/ts\/model_service_worker.py\", line 210, in <module>\n2022-09-26T15:51:40,494 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\n2022-09-26T15:51:40,494 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/ts\/model_service_worker.py\", line 181, in run_server\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/ts\/model_service_worker.py\", line 139, in handle_connection\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/ts\/model_service_worker.py\", line 104, in load_model\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/ts\/model_loader.py\", line 151, in load\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker_pytorch_serving_container\/handler_service.py\", line 51, in initialize\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker_inference\/default_handler_service.py\", line 66, in initialize\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker_inference\/transformer.py\", line 162, in validate_and_initialize\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._model = self._model_fn(model_dir)\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/sagemaker_pytorch_serving_container\/default_pytorch_inference_handler.py\", line 73, in default_model_fn\n2022-09-26T15:51:40,495 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     raise ValueError(\n2022-09-26T15:51:40,496 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ValueError: Exactly one .pth or .pt file is required for PyTorch models: []\n\n\nIt seems like it's having problems loading the model, saying only one .pth file is required, however in the invocation function i point to the exact model artifact present at that S3 bucket prefix. I'm having a hard time trying to fix this issue, so it would be very helpful if anyone had some suggestions!\n\nInstead of giving the MultiDataModel a model, I also tried providing it an ECR docker image with the same inference code, but I would get the same error during invocation of the endpoint.",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-26T17:18:04.935Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hey! I see a couple of potential issues that you might want to carefully check.\n\n1\/ Note that the CloudWatch the logs you're looking at are the errors for the default model worker W-9000-model_1.0. So these messages are irrelevant to you prediction request. See the Fix: Don't load default model in MME mode for detailed description of the issue. When you make predictions, the models are lazy-loaded from your model_data_prefix . Carefully check the further logs and timings and see what happens after it tries to load your TargetModel = \"\/model.tar.gz\". According to the InternalServerException exception, you are executing the model ec1cd509c40ca81ffc3fb09deb4599e2, so look for the logs of the worker W-9001-ec1cd509c40ca81ffc3fb09deb4599e2 and you might see some other errors.\n\n2\/ There are two types of model.tar.gz in SageMaker - one is plain model produced by an estimator when you call fit(), another when you deploy the model and it's repackaged with your inference code (see the SageMaker Python SDK source code fragments: 1 and 2).\n\nMake sure that your model_data_prefix contains repackaged models and they are not repackaged twice. Look into your model.tar.gz and make sure it contains both your PyTorch model as well as inference.py with the code dir.\n\nThe location of repackaged model should be accessible as pytorch_model.repacked_model_data after you deployed the endpoint.\n\nFor better clarity I recommend to create a separate path for models in your multi-model endpoint and copy the models to it with the following API:\n\nmodels.add_model(model_data_source=pytorch_model.repacked_model_data, model_data_path=model_name)\n\n\nHere the model_name can be something like model_1.tar.gz, model_2.tar.gz etc. Note that leading slash \/ is not necessary in the model name.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Excessive Memory use when deploying PipelineModel using the pre-build Scikit container in Sagemaker",
        "Question_creation_time":1663945337107,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2hdNVBreSFyX1iSPDRPMXw\/excessive-memory-use-when-deploying-pipeline-model-using-the-pre-build-scikit-container-in-sagemaker",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_upvote_count":0,
        "Question_view_count":17,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I am using the pre-build Scikit container in Sagemaker to deploy an endpoint based on a model that contains a 59.4 MB model.tar.gz file. The following line was used to deploy the endpoint:\n\nsm_model.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\", endpoint_name=endpoint_name)\n\nHowever, the after the endpoint was created, it fails to allocate memory to works. These error messages and warnings keep showing in the logs:\n\n[Errno 12] Cannot allocate memory [WARNING] Worker with pid 242 was terminated due to signal 9\n\nAs far as I know, the xlarge instance has 16 GB of memory. The endpoint memory usage is at 60% while it still fails to allocate memory to workers. May I ask if anyone has any insight on why this is happening and how to solve this issue without using an instance that has more memory?",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"How to pass values for a \"shap_baseline\" if we have categorical values (string values) as features in classsagemaker.clarify.SHAPConfig method.",
        "Question_creation_time":1663938111276,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVdzi-7h5RAapCYJUehzuZw\/how-to-pass-values-for-a-shap-baseline-if-we-have-categorical-values-string-values-as-features-in-classsagemaker-clarify-shap-config-method",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Ground Truth",
            "Amazon SageMaker Clarify",
            "Monitoring"
        ],
        "Question_upvote_count":0,
        "Question_view_count":41,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"using this documentation i passing a single row as to shap_baseline parameter to implement explainability monitoring , a similar implementation of what is done in in this github repo implementation. if I am passing a single row as input to shap_baseline parameter, the schedule is failing by concatenating 2 rows. If i ignore the shap_baseline (as it is optional), the schedule is taking forever to run. Help of any kind is really appreciated.\n\nthanks for your time and effort :)",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-04T05:28:35.427Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks for your question\n\nThere is some guidance and information about selecting a proper baseline from here that may help: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/clarify-feature-attribute-shap-baselines.html\n\nif I am passing a single row as input to shap_baseline parameter, the schedule is failing by concatenating 2 rows\n\nFor this, you may want to open a ticket via AWS Support in the AWS console so we can take a better look at this internally, and include your processing job details if possible (such as your analysis configurations, Job ARN, etc.)\n\nIf i ignore the shap_baseline (as it is optional), the schedule is taking forever to run\n\nThere is a num_clusters parameter in the SHAPConfig which you can explicitly set to reduce the size of the baseline dataset that is generated. A lower number (low single digits) will generally provide faster runtime",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to create custom templates for model training and building in sagemaker studio?",
        "Question_creation_time":1663805155971,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-fW_jQSjSKiwACGXqFh1sA\/how-to-create-custom-templates-for-model-training-and-building-in-sagemaker-studio",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0,
        "Question_view_count":34,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am going through documentation provided here , https:\/\/github.com\/aws-samples\/amazon-sagemaker-build-train-deploy and would like to create my own model building, training and deploying templates. can I download\/clone the sagemaker provided templates and start modifying to my own needs. I understand , we need to set up a aws catalog portfolio and products under it, to be able to use such templates. my question is which project do i need to clone and modify , say if i want to build my own training and model building template, which particular code base or code file do i need to change. I assume , the train.py file here -> https:\/\/github.com\/aws-samples\/amazon-sagemaker-build-train-deploy\/blob\/master\/08_projects\/modelbuild\/pipelines\/endtoendmlsm\/train\/train.py would be one i will customize. but once i change these files, how do i use them. how would i set up my custom template, create a product under my catalog? if yes, how to I link my custom train.py code to this new custom sagemaker project template. the documentation or samples only show how to use the prebuild templates under sagemaker project templates, but how do i get my own template pushed there ? can this be done via say cloudformation ?",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-22T03:53:38.010Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nThis workshop works through the typical ML process to build and deploy a model to predict the fault on machine based on synthetic datasets. To start building it, you will need to have a AWS account, the link is here.\n\nOnce you have an account, you can start from module 1 from this step. I would recommend follow each module in order and try to implement in your account. This will help you understand the methodology and familiar with the service.\n\nTo modify it, you will need to collect and use your datasets, identifying your business problem. You could try to re-use the same algorithm in the module but it depends on the issue you try to resolve.\n\nHope it helps\n\nThanks,",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"AWS Glue notebook - Kernel fails with Glue version exception",
        "Question_creation_time":1663701548390,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQotxGT8SRwefmNxpd0AKOw\/aws-glue-notebook-kernel-fails-with-glue-version-exception",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Command Line Interface",
            "AWS Glue"
        ],
        "Question_upvote_count":1,
        "Question_view_count":134,
        "Question_answer_count":3,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to run the notebooks locally, I followed the instructions provided here (for Windows): https:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/interactive-sessions.html\n\nI ran the pip3 install --upgrade jupyter boto3 aws-glue-sessions, which upgraded aws-glue-session to version 0.35\n\nbut the when starting the notebook the kernel fails to launch and throws the following error,\n\n  File \"d:\\python38\\lib\\site-packages\\aws_glue_interactive_sessions_kernel\\glue_pyspark\\GlueKernel.py\", line 100, in __init__\n    self.set_glue_version(os_env_glue_version)\n  File \"d:\\python38\\lib\\site-packages\\aws_glue_interactive_sessions_kernel\\glue_pyspark\\GlueKernel.py\", line 443, in set_glue_version\n    raise Exception(f\"Valid Glue versions are {VALID_GLUE_VERSIONS}\")\nException: Valid Glue versions are {'2.0', '3.0'}\n\n\nSetting the the glue_version = 2.0 in .aws\/config and environment variables, does not help either. any help on what could be causing this will be much appreciated!",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-21T12:26:31.730Z",
                "Answer_upvote_count":2,
                "Answer_body":"I'm having this same exact issue and found the culprit, some various environment variables aren't being properly seen as null in Windows when they're being looked up if they aren't set.\n\nFind the GlueKernel.py file for the glue interactive sessions package, in the site-packages folder for your Python environment like:\n\nsite-packages\\aws_glue_interactive_sessions_kernel\\glue_pyspark\\GlueKernel.py\n\nChange the function at line 871 to\n\nEvery environment variable lookup was always being returned as '${SOME_KEY}' instead of being None.\n\ndef _retrieve_os_env_variable(self, key):\n    _, output = subprocess.getstatusoutput(f\"echo ${key}\")\n    if(output != '${}'.format(key)):\n            return output\n        else:\n            return os.environ.get(key)",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-10-13T20:17:19.277Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nHave you tried to set the glue version through the spark magic as described here?\n\n%glue_version 2.0\n\n\nor\n\n%glue_version 3.0\n\n\nthis need to be set in the first cell before you start the session.\n\nI tested this locally and it worked, hope this helps.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-10-31T17:52:03.852Z",
                "Answer_upvote_count":0,
                "Answer_body":"I had to manually hard code the version as 3.0 in the set_glue_version method inside of GlueKernel.py. This got rid of the error, but obviously isn't a great solution. This implementation is pretty buggy.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"is it possible to create steps within the sagemaker pipeline via cloudformation?",
        "Question_creation_time":1663606095949,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3aphi0KgRNyk6AcB9c9Spg\/is-it-possible-to-create-steps-within-the-sagemaker-pipeline-via-cloudformation",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS CloudFormation",
            "Machine Learning & AI",
            "Amazon SageMaker Studio Lab",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0,
        "Question_view_count":38,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am experimenting with sagemaker studio, while i was able to create the sagemaker studio domain and user profiles via cloudformation. I was wondering if, it was possible to create sagemaker projects and other resources like model registry group and steps for preprocessing , training ... via cloudformation? if is it possible , are there any samples, examples around this, if it is not supported or is not possible via cfn , may be help me on how can create link this to existing user and domain in studio .",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-20T08:32:00.559Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi there,\n\nAll the resources for the CloudFormation Sagemaker crossover can be found here\n\nHope this helps to inform you more\n\nRegards NN",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How do I create a Dockerfile for BYOC training which allows me to pass in entry_point and source_dir arguments to SM Estimator?",
        "Question_creation_time":1663592214344,
        "Question_link":"https:\/\/repost.aws\/questions\/QUiVxzpWc9TMS0OkcqgqKPmQ\/how-do-i-create-a-dockerfile-for-byoc-training-which-allows-me-to-pass-in-entry-point-and-source-dir-arguments-to-sm-estimator",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Containers",
            "DevOps"
        ],
        "Question_upvote_count":0,
        "Question_view_count":45,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am following this tutorial: https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/tensorflow_bring_your_own\/container\/Dockerfile\n\nby SM to create a docker image. According to this tutorial, the training scripts are copied into the docker container meaning that you have to rebuild the container if you make any changes.\n\nMy question is: How do I create a Dockerfile such that I don't have to copy my script when I am building the image? Instead, I want to pass in the entry_point and source_dir arguments to the SM Estimator which copies those files upon calling the .fit method.",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-19T14:06:50.267Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi there,\n\nIt seems like this should be possible through the sagemaker script mode, which allows you to pass in the entry_point argument through the SM estimator as can be seen here\n\nHopefully this resource will help provide some insight into your problem.\n\nRe: so I kept looking and I believe this is a more appropriate resource to address your problem. Hopefully this one is what you are looking for.\n\nRegards NN",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Is Redshift mixing up my data columns when creating a model?",
        "Question_creation_time":1663314635540,
        "Question_link":"https:\/\/repost.aws\/questions\/QUs7vw6wMDTUOLwMaZlqwzgQ\/is-redshift-mixing-up-my-data-columns-when-creating-a-model",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Redshift"
        ],
        "Question_upvote_count":0,
        "Question_view_count":39,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello,\n\nI'm using running:\n\ncreate model predict_xxxxx\nfrom (select col1, col2, col3 from my_table)\ntarget col3\nfunction predict_xxx\niam_role 'arn:aws:iam::xxxxxxx:role\/RedshiftML'\nproblem_type regression\nobjective 'mse'\nsettings (\n    s3_bucket 'redshiftml-xxxxxxx',\n    s3_garbage_collect off,\n    max_runtime 1800\n);\n\n\nWhich then generates input data files in CSV format in the S3 bucket I specified, but when I open up those files and look at them, all the columns in my select statement are present, but the column headers are mismatched with the data below them. I see col1 data under the col2 column and so on. I know the data is mixed up because the data types and numeric ranges are different for each column. I double-checked my table and the columns and data are matched correctly. Is Redshift\/Sagemaker then using that mismatched data to train the model? I have tried with only two column and it still gets mixed up. I've tried using a table instead of a select expression and the problem persists.\n\nAny insight is appreciated.\n\nThanks,\n\nSV",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-19T09:22:11.348Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi there, Is it not possible that the table contains a null header and sagemaker is reading that header as col1 instead thus giving you that shift in the data structure. If that is not the case then it is possible you already had an col0 and it is moving up the data a column when you append col1, col2 and so on.\n\nHopefully this gives you more to think about and puts you in the right direction.\n\nRegards NN",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to export tresained models to ECR as container image",
        "Question_creation_time":1663258467464,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZHWz5-hpSc-80dEIkuxwQw\/how-to-export-tresained-models-to-ecr-as-container-image",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon Elastic Container Registry (ECR)",
            "Containers",
            "Amazon SageMaker Studio Lab"
        ],
        "Question_upvote_count":0,
        "Question_view_count":29,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I want to train and build the model in Sagemaker studio and then be able to export the model as a container image to ECR, so I can use the model in external platform by sharing the ECR image to another account where I Can create container with the image from ECR",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-16T23:05:33.114Z",
                "Answer_upvote_count":0,
                "Answer_body":"The models you train in SageMaker are stored in S3 as .tar.gz files that you can use to deploy to an endpoint, or even test locally (extracting the model file from the tar file). If you are using a built-in algorithm, you can share the .tar.gz file to the second account and deploy the model in the second account, since built-in algorithm containers can be accessed from any AWS account.\n\nIf you are using a custom training image (docs here), you can push this image to ECR and allow a second account to pull the image and then use the image with the model that you have trained. However, note that Studio at this time does not support building Docker images out of the box. You can use SageMaker Notebook Instances instead.\n\nI would recommend keeping the model (.tar.gz) and the image (Docker) separate, since you can easily retrain and deploy the newer versions of models without updating the image every single time.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Glue Interactive vs SageMaker Processing?",
        "Question_creation_time":1663171688837,
        "Question_link":"https:\/\/repost.aws\/questions\/QU7J5WaZe3Qzi2giJaOmBFDQ\/glue-interactive-vs-sage-maker-processing",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics",
            "Database"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Glue",
            "Extract Transform & Load Data"
        ],
        "Question_upvote_count":0,
        "Question_view_count":63,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"Greetings! I'm a data scientist working in SageMaker notebooks. I'd appreciate an explanation about when should I use Glue Interactive and not SageMaker Processing jobs. To my understanding, they are very similar and I can't differentiate them.\n\nThank you!",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-14T21:34:08.024Z",
                "Answer_upvote_count":2,
                "Answer_body":"Hello! It depends on what you are trying to achieve.\n\nLet us just talk about notebooks first - Sagemaker notebook (or even Glue notebook) is quite efficient for quick prototyping and analysis of data. For example, if you just want to make certain charts from a CSV or do quick data wrangling etc. then the Notebook is often the preferred choice. The notebook is also fantastic for documenting algorithms. The interactivity helps process step by step and to change your processing along based on the data that you would see. From a tooling perspective, the Glue notebooks provide the data engineer ability to run Jupyter notebok or Zeppelin notebook. SageMaker notebook is the tool preferred by data scientists and Machine learning engineers and provides the Jupyter notebook interface.\n\nSagemaker provides multiple computing options including ability to choose EC2 instances. in SageMaker Processing you can customize the execution environment, as you could provide a Docker image\n\nA Glue job is typically built for executing ETL jobs in a Spark based\/Python serverless job that executes in a cluster of nodes to parallel process data in multiple nodes. AWS Glue is a serverless data integration platform that makes combining, preparing, and finding data for application development, machine learning, and analytics a breeze. It delivers all of the features required for data integration, allowing you to begin analyzing and putting your data to use in minutes rather than months. To make data integration simpler, AWS Glue offers both code-based and visual interfaces. The AWS Glue Data Catalog allows users to quickly locate and retrieve data. With just limited clicks in AWS Glue Studio, ETL (extract, transform, and load) developers and data engineers can graphically construct, execute, and monitor ETL processes. AWS Glue DataBrew allows data analysts and scientists to visually enhance, clean, and standardize information without writing codes. AWS Glue scans your data sources, recognizes data types, and recommends schemas for storing your data. It produces the code needed to conduct your data transformations and loading operations automatically. AWS Glue makes it simple to perform and manage hundreds of ETL processes, as well as to mix and duplicate data across numerous data stores using SQL.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-09-15T13:21:35.278Z",
                "Answer_upvote_count":1,
                "Answer_body":"I would suggest that you use Sagemaker processing for the data cleansing and preparation. I have led projects where all the data cleansing, preparation and model build and testing have been done in Sagemaker and the data scientists love the tool.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Unable to compile model to Neuron: no error message, no output",
        "Question_creation_time":1663166637969,
        "Question_link":"https:\/\/repost.aws\/questions\/QUA_oVwSPQReCt96QyX4cz-g\/unable-to-compile-model-to-neuron-no-error-message-no-output",
        "Question_topic":[
            "Machine Learning & AI",
            "Compute"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Neuron"
        ],
        "Question_upvote_count":0,
        "Question_view_count":71,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi. We are trying to convert all our in-house pytorch models to aws-neuron on inferentia. We successfully converted one, but the second model we tried did not compile. Unfortunately, compilation did not generate any error message nor log of any kind, so we are stuck. The model is rather simple, but large, U-Net, with partial convolutions instead of regular ones, but otherwise no fancy operators. Conversion of this model to torchscript is ok on the same instance. Could it be a memory problem ?",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-15T09:25:42.678Z",
                "Answer_upvote_count":2,
                "Answer_body":"Hi, in order to see more information about the error, you can enable debugging during tracing by passing 'verbose' to the tracing command like this:\n\nimport torch\nimport torch.neuron\ntorch.neuron.trace(\n    model,\n    example_inputs=inp,\n    verbose=\"debug\",\n    compiler_workdir=\"logs\" # dir where debugging logs will be saved\n)\n\n\nYou'll see the error messages in the console and they will also be saved to the \"logs\" dir.\n\nIt is always good to run the NeuronSDK analyzer first to make sure the model is: 1\/ torch.jit traceable; 2\/ supported by the compiler\n\nimport torch\nimport torch.neuron\ntorch.neuron.analyze_model(model, example_inputs=inp)\n\n\nYou can also see a sample that shows how to compile an U-net Pytorch (3rd party implementation) to Inf1 instances here: https:\/\/github.com\/samir-souza\/laboratory\/blob\/master\/05_Inferentia\/03_UnetPytorch\/03_UnetPytorch.ipynb\n\nRef: https:\/\/awsdocs-neuron.readthedocs-hosted.com\/en\/latest\/neuron-guide\/neuron-frameworks\/pytorch-neuron\/api-compilation-python-api.html\n\nIf everything fails, try to look for something like this in the logs:\n\nINFO:Neuron:Compile command returned: -11\nWARNING:Neuron:torch.neuron.trace failed on _NeuronGraph$647; falling back to native python function call\nERROR:Neuron:neuron-cc failed with the following command line call:\n\n\nAnd paste here, please. With the \"Compile command returned:\" code it is possible to identify the error. You are suspecting that there is some issue related to memory, maybe Out of Memory. Normally when that is the case, you'll find the code: -9 in this part of the error.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2022-09-21T07:33:00.671Z",
                "Answer_upvote_count":0,
                "Answer_body":"Following your answer we were able to check the log and got\n\nINFO:Neuron:Compile command returned: -9\n\nwhich is apparently an out of memory error. Switching to a 6x instance solved the problem",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"how to trigger sagemaker pipeline via code change in github?",
        "Question_creation_time":1663125520507,
        "Question_link":"https:\/\/repost.aws\/questions\/QUaQEgFCVaTxCY_CINaOXSsw\/how-to-trigger-sagemaker-pipeline-via-code-change-in-github",
        "Question_topic":[
            "Developer Tools",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS CodePipeline",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":42,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"based on the aws docs and sample code provided here, https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/6299535b80b44ef0b61b95c979b1511157965810\/sagemaker-pipelines\/tabular\/customizing_build_train_deploy_project\/modelbuild\/pipelines\/customer_churn\/pipeline.py, I can wrap different ML steps like training into a step , in a sagemaker pipeline code. after that, i can define sagemaker projects to create a CI\/CD pipeline to build and deploy models. the examples i saw builds\/deploy model in aws code commit\/pipeline. is it possible to move this part to github or gitlab for repository and then deploy from gitlab or other tools like jenkins. are there any examples or code samples to do this?? also, instead of working in sagemaker studio IDE, is it possible to set up code repo in github or gitlab and we use our own IDE to push changes to the pipeline or sagemkaer projects code to build\/deploy model? and somehow hook that into studio such that for example , we make a change to hyperparameter value in gitlab, that will trigger the pipeline in studio?",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-14T21:18:41.617Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can create SageMaker projects using third party source code repositories such as Gitlab, Github etc. Here's a blog that walks through a solution - https:\/\/aws.amazon.com\/blogs\/machine-learning\/create-amazon-sagemaker-projects-using-third-party-source-control-and-jenkins\/ If you did not want to use Projects, you can still use pipelines with your source code. You can create an Amazon EventBridge rule to trigger a SageMaker pipeline execution based on events or a given schedule. See an example here - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pipeline-eventbridge.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to use sagemaker-pyspark in batch inference",
        "Question_creation_time":1663084288909,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4mfQyFCoTji_5XYrCAQEGQ\/how-to-use-sagemaker-pyspark-in-batch-inference",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":65,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"am trying to execute the code below\n\nENDPOINT_NAME = \"my-endpoint\"\nfrom sagemaker_pyspark import SageMakerModel\nfrom sagemaker_pyspark import EndpointCreationPolicy\nfrom sagemaker_pyspark.transformation.serializers import ProtobufRequestRowSerializer\nfrom sagemaker_pyspark.transformation.deserializers import ProtobufResponseRowDeserializer\nfrom pyspark.sql.types import StructType, StructField, MapType, StringType, IntegerType, ArrayType, FloatType\n\nattachedModel = SageMakerModel.fromEndpoint(\n    endpointName = ENDPOINT_NAME,\n    requestRowSerializer=ProtobufRequestRowSerializer(\n        featuresColumnName = \"col1\"\n    ),\n    responseRowDeserializer=ProtobufResponseRowDeserializer(schema=StructType([\n        StructField('prediction', MapType(StringType(), FloatType()))\n    ]))\n)\n\ndata=SageMakerModel.transform(attachedModel, df['col1'])\n\n\nI keep getting the below error though\n\npy4j.protocol.Py4JError: An error occurred while calling o58.__getstate__. Trace:\npy4j.Py4JException: Method __getstate__([]) does not exist\n        at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n        at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n        at py4j.Gateway.invoke(Gateway.java:274)\n        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n        at py4j.commands.CallCommand.execute(CallCommand.java:79)\n        at py4j.GatewayConnection.run(GatewayConnection.java:238)\n        at java.lang.Thread.run(Thread.java:748)\n\n\nany ideas ?",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-19T09:42:09.575Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi there,\n\nit would seem to me that there maybe an discrepancy in when one of your outputs is returned, they should only be returned after an if else statement and defined within the if else statement. Here is a resource that seems to address a problem similar to yours, perhaps applying a similar strategy will produce significant results.\n\nHopefully this will provide some insight into your problem.\n\nRegards NN",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"ERROR while fitting RCF data : An error occurred (EntityTooLarge) when calling the PutObject operation: Your proposed upload exceeds the maximum allowed size.",
        "Question_creation_time":1663020590535,
        "Question_link":"https:\/\/repost.aws\/questions\/QUO0RgnpwvQOy3ays1fRLw4A\/error-while-fitting-rcf-data-an-error-occurred-entity-too-large-when-calling-the-put-object-operation-your-proposed-upload-exceeds-the-maximum-allowed-size",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Model Training"
        ],
        "Question_upvote_count":0,
        "Question_view_count":38,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am fitting a RCF model to a dataset using rcf.fit(rcf.record_set(data[['Variable 1','Variable 2']].values.reshape(-1, 1))), but getting the below error :\n\nAn error occurred (EntityTooLarge) when calling the PutObject operation: Your proposed upload exceeds the maximum allowed size.\n\nThe size of the ndarray is 239393964\n\nHow can I fix this?",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-15T02:28:44.391Z",
                "Answer_upvote_count":0,
                "Answer_body":"This error occurs when the object being uploaded to S3 is too large. If a single PUT operation is being used by the AWS SDKs, REST API, or AWS CLI, you can upload a single object up to 5 GB in size.\n\nUploading objects - https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/userguide\/upload-objects.html\n\nTo upload objects larger than 5 GB in size, you can use multipart upload, with multipart upload you can upload a single large object up to 5 TB in size.\n\nUploading and copying objects using multipart upload - https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/userguide\/mpuoverview.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"neo compilation job failed on Yolov5\/v7 model",
        "Question_creation_time":1662741786680,
        "Question_link":"https:\/\/repost.aws\/questions\/QUV00cs8DbQ6WVPa7J_LLXmw\/neo-compilation-job-failed-on-yolov-5-v-7-model",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":58,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\n\nI was trying to use SageMaker Neo compilation to convert a yolo model(trained with our custom data) to a coreml format, but got an error on input config:\n\nClientError: InputConfiguration: Unable to determine the type of the model, i.e. the source framework. Please provide the value of argument \"source\", from one of [\"tensorflow\", \"pytorch\", \"mil\"]. Note that model conversion requires the source package that generates the model. Please make sure you have the appropriate version of source package installed.\n\nI've tried both latest yolov7 model and yolov5 model, but get the same error. Seems Neo cannot recognize the Yolo model.\n\nBut when I tried to use the yolov4 model from this tutorial post: https:\/\/aws.amazon.com\/de\/blogs\/machine-learning\/speed-up-yolov4-inference-to-twice-as-fast-on-amazon-sagemaker\/, it works fine.\n\nAny idea if Neo compilation can work with Yolov7\/v5 model?",
        "Answers":[
            {
                "Answer_creation_date":"2022-10-31T16:35:05.811Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, It would be needed to see your code to help you, but what I can do is to share the steps required to compile Yolov5 with Neo. There are two important things here: 1\/ the model needs to be torch.jit.trace(able) 2\/ you need to replace some unsupported operators by friendly ones\n\nIn the following sample I'm using a specific version of the model: v6.2\n\nThese dependencies are required if you're running on SageMaker Studio.\n\n## required if you're using Data Science Kernel on Sagemaker Studio\n!apt update -y && apt install -y libgl1\n%pip install torch\n\n\nNext, you need to install specific yolov5 dependencies\n\n## Yolov5 dependencies\n%pip install -r https:\/\/raw.githubusercontent.com\/ultralytics\/yolov5\/v6.2\/requirements.txt  # install dependencies\n\n\nDownload a pre-trained COCO80 model version 6.2 from the model zoo.\n\nimport torch\nimport torch.nn as nn\n\nmodel_type='l'\nassert(model_type in ['n', 's', 'm', 'l', 'x'])\n\nx = torch.rand([1, 3, 640, 640], dtype=torch.float32)\nmodel = torch.hub.load('ultralytics\/yolov5:v6.2', f'yolov5{model_type}', pretrained=True)\nmodel.eval()\n\n\nThe following block replaces some ops in the model to make it compatible with SageMaker Neo. Then the model is .jit.traced and exported\n\n# from Yolov5 repo. It will be available after invoking torch.hub.load\nimport models\nfrom utils.activations import Hardswish, SiLU\n\n# Update model\nfor k, m in model.named_modules():\n    if isinstance(m, models.common.Conv):  \n        if isinstance(m.act, nn.Hardswish):\n            m.act = Hardswish()\n        elif isinstance(m.act, nn.SiLU): # assign export-friendly activations\n            m.act = SiLU()\n\ny = model(x) # warmup\ntry:    \n    traced = torch.jit.trace(model, x)\n    traced.save('model.pth')\n    \n    print(\"Cool! Model is jit traceable\")\nexcept Exception as e:\n    print(\"Ops. Something went wrong. Model is not traceable\")\n\n\nNow, you need to create a .tar.gz file with the traced model and upload it to S3\n\nimport tarfile\nimport boto3\nimport io\n\nbucket_name='<<<YOUR S3 BUCKET HERE>>>'\nmodel_name='yolov5'\nkey_path=f'models\/{model_name}\/model.tar.gz'\ns3_client = boto3.client('s3')\ns3_uri = f\"s3:\/\/{bucket_name}\/{key_path}\"\n\nwith io.BytesIO() as f:\n    with tarfile.open(fileobj=f, mode='w:gz') as tar:\n        tar.add('model.pth')\n        tar.list()\n    f.seek(0)\n    s3_client.put_object(Body=f, Bucket=bucket_name, Key=key_path)\nprint(s3_uri)\n\n\nFinally, create a compilation job on Neo.\n\nimport time\nimport boto3\n\nrole='<<<YOUR ROLE_HERE>>>' # running on SageMaker? import sagemaker; sagemaker.Session(); role = sagemaker.get_execution_role()\n\nsm_client = boto3.client('sagemaker')\ncompilation_job_name = f'{model_name}-pytorch-{int(time.time()*1000)}'\nsm_client.create_compilation_job(\n    CompilationJobName=compilation_job_name,\n    RoleArn=role,\n    InputConfig={\n        'S3Uri': s3_uri,\n        'DataInputConfig': '{\"input\": [1,3,640,640]}',\n        'Framework': 'PYTORCH'\n    },\n    OutputConfig={\n        'S3OutputLocation': f's3:\/\/{bucket_name}\/{model_name}-pytorch\/optimized\/',\n        'TargetPlatform': {\n            'Os': 'LINUX',\n            'Arch': 'ARM64', # change this to X86_64 if you need\n            #'Accelerator': 'NVIDIA' # uncomment this if you have a GPU Nvidia\n        },\n        # uncomment or change the following line depending on your edge device\n        # Jetson Xavier: sm_72; Jetson Nano: sm_53\n        #'CompilerOptions': '{\"trt-ver\": \"7.1.3\", \"cuda-ver\": \"10.2\", \"gpu-code\": \"sm_72\"}' # Jetpack 4.4.1        \n    },\n    StoppingCondition={ 'MaxRuntimeInSeconds': 18000 }\n)\n\n#check for the compilation job to complete\nwhile True:\n    resp = sm_client.describe_compilation_job(CompilationJobName=compilation_job_name)    \n    if resp['CompilationJobStatus'] in ['STARTING', 'INPROGRESS']:\n        print('Running...')\n    else:\n        print(resp['CompilationJobStatus'], compilation_job_name)\n        break\n    time.sleep(5)",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Yolo5 model deployment into SageMaker endpoint",
        "Question_creation_time":1662693076387,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhNfeNlTPQSOwfUg_rCnUpw\/yolo-5-model-deployment-into-sage-maker-endpoint",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Front-End Web & Mobile",
            "Networking & Content Delivery",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Lambda",
            "Amazon API Gateway",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":38,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a trained yolo5 model which I deployed into Real-time end-point in SageMaker. I tried almost all gpus computes available but I could not get better than 2.6 sec for inference time. This is a light model and my target is to have < 1sec inference time. Could you please help me with any hints? I transformed the model from pytorch to TF format. Thank you!",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-09T11:25:25.875Z",
                "Answer_upvote_count":0,
                "Answer_body":"Have you read the following two blog posts?\n\nMaximize TensorFlow performance on Amazon SageMaker endpoints for real-time inference\nReduce computer vision inference latency using gRPC with TensorFlow serving on Amazon SageMaker?\n\nThe first explores a few parameters that you can use to maximize the performance of a TensorFlow-based SageMaker real-time endpoint. These parameters are in essence overprovisioning serving processes and adjusting their parallel processing capabilities. As we saw in the tables, this overprovisioning and adjustment leads to better utilization of resources and higher throughput, sometimes an increase as much as 1,000%.\n\nThe second demonstrates how to reduce model serving latency for TensorFlow computer vision models on SageMaker via in-server gRPC communication, leading to gains of 75% in latency improvement in the example shown.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Can do this? multimodel-endpoint + async inferce",
        "Question_creation_time":1662629203617,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFGfav6yyR6idYoGOkgH3sA\/sagemaker-can-do-this-multimodel-endpoint-async-inferce",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0,
        "Question_view_count":61,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Following this document, I try to async inference with Multi Model Endpoint.\n\nI try to set **kwargs from Model().deploy() to MultiDataModel().deploy(**kwargs) . From that document, MultiDataModel(**kwargs) pass to Model(**kwargs). I assume MultiDataModel().deploy(**kwargs) pass to Model().deploy(**kwargs).\n\nSo, I try to like that.\n\nfrom sagemaker.async_inference.async_inference_config import AsyncInferenceConfig\n\nasync_config = AsyncInferenceConfig(\n    output_path=f\"s3:\/\/{bucket}\/{prefix}\/output\",\n    max_concurrent_invocations_per_instance=4\n)\n\nmme = MultiDataModel(\n    name='MultiModel',\n    model_data_prefix=multi_model_s3uri,\n    model=model,  # passing our model\n    sagemaker_session=sess\n)\n\npredictor = mme.deploy(\n    initial_instance_count=1,\n    instance_type=\"ml.\"+ instance_type,\n    async_inference_config  = async_config\n)\n\n\nBut, I realize that <class Predictor != class AsyncPredictor>. I want to get AsyncPredictor(not Predictor) from MultiModelEndpoint.deploy().",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-10T22:19:58.898Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nUnfortunately, the multi model is not supported to run an async inferences yet, but alternatively you could host several models within a single container and run them using BYOS strategy.\n\nHere is an example to utilize async inference for multiple models packaged in a single tar.\n\nPackage multiple models in a single tar.gz file:\nwith tarfile.open(\"multi-model.tar.gz\", \"w:gz\") as mm:\n    mm.add(\".\/multi-model\/xgboost-model1\")\n    mm.add(\".\/multi-model\/xgboost-model2\")\n\nUpload the model to S3 bucket:\nsm_session = sagemaker.session.Session()\nsm_client = boto_session.client(\"sagemaker\")\nsm_runtime = boto_session.client(\"sagemaker-runtime\")\n\n def upload_model(input_location):\n    prefix = \"async-multi-model-example\/models\n    return sm_session.upload_data(\n        input_location, \n        bucket=sm_session.default_bucket(),\n        key_prefix=\"async-multi-model-example\")\n\nmodel_url = upload_model(\"multi-model.tar.gz\")\n\nCreate a sageMaker model from pre-trained model tar.gz:\nfrom sagemaker import image_uris\ncontainer = image_uris.retrieve(region=region, framework=\"xgboost\", version=\"1.2-1\")\n\ncreate_model_response = sm_client.create_model(\n    ModelName=\"async-multi-model\",\n    ExecutionRoleArn=sm_role,\n    PrimaryContainer={\n        \"Image\": container,\n        \"ModelDataUrl\": model_url,\n    },\n)\n\nCreate an endpoint with config:\ncreate_endpoint_config_response = sm_client.create_endpoint_config(\n    EndpointConfigName=\"async-multi-model-config\",\n    ProductionVariants=[\n        {\n            \"VariantName\": \"variant1\",\n            \"ModelName\": model_name,\n            \"InstanceType\": \"ml.m5.xlarge\",\n            \"InitialInstanceCount\": 1,\n        }\n    ],\n    AsyncInferenceConfig={\n        \"OutputConfig\": {\n            \"S3OutputPath\": f\"s3:\/\/{s3_bucket}\/{bucket_prefix}\/output\",\n        },\n        \"ClientConfig\": {\"MaxConcurrentInvocationsPerInstance\": 4},\n    },\n)\n\nCreate the endpoint\ncreate_endpoint_response = sm_client.create_endpoint(\n    EndpointName=\"async-multi-model-endpoint\", EndpointConfigName=\"async-multi-model-config\"\n)\n\n\nInvoke the endpoint async:\n# Specify the location of the input. Here, a single SVM sample\ninput_location = \"s3:\/\/{s3_bucket}\/test_point_0.libsvm\"\n\nendpoint_name=\"async-multi-model-endpoint\"\nresponse = sm_runtime.invoke_endpoint_async(\n                            EndpointName=endpoint_name, \n                            InputLocation=input_location)\n\n\nThis is a customized alternative solution since the feature is not enabled yet, you may going to a bit off the beaten path with that.\n\nHope it helps",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker debugger built in rule CreateXgboostRule not generating report as expected",
        "Question_creation_time":1662562294513,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuZEDvbaeRfqOT_g2Sxlw7w\/sage-maker-debugger-built-in-rule-create-xgboost-rule-not-generating-report-as-expected",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Build & Train ML Models",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":48,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm currently working with a SageMaker hosted XGBoost model; I've added the built in rule \"CreateXgboostRule\" to generate a training report, however, only the ProfilerReport is generated in the S3 rule-output folder - the expected result based on the dev doc is for a CreateXGBoostRule folder as well within this same folder.\n\nThe code I'm using is based directly on the example provided in: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/debugger-training-xgboost-report.html\n\nimport boto3\nimport sagemaker\nfrom sagemaker.estimator import Estimator\nfrom sagemaker import image_uris\nfrom sagemaker.debugger import Rule, rule_configs\n\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\n\nregion = boto3.Session().region_name\nxgboost_container=sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n\nestimator=Estimator(\n    role=sagemaker.get_execution_role()\n    image_uri=xgboost_container,\n    base_job_name=\"debugger-xgboost-report-demo\",\n    instance_count=1,\n    instance_type=\"ml.m5.2xlarge\",\n    \n    # Add the Debugger XGBoost report rule\n    rules=rules\n)\n\nestimator.fit(wait=False)\n\n\nI've tried rewriting the estimator a number of ways, verified \"rules\" is receiving an array of objects, tried different versions of XGBoost within the region, but everything still results in the built in rule only creating the ProfilerReport with no CreateXGBoostRule directory under rule-output.\n\nAny ideas would be greatly appreciated! Thanks.",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-08T13:53:26.107Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Dennis, I tried a code above in AWS SageMaker notebook and didn't get any ProfilerReport as well.\n\nThe possible issue is that there is no data to train hence nothing to report. To prove this hypothesis I decided to take a sample XGBoost notebook and to add report functionality in it. This link tells how to access sample notebooks. I used \"xgboost_customer_churn.ipynb\".\n\nHere some changes that I made:\n\nIn order to be able to run it smoothly, instead of \"1.6-1\" XGBoost image I set the \"1.2-1\":\ncontainer = sagemaker.image_uris.retrieve(\"xgboost\", sess.boto_region_name, \"1.2-1\")\n\nI added reporting to the training cell:\nfrom sagemaker.debugger import Rule, rule_configs\n\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\nsess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(\n    container,\n    role,\n    instance_count=1,\n    instance_type=\"ml.m4.xlarge\",\n    output_path=\"s3:\/\/{}\/{}\/output\".format(bucket, prefix),\n    sagemaker_session=sess,\n    base_job_name=\"debugger-xgboost-report-demo\",\n    rules=rules\n)\n\nxgb.set_hyperparameters(\n    max_depth=5,\n    eta=0.2,\n    gamma=4,\n    min_child_weight=6,\n    subsample=0.8,\n    verbosity=0,\n    objective=\"binary:logistic\",\n    num_round=100,\n)\n\nxgb.fit({\"train\": s3_input_train, \"validation\": s3_input_validation})\n\n\nAll the rest I run without changes.\n\nHowever, it was a little confusing to fing a ProfilerReport. In this particular example it had the path: sagemaker-region-11122233\/sagemaker\/DEMO-xgboost-churn\/output\/debugger-xgboost-report-demo-2022-00-000\/rule-output\/CreateXgboostReport\/\n\nIn order to get this path you can use (run this code in the SageMaker notebook):\n\nxgb.output_path - gives the first part of the path (including the bucket name)\n\nxgb.latest_training_job.job_name - gives the second part of the path (training job name)\n\nIf you combine these two, you will get the full path towards the rule-output directory.\n\nI hope this helps.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to serve a pretrained model from hugging face in sagemaker without custom script?",
        "Question_creation_time":1662514340477,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFjO6dWOKQW2RdY8XyeP0-A\/how-to-serve-a-pretrained-model-from-hugging-face-in-sagemaker-without-custom-script",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":31,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have been working with an example , where I write my own custom script ( sample below) , where i am overriding the predict_fn and other functions. I have tested my model without the custom script or inference.py. in the event when we don't provide our custom script, how is the model called? what does the default code for predict_fn look like, when I don't override it?\n\ninference.py\n\n\nimport os\nimport json\nimport torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef model_fn(model_dir):\n   model_dir = '.\/pytorch_model.bin'\n\n    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(device).eval()\n    \n    model_dict = {'model':model, 'tokenizer':tokenizer}\n    \n    return model_dict\n        \n\ndef predict_fn(input_data, model_dict):\n \n    input = input_data.pop('inputs')\n   \n    \n    tokenizer = model_dict['tokenizer']\n    model = model_dict['model']\n\n    input_ids = tokenizer(input, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n     ....\n    \n    \ndef input_fn(request_body, request_content_type):\n    return  json.loads(request_body)",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-07T19:46:58.567Z",
                "Answer_upvote_count":0,
                "Answer_body":"Usually you have to write your own inference code. Default predict_fn function is just returning a model. You may want to check the documentation here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/adapt-inference-container.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Does SageMaker's CreateModel API support model data from versioned S3 objects?",
        "Question_creation_time":1662493921224,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmyE-SweqTvazF0y_5OJtpA\/does-sage-makers-create-model-api-support-model-data-from-versioned-s-3-objects",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":45,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"In the following AWS CLI invocation, I'm attempting to specify a ModelDataUrl which contains an embedded versionId to reference an object whose bucket has versioning enabled. Does SageMaker's CreateModel API support model data originating from a specific object version? If so, how does one specify the object version?\n\nAWS CLI Invocation:\n\naws sagemaker create-model --model-name VersionedArtifacts \\\n  --execution-role-arn arn:aws:iam::<account-id>:role\/service-role\/<role-name> \\\n  --primary-container \"Image=<account-id>.dkr.ecr.us-east-1.amazonaws.com\/<repository>:<tag>,ModelDataUrl=https:\/\/s3.us-east-1.amazonaws.com\/<versioned-bucket>\/versioned-artifacts.tar.gz?versionId=<version-id>\n\n\nResulting AWS CLI Error:\n\nAn error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at https:\/\/s3.us-east-1.amazonaws.com\/<versioned-bucket>\/versioned-artifacts.tar.gz?versionId=<version-id>.",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-14T01:20:52.840Z",
                "Answer_upvote_count":0,
                "Answer_body":"I can't rule it out completely as haven't been able to find a source in the docs, but am ~90% sure S3 object versioning is not supported here: Your code makes sense as the only way to try and send through the version parameter, and the missing model data error would make sense if the service is simply trying to read ?versionId=... as part of the key.\n\nI requested further clarification on the ContainerDefinition doc page, but believe you'll need to architect for separate object keys rather than object versions. You'll see that by default the training job API already adds a job-name-specific suffix to your provided output data location, so usually this is only a challenge if you're planning to set up a centralised repository of model.tar.gz in S3 and explicitly copy models into it.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Is my Jupyter Notebook hung?",
        "Question_creation_time":1662151013058,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmBX_HRdkTjKnKzg_15etmw\/is-my-jupyter-notebook-hung",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":33,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Is there anyway to tell if the current cell I am executing is hung? I am running an AutoGluon TabularPredictor().fit(). The cell it is executing is has the * indication that it is running, I checked the notebook running tab, and it says my notebook is running, but when I look at the AutoGluon models folder, it does not look like anything new has being written for hours.\nAre there any other ways I can check to see if my model is still trying to fit?",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"how to inference parameters to a huggingface model hosted in sagemaker?",
        "Question_creation_time":1662145723356,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeyT2jYhVQdOoGXrQxoZ4nw\/how-to-inference-parameters-to-a-huggingface-model-hosted-in-sagemaker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":27,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I created a model resource in sagemaker . the model is a tar file , downloaded from hugging face and fine tuned. based on the documentation provided ( sample code below) . the code sample is passing HF_TASK inference parameter and i assume this is hugging face specific, but is it possible to pass other parameters like padding or truncation and max_length ? such as padding : True truncation: True max_length = 512 ...\n\nhow do i pass these value?\n\nimport sagemaker \n\nhub = { \n   'HF_TASK' : 'text2text-generation'\n}\nrole = sagemaker.get_execution_role()\n\nhuggingface_model = HuggingFaceModel( transformers_version='4.6.1', env=hub...\n\npredictor = huggingface_model.deploy( ....",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"Sagemaker Region",
        "Question_creation_time":1662026822143,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyMQN5wWjQ1m_3Xp5YbhspA\/sagemaker-region",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":36,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi everyone! Im having problems trying to connect to sagemaker on us-east-2, i have no problems with other regions but every project on sagemaker is made on us-east-2, \u00bfis anyone having same problems? thank u !",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"Outgoing mail for sagemaker labeling job",
        "Question_creation_time":1662016903588,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqbxtiU_kSe-GoPcj6g0pzg\/outgoing-mail-for-sagemaker-labeling-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":36,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"When having made a labeling job on Ground Truth, an outgoing mail should be sent to team member, but in my case, mail not be sent with no error message.\n\nin case no private team created (the first job creation) : mail can be sent. (set up a team during job creation)\nin case a private team already set up: mail cannot be sent. (select a existing team during job creation)\n\nI think policies of the job role might not be enough, for example, cognito policy. How can I make sure the cause of the error?",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-02T14:53:23.761Z",
                "Answer_upvote_count":0,
                "Answer_body":"To Successfully create a SageMaker Labeling Job you will need the following Permission Policies applied within your account:\n\nThe IAM entity you have used to create the job will need permissions outlined in the \"Permissions Required to Use the Amazon SageMaker Ground Truth Console\" [1]\nYour Labelling Job Role will need SageMakerFullAccess [2]\n\nWith these permissions in place your job should create successfully.\n\nLinks to documentation provided by AWS:\n\n[1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security_iam_id-based-policy-examples.html#console-permissions\n\n[2] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security-iam-awsmanpol.html#security-iam-awsmanpol-AmazonSageMakerFullAccess",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"How can I use serverless inference with models generated by SageMaker Autopilot?",
        "Question_creation_time":1661981861600,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJpArvoZTSkiWkDMVDW1avw\/how-can-i-use-serverless-inference-with-models-generated-by-sage-maker-autopilot",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Amazon SageMaker Autopilot"
        ],
        "Question_upvote_count":0,
        "Question_view_count":32,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"There are a few articles about deploying SageMaker models to use serverless inference, but I am not clear on how to do that with autopilot models in particular. In other words, I do not understand which steps should be different and how to find information such as what my model ARN is. Thanks.",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-05T22:49:27.371Z",
                "Answer_upvote_count":0,
                "Answer_body":"At this point there is no direct integration between SageMaker Autopilot and Serverless inferencing. One way to do this take the model.tar.gz from the best training job and deploy it manually as an serverless endpoint. Usually Autopilot uses one of the SageMaker Built in Algorithms and you will be able to us the specific container to deploy it.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"PySpark on Sagemaker - force stop of the execution for all cells",
        "Question_creation_time":1661935002270,
        "Question_link":"https:\/\/repost.aws\/questions\/QUW0rkjqaNS7aNKZV3mUnFGw\/py-spark-on-sagemaker-force-stop-of-the-execution-for-all-cells",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":39,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi, I am using Jupyter Notebook on SageMaker with PySpark kernel. I want to implement some data checks so that the processing of the notebook is stopped if some conditions is meet. For Python kernel one can simply use raise statement as in the example below. If the notebook was run using \"Run All\" button then the code below would stop not only the execution of the error cell, but it would not proceed to the next cells.\n\nif type(age) is not int:\n    raise TypeError(\"Age must be an integer\")\nelif age < 0:\n    raise ValueError(\"Sorry you can't be born in the future\")\n\n\nHowever, if we use the same piece of code with PySpark kernel, execution of the error cell would be stopped but the notebook would still proceed with the execution of the next cells (as \"Run All\" was used). So in fact calculations would be proceeded even if age is not integer or is < 0.\n\nHow to force PySpark kernel to stop executing the entire notebook on error, just as it is the case for Python kernel?\n\nAdding a print screen with output notebook after \"Run All\":",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-01T10:46:43.032Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello\n\nThe functionality you have explained here is available when using the PySpark Kernel within a SageMaker Jupyter Notebook.\n\nWhen raising an error within a cell, if that error is flagged in the run time, the notebook will not continue to run the following cells even when using the Run All Option.\n\nWhen an error is met, the notebook will output the information of this error and will stop executing.\n\nI have tested this in a SageMaker Jupyter Notebook Instance with a SparkMagic PySpark Kernel and found the above to be true.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Reason why errors occur when starting SageMaker Studio",
        "Question_creation_time":1661920764293,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfTu0DsSxTf-B8n4srTzG9A\/reason-why-errors-occur-when-starting-sage-maker-studio",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":152,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello! I have a question about errors found when starting SageMaker Studio (below).\n\nAccessDeniedException User: X is not auth**orized to perform: sagemaker:CreateDomain on resource: arn:aws:sagemaker:ap-northeast-1:XX because no identity-based policy allows the sagemaker:CreateDomain action\n\nValidationException Access denied in getting\/deleting the portfolio shared by SageMaker. Please call withservicecatalog:ListAcceptedPortfolioShares permission.\n\nAccessDeniedException User: X is not authorized to perform: sagemaker:CreateUserProfile on resource: arn:aws:sagemaker:ap-northeast-1:XX because no identity-based policy allows the sagemaker:CreateUserProfile action\n\nI resolved the errors by adding some inline policies, but I cannot understand the reason why the errors occur on my user with S3 Full Access and SageMaker Full Access policies.\n\nI'd happy to tell me any information about the errors. Thank you!",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-31T12:25:56.766Z",
                "Answer_upvote_count":1,
                "Answer_body":"As far as I know, according the aws docs the passRole action should be granted to the SageMake execution role for some cases such as creating images. So your s3:* and sagemaker:* is not enough, but still need to add iam:passRole to policies",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"I want to deploy my model as a Serverless inference",
        "Question_creation_time":1661852626896,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGFC_kpAJTx6NcEG9_ZqUyQ\/i-want-to-deploy-my-model-as-a-serverless-inference",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0,
        "Question_view_count":58,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hey I trained a sickst learn model using python sdk and I want to deploy the model as a Serverless inference now. I am new to AWS and can't seem to make sense of the documentation. the model is fit it an estimator as follow:\n\nfrom sagemaker.sklearn.estimator import SKLearn\nenable_local_mode_training = False\n\n\ninputs = {\"train\": trainpath, \"test\": testpath}\n\nestimator_parameters = {\n    \"entry_point\": \"script_rf.py\",\n    \"framework_version\": \"1.0-1\",\n    \"py_version\": \"py3\",\n    \"instance_type\": 'ml.c5.xlarge',\n    \"instance_count\": 1,\n    \"role\": role,\n    \"base_job_name\": \"randomforestclassifier-model\"\n}\n\nestimator = SKLearn(**estimator_parameters)\nestimator.fit(inputs)\n\n\n\nthis works fine but now when I try to deploy it it doesn't work. I tried this code: https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploying-ml-models-using-sagemaker-serverless-inference-preview\/ but i keep getting errors because somethings are not defined like the image_uri which I am not using.\n\nI used this\n\nm_boto3 = boto3.client('sagemaker')\n\nestimator.latest_training_job.wait(logs='None')\nartifact = m_boto3.describe_training_job(\n    TrainingJobName=estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n\nprint('Model artifact persisted at ' + artifact)\n\n\n\nbut then the endpoint is not Serverless. please help",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-30T19:53:33.202Z",
                "Answer_upvote_count":0,
                "Answer_body":"To deploy a serverless endpoint on Amazon SageMaker you will need 3 steps:\n\n(1) Model creation\n\nWhere you create the model with client.create_model\n\n(2) Endpoint configuration creation\n\nWhere you create a configuration file with client.create_endpoint_config, and make sure you have a serverless configuration with two parameters: MemorySizeInMB, and MaxConcurrency within the config, which should look like\n\n\"ServerlessConfig\": {\n        \"MemorySizeInMB\": 4096,\n        \"MaxConcurrency\": 1,\n},\n\n\n(3) Endpoint creation and invocation\n\nOnce you have created a model, with endpoint configuration with serverless endpoint detail, then you can run client.create_endpoint to create a serverless endpoint.\n\nReference: Deploying ML models using SageMaker Serverless Inference",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-08-30T16:03:41.716Z",
                "Answer_upvote_count":0,
                "Answer_body":"Actually creating a serverless endpoint has become much easier because you can now use the SageMaker Python SDK to so, i.e. you don't have to use boto3 anymore, you don't have to create models or endpoint configurations anymore, and you also don't have to specify the image_uri anymore.\n\nInstead, once you your estimator you can just use these lines of code:\n\nfrom sagemaker.serverless import ServerlessInferenceConfig\nserverless_config = ServerlessInferenceConfig()\n\nserverless_config = ServerlessInferenceConfig(\n  memory_size_in_mb=4096,\n  max_concurrency=10,\n)\n\nserverless_predictor = estimator.deploy(serverless_inference_config=serverless_config)\n\n\nSee also the documentation: https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#sagemaker-serverless-inference",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to use sagemaker.processing.Processor run method",
        "Question_creation_time":1661777469790,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhZktmd-_Q3mFdOyFaNU9NA\/how-to-use-sagemaker-processing-processor-run-method",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":31,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"This is sagemaker docs. What is the purpose of sagemaker.processing.Processor as its run method does not have input for code or script? then how can I use it?\n\nOf course, I can use FrameworkProcessor, ScriptProcessor, SklearnProcessor because I can provide my processing.py. But for the sagemaker.processing.Processor, how can I use it?",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-30T01:07:27.065Z",
                "Answer_upvote_count":1,
                "Answer_body":"sagemaker.processing.Processor is the base class which is extended by ScriptProcessor, SKLearnProcessor etc... Its not recommended to directly use the base Processsor class. However you can still have a custom docker image and Run it using the base Processor module.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-09-01T00:02:44.729Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thank you, I understand the Processor is a base class. Then, find out how to provide my script to Processor via the entrypoint and run it.\n\ncontainer_base_path = \"\/opt\/ml\/processing\"\n\nprocessor = Processor(\n    role=os.environ['ROLE'],\n    image_uri=image_url,\n    instance_count=1,\n    instance_type='ml.m4.xlarge',\n    entrypoint=[\"python\", f\"{container_base_path}\/input\/process-data.py\"]\n)\n\nprocessor.run(\n    job_name=f'processor-{strftime(\"%Y-%m-%d-%H-%M-%S\")}',\n    inputs=[\n        ProcessingInput(\n            source=data_input_path,\n            destination=f\"{container_base_path}\/data\/\"\n        ),\n        ProcessingInput(\n            source=code_input_path,\n            destination=f\"{container_base_path}\/input\/\"\n        )\n    ],\n...\n\n\nI feel that SageMaker SDK is not consistent for developers because each processor has a different way to pass my code into it.\n\nProcessor: entrypoint\nScriptProcessor: command\nSklearnProcessor='my_script.py' in the run method",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to get batch transform with jsonl data?",
        "Question_creation_time":1661703317094,
        "Question_link":"https:\/\/repost.aws\/questions\/QUkP-cRiP3QiCAIqnwyirz1A\/how-to-get-batch-transform-with-jsonl-data",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon CloudWatch Logs"
        ],
        "Question_upvote_count":0,
        "Question_view_count":34,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am using my own inference.py file as a entry point for inference. I have tested this pytorch model, served as a real time endpoint in amaon sagemaker. but when i try to create a batch job and use multiple json object in my input file (jsonl format) . i get the following error at the input_fn function on this line data = json.loads(request_body), in cloudwatch logs ==>\n\ndata = json.loads(request_body) raise JSONDecodeError(\"Extra data\", s, end) json.decoder.JSONDecodeError: Extra data : line 2 column 1 (Char ..)\n\nI am not sure why am i getting extra data on line 2 error, because this is supposed to be batch job with multiple json input and each line.\n\ninference.py\n\ndef model_fn(model_dir):\n   \/\/load the model\n\n\n\n\ndef input_fn(request_body, request_content_type):\n    input_data= json.loads(request_body)\n    return data\n\ndef predict_fn(input_data, model):\n    return model.predict(input_data)\n\n\nset up batch job\n\nresponse = client.create_transform_job(\n    TransformJobName='some-job',\n    ModelName='mypytorchmodel',\n    ModelClientConfig={\n        'InvocationsTimeoutInSeconds': 3600,\n        'InvocationsMaxRetries': 1\n    },\n    BatchStrategy='MultiRecord',\n    TransformInput={\n        'DataSource': {\n            'S3DataSource': {\n                'S3DataType': 'S3Prefix',\n                'S3Uri': 's3:\/\/inputpath'\n            }\n        },\n        'ContentType': 'application\/json',\n        'SplitType': 'Line'\n    },\n    TransformOutput={\n        'S3OutputPath': 's3:\/\/outputpath',\n        'Accept': 'application\/json',\n        'AssembleWith': 'Line',\n    },\n    TransformResources={\n        'InstanceType': 'ml.g4dn.xlarge'\n        'InstanceCount': 1\n    }\n)\n\n\ninput file\n\n{\"input\" : \"some text here\"}\n{\"input\" : \"another\"}\n...",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-30T16:00:00.274Z",
                "Answer_upvote_count":1,
                "Answer_body":"You're seeing this because of your MultiRecord batch strategy: SageMaker is aware of how to split your source data files into individual records (because you configured SplitType), but is composing batches with multiple records and trying to send those through to your model\/endpoint. It seems like your inference input handler is not capable of interpreting JSONLines chunks, only single JSON objects.\n\nOne way of fixing this would be to switch to SingleRecord batch strategy, which would result in each record triggering a separate inference request to your model.\n\nIf you're concerned about the HTTP overhead of request-per-record limiting your job performance, you could alternatively stick with MultiRecord but edit your input_fn to handle JSONLines data. I'd probably suggest to set a different ContentType to explicitly signal your container when to expect JSONLines vs single-record JSON. Your input_fn can detect that different request_content_type (e.g. application\/x-jsonlines) and use a different parsing logic.\n\nI'm not 100% sure whether the request_body supports iterating through lines like a file would ([json.loads(l) for l in request_body]), whether you could treat it like a string ([json.loads(l) for l in request_body.split(\"\\n\")]), or perhaps it's a binary string you'd need to decode first e.g. request_body.decode(\"utf-8\").split(\"\\n\")... Would need to check - but something along these lines should allow you to first split your body by newlines, then parse each line as a valid JSON object.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"No such file or directory: '\/opt\/ml\/input\/data\/test\/revenue_train.csv' Sagemaker [SM_CHANNEL_TRAIN]",
        "Question_creation_time":1661681019239,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBY-fIUMuRDqwBmObCn8GqQ\/no-such-file-or-directory-opt-ml-input-data-test-revenue-train-csv-sagemaker-sm-channel-train",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Model Training",
            "Amazon SageMaker Model Building",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0,
        "Question_view_count":54,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to deploy my RandomForestClassifier on Amazon Sagemaker using Python SDK. I have been following this example https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-script-mode\/sagemaker-script-mode.ipynb but keep getting an error that the train file was not found. I think the file were not uploaded to the correct channel. When I run the script as follows it works fine.\n\n! python script_rf.py --model-dir .\/ \\\n                   --train .\/ \\\n                   --test .\/ \\\n\n\nThis is my script code:\n\n# inference functions ---------------\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n    return clf\n\nif __name__ =='__main__':\n\n    print('extracting arguments')\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--max_depth', type=int, default=2)\n    parser.add_argument('--n_estimators', type=int, default=100)\n    parser.add_argument('--random_state', type=int, default=0)\n    \n\n    # Data, model, and output directories\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--train-file', type=str, default='revenue_train.csv')\n    parser.add_argument('--test-file', type=str, default='revenue_test.csv')\n    \n    args, _ = parser.parse_known_args()\n    \n    print('reading data')\n    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n    \n    if len(train_df) == 0:\n        raise ValueError(('There are no files in {}.\\n').format(args.train, \"train\"))\n\n    print('building training and testing datasets')\n    attributes = ['available_minutes_100','ampido_slots_amount','ampido_slots_amount_100','ampido_slots_amount_200','ampido_slots_amount_300','min_dist_loc','count_event','min_dist_phouses','count_phouses','min_dist_stops','count_stops','min_dist_tickets','count_tickets','min_dist_google','min_dist_psa','count_psa']\n    X_train = train_df[attributes]\n    X_test = test_df[attributes]\n    y_train = train_df['target']\n    y_test = test_df['target']\n    \n    # train\n    print('training model')\n    model = RandomForestClassifier(\n        max_depth =args.max_depth, n_estimators = args.n_estimators)\n    \n    model.fit(X_train, y_train)\n     \n    # persist model\n    path = os.path.join(args.model_dir, \"model_rf.joblib\")\n    joblib.dump(model, path)\n    print('model persisted at ' + path)\n    \n    # print accuracy and confusion matrix \n    print('validating model')\n    y_pred=model.predict(X_test) \n    print('Confusion Matrix:')\n    result = confusion_matrix(y_test, y_pred)\n    print(result)\n    print('Accuracy:')\n    result2 = accuracy_score(y_test, y_pred)\n    print(result2)\n\n\nthe error is raised in the train_df line of the script (FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/input\/data\/test\/revenue_train.csv').\n\nI tried specifying the input parameters:\n\n# change channel input dirs \ninputs = {\n    \"train\": \"ampido-exports\/production\/revenue_train\",\n    \"test\": \"ampido-exports\/production\/revenue_test\",\n}\nfrom sagemaker.sklearn.estimator import SKLearn\nenable_local_mode_training = False\n\n\nhyperparameters = {\"max_depth\": 2, 'random_state':0, \"n_estimators\": 100}\n\nif enable_local_mode_training:\n    train_instance_type = \"local\"\n    inputs = {\"train\": trainpath, \"test\": testpath}\n\nelse:\n    train_instance_type = \"ml.c5.xlarge\"\n    inputs = {\"train\": trainpath, \"test\": testpath}\n\nestimator_parameters = {\n    \"entry_point\": \"script_rf.py\",\n    \"framework_version\": \"1.0-1\",\n    \"py_version\": \"py3\",\n    \"instance_type\": train_instance_type,\n    \"instance_count\": 1,\n    \"hyperparameters\": hyperparameters,\n    \"role\": role,\n    \"base_job_name\": \"randomforestclassifier-model\",\n    'channel_input_dirs' : inputs\n}\n\nestimator = SKLearn(**estimator_parameters)\nestimator.fit(inputs)\n\n\nbut i still get the error FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/input\/data\/test\/revenue_train.csv",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-29T07:18:21.655Z",
                "Answer_upvote_count":1,
                "Answer_body":"Your code has a typo that results in the issue, maybe caused by copy-pasting. Note that instead of a training path you're passing a test path location as the default to args.train:\n\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n                           ^^^^^                                                ^^^^\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n\nLater, when you try to access the training file from the test location, the file is obviously not there:\n\nNo such file or directory: '\/opt\/ml\/input\/data\/test\/revenue_train.csv\n                                               ^^^^         ^^^^^\n\n\nChanging the default argument for the args.train to SM_CHANNEL_TRAIN should resolve the issue,",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to get logs or print statements from SageMaker PyTorch deployed endpoint?",
        "Question_creation_time":1661549640227,
        "Question_link":"https:\/\/repost.aws\/questions\/QU74MThjkyRVCtySw-DEozrQ\/how-to-get-logs-or-print-statements-from-sage-maker-py-torch-deployed-endpoint",
        "Question_topic":[
            "Machine Learning & AI",
            "Developer Tools",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Monitoring & Logging",
            "Amazon CloudWatch Logs",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0,
        "Question_view_count":48,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I've deployed an extended Pytorch model as an endpoint and I'm trying to make inference requests to it. Problem is, the responses from the endpoint get timed out and CloudWatch logs show nothing beyond:\n\ntimestamp\tmessage\n1661544743589\tWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n1661544749569\tModel server started.\n\nNow in my inference.py file, which I provided as the entry point I've set logging as follows:\n\nimport logging\nimport sys\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(logging.StreamHandler(sys.stdout))\nlogger.info(\"Loading file.\")\nprint(\"Loading file.\")\n\n\nI wish to see those logs\/prints. How can I accomplish that?",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-30T16:47:09.082Z",
                "Answer_upvote_count":0,
                "Answer_body":"I've come across similar issues in the past of log messages not making it through to CloudWatch, and can suggest setting environment variable PYTHONUNBUFFERED=1 (discussed further here on StackOverflow wrt containerized Python in general).\n\nThe procedure for this may vary a little depending how you're creating your model, endpoint config & endpoint (e.g. direct boto3\/API calls, SageMaker SDK Estimator.deploy() or PyTorchModel). PyTorchModel should accept an env={\"PYTHONUNBUFFERED\": \"1\"} constructor argument for example.\n\nIf you are using the SageMaker Python SDK, do watch out that some methods (especially shortcuts like Estimator.deploy()) may re-use existing models & endpoint configs rather than re-creating each time they're run. Check you see the environment variable set in the SageMaker > Inference > Models > {Your Model Name} details page in AWS Console, and run DeleteModel first if needed to force an update!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-08-30T16:30:29.746Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nPlease note that you can view the logs under CloudWatch logs. There would be a generated log stream under your pytorch-inference tab when it is inService (Navigate to inference tab under SageMaker console > Endpoints > click on the endpoint name > locate \"view logs\"). The previous steps will take you to CloudWatch logs console. You then click on log groups and locate \/aws\/sagemaker\/Endpoints\/pytorch-inference-YYYY-MM-DD-HH-MM-SS-sss > AllTraffic\/i-instanceId. For example, for the code snippet you shared, If you add the lines of code below for logging purpose within the inference.py script):\n\nimport logging\nimport sys\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(logging.StreamHandler(sys.stdout))\nlogger.info(\"Loading file.\")\nprint(\"Loading file. --> from print statement\")\n\n# rest of the inference script from here\n\n\nThe above will show up under AllTraffic\/instance-id (once the endpoint is inService) as the following:\n\n2022-08-30 15:48:42,183 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Loading file.\n2022-08-30 15:48:42,936 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Loading file. --> from print statement\n\n\nFrom above, you can see that print statements will show up as INFO level log.\n\nPlease see the link [1] for more details on how the log stream is captured for inference jobs. If you have logging statements within your input_fn or say within your predict_fn function, those statements will show up when there is a prediction\/scoring made. I hope the shared information is helpful.\n\nReference: [1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-logs-metrics.html#inference-pipeline-logs\n\nplease see @Alex_T answer for PYTHONUNBUFFERED=1 which forces print\/logging to stdout. If you are bringing your own container you can define this parameter in the Dockerfile.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Jupyter Notebook",
        "Question_creation_time":1661528978544,
        "Question_link":"https:\/\/repost.aws\/questions\/QUowhjHLZvR3aS6tDhH38uZQ\/jupyter-notebook",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":63,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I want to start a Jupyter Notebook instance, then load a ipynb file I have uploaded into a bucket. How do I this? I have never been able to initiate Sagemaker as of yet.",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-28T13:02:55.531Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can follow this tutorial to start a Jupyter Notebook instance on SageMaker. Once the instance is running and you are in the notebook environment you can upload your ipynb file.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-08-29T10:26:08.649Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can also try the free service AWS SageMaker Studio Lab https:\/\/studiolab.sagemaker.aws\/",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker GT Streaming Labelling Job Internal Server Error (Job Failed)",
        "Question_creation_time":1661525322145,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZD0isCgLSYqxXhdah_0jkQ\/sage-maker-gt-streaming-labelling-job-internal-server-error-job-failed",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth",
            "Amazon CloudWatch Logs"
        ],
        "Question_upvote_count":0,
        "Question_view_count":61,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I launched a Sagemaker Ground Truth Labeling Job with a vendor for 3D Point Cloud Object Tracking using the API. Yesterday, my job failed with the reason for failure\n\nInternalServerError: We encountered an internal error. Submit a new job.\n\n\nThe last three Cloudwatch logs before I saw the failure were\n\n{ \"labeling-job-name\": \"scand-trial2-seq10v2stream\", \"event-name\": \"BATCH_ANNOTATION_STATUS_EVALUATED\", \"event-log-message\": \"Batch of annotation tasks completed with status FAILED. \" }\n\n{ \"labeling-job-name\": \"scand-trial2-seq10v2stream\", \"event-name\": \"EXPORTED_LABELED_MANIFEST\", \"event-log-message\": \"Labeled manifest written to S3 output location.\" }\n\n{ \"labeling-job-name\": \"scand-trial2-seq10v2stream\", \"event-name\": \"IDLE_TIMER_COUNT_INTERRUPTED\", \"event-log-message\": \"System detected incoming objects. Timer to monitor idleness was reset.\" }\n\nMy vendor confirmed that they had annotations saved before this happened, but my output manifest is empty. In addition, I set the expiration time for my job to be 30 days, and this failed at 20 days. What could have caused this error and is there any known method for retrieving the worker annotations that were saved but not submitted?",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-01T20:36:14.799Z",
                "Answer_upvote_count":0,
                "Answer_body":"I am afraid to inform you that the \"Internal Server Error\" can occur due to various reasons, In order to deep-dive further and investigate in to the service level logs we would request you to open a support ticket with the AWS premium support with the following information.\n\n-- Account Id\n\n-- Region\n\n-- Failed Job ARNs and cloudwatch logs\n\nNote :Due to security reason, this post is not suitable for sharing customer's resource and account details.\n\nThank you.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"deploying previously trained model with Sagemaker Python SDK (StatusExceptionError)",
        "Question_creation_time":1661503967725,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXT-lr_7ASxSSzx0lRAaEvg\/deploying-previously-trained-model-with-sagemaker-python-sdk-status-exception-error",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Model Training",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0,
        "Question_view_count":32,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am using a pertained Random Forest Model and trying to deploy it on Amazon Sagemker using Python SDK:\n\nfrom sagemaker.sklearn.estimator import SKLearn\n\nsklearn_estimator = SKLearn(\n    entry_point='script.py',\n    role = get_execution_role(),\n    instance_count=1,\n    instance_type='ml.m4.xlarge',\n    framework_version='0.20.0',\n    base_job_name='rf-scikit')\n\nsklearn_estimator.fit({'train':trainpath, 'test': testpath}, wait=False)\n\nsklearn_estimator.latest_training_job.wait(logs='None')\nartifact = m_boto3.describe_training_job(\n    TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n\nprint('Model artifact persisted at ' + artifact)\n\n\nI get the following StatusException Error\n\n2022-08-25 12:03:27 Starting - Starting the training job....\n2022-08-25 12:03:52 Starting - Preparing the instances for training............\n2022-08-25 12:04:55 Downloading - Downloading input data......\n2022-08-25 12:05:31 Training - Downloading the training image.........\n2022-08-25 12:06:22 Training - Training image download completed. Training in progress..\n2022-08-25 12:06:32 Uploading - Uploading generated training model.\n2022-08-25 12:06:43 Failed - Training job failed\n---------------------------------------------------------------------------\nUnexpectedStatusException                 Traceback (most recent call last)\n<ipython-input-37-628f942a78d3> in <module>\n----> 1 sklearn_estimator.latest_training_job.wait(logs='None')\n      2 artifact = m_boto3.describe_training_job(\n      3     TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n      4 \n      5 print('Model artifact persisted at ' + artifact)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n   2109             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   2110         else:\n-> 2111             self.sagemaker_session.wait_for_job(self.job_name)\n   2112 \n   2113     def describe(self):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_job(self, job, poll)\n   3226             lambda last_desc: _train_done(self.sagemaker_client, job, last_desc), None, poll\n   3227         )\n-> 3228         self._check_job_status(job, desc, \"TrainingJobStatus\")\n   3229         return desc\n   3230 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n   3390                 message=message,\n   3391                 allowed_statuses=[\"Completed\", \"Stopped\"],\n-> 3392                 actual_status=status,\n   3393             )\n   3394 \n\nUnexpectedStatusException: Error for Training job rf-scikit-2022-08-25-12-03-25-931: Failed. Reason: AlgorithmError: framework error: \nTraceback (most recent call last):\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_trainer.py\", line 84, in train\n    entrypoint()\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py\", line 39, in main\n    train(environment.Environment())\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py\", line 35, in train\n    runner_type=runner.ProcessRunnerType)\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/entry_point.py\", line 100, in run\n    wait, capture_error\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py\", line 291, in run\n    cwd=environment.code_dir,\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py\", line 208, in check_error\n    info=extra_info,\nsagemaker_training.errors.ExecuteUserScriptError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"\"\nCommand \"\/miniconda3\/bin\/python script.py\"\n\nExecuteUserScriptErr\n\n\nThe pertained model works fine and I don't know what the problem is, please help",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-26T14:45:09.876Z",
                "Answer_upvote_count":0,
                "Answer_body":"You might consider reviewing your 'script.py' entry point. There could be a variety of reasons for a training job to fail but the most likely, I can see, from the description and output would be related to \"where\" the model artifacts were written to within your script.\n\nThe SageMaker Github examples contain has an example of using a RandomForestRegressor in a script - https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-script-mode\/sagemaker-script-mode.ipynb\n\nI'm sharing this example because if you refer to the Scikit-learn section, you'll find the \"train_deploy_scikitlearn_without_dependencies.py\" script is referenced and the model is dumped to the model_dir: joblib.dump(model, os.path.join(args.model_dir, \"model.joblib\")). If we were to change that to some arbitrary location in the script then the example training job would fail with an AlgorithmError: framework error as well. As long as the 10 second training is expected then I see the output location as a likely cause.\n\nFor more details on this you can refer to the following two resources:\n\nHow Amazon SageMaker Processes Training Output - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-output.html\nUsing the SageMaker Python SDK - https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\n\nIn the first resource, you'll find that your algorithm should write all final model artifacts to opt\/ml\/model. In the second resource, you'll find more information on proper use of the SageMaker Python SDK and various implementations.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Deploying a Random Forest Model on Amazon Sagemaker always getting a UnexpectedStatusException with Reason: AlgorithmError",
        "Question_creation_time":1661503022955,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMmnoFG_HQ0qiMWxlSlPlcQ\/deploying-a-random-forest-model-on-amazon-sagemaker-always-getting-a-unexpected-status-exception-with-reason-algorithm-error",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Developer Tools",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Lambda",
            "AWS CodeDeploy",
            "Amazon SageMaker",
            "Amazon Machine Images (AMI)"
        ],
        "Question_upvote_count":0,
        "Question_view_count":21,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hey I am trying to deploy my RandomForest Classifier on Amazon Sagemaker but get a StatusException Error even though the script worked fine before:\n\nThe script runs fine and prints out the confusion matrix and accuracy as expected. When I try to deploy the model to amazon Sagemaker using the script it does not work.\n\n! python script.py --n-estimators 100\n--max_depth 2\n--model-dir .\/\n--train .\/\n--test .\/ \\\n\nConfusion Matrix: [[13 8] [ 1 17]] Accuracy: 0.7692307692307693\n\nI used the Estimator from Sagemaker Python SDK\n\nfrom sagemaker.sklearn.estimator import SKLearn sklearn_estimator = SKLearn( entry_point='script.py', role = get_execution_role(), instance_count=1, instance_type='ml.m4.xlarge', framework_version='0.20.0', base_job_name='rf-scikit')\n\nI launched the training job as follows\n\nsklearn_estimator.fit({'train':trainpath, 'test': testpath}, wait=False)\n\nHere I am trying to deploy the model which leads to the StatusExceptionError that I cannot seem to fix\n\nsklearn_estimator.latest_training_job.wait(logs='None') artifact = m_boto3.describe_training_job( TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts'['S3ModelArtifacts']\n\nprint('Model artifact persisted at ' + artifact)\n\n2022-08-25 12:03:27 Starting - Starting the training job.... 2022-08-25 12:03:52 Starting - Preparing the instances for training............ 2022-08-25 12:04:55 Downloading - Downloading input data...... 2022-08-25 12:05:31 Training - Downloading the training image......... 2022-08-25 12:06:22 Training - Training image download completed. Training in progress.. 2022-08-25 12:06:32 Uploading - Uploading generated training model. 2022-08-25 12:06:43 Failed - Training job failed\n\nUnexpectedStatusException Traceback (most recent call last) <ipython-input-37-628f942a78d3> in <module> ----> 1 sklearn_estimator.latest_training_job.wait(logs='None') 2 artifact = m_boto3.describe_training_job( 3 TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts'] 4 5 print('Model artifact persisted at ' + artifact)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in wait(self, logs) 2109 self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs) 2110 else: -> 2111 self.sagemaker_session.wait_for_job(self.job_name) 2112 2113 def describe(self):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_job(self, job, poll) 3226 lambda last_desc: _train_done(self.sagemaker_client, job, last_desc), None, poll 3227 ) -> 3228 self._check_job_status(job, desc, \"TrainingJobStatus\") 3229 return desc 3230\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name) 3390 message=message, 3391 allowed_statuses=[\"Completed\", \"Stopped\"], -> 3392 actual_status=status, 3393 ) 3394\n\nUnexpectedStatusException: Error for Training job rf-scikit-2022-08-25-12-03-25-931: Failed. Reason: AlgorithmError: framework error: Traceback (most recent call last): File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_trainer.py\", line 84, in train entrypoint() File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py\", line 39, in main train(environment.Environment()) File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py\", line 35, in train runner_type=runner.ProcessRunnerType) File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/entry_point.py\", line 100, in run wait, capture_error File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py\", line 291, in run cwd=environment.code_dir, File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py\", line 208, in check_error info=extra_info, sagemaker_training.errors.ExecuteUserScriptError: ExecuteUserScriptError: ExitCode 1 ErrorMessage \"\" Command \"\/miniconda3\/bin\/python script.py\"\n\nExecuteUserScriptErr\n\nI am happy for some help",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"SKLearn Processing Container - Error: \"WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager.\"",
        "Question_creation_time":1661439501840,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVtb8YdmWSG2wFr5aMI5MaA\/sk-learn-processing-container-error-warning-running-pip-as-the-root-user-can-result-in-broken-permissions-and-conflicting-behaviour-with-the-system-package-manager",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers",
            "Management & Governance"
        ],
        "Question_tag":[
            "AWS Deep Learning Containers",
            "Amazon SageMaker",
            "Containers",
            "Amazon CloudWatch Logs"
        ],
        "Question_upvote_count":0,
        "Question_view_count":54,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hey all,\n\nI am trying to run the script below in the writefile titled \"vw_aws_a_bijlageprofile.py\". This code has worked for me using other data sources, but now I am getting the following error message from the CloudWatch Logs:\n\n\"***2022-08-24T20:09:19.708-05:00\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https:\/\/pip.pypa.io\/warnings\/venv***\"\n\nAny idea how I get around this error?\n\nFull code below.\n\nThank you in advance!!!!\n\n%%writefile vw_aws_a_bijlageprofile.py\n\nimport os\nimport sys\nimport subprocess\ndef install(package):\n    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\ninstall('awswrangler')\ninstall('tqdm')\ninstall('pandas')\ninstall('botocore')\ninstall('ruamel.yaml')\ninstall('pandas-profiling')\nimport awswrangler as wr\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom dateutil.relativedelta import relativedelta\nfrom string import Template\nimport gc\nimport boto3\n\nfrom pandas_profiling import ProfileReport\n\nclient = boto3.client('s3')\nsession = boto3.Session(region_name=\"eu-west-2\")\n\n\ndef run_profile():\n\n\n\n    query = \"\"\"\n    SELECT  * FROM \"intl-euro-archmcc-database\".\"vw_aws_a_bijlage\"\n    ;\n    \"\"\"\n                                        #swich table name above\n        \n    tableforprofile = wr.athena.read_sql_query(query,\n                                            database=\"intl-euro-archmcc-database\",\n                                            boto3_session=session,\n                                            ctas_approach=False,\n                                            workgroup='DataScientists')\n    print(\"read in the table queried above\")\n\n    print(\"got rid of missing and added a new index\")\n\n    profile_tblforprofile = ProfileReport(tableforprofile, \n                                  title=\"Pandas Profiling Report\", \n                                  minimal=True)\n\n    print(\"Generated table profile\")\n                                      \n    return profile_tblforprofile\n\n\nif __name__ == '__main__':\n\n    profile_tblforprofile = run_profile()\n    \n    print(\"Generated outputs\")\n\n    output_path_tblforprofile = ('\/opt\/ml\/processing\/output\/profile_vw_aws_a_bijlage.html')\n                                    #switch profile name above\n    print(output_path_tblforprofile)\n    \n    profile_tblforprofile.to_file(output_path_tblforprofile)\n\n\nimport sagemaker\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsession = boto3.Session(region_name=\"eu-west-2\")\n\nbucket = 'intl-euro-uk-datascientist-prod'\n\nprefix = 'Mark'\n\nsm_session = sagemaker.Session(boto_session=session, default_bucket=bucket)\nsm_session.upload_data(path='vw_aws_a_bijlageprofile.py',\n                                bucket=bucket,\n                                key_prefix=f'{prefix}\/source')\n\nimport boto3\n#import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nregion = boto3.session.Session().region_name\n\n\nS3_ROOT_PATH = \"s3:\/\/{}\/{}\".format(bucket, prefix)\n\nrole = get_execution_role()\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     sagemaker_session=sm_session,\n                                     instance_type='ml.m5.24xlarge',\n                                     instance_count=1)\n\nsklearn_processor.run(code='s3:\/\/{}\/{}\/source\/vw_aws_a_bijlageprofile.py'.format(bucket, prefix),\n                      inputs=[],\n                      outputs=[ProcessingOutput(output_name='output',\n                                                source='\/opt\/ml\/processing\/output',\n                                                destination='s3:\/\/intl-euro-uk-datascientist-prod\/Mark\/IODataProfiles\/')])",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-29T12:35:36.340Z",
                "Answer_upvote_count":0,
                "Answer_body":"This is not an error. This is just a warning message that something can go wrong, e. g. on your local machine or on the on-premise server, where you usually setup a virtual env.\n\nIn contrast, Amazon SageMaker is a managed cloud service that is designed to work with containers and to run your code as root without a virtual env. There are no different package managers, there's only one package manager, and no conflicting behaviour can occur in this scenario.\n\nSo, this warning is expected in the SageMaker environment and you can just safely ignore this log message.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Neo with SageMaker Serverless Inference possible?",
        "Question_creation_time":1661345484756,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEjedWnu4Q5iwaGVNul__1w\/sage-maker-neo-with-sage-maker-serverless-inference-possible",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":36,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Can I use SageMaker Neo to compile my model and then deploy it to SageMaker Serverless inference? Which instance type in Neo should I use in that case?",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"Can Amazon Comprehend extract data from documents?",
        "Question_creation_time":1661289910533,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1g7uVKhDTfWHIZpFk-a1hA\/can-amazon-comprehend-extract-data-from-documents",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Comprehend"
        ],
        "Question_upvote_count":0,
        "Question_view_count":42,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi! My team and I have the following scenario: we want to extract some fields from several PDF documents, that may or may not follow the same pattern. To exemplify, let's say we want to extract these 3 fields from these documents:\n\nSo, we have a Name, a Code (called CNPJ) for this person, and its Address. Obviously, these fields would vary between documents, but the CNPJ would always keep its format, only changing the sequence of numbers. During our research to solve this challenge, we came across Amazon Comprehend and its Custom Named Entity Recognition. Our idea was to create these three entities - Name, CNPJ and Address - using a Ground Truth Labeling Job.\n\nTo do this, we Textracted some of our PDF's, generating .txt files for each one of them, and then uploaded these files to an S3 Bucket. After that, we proceeded to create the Labeling Job, using an Automated data setup to generate the input manifest file so the labeling could start. And what happened was that as I inputted many .txt files, each line in these files got recognized as a separate object, resulting in more than 7700 objects to be labeled. Of course, approximately 90% of these objects didn't had any labeling to be done, resulting in me having to continuously skip these lines until I had to label one of those objects, and also in a very high money cost due to the high number of objects.\n\nSo, I have a few questions. For starters, was Amazon Comprehend a good choice for this job? If it wasn't, what would be the best solution? If it was a good choice, what could I have done to optimize the labeling job? Were the \"useless\" objects really necessary?",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-24T09:20:53.080Z",
                "Answer_upvote_count":2,
                "Answer_body":"I'm not sure you need Amazon Comprehend to achieve your goals. Amazon Textract supports 'Form Extraction' which is designed to find key-value pairs as in your example. Take a look at the docs: https:\/\/docs.aws.amazon.com\/textract\/latest\/dg\/how-it-works-analyzing.html\n\nI hope this helps!",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to specify target feature in Sagemaker XGBoost?",
        "Question_creation_time":1661221322082,
        "Question_link":"https:\/\/repost.aws\/questions\/QUoW_FqSbIQKW0MqNJLlA2AA\/how-to-specify-target-feature-in-sagemaker-xg-boost",
        "Question_topic":[
            "Machine Learning & AI",
            "Database"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Extract Transform & Load Data"
        ],
        "Question_upvote_count":0,
        "Question_view_count":26,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I am considering migrating a data science project from Datarobot to Sagemaker. I am familiar with writing Python and have been going through one of the tutorial Jupyter notebooks to see how to explore the data and to build and deploy and estimator. But, I cannot see how to specify the target feature. I have entirely numerical data in a csv file. One of the fields in that file is the intended target for estimation, the rest are information from which the estimate is to be made.\n\nHow do I specify the column that is to be estimated? The code I expect should have this is ...\n\ncontainer = sm.image_uris.retrieve(\"xgboost\", session.boto_region_name, \"1.5-1\")\n\nxgb = sm.estimator.Estimator(\n    container,\n    role,\n    instance_count=1,\n    instance_type=\"ml.m4.xlarge\",\n    output_path=\"s3:\/\/xxxxxx001\/\",\n    sagemaker_session=session,\n)\n\nxgb.set_hyperparameters(\n    max_depth=5,\n    eta=0.2,\n    gamma=4,\n    min_child_weight=6,\n    subsample=0.8,\n    verbosity=0,\n    num_round=100,\n)\ns3_input_train = TrainingInput(\n    s3_data=\"s3:\/\/xxxxxx001\/data.csv\", content_type=\"csv\"\n)\nxgb.fit({\"train\": s3_input_train})",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-23T23:42:54.174Z",
                "Answer_upvote_count":0,
                "Answer_body":"On a badly formatted page on the AWS documentation, I found a statement that - the CSV file must have no headers and the target field must be the first field. So, apparently, it is not possible to specify the target. So primitive, yeah?",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"[Solved]download image from S3 to Endpoint(made by Sagemaker) with s3url(s3:\/\/~~)",
        "Question_creation_time":1660886637665,
        "Question_link":"https:\/\/repost.aws\/questions\/QULB64ZDsXSPuHBjqAWik3hQ\/solved-download-image-from-s-3-to-endpoint-made-by-sagemaker-with-s-3-url-s-3",
        "Question_topic":[
            "Machine Learning & AI",
            "Storage"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "S3 Select",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0,
        "Question_view_count":102,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I try to download image(.jpg, .png.) from S3 to Endpoint(made by Sagemaker) with s3url(s3:\/\/~~)\n\nBecause At the endpoint made by sagemaker, To send s3url is faster than to send image.\n\nI can download image at sagemaker notebook, from s3 to sagemaker local.\n\nbut I can't download image from s3 to sagemaker endpoint.\n\nThat local download code can not work.",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-23T00:55:10.041Z",
                "Answer_upvote_count":0,
                "Answer_body":"I solve this! I try to download image at endpoint. but endpoint can not connect outside network except Lambda.\n\nI make request with s3url\nDownload image from s3 to lambda\nTransmit image from lambda to endpoint",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Ask AWS SageMaker",
        "Question_creation_time":1660876307100,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCKKplP7ES22DuZf8QJ38JA\/ask-aws-sage-maker",
        "Question_topic":[
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_upvote_count":0,
        "Question_view_count":35,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Can we make a new code through the sagemaker studio?\nIn my computer, GPU is GTX2080ti model, so if I use AWS sagemaker for paid service, can I get better performance?\nHow much GPU performance can you improve compared to before?\nI want to proceed with object segmentation through AWS sagemaker, can I use the code I used through sagemaker studio?",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-25T11:35:31.608Z",
                "Answer_upvote_count":0,
                "Answer_body":"My apologies, I am not fully sure on all the questions. But let me still make an attempt to respond to see if it helps.\n\nYes, you can write your own custom code through SageMaker studio.\n\nThis may not be an apple to apple comparison. The main advantage in this context, is your able to scale out your training to multiple nodes and cores (if your underlying model supports that). Likewise you can scale out the deployment as well. Typically the studio notebook is backed by a lightweight EC2 instance, but there are a large range of EC2 instances for training on SageMaker. Please refer to the following links for further assistance. 1. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks-available-instance-types.html 2. https:\/\/aws.amazon.com\/ec2\/instance-types\/\n\nPlease refer to the response above for question # 2.\n\nDid you mean semantic segmentation? If yes, the answer is yes too.\n\nHope that helps!\n\nRegards, Punya",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"AWS SageMaker Notebook Instance is not continuing running the cell when I leave my Laptop to execute the cells over a period of time. Can you tell me how can I solve this?",
        "Question_creation_time":1660857294691,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPbfNFX-sQhmUncaRIpCw_A\/aws-sage-maker-notebook-instance-is-not-continuing-running-the-cell-when-i-leave-my-laptop-to-execute-the-cells-over-a-period-of-time-can-you-tell-me-how-can-i-solve-this",
        "Question_topic":[
            "Machine Learning & AI",
            "Compute",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Compute",
            "DevOps"
        ],
        "Question_upvote_count":0,
        "Question_view_count":105,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello, When I leave the SageMaker Jupyter notebook to execute the cells for a long period of time, after about 8 hours, it is signing me out of the console and when I log back in, it is not running\/continuing its execution of the cells. There was No error message displayed when this happened. When I left the notebook running, the laptop was ON and the screen was active. Could you tell me how can I leave the notebook to run for a long period of time without worrying about being active on the laptop all the time? Thank you.",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-25T12:02:51.397Z",
                "Answer_upvote_count":0,
                "Answer_body":"You may review this blog https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-a-custom-entity-recognizer-for-pdf-documents-using-amazon-comprehend\/ . It has a detailed example now how to build customer recognizer for PDF documents on Amazon Comprehend.\n\nTo create the annotation for the PDF documents, it does it Amazon Ground Truth.\n\nHope this helps!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-09-09T04:56:00.842Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello - if you are being signed out of the AWS console, you may need to set a longer IAM session duration, e.g. to 12 hours.\n\nIf you are not being logged out of AWS console and your SageMaker notebook is still timing out, you may need to run your scripts using nohup command or use SageMaker training so training will continue even if you get disconnected from your Sagemaker notebook.\n\nHope this helps. Thank you!",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Jumpstart 'Explain Credit Decisions' fails to deploy",
        "Question_creation_time":1660818001611,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMx_45pb_TLOfMOZINMG2JA\/sagemaker-jumpstart-explain-credit-decisions-fails-to-deploy",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Glue",
            "Amazon SageMaker JumpStart"
        ],
        "Question_upvote_count":0,
        "Question_view_count":85,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Im trying to use the Sagemaker Jumpstart 'Explain Credit Decisions' via the Studio Jumpstart menu. However, everything works as instructed until it hits the 'glue.wait_for_workflow_finished(config.GLUE_WORKFLOW, glue_run_id)' step in the datasets notebook.\n\nThis produces a \"failed to execute with exception Internal service error: Invalid Input Provided\" (error in the Glue console) and falls over on the job part of the glue job.\n\nDoes anyone have any ideas? This is as much information as is available in the console logs.",
        "Answers":[
            {
                "Answer_creation_date":"2022-09-01T23:31:00.426Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello, Hope you are doing well.\n\nTo unblock the issue, Please find the below instruction.\n\nIf you are using the guleRole: AmazonSageMakerServiceCatalogProductsGlueRole. Nothing need to be done. It should be good to execute the notebook with the latest version.\n\nIf you are using the role: AmazonSageMakerServiceCatalogProductsUseRole. Please follow the instruction below:\n\nAttach the following inline policy to AmazonSageMakerServiceCatalogProductsUseRole and AmazonSageMakerServiceCatalogProductsGlueRole:\n\n  Navigate to IAM Console, find the role named AmazonSageMakerServiceCatalogProductsUseRole.\n  Click on the role -> click \"Add Permissions\" -> Create Inline Policy\n  In the pop up menu, for \"Service\", choose \"Glue\"\n  In \"Actions\", choose \"Read\" permission, and select \"GetUserDefinedFunctions\".\n  In \"Resources\", choose \"All Resources\" -> Review Policy\n  Name the inline policy as \"glueGetUserDefinedFunctions\" and then click \"Create Policy\".\n  Repeat the above steps to add the same inline policy for AmazonSageMakerServiceCatalogProductsGlueRole\n  Return to SageMaker Solutions Notebook and re-run the Glue workflow.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to check smdistributed-modelparallel version?",
        "Question_creation_time":1660805972479,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsfpWY8CuRsiyHg_x7qyJzw\/how-to-check-smdistributed-modelparallel-version",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_upvote_count":0,
        "Question_view_count":31,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"According to the doc ( https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/smd_model_parallel_general.html ), there are different parameters depending on the version of smdistributed-modelparallel module \/ package. However, I am unable to find a way to check the version (e.g. via sagemaker python SDK) or just from the training container documentation (e.g. https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md#huggingface-training-containers ).\n\nAny idea?\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-18T07:32:42.126Z",
                "Answer_upvote_count":0,
                "Answer_body":"Have not yet found a programmatic way to check the version.\n\nHowever, for each DLC (Deep Learning Container) available at https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md , we can look at the corresponding docker build files.\n\nE.g. for PyTorch 1.10.2 with HuggingFace transformers DLC, the corresponding dockerfile is here: https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/huggingface\/pytorch\/training\/docker\/1.10\/py3\/cu113\/Dockerfile.gpu\n\nAnd we can see that the version: smdistributed_modelparallel-1.8.1-cp38-cp38-linux_x86_64.whl.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Can't Import Modules In Sagemaker Jupyter Notebook",
        "Question_creation_time":1660764982506,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBSiq8Lx5St-1D8CT3k328g\/cant-import-modules-in-sagemaker-jupyter-notebook",
        "Question_topic":[
            "Developer Tools",
            "Machine Learning & AI",
            "Analytics",
            "Compute"
        ],
        "Question_tag":[
            "Developer Tools",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Elastic MapReduce",
            "FPGA Development"
        ],
        "Question_upvote_count":0,
        "Question_view_count":117,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I've tried to use the pip install librosa command within a jupyter notebook, yet I get a modulenotfounderror. Is there a place where I can use pip install librosa?\n\nThanks.",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-17T19:57:53.668Z",
                "Answer_upvote_count":2,
                "Answer_body":"Tested in one of the notebook in my account and it works with !pip install librosa\n\nPlease review the configuration of your notebook instances, under the Network section, you should see the result as below:\n\n*No custom VPC settings applied.\n\nDirect internet access *",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"how does input_fn, predict_fn and output_fn work in aws sagemaker script mode?",
        "Question_creation_time":1660713837979,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMJK2lci1RZa81gB0P--NWg\/how-does-input-fn-predict-fn-and-output-fn-work-in-aws-sagemaker-script-mode",
        "Question_topic":[
            "Machine Learning & AI",
            "AWS Well-Architected Framework"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Security",
            "Amazon SageMaker Model Building"
        ],
        "Question_upvote_count":0,
        "Question_view_count":63,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"i am trying to understand how input_fn, predict_fn and outout_fn work? I am able to understand what they are, but I am not able to understand how they are called (invoked), can anyone help me understand the same",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-17T13:11:57.759Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi,\n\nTake a look at this implementation using scikit_learn\n\nThey are all implemented as part of the HandlerService object.\n\ninput_fn for preprocessing when data comes in\npredict_fn for making a prediction\noutput_fn for postprocessing\n\nGenerally, input_fn and output_fn have default implementations provided by SageMaker, but predict_fn does not.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker training instance",
        "Question_creation_time":1660674061917,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4_NA-4cTS8aGkpKawI0qVA\/sagemaker-training-instance",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Model Training"
        ],
        "Question_upvote_count":0,
        "Question_view_count":61,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a doubt with choosing instance for training job in sagemaker. Is ml.m5.2xlarge with count as 2 and ml.m5.4xlarge are same ?\n\nI would like to know if there is any best practice guide to choose the instance for training in sagemaker.\n\nThank you \ud83d\ude42",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-16T19:10:15.637Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nThe question is quite broad since there will be multiple factors to consider.\n\nHere is the pricing range provided. You can calculate the same and find out if using 2ml.m5.2xlarge with count as 2 OR ml.m5.4xlarge would be better for your use-case.\n\nPricing- https:\/\/aws.amazon.com\/sagemaker\/pricing\/\n\nYou can refer to the training best practice #6 and find out what is the specification and which instance would you choose from Compute\/Memory Optimized image or the Standard one.\n\nHere is a 3rd party link provided that would could be helpful, however it is not verified by AWS and is based on the research that I found on the internet based on what to consider while choosing a training instance.\n\nLink- https:\/\/datachef.co\/blog\/how-to-choose-the-best-training-instance-on-sagemaker\/",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-08-19T02:01:30.079Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello, to help on the experimentation, you can create a pipeline and run multiple training jobs simultaneously with different instance configurations https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/main\/sagemaker-pipeline-multi-model. Hope this helps!",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to save a .html file to S3 that is created in a Sagemaker processing container",
        "Question_creation_time":1660653174738,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5abOieUyQZSFvyRwfApRVA\/how-to-save-a-html-file-to-s-3-that-is-created-in-a-sagemaker-processing-container",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon Simple Storage Service",
            "Amazon SageMaker",
            "Containers"
        ],
        "Question_upvote_count":0,
        "Question_view_count":115,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Error message: \"FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/processing\/output\/profile_case.html'\"\n\nBackground: I am working in Sagemaker using python trying to profile a dataframe that is saved in a S3 bucket with pandas profiling. The data is very large so instead of spinning up a large EC2 instance, I am using a SKLearn processor.\n\nEverything runs fine but when the job finishes it does not save the pandas profile (a .html file) in a S3 bucket or back in the instance Sagemaker is running in.\n\nWhen I try to export the .html file that is created from the pandas profile, I keep getting errors saying that the file cannot be found.\n\nDoes anyone know of a way to export the .html file out of the temporary 24xl instance that the SKLearn processor is running in to S3? Below is the exact code I am using:\n\nimport os\nimport sys\nimport subprocess\ndef install(package):\n    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\ninstall('awswrangler')\ninstall('tqdm')\ninstall('pandas')\ninstall('botocore==1.19.4')\ninstall('ruamel.yaml')\ninstall('pandas-profiling==2.13.0')\nimport awswrangler as wr\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom dateutil.relativedelta import relativedelta\nfrom string import Template\nimport gc\nimport boto3\n\nfrom pandas_profiling import ProfileReport\n\nclient = boto3.client('s3')\nsession = boto3.Session(region_name=\"eu-west-2\")\n\n%%writefile casetableprofile.py\n\nimport os\nimport sys\nimport subprocess\ndef install(package):\n    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\ninstall('awswrangler')\ninstall('tqdm')\ninstall('pandas')\ninstall('botocore')\ninstall('ruamel.yaml')\ninstall('pandas-profiling')\nimport awswrangler as wr\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom dateutil.relativedelta import relativedelta\nfrom string import Template\nimport gc\nimport boto3\n\nfrom pandas_profiling import ProfileReport\n\nclient = boto3.client('s3')\nsession = boto3.Session(region_name=\"eu-west-2\")\n\n\n\n\ndef run_profile():\n\n\n\n    query = \"\"\"\n    SELECT  * FROM \"healthcloud-refined\".\"case\"\n    ;\n    \"\"\"\n    tableforprofile = wr.athena.read_sql_query(query,\n                                            database=\"healthcloud-refined\",\n                                            boto3_session=session,\n                                            ctas_approach=False,\n                                            workgroup='DataScientists')\n    print(\"read in the table queried above\")\n\n    print(\"got rid of missing and added a new index\")\n\n    profile_tblforprofile = ProfileReport(tableforprofile, \n                                  title=\"Pandas Profiling Report\", \n                                  minimal=True)\n\n    print(\"Generated carerequest profile\")\n                                      \n    return profile_tblforprofile\n\n\nif __name__ == '__main__':\n\n    profile_tblforprofile = run_profile()\n    \n    print(\"Generated outputs\")\n\n    output_path_tblforprofile = ('profile_case.html')\n    print(output_path_tblforprofile)\n    \n    profile_tblforprofile.to_file(output_path_tblforprofile)\n\n    \n    #Below is the only part where I am getting errors\nimport boto3\nimport os   \ns3 = boto3.resource('s3')\ns3.meta.client.upload_file('\/opt\/ml\/processing\/output\/profile_case.html', 'intl-euro-uk-datascientist-prod','Mark\/healthclouddataprofiles\/{}'.format(output_path_tblforprofile))  \n\nimport sagemaker\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsession = boto3.Session(region_name=\"eu-west-2\")\n\nbucket = 'intl-euro-uk-datascientist-prod'\n\nprefix = 'Mark'\n\nsm_session = sagemaker.Session(boto_session=session, default_bucket=bucket)\nsm_session.upload_data(path='.\/casetableprofile.py',\n                                bucket=bucket,\n                                key_prefix=f'{prefix}\/source')\n\nimport boto3\n#import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nregion = boto3.session.Session().region_name\n\n\nS3_ROOT_PATH = \"s3:\/\/{}\/{}\".format(bucket, prefix)\n\nrole = get_execution_role()\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     sagemaker_session=sm_session,\n                                     instance_type='ml.m5.24xlarge',\n                                     instance_count=1)\n\nsklearn_processor.run(code='s3:\/\/{}\/{}\/source\/casetableprofile.py'.format(bucket, prefix),\n                      inputs=[],\n                      outputs=[ProcessingOutput(output_name='output',\n                                                source='\/opt\/ml\/processing\/output',\n                                                destination='s3:\/\/intl-euro-uk-datascientist-prod\/Mark\/')])\n\n\nThank you in advance!!!",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-17T02:50:02.950Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi,\n\nFirstly, you should not (usually) need to directly interact with S3 from your processing script: The fact that you've configured your ProcessingOutput means that any files your script saves in \/opt\/ml\/processing\/output should automatically get uploaded to your s3:\/\/... destination URL. Of course there might be particular special cases where you want to directly access S3 from your script, but in general the processing job inputs and outputs should do it for you, to keep your code nice and simple.\n\nI'm no Pandas Profiler expert, but I think the error might be coming from here:\n\n    output_path_tblforprofile = ('profile_case.html')\n    print(output_path_tblforprofile)\n    \n    profile_tblforprofile.to_file(output_path_tblforprofile)\n\nDoesn't this just save the report to profile_case.html in your current working directory? That's not the \/opt\/ml\/processing\/output directory: It's usually the folder where the script is downloaded to the container I believe. The FileNotFound error is telling you that the HTML file is not getting created in the folder you expect, I think.\n\nSo I would suggest to make your output path explicit e.g. \/opt\/ml\/processing\/output\/profile_case.html, and also remove the boto3\/s3 section at the end - hope that helps!",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Debugger: cannot load training information of estimator",
        "Question_creation_time":1660301495882,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUl_ylpIuQUWy0N-CDqP_ag\/sage-maker-debugger-cannot-load-training-information-of-estimator",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Model Training"
        ],
        "Question_upvote_count":0,
        "Question_view_count":72,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am using a SageMaker notebook for training a ML model. When I created and trained the estimator successfully with the following script, I could load the debugging information (s3_output_path) as expected:\n\nfrom sagemaker.debugger import Rule, DebuggerHookConfig, CollectionConfig, rule_configs\nrules = [\n    Rule.sagemaker(rule_configs.loss_not_decreasing()),\n    Rule.sagemaker(rule_configs.vanishing_gradient()),\n    Rule.sagemaker(rule_configs.overfit()),\n    Rule.sagemaker(rule_configs.overtraining()),\n    Rule.sagemaker(rule_configs.poor_weight_initialization())]\n\ncollection_configs=[CollectionConfig(name=\"CrossEntropyLoss_output_0\", parameters={\n    \"include_regex\": \"CrossEntropyLoss_output_0\", \"train.save_interval\": \"100\",\"eval.save_interval\": \"10\"})]\n\ndebugger_config = DebuggerHookConfig(\n    collection_configs=collection_configs)\n\nestimator = PyTorch(\nrole=sagemaker.get_execution_role(),\ninstance_count=1,\ninstance_type=\"ml.m5.xlarge\",\n#instance_type=\"ml.g4dn.2xlarge\",\nentry_point=\"train.py\",\nframework_version=\"1.8\",\npy_version=\"py36\",\nhyperparameters=hyperparameters,\ndebugger_hook_config=debugger_config,\nrules=rules,\n)\n\nestimator.fit({\"training\": inputs})\n\ns3_output_path = estimator.latest_job_debugger_artifacts_path()\n\n\nAfter the kernel died, I attached the estimator and tried to access the debugging information of the training:\n\nestimator = sagemaker.estimator.Estimator.attach('pytorch-training-2022-06-07-11-07-09-804')\n\ns3_output_path = estimator.latest_job_debugger_artifacts_path()\nrules_path = estimator.debugger_rules\n\n\nThe return values of these 2 functions were None. Could this be a problem with the attach-function? And how can I access training information of the debugger after the kernel was shut down?",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-30T02:52:34.475Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nIt will be difficult to analyze why the two values of the functions were none. We will require more information on the same. I will request you to open a support case with AWS PS and one of our Sagemaker engineer should be able to assist you.\n\nThank you.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"[problem at MMS predict] At MMS(sagemaker), error code(500), type(InternalServerException)",
        "Question_creation_time":1660209790025,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBCxtcfyrTymZ7isHG3X5Qg\/problem-at-mms-predict-at-mms-sagemaker-error-code-500-type-internal-server-exception",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":80,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"I make pytorch model with sagemaker, MMS. This is my mms code.\n\n%%time\ninstance_type = 'c5.large'\n# accelerator_type = 'eia2.medium'\npredictor = mme.deploy(\n    initial_instance_count=1,\n    instance_type=f\"ml.{instance_type}\"\n)\n\nmme.add_model(model_data_source=model_path, model_data_path=\"model.tar.gz\")\nlist(mme.list_models())\n#> [ 'model.tar.gz']\n\nI try to predict with this code.\n\nstart_time = time.time()\npredicted_value = predictor.predict(requests, target_model=\"LV1\")\nduration = time.time() - start_time\nprint(\"${:,.2f}, took {:,d} ms\\n\".format(predicted_value[0], int(duration * 1000)))\n\nAnd, return error message.\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"{\n  \"code\": 500,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Failed to start workers\"\n}\n\n\nMMS with pytorch is 'little' difficult. X)\n\nhelp me, please.",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-12T14:02:03.533Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi , I think your target model on the prediction needs to have the name of the model you have deployed - for example , when you are adding the model with mme.add_model(model_data_source=model_path, model_data_path=\"model.tar.gz\") the model_data_path contains the name of the model . From the sagemaker-examples: (https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb) **model_data_path is the relative path to the S3 prefix we specified above (i.e. model_data_prefix) where our endpoint will source models for inference requests.Since this is a relative path, we can simply pass the name of what we wish to call the model artifact at inference time (i.e. Chicago_IL.tar.gz). In your case \"model.tar.gz\". However, when predicting you call the model ,target_model=\"LV1\"?",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2022-08-16T02:33:28.778Z",
                "Answer_upvote_count":0,
                "Answer_body":"Accoding to your comment, I modify code and excution. I try 2 solution.\n\n#1 predictor.predict\n\npredicted_value = predictor.predict(data=requests, target_model=\"modal.tar.gz\")\n\nreturn\n\nValidationError: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Failed to download model data(bucket: sagemaker-ap-northeast-2-344487737937, key: LouisVuiotton-cpu-2022-08-16-02-02-04-408-c6i-large\/model\/modal.tar.gz). Please ensure that there is an object located at the URL and that the role passed to CreateModel has permissions to download the model.\n\n\n#2 With boto3, invoke_endpoint()\n\nimport boto3\n\nclient = boto3.client('sagemaker-runtime')\nendpoint_name = predictor.endpoint_name\nresponse = client.invoke_endpoint(\n    EndpointName=endpoint_name,\n    Body=requests,\n    ContentType='application\/x-image',\n#     Accept='string',\n#     CustomAttributes='string',\n    TargetModel='model.tar.gz',\n#     TargetVariant='string',\n#     TargetContainerHostname='string',\n#     InferenceId='string'\n)\n\nreturn\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"{\n  \"code\": 500,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Failed to start workers\"\n}\n\". See https:\/\/ap-northeast-2.console.aws.amazon.com\/cloudwatch\/home?region=ap-northeast-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/LV-multi-2022-08-16-02-11-15 in account 344487737937 for more information.\n\n\nI assume sol 2, boto3.invoke_endpoint's result [ \"message\": \"Failed to start workers\" ] come from sol 1, [that the role passed to CreateModel has permissions to download the model.].\n\nI already use excution role [''arn:aws:iam::344487737937:role\/service-role\/AmazonSageMaker-ExecutionRole-20220713T151818\"]. How to I get additional role (that the role passed to CreateModel has permissions to download the model.)?",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker endpoint running but constantly restarting",
        "Question_creation_time":1660140840225,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQi42sIDTTSW5KN3P-DT4LQ\/sagemaker-endpoint-running-but-constantly-restarting",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon CloudWatch Logs",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0,
        "Question_view_count":81,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I have deployed a model to a Sagemaker endpoint using BentoML\/BentoCTL. This is a tool for building APIs and containerizing models. To test, I use curl with a JSON payload to make a request. When I run the created docker container on my local machine I can successfully invoke it and get responses back. So I don't think the problem is in the docker image.\n\nWhen I deploy to sagemaker, I receive the message {\"message\":\"Service Unavailable\"} as a response to my curl request. I can see the endpoint running in the Sagemaker\/Endpoints dashboard. Viewing the cloudwatch logs, it appears that the the endpoint is constantly restarting. There are messages that are printed at startup (e.g. Tensorflow loading messages) that are written to the log over and over.\n\nI thought that this might be due to using an instance type with low memory (t2.medium) so I switched to m5.4xlarge as a test, but the result is the same.\n\nWhat can I do? How can I determine what's causing the endless restarts?",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-12T17:17:23.953Z",
                "Answer_upvote_count":0,
                "Answer_body":"When you mean restart? Does it mean \"Updating\" the endpoint? Do you have an autoscaling policy attached to the endpoint? Do you see any errors in the Cloudwatch logs?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-09-09T05:06:38.531Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello - can you check the Tensorflow version and use the latest supported version 2.2? Thank you!\n\nhttps:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images\/ https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/deploying_tensorflow_serving.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Data Capture does not write files",
        "Question_creation_time":1660135320930,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKWPP4eXTTZe5qIUDJAXnsQ\/sagemaker-data-capture-does-not-write-files",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Lambda",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":52,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I want to enable data capture for a specific endpoint (so far, only via the console). The endpoint works fine and also logs & returns the desired results. However, no files are written to the specified S3 location.\n\nEndpoint Configuration\n\nThe endpoint is based on a training job with a scikit learn classifier. It has only one variant which is a ml.m4.xlarge instance type. Data Capture is enabled with a sampling percentage of 100%. As data capture storage locations I tried s3:\/\/<bucket-name> as well as s3:\/\/<bucket-name>\/<some-other-path>. With the \"Capture content type\" I tried leaving everything blank, setting text\/csv in \"CSV\/Text\" and application\/json in \"JSON\".\n\nEndpoint Invokation\n\nThe endpoint is invoked in a Lambda function with a client. Here's the call:\n\nsagemaker_body_source = {\n            \"segments\": segments,\n            \"language\": language\n        }\npayload = json.dumps(sagemaker_body_source).encode()\nresponse = self.client.invoke_endpoint(EndpointName=endpoint_name,\n                                       Body=payload,\n                                       ContentType='application\/json',\n                                       Accept='application\/json')\nresult = json.loads(response['Body'].read().decode())\nreturn result[\"predictions\"]\n\n\nInternally, the endpoint uses a Flask API with an \/invocation path that returns the result.\n\nLogs\n\nThe endpoint itself works fine and the Flask API is logging input and output:\n\nINFO:api:body: {'segments': [<strings...>], 'language': 'de'}\n\nINFO:api:output: {'predictions': [{'text': 'some text', 'label': 'some_label'}, ....]}",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-16T13:26:08.966Z",
                "Answer_upvote_count":0,
                "Answer_body":"So the issue seemed to be related to the IAM role. The default role (ModelEndpoint-Role) does not have access to write S3 files. It worked via the SDK since it uses another role in the sagemaker studio. I did not receive any error message about this.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"How can make multi model endpoint with SageMaker?",
        "Question_creation_time":1660122368052,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJQBp6A_dSQm1RJ3f8AYMmg\/how-can-make-multi-model-endpoint-with-sage-maker",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_upvote_count":0,
        "Question_view_count":53,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"This is my code.\n\nfrom datetime import datetime\nfrom sagemaker.multidatamodel import MultiDataModel\nmme = MultiDataModel(\n    name=\"LV-multi-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"),\n    model_data_prefix=model_dir, # 2\uc5d0\uc11c \uad6c\ud55c \ubaa8\ub378\uc774 \ubaa8\uc5ec\uc788\ub294 \ud3f4\ub354(\uacbd\ub85c)!!,\n    model=sagemaker_model,  # \ubaa8\ub378 \uac1d\uccb4 1\uac1c \uc6b0\uc120 \ub123\uae30\n    sagemaker_session=sess\n)\n\npredictor = mme.deploy(\n    initial_instance_count=1,\n    instance_type=\"ml.g4dn.xlarge\"\n)\n\nAnd error message. How can I find Ecr Image(within multi-models=true)?\n\nClientError: An error occurred (ValidationException) when calling the CreateModel operation: Your Ecr Image 763104351884.dkr.ecr.ap-northeast-2.amazonaws.com\/pytorch-inference:1.8.1-gpu-py3 does not contain required com.amazonaws.sagemaker.capabilities.multi-models=true Docker label(s).",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-10T13:57:13.476Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi there - thanks for opening this thread. Multi-model endpoints are not supported on GPU instance types, see here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-model-endpoints.html#multi-model-endpoint-instance\n\nIn order to host a multi-model endpoint, choose a CPU instance type instead. The ECR image for CPUs will contain the required com.amazonaws.sagemaker.capabilities.multi-models=true label, see here: https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Installation of jupyter notebook on Deep Learning AMI GPU TensorFlow 2.9.1 (Amazon Linux 2)",
        "Question_creation_time":1660090729750,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJQNSYLw6SgKO_0oZamQKqg\/installation-of-jupyter-notebook-on-deep-learning-ami-gpu-tensor-flow-2-9-1-amazon-linux-2",
        "Question_topic":[
            "Machine Learning & AI",
            "Compute"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon EC2",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":68,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I've been unable to install jupyter notebook seemingly regardless of the virtual environment I have set on Deep Learning AMI GPU TensorFlow 2.9.1 (Amazon Linux 2). Has anyone successfully managed this? The error is the oft occurring: ModuleNotFoundError: No module named 'pysqlite2' But as most of the solutions involved rebuilding python, I don't want to do that given this is a distribution for which jupyter is typically the go to tool for such an AMI.",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-11T12:09:48.056Z",
                "Answer_upvote_count":0,
                "Answer_body":"The DLAMI is a great choice for learning or teaching machine learning and deep learning frameworks. I understood that you are getting 'ModuleNotFoundError' error while installing the Jupyter Notebook server. As per the AWS documentation for Deep Learning AMI, certain Python modules may not be set up on the DLAMI used. So if you get an error like \"xyz module not found\", then you have to install the necessary modules to proceed.\n\nI tried the installation using the DLAMI with different OS. The installation of Jupyter Notebook on DLAMI Ubuntu 20.04 was successful as it has required packages to run \u2018Jupyter notebook\u2019. However, DLAMI Amazon Linux required additional module installation as it was not set up on it.\n\nSo here you need to activate the required Python modules to proceed with the installations.\n\nReference : https:\/\/docs.aws.amazon.com\/dlami\/latest\/devguide\/tutorial-jupyter.html",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-08-11T18:43:34.388Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks for the response. But as a result of \"xyz module not found\" occurring, I followed up with: pip install xyz. But no sqlite option was available, and in googling around this the only offered solutions were rather involved, and included rebuilding python. Shame that this distribution doesn't out of the box support jupyter, however, as there doesn't seem to be a simple way to get it going. I opted for a different AMI.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"not authorized to perform: sagemaker:CreateModel on resource",
        "Question_creation_time":1660056986989,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1sGemgvLQQS-w46eoBoo6w\/not-authorized-to-perform-sagemaker-create-model-on-resource",
        "Question_topic":[
            "Machine Learning & AI",
            "DevOps",
            "Security, Identity, & Compliance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "DevOps",
            "IAM Policies"
        ],
        "Question_upvote_count":0,
        "Question_view_count":62,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have been given AmazonSagemakerFullAccess by my companie's AWS admin. No one at our company can figure out why I can't get this line to run to launch the model.\n\n***** CODE PRODUCING ERROR *****\n\nlang_id = sagemaker.Model( image_uri=container, model_data=model_location, role=role, sagemaker_session=sess ) lang_id.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")\n\n***** ERROR MESSAGE *****\n\nClientError Traceback (most recent call last) <ipython-input-5-4c80ec284a4b> in <module> 2 image_uri=container, model_data=model_location, role=role, sagemaker_session=sess 3 ) ----> 4 lang_id.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\") 5 6 from sagemaker.deserializers import JSONDeserializer\n\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/sagemaker\/model.py in deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, **kwargs) 1132 1133 self._create_sagemaker_model( -> 1134 instance_type, accelerator_type, tags, serverless_inference_config 1135 ) 1136\n\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/sagemaker\/model.py in _create_sagemaker_model(self, instance_type, accelerator_type, tags, serverless_inference_config) 671 tags=tags, 672 ) --> 673 self.sagemaker_session.create_model(**create_model_args) 674 675 def _ensure_base_name_if_needed(self, image_uri, script_uri, model_uri):\n\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in create_model(self, name, role, container_defs, vpc_config, enable_network_isolation, primary_container, tags) 2715 raise 2716 -> 2717 self._intercept_create_request(create_model_request, submit, self.create_model.name) 2718 return name 2719\n\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _intercept_create_request(self, request, create, func_name) 4294 func_name (str): the name of the function needed intercepting 4295 \"\"\" -> 4296 return create(request) 4297 4298\n\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in submit(request) 2703 LOGGER.debug(\"CreateModel request: %s\", json.dumps(request, indent=4)) 2704 try: -> 2705 self.sagemaker_client.create_model(**request) 2706 except ClientError as e: 2707 error_code = e.response[\"Error\"][\"Code\"]\n\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs) 506 ) 507 # The \"self\" in this scope is referring to the BaseClient. --> 508 return self._make_api_call(operation_name, kwargs) 509 510 _api_call.name = str(py_operation_name)\n\n~\/anaconda3\/envs\/tensorflow2_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params) 909 error_code = parsed_response.get(\"Error\", {}).get(\"Code\") 910 error_class = self.exceptions.from_code(error_code) --> 911 raise error_class(parsed_response, operation_name) 912 else: 913 return parsed_response\n\nClientError: An error occurred (AccessDeniedException) when calling the CreateModel operation: User: arn:aws:sts::XXXXXXXXXX:assumed-role\/sagemakeraccesstoservices\/SageMaker is not authorized to perform: sagemaker:CreateModel on resource: arn:aws:sagemaker:us-east-2:XXXXXXXXXX:model\/blazingtext-2022-08-09-13-58-21-739 because no identity-based policy allows the sagemaker:CreateModel action",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-10T08:36:27.079Z",
                "Answer_upvote_count":0,
                "Answer_body":"Based on your description I understand that you are trying to create a Model using the Amazon SageMaker Python SDK with an assumed IAM Role with the AmazonSagemakerFullAccess policy, which should allow sagemaker:CreateModel. It's difficult to identify what the underlying issue is since multiple IAM mechanisms (like SCPs or explicit Denies) can prevent the sagemaker:CreateModel permission.\n\nTo work towards a solution I would recommend:\n\nSimulate the sagemaker:CreateModel action using the IAM Policy Simulator with your User\/Role. This may identify the root cause for your issue and how it can be fixed.\nOpen an AWS Support ticket describing the issue. AWS Support engineers are highly trained, experienced and well equipped to provide you with timely assistance.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Does a completed training job incurr charges?",
        "Question_creation_time":1659691485459,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvqrWFrV7SEG9L-GcLmU0ag\/does-a-completed-training-job-incurr-charges",
        "Question_topic":[
            "Machine Learning & AI",
            "Cloud Financial Management"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Billing"
        ],
        "Question_upvote_count":0,
        "Question_view_count":46,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello.\n\nAs you can see in the screenshot below, there are couple of completed jobs in sagemaker. (Similar in the processing jobs and training jobs menu.)\n\nIf I select one and click the actions button, the Stop menu is inactivated, as you can see.\n\nDoes this kind of completed job incurr charges?\n\nIf so, what do I have to do to Delete or Stop this job?\n\nThank you.",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-05T09:57:31.251Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi there. No, completed jobs don't incur charges. They only incur charges for the time they were running.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How can I terminate Amazon SageMaker RunInstance?",
        "Question_creation_time":1659686032815,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5_3j9z8tSdit5HkWWenrOw\/how-can-i-terminate-amazon-sage-maker-run-instance",
        "Question_topic":[
            "Machine Learning & AI",
            "Cloud Financial Management"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Billing"
        ],
        "Question_upvote_count":0,
        "Question_view_count":179,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello.\n\nI had an unexpected billing, and that was because the SageMaker RunInstance was still running. (Especailly DataWrangler ; see the screenshot below.)\n\nI didn't know how to terminate, so I contacted to the AWS support center.\n\nI followed all the instructions they gave, which means that I deleted endpoints \/ models \/ notebook instances \/ s3buckets \/ cloudwatch log groups.\n\nBut after 24 hours of monitoring, AWS support center said that the SageMaker RunInstance is still running.\n\nThey gave me the same instructions and one additional instruction : stop the training jobs (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tasks-stop-training-job.html)\n\nFirstly I checked endpoints \/ models \/ notebook instances \/ s3buckets \/ cloudwatch log groups again, and I found nothing in those tabs.\n\nSecondly I tried to stop the jobs, but I have deleted the domain since I tried to delete everything - thought it would be better (Maybe that was a fault) - So I created the domain again with quick start option (to get access to the studio), and got into the studio to terminate jobs. But all of the jobs are already completed. There's nothing with activated 'stop training jobs' button.\n\nSo I followed the instruction, but there was nothing I could do.\n\nI really want to STOP this SageMaker RunInstance, but I don't know why it's still running. What should I do..? Please help me.",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-05T08:22:22.442Z",
                "Answer_upvote_count":0,
                "Answer_body":"Be sure that you're connected to the correct AWS region. Follow the documentationt to stop Data Wrangler and you Studio Notebook. Hope it helps",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Attach more than one lifecycle configuration to a notebook instance",
        "Question_creation_time":1659629781823,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsFSQu_SjRwq8IsBnwFMjEg\/attach-more-than-one-lifecycle-configuration-to-a-notebook-instance",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Account Management"
        ],
        "Question_upvote_count":0,
        "Question_view_count":40,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have implemented the auto-stop-idle and persistent-conda-kernels lifecycle configurations and they both work. However, I cannot seem to find how to attach both to a single notebook instance. How can I do that?\n\nLifecycle configuration scripts found in aws sample scripts.",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-04T19:32:05.522Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, you'll have to merge both into a single script and attach it to the notebook instance.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Trigger and\/or monitor Automated Data Labeling Job",
        "Question_creation_time":1659628069730,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5eZvKOO5R4y3FAXlbpUqiw\/trigger-and-or-monitor-automated-data-labeling-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":61,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am following the documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html#sms-auto-labeling-ec2 and I have created a Ground Truth labeling job for object detection. I also enabled automated data labeling when creating the job. I have roughly 5000 images in my dataset. I have manually labelled ( by creating myself as a worker ) 129 of these images. How many do I need to label before the automated labeling job triggers? How do I know if a job was triggered\/succeeded\/failed etc?\n\nThanks",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-16T13:46:06.220Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nAs per this link to the documentation: \"The minimum number of objects allowed for automated data labeling is 1,250, but we strongly suggest providing a minimum of 5,000 objects\" when using active learning in Ground Truth to automate labeling.\n\nYou should also be able to see the status of your labeling job in the Labeling Jobs section of the console if that is how you created the job in the first place.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Inference for Tensorflow Base64 Input Error through API Gateway",
        "Question_creation_time":1659593262466,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCAh0uL1NRmCcZANpWnZd7A\/sagemaker-inference-for-tensorflow-base-64-input-error-through-api-gateway",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Front-End Web & Mobile",
            "Networking & Content Delivery",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Lambda",
            "Amazon API Gateway",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":91,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"When I am trying to call my Sagemaker TF endpoint using API Gateway -> Lambda Func by passing a Base 64 String (an image) I am getting an unsupported string error. I also tried with application\/Json but I am still getting the error. Need Suggestion.\n\nIn Notebook Instance this is how my input looks: <CODE> input = { 'instances': [{\"b64\": \"iV\"}] }\n\nIn Lambda function I am doing this: <CODE>\n\ninstance = [{\"b64\": \"b64string\"}] pleasework=json.dumps({\"instances\": instance}) response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME_BASE64,ContentType='string',Accept='string' ,Body=pleasework)\n\nERROR: Inference Error: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from primary with message \"{\"error\": \"Unsupported Media Type: string\"}\".\n\nIncase if I pass application\/json I get this error:\n\nReceived client error (400) from primary with message \"{ \"error\": \"Failed to process element: 0 of 'instances' list. Error: INVALID_ARGUMENT: JSON Value: {\\n \"b64\": \"iV\"\\n} Type: Object is not of expected type: uint8\"}\"",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-05T18:18:56.773Z",
                "Answer_upvote_count":0,
                "Answer_body":"I would suggest testing invoking your model locally first and confirming what the input your model is expecting using the saved_model CLI. Kindly see this link: https:\/\/www.tensorflow.org\/guide\/saved_model#the_savedmodel_format_on_disk\n\nThen when invoking the model confirm that instance is in the correct input format shape your model expects.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Training Job. Python modules installation Error",
        "Question_creation_time":1659396236435,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmwxhCTLzTp2f9ePN1V2oLg\/sagemaker-training-job-python-modules-installation-error",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Model Training"
        ],
        "Question_upvote_count":0,
        "Question_view_count":128,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a problem with Python module installation that requires pre-installation of another module. Both modules were added to the requirement.txt file. However, the error occurs when installing main module:\n\n2022-07-29 01:18:26.460132: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n\"2022-07-29 01:18:26.470589: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\"\n2022-07-29 01:18:26.765280: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n\"2022-07-29 01:18:31,908 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\"\n\"2022-07-29 01:18:31,917 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\"\n\"2022-07-29 01:18:33,117 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\"\n\/usr\/local\/bin\/python3.9 -m pip install -r requirements.txt\nCollecting Cython==0.29.31\nDownloading Cython-0.29.31-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (2.0 MB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0\/2.0 MB 33.1 MB\/s eta 0:00:00\nRequirement already satisfied: wheel==0.37.1 in \/usr\/local\/lib\/python3.9\/site-packages (from -r requirements.txt (line 2)) (0.37.1)\nCollecting scikit-image==0.19.2\nDownloading scikit_image-0.19.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.0\/14.0 MB 83.1 MB\/s eta 0:00:00\nCollecting parallelbar==0.1.19\nDownloading parallelbar-0.1.19-py3-none-any.whl (5.6 kB)\nCollecting albumentations==1.0.3\nDownloading albumentations-1.0.3-py3-none-any.whl (98 kB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 98.7\/98.7 kB 6.6 MB\/s eta 0:00:00\nCollecting tensorflow_addons==0.16.1\nDownloading tensorflow_addons-0.16.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.1\/1.1 MB 54.4 MB\/s eta 0:00:00\nRequirement already satisfied: tensorflow-io==0.24.0 in \/usr\/local\/lib\/python3.9\/site-packages (from -r requirements.txt (line 7)) (0.24.0)\nRequirement already satisfied: tensorboard==2.8.0 in \/usr\/local\/lib\/python3.9\/site-packages (from -r requirements.txt (line 8)) (2.8.0)\nCollecting universal-pathlib==0.0.12\nDownloading universal_pathlib-0.0.12-py3-none-any.whl (19 kB)\nCollecting setuptools==63.2.0\nDownloading setuptools-63.2.0-py3-none-any.whl (1.2 MB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.2\/1.2 MB 58.9 MB\/s eta 0:00:00\nCollecting pynanosvg==0.3.1\nDownloading pynanosvg-0.3.1.tar.gz (346 kB)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 346.0\/346.0 kB 17.5 MB\/s eta 0:00:00\nPreparing metadata (setup.py): started\nPreparing metadata (setup.py): finished with status 'error'\n\"error: subprocess-exited-with-error\n  \n  \u00d7 python setup.py egg_info did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [6 lines of output]\n      Traceback (most recent call last):\n        File \"\"<string>\"\", line 2, in <module>\n        File \"\"<pip-setuptools-caller>\"\", line 34, in <module>\n        File \"\"\/tmp\/pip-install-1mt2gkfy\/pynanosvg_d6162ffce95948abb4262061a011908c\/setup.py\"\", line 2, in <module>\n          from Cython.Build import cythonize\n      ModuleNotFoundError: No module named 'Cython'\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\"\nerror: metadata-generation-failed\n\u00d7 Encountered error while generating package metadata.\n\u2570\u2500> See above for output.\n\"note: This is an issue with the package mentioned above, not pip.\"\nhint: See above for details.\n[notice] A new release of pip available: 22.1.2 -> 22.2.1\n\"[notice] To update, run: pip install --upgrade pip\"\n\"2022-07-29 01:18:36,187 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\"\n\"2022-07-29 01:18:36,187 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\"\n\"2022-07-29 01:18:36,188 sagemaker-training-toolkit ERROR    Reporting training FAILURE\"\n\"2022-07-29 01:18:36,188 sagemaker-training-toolkit ERROR    InstallRequirementsError:\"\nExitCode 1\n\"ErrorMessage \"\"      ModuleNotFoundError: No module named 'Cython'\n       [end of output]      note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed  \u00d7 Encountered error while generating package metadata. \u2570\u2500> See above for output. note: This is an issue with the package mentioned above, not pip. hint: See above for details.\"\"\"\n\"Command \"\"\/usr\/local\/bin\/python3.9 -m pip install -r requirements.txt\"\"\"\n\"2022-07-29 01:18:36,188 sagemaker-training-toolkit ERROR    Encountered exit_code 1\"",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-02T02:49:06.887Z",
                "Answer_upvote_count":0,
                "Answer_body":"Since Cython does seem to be downloaded before the error, I suspect the problem is something in other packages' install process requiring it before pip is done installing it. It looks like others have found similar (non-SageMaker-specific) issues with Cython e.g. here and here.\n\nThings I would suggest to try:\n\nExplicitly specify Cython (maybe without a version at first) right at the top of your requirements.txt file if you're not already - just in case this can convince pip to treat it properly.\n\nCustomize the TensorFlow container image you're targeting to pre-install Cython.\n\nIf you're not sure what base container URI you're using, you can fetch it with sagemaker.image_uris.retrieve(...) (doc here).\n\nFrom that, you can create a minimal Dockerfile something like\n\nFROM XYZ.dkr.ecr.ABC.amazonaws.com\/...\nRUN pip install Cython==0.29.31\n\nOnce you build this customized container image, and push it to Amazon ECR in your AWS account & region, you can use it by setting the image_uri parameter in your TensorFlow Estimator. Note that the frameworks typically have separate container images for training vs serving, and GPU vs CPU-only, so you may need to create a pair of containers if wanting to do inference too.\n\nIf you're working inside SageMaker Studio you won't directly be able to docker build, but you can install the sm-docker build solution based on AWS CodeBuild. The \"Prepare custom training and inference containers\" section of this notebook gives an example of similar approach.\n\nIf you'd really like to avoid touching containers and ECR, you could instead remove your requirements.txt and install dependencies within the script via something like subprocess.check_call([\"pip\", \"install\", ...]). It's hacky, but this way you could run a pip install just for Cython first... Then install all the other dependencies in one other command.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Custom Post Annotation Lambda Function for Custom Labeling Job",
        "Question_creation_time":1659025093082,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYzKc8ISiT4eU9cXCsdUqEg\/custom-post-annotation-lambda-function-for-custom-labeling-job",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "AWS Lambda",
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth"
        ],
        "Question_upvote_count":0,
        "Question_view_count":71,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello,\n\nI have implemented a post annotation lambda function for my sagemaker ground truth custom job.\n\nAfter the annotations are finished, the results come after the consolidation of the annotations are saved in a subdirectory called \"iteration_X\" of \"annotations\/consolidated-annotation\/consolidation-response\/\".\n\nHowever, the outcome of the annotations is never successful and from the log of the lambda function used for the post annotation I always receive this type of error:\n\n{\n    \"labeling-job-name\": \"labeling-job-full-dataset-test-giusy-10\",\n    \"event-name\": \"ANNOTATION_CONSOLIDATION_LAMBDA_SCHEMA_MATCHING_FAILED\",\n    \"event-log-message\": \"ERROR: Annotation consolidation Lambda response did not match expected data format for line 1.\"\n}\n\n\nBased on this guide (https:\/\/docs.aws.amazon.com\/id_id\/sagemaker\/latest\/dg\/sms-custom-templates-step3-lambda-requirements.html) I made sure that my lambda function returns:\n\nRESPONSE:\n\n\n[\n  {\n    \"datasetObjectId\": \"1\",\n    \"consolidatedAnnotation\": {\n      \"content\": {\n        \"annotations\": {\n          \"relations\": [\n            {\n              \"subj\": \"CW\",\n              \"predicate\": \"adjust\",\n              \"obj\": \"key\"\n            },\n            {\n              \"subj\": \"key\",\n              \"predicate\": \"with\",\n              \"obj\": \"right_hand\"\n            },\n            {\n              \"subj\": \"key\",\n              \"predicate\": \"on\",\n              \"obj\": \"lock\"\n            }\n          ],\n          \"groundings\": {\n            \"pre_frame\": [\n              {\n                \"object\": \"right_hand\",\n                \"left\": 776.5,\n                \"top\": 219.5,\n                \"width\": 282.52,\n                \"height\": 246.5\n              },\n              {\n                \"object\": \"lock\",\n                \"left\": 716.4,\n                \"top\": 255.6,\n                \"width\": 93.60000000000002,\n                \"height\": 111.6\n              }\n            ],\n            \"pnr_frame\": [\n              {\n                \"object\": \"right_hand\",\n                \"left\": 974.16,\n                \"top\": 275.14,\n                \"width\": 287.21,\n                \"height\": 215.84\n              },\n              {\n                \"object\": \"lock\",\n                \"left\": 914.4,\n                \"top\": 291.6,\n                \"width\": 97.20000000000005,\n                \"height\": 90\n              }\n            ],\n            \"post_frame\": [\n              {\n                \"object\": \"right_hand\",\n                \"left\": 858.58,\n                \"top\": 240.54,\n                \"width\": 316.28,\n                \"height\": 209.37\n              },\n              {\n                \"object\": \"lock\",\n                \"left\": 741.6,\n                \"top\": 237.6,\n                \"width\": 61.19999999999993,\n                \"height\": 169.20000000000002\n              }\n            ]\n          },\n          \"timestamp\": \"0\",\n          \"clip_uid\": \"undefined\"\n        }\n      }\n    }\n  }\n]\n\n\nI can't figure out how to avoid this type of error and make the annotations go through when the job is finished.",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-30T16:31:54.591Z",
                "Answer_upvote_count":0,
                "Answer_body":"It may be possible that ground truth is attempting to read the result from the annotations\/consolidated-annotation\/ directory and thus not actually reading the consolidated response but the file containing them",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-07-30T08:20:10.765Z",
                "Answer_upvote_count":0,
                "Answer_body":"inside the lambda function method I'm reading from the data from: \"annotations\/consolidated-annotation\/consolidation-request\/\" because it contains all the data necessary for the desired response.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"\"FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/input\/data\/training\/train\/data.csv'",
        "Question_creation_time":1659016644599,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuxHzyW8yRNqccGuILrD3vw\/file-not-found-error-errno-2-no-such-file-or-directory-opt-ml-input-data-training-train-data-csv",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":64,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I want to run sagemaker and I adjusted my code according to the following example:\n\nstep 3: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-preprocess-data.html\n\nand\n\nstep 4: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model.html\n\nbut I can't finish the training. Sagemaker returns the following error:\n\nErrorMessage \"FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/input\/data\/training\/train\/data.csv\n\nTo upload data on s3, in my code I set the paths as follows:\n\nbucket = sagemaker_session.default_bucket() prefix = 'sagemaker_forecasting_ml'\n\ntrain_1d.to_csv('train_1d.csv', sep=',', index=False, header=False) boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'data\/train_1d.csv')).upload_file('train_1d.csv')\n\ntraining_1d_s3_path = TrainingInput( \"s3:\/\/{}\/{}\/{}\".format(bucket, prefix, \"data\/train.csv\"), content_type=\"csv\" )\n\nWhy sagemaker can't find a path when data is on it?",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-28T14:20:35.914Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi! Looks like you are uploading a file called \"train_1d.csv\" and then requesting a file with a different name called \"train.csv\". Might this be the error? Hope it helps.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"ImportError: cannot import name 'dataclass_transform' from 'typing_extensions' (\/home\/ec2-user\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/typing_extensions.py)",
        "Question_creation_time":1659014190621,
        "Question_link":"https:\/\/repost.aws\/questions\/QULlX63PqqQ1q-W5kCzwTKow\/import-error-cannot-import-name-dataclass-transform-from-typing-extensions-home-ec-2-user-anaconda-3-envs-tensorflow-2-p-38-lib-python-3-8-site-packages-typing-extensions-py",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":344,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi AWS, I am running the code for dalle mini to convert a text into an image. Here is the code for the same:\n\nimport jax\nimport jax.numpy as jnp\nfrom huggingface_hub import hf_hub_url, cached_download, hf_hub_download\nimport shutil\nfrom dalle_mini import DalleBart, DalleBartProcessor\nfrom vqgan_jax.modeling_flax_vqgan import VQModel\nfrom typing_extensions import dataclass_transform\nfrom transformers import CLIPProcessor, FlaxCLIPModel\nfrom IPython.display import display\n\n# TF_CPP_MIN_LOG_LEVEL=0\nprint(jax.local_device_count())\nprint(jax.devices())\n\ndalle_mini_files_list = ['config.json', 'tokenizer.json', 'tokenizer_config.json', 'merges.txt', 'vocab.json', 'special_tokens_map.json', 'enwiki-words-frequency.txt', 'flax_model.msgpack']\n\nvqgan_files_list = ['config.json',  'flax_model.msgpack']\n\nfor each_file in dalle_mini_files_list:\n   downloaded_file = hf_hub_download(\"dalle-mini\/dalle-mini\", filename=each_file)\n   target_path = '\/home\/ec2-user\/SageMaker\/huggingface-sagemaker\/content\/dalle-mini\/' + each_file\n   shutil.copy(downloaded_file, target_path)\n\nfor each_file in vqgan_files_list:\n   downloaded_file = hf_hub_download(\"dalle-mini\/vqgan_imagenet_f16_16384\", filename=each_file)\n   target_path = '\/home\/ec2-user\/SageMaker\/huggingface-sagemaker\/content\/dalle-mini\/vqgan\/' + each_file\n   shutil.copy(downloaded_file, target_path)\n\nDALLE_MODEL_LOCATION = '\/home\/ec2-user\/huggingface-sagemaker\/dalle_mini\/content\/dalle-mini'\nDALLE_COMMIT_ID = None\nmodel, params = DalleBart.from_pretrained(    \n      DALLE_MODEL_LOCATION, revision=DALLE_COMMIT_ID, dtype=jnp.float32, _do_init=False,\n)\n\nVQGAN_LOCAL_REPO = '\/home\/ec2-user\/SageMaker\/dalle_mini\/content\/dalle-mini\/vqgan'\nVQGAN_LCOAL_COMMIT_ID = None\nvqgan, vqgan_params = VQModel.from_pretrained(\n     VQGAN_LOCAL_REPO, revision=VQGAN_LCOAL_COMMIT_ID, _do_init=False\n)\n\n\nprint(model.config)\nprint(vqgan.config)\n\nDALLE_MODEL_LOCATION = '\/home\/ec2-user\/SageMaker\/dalle_mini\/content\/dalle-mini'\nDALLE_COMMIT_ID = None\nprocessor = DalleBartProcessor.from_pretrained(\n     DALLE_MODEL_LOCATION, \n     revision=DALLE_COMMIT_ID)\n\nprint(processor)\n\n# # Works for all available devices to replicate the module\nfrom flax.jax_utils import replicate\nimport random\n\nparams = replicate(params)\nvqgan_params = replicate(vqgan_params)\n\n@partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(3, 4, 5, 6))\ndef p_generate(\n    tokenized_prompt, key, params, top_k, top_p, temperature, condition_scale\n):\n  return model.generate(\n      **tokenized_prompt,\n      prng_key=key,\n      params=params,\n      top_k=top_k,\n      top_p=top_p,\n      temperature=temperature,\n      condition_scale=condition_scale,\n  )\n\n#decode the images\n@partial(jax.pmap, axis_name=\"batch\")\ndef p_decode(indices, params):\n    return vqgan.decode_code(indices, params=params)\n\n\n# entering the prompts\nprompts = [\n    \"sunset over a lake in the mountains\",\n    \"the Eiffel tower landing on the moon\",\n]\n\ntokenized_prompts = processor(prompts)\ntokenized_prompt = replicate(tokenized_prompts)\n\n\n\n# create a random key\nseed = random.randint(0, 2**32 - 1)\nkey = jax.random.PRNGKey(seed)\n\n\nn_predictions = 4\n\n# We can customize generation parameters (see https:\/\/huggingface.co\/blog\/how-to-generate)\ngen_top_k = None\ngen_top_p = None\ntemperature = None\ncond_scale = 10.0\n\nprint(f\"Prompts: {prompts}\\n\")\n\nimages = []\nfor i in trange(max(n_predictions \/\/ jax.device_count(), 1)):\n    # get a new key\n    key, subkey = jax.random.split(key)\n    # generate images\n    encoded_images = p_generate(\n        tokenized_prompt,\n        shard_prng_key(subkey),\n        params,\n        gen_top_k,\n        gen_top_p,\n        temperature,\n        cond_scale,\n    )\n    # remove BOS\n    encoded_images = encoded_images.sequences[..., 1:]\n    # decode images\n    decoded_images = p_decode(encoded_images, vqgan_params)\n    decoded_images = decoded_images.clip(0.0, 1.0).reshape((-1, 256, 256, 3))\n    for decoded_img in decoded_images:\n        img = Image.fromarray(np.asarray(decoded_img * 255, dtype=np.uint8))\n        images.append(img)\n        display(img)\n\n\nand the error I am getting is:\n\nImportError Traceback (most recent call last) ~\/SageMaker\/huggingface-sagemaker\/code\/inference.py in <module> 5 #import DalleBart 6 #from dalle_mini import DalleBart, DalleBartProcessor ----> 7 from vqgan_jax.modeling_flax_vqgan import VQModel 8 from typing_extensions import dataclass_transform 9 #from transformers import CLIPProcessor, FlaxCLIPModel\n\n~\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/vqgan_jax\/modeling_flax_vqgan.py in <module> 8 import jax.numpy as jnp 9 import numpy as np ---> 10 import flax.linen as nn 11 from flax.core.frozen_dict import FrozenDict 12\n\n~\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/flax\/init.py in <module> 16 \"\"\"Flax API.\"\"\" 17 ---> 18 from . import core as core 19 from . import linen as linen 20 from . import optim as optim\n\n~\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/flax\/core\/init.py in <module> 26 ) 27 ---> 28 from .scope import ( 29 Scope as Scope, 30 Array as Array,\n\n~\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/flax\/core\/scope.py in <module> 26 from flax import config 27 from flax import errors ---> 28 from flax import struct 29 from flax import traceback_util 30 from .frozen_dict import freeze\n\n~\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/flax\/struct.py in <module> 23 24 import jax ---> 25 from typing_extensions import dataclass_transform # pytype: disable=not-supported-yet 26 27\n\nImportError: cannot import name 'dataclass_transform' from 'typing_extensions' (\/home\/ec2-user\/anaconda3\/envs\/tensorflow2_p38\/lib\/python3.8\/site-packages\/typing_extensions.py)\n\nPlease help me ASAP as I need to fix it urgently.",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-29T07:46:18.042Z",
                "Answer_upvote_count":0,
                "Answer_body":"I am getting a different error now:\n\n\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/flax\/core\/lift.py:112: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead. scopes, treedef = jax.tree_flatten(scope_tree) \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/flax\/core\/lift.py:729: FutureWarning: jax.tree_leaves is deprecated, and will be removed in a future release. Use jax.tree_util.tree_leaves instead. lengths = set(jax.tree_leaves(lengths)) \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/flax\/core\/axes_scan.py:134: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead. in_avals, in_tree = jax.tree_flatten(input_avals) \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/flax\/linen\/transforms.py:249: FutureWarning: jax.tree_leaves is deprecated, and will be removed in a future release. Use jax.tree_util.tree_leaves instead. jax.tree_leaves(tree))) \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/flax\/core\/axes_scan.py:146: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead. broadcast_in, constants_out = jax.tree_unflatten(out_tree(), out_flat) Killed\n\nI have imported the library in the inference.py script:\n\nfrom jax.tree_util import (tree_flatten, tree_leaves, tree_unflatten)\n\nBut still I am experiencing this warning and the running of code is interrupts and the session gets killed.\n\nAny reason. Please let me know.\n\nThanks",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Register MultiDataModel in Model Registry",
        "Question_creation_time":1658995322336,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJS8TpSeOStayVIvQFvpN0A\/register-multi-data-model-in-model-registry",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0,
        "Question_view_count":47,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am working with Pipelines and provided Sagemaker Project template for building, training and deploying models in Sagemaker Studio and I need to create Multi Model Endpoint. I tried to register MultiDataModel and deploy it as is, but I can not register MultiDataModel and the error is:\n\nAttributeError: 'MultiDataModel' object has no attribute 'vpc_config'.\n\n\nI also tried to create pipeline model that contains SKlearn model for processing input data and MultiModelData, but I can not register the pipeline model and the error I got is:\n\nbotocore.exceptions.ClientError: An error occurred (ValidationException) when calling the UpdatePipeline operation: Unable to parse pipeline definition. Unknown Argument member 'Mode'.\n\n\nQuestion: Is there any possibility to work with MultiDataModel and multi model endpoint in Sagemake Projects Pipelines, because I will need that endpoint for production, or I will need to work with Notebook Instances If I want to use multi data endpoint?",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-30T09:42:15.671Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yes it would seem using notebook instances would be the easiest way to implement and register multimodeldata and multimodal end points",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Studio encountered an error when creating your project(github and codepipeline template)",
        "Question_creation_time":1658949790257,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOCKdskABQumCC7OnzBZR4g\/sagemaker-studio-encountered-an-error-when-creating-your-project-github-and-codepipeline-template",
        "Question_topic":[
            "Developer Tools",
            "Machine Learning & AI",
            "Management & Governance",
            "DevOps"
        ],
        "Question_tag":[
            "AWS CodePipeline",
            "Amazon SageMaker",
            "AWS CloudFormation",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_upvote_count":0,
        "Question_view_count":68,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I trying tutorial on \"MLOps template for model building, training, and deployment with third-party Git repositories using CodePipeline\". But I am getting error as shown in image",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-28T07:44:00.283Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hello. It seems like you are having permission problems according to the snapshot you provided. If you head to the Cloudformation service, you will probably get a better understanding of where the tamplate is failing. Make sure to have followed the prerequisites and check out this section.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Retrieve Linear Learner Weights",
        "Question_creation_time":1658859121171,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXh8p1bCgT8a2gLfTDssY7w\/retrieve-linear-learner-weights",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon S3 Glacier",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Pipelines",
            "Amazon SageMaker Model Building"
        ],
        "Question_upvote_count":0,
        "Question_view_count":49,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Unable to find the correct attribute to list Linear Learner weights. I fitted the estimator with the training data and linear Learner creates a weight vector w, which is fundamental in this algorithm. How can I print the resulting weight vector after training\/fitting?",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-27T16:22:33.082Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hey! You will have to get this weight vector from the model artifact the training job returns in S3. I believe this post will help you retrieve said vector! Hope it helps!",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Mandate user to enable encryption while Sagemaker notebook creation?",
        "Question_creation_time":1658816453567,
        "Question_link":"https:\/\/repost.aws\/questions\/QUA0H60oMZTUy1JgSwFVXV4g\/mandate-user-to-enable-encryption-while-sagemaker-notebook-creation",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Encryption"
        ],
        "Question_upvote_count":0,
        "Question_view_count":49,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"1.We would like to mandate user to enable KMS encryption while creating Sagemaker notebooks, I would like to know any methods via policy or any other way?",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-27T02:04:13.390Z",
                "Answer_upvote_count":0,
                "Answer_body":"If the user is creating the notebook from the console the encryption is an optional field and we will not be able to enforce it. One alternate way to do this is to use the Boto3 API to create the notebook instance programatically. This way we can check for the encryption or automatically add encryption fields.\n\nhttps:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_notebook_instance",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-07-27T00:21:44.051Z",
                "Answer_upvote_count":0,
                "Answer_body":"Sorry I haven't been able to test this yet, but thought it was worth adding:\n\nAccording to the IAM reference page for Amazon SageMaker, the sagemaker:CreateNotebookInstance action supports specifying the sagemaker:VolumeKmsKey condition key.\n\nTherefore I believe you should be able to prevent users creating notebook instances by modifying their IAM permissions to only allow CreateNotebookInstance where VolumeKmsKey is provided. If you're new to the concept of condition keys in IAM, you can find more info here.\n\nI would mention that even if this works as expected, the error message a user sees when they're prevented from creating the instance will be a pretty generic \"Access denied\" - so you'll need to educate them on the requirement for a good user experience.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Line magic error in SageMaker Studio",
        "Question_creation_time":1658670880505,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZPg9WRBtSwGvYhI9-3tscA\/line-magic-error-in-sage-maker-studio",
        "Question_topic":[
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_upvote_count":0,
        "Question_view_count":60,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi AWS, I am trying to create virtual environment in SageMaker Studio but while doing so I am experiencing a line magic function error. Also I am not able to import the libraries. I am attaching the error screenshot for the same.\n\nThanks",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-25T08:28:37.568Z",
                "Answer_upvote_count":0,
                "Answer_body":"As far as I can tell, %virtualenv and %import are not standard line magics in IPython - and from a quick search around, I couldn't see what package implements them?\n\nBut I think you might want to consider more fundamental changes to your workflow and will try to make a case for it...\n\nIn SageMaker Studio, as the other answer noted already, kernel environments are containerized - rather than conda-based. This is nicely consistent with how SageMaker job environments (training, deployment, processing, etc) are also container-based.\n\nAs a result, my usual environment management suggestions would be:\n\nFor quickly and interactively experimenting with different library installations in the notebook, simply use !pip shell commands to install\/edit packages directly in the running container. (e.g. !pip install abc==x.y.z, !pip show abc, and so on). You could even save these dependencies in a requirements.txt and !pip install from that file (which would be nice because SageMaker script-mode jobs can accept a requirements.txt).\nFor formalizing an environment to be standard, repeatable, and shareable between users without them having to re-run the install commands: set up a custom kernel container image.\n\nAs outlined here, all your open notebooks with the same kernel and instance type selected will share a running container. When you delete\/restart the \"app\" (container), these changes will be lost. Keeping running\/customized kernel environments as disposable as possible, and trying to use SageMaker jobs (training, processing, etc) early in the workflow instead of sticking everything in notebook, helps to prevent reproducibility problems.\n\nBecause they're containerized, I find I don't really need to worry about environment management\/virtualization technologies: In notebook experimentation, my different kernels are fully separated anyway and I can just restart the container (\"app\") to reset. In SageMaker jobs there's only one task being run in the container, so it doesn't need to play nice with other workloads. It's different, and actually quite nice, compared to working on my local laptop where I have to carefully manage separate project environments and fix things if they break!\n\nEdit to add: The following works for me... Perhaps your problem was that the code folder didn't exist yet? os.makedirs() or !mkdir or UI actions can fix that:\n\n%%writefile src\/main.py\n\nimport os",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-07-28T04:44:00.831Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nWhen running a notebook on SageMaker Studio you can select the default environment you want by clicking on the top right corner on the kernel you currently have enabled (Default should be Data Science) and select the one you want from the dropdown.\n\nIn the case you want to install more\/custom packages, then you can install those straight from your notebook using one of the valid magic cell commands. These are %conda install and %pip install (see doc: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-notebooks-add-external.html)\n\nAlternatively you can open a terminal and use your tool of choice to manage the python packages installed.\n\nIn terms of importing packages, the correct syntax for that is without the %. So please instead of %import <package_name> try import <package_name>.\n\nLet us know if this solved your issue,\n\nhave a nice day",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Error for Training job catboost-classification-model , ErrorMessage \"TypeError: Cannot convert 'xxx'' to float",
        "Question_creation_time":1658510281955,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVfbc_AsXRzaxAl69MMFlsQ\/error-for-training-job-catboost-classification-model-error-message-type-error-cannot-convert-xxx-to-float",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_upvote_count":0,
        "Question_view_count":200,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"When I performed the following AWS tutorial, I got an error when training the model. https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/lightgbm_catboost_tabular\/Amazon_Tabular_Classification_LightGBM_CatBoost.ipynb\n\nThe error that occurred is\n\nUnexpectedStatusException: Error for Training job jumpstart-catboost-classification-model-2022-07-22-07-33-18-038: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1 ErrorMessage \"TypeError: Cannot convert 'b'BROOKLYN'' to float\n\n\nThese are all the files that I have upload in S3 bucket : Amazon S3 --> Buckets---> R-sandbox-sagemaker--->ml\/---> train\/ and in the train folder 'data.csv' and 'categorical_index.json' are uploaded based on the mentioned tutorial. data point \"BROOKLYN\" is in the categorical column, its index is already included in the JSON file to tell Catboost that it is categorical data. Data has 55 categorical data columns; only two of them are integers , all other string\n\nCould you give me some advice on how to solve it?\n\nAlso here all the code and full traceback of the issue:\n\n!pip install sagemaker ipywidgets --upgrade \u2013quiet\nimport sagemaker, boto3, json\nfrom sagemaker import get_execution_role\n\naws_role = get_execution_role()\naws_region = boto3.Session().region_name\nsess = sagemaker.Session()\n\n##2.1 Retrieve Training Artifacts-\n#retrieve the training docker container, the training algorithm source, and the tabular algorithm. Note that model_version=\"*\" fetches the latest model.\n# Currently, not all the object detection models in jumpstart support finetuning. Thus, we manually select a model\n# which supports finetuning.\n\nfrom sagemaker import image_uris, model_uris, script_uris\ntrain_model_id, train_model_version, train_scope = \"catboost-classification-model\", \"*\", \"training\"\ntraining_instance_type = \"ml.m5.xlarge\"\n\n# Retrieve the docker image\ntrain_image_uri = image_uris.retrieve(\n    region=None,\n    framework=None,\n    model_id=train_model_id,\n    model_version=train_model_version,\n    image_scope=train_scope,\n    instance_type=training_instance_type,\n)\n# Retrieve the training script\ntrain_source_uri = script_uris.retrieve(\n    model_id=train_model_id, model_version=train_model_version, script_scope=train_scope\n)\n# Retrieve the pre-trained model tarball to further fine-tune\ntrain_model_uri = model_uris.retrieve(\n    model_id=train_model_id, model_version=train_model_version, model_scope=train_scope\n)\n## 2.2 Set Training Parameters\n# Sample training data is available in this bucket\ntraining_data_bucket = \"R-sandbox-sagemaker\"\ntraining_data_prefix = \"ml\"\n\ntraining_dataset_s3_path = f\"s3:\/\/{training_data_bucket}\/{training_data_prefix}\"\n\noutput_bucket = sess.default_bucket()\noutput_prefix = \"jumpstart-example-tabular-training\"\n\ns3_output_location = f\"s3:\/\/{output_bucket}\/{output_prefix}\/output\"\n\nfrom sagemaker import hyperparameters\n# Retrieve the default hyper-parameters for fine-tuning the model\nhyperparameters = hyperparameters.retrieve_default(\n    model_id=train_model_id, model_version=train_model_version\n)\n\n# [Optional] Override default hyperparameters with custom values\nhyperparameters[\n    \"iterations\"\n] = \"500\"  # The same hyperparameter is named as \"iterations\" for CatBoost\nprint(hyperparameters)\n\n## 2.3. Train with Automatic Model Tuning\nfrom sagemaker.tuner import ContinuousParameter, IntegerParameter, HyperparameterTuner\n\nuse_amt = True\nif train_model_id == \"lightgbm-classification-model\":\n    hyperparameter_ranges = {\n        \"learning_rate\": ContinuousParameter(1e-4, 1, scaling_type=\"Logarithmic\"),\n        \"num_boost_round\": IntegerParameter(2, 30),\n        \"early_stopping_rounds\": IntegerParameter(2, 30),\n        \"num_leaves\": IntegerParameter(10, 50),\n        \"feature_fraction\": ContinuousParameter(0, 1),\n        \"bagging_fraction\": ContinuousParameter(0, 1),\n        \"bagging_freq\": IntegerParameter(1, 10),\n        \"max_depth\": IntegerParameter(5, 30),\n        \"min_data_in_leaf\": IntegerParameter(5, 50),\n    }\nif train_model_id == \"catboost-classification-model\":\n    hyperparameter_ranges = {\n        \"learning_rate\": ContinuousParameter(0.00001, 0.1, scaling_type=\"Logarithmic\"),\n        \"iterations\": IntegerParameter(50, 1000),\n        \"early_stopping_rounds\": IntegerParameter(1, 10),\n        \"depth\": IntegerParameter(1, 10),\n        \"l2_leaf_reg\": IntegerParameter(1, 10),\n        \"random_strength\": ContinuousParameter(0.01, 10, scaling_type=\"Logarithmic\"),\n    }\n## 2.4. Start Training\nfrom sagemaker.estimator import Estimator\nfrom sagemaker.utils import name_from_base\ntraining_job_name = name_from_base(f\"jumpstart-{'catboost-classification-model'}-training\")\n\n# Create SageMaker Estimator instance\ntabular_estimator = Estimator(\n    role=aws_role,\n    image_uri=train_image_uri,\n    source_dir=train_source_uri,\n    model_uri=train_model_uri,\n    entry_point=\"transfer_learning.py\",\n    instance_count=1,\n    instance_type=training_instance_type,\n    max_run=360000,\n    #hyperparameters=hyperparameters,\n    output_path=s3_output_location,\n)\n# Launch a SageMaker Training job by passing s3 path of the training data\ntabular_estimator.fit(\n        {\"training\": training_dataset_s3_path}, logs=True, job_name=training_job_name\n    )\n\n2022-07-22 07:33:18 Starting - Starting the training job...\n2022-07-22 07:33:46 Starting - Preparing the instances for trainingProfilerReport-1658475198: InProgress\n2022-07-22 07:35:06 Downloading - Downloading input data...\n2022-07-22 07:35:46 Training - Downloading the training image...\n2022-07-22 07:36:11 Training - Training image download completed. Training in progress..bash: cannot set terminal process group (-1): Inappropriate ioctl for device\nbash: no job control in this shell\n2022-07-22 07:36:14,025 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n2022-07-22 07:36:14,027 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2022-07-22 07:36:14,036 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n2022-07-22 07:36:14,041 sagemaker_pytorch_container.training INFO     Invoking user training script.\n2022-07-22 07:36:15,901 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n\/opt\/conda\/bin\/python3.8 -m pip install -r requirements.txt\nProcessing .\/catboost\/tenacity-8.0.1-py3-none-any.whl\nProcessing .\/catboost\/plotly-5.1.0-py2.py3-none-any.whl\nProcessing .\/catboost\/graphviz-0.17-py3-none-any.whl\nProcessing .\/catboost\/catboost-1.0.1-cp38-none-manylinux1_x86_64.whl\nProcessing .\/sagemaker_jumpstart_script_utilities-1.0.0-py2.py3-none-any.whl\nRequirement already satisfied: six in \/opt\/conda\/lib\/python3.8\/site-packages (from plotly==5.1.0->-r requirements.txt (line 2)) (1.16.0)\nRequirement already satisfied: numpy>=1.16.0 in \/opt\/conda\/lib\/python3.8\/site-packages (from catboost==1.0.1->-r requirements.txt (line 4)) (1.19.1)\nRequirement already satisfied: scipy in \/opt\/conda\/lib\/python3.8\/site-packages (from catboost==1.0.1->-r requirements.txt (line 4)) (1.7.1)\nRequirement already satisfied: matplotlib in \/opt\/conda\/lib\/python3.8\/site-packages (from catboost==1.0.1->-r requirements.txt (line 4)) (3.4.3)\nRequirement already satisfied: pandas>=0.24.0 in \/opt\/conda\/lib\/python3.8\/site-packages (from catboost==1.0.1->-r requirements.txt (line 4)) (1.2.4)\nRequirement already satisfied: python-dateutil>=2.7.3 in \/opt\/conda\/lib\/python3.8\/site-packages (from pandas>=0.24.0->catboost==1.0.1->-r requirements.txt (line 4)) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in \/opt\/conda\/lib\/python3.8\/site-packages (from pandas>=0.24.0->catboost==1.0.1->-r requirements.txt (line 4)) (2021.3)\nRequirement already satisfied: pillow>=6.2.0 in \/opt\/conda\/lib\/python3.8\/site-packages (from matplotlib->catboost==1.0.1->-r requirements.txt (line 4)) (8.3.2)\nRequirement already satisfied: pyparsing>=2.2.1 in \/opt\/conda\/lib\/python3.8\/site-packages (from matplotlib->catboost==1.0.1->-r requirements.txt (line 4)) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in \/opt\/conda\/lib\/python3.8\/site-packages (from matplotlib->catboost==1.0.1->-r requirements.txt (line 4)) (0.10.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in \/opt\/conda\/lib\/python3.8\/site-packages (from matplotlib->catboost==1.0.1->-r requirements.txt (line 4)) (1.3.2)\ntenacity is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nInstalling collected packages: plotly, graphviz, sagemaker-jumpstart-script-utilities, catboost\nAttempting uninstall: plotly\nFound existing installation: plotly 5.3.1\nUninstalling plotly-5.3.1:\nSuccessfully uninstalled plotly-5.3.1\nSuccessfully installed catboost-1.0.1 graphviz-0.17 plotly-5.1.0 sagemaker-jumpstart-script-utilities-1.0.0\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https:\/\/pip.pypa.io\/warnings\/venv\n2022-07-22 07:36:32,568 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2022-07-22 07:36:32,604 sagemaker-training-toolkit INFO     Invoking user script\nTraining Env:\n{\n    \"additional_framework_parameters\": {},\n    \"channel_input_dirs\": {\n        \"model\": \"\/opt\/ml\/input\/data\/model\",\n        \"training\": \"\/opt\/ml\/input\/data\/training\"\n    },\n    \"current_host\": \"algo-1\",\n    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n    \"hosts\": [\n        \"algo-1\"\n    ],\n    \"hyperparameters\": {},\n    \"input_config_dir\": \"\/opt\/ml\/input\/config\",\n    \"input_data_config\": {\n        \"model\": {\n            \"ContentType\": \"application\/x-sagemaker-model\",\n            \"TrainingInputMode\": \"File\",\n            \"S3DistributionType\": \"FullyReplicated\",\n            \"RecordWrapperType\": \"None\"\n        },\n        \"training\": {\n            \"TrainingInputMode\": \"File\",\n            \"S3DistributionType\": \"FullyReplicated\",\n            \"RecordWrapperType\": \"None\"\n        }\n    },\n    \"input_dir\": \"\/opt\/ml\/input\",\n    \"is_master\": true,\n    \"job_name\": \"jumpstart-catboost-classification-model-2022-07-22-07-33-18-038\",\n    \"log_level\": 20,\n    \"master_hostname\": \"algo-1\",\n    \"model_dir\": \"\/opt\/ml\/model\",\n    \"module_dir\": \"s3:\/\/jumpstart-cache-prod-us-east-1\/source-directory-tarballs\/catboost\/transfer_learning\/classification\/v1.1.3\/sourcedir.tar.gz\",\n    \"module_name\": \"transfer_learning\",\n    \"network_interface_name\": \"eth0\",\n    \"num_cpus\": 4,\n    \"num_gpus\": 0,\n    \"output_data_dir\": \"\/opt\/ml\/output\/data\",\n    \"output_dir\": \"\/opt\/ml\/output\",\n    \"output_intermediate_dir\": \"\/opt\/ml\/output\/intermediate\",\n    \"resource_config\": {\n        \"current_host\": \"algo-1\",\n        \"current_instance_type\": \"ml.m5.xlarge\",\n        \"current_group_name\": \"homogeneousCluster\",\n        \"hosts\": [\n            \"algo-1\"\n        ],\n        \"instance_groups\": [\n            {\n                \"instance_group_name\": \"homogeneousCluster\",\n                \"instance_type\": \"ml.m5.xlarge\",\n                \"hosts\": [\n                    \"algo-1\"\n                ]\n            }\n        ],\n        \"network_interface_name\": \"eth0\"\n    },\n    \"user_entry_point\": \"transfer_learning.py\"\n}\nEnvironment variables:\nSM_HOSTS=[\"algo-1\"]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={}\nSM_USER_ENTRY_POINT=transfer_learning.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"}\nSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application\/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[\"model\",\"training\"]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=transfer_learning\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=4\nSM_NUM_GPUS=0\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/jumpstart-cache-prod-us-east-1\/source-directory-tarballs\/catboost\/transfer_learning\/classification\/v1.1.3\/sourcedir.tar.gz\nSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"\/opt\/ml\/input\/data\/model\",\"training\":\"\/opt\/ml\/input\/data\/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"\/opt\/ml\/input\/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application\/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"\/opt\/ml\/input\",\"is_master\":true,\"job_name\":\"jumpstart-catboost-classification-model-2022-07-22-07-33-18-038\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"\/opt\/ml\/model\",\"module_dir\":\"s3:\/\/jumpstart-cache-prod-us-east-1\/source-directory-tarballs\/catboost\/transfer_learning\/classification\/v1.1.3\/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"\/opt\/ml\/output\/data\",\"output_dir\":\"\/opt\/ml\/output\",\"output_intermediate_dir\":\"\/opt\/ml\/output\/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\nSM_USER_ARGS=[]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_CHANNEL_MODEL=\/opt\/ml\/input\/data\/model\nSM_CHANNEL_TRAINING=\/opt\/ml\/input\/data\/training\nPYTHONPATH=\/opt\/ml\/code:\/opt\/conda\/bin:\/opt\/conda\/lib\/python38.zip:\/opt\/conda\/lib\/python3.8:\/opt\/conda\/lib\/python3.8\/lib-dynload:\/opt\/conda\/lib\/python3.8\/site-packages\nInvoking script with the following command:\n\/opt\/conda\/bin\/python3.8 transfer_learning.py\nINFO:root:Validation data is not found. 20.0% of training data is randomly selected as validation data. The seed for random sampling is 200.\nTraceback (most recent call last):\n  File \"_catboost.pyx\", line 2167, in _catboost.get_float_feature\nFile \"_catboost.pyx\", line 1125, in _catboost._FloatOrNan\n  File \"_catboost.pyx\", line 949, in _catboost._FloatOrNanFromString\nTypeError: Cannot convert 'b'BROOKLYN'' to float\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"transfer_learning.py\", line 221, in <module>\nru\n```",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-19T14:50:29.012Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello - can you remove the categorical_index.json inside Amazon S3 --> Buckets---> R-sandbox-sagemaker--->ml\/---> train\/ and place it in the directory above it, i.e. Amazon S3 --> Buckets---> R-sandbox-sagemaker--->ml\/. So only data.csv is in Amazon S3 --> Buckets---> R-sandbox-sagemaker--->ml\/---> train\/. Then re-run. Please let me know if this works.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-08-19T01:25:57.015Z",
                "Answer_upvote_count":0,
                "Answer_body":"@AWS-User-9634033 I did what ever you suggest now it gets the below error:\n\n2022-08-19 14:43:30 Uploading - Uploading generated training model\n2022-08-19 14:43:30 Failed - Training job failed\nProfilerReport-1660920003: Stopping\n\n---------------------------------------------------------------------------\nUnexpectedStatusException                 Traceback (most recent call last)\n\/tmp\/ipykernel_9559\/1137348215.py in <cell line: 23>()\n     21 \n     22 # Launch a SageMaker Training job by passing s3 path of the training data\n---> 23 tabular_estimator.fit(\n     24         {\"training\": training_dataset_s3_path}, logs=True, job_name=training_job_name\n     25     )\n\n~\/anaconda3\/envs\/python3\/lib\/python3.8\/site-packages\/sagemaker\/workflow\/pipeline_context.py in wrapper(*args, **kwargs)\n    246             return self_instance.sagemaker_session.context\n    247 \n--> 248         return run_func(*args, **kwargs)\n    249 \n    250     return wrapper\n\n~\/anaconda3\/envs\/python3\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)\n   1062         self.jobs.append(self.latest_training_job)\n   1063         if wait:\n-> 1064             self.latest_training_job.wait(logs=logs)\n   1065 \n   1066     def _compilation_job_name(self):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n   2145         # If logs are requested, call logs_for_jobs.\n   2146         if logs != \"None\":\n-> 2147             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   2148         else:\n   2149             self.sagemaker_session.wait_for_job(self.job_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.8\/site-packages\/sagemaker\/session.py in logs_for_job(self, job_name, wait, poll, log_type)\n   3851 \n   3852         if wait:\n-> 3853             self._check_job_status(job_name, description, \"TrainingJobStatus\")\n   3854             if dot:\n   3855                 print()\n\n~\/anaconda3\/envs\/python3\/lib\/python3.8\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n   3389                     actual_status=status,\n   3390                 )\n-> 3391             raise exceptions.UnexpectedStatusException(\n   3392                 message=message,\n   3393                 allowed_statuses=[\"Completed\", \"Stopped\"],\n\nUnexpectedStatusException: Error for Training job jumpstart-catboost-classification-model-2022-08-19-14-40-03-772: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"\"\nCommand \"\/opt\/conda\/bin\/python3.8 transfer_learning.py\", exit code: 1",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Input and Output interface for the CatBoost algorithm",
        "Question_creation_time":1658463993278,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-0PVSBTSR4GvFO3E5FusCQ\/input-and-output-interface-for-the-cat-boost-algorithm",
        "Question_topic":[
            "Developer Tools",
            "Security, Identity, & Compliance",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS CodeBuild",
            "AWS CodeDeploy",
            "AWS Artifact",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":66,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"to set up CatBoost Classifier as a built-in algorithm, aws in this [https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/catboost.html] suggested this notebook [https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/lightgbm_catboost_tabular\/Amazon_Tabular_Classification_LightGBM_CatBoost.ipynb] , my question is should I prepare inference file on top of the train.csv? if yes what is that and how it should be prepared?",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-22T15:29:47.312Z",
                "Answer_upvote_count":1,
                "Answer_body":"According to the documentation,[https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/catboost.html] 'The CatBoost built-in algorithm runs in script mode, but the training script is provided for you and there is no need to replace it. If you have extensive experience using script mode to create a SageMaker training job, then you can incorporate your own CatBoost training scripts.' Is the same with the Inference script, all provided artifacts.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2022-07-22T15:34:44.534Z",
                "Answer_upvote_count":1,
                "Answer_body":"For the built-in algorithms, you can simply specify estimator.deploy(), or tuner.deploy() and the trained model will be deployed to an endpoint for inference.\n\nYou can also bring your own code\/model, in which case, you'll need an inference.py file. See Use your own Inference Code for details.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Embedding categorical feature indices for the Catboost algorithm in AWS",
        "Question_creation_time":1658411979721,
        "Question_link":"https:\/\/repost.aws\/questions\/QU8Tddi-4ZQCe_ZyuCyep7Hw\/embedding-categorical-feature-indices-for-the-catboost-algorithm-in-aws",
        "Question_topic":[
            "Developer Tools",
            "Security, Identity, & Compliance",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS CodeDeploy",
            "AWS Artifact",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":105,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"According to the below link : https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/lightgbm_catboost_tabular\/Amazon_Tabular_Classification_LightGBM_CatBoost.ipynb if the prediction includes categorical feature(s), a json-format file have to be used. Preparing the JSON file was confusing. here is more information provided there: \" If the predictors include categorical feature(s), a json-format file named 'categorical_index.json' should be included in the input directory to indicate the column index(es) of the categorical features. Within the json-format file, it should have a python directory where the key is a string of 'cat_index_list' and the value is a list of unique integer(s). Each integer in the list indicates the column index of categorical features in the 'data.csv'. The range of each integer should be more than 0 (index 0 indicates the target) and less than the total number of columns.\"\n\nCould you please advise me to create the JSON-format file for the categorical feature indices?",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-22T16:34:35.379Z",
                "Answer_upvote_count":0,
                "Answer_body":"According to the documentation : Categorical features input format: If your predictors include categorical features, you can provide a JSON file named categorical_index.json in the same location as your data directories. This file should contain a Python dictionary where the key is the string \"cat_index_list\" and the value is a list of unique integers. Each integer in the value list should indicate the column index of the corresponding categorical features in your training data CSV file. Each value should be a positive integer (greater than zero because zero represents the target value), less than the Int32.MaxValue (2147483647), and less than the total number of columns. There should only be one categorical index JSON file. {\"cat_index_list\":[ 1,2,3]} 1,2,3 being the indices of the categorial columns.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Build error on\u300eMLOps and integrations\u300f",
        "Question_creation_time":1658392692433,
        "Question_link":"https:\/\/repost.aws\/questions\/QUoaMO0NfzTDKQ-sG2P0cbNw\/build-error-on%E3%80%8E-ml-ops-and-integrations%E3%80%8F",
        "Question_topic":[
            "Developer Tools",
            "Machine Learning & AI",
            "Training and Certification"
        ],
        "Question_tag":[
            "AWS CodePipeline",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Training and Certification"
        ],
        "Question_upvote_count":0,
        "Question_view_count":61,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello, I have a question about one of the Workshops published in the AWS Workshop studio.\n\nI am going through the \"MLOps and integrations\" hands-on, but it is not working well during the process.\n\nHere is the URL for the hands-on.\n\nSpecifically, the build process performed when deploying the provided CloudFormation template will generate an error occurs. I think that the URL for S3 listed in the distributed source(source\/test.py) in the workshop is probably not valid.\n\nDo any of you know how to solve this problem?\n\n----Excerpts from the CodeBuild error log-----\n\nTraceback (most recent call last): File \"source\/test.py\", line 20, in <module> test_file,\n\nFile \"\/usr\/local\/lib\/python3.6\/site-packages\/wget.py\", line 526, in download (tmpfile, headers) = ulib.urlretrieve(binurl, tmpfile, callback)\n\nFile \"\/usr\/local\/lib\/python3.6\/urllib\/request.py\", line 248, in urlretrieve with contextlib.closing(urlopen(url, data)) as fp:\n\nFile \"\/usr\/local\/lib\/python3.6\/urllib\/request.py\", line 223, in urlopen return opener.open(url, data, timeout)\n\nFile \"\/usr\/local\/lib\/python3.6\/urllib\/request.py\", line 532, in open response = meth(req, response)\n\nFile \"\/usr\/local\/lib\/python3.6\/urllib\/request.py\", line 642, in http_response 'http', request, response, code, msg, hdrs)\n\nFile \"\/usr\/local\/lib\/python3.6\/urllib\/request.py\", line 570, in error return self._call_chain(*args)\n\nFile \"\/usr\/local\/lib\/python3.6\/urllib\/request.py\", line 504, in _call_chain result = func(*args)\n\nFile \"\/usr\/local\/lib\/python3.6\/urllib\/request.py\", line 650, in http_error_default raise HTTPError(req.full_url, code, msg, hdrs, fp)\n\nurllib.error.HTTPError: HTTP Error 403: Forbidden",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-26T15:12:22.449Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi , Have you created your user token from github, as indicated in the 'Set up your environment' part of the workshop and then input the token on the \"Create the Cloud Formation stack\" config? - (Create Github token - https:\/\/docs.github.com\/en\/authentication\/keeping-your-account-and-data-secure\/creating-a-personal-access-token)",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"FileNotFoundError: [Errno 2] No such file or directory: '\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.8\/site-packages\/sagemaker\/image_uri_config\/catboost.json'",
        "Question_creation_time":1658343315554,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3HFACW88SuKcGZ2izeOsuA\/file-not-found-error-errno-2-no-such-file-or-directory-home-ec-2-user-anaconda-3-envs-python-3-lib-python-3-8-site-packages-sagemaker-image-uri-config-catboost-json",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "AWS Deep Learning Containers",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Elastic Container Registry (ECR)",
            "Amazon Elastic Container Service"
        ],
        "Question_upvote_count":0,
        "Question_view_count":87,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"I have an issue while getting Catboost image URI. It is a function for generating ECR image URIs for pre-built SageMaker Docker images. Here is my code catboost_container = sagemaker.image_uris.retrieve(\"catboost\", my_region, \"latest\")",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-21T02:18:42.829Z",
                "Answer_upvote_count":1,
                "Answer_body":"As illustrated here in the docs for the algorithm, the parameters for retrieving this URI are a bit different: It's more like using the new JumpStart models (if you're familiar with that) than the old-style pre-built algorithms.\n\ntrain_model_id, train_model_version, train_scope = \"catboost-classification-model\", \"*\", \"training\"\ntraining_instance_type = \"ml.m5.xlarge\"\n\n# Retrieve the docker image\ntrain_image_uri = image_uris.retrieve(\n    region=None,\n    framework=None,\n    model_id=train_model_id,\n    model_version=train_model_version,\n    image_scope=train_scope,\n    instance_type=training_instance_type\n)\n\n\nI tested the above snippet from the doc page on SageMaker Studio and it worked OK. If you still see errors, it's likely your SageMaker Python SDK version is outdated (which can happen if for example you don't restart SM Studio apps or SM Notebook Instances regularly). Can check with sagemaker.__version__ and upgrade with !pip install --upgrade sagemaker if needed.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2022-07-22T17:41:29.834Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks for your guidence, I make the JSON file accordingly however another issue come up : I posted it in https:\/\/repost.aws\/questions\/QUVfbc_AsXRzaxAl69MMFlsQ\/error-for-training-job-catboost-classification-model-error-message-type-error-cannot-convert-xxx-to-float\n\nI'm trying to find a solution and I have searched a lot but with no success. Please share your thoughts on this as well! Thanks",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"InternalServerError with SageMaker Batch transform job",
        "Question_creation_time":1658253043323,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyua146pJQ0a7kJZK-JgGHw\/internal-server-error-with-sage-maker-batch-transform-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":59,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"We have a few thousand models on SageMaker backed by our own containers on ECR. We have a problem with one of the models that when we start a Batch transform job with it the job is pending, then after ~20 minutes the job is marked as failed with \"InternalServerError: We encountered an internal error. Please try again.\". Attempting the job again didn't help. Is there any way to debug this?\n\nThe models execution role, container, security group and subnets are set correctly. The containers are all in the same repo which the execution role has permission to access. The container image is ~3GB, but is definitely not the largest one that we have run, and all our other models run fine with the same job parameters.",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"Should SageMaker Canvas region and S3 region be the same?",
        "Question_creation_time":1658221373656,
        "Question_link":"https:\/\/repost.aws\/questions\/QULHZtj6HwQReouXor72UuSg\/should-sage-maker-canvas-region-and-s-3-region-be-the-same",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Canvas"
        ],
        "Question_upvote_count":0,
        "Question_view_count":56,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi, I'm going to use the canvas by connecting to S3. When using sagemaker canvas, should the canvas region and S3 region be the same? Thank you.",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-19T12:35:55.411Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi, S3 does not have to be in the same region as SageMaker Canvas, but make sure your user has the correct permissions to access the bucket!",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Notebook Kernel Dying During Training",
        "Question_creation_time":1658076038858,
        "Question_link":"https:\/\/repost.aws\/questions\/QUO7VnXi3jT76cpgAEPZ_k7g\/sagemaker-notebook-kernel-dying-during-training",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":66,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I created a machine learning pipeline in a Sagemaker notebook instance (ml.m4.10xlarge, volume size: 16384GB EBS) where the kernel keeps restarting around 20% of the way through the process. I want to upgrade my notebook instances to meet the requirements for my workflow but am a bit confused as to what instance type would be sufficient to complete the task and also where I can purchase notebook instances.\n\nAny help is appreciated and I am happy to provide further commentary as needed.",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-18T03:10:06.004Z",
                "Answer_upvote_count":0,
                "Answer_body":"You should probably be using SageMaker training jobs for this, rather than trying to scale up your notebook instance.\n\nSageMaker is more than a managed Jupyter service. By running your model training through the training job APIs (for e.g. as discussed here, using the high-level SageMaker Python SDK, you get benefits of:\n\nAutomatic tracking of runs (e.g. input parameters and code, output artifacts, logs, resource usage metrics, custom algorithm metrics, container image, etc.)\nReproducible containerized environments (pre-built containers with requirements.txt support, in case you don't want to build customized containers yourself)\nRight-sizing your infrastructure usage to optimize cost - keep your notebook instance small, request bigger instance(s) for your training job, and only pay for the time the training job is actually running.\nIntegration with SageMaker options for model deployment \/ batch inference, etc.\nTraining runs separate from the notebook, so you can e.g. restart your notebook kernel, kill the notebook instance, struggle with connectivity, etc... during training with no impact.\n\nSo I would suggest to set up your training job referring to the Using XYZ with the SageMaker Python SDK sections of the developer guide, and the Amazon SageMaker Examples. This likely won't immediately fix your scaling challenge, but it should put you in a better position for scaling further (e.g. distributed training) and tracking your work. For most of my work, I just use e.g. t3.medium notebooks and interact with the SageMaker APIs to run jobs with on-demand infrastructure.\n\nWith that being said, your instance already sounds very large (160GB RAM, 16TB disk). The most common causes I've seen of kernel dying are failure to allocate memory - so if you're using in-memory libraries like Scikit-Learn\/etc, perhaps it could be that one of them is not able to handle a single massive data structure, even though there is physical memory available? E.g. due to something assuming 32 bit indexing, or some other aspect of the script\/libraries being used. It's interesting that you manage to get 20% of the way through training, since often ML training is usually pretty homogeneous (e.g. for gradient descent, often if you can complete one epoch, you can run 'em all). Perhaps you have a memory leak somewhere? Giving more details about what framework & model type you're using might help guide suggestions, but ultimately I think it might require debugging your code to see where exactly things are going wrong.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Policy that allows only one SSO user to access a resource",
        "Question_creation_time":1658054479812,
        "Question_link":"https:\/\/repost.aws\/questions\/QUruheXJHaQVu_S9LIzUyDAw\/policy-that-allows-only-one-sso-user-to-access-a-resource",
        "Question_topic":[
            "Security, Identity, & Compliance",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_tag":[
            "AWS Identity and Access Management",
            "Amazon SageMaker",
            "AWS IAM Identity Center",
            "DevOps",
            "IAM Policies"
        ],
        "Question_upvote_count":1,
        "Question_view_count":90,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"We are in a process to move all of our IAM users to aws SSO we used to have this policy for sagemaker :\n\n\"\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:ListTags\",\n                \"sagemaker:DeleteNotebookInstance\",\n                \"sagemaker:StopNotebookInstance\",\n                \"sagemaker:CreatePresignedNotebookInstanceUrl\",\n                \"sagemaker:DescribeNotebookInstance\",\n                \"sagemaker:StartNotebookInstance\",\n                \"sagemaker:UpdateNotebookInstance\"\n            ],\n            \"Resource\": \"arn:aws:sagemaker:::notebook-instance\/${aws:username}*\"\n        },\n        {\n            \"Sid\": \"VisualEditor1\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:ListNotebookInstanceLifecycleConfigs\",\n                \"sagemaker:ListNotebookInstances\",\n                \"sagemaker:ListCodeRepositories\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\n\n\"\n\nthis would give access to each user to use his\\hers own notebook now on the new SSO permission set i gave this\n\n\"\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"glue:CreateScript\",\n                \"secretsmanager:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:ListTags\",\n                \"sagemaker:DeleteNotebookInstance\",\n                \"sagemaker:StopNotebookInstance\",\n                \"sagemaker:CreatePresignedNotebookInstanceUrl\",\n                \"sagemaker:Describe*\",\n                \"sagemaker:StartNotebookInstance\",\n                \"sagemaker:UpdateNotebookInstance\",\n                \"sagemaker:CreatePresignedDomainUrl\",\n                \"sagemaker:*\"\n            ],\n            \"Resource\": \"arn:aws:sagemaker:::notebook-instance\/*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"aws:ResourceTag\/Owner\": \"${identitystore:UserId}\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:ListTags\",\n                \"sagemaker:Describe*\",\n                \"sagemaker:StartNotebookInstance\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\"\n\n\nthis is what i tried but i cant make it work please assist?",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-18T12:46:17.804Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hello,\n\nI understand that you are currently trying to restrict access to Sagemaker notebook using SSO identity's UserID.\n\nCurrently, I leveraged your provided SSO Permission set and tweaked it out as you can see below, and finally tested it out on AWS SageMaker Console by logging in as an AWS SSO User, and was able to see successful start\/stop\/describing of the SageMaker notebook (with Tags - Owner:UserId) corresponding to the SSO UserId.\n\n{\n\t\"Version\": \"2012-10-17\",\n\t\"Statement\": [\n\t\t{\n\t\t\t\"Effect\": \"Allow\",\n\t\t\t\"Action\": [\n\t\t\t\t\"glue:CreateScript\",\n\t\t\t\t\"secretsmanager:*\"\n\t\t\t],\n\t\t\t\"Resource\": \"*\"\n\t\t},\n\t\t{\n\t\t\t\"Effect\": \"Allow\",\n\t\t\t\"Action\": [\n\t\t\t\t\"sagemaker:ListTags\",\n\t\t\t\t\"sagemaker:DeleteNotebookInstance\",\n\t\t\t\t\"sagemaker:StopNotebookInstance\",\n\t\t\t\t\"sagemaker:CreatePresignedNotebookInstanceUrl\",\n\t\t\t\t\"sagemaker:Describe*\",\n\t\t\t\t\"sagemaker:StartNotebookInstance\",\n\t\t\t\t\"sagemaker:UpdateNotebookInstance\",\n\t\t\t\t\"sagemaker:CreatePresignedDomainUrl\"\n\t\t\t],\n\t\t\t\"Resource\": \"arn:aws:sagemaker:us-east-1:7XXXXXXXXX:notebook-instance\/*\",\n\t\t\t\"Condition\": {\n\t\t\t\t\"StringEquals\": {\n\t\t\t\t\t\"sagemaker:ResourceTag\/Owner\": \"${identitystore:UserId}\"\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\t{\n\t\t\t\"Sid\": \"VisualEditor1\",\n\t\t\t\"Effect\": \"Allow\",\n\t\t\t\"Action\": [\n\t\t\t\t\"sagemaker:ListNotebookInstanceLifecycleConfigs\",\n\t\t\t\t\"sagemaker:ListNotebookInstances\",\n\t\t\t\t\"sagemaker:ListCodeRepositories\"\n\t\t\t],\n\t\t\t\"Resource\": \"*\"\n\t\t}\n\t]\n}\n\n\nHowever, in case if this SSO User tried to stop any other Sagemaker notebooks, which didn't have the tags corresponding to their UserId, then the following errors were observed as expected behavior -\n\nUser: arn:aws:sts::7XXXXXXXXX:assumed-role\/AWSReservedSSO_SageMXXXXXXXXXbe\/test1 is not authorized to perform: sagemaker:StopNotebookInstance on resource: arn:aws:sagemaker:us-east-1:7XXXXXXXXX:notebook-instance\/userachecking because no identity-based policy allows the sagemaker:StopNotebookInstance action\n\nor \n\nUser: arn:aws:sts::7XXXXXXXXX:assumed-role\/AWSReservedSSO_SageMXXXXXXXXXbe\/test1 is not authorized to perform: sagemaker:DescribeNotebookInstance on resource: arn:aws:sagemaker:us-east-1:7XXXXXXXXX:notebook-instance\/Test1Check because no identity-based policy allows the sagemaker:DescribeNotebookInstance action\n\n\nAlso, please note that unlike your provided IAM policy, your SSO permission set policy was missing the action - sagemaker:ListNotebookInstances which also raised an error for not being able to list out the notebook instances on AWS SageMaker Console in my testing. Hence, I had added the appropriate Sagemaker list actions to your permission set as well.\n\nAdditional Information -\n\na. ${identitystore:UserId} -> Each user in the AWS SSO identity store is assigned a unique UserId. You can view the UserId for your users by using the AWS SSO console and navigating to each user or by using the DescribeUser API action. [1]\n\nb. ListNotebookInstances -> Returns a list of the SageMaker notebook instances in the requester's account in an AWS Region. [2]\n\nc. ResourceTag -> You can use the ResourceTag\/key-name condition key to determine whether to allow access to the resource based on the tags that are attached to the resource. [3][4]\n\nd. sagemaker:ResourceTag\/ -> Filters access by the preface string for a tag key and value pair attached to a resource [5]\n\ne. sagemaker:ResourceTag\/${TagKey} -> Filters access by a tag key and value pair [5]\n\nI hope the shared information is insightful to your query. In case, if you have any other queries or concerns regarding AWS SSO or Sagemaker services or any account specific configuration that you would like to discuss, then please feel free to reach out to our team directly by creating a support case with our premium support team.\n\nHave a wonderful day ahead and stay safe.\n\nReferences:\n\n[1] https:\/\/docs.aws.amazon.com\/singlesignon\/latest\/userguide\/using-predefined-attributes.html\n\n[2] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ListNotebookInstances.html\n\n[3] https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/access_tags.html\n\n[4] https:\/\/aws.amazon.com\/blogs\/security\/simplify-granting-access-to-your-aws-resources-by-using-tags-on-aws-iam-users-and-roles\/\n\n[5] https:\/\/docs.aws.amazon.com\/service-authorization\/latest\/reference\/list_amazonsagemaker.html#amazonsagemaker-policy-keys",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Suspicious Billing with SageMaker",
        "Question_creation_time":1658000861225,
        "Question_link":"https:\/\/repost.aws\/questions\/QU31OY4Rt9TVeEJBL4ctmgnA\/suspicious-billing-with-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":49,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi, I'm being charged up to USD 16,000 with AWS SageMaker when I'm not using anything. I keep receiving suspicious activity notification with my account. What should I do to terminate this SageMaker billing? Should I close my account?",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-18T01:29:48.249Z",
                "Answer_upvote_count":0,
                "Answer_body":"If you identify suspicious activity on your account, you should\n\nIdentify any unauthorized actions taken by the AWS Identity and Access Management (IAM) identities in your account.\nIdentify any unauthorized access or changes to your account.\nIdentify the creation of any unauthorized resources or IAM users. Steps to perform these activities can be found here: https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/potential-account-compromise\/\n\nYou should as well look at all the resources in your account and identify\/terminate the ones that are not legitimate.\n\nAWS cost explorer can help you identify where these Sagemaker charges are and you can terminate them: https:\/\/aws.amazon.com\/aws-cost-management\/aws-cost-explorer\/\n\nOpen a case with support if you have any question. Whatever support level you currently have you can always create a Billing\/account case to understand any past or upcoming charges.\n\nOnce this is done, make sure you apply security best practices to protect your aws account : https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/security-best-practices\/",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Lambda Function to invoke sagemaker endpoint",
        "Question_creation_time":1657965474513,
        "Question_link":"https:\/\/repost.aws\/questions\/QU33wE3pnRS9Om2yfVt4EIAg\/lambda-function-to-invoke-sagemaker-endpoint",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Lambda",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":107,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi AWS, I need to create a lambda function that will invoke the SageMaker endpoint that will send a text description for which it will return a generated image.\n\nThe endpoint is generated using HuggingFace model.\n\nI need your help with the code and the steps to obtain it.\n\nThanks Arjun Goel",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-17T16:20:21.662Z",
                "Answer_upvote_count":0,
                "Answer_body":"There is a blog that shows how you can invoke a Sagemaker endpoint from a lambda function - https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-07-16T18:28:31.470Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yes I have gone through the approach as mentioned by you above but the problem is I am not a Machine Learning Expert so it will take me sometime to write inference for Text-to-Image. If you have python code for that it would be great as I need to complete that on urgent basis.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Endpoint is not created when deploying HuggingFace Model using it.",
        "Question_creation_time":1657903012949,
        "Question_link":"https:\/\/repost.aws\/questions\/QUT4ywRDmOTO-8YSR4MBrVKg\/sagemaker-endpoint-is-not-created-when-deploying-hugging-face-model-using-it",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Lambda",
            "Amazon SageMaker",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0,
        "Question_view_count":54,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to deploy the HuggingFace model onto sagemaker. Here is the link for the model: https:\/\/huggingface.co\/dalle-mini\/dalle-mini\n\nI am testing in my personal account and here is the code for the same:\n\nfrom sagemaker.huggingface import HuggingFaceModel\nimport sagemaker\n\nsess = sagemaker.Session()\n# sagemaker session bucket -> used for uploading data, models and logs\n# sagemaker will automatically create this bucket if it not exists\nsagemaker_session_bucket='sagemaker-hugging-face-model-demo'\nif sagemaker_session_bucket == 'sagemaker-hugging-face-model-demo' and sess is not None:\n    # set to default bucket if a bucket name is not given\n    sagemaker_session_bucket = sess.default_bucket()\n\nrole = sagemaker.get_execution_role()\nsess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n\nprint(f\"sagemaker role arn: {role}\")\nprint(f\"sagemaker bucket: {sess.default_bucket()}\")\nprint(f\"sagemaker session region: {sess.boto_region_name}\")\n\n\nhub = {\n    'HF_MODEL_ID':'dalle-mini\/dalle-mini',\n    'HF_TASK':'Text-to-image'\n}\n\nhuggingface_model = HuggingFaceModel(\n  env=hub,\n  role=role,\n  #image_uri=\"428136181372.dkr.ecr.ca-central-1.amazonaws.com\/sagemaker-hugging-face\",\n  transformers_version=\"4.6.1\",     # transformers version used\n  pytorch_version=\"1.7\",          # pytorch version used\n  py_version='py36'\n)\n\n# deploy model to Sagemaker Inference\npredictor = huggingface_model.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m5.xlarge'\n)\n\n\n\nWhen I am trying to create the sagemaker endpoint I am experiencing the error: ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Requested image 428136181372.dkr.ecr.ca-central-1.amazonaws.com\/sagemaker-hugging-face not found.\n\nAlso I need to create a lambda function that will invoke the SageMaker endpoint that will send a text description for which it will return a generated image. E.g. --> The text Sun is shining should be transformed to image after the lambda function invokes the sagemaker endpoint.\n\nAlso need to know what should be the ContentType for image.",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-18T03:49:36.138Z",
                "Answer_upvote_count":0,
                "Answer_body":"I see you have an incorrect-looking image_uri commented-out there...\n\nOne aspect of the SageMaker Python SDK that can be a little confusing at first is there is no direct correspondence between a \"model\" in the SDK (e.g. HuggingFaceModel) and a \"Model\" in the SageMaker APIs (as shown in Inference > Models page of the AWS Console for SageMaker).\n\nThe reason for this is that SDK \"Model\" constructors don't collect quite all the information needed to define API Models: If image_uri is not specified, you don't know until you .deploy() or .transformer() to a particular instance_type, whether you're using a CPU or GPU instance and therefore whether you should be using the CPU or GPU container image... And a specific container image is needed before it can create the API Model. Because of this:\n\nWhen you first ran the code (I guess with image_uri included), the Model was not actually created in SageMaker API\/Console until it reached the .deploy() step\nIn some situations, the SDK might re-use the initially created API Model rather than re-creating it with the new parameters (e.g. are you specifying a specific name that you just removed for publishing the code snippet?)\n\nSo if you removed the explicit image_uri and are still seeing the error about incorrect image URI, I would go in to SageMaker Console and explicitly delete the previous Model to force your code to create it from scratch using the updated params. (Of course there are also API\/SDK ways to do this e.g. huggingface_model.delete_model()). When you just use the HuggingFaceModel class provide the framework version parameters, it should be able to look up the correct URI itself.\n\nSince AWS Lambda runtimes don't have the high-level SageMaker Python SDK installed by default, I'd probably suggest to use plain boto3 SageMakerRuntime invoke_endpoint there (rather than e.g. predictor.predict() as you'll usually see used in notebooks).\n\nI'm not sure yet what format the default pipeline will expect for your image inputs, or even if the default model serving stack is already set up to return images nicely (since Hugging Face has historically mainly been used for text). Possibly you'll need to customize the output processing, which you can do by defining your own output_fn (and even predict_fn, model_fn, input_fn if needed) as documented here. I'd first try sending in your input as application\/json similar to { \"instances\": [\"Sun is shining\"] } with an application\/json Accept header as well, and see what type of response that gets you.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"3d Ground truth labelling job issue in bbox",
        "Question_creation_time":1657799060933,
        "Question_link":"https:\/\/repost.aws\/questions\/QUnfsUXUBLT0i2HEn3-CW3jQ\/3-d-ground-truth-labelling-job-issue-in-bbox",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth"
        ],
        "Question_upvote_count":0,
        "Question_view_count":45,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi, I have created a 3d labelling job for 100 frames and got the job running. When I started annotating the objects with cuboid I am facing 2 issues,\n\nThe size of the bounding box for respective object is changing (increased or decreased) as I move to the next frames and drawing the bounding box (with increased or decreased size to the earlier frames). I believe this could be because of autofill functionality. Please confirm me whether I have an option to off the autofill functionality.\nThe bounding boxes are getting scattered or displaced each time I login and start working. If I annotate the objects for 10 frames, save it and logout for today, later when I login back the bounding boxes were scattered for couple of frames. Can you please let me know what could be done to reduce this issue.",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"[bug report] Sagemaker data wrangler: An error occurred loading this view",
        "Question_creation_time":1657768857538,
        "Question_link":"https:\/\/repost.aws\/questions\/QUr_PSwJDeTxG5mPrjSYKZrA\/bug-report-sagemaker-data-wrangler-an-error-occurred-loading-this-view",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Data Wrangler"
        ],
        "Question_upvote_count":0,
        "Question_view_count":53,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello,\n\nI import my data from Athena, then add a new custom data transform. As soon as I click on the \"Custom transform\" option, the error occurs with message: An error occurred loading this view. There is no other useful message to find out the problem. Please tell me how to troubleshot or fix this problem.\n\nThank you",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-19T08:23:23.433Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi! Please open a support ticket with AWS Support, Support Team will be able to help you with this issue!",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Increase Limit on Lineage Tracking entities for sagemaker",
        "Question_creation_time":1657755893027,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwNhM5g6qRN2orm9ttoPV6w\/increase-limit-on-lineage-tracking-entities-for-sagemaker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":38,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"My Team is trying to onboard to sagemaker lineage tracking entities and we basically track models, datasets, associations between these entities all the way to endpoints. Currently, we have been using another system for the same. We currently require that we create dataset entities prior to our training job so that we can use this to reference during our training jobs. The problem comes with the constraints on the amount of manual entities that can be created. As per the doc, the limits are\n\nMaximum number of manually created lineage entities Actions: 3000 Artifacts: 6000 Associations: 6000 Contexts: 500\n\nOur current system holds about 1500 datasets and 1000 models which means that we might hit the limit in the near future if we onboard to sagemaker. Is there a provision to increase the limits on these? I am not sure why these limits are placed. These entities must be pretty cheap to store. Please let me know if there is any way to get this limit increased",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-15T04:37:37.084Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can double-check in the self-service AWS Service Quotas tool on your AWS account, but it looks to me like these quotas are not listed there (for now, at least).\n\n...So please raise a request via AWS Support from your account: Yes I believe there is flexibility in these quotas, but if you can include information in the request about projected requirements, usage pattern and the use case, that should help the team there assess the ask as quickly as possible.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Notebook keeps hanging\/freezing",
        "Question_creation_time":1657750167416,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbUkR0L2-Q1CcAHtTbLYJmg\/sagemaker-notebook-keeps-hanging-freezing",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":50,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I have been using Sagemaker Studio Notebook and suddenly it started hanging. When this happens, the notebook freezes completely. Than I have to wait some seconds (the delay duration is not constant and is common to reach about 30 seconds) and then it just freezes again, making its usage impossible. I was using a temporary account provided by Udacity and after trying different approaches to find and solve the problem, I switched to a personal account but the problem persists. Approaches I have tried so far:\n\nShutdow and start kernel\nRestart kernel\nRestart kernel and clear outputs\nLog out and Login (from Sagemaker)\nLog out and Login (from AWS)\nChange region\nTrying a different browser (I tried Chrome and Firefox)\nTrying using other account (personal)\n\nI also checked CloudWatch logs but didn't find anything that seemed unusual.",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-15T03:52:28.682Z",
                "Answer_upvote_count":1,
                "Answer_body":"The most likely cause of this from my experience is a (very) large number of active git changes.\n\nGiven your \"current\" working folder (the one you're navigated to in the folder sidebar menu), the jupyterlab-git integration regularly checks if you're inside a git repository and polls for changes in that repository if so.\n\nWhen this list is very large, I've sometimes seen it cause significant slowdowns in the overall UI because of the way the underlying (open-source) extension works. This has been discussed before for example in this GitHub issue - which is now marked closed but I've still seen it happening.\n\nFor example, maybe you (like me \ud83d\ude05) forgot to gitignore a data folder or node_modules and generated thousands of untracked files there: You might see a significant slowdown whenever you're navigated to a folder within the scope of that git repo.\n\nSuggested solution would be:\n\nUse the folder sidebar to navigate anywhere other than the affected git repository (e.g. to your root folder?), and you should see the slowdown resolve pretty much immediately if this is the underlying cause\nNow the tricky task of finding and clearing up the problemmatic folder(s) without navigating to them in the folder GUI:\nYou could use a System Terminal, cd to the affected folder and run git status to see where the many changes are hiding, if you're not sure already\nAdd a .gitignore file (or modify your existing one) to make git ignore those changes. Because it starts with a dot, .gitignore is hidden by default in the JupyterLab file browser anyway. I usually use a system terminal to e.g. cp myrepo\/.gitignore gitignore.txt to create a visible copy (somewhere other than the repository folder which you're trying to avoid navigating to!) and then mv gitignore.txt myrepo\/.gitignore to overwrite with my edited version\n\nAlternatively (if e.g. it's a folder full of new files that you no longer care about like node_modules) you could just slog through the slowness to delete the problemmatic folder in the UI - but of course the problem would return if you re-created them later without .gitignore.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"SageMaker XGBoost batch transform AttributeError",
        "Question_creation_time":1657717396655,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3Hva4yNpSfOtRQTjKVMvvg\/sage-maker-xg-boost-batch-transform-attribute-error",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":71,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\n\nAfter training XGBoost model using SageMaker inbuilt algorithm, I am trying to perform batch transform operation. I am doing the same steps as for linear learner model which worked fine there. However in case of XGBoost I get a following error while creating a transformer:\n\nAttributeError: module 'sagemaker' has no attribute 'utils'\n\n\nThe piece of code causing the error is:\n\nxgb_transformer = xgb_estimator.transformer(\n    instance_count = 1, \n    instance_type = 'ml.m4.10xlarge',\n    output_path = '{}\/{}'.format(output_path,'output')\n)\n\n\nI use '1.5-1' version of XGBoost as image in training, and 2.86.2 version of SageMaker.\n\nAny help would be highly appreciated!",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-15T17:46:23.737Z",
                "Answer_upvote_count":0,
                "Answer_body":"I have tried replicating the issue and by updating the SageMaker Package to the latest version sagemaker-2.99.0 I did not find any issues creating the batch transform job. Requesting you to update the sagemaker package to the latest version and let us know if you continue to face the issue.\n\n!pip3 install -U sagemaker\n\nIf you continue to face the issue I would encourage you to open a support ticket with AWS along with the sample code, the training job and the batch transform job ARN. Due to security reason, this post is not suitable for sharing customer's resource.\n\nReferences:\n\n[1] https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/aws_sagemaker_studio\/sagemaker_studio_image_build\/xgboost_bring_your_own\/Batch_Transform_BYO_XGB.html\n\n[2] https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_applying_machine_learning\/xgboost_customer_churn\/xgboost_customer_churn.ipynb",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Export Autopilot model to GovCloud region",
        "Question_creation_time":1657645831488,
        "Question_link":"https:\/\/repost.aws\/questions\/QUV6_OvmWMRjiuQIx82Z4_Eg\/export-autopilot-model-to-gov-cloud-region",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Autopilot"
        ],
        "Question_upvote_count":0,
        "Question_view_count":41,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi As AWS Sagemaker autopilot is not available in GovCloud region, is it possible to export a model trained on non-GovCloud environment in GovCloud environment.\n\nWhat I have done:\n\nRan Autopilot on non-GovCloud environment\nI was able to download the output model.joblib(preprocessing) and xgboost models from output bucket in S3 bucket\nI was not able to load the model.joblib preprocessing model since the sagemaker-sklearn-extention gives symbol not found error(https:\/\/issuehint.com\/issue\/awslabs\/ml-io\/28)\n\nThanks for your help and insights in exporting the model to GovCloud environment.",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"OSError: [Errno 28] No space left on device -- PyTorch, CNN, estimator",
        "Question_creation_time":1657555011984,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuRZhu6ZpTPSmVO-4P2GDmQ\/os-error-errno-28-no-space-left-on-device-py-torch-cnn-estimator",
        "Question_topic":[
            "Machine Learning & AI",
            "Storage"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Storage"
        ],
        "Question_upvote_count":0,
        "Question_view_count":404,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"hey there, I'm training a convolutional neural network (CNN) on a large dataset (10k images - 50 GB) stored on S3 bucket using estimator(sagemaker infrastructure) . everything works well when I work with 2000 images which has the total size of almost 5-10 GB. however, I get an error when I increase number of images to 3000 or more. the error indicates that there is no space left on device. I also attached the estimator setup to this message, as you can see I am using ml.g4dn.12xlarge instance which has 192 GB of memory!! I also increased the volume size to 900 GB. I still don't know why I am getting space\/storage error!! I know that error is related the function \"_get_train_data_loader\" in which it is trying to download the images!! I was reading somewhere that EFS (elastic file system) might help with this issue, if so, I don't know how to specify it in the estimator. estimator = PyTorch( entry_point=\"pbdl_sm.py\", role=role, framework_version=\"1.4.0\", py_version=\"py3\", instance_count=1, instance_type=\"ml.g4dn.12xlarge\", volume_size = 900, hyperparameters={\"epochs\": 6, \"backend\": \"gloo\",\"lr\": 0.001,\"train_size\":2900,\"n_realz\":3000}, )",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-13T19:49:21.485Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi\n\nThank you for reaching out to us.\n\nIn general the no space left on device error occurs when there is a high disk utilization, Requesting you to review the instance metrics and cloudwatch metrics\/logs of the training job for more detailed information.\n\nIt also depends on the various factors like learning rate, number epochs and configuration of the training job and the estimator, However I would recommend you to try distributed training with the pytorch using the smdistributed [1] on multiple instances , Currently, the following are supported: distributed training with parameter servers, SageMaker Distributed (SMD) Data and Model Parallelism, and MPI. SMD Model Parallelism can only be used with MPI.\n\nTo enable the SageMaker distributed data parallelism: { \"smdistributed\": { \"dataparallel\": { \"enabled\": True } } }\n\nI would also recommend to try the PIPE mode, With Pipe input mode, your dataset is streamed directly to your training instances instead of being downloaded first. This means that your training jobs start sooner, finish quicker, and need less disk space.\n\nIf you are facing any issues and require further investigation on the issue, I would encourage you to open a case with the premium support along with the training job ARN and the cloudwatch logs of the job.Due to security reason, this post is not suitable for sharing customer's resource.\n\nReference:\n\n[1] https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/sagemaker.pytorch.html\n\n[2] https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/\n\n[3] https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/80df7d61a4bf14a11f0442020e2003a7c1f78115\/advanced_functionality\/pipe_bring_your_own\/train.py\n\n[4] https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/training\/distributed_training\/pytorch\/data_parallel\/maskrcnn\/pytorch_smdataparallel_maskrcnn_demo.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"[Sagemaker] - Does Sagemaker support Xpress model",
        "Question_creation_time":1657298680086,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOPI761YwSQWqpUusp3Mo_g\/sagemaker-does-sagemaker-support-xpress-model",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_upvote_count":0,
        "Question_view_count":63,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\n\nWe have a use case to use Sagemaker towards running Xpress based science models. Can anyone please tell what are the pros and cons of using sagemaker towards running the Xpress models\n\nThanks",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-11T21:17:16.850Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nThank you for using AWS SageMaker.\n\nUsing Xpress models on SageMaker will have similar pros and cons of using Bring your container(BYOC) on SageMaker.\n\nWhile using BYOC users have all the options to control the package version and their desire packages at their end and will have full control over it with the help of docker image. But it requires intermediate knowledge of Docker and is not recommended unless you are comfortable writing your own machine learning algorithm. Along with that you will need to maintain your docker image and any sort of update that is required for the dependent packages to run your inference code.\n\nFor more details, I'd recommend you to visit our AWS public documentation in BYOC. If incase any further assistance is required, I'd recommend to open a case with SageMaker Support engineering team for further guidance. To open a support case with AWS using the link: https:\/\/console.aws.amazon.com\/support\/home?#\/case\/create",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to feed seed code to GitHub Repository from Sagemaker Projects Organization Template created with Service Catalog?",
        "Question_creation_time":1657259337697,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_Y4T-A3aQySFeRr3feBscA\/how-to-feed-seed-code-to-git-hub-repository-from-sagemaker-projects-organization-template-created-with-service-catalog",
        "Question_topic":[
            "Developer Tools",
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "AWS CodeBuild",
            "Amazon SageMaker",
            "AWS CloudFormation",
            "AWS Service Catalog"
        ],
        "Question_upvote_count":0,
        "Question_view_count":169,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"The objective is to replicate \"MLOps template for model building, training, and deployment with third-party Git repositories using Jenkins\" builtin Sagemaker Project template. I want to feed custom seed code to the Github repository each time a project is created using my organization custom template instead of the default seed code that the builtin template feeds.\n\nI am able to create the custom template using service catalog but I could not find a solution for feeding the seed code to github repo. So, I decided to see how the built in project template is doing this and it is using resources from this bucket \"s3:\/\/sagemaker-servicecatalog-seedcode-us-east-1\/bootstrap\/GitRepositorySeedCodeCheckinCodeBuildProject-v1.0.zip\" but I could not access it. I am not sure how to achieve the objective?",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-11T17:23:15.771Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can download the seed package using awscli s3 cp <s3_uri> <target_path> or by using this URL: https:\/\/sagemaker-servicecatalog-seedcode-us-east-1.s3.amazonaws.com\/bootstrap\/GitRepositorySeedCodeCheckinCodeBuildProject-v1.0.zip\n\nThis .zip is used by CodeBuild that is called when the template is deployed (by a lambda mapped to a CFN custom component). If you take a look in the template you'll find a component named \"SageMakerModelBuildSeedCodeCheckinProjectTriggerLambdaInvoker\". You can find some env vars defined for this component like: SEEDCODE_BUCKET_NAME and SEEDCODE_BUCKET_KEY. These vars point to an S3 uri that has another .zip file with the content of the seed for the git repo. If you get the default values defined there you can re-create the URL and download the .zip file as well: https:\/\/sagemaker-servicecatalog-seedcode-us-east-1.s3.amazonaws.com\/toolchain\/model-building-workflow-jenkins-v1.0.zip\n\nSo, in the end, if you want to change the content that is pushed to the git repo, you can redefine these 2 vars and point to an S3 path that contains a .zip file you created.\n\nBonus: If you're a curious person, I recommend you to take a look at the .java file (src\/main\/java\/GitRepositorySeedCodeBootStrapper.java) inside the .zip of the CodeBuild .zip for you to understand what it does to prepare the git repo like: download a .zip, unpack it, commit\/push to the git repo.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"\"Failure reason Image size 12704675783 is greater than supported size 10737418240\" when creating serverless endpoint in SageMaker.",
        "Question_creation_time":1657243407002,
        "Question_link":"https:\/\/repost.aws\/questions\/QU90699ONgQD2t2HUKzm9AUA\/failure-reason-image-size-12704675783-is-greater-than-supported-size-10737418240-when-creating-serverless-endpoint-in-sage-maker",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_upvote_count":0,
        "Question_view_count":220,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"How to reproduce the error: We want to run Python Inference in SageMaker. Because our model is pre-trained out side the SageMaker and has some special logic, so we need to create customer image. We see the document https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/prebuilt-containers-extend.html#prebuilt-containers-extend-tutorial We use the 763104351884.dkr.ecr.us-east-1.amazonaws.com\/pytorch-inference:1.11.0-gpu-py38-cu113-ubuntu20.04-sagemaker to be the base image. We wrote a dockerfile and use \"docker build\" to create a new image. Also, use \"docker push\" to push new image to Amazon ECR. We pushed it to 935877503070.dkr.ecr.us-east-1.amazonaws.com\/pytorch-inference:testaisage Then, we follow the document: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html Then, we went to SageMaker console https:\/\/us-east-1.console.aws.amazon.com\/sagemaker\/home?region=us-east-1#\/models We created model. We input the \"935877503070.dkr.ecr.us-east-1.amazonaws.com\/pytorch-inference:testaisage\" of our new image to \"Location of inference code image\". Then, we create Endpoint configuration. Then, we create Endpoint. But the Endpoint shows \"Failure reason Image size 12704675783 is greater than supported size 10737418240\".",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-08T09:20:53.961Z",
                "Answer_upvote_count":1,
                "Answer_body":"As you're seeing in the error message, SageMaker Serverless Inference imposes a limit of 10GiB (10737418240 bytes) on your deployed container size - which helps deliver quality of service for considerations like cold-start time. From a quick look I didn't see this mentioned in the SageMaker serverless docs, but as mentioned in the launch blog post, SageMaker Serverless is backed by AWS Lambda and the AWS Lambda quotas page lists the limit.\n\nSo to solve the issue (and still use SageMaker Serverless Inference), you'll need to look at optimizing that container image size by removing any unnecessary bloat (need to find almost 2GiB from the number you posted).\n\nSome suggestions on that:\n\nAre you currently building your actual model in to the image itself? The typical pattern on SageMaker is to host a model.tar.gz tarball on S3, which gets downloaded and extracted into your container at runtime. For large language models and similar, this can be a big size saving (although of course, optimizing overall S3+image size can still help give you the best start-up times). The contents of this file are flexible so you could offload multiple artifacts.\nI saw you're using the standard PyTorch DLC as a base... Are you replacing the entire serving stack, or slotting your custom logic into the one the DLC provides? The stack already provided in the PyTorch container already provides (see docs here) customization to model loading via model_fn, input de-serialization via input_fn, output serialization via output_fn, and actual prediction via predict_fn. The APIs between these user-defined functions are very flexible (for example can return pretty much whatever you like from model_fn, so long as predict_fn knows how to use it) - so I find in practice that it can support even complex requirements like custom request formats, pipelining multiple models together, advanced pre-processing, etc. I've seen some customers go straight to building custom serving stacks (and installing their dependencies alongside the existing e.g. TorchServe in the image) before realising that the pre-built could already support what they needed. Again, this inference.py script would live in your model.tar.gz.\nGeneral non-SageMaker-specific container image optimization guidelines would still apply: Like for e.g. you might see the AWS DLCs clearing apt caches in the same RUN command as performing apt installs. If you find yourself really struggling with the size of the base AWS DLC you could look in to building from scratch \/ another base, and installing everything you need... But of course, would need to do the due diligence to check you're including everything you need & it's optimized well.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-07-08T09:18:11.681Z",
                "Answer_upvote_count":0,
                "Answer_body":"You need a smaller container image. Also, take into consideration that at the moment SageMaker serverless endpoints do not support GPU acceleration (see https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html#serverless-endpoints-how-it-works-exclusions).",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Cant generate XGBoost training report in sagemaker, only profiler_report.",
        "Question_creation_time":1657222531986,
        "Question_link":"https:\/\/repost.aws\/questions\/QUskI-0YIvQ_2GRSkzyGiD2A\/cant-generate-xg-boost-training-report-in-sagemaker-only-profiler-report",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon S3 Glacier",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Model Training"
        ],
        "Question_upvote_count":0,
        "Question_view_count":57,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to generate the XGBoost training report to see feature importances however the following code only generates the profiler report.\n\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np\nimport pandas as pd\nfrom sagemaker.predictor import csv_serializer\nfrom sagemaker.debugger import Rule, rule_configs\n\n# Define IAM role\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\nrole = get_execution_role()\nprefix = 'sagemaker\/models'\nmy_region = boto3.session.Session().region_name \n\n# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\nxgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", my_region, \"latest\")\n\n\n\nbucket_name = 'binary-base' \ns3 = boto3.resource('s3')\ntry:\n    if  my_region == 'us-east-1':\n      s3.create_bucket(Bucket=bucket_name)\n    else: \n      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n    print('S3 bucket created successfully')\nexcept Exception as e:\n    print('S3 error: ',e)\n\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train\/train.csv')).upload_file('..\/Data\/Base_Model_Data_No_Labels\/train.csv')\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'validation\/val.csv')).upload_file('..\/Data\/Base_Model_Data_No_Labels\/val.csv')\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test\/test.csv')).upload_file('..\/Data\/Base_Model_Data\/test.csv'\n\n\nsess = sagemaker.Session()\nxgb = sagemaker.estimator.Estimator(xgboost_container,\n                                    role, \n                                    volume_size =5,\n                                    instance_count=1, \n                                    instance_type='ml.m4.xlarge',\n                                    output_path='s3:\/\/{}\/{}\/output'.format(bucket_name, prefix, 'xgboost_model'),\n                                    sagemaker_session=sess, \n                                    rules=rules)\n\nxgb.set_hyperparameters(objective='binary:logistic',\n                        num_round=100, \n                        scale_pos_weight=8.5)\n\nxgb.fit({'train': s3_input_train, \"validation\": s3_input_val}, wait=True)\n\n\nWhen Checking the output path via:\n\nrule_output_path = xgb.output_path + \"\/\" + xgb.latest_training_job.job_name + \"\/rule-output\"\n! aws s3 ls {rule_output_path} --recursive\n\n\nWhich Outputs:\n\n2022-07-07 18:40:27     329715 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-report.html\n2022-07-07 18:40:26     171087 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-report.ipynb\n2022-07-07 18:40:23        191 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/BatchSize.json\n2022-07-07 18:40:23        199 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/CPUBottleneck.json\n2022-07-07 18:40:23        126 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/Dataloader.json\n2022-07-07 18:40:23        127 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/GPUMemoryIncrease.json\n2022-07-07 18:40:23        198 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/IOBottleneck.json\n2022-07-07 18:40:23        119 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/LoadBalancing.json\n2022-07-07 18:40:23        151 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/LowGPUUtilization.json\n2022-07-07 18:40:23        179 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/MaxInitializationTime.json\n2022-07-07 18:40:23        133 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/OverallFrameworkMetrics.json\n2022-07-07 18:40:23        465 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/OverallSystemUsage.json\n2022-07-07 18:40:23        156 sagemaker\/models\/output\/xgboost-2022-07-07-18-35-55-436\/rule-output\/ProfilerReport-1657218955\/profiler-output\/profiler-reports\/StepOutlier.json\n\n\nAs you can see only the profiler report in created which does not interest me. Why isn't there a CreateXGBoostReport folder generated with the training report? How do I generate this\/what am I missing?",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"Unable to resolve the private dns name of a Sagemaker runtime VPC interface endpoint",
        "Question_creation_time":1657193804686,
        "Question_link":"https:\/\/repost.aws\/questions\/QUL78EiZhMTeq3v_FVsB-kXQ\/unable-to-resolve-the-private-dns-name-of-a-sagemaker-runtime-vpc-interface-endpoint",
        "Question_topic":[
            "Networking & Content Delivery",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon VPC",
            "Amazon SageMaker",
            "Domain Name System (DNS)"
        ],
        "Question_upvote_count":0,
        "Question_view_count":166,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I deployed interface endpoints for multiple AWS services into a dedicated subnet in my VPC. Besides a Sagemaker runtime interface endpoint I also created endpoints for CloudWatch logs, KMS and more.\n\nThe resolving of the service domain name (e.g. kms.eu-central-1.amazonaws.com) works for all endpoints, except for the Sagemaker runtime (e.g. runtime.sagemaker.eu-central-1.amazonaws.com).The endpoint specific domain names of the Sagemaker runtime endpoint also work, only the resolving of runtime.sagemaker.eu-central-1.amazonaws.com fails with no answers from the DNS server.\n\nI tried multiple times to recreate the endpoint, but that didn't help either. The resolving of the domain works if I deploy the endpoint into another test VPC.\n\nAny ideas on what could be wrong? Thanks in advance! Bert",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-08T09:43:02.973Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Bert,\n\nThere are two possible reasons.\n\nPrivate DNS name is not enable for this endpoint.\nThe endpoint doesn't have an interface in the subnet where the EC2 instance is running.\n\nVerify that private DNS is enabled for your endpoint.\n\n$ aws ec2 describe-vpc-endpoints --vpc-endpoint-ids vpce-044a93bEXAMPLE --query 'VpcEndpoints[].PrivateDnsEnabled' --region REGION\n\n\nIf the response is not \"true\", modify the endpoint and enable private DNS names. If it is already enabled, make sure that the endpoint has an interface in the AZ ( in any subnet in that AZ) where you your EC2 instance is running.\n\nFor example, if your EC2 instance is in eu-central-1a, make sure that the endpoint has an interface in one of the subnets in eu-central-1a.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-07-08T01:24:23.638Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Renjith_R\n\nThank you for the suggestions.\n\nPrivate DNS is enabled, so that is not the issue.\n\nAs far as I know the endpoint doesn't need to be in the same subnet or even in the same AZ as the EC2 instance calling the endpoint. At least this works for all the other endpoints like KMS and CloudWatch Logs. I tested it anyway and deployed the Sagemaker runtime endpoint into the same subnet as the EC2 instance and there it works.\n\nAnother test I did was deploying the Sagemaker runtime again into my \"endpoint subnet\" with private dns name disabled and attaching a private hosted zone for the Sagemaker runtime domain pointing to the private IP of the Sagemaker runtime endpoint. This works as well.\n\nAfterwards I removed the private hosted zone and enabled private dns name again and it stopped working.\n\nBest regards\n\nBert",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Not able to convert Hugging Face fine-tuned BERT model into AWS Neuron",
        "Question_creation_time":1657030067879,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzDs6ITDqQYegTLVfTmNKOA\/not-able-to-convert-hugging-face-fine-tuned-bert-model-into-aws-neuron",
        "Question_topic":[
            "Machine Learning & AI",
            "Compute"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Neuron"
        ],
        "Question_upvote_count":0,
        "Question_view_count":114,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi Team,\n\nI have a fine-tuned BERT model which was trained using following libraries. torch == 1.8.1+cu111 transformers == 4.19.4\n\nAnd not able to convert that fine-tuned BERT model into AWS neuron and getting following compilation errors. Could you please help me to resolve this issue?\n\nNote: Trying to compile BERT model on SageMaker notebook instance and with \"conda_python3\" conda environment.\n\nInstallation:\n\nSet Pip repository to point to the Neuron repository\n\n!pip config set global.extra-index-url https:\/\/pip.repos.neuron.amazonaws.com\n\nInstall Neuron PyTorch - Note: Tried both options below.\n\n\"#!pip install torch-neuron==1.8.1.* neuron-cc[tensorflow] \"protobuf<4\" torchvision sagemaker>=2.79.0 transformers==4.17.0 --upgrade\" !pip install --upgrade torch-neuron neuron-cc[tensorflow] \"protobuf<4\" torchvision\n\nModel compilation:\n\nimport os\nimport tensorflow  # to workaround a protobuf version conflict issue\nimport torch\nimport torch.neuron\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_path = 'model\/' # Model artifacts are stored in 'model\/' directory\n\n# load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path, torchscript=True)\n\n# create dummy input for max length 128\ndummy_input = \"dummy input which will be padded later\"\nmax_length = 128\nembeddings = tokenizer(dummy_input, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\nneuron_inputs = tuple(embeddings.values())\n\n# compile model with torch.neuron.trace and update config\nmodel_neuron = torch.neuron.trace(model, neuron_inputs)\nmodel.config.update({\"traced_sequence_length\": max_length})\n\n# save tokenizer, neuron model and config for later use\nsave_dir=\"tmpd\"\nos.makedirs(\"tmpd\",exist_ok=True)\nmodel_neuron.save(os.path.join(save_dir,\"neuron_model.pt\"))\ntokenizer.save_pretrained(save_dir)\nmodel.config.save_pretrained(save_dir)\n\n\nModel artifacts: We have got this model artifacts from multi-label topic classification model.\n\nconfig.json model.tar.gz pytorch_model.bin special_tokens_map.json tokenizer_config.json tokenizer.json\n\nError logs:\n\nINFO:Neuron:There are 3 ops of 1 different types in the TorchScript that are not compiled by neuron-cc: aten::embedding, (For more information see https:\/\/github.com\/aws\/aws-neuron-sdk\/blob\/master\/release-notes\/neuron-cc-ops\/neuron-cc-ops-pytorch.md)\nINFO:Neuron:Number of arithmetic operators (pre-compilation) before = 565, fused = 548, percent fused = 96.99%\nINFO:Neuron:Number of neuron graph operations 1601 did not match traced graph 1323 - using heuristic matching of hierarchical information\nWARNING:tensorflow:From \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/torch_neuron\/ops\/aten.py:2022: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nINFO:Neuron:Compiling function _NeuronGraph$698 with neuron-cc\nINFO:Neuron:Compiling with command line: '\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/neuron-cc compile \/tmp\/tmpv4gg13ze\/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output \/tmp\/tmpv4gg13ze\/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 128, 768], \"float32\"], \"1:0\": [[1, 1, 1, 128], \"float32\"]}, \"outputs\": [\"Linear_5\/aten_linear\/Add:0\"]} --verbose 35'\nINFO:Neuron:Compile command returned: -9\nWARNING:Neuron:torch.neuron.trace failed on _NeuronGraph$698; falling back to native python function call\nERROR:Neuron:neuron-cc failed with the following command line call:\n\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/neuron-cc compile \/tmp\/tmpv4gg13ze\/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output \/tmp\/tmpv4gg13ze\/graph_def.neff --io-config '{\"inputs\": {\"0:0\": [[1, 128, 768], \"float32\"], \"1:0\": [[1, 1, 1, 128], \"float32\"]}, \"outputs\": [\"Linear_5\/aten_linear\/Add:0\"]}' --verbose 35\nTraceback (most recent call last):\n  File \"\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/torch_neuron\/convert.py\", line 382, in op_converter\n    item, inputs, compiler_workdir=sg_workdir, **kwargs)\n  File \"\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/torch_neuron\/decorators.py\", line 220, in trace\n    'neuron-cc failed with the following command line call:\\n{}'.format(command))\nsubprocess.SubprocessError: neuron-cc failed with the following command line call:\n\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/neuron-cc compile \/tmp\/tmpv4gg13ze\/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output \/tmp\/tmpv4gg13ze\/graph_def.neff --io-config '{\"inputs\": {\"0:0\": [[1, 128, 768], \"float32\"], \"1:0\": [[1, 1, 1, 128], \"float32\"]}, \"outputs\": [\"Linear_5\/aten_linear\/Add:0\"]}' --verbose 35\nINFO:Neuron:Number of arithmetic operators (post-compilation) before = 565, compiled = 0, percent compiled = 0.0%\nINFO:Neuron:The neuron partitioner created 1 sub-graphs\nINFO:Neuron:Neuron successfully compiled 0 sub-graphs, Total fused subgraphs = 1, Percent of model sub-graphs successfully compiled = 0.0%\nINFO:Neuron:Compiled these operators (and operator counts) to Neuron:\nINFO:Neuron:Not compiled operators (and operator counts) to Neuron:\nINFO:Neuron: => aten::Int: 97 [supported]\nINFO:Neuron: => aten::add: 39 [supported]\nINFO:Neuron: => aten::contiguous: 12 [supported]\nINFO:Neuron: => aten::div: 12 [supported]\nINFO:Neuron: => aten::dropout: 38 [supported]\nINFO:Neuron: => aten::embedding: 3 [not supported]\nINFO:Neuron: => aten::gelu: 12 [supported]\nINFO:Neuron: => aten::layer_norm: 25 [supported]\nINFO:Neuron: => aten::linear: 74 [supported]\nINFO:Neuron: => aten::matmul: 24 [supported]\nINFO:Neuron: => aten::mul: 1 [supported]\nINFO:Neuron: => aten::permute: 48 [supported]\nINFO:Neuron: => aten::rsub: 1 [supported]\nINFO:Neuron: => aten::select: 1 [supported]\nINFO:Neuron: => aten::size: 97 [supported]\nINFO:Neuron: => aten::slice: 5 [supported]\nINFO:Neuron: => aten::softmax: 12 [supported]\nINFO:Neuron: => aten::tanh: 1 [supported]\nINFO:Neuron: => aten::to: 1 [supported]\nINFO:Neuron: => aten::transpose: 12 [supported]\nINFO:Neuron: => aten::unsqueeze: 2 [supported]\nINFO:Neuron: => aten::view: 48 [supported]\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-1-97bba321d013> in <module>\n     18 \n     19 # compile model with torch.neuron.trace and update config\n---> 20 model_neuron = torch.neuron.trace(model, neuron_inputs)\n     21 model.config.update({\"traced_sequence_length\": max_length})\n     22 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/torch_neuron\/convert.py in trace(func, example_inputs, fallback, op_whitelist, minimum_segment_size, subgraph_builder_function, subgraph_inputs_pruning, skip_compiler, debug_must_trace, allow_no_ops_on_neuron, compiler_workdir, dynamic_batch_size, compiler_timeout, _neuron_trace, compiler_args, optimizations, verbose, **kwargs)\n    182         logger.debug(\"skip_inference_context - trace with fallback at {}\".format(get_file_and_line()))\n    183         neuron_graph = cu.compile_fused_operators(neuron_graph, **compile_kwargs)\n--> 184     cu.stats_post_compiler(neuron_graph)\n    185 \n    186     # Wrap the compiled version of the model in a script module. Note that this is\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/torch_neuron\/convert.py in stats_post_compiler(self, neuron_graph)\n    491         if succesful_compilations == 0 and not self.allow_no_ops_on_neuron:\n    492             raise RuntimeError(\n--> 493                 \"No operations were successfully partitioned and compiled to neuron for this model - aborting trace!\")\n    494 \n    495         if percent_operations_compiled < 50.0:\n\nRuntimeError: No operations were successfully partitioned and compiled to neuron for this model - aborting trace!\n\n\nThanks a lot.",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-07T13:27:24.982Z",
                "Answer_upvote_count":0,
                "Answer_body":"The error message in your log shows Compile command returned: -9. This message typically indicates that the compiler process was killed. Normally this is due to the the OOM (out of memory) killer (run by the linux operating system) killing the compilation process due to memory exhaustion. The most recent version of torch-neuron should provide an updated message for -9 errors that reflects the typical cause for this failure mode.\n\nWe recommend you try compiling on an instance with more memory, such as an inf1.6xlarge. Note: you only need the larger instance for compilation; you can still use a smaller instance (such as an inf1.xlarge) to run inference.\n\nPlease let us know if compiling on a larger instance resolved the error you\u2019re seeing.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to search for Amazon SageMaker Models using Tags?",
        "Question_creation_time":1656956445714,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfe51Pe1zRKqts-P7SZVuSA\/how-to-search-for-amazon-sage-maker-models-using-tags",
        "Question_topic":[
            "Developer Tools",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS CodePipeline",
            "Build & Train ML Models",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Model Building"
        ],
        "Question_upvote_count":0,
        "Question_view_count":65,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"When I open the Model details in the Amazon SageMaker console, the details clearly show Tags that have been added to the model during it's creation.\n\n| Tags | | Key | | sagemaker:project-name | | aws:cloudformation:stack-name| | sagemaker:deployment-stage| | sagemaker:deployment-stage|\n\nBut the Search API provided by Amazon SageMaker, https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_Search.html\n\nmentions only the following resources can be searched for using Tags: Valid Values: TrainingJob | Experiment | ExperimentTrial | ExperimentTrialComponent | Endpoint | ModelPackage | ModelPackageGroup | Pipeline | PipelineExecution | FeatureGroup | Project | FeatureMetadata\n\nI wish to obtain Model details, not the ModelPackageGroup\/ModelPackage details, using Tags so if there is a way to do that, please share. Also if there is no way to obtain it using Tags, like the Search API Documentation suggests, what is the purpose of the Tags still present in the Model details?",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-17T18:27:41.176Z",
                "Answer_upvote_count":0,
                "Answer_body":"DescribeModel will provide information about model details, the tags in the model was created as a part of the CloudFormation deployment. It identify each resources by CF.\n\nTo search by tags, you could add custom tags for your purpose.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-07-04T18:23:35.438Z",
                "Answer_upvote_count":0,
                "Answer_body":"To use custom tag, you will need to search by GetResources from AWS Resource Groups: https:\/\/docs.aws.amazon.com\/ARG\/latest\/userguide\/find-resources-to-tag.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to set spark configuration parameters in PySparkProcessor() in sagemaker processing job?",
        "Question_creation_time":1656608387774,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhyE6RyH-QwaUsVFnslKjlg\/how-to-set-spark-configuration-parameters-in-py-spark-processor-in-sagemaker-processing-job",
        "Question_topic":[
            "Compute",
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "High Performance Compute",
            "Amazon SageMaker",
            "Amazon EMR",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":137,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi folks, I'm trying to set the spark executor instances & memory, driver memory and switch of dynamic allocation. What is the correct way to do it?",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-01T08:44:21.352Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi! You can achieve this by passing a \"configuration\" dictionary to the PySparkProcessor. Have a look at the example below to see exactly how to achieve this: https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_processing.html#configuration-override\n\nhappy coding",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"AWS sagemaker abalone example pipeline endpoint json rejected",
        "Question_creation_time":1656607883243,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJRDfNUpdR-WQgjsw_UOi5g\/aws-sagemaker-abalone-example-pipeline-endpoint-json-rejected",
        "Question_topic":[
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_upvote_count":0,
        "Question_view_count":145,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"We've just created a train\/build\/deploy template in AWS SageMaker which provides a deployment of an Abalone model. We're trying to test it via the Test Inference endpoint, but the JSON there is rejected with the following message:\n\nError invoking endpoint: Received client error (415) from model with message \"application\/json is not an accepted ContentType: csv, libsvm, parquet, recordio-protobuf, text\/csv, text\/libsvm, text\/x-libsvm, application\/x-parquet, application\/x-recordio-protobuf.\". See https:\/\/eu-west-1.console.aws.amazon.com\/cloudwatch\/home?region=eu-west-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/USEngProbOfConversion-staging in account 607522716587 for more information.\n\nHowever the Test Inference endpoint only allows us to hit the endpoint with JSON - what can we do? Here's a screenshot (if this dropbox embed works):\n\nbut that's not working so here's the request dump:\n\n{\n  \"body\": {\n    \"s-x\": \"M\",\n    \"length\": 3,\n    \"diameter\": 5,\n    \"height\": 7,\n    \"whole_weight\": 45,\n    \"shucked_weight\": 34,\n    \"viscera_weight\": 23,\n    \"shell_weight\": 76\n  },\n  \"contentType\": \"application\/json\",\n  \"endpointName\": \"USEngProbOfConversion-staging\",\n  \"customURL\": \"\",\n  \"customHeaders\": [\n    {\n      \"Key\": \"sm_endpoint_name\",\n      \"Value\": \"USEngProbOfConversion-staging\"\n    },\n    {\n      \"Key\": \"\",\n      \"Value\": \"\"\n    }\n  ]\n}",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-01T03:03:27.263Z",
                "Answer_upvote_count":0,
                "Answer_body":"So as you saw already, the SageMaker Studio \"Test inference\" UI currently only supports JSON format... But this is a constraint of the UI, not your endpoint.\n\nIf you want to test your deployed endpoint with non-JSON data, you can do this from code (e.g. from a notebook):\n\nUsing the sagemaker Python SDK, create a Predictor specifying your endpoint name and the relevant de\/serializers (from sagemaker.(de)serializers - for example sagemaker.serializers.CSVSerializer). Then call predictor.predict(data).\nUsing a boto3.client(\"sagemaker-runtime\"), serialize your data to required format yourself (e.g. \"M,3,5,7,45,34,23,76\") and then call invoke_endpoint().\n\nThis would be necessary if you're using a pre-built algorithm that doesn't support JSON as a request\/response format. Since you're using the Abalone pipeline example, I guess it's likely you're using XGBoost as a pre-built SageMaker algorithm?\n\nAlternatively, if you're building a custom algorithm with your own training script OR would be interested in using XGBoost as a script-mode framework - more information in the SageMaker SDK doc), you may like to extend your algorithm to accept application\/json requests and return JSON responses.\n\nThe process for this will vary a little by framework, but should be documented here for XGBoost. Essentially, you'll want to provide a script file e.g. inference.py which defines special functions input_fn() and output_fn(). You can provide implementations of these functions that accept application\/json content types and de\/serialize appropriately.\n\nThat way you could make your deployed endpoint support JSON format and therefore be able to use the test UI in SageMaker Studio.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Use S3 as a git repo",
        "Question_creation_time":1656607653458,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUeFHB_qvQw67d9knOyw1Ig\/use-s-3-as-a-git-repo",
        "Question_topic":[
            "Machine Learning & AI",
            "Storage"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "S3 Object Lock"
        ],
        "Question_upvote_count":0,
        "Question_view_count":236,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a sagemaker notebook that has no connections to internet or codecommit but has access to 1 s3 bucket. I would like to use that 1 s3 bucket as a place to house git repos, ideally I would like to be able to pull\/push to repos in that bucket from other sagemaker notebooks or ec2 instances that have connections to that bucket. Has anyone tried this before?",
        "Answers":[
            {
                "Answer_creation_date":"2022-06-30T16:53:27.277Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi! I believe from AWS we don't have an official solution for this strategy. The are some solutions out there like https:\/\/github.com\/bgahagan\/git-remote-s3 you could use if you can upload the latest release to the S3 bucket and install it on your notebook. Hope this helps!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-07-01T12:12:07.011Z",
                "Answer_upvote_count":0,
                "Answer_body":"Check out these assets to see if they can help to accomplish this: https:\/\/aws.amazon.com\/blogs\/machine-learning\/how-to-use-common-workflows-on-amazon-sagemaker-notebook-instances\/ https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-git-repo.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How long does it take for AWS tech support team to respond to a \"system impaired\" issue?",
        "Question_creation_time":1656580942341,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFgnjt9J3T0iXhE0axG10vQ\/how-long-does-it-take-for-aws-tech-support-team-to-respond-to-a-system-impaired-issue",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Support Case"
        ],
        "Question_upvote_count":0,
        "Question_view_count":76,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi all,\n\nI have raised a ticket for multiple issues we've been having with SageMaker lately, the ticket was created more than 36 hours ago, and I have not had any response, in fact the ticket hasn't even been assigned yet.\n\nThe case ID is 10300240931.\n\nI thought AWS guarantee a response under 12 hours for \"system impaired\" issues, does anyone know what I can do to accelerate this?\n\nthank you! Ruoy",
        "Answers":[
            {
                "Answer_creation_date":"2022-06-30T10:34:31.689Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Ruoy! My advice here is to scale this issue via your account team, they will have the mechanisms to scale this concern. If you are on basic or developer support, you could look into upgrading to business support for a day and open a live chat with support! Hope this helps",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Studio JupyterServer App does not load",
        "Question_creation_time":1656519218822,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5Da9xot8TwST0MP_8uRV2A\/sagemaker-studio-jupyter-server-app-does-not-load",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":85,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"After months of seamless work in SageMaker Studio, the JupyterServer App won't load for the last 4 days. The Control Panel shows that the JupyterServer is in \"Pending\" or \"Failed\" state after I try to launch the app. When clicking \"Launch app\", the screen shows that:\n\n\"The JupyterServer app default encountered a problem and was stopped.\"\nThe \"Restart Now\" button is visible, but pressing this results in the same behaviour. I created a new JupyterServer App and it experiences the same problem under that account. I use a different account for another project and the JupyterServer under that account works perfectly. I even mounted the EFS associated with the App on an EC2 instance and deleted some files to reduce the EFS volume but it did not help (it was 995 MB and as far as I know, 5GB is the default limit).\n\nI found a post stating the same problem from 2 years ago, but could not follow the advice to delete the app and create a new one, since the Delete option is not available in the Action dropdown (https:\/\/repost.aws\/questions\/QUxoSA7eTzQbK-T4OWjJvSmQ\/sage-maker-studio-will-not-load). All apps that I create is \"default\".\n\nPlease help, how could I overcome this and access Jupyter Lab again? Thank you.",
        "Answers":[
            {
                "Answer_creation_date":"2022-06-30T03:09:11.417Z",
                "Answer_upvote_count":0,
                "Answer_body":"Even if you're not seeing the \"Delete app\" option in console (because the app Failed), the good news is that the \"Restart\" button should be doing the same thing for you: So how can you find out more about what's breaking here and hopefully fix it?\n\nTo see logs, you can open the \/aws\/sagemaker\/studio log group in Amazon CloudWatch: Here you should find log streams like {DomainID}\/{UserProfileName}\/JupyterServer\/default and (if you have a lifecycle configuration script set up?) {DomainID}\/{UserProfileName}\/JupyterServer\/default\/LifecycleConfigOnStart.\n\nIf you do have a custom lifecycle configuration script set up, this can often be a point of failure: I'd suggest trying to detach it and\/or adding some extra flags like set -ux to help debug what might be going wrong in it. Also since SageMaker Studio recently launched JupyterLab v3 support in parallel to JLv1. If you've been experimenting with both versions, it's worth checking which version your user is currently configured to use, and remembering that some setup scripts which might work on one version could break on another.\n\nYour user's EFS home folder (and any additional setup the LCC script does) will be the only data persisted between JupyterServer launches, so content could be another point of failure. It sounds like you started to explore this already.\n\nYou could try to delete (or otherwise edit) the user's ~\/.jupyter folder from EFS to clear any customized Jupyter configuration settings that might be causing problems during start-up. Again, this may be useful if you're using any features or extensions for which the Jupyter configuration API changed between JLv1 and v3.\nI haven't found overall data volume to cause these kind of start-up problems myself so far. I have seen some cases where having a git repository with many active changes (e.g. thousands) causes a UI slowdown when working in the repository's folder, but I haven't seen it prevent the actual start-up I think?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-07-01T19:57:55.897Z",
                "Answer_upvote_count":0,
                "Answer_body":"@Alex_T thank you for the reply. The CloudWatch logs don't show any sign of failing notebooks, and I did not have any LCC either. The final solution was the following:\n\nI mounted the EFS associated to the SageMaker domain on an EC2 instance and made a backup of all notebooks and other files (saved them on my local computer too).\nThen, I deleted the SageMaker domain by following these steps: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-delete-domain.html (via CLI)\nCreated a new SageMaker domain. The JupyterServer App starts now.\nFinally, I mounted the new EFS to the EC2 instance and uploaded the notebooks and files. They are all visible and working in Jupyter.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Can I connect a Sagemaker \"Studio\" instance to a private github repository?",
        "Question_creation_time":1656504273482,
        "Question_link":"https:\/\/repost.aws\/questions\/QUH33ZXpiAQ_aV2TesXBNOBw\/can-i-connect-a-sagemaker-studio-instance-to-a-private-github-repository",
        "Question_topic":[
            "Developer Tools",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS CodeCommit",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":74,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I've successfully connected a Sagemaker \"notebook\" to a private github repository, but wondering if it isn't possible for a studio instance? Failing that is there an easy way to get the remote codecommit git url for an existing \"studio\" instance so that code there can at least be pulled to my local machine?",
        "Answers":[
            {
                "Answer_creation_date":"2022-06-30T10:17:37.550Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi tansaku!\n\nFor sure, SageMaker Studio is integrated with Git, so you can connect to both your public and private repositories!\n\nWhen you try to connect to a private repository, you will be asked to enter your username and password. Best practices here are to create a personal access token instead of using your password.\n\nIf you want to cache your credentials avoiding typing them each time you interact with the Github server, you can cache them or store them in the home folder with the following command run from the System Terminal:\n\n$ git config --global git credential.helper [cache|store]\n\n\nIf you choose store to store your credentials, it will be written to the ~\/.git-credentials file located in your home folder. The cache parameter stores credentials in memory and never on disk. It also accepts the --timeout <seconds> option, which changes the amount of time the daemon is kept running (default is \"900\" or 15 minutes).\n\nOnce it executes the command, the next time it pushes it will ask for the credentials and store them, after that it shouldn't ask again.\n\nHope this helps!",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Studio Jupyterlab 3.0 working poorly with SM Resources UI",
        "Question_creation_time":1656420408518,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOmPLv2iwRyuEcQom58DgbA\/sage-maker-studio-jupyterlab-3-0-working-poorly-with-sm-resources-ui",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":3,
        "Question_view_count":195,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi all,\n\nSince Jupyterlab 3.0 was finally released on SM Studio, we have been super happy with it, however, for reasons unknown to us, the jupterlab interface works very poorly with SM resources, the following phenomenon have been observes:\n\nIt takes FOREVER to load the page for SM pipelines, and half the time it reports error (\"Error listing pipeline executions: Rate exceeded\")\nChanging instance type and size for a notebook is now super laggy, and do not work half the time\n\nAnyone knows if this is merely a lack of optimisation on the service team's part or is there something I can do to stop this behaviour? it's making our work very slow and unbearable, we don't want to revert back to 1.0 so any help would be greatly appreciated!\n\nBest, RUoy",
        "Answers":[
            {
                "Answer_creation_date":"2022-06-29T19:26:38.598Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thank you for sharing your observations.\n\nIn order to troubleshoot the issue, I would request you to delete the existing default Jupyter server app for the user profile and try creating a new app by launching the studio. Let us know if you are still facing the issue\n\nRequesting you to confirm if it is same behavior with all the users of the studio or specific user profiles.\n\nIn order deep-dive further and investigate I would request you to create a support ticket with the AWS technical team with the following information -- Domain ID -- User-profile ARN -- Cloudwatch logs\n\nand also the requesting you to share the list-pipelines[1] output from the CLI command.\n\nNote:If you still have difficulties, I recommend to cut a support case and provide more detail about your account information and above requested details. Due to security reason, we cannot discuss account specific issue in the public posts.\n\nThank you. Reference:\n\n[1] https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/list-pipelines.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"how can I use sagemaker_sklearn_extension in Sagemaker job?",
        "Question_creation_time":1656223637583,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMUk4WTgJRD68w1PvBudSow\/how-can-i-use-sagemaker-sklearn-extension-in-sagemaker-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":60,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm creating a data processing job in sagemaker notebook:\n\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nsklearn_processor = SKLearnProcessor(role=role,\n                                     base_job_name='end-to-end-ml-sm-proc',\n                                     instance_type='ml.m5.large',\n                                     instance_count=1,\n                                     framework_version='0.23-1')\n\n\nmy processing script uses :\n\nfrom sagemaker_sklearn_extension.decomposition import RobustPCA\n\n\nand I get an error during the job exectution:\n\nTraceback (most recent call last):\n  File \"\/opt\/ml\/processing\/input\/code\/preprocessor.py\", line 14, in <module>\n    from sagemaker_sklearn_extension.decomposition import RobustPCA\nModuleNotFoundError: No module named 'sagemaker_sklearn_extension'\n\n\nas far as I understrand : framework_version='0.23-1' should make sagemaker create docker image based on image from that repo: https:\/\/github.com\/aws\/sagemaker-scikit-learn-container and the 0.23-1 branch handles extensions installation (if extenssion\/Dockerfile.cpu file is executed), but I don't see how I can make Sagemaker run that script when creating the job.\n\nhow can I use sagemaker_sklearn_extension in Sagemaker job?",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-01T13:55:05.505Z",
                "Answer_upvote_count":0,
                "Answer_body":"There is a way to install the packages that you need via subprocess on the entry_point.py:\n\nimport subprocess\n\nlets pip install the custom package\n\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sagemaker-scikit-learn-extension==(your version)\"])",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to register a multi container model to a model registry?",
        "Question_creation_time":1655990380836,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwYjH1ZG2SNqPAB9VGmXT1A\/how-to-register-a-multi-container-model-to-a-model-registry",
        "Question_topic":[
            "Developer Tools",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS CodePipeline",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Pipelines",
            "Amazon SageMaker Model Building"
        ],
        "Question_upvote_count":0,
        "Question_view_count":157,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have created a multi-container model in SageMaker notebook and deployed it through an endpoint. But while attempting to do the same through a SageMaker Studio Project (build, train and deploy model template), I need to register the multi-container model through a 'sagemaker.workflow.step_collections.RegisterModel' step, which I am unable to do. What I understand till now is multi-container model is created through boto3 api call. I haven't found a way to create it using 'sagemaker.model.Model' and hence not being able to register it. Please help.",
        "Answers":[
            {
                "Answer_creation_date":"2022-06-23T13:42:23.974Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nin order to register a multi-container model you need to use the PipelineModel class instead of the Model one you are trying to do currently.\n\nAn example of this can be found in this notebook: https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-pipelines\/tabular\/train-register-deploy-pipeline-model\/train%20register%20and%20deploy%20a%20pipeline%20model.ipynb\n\nIn this example a pipeline model is created with 2 containers, first one is an sklearn one doing some preprocessing and the second an XGBoost one for ML inference. However, in your case, the number or containers, type, invocation order and more, may differ, but you should be able to draw parallels based on this example.\n\nHope this helps,",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Unable to configure SageMaker execution Role with access to S3 bucket in another AWS account",
        "Question_creation_time":1655804671957,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQgytxULcQxqXZVqFzklfBg\/unable-to-configure-sage-maker-execution-role-with-access-to-s-3-bucket-in-another-aws-account",
        "Question_topic":[
            "Storage",
            "Security, Identity, & Compliance",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon Simple Storage Service",
            "AWS Identity and Access Management",
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth"
        ],
        "Question_upvote_count":0,
        "Question_view_count":171,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Requirement: Create SakeMaker GroundTruth labeling job with input\/output location pointing to S3 bucket in another AWS account\n\nHigh Level Steps Followed: Lets say, Account_A: SageMaker GroundTruth labeling job and Account_B: S3 bucket\n\nCreate role AmazonSageMaker-ExecutionRole in Account_A with 3 policies attached:\nAmazonSageMakerFullAccess\nAccount_B_S3_AccessPolicy: Policy with necessary S3 permissions to access S3 bucket in Account_B\nAssumeRolePolicy: Assume role policy for arn:aws:iam::Account_B:role\/Cross-Account-S3-Access-Role\nCreate role Cross-Account-S3-Access-Role in Account_B with 1 policy and 1 trust relationship attached:\nS3_AccessPolicy: Policy with necessary S3 permissions to access S3 bucket in the this Account_B\nTrustRelationship: For principal arn:aws:iam::Account_A:role\/AmazonSageMaker-ExecutionRole\n\nError: While trying to create SakeMaker GroundTruth labeling job with IAM role as AmazonSageMaker-ExecutionRole, it throws error AccessDenied: Access Denied - The S3 bucket 'Account_B_S3_bucket_name' you entered in Input dataset location cannot be reached. Either the bucket does not exist, or you do not have permission to access it. If the bucket does not exist, update Input dataset location with a new S3 URI. If the bucket exists, give the IAM entity you are using to create this labeling job permission to read and write to this S3 bucket, and try your request again.",
        "Answers":[
            {
                "Answer_creation_date":"2022-06-21T14:37:16.595Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Kalmesh,\n\nThe labeling job will not assume other roles, so granting access via AssumeRolePolicy and TrustRelationship will not work in this case. The permissions need to be granted directly to AmazonSageMaker-ExecutionRole.\n\nThe easiest way to accomplish this is:\n\nAttach a policy to AmazonSageMaker-ExecutionRole granting appropriate permissions to the bucket in Account B. (It sounds like you have already done this via Account_B_S3_AccessPolicy)\nDefine a corresponding bucket policy on the S3 Bucket in Account B to allow access for the principal arn:aws:iam::Account_A:role\/AmazonSageMaker-ExecutionRole\n\nThere is also a more detailed explanation of the setup required in this knowledge center article: How can I grant a user in another AWS account the access to upload objects to my Amazon S3 bucket?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-07-01T07:36:15.275Z",
                "Answer_upvote_count":0,
                "Answer_body":"Reverted back to original approach where access to the SageMaker execution role was provided through direct S3 bucket policy.\n\nWhile creating the GT job from console:\n\nExpects the user creating the job also to have access to the data in cross account S3 bucket; Updated bucket policy to have access for both SageMaker execution role as well as user\nExpects the manifest in own account's S3 bucket; Fails with 403 if manifest is in cross account S3 bucket even though SageMaker execution role had access to the cross account S3 bucket\n\nWhile creating the GT job from CLI: Above restrictions doesn't apply and was able to create the GT job.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Input Manifest Errors in Sagemaker Ground Truth for Custom Labeling Job",
        "Question_creation_time":1655745668932,
        "Question_link":"https:\/\/repost.aws\/questions\/QUn7gIM_MkSHmd9IzuV4_pmw\/input-manifest-errors-in-sagemaker-ground-truth-for-custom-labeling-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth",
            "Amazon Comprehend"
        ],
        "Question_upvote_count":0,
        "Question_view_count":77,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am attempting to create a native PDF annotation labeling job for use with Comprehend to identify entities within similar documents. I have around 20 pdf files, all of them around 100-300 pages long.\n\nI used the tools and followed the directions from this blog post. I've struggled a little with the tools but ultimately got everything working.\n\nMy problem comes from the labeling job itself. When I open the labeling job in a private workforce that I've created, I find only a blank page. I did some research and found that there is something wrong with the input manifest as it seems AWS isn't able to parse it for some reason.\n\nI checked my manifest and found that it was generated as multiple objects. Each object was a single page from my PDFs. This seems normal, however the objects were not put into a list or 'top level' object, which does not fit JSON Lines guidelines. I attempted a quick fix of placing these objects all within a list (which satisfies JSON Lines) but it does not seem to help.\n\nAny suggestions or advice would be greatly appreciated.",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-19T06:55:04.062Z",
                "Answer_upvote_count":0,
                "Answer_body":"Dear Customer,\u2028\u2028\u2028\n\nThank you so much for reaching to us. I understand that you followed our AWS Comprehend documentation for annotating PDF\u2019s, in-order to annotate your training PDFs in SageMaker Ground Truth. However, there were some issues with the tool showing a blank page in the UI, and you are assuming it may be an issue with the input manifest. Hence you were looking for guidance in resolving this issue.\u2028\u2028\n\nThank you so much for providing the details.\u2028\u2028\n\nTo further better assist you on this issue, can you please create a Support Ticket to AWS. Below link will assist you to create the Support Ticket. [+]https:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html\u2028\u2028 [+]https:\/\/console.aws.amazon.com\/support\/home#\/case\/create\n\n\u2014While creating the support ticket, we kindly request you to provide the below information\n\nUse-case description.\nGround Truth Job ARN details\nScreen-shots of the issue you are facing.\nLog Files for the Ground truth Job(This logs from your labeling jobs appear in Amazon CloudWatch under the \/aws\/sagemaker\/LabelingJobs group.).\n\n\nThe reason behind this ask is this would help us to understand your use-case in a better way, further if we might need to deep dive and access the job created from our internal tools, we will have more visibility through the support ticket.\n\nRest assured we will do everything best in our abilities to assist you on this issue. \u2028\u2028 Thanks.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Why does my kernel keep restarting when I try to download pre-trained Hugging Face BERT models weights to Amazon SageMaker?",
        "Question_creation_time":1655735296435,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQYlSFOh6TjSDwR4if_vqOQ\/why-does-my-kernel-keep-restarting-when-i-try-to-download-pre-trained-hugging-face-bert-models-weights-to-amazon-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":48,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"When I try to download the pre-trained Hugging Face BERT models weights to the conda_pytorch_p36 kernel of my Amazon SageMaker Notebook instance using the following command, the kernel always restarts:\n\nPRE_TRAINED_MODEL_NAME2='sshleifer\/distilbart-cnn-12-6'\nmodel2 = BartForConditionalGeneration.from_pretrained(PRE_TRAINED_MODEL_NAME2, cache_dir='hf_cache_dir\/')\n\n\nNote I have installed following libraries using pip commands.\n\n!pip install transformers==4.17.0\n\n\nThe result is the same for Hugging Face \"facebook\/bart-large-cnn\" models.\n\nWhy is this happening, and how do I resolve the issue?",
        "Answers":[
            {
                "Answer_creation_date":"2022-06-26T06:07:25.840Z",
                "Answer_upvote_count":0,
                "Answer_body":"This typically happens when there's high resource utilization on the notebook instance and increasing instance type may help. Additionally, I would suggest that you open a support case under SageMaker queue by providing Sagemaker notebook ARN and associated Cloudwatch logs recorded for this notebook so that a support engineer can further troubleshoot the issue.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Cloudformation Deployment for Serverless Sagemaker Model Endpoint",
        "Question_creation_time":1655512915811,
        "Question_link":"https:\/\/repost.aws\/questions\/QUo7p2DuabQaapi9C4_nPVZg\/cloudformation-deployment-for-serverless-sagemaker-model-endpoint",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI",
            "Management & Governance",
            "DevOps"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "AWS CloudFormation",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_upvote_count":0,
        "Question_view_count":66,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi, i want to deploy a serverless model using CloudFormation.\n\nI've created the model, the endpoint configuration, and when I try to create the endpoint the script times out, because it can't find a saved model to attach to the endpoint (because I never trained one for this instance).\n\nI've tried to look around for a trainingjob cloudformation API, but there doesn't seem to be one.\n\nHow do I solve this issue?",
        "Answers":[
            {
                "Answer_creation_date":"2022-06-20T23:10:52.687Z",
                "Answer_upvote_count":0,
                "Answer_body":"In general I would like to confirm if you are bringing your own model which is trained outside the SageMaker ?\n\nIf so SageMaker expects to Upload the pre-trained model to S3 and specify the ModelDataUrl i.e s3 location along with the Container details with the CreateModel API call. For more details on bring your own model, please refer to our public examples [1][2][4][5]\n\nAlso as an alternative please refer to the following example to Bring your own model with Amazon SageMaker script mode which can be used for training and hosting ML models using the script mode[2]\n\nIf you have other questions or require any further clarifications please don't hesitate to reply or open a premium support case with the CloudFormation team for further investigation of the issue along with the error message and the resource details.\n\n[1] https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/advanced_functionality\/xgboost_bring_your_own_model\/xgboost_bring_your_own_model.html#Upload-the-pre-trained-model-to-S3\n\n[2] https:\/\/aws.amazon.com\/blogs\/machine-learning\/bring-your-own-model-with-amazon-sagemaker-script-mode\/\n\n[3] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\n\n[4] https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-model.html\n\n[5] https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-properties-sagemaker-model-containerdefinition.html",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-06-21T20:37:40.461Z",
                "Answer_upvote_count":0,
                "Answer_body":"thank you these docs were very helpful. It appears that when I use cloudformation for provisioning resources for my model, I need to provision the ECR, then do the training step, then create the model, enpoint config, then endpoint. I can't really do it all in 1 script, i guess.\n\neverything works now for me, so thank you for pointing me to this resource!",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Inconsistent keras model.summary() output shapes on AWS SageMaker and EC2",
        "Question_creation_time":1655494107206,
        "Question_link":"https:\/\/repost.aws\/questions\/QU8UWsqxW8RbejgzLHYIFduA\/inconsistent-keras-model-summary-output-shapes-on-aws-sage-maker-and-ec-2",
        "Question_topic":[
            "Machine Learning & AI",
            "Compute"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon EC2",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":36,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have the following model in a jupyter notebook:\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\n\n\n\nphysical_devices = tf.config.list_physical_devices('GPU')\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\n\nSIZE = (549, 549)\nSHUFFLE = False \nBATCH = 32\nEPOCHS = 20\n\ntrain_datagen =  DataGenerator(train_files, batch_size=BATCH, dim=SIZE, n_channels=1, shuffle=SHUFFLE)\ntest_datagen =  DataGenerator(test_files, batch_size=BATCH, dim=SIZE, n_channels=1, shuffle=SHUFFLE)\n\n\ninp = layers.Input(shape=(*SIZE, 1))\n\nx = layers.Conv2D(filters=549, kernel_size=(5,5), padding=\"same\", activation=\"relu\")(inp)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=549, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")(x)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=549, kernel_size=(1, 1), padding=\"same\", activation=\"relu\")(x)\nx = layers.BatchNormalization()(x)\n\nx = layers.Conv2D(filters=549, kernel_size=(3, 3), padding=\"same\", activation=\"sigmoid\")(x)\n\nmodel = Model(inp, x)\n\nmodel.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam())\n\nmodel.summary()\n\nSagemaker and EC2 are running tensorflow 2.7.1. The EC2 instance is p3.2xlarge with Deep Learning AMI GPU TensorFlow 2.7.0 (Amazon Linux 2) 20220607. The SageMaker notebook is using ml.p3.2xlarge and I am using the conda_tensorflow2_p38 kernel. The notebook is in an FSx Lustre file system that is mounted to both SageMaker and EC2 so it is definitely the same code running on both machines.\n\nnvidia-smi output on SageMaker:\n\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n| N\/A   37C    P0    24W \/ 300W |      0MiB \/ 16384MiB |      0%      Default |\n|                               |                      |                  N\/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n\nnvidia-smi output on EC2:\n\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n| N\/A   42C    P0    51W \/ 300W |   2460MiB \/ 16384MiB |      0%      Default |\n|                               |                      |                  N\/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N\/A  N\/A     11802      C   \/bin\/python3.8                    537MiB |\n|    0   N\/A  N\/A     26391      C   python3.8                        1921MiB |\n+-----------------------------------------------------------------------------+\n\n\nThe model.summary() output on SageMaker is:\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 549, 549, 1)]     0         \n                                                                 \n conv2d (Conv2D)             (None, 549, 549, 1)       7535574   \n                                                                 \n batch_normalization (BatchN  (None, 549, 549, 1)      4         \n ormalization)                                                   \n                                                                 \n conv2d_1 (Conv2D)           (None, 549, 549, 1)       2713158   \n                                                                 \n batch_normalization_1 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_2 (Conv2D)           (None, 549, 549, 1)       301950    \n                                                                 \n batch_normalization_2 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_3 (Conv2D)           (None, 549, 549, 1)       2713158   \n                                                                 \n=================================================================\nTotal params: 13,263,852\nTrainable params: 13,263,846\nNon-trainable params: 6\n\n\nThe model.summary() output on EC2 is (notice the shape change):\n\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 549, 549, 1)]     0         \n                                                                 \n conv2d (Conv2D)             (None, 549, 549, 549)     14274     \n                                                                 \n batch_normalization (BatchN  (None, 549, 549, 549)    2196      \n ormalization)                                                   \n                                                                 \n conv2d_1 (Conv2D)           (None, 549, 549, 549)     2713158   \n                                                                 \n batch_normalization_1 (Batc  (None, 549, 549, 549)    2196      \n hNormalization)                                                 \n                                                                 \n conv2d_2 (Conv2D)           (None, 549, 549, 549)     301950    \n                                                                 \n batch_normalization_2 (Batc  (None, 549, 549, 549)    2196      \n hNormalization)                                                 \n                                                                 \n conv2d_3 (Conv2D)           (None, 549, 549, 549)     2713158   \n                                                                 \n=================================================================\nTotal params: 5,749,128\nTrainable params: 5,745,834\nNon-trainable params: 3,294\n_________________________________________________________________\n\nOne other thing that is interesting, if I change my model on the EC2 instance to:\n\ninp = layers.Input(shape=(*SIZE, 1))\n\nx = layers.Conv2D(filters=1, kernel_size=(5,5), padding=\"same\", activation=\"relu\")(inp)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=1, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")(x)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=1, kernel_size=(1, 1), padding=\"same\", activation=\"relu\")(x)\nx = layers.BatchNormalization()(x)\n\nx = layers.Conv2D(filters=1, kernel_size=(3, 3), padding=\"same\", activation=\"sigmoid\")(x)\n\nmodel = Model(inp, x)\n\nmodel.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam())\n\nMy model.summary() output becomes:\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 549, 549, 1)]     0         \n                                                                 \n conv2d_8 (Conv2D)           (None, 549, 549, 1)       26        \n                                                                 \n batch_normalization_6 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_9 (Conv2D)           (None, 549, 549, 1)       10        \n                                                                 \n batch_normalization_7 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_10 (Conv2D)          (None, 549, 549, 1)       2         \n                                                                 \n batch_normalization_8 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_11 (Conv2D)          (None, 549, 549, 1)       10        \n                                                                 \n=================================================================\nTotal params: 60\nTrainable params: 54\nNon-trainable params: 6\n_________________________________________________________________\n\nIn the last model the shape is similar to SageMaker but the trainable parameters are very low.\n\nAny ideas as to why the output shape is different and why this is happening with the filters? When I run this model on my personal computer, the shape is the same as EC2. I think there might be an issue with SageMaker.",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-20T13:27:09.004Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello, I am checking the versions you mentioned and in my notebook instance , using conda_tensorflow2_p38 I get Tensorflow version 2.5. Is it the same for you or have you upgraded it to tensorflow 2.7?: import tensorflow as tf print(tf.version) 2.5.0",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Call last Sagemaker Model in Batch Transform Jobs",
        "Question_creation_time":1655467756412,
        "Question_link":"https:\/\/repost.aws\/questions\/QUF0u2FxOyTqK8-9hQG40i7g\/call-last-sagemaker-model-in-batch-transform-jobs",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Lambda",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":93,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi Dears,\n\nHope this message finds you well\n\nI have a sagemaker model, buit by on demand notebook. I have been used batch transform jobs using lambda function, It take input inference json from s3 to create batch transform job and have finally predictions.\n\nThe question how can I make lambda to use last trained model automaticity ? model_name = 'forecasting-deepar-2022-05-20-22-23-20-225',\n\nLambda code :\n\nif 'input_data_4' in file:\n\n            def batch_transform():\n                transformer = Transformer(\n                    model_name = 'forecasting-deepar-2022-05-20-22-23-20-225',\n                    instance_count = 2,\n                    instance_type = 'ml.m5.xlarge',\n                    assemble_with = 'Line',\n                    output_path = output_data_path,\n                    base_transform_job_name ='daily-output-predictions-to-s3',\n                    #sagemaker_session = sagemaker.session.Session,\n                    accept = 'application\/jsonlines')\n                transformer.transform(data = input_data_path, \n                                    content_type = 'application\/jsonlines', \n                                    split_type = 'Line',\n                                    wait=False, \n                                    logs=True)\n                    #Waits for the Pipeline Transform Job to finish.\n                print('Batch Transform Job Created successfully!')\n            batch_transform()\n\n\nThanks Basem",
        "Answers":[
            {
                "Answer_creation_date":"2022-06-23T08:04:25.907Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Basem,\n\nIf I understood correctly you'd like your Lambda function to automatically choose the latest SageMaker model when it runs, instead of hard-coding the model name.\n\nAlthough you could do this simply with boto3.client(\"sagemaker\").list_models(...) (which can sort by creation time), I would not recommend it. The reason is that in general this lists all models present in SageMaker - which might include some for different use cases in future, even if you only have the one DeepAR forecasting use-case today. You'd have to manually filter after the API call.\n\nA better approach would probably be to register your forecasting models in SageMaker Model Registry - which will allow you to register different versions and track extra metadata like metrics and approval status for each version if you need.\n\nFirst (e.g. from your notebook) you can create a model package group to track your forecasting models.\nThen (when you create your SageMaker Model) you can register it as a new version in the group - via Model.register().\nAt the point you want to look up which model to use, you can then list_model_packages which can filter to your specific group of models, and also by approval status if you like.\n\nSo for example you could set the model package group name as a configuration environment variable for your Lambda function, and have the function dynamically look up the latest version to use from the group when needed.\n\nOf course there are also many more custom ways to do this such as creating an SSM Parameter to track the name of your current accepted model, or creating your own model registry using a data store like DynamoDB... But SageMaker Model Registry seems like the most purpose-built tool for the job here to me.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Pipeline with lambda step: unable to update\/upsert pipeline",
        "Question_creation_time":1655388150771,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0AGShN-8TqKCFzM5FBI-ig\/sage-maker-pipeline-with-lambda-step-unable-to-update-upsert-pipeline",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0,
        "Question_view_count":123,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi there,\n\nI have a SM pipeline which contains a lambda step.\n\nI have recently run into problems with the pipeline\/lambda function, everytime I call the pipeline.upsert() method, I receive the following error message(s):\n\nResourceConflictException: An error occurred (ResourceConflictException) when calling the UpdateFunctionCode operation: The operation cannot be performed at this time. An update is in progress for resource: arn:aws:lambda:eu-west-x:XXXXXXXXXXXX:function:evaluation-input-generator\n\nDuring handling of the above exception, another exception occurred: \n\nValueError: {'Message': 'The operation cannot be performed at this time. An update is in progress for resource: arn:aws:lambda:eu-west-X:XXXXXXXXXXXX:function:evaluation-input-generator', 'Code': 'ResourceConflictException'}\n\n\nFor some reasons, the lambda seems to be permanently stuck in An update is in progress for resource: ..., however, I suspect that this is not the case, I am able to call the lambda helper's update method for the function and it will return status 200.\n\nthis behaviour is observed in multiple SM pipelines, any help would be greatly appreciated!\n\nBest, Ruoy",
        "Answers":[
            {
                "Answer_creation_date":"2022-06-23T16:17:26.629Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello Ruoy,\n\nSince this issue is observed in multiple SM pipelines, I would request you to open a case with Premium support so that engineer can inspect lambda configuration and the cloudtrail events and get to the root cause of the issue.\n\nhttps:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"FSxLustre FileSystemInput in Sagemaker TrainingJob leads to: InternalServerError",
        "Question_creation_time":1655298419776,
        "Question_link":"https:\/\/repost.aws\/questions\/QUignDqPc7QqCIBvB3r4g3Vw\/f-sx-lustre-file-system-input-in-sagemaker-training-job-leads-to-internal-server-error",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon FSx for Lustre",
            "Amazon SageMaker",
            "Amazon SageMaker Model Training"
        ],
        "Question_upvote_count":0,
        "Question_view_count":79,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"We are submitting a Sagemaker Training job with Sagemaker SDK with a custom docker image. The job finishes successfully for EFS FileSystemInput or TrainingInput. Trying to use the FIleSystemInput with FSxLustre configuration leads to the training job dying during the Preparing the instances for training stage:\n\nInternalServerError: We encountered an internal error. Please try again.\n\n\nThis error is persistent upon re-submission.\n\nWhat we figured out until now:\n\nthe job errors before the training image is downloaded.\nspecifying an invalid mount point leads to a proper error: ClientError: Unable to mount file system: xxx directory path: yyy. Incorrect mount path. Please ensure the mount path specified exists on the filesystem.\nthe job finishes successfully when running locally with docker-compose (Estimator with instance_type=\"local\").\nwe can mount the FSx file system on an EC2 instance with the TrainingJob's VPC and security group.\n\nHow can we narrow the problem down further and get more information about the failure reason? Can you suggest likely problems that could cause this behavior?",
        "Answers":[
            {
                "Answer_creation_date":"2022-06-29T11:53:00.486Z",
                "Answer_upvote_count":0,
                "Answer_body":"InternalServerError means that was an unforeseen error, during training job orchestration, that wasn't mapped to a known user facing error.\nYou should create an AWS Support case to uncover the root cause.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Determining the \"right\" instance type running Jupyter notebook in Sagemaker when reading\/writing a huge parquet file?",
        "Question_creation_time":1655268194182,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvBSWuAqZSru0kQHeulQqLw\/determining-the-right-instance-type-running-jupyter-notebook-in-sagemaker-when-reading-writing-a-huge-parquet-file",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":57,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am unclear as o how to determine the \"right\" instance type running Jupyter notebook in Sagemaker. When reading\/writing a small size parquet file, no problem; but when I try to read\/write a huge parquet file, the program stops and gives an error, \"Job aborted due to stage failure: Task 21 in stage 33.0 failed 1 times, most recent failure: Lost task 21.0 in stage 33.0 (TID 1755, localhost, executor driver\" I would appreciate any insights please... thanks.",
        "Answers":[
            {
                "Answer_creation_date":"2022-06-30T08:40:43.692Z",
                "Answer_upvote_count":0,
                "Answer_body":"For notebook instance it's mostly trial-and-error, at least for now. Once your model is ready to be deployed, there is the SageMaker Inference Recommender that can do automated load testing and give you recommendation on the instance size.\n\nIt's hard to give a recommendation on the notebook instance because you might test a 100MB dataset today, but choose to go with a 500GB dataset tomorrow, so the recommendations are no longer valid.\n\nYou might want to try experimenting with a smaller dataset sampled from the original big dataset, once you are confident with the model training code, use distributed training to run it on the complete big dataset.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Deploy YOLOv5 in sagemaker - ModelError: InvokeEndpoint operation: Received server error (0)",
        "Question_creation_time":1655212102106,
        "Question_link":"https:\/\/repost.aws\/questions\/QULAis68RtShua1Wg8A5EFXg\/deploy-yol-ov-5-in-sagemaker-model-error-invoke-endpoint-operation-received-server-error-0",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Command Line Interface",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":63,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm trying to deploy custom trained Yolov5 model in Sagemaker for inference. (Note : The model was not trained in sagemaker).\n\nFollowed this doc for deploying the model and inference script - Sagemaker docs\n\nModelError                                Traceback (most recent call last)\n<ipython-input-7-063ca701eab7> in <module>\n----> 1 result1=predictor.predict(\"FILE0032.JPG\")\n      2 print(result1)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model, target_variant, inference_id)\n    159             data, initial_args, target_model, target_variant, inference_id\n    160         )\n--> 161         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n    162         return self._handle_response(response)\n    163 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    399                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    400             # The \"self\" in this scope is referring to the BaseClient.\n--> 401             return self._make_api_call(operation_name, kwargs)\n    402 \n    403         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    729             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n    730             error_class = self.exceptions.from_code(error_code)\n--> 731             raise error_class(parsed_response, operation_name)\n    732         else:\n    733             return parsed_response\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https:\/\/ap-south-1.console.aws.amazon.com\/cloudwatch\/home?region=ap-south-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/pytorch-inference-2022-06-14-11-58-04-086 in account 772044684908 for more information.\n\n\nAfter researching about InvokeEndpoint, tried this\n\nimport boto3\n\nsagemaker_runtime = boto3.client(\"sagemaker-runtime\", region_name='ap-south-1')\nendpoint_name='pytorch-inference-2022-06-14-11-58-04-086'\nresponse = sagemaker_runtime.invoke_endpoint(\n                            EndpointName=endpoint_name, \n                            Body=bytes('{\"features\": [\"This is great!\"]}', 'utf-8') # Replace with your own data.\n                            )\nprint(response['Body'].read().decode('utf-8'))\n\n\nBut this didn't help as well,\n\ndetailed output :\n\nReadTimeoutError                          Traceback (most recent call last)\n<ipython-input-8-b5ca204734c4> in <module>\n     12 response = sagemaker_runtime.invoke_endpoint(\n     13                             EndpointName=endpoint_name,\n---> 14                             Body=bytes('{\"features\": [\"This is great!\"]}', 'utf-8') # Replace with your own data.\n     15                             )\n     16 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    399                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    400             # The \"self\" in this scope is referring to the BaseClient.\n--> 401             return self._make_api_call(operation_name, kwargs)\n    402 \n    403         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    716             apply_request_checksum(request_dict)\n    717             http, parsed_response = self._make_request(\n--> 718                 operation_model, request_dict, request_context)\n    719 \n    720         self.meta.events.emit(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_request(self, operation_model, request_dict, request_context)\n    735     def _make_request(self, operation_model, request_dict, request_context):\n    736         try:\n--> 737             return self._endpoint.make_request(operation_model, request_dict)\n    738         except Exception as e:\n    739             self.meta.events.emit(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in make_request(self, operation_model, request_dict)\n    105         logger.debug(\"Making request for %s with params: %s\",\n    106                      operation_model, request_dict)\n--> 107         return self._send_request(request_dict, operation_model)\n    108 \n    109     def create_request(self, params, operation_model=None):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _send_request(self, request_dict, operation_model)\n    182             request, operation_model, context)\n    183         while self._needs_retry(attempts, operation_model, request_dict,\n--> 184                                 success_response, exception):\n    185             attempts += 1\n    186             self._update_retries_context(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _needs_retry(self, attempts, operation_model, request_dict, response, caught_exception)\n    306             event_name, response=response, endpoint=self,\n    307             operation=operation_model, attempts=attempts,\n--> 308             caught_exception=caught_exception, request_dict=request_dict)\n    309         handler_response = first_non_none_response(responses)\n    310         if handler_response is None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in emit(self, event_name, **kwargs)\n    356     def emit(self, event_name, **kwargs):\n    357         aliased_event_name = self._alias_event_name(event_name)\n--> 358         return self._emitter.emit(aliased_event_name, **kwargs)\n    359 \n    360     def emit_until_response(self, event_name, **kwargs):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in emit(self, event_name, **kwargs)\n    227                  handlers.\n    228         \"\"\"\n--> 229         return self._emit(event_name, kwargs)\n    230 \n    231     def emit_until_response(self, event_name, **kwargs):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/hooks.py in _emit(self, event_name, kwargs, stop_on_response)\n    210         for handler in handlers_to_call:\n    211             logger.debug('Event %s: calling handler %s', event_name, handler)\n--> 212             response = handler(**kwargs)\n    213             responses.append((handler, response))\n    214             if stop_on_response and response is not None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempts, response, caught_exception, **kwargs)\n    192             checker_kwargs.update({'retries_context': retries_context})\n    193 \n--> 194         if self._checker(**checker_kwargs):\n    195             result = self._action(attempts=attempts)\n    196             logger.debug(\"Retry needed, action of: %s\", result)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception, retries_context)\n    266 \n    267         should_retry = self._should_retry(attempt_number, response,\n--> 268                                           caught_exception)\n    269         if should_retry:\n    270             if attempt_number >= self._max_attempts:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in _should_retry(self, attempt_number, response, caught_exception)\n    292             # If we've exceeded the max attempts we just let the exception\n    293             # propogate if one has occurred.\n--> 294             return self._checker(attempt_number, response, caught_exception)\n    295 \n    296 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    332         for checker in self._checkers:\n    333             checker_response = checker(attempt_number, response,\n--> 334                                        caught_exception)\n    335             if checker_response:\n    336                 return checker_response\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in __call__(self, attempt_number, response, caught_exception)\n    232         elif caught_exception is not None:\n    233             return self._check_caught_exception(\n--> 234                 attempt_number, caught_exception)\n    235         else:\n    236             raise ValueError(\"Both response and caught_exception are None.\")\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/retryhandler.py in _check_caught_exception(self, attempt_number, caught_exception)\n    374         # the MaxAttemptsDecorator is not interested in retrying the exception\n    375         # then this exception just propogates out past the retry code.\n--> 376         raise caught_exception\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _do_get_response(self, request, operation_model, context)\n    247             http_response = first_non_none_response(responses)\n    248             if http_response is None:\n--> 249                 http_response = self._send(request)\n    250         except HTTPClientError as e:\n    251             return (None, e)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/endpoint.py in _send(self, request)\n    319 \n    320     def _send(self, request):\n--> 321         return self.http_session.send(request)\n    322 \n    323 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/httpsession.py in send(self, request)\n    449             raise ConnectTimeoutError(endpoint_url=request.url, error=e)\n    450         except URLLib3ReadTimeoutError as e:\n--> 451             raise ReadTimeoutError(endpoint_url=request.url, error=e)\n    452         except ProtocolError as e:\n    453             raise ConnectionClosedError(\n\nReadTimeoutError: Read timeout on endpoint URL: \"https:\/\/runtime.sagemaker.ap-south-1.amazonaws.com\/endpoints\/pytorch-inference-2022-06-14-11-58-04-086\/invocations\"",
        "Answers":[
            {
                "Answer_creation_date":"2022-06-15T06:18:29.169Z",
                "Answer_upvote_count":0,
                "Answer_body":"Is there any error in your CloudWatch Logs that could point to the issue?\n\nI see you are sending a string named \"FILE0032.JPG\". The .predict function will make a prediction to the endpoint with the string \"FILE0032.JPG\" not the serialized file \"FILE0032.JPG\"\n\nKindly see how a YOLOv4 model is invoked here.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-06-15T00:29:48.450Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks for reply. There is no error in CloudWatch logs. (Pasted below) Sorry for the long description, i thought detailed info would be helpful.\n\n2022-06-15T11:15:21.349+05:30\tWarning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\tAllTraffic\/i-0ed6739cdaf7cf56a\n\n2022-06-15T11:15:21.349+05:30\tlog4j:WARN Continuable parsing error 2 and column 16\tAllTraffic\/i-0ed6739cdaf7cf56a\n\n2022-06-15T11:15:21.349+05:30\tlog4j:WARN Document root element \"Configuration\", must match DOCTYPE root \"null\".\tAllTraffic\/i-0ed6739cdaf7cf56a\n\n2022-06-15T11:15:21.349+05:30\tlog4j:WARN Continuable parsing error 2 and column 16\tAllTraffic\/i-0ed6739cdaf7cf56a\n\n2022-06-15T11:15:21.349+05:30\tlog4j:WARN Document is invalid: no grammar found.\tAllTraffic\/i-0ed6739cdaf7cf56a\n\n2022-06-15T11:15:21.349+05:30\tlog4j:ERROR DOM element is - not a <log4j:configuration> element.\tAllTraffic\/i-0ed6739cdaf7cf56a\n\n2022-06-15T11:15:21.349+05:30\tlog4j:WARN No appenders could be found for logger (io.netty.util.internal.PlatformDependent0).\tAllTraffic\/i-0ed6739cdaf7cf56a\n\n2022-06-15T11:15:21.349+05:30\tlog4j:WARN Please initialize the log4j system properly.\tAllTraffic\/i-0ed6739cdaf7cf56a\n\n2022-06-15T11:15:21.599+05:30\tlog4j:WARN See http:\/\/logging.apache.org\/log4j\/1.2\/faq.html#noconfig for more info.\tAllTraffic\/i-0ed6739cdaf7cf56a\n\n2022-06-15T11:15:27.349+05:30\tModel server started.\n\n\nI tried this example, it says \"An entry_point script isn\u2019t necessary and can be a blank file. The environment variables in the env parameter are also optional\" in the tutorial But when i tried it, it threw this error\n\n---------------------------------------------------------------------------\nModelError                                Traceback (most recent call last)\n<ipython-input-25-b706a4fea979> in <module>\n     13 for i in range(iters):\n     14     t0 = time.time()\n---> 15     response = client.invoke_endpoint(EndpointName=optimized_predictor.endpoint_name, Body=body, ContentType=content_type)\n     16     t1 = time.time()\n     17     #convert to millis\n\n~\/anaconda3\/envs\/pytorch_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    399                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    400             # The \"self\" in this scope is referring to the BaseClient.\n--> 401             return self._make_api_call(operation_name, kwargs)\n    402 \n    403         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/pytorch_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    729             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n    730             error_class = self.exceptions.from_code(error_code)\n--> 731             raise error_class(parsed_response, operation_name)\n    732         else:\n    733             return parsed_response\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary with message \"Content type applicatoin\/x-image is not supported by this framework.\n\n            Please implement input_fn to to deserialize the request data or an output_fn to\n            serialize the response. For more information, see the SageMaker Python SDK README.\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.6\/site-packages\/sagemaker_inference\/decoder.py\", line 106, in decode\n    decoder = _decoder_map[content_type]\nKeyError: 'applicatoin\/x-image'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py\", line 128, in transform\n    result = self._transform_fn(self._model, input_data, content_type, accept)\n  File \"\/usr\/local\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py\", line 233, in _default_transform_fn\n    data = self._input_fn(input_data, content_type)\n  File \"\/usr\/local\/lib\/python3.6\/site-packages\/sagemaker_pytorch_serving_container\/default_inference_handler.py\", line 111, in default_input_fn\n    np_array = decoder.decode(input_data, content_type)\n  File \"\/usr\/local\/lib\/python3.6\/site-packages\/sagemaker_inference\/decoder.py\", line 109, in decode\n    raise errors.UnsupportedFormatError(content_type)\nsagemaker_inference.errors.UnsupportedFormatError: Content type applicatoin\/x-image is not supported by this framework.\n\n            Please implement input_fn to to deserialize the request data or an output_fn to\n            serialize the response. For more information, see the SageMaker Python SDK README.\n\". See https:\/\/ap-south-1.console.aws.amazon.com\/cloudwatch\/home?region=ap-south-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-inference-pytorch-ml-c5-2022-06-15-05-44-12-970 in account 772044684908 for more information.\n\n\nFYI,\n\ntorch.__version__ 1.6.0 kernel conda_pytorch_p36 (Same steps followed as mentioned in the tutorial)\n\nVery confused on how to proceed after this? Why SageMaker is this much complex? Any kind of help would be appreciated. Thanks Marc.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Extending Docker image for SageMaker Inference",
        "Question_creation_time":1655198456555,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxESyB86cSMCN3dOUMyi4cw\/extending-docker-image-for-sage-maker-inference",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":217,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm trying to create my own Docker image for use with SageMaker Batch Transform by extending an existing one. Following the documentation at https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/prebuilt-containers-extend.html, I have created the following to run Detectron 2:\n\nFROM 763104351884.dkr.ecr.eu-west-2.amazonaws.com\/pytorch-inference:1.10.2-gpu-py38-cu113-ubuntu20.04-sagemaker\n\n############# Installing latest builds ############\nRUN pip install --upgrade torch==1.10.2+cu113 torchvision==0.11.3+cu113 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\n\nENV FORCE_CUDA=\"1\"\n# Build D2 only for Turing (G4) and Volta (P3) architectures. Use P3 for batch transforms and G4 for inference on endpoints\nENV TORCH_CUDA_ARCH_LIST=\"Turing;Volta\"\n\n# Install Detectron2\nRUN pip install \\\n   --no-cache-dir pycocotools~=2.0.0 \\\n   --no-cache-dir https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu113\/torch1.10\/detectron2-0.6%2Bcu113-cp38-cp38-linux_x86_64.whl\n   \n# Set a fixed model cache directory. Detectron2 requirement\nENV FVCORE_CACHE=\"\/tmp\"\n\n############# SageMaker section ##############\n\nENV PATH=\"\/opt\/ml\/code:${PATH}\"\n\nCOPY inference.py \/opt\/ml\/code\/inference.py\n\nENV SAGEMAKER_SUBMIT_DIRECTORY \/opt\/ml\/code\nENV SAGEMAKER_PROGRAM inference.py\n\n\nI then create a model (create-model) with this image using the following configuration:\n\n{\n\"ExecutionRoleArn\": \"arn:aws:iam::[redacted]:role\/model-role\",\n\"ModelName\": \"model-test\",\n\"PrimaryContainer\": { \n  \"Environment\": {\n    \"SAGEMAKER_PROGRAM\": \"inference.py\",\n    \"SAGEMAKER_SUBMIT_DIRECTORY\": \"\/opt\/ml\/code\",\n    \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\",\n    \"SAGEMAKER_REGION\": \"eu-west-2\",\n    \"MMS_DEFAULT_RESPONSE_TIMEOUT\": \"500\"\n   },\n  \"Image\": \"[redacted].dkr.ecr.eu-west-2.amazonaws.com\/my-image:latest\",\n  \"ModelDataUrl\": \"s3:\/\/[redacted]\/training\/output\/model.tar.gz\"\n}\n}\n\n\nAnd submit a batch transform job (create-transform-job) using the following configuration:\n\n{\n\"MaxPayloadInMB\": 16,\n\"ModelName\": \"model-test\",\n\"TransformInput\": { \n    \"ContentType\": \"application\/x-image\",\n    \"DataSource\": { \n      \"S3DataSource\": { \n          \"S3DataType\": \"ManifestFile\",\n          \"S3Uri\": \"s3:\/\/[redacted]\/manifests\/input.manifest\"\n      }\n    }\n},\n\"TransformJobName\": \"transform-test\",\n\"TransformOutput\": { \n    \"S3OutputPath\": \"s3:\/\/[redacted]\/predictions\/\"\n},\n\"TransformResources\": { \n    \"InstanceCount\": 1,\n    \"InstanceType\": \"ml.m5.large\"\n}\n}\n\n\nBoth of the above commands submit fine, but the transform job doesn't complete. When I look in the logs, the errors I'm getting seem to indicate that it's not using my inference script (inference.py, specified above) but is instead using the default script (default_pytorch_inference_handler.py) and therefore can't find the model.\n\nWhat am I missing so that it uses my inference script instead, and hence my model?",
        "Answers":[
            {
                "Answer_creation_date":"2022-06-16T20:50:39.142Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nThank you for using AWS SageMaker.\n\nIt is difficult to identify why this behavior is observed without any logs for the mentioned task under your account. Looking at the above snippet shared, I was able to identify that the extending docker image used is based on GPU instance \"pytorch-inference:1.10.2-gpu-py38-cu113-ubuntu20.04-sagemaker \" and the Batch transform job that was created was using CPU instances i.e (\"InstanceType\": \"ml.m5.large\").\n\nI'd recommend to fix that configuration and try running the batch transform job once again. If you still observe similar issue, I'd recommend you to reach out to AWS Support for further investigation of the issue along with all the details and logs as sharing logs is not recommended to share on this platform.\n\nOpen a support case with AWS using the link: https:\/\/console.aws.amazon.com\/support\/home?#\/case\/create",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Canvas Lifecycle Config",
        "Question_creation_time":1654810759004,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0LTBVP38RoSVvEmmmyc7xw\/sagemaker-canvas-lifecycle-config",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Canvas"
        ],
        "Question_upvote_count":0,
        "Question_view_count":90,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"For our Sagemaker Notebook\/studio deployments we have used lifecycle configs to turn off the app when inactive. Is this possible to apply to Canvas? Is this a supported function? is there an alternative?",
        "Answers":[
            {
                "Answer_creation_date":"2022-06-13T22:08:56.380Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello, according to the service FAQ the approach to stop the session is by clicking on the Log Out button on the Canvas app",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Studio default server failing",
        "Question_creation_time":1654597460713,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJhSBFYdSQ8aGHNA3DjMk-A\/sagemaker-studio-default-server-failing",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":54,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello, I am trying to start Amazon SageMaker Studio today but without success. The default server is failing to start. On CloudWatch I only see SIGKILL messages as errors:\n\n2022-06-07T10:55:01.523+02:00\t2022-06-07 08:55:00,466 WARN killing 'jupyterlabserver' (11) with SIGKILL\n2022-06-07T10:55:04.524+02:00\t2022-06-07 08:55:01,470 INFO waiting for jupyterlabserver-listener, jupyterlabserver to die\n\n\nOn the Apps section I see the default server in status \"Failed\", and I cannot do anything, not even delete it.\n\nI cannot use the service right now. Is it maybe related to the new release of Jupyter Lab v3.0?\n\nThanks",
        "Answers":[
            {
                "Answer_creation_date":"2022-06-08T07:33:16.014Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, Could you try the steps from this documentation? https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tasks-update-studio.html You mention that you cannot delete it. How does the UI look like? Is the Detele button greyed out? Could you try shutting down all the services in the Studio and create a new one?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-06-14T09:59:37.829Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello. Just as an update for this question, the final solution was to delete the entire SageMaker domain suing the AWS cli and recreate it from scratch. Having done that the problem is no more there, so I encountered by chance some unexpected AWS internal error that stuck my entire domain. Best",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"GC overhead limit exceeded",
        "Question_creation_time":1654390632353,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDc_WeDqcTjitN1bkIJRosg\/gc-overhead-limit-exceeded",
        "Question_topic":[
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "High Performance Compute",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":87,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a modest size dataset, and I am running Jupyter Notebook in Sagemaker (instance type ml.c5.xlarge with 200G instance size). I receive the error message \" GC overhead limit exceeded\" Everything ran fine with small data size. BTW, I need to go through the dataframe one row at a time using df.collect(), which seems t be an expensive operation... Would you suggest another way of accomplishing this? I would appreciate your kind help.",
        "Answers":[
            {
                "Answer_creation_date":"2022-06-08T07:40:20.941Z",
                "Answer_upvote_count":0,
                "Answer_body":"The GC overhead limit exceeded error indicates that the JVM spent a lot of time on garbage collection but recovered very little memory, so it throws this error to let you know that your program is not making much progress but wasting time on doing useless garbage collection task. Iterating through the dataframe might be the problem, because you might be creating a lot of temporary objects when you go through each line, and they couldn't be garbage collected. What is the framework that you are using? And what are you trying to do by going through the dataframe row-by-row? Maybe you can think about processing multiple lines in a batch? For example using some vectorization or matrix operation as georgios_s suggested in the comment.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Studio - Jupyter proxy function does not work",
        "Question_creation_time":1654087666657,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuptYf-6wTNmhZaNCJWRhRA\/sage-maker-studio-jupyter-proxy-function-does-not-work",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":44,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to run TensorBoard 2.9.0 in SageMaker Studio, as described here.\n\nHowever, after launching TensorBoard, when I navigate to the proxy URL https:\/\/<DOMAIN>.studio.<REGION>.sagemaker.aws\/jupyter\/default\/proxy\/6006\/, I get error 500.\n\nI am using the \"PyTorch 1.10 Python 3.8 CPU Optimized\" image. Any suggestions?\n\nI have also tried with the new JupyterLab 3 server version, but same result.",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-01T14:27:14.708Z",
                "Answer_upvote_count":0,
                "Answer_body":"I managed to reproduce the error 500 by not starting Tensorboard. So, please, make sure you executed in Studio's terminal the command to launch Tensorboard first: tensorboard --logdir path\/to\/your\/logs\n\nYou need to see something like:\n\nTensorFlow installation not found - running with reduced feature set.\n\nNOTE: Using experimental fast data loading logic. To disable, pass\n    \"--load_fast=false\" and report issues on GitHub. More details:\n    https:\/\/github.com\/tensorflow\/tensorboard\/issues\/4784\n\nServing TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\nTensorBoard 2.9.1 at http:\/\/localhost:6006\/ (Press CTRL+C to quit)\n\n\n\nThen, after that you can open a new tab and try to open it.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Pipelines - Batch Transform job using generated predictions as input for the model",
        "Question_creation_time":1653948738611,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3dJWSSmiTQaBwCCY1JQsVQ\/sagemaker-pipelines-batch-transform-job-using-generated-predictions-as-input-for-the-model",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0,
        "Question_view_count":253,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi all! So, we're trying to implement a very simple Sagemaker Pipeline with 3 steps:\n\nETL: for now it only runs a simple query\nBatch transform: uses the ETL's result and generates predictions with a batch transform job\nReport: generates an HTML report\n\nThe thing is, when running the batch transform job alone in the Pipeline, everything runs OK. But when trying to run all the steps in a Pipeline, the batch transform job fails, and what we have seen in the logs is that the job takes the dataset which was generated in the ETL step, generates the predictions and saves them correctly in S3 (this is where we would expect the job to stop) but then it resends those predictions to the endpoint, as if they were a new input, and so the step fails as the model receives an array of 1 column thus mismatching the number of features which it was trained with.\n\nThere's not much info out there on this, and Sagemaker is painfully hard to debug. Has anyone experienced anything like this?\n\nOur model and transformer code:\n\nmodel = XGBoostModel(\n    model_data=f\"s3:\/\/{BUCKET}\/{MODEL_ARTIFACTS_PATH}\/artifacts.gzip\",\n    role=get_execution_role(),\n    entry_point=\"predict.py\",\n    framework_version=\"1.3-1\",\n)\n\ntransformer = model.transformer(\n    instance_count=1,\n    instance_type=\"ml.m5.large\",\n    output_path=f\"s3:\/\/{BUCKET}\/{PREDICTIONS_PATH}\/\",\n    accept=\"text\/csv\",\n)\n\nstep = TransformStep(\n    name=\"Batch\",\n    transformer=transformer,\n    inputs=TransformInput(\n        data=etl_step.properties.ProcessingOutputConfig.Outputs[\n            \"dataset\"\n        ].S3Output.S3Uri,\n        content_type=\"text\/csv\",\n        split_type=\"Line\",\n    ),\n    depends_on=[etl_step],\n)\n\nAnd our inference script:\n\ndef input_fn(request_body, content_type):\n    return pd.read_csv(StringIO(request_body), header=None).values\n\n\ndef predict_fn(input_obj, model):\n    \"\"\"\n    Function which takes the result of input_fn and generates\n    predictions.\n    \"\"\"\n    return model.predict_proba(input_obj)[:, 1]\n\n\ndef output_fn(predictions, content_type):\n    return \",\".join(str(pred) for pred in predictions)",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-31T08:09:54.111Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nThe issue you describe could happen if the prediction file is written in the same location where the input files are, and thus triggering one more round of prediction.\n\nCan you check that the\n\netl_step.properties.ProcessingOutputConfig.Outputs[\n            \"dataset\"\n        ].S3Output.S3Uri\n\n\nand\n\nf\"s3:\/\/{BUCKET}\/{PREDICTIONS_PATH}\/\"\n\n\npoint a different path in your s3 bucket?\n\nDid this work?\n\nThank you",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"how to log error\/messages in while running a sagemaker batch transform job?",
        "Question_creation_time":1653699481978,
        "Question_link":"https:\/\/repost.aws\/questions\/QUNirOT1cMSfig9ANtgmZMpg\/how-to-log-error-messages-in-while-running-a-sagemaker-batch-transform-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":58,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"i'm using a hugging face model and a container to create a batch transform job in sagemaker. i have a custom inference code and in the output_fn function i'm returning json_dumps(prediction). I'm using print(prediction) to see, if i can see it in the cloudwatch logs to find out type and what prediction is. how can log these messages . Also, the inference output i get is in the form below., i'm not sure why is it not a json object in each line instead it has square brackets. I want to use the filter to match the input and output in the batch job. I'm not sure how the output should look like , because i'm trying to associate input with output by using dataprocessing config as below. but i get an error. the documenation has example of csv not json. what should the output look like so that i can associate the input with output when they both are in json format.\n\n  \"DataProcessing\": {\n        \"JoinSource\": \"Input\"\n    },\n\n[ output text 1 ]\n[output text 2 ]\n\n# Serialize the prediction result into the desired response content type\ndef output_fn(prediction, accept=JSON_CONTENT_TYPE):\n    logger.info(\"Serializing the generated output.\")\n    if accept == JSON_CONTENT_TYPE:\n        output = json.dumps(prediction)\n        return output, accept\n    raise Exception(\"Requested unsupported ContentType in Accept: {}\".format(accept))",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"SageMaker Multi Model endpoint creation fails while creating for model built on container sagemaker-scikit-learn:0.23-1-cpu-py3",
        "Question_creation_time":1653574428837,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHZiKPwmxRyy0C0Nc0ONwuQ\/sage-maker-multi-model-endpoint-creation-fails-while-creating-for-model-built-on-container-sagemaker-scikit-learn-0-23-1-cpu-py-3",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0,
        "Question_view_count":98,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am working on a use-case where I am using SageMaker multi-model endpoint for model inference and the models are trained using Databricks MLFlow platform. When I tried deploying a model trained from Databricks MLFlow platform on a single endpoint on SageMaker then it worked fine but the creation of multi-model endpoint for 'sagemaker-scikit-learn:0.23-1-cpu-py3' container is failed with the following error:\n\nCode Snippet::>> name = \"sample-mme\"\n\nsagemaker_client = boto3.client('sagemaker')\n\nmodel_path = \"s3:\/\/test-bucket\/multi-models\"\n\nexecution_role_arn = \"IAM:\/\/sample-role\"\n\nBASE_IMAGE = image_uris.retrieve( region=region, framework=\"sklearn\",version='0.23-1',image_scope='inference' )\n\ncontainer = { 'Image': BASE_IMAGE, 'ModelDataUrl': model_path, 'Mode': 'MultiModel', 'MultiModelConfig': { 'ModelCacheSetting': 'Enabled' } }\n\nmodel_response = sagemaker_client.create_model( ModelName=name, ExecutionRoleArn=execution_role_arn, Containers=[container] )\n\nconfig_response = sagemaker_client.create_endpoint_config( EndpointConfigName=f'{name}-config', ProductionVariants=[ { 'InstanceType': instance_type, 'InitialInstanceCount': instance_count, 'InitialVariantWeight': 1, 'ModelName': name, 'VariantName': 'AllTraffic' } ] )\n\nresponse = sagemaker_client.create_endpoint( EndpointName=f'{name}-endpoint', EndpointConfigName=f'{name}-config' )\n\nEndpoint creation is taking a lot if time and failing with the following error message :\n\nsagemaker_containers._errors.ImportModuleError: 'NoneType' object has no attribute 'startswith'\n\nPlease provide me with some help to fix this.\n\nAlso, my understanding is that I can train a model on the DataBricks MLFlow platform using sklearn libraries, and then I can store model artifacts \"model.tar.gz\" under the s3 directory for storing all multi-models. Now I can create a multi-model endpoint in SageMaker using the same s3 directory as the model path and using the above code. Once the endpoint is ready, I can do inference by providing the target model. Please let me know if my understanding is correct and share any relevant documents to follow for my use case.",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-31T20:01:20.662Z",
                "Answer_upvote_count":0,
                "Answer_body":"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/multi_model_sklearn_home_value\/sklearn_multi_model_endpoint_home_value.ipynb https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/advanced_functionality\/kmeans_bring_your_own_model\/kmeans_bring_your_own_model.html\n\nThe above notebook shows how to seed a pre-existing model in an already built container. This functionality could be replicated with other Amazon SageMaker Algorithms, as well as the TensorFlow and MXNet containers. Although this is certainly an easy method to bring your own model, it is not likely to provide the flexibility of a bringing your own scoring container. Please refer to other example notebooks which show how to dockerize your own training and scoring container which could be modified appropriately to your use case.\n\nIn general it is recommended to Bring your Own docker container along with your custom model, SageMaker Inference Toolkit is a library that bootstraps MMS in a way that is compatible with SageMaker multi-model endpoints, while still allowing you to tweak important performance parameters, such as the number of workers per model. The inference container in this example uses the Inference Toolkit to start MMS which can be seen in the container\/dockerd-entrypoint.py file.\n\nhttps:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.html#Upload-model-artifacts-to-S3\n\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\n\nIn order to deep-dive further I would request you to open a support ticket with the aws premium support for further investigation in to the cloudwatch logs and the specific resource of the endpoint.\n\nIf you still have difficulties, I recommend to cut a support case and provide more detail about your account information and cloudwatch logs. Due to security reason, we cannot discuss account specific issue in the public posts.\n\nThank you",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker instances keep awakening and charge the credit",
        "Question_creation_time":1653535822137,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjCMOSHaPR4WwWP1SoFzzng\/sagemaker-instances-keep-awakening-and-charge-the-credit",
        "Question_topic":[
            "Cloud Financial Management",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Cost and Usage Report",
            "Amazon SageMaker",
            "Amazon SageMaker Data Wrangler"
        ],
        "Question_upvote_count":0,
        "Question_view_count":306,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have tried Data Wrangler in Sagemaker last month and close the service. A few weeks later I have noticed the credit was charge $1 every hour and just realized that the Data Wranger auto-save the flow every minute. So, I deleted the unsaved flow and shut down all the services and instances according to advice on these two links :\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-lab-use-shutdown.html\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-cleanup.html\n\nThen, I left the Sagemaker untouched for the whole month of May, and just got back to the console yesterday. This is what I found out for May's bill:\n\nAmazon SageMaker RunInstance $531.74\nDetail\tUsage\tTotal\n$0.00 for Host:ml.m5.xlarge per hour under monthly free tier\t125.000 Hrs\t$0.00\n$0.00 for Notebk:ml.t2.medium per hour under monthly free tier\t107.056 Hrs\t$0.00\n$0.00 per Data Wrangler Interactive ml.m5.4xlarge hour under monthly free tier\t25.000 Hrs\t$0.00\n$0.23 per Hosting ml.m5.xlarge hour in US East (N. Virginia)\t88.997 Hrs\t$20.47\n$0.922 per Data Wrangler Interactive ml.m5.4xlarge hour in US East (N. Virginia)\t554.521 Hrs\t$511.27\n\nSo, with another attempt, I installed an extension to automatically shut down idle kernels and set the limit to 10 min from advice here: https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-costs-by-automatically-shutting-down-idle-resources-within-amazon-sagemaker-studio\/ Checked the cost in usage report, it turns out that the service was shut down after installing the extension but then it revoked itself after 5 hours later (during my sleep time). There's still cost from Studio although with less charge than previous one.\n\nService\tOperation\tUsageType\tStartTime\tEndTime\tUsageValue\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/24\/2022 23:00\t5\/25\/2022 0:00\t1\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/25\/2022 0:00\t5\/25\/2022 1:00\t1\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/25\/2022 1:00\t5\/25\/2022 2:00\t1\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/25\/2022 2:00\t5\/25\/2022 3:00\t0.76484417\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/25\/2022 8:00\t5\/25\/2022 9:00\t0.36636722\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/25\/2022 9:00\t5\/25\/2022 10:00\t0.38959556\n\nDuring this time, I'm sure that there're no running instances, running apps, kernel sessions or terminal sessions. I even deleted the user profile. Last thing I haven't tried is to set up scheduled shutdown coz I think the services should not cause difficulty to our life that much. Any advice for any effective action to completely shutdown the Sagemaker instance? Thanks.",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-26T16:33:36.084Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, you can shut down SageMaker Studio resources per the documentation here - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks-run-and-manage-shut-down.html (you've linked to Studio Lab documentation, so making sure). In addition, I see you've been charged for hosting and you've since deleted the endpoints, and don't see the hosting charges after deleting them.\n\nFor data wrangler, once you have the flow saved, you need to shut down the app (closing the window does not automatically shut down the app). Note that if you open the DW flow later, it does start a compute instance, which you'll then have to shut down.\n\nIf you've deleted the user profile (and associated apps), you shouldn't be seeing any more Studio charges for that user profile. If you still see the DW charges (and there's no other user profile), please cut a ticket to support for further investigation.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Which GPU instances are supported by the sagemaker algorithm forecasting-deepar?",
        "Question_creation_time":1653437621164,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0TwRR6KzRzuS5Xme3uMdEw\/which-gpu-instances-are-supported-by-the-sagemaker-algorithm-forecasting-deepar",
        "Question_topic":[
            "Machine Learning & AI",
            "Compute"
        ],
        "Question_tag":[
            "AWS Deep Learning Containers",
            "High Performance Compute",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":52,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I previously ran a hyperparameter tuning job for SageMaker DeepAR with the instance type ml.c5.18xlarge but it seems insufficient to complete the tuning job within the max_run time specified in my account. Now, having tried to use the accelerated GPU instance ml.g4dn.16xlarge, I am prompted with an error - \"Instance type ml.g4dn.16xlarge is not supported by algorithm forecasting-deepar.\"\n\nI cannot find any documentation that indicates the list of instance types supported by deepar. What GPU\/CPU instances have more compute capacity than ml.c5.18xlarge which I could leverage for my tuning job?\n\nIf there isn't, I would appreciate any recommendations as to how I could hasten the run time of the job. I require the tuning job to complete within the max run time of 432000 seconds. Thank you in advance!",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-25T15:09:21.707Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi, thanks for pointing this out. Indeed, all g4dn instances are currently not supported by the forecasting-deepar algorithm, but as you rightly point out, this is currently not documented. I will raise this with the service team to include in in the documentation.\n\nIn the meantime, you can try out the P3 instances instead - these are also powerful GPU instances and should help you speed up the training time.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker XGBoost Forecast Horizon Perameter",
        "Question_creation_time":1653415645056,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxwI3shNwReWGx-T3AEhtPg\/sagemaker-xg-boost-forecast-horizon-perameter",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Build & Train ML Models",
            "Amazon SageMaker",
            "Amazon SageMaker Model Training",
            "Amazon SageMaker Model Building"
        ],
        "Question_upvote_count":0,
        "Question_view_count":58,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Which XGBoost hypertuning perameter is used to set the forecast horizon?\n\nI do not see a parameter listed here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost_hyperparameters.html",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-24T18:53:35.017Z",
                "Answer_upvote_count":0,
                "Answer_body":"That's because you can XGBoost only for regression, classification (binary and multiclass), and ranking problems, but not for forecasts. See more info here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\n\nFor forecasting you could use, for example, DeepAR: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Textract to multi column pdf files",
        "Question_creation_time":1653356450043,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5ETLegQmRbeaA3sRyLox8g\/textract-to-multi-column-pdf-files",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon Textract",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":128,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am using the code below that I took from an example https:\/\/aws.amazon.com\/pt\/blogs\/machine-learning\/automatically-extract-text-and-structured-data-from-documents-with-amazon-textract\/, in the example it is used only for a case of 2 columns, in the code where there is division by 2, if my file has 4 columns for example, I just change that it works. But how to detect the amount of columns automatically or some way that I don't need this manual input anymore? In summary I want to use this code for cases of pdf files that have more than 2 columns, how to do it?\n\nimport boto3\n# Document\ns3BucketName = \"amazon-textract-public-content\"\ndocumentName = \"blogs\/two-column-image.jpg\"\n\n# Amazon Textract client\ntextract = boto3.client('textract')\n\n# Call Amazon Textract\nresponse = textract.detect_document_text(\n    Document={\n        'S3Object': {\n            'Bucket': s3BucketName,\n            'Name': documentName\n        }\n    })\n\n#print(response)\n\n# Detect columns and print lines\ncolumns = []\nlines = []\nfor item in response[\"Blocks\"]:\n      if item[\"BlockType\"] == \"LINE\":\n        column_found=False\n        for index, column in enumerate(columns):\n            bbox_left = item[\"Geometry\"][\"BoundingBox\"][\"Left\"]\n            bbox_right = item[\"Geometry\"][\"BoundingBox\"][\"Left\"] + item[\"Geometry\"][\"BoundingBox\"][\"Width\"]\n            bbox_centre = item[\"Geometry\"][\"BoundingBox\"][\"Left\"] + item[\"Geometry\"][\"BoundingBox\"][\"Width\"]\/2\n            column_centre = column['left'] + column['right']\/2\n\n            if (bbox_centre > column['left'] and bbox_centre < column['right']) or (column_centre > bbox_left and column_centre < bbox_right):\n                #Bbox appears inside the column\n                lines.append([index, item[\"Text\"]])\n                column_found=True\n                break\n        if not column_found:\n            columns.append({'left':item[\"Geometry\"][\"BoundingBox\"][\"Left\"], 'right':item[\"Geometry\"][\"BoundingBox\"][\"Left\"] + item[\"Geometry\"][\"BoundingBox\"][\"Width\"]})\n            lines.append([len(columns)-1, item[\"Text\"]])\n\nlines.sort(key=lambda x: x[0])\nfor line in lines:\n    print (line[1])",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-24T04:45:56.663Z",
                "Answer_upvote_count":0,
                "Answer_body":"You may like to try the Amazon Textract Response Parser for this, and note in particular that the JavaScript\/TypeScript library's getLineClustersInReadingOrder() implementation is very different from the Python library's getLinesInReadingOrder().\n\nFrom a very biased (author's) perspective I would argue that the JS library's current heuristic is better. You can see a couple of example images it's tested against in the code repository - and I'd suggest it's well worth trying out if you're able to consume components in JS or TS as well as Python.\n\nBut ultimately, all these methods are rule-based heuristics and none are perfect: Often what you gain in performance on some use cases, you lose in code maintainability and weird\/counter-intuitive errors on others. At the extreme, many complex layouts even challenge\/break the idea that there's \"one correct reading order\" for content on a page anyway - like posters or advertisements with very variable text.\n\nI'd suggest to go with the simplest method that works well enough for your actual documents, and also to revisit why you're trying to extract this columnar structure in the first place in case there are better options:\n\nIf you're just looking for a 1D text sequence to feed into a downstream NLP model, maybe revisit whether Textract Queries, Comprehend Native Document NER or other custom 2D layout-aware language models on SageMaker could solve that end task better?\nIf you're just looking for a transcript of the document, maybe you could present it to end users in 2D format (using the word positions & boxes returned by Textract) instead of trying to reduce to a single sequence?",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Asynchronous Endpoint Configuration",
        "Question_creation_time":1653000488230,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZNbZZQHhSl2RYUtLU8zpSQ\/sagemaker-asynchronous-endpoint-configuration",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Autopilot",
            "Amazon SageMaker Model Building",
            "Amazon SageMaker JumpStart",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0,
        "Question_view_count":80,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"We deployed a LighGBM Regression model and endpoint using Sagemaker Jumpstart. We have attempted to configure this endpoint as 'asynchronous' via the console. Receiving Error: ValidationException-Network Isolation is not supported when specifying an AsyncInferenceConfig.\n\nLooking at the model's network details the model has Enable Network Isolation set as 'True'. This was default output setting set by JumpStart.\n\nHow can we diasble Network Isolation to in order to make this endpoint asynchronous?",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-20T05:18:58.005Z",
                "Answer_upvote_count":1,
                "Answer_body":"Vanilla SageMaker \"Models\" (as opposed to versioned ModelPackages) are immutable in the API with no \"UpdateModel\" action... But I think you should be able to create a new Model copying the settings of the current one.\n\nI'd suggest to:\n\nUse DescribeModel (via boto3.client(\"sagemaker\").describe_model(), assuming you're using Python) to fetch all the parameters of the existing JumpStart model such as the S3 artifact location and other settings\nUse CreateModel (create_model()) to create a new model with same configuration but network isolation disabled\nUse your new model to try and deploy an async endpoint\n\nProbably you'd find the low-level boto3 SDK more intuitive for this task than the high-level sagemaker SDK's Model class - because the latter does some magic that makes typical build\/train\/deploy workflows easier but can be less natural for hacking around with existing model definitions. For example, creating an SMSDK Model object doesn't actually create a Model in the SageMaker API, because deployment instance type affects choice of container image so that gets deferred until a .deploy() call or similar later.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Error in Creating Project in SageMaker Studio",
        "Question_creation_time":1652920054013,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsvzAjgcJTzGrh0wfKRDvjQ\/error-in-creating-project-in-sage-maker-studio",
        "Question_topic":[
            "Machine Learning & AI",
            "Security, Identity, & Compliance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "IAM Policies"
        ],
        "Question_upvote_count":0,
        "Question_view_count":463,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I encountered following erorr when I tried to create a new project in SageMaker Studio. I tried to add AdminAccess to role\/service-role\/AmazonSageMakerServiceCatalogProductsLaunchRole, but still doesn't help. What could be the problem?\n\nError getting the details of Service Catalog Provisioning Parameters. Error message: ValidationException: Access denied while assuming the role arn:aws:iam::XXXXXXXXX:role\/service-role\/AmazonSageMakerServiceCatalogProductsLaunchRole. Args: {\"productId\":\"prod-xxxxxxxx\",\"provisioningArtifactId\":\"pa-xxxxxxxxx\",\"pathId\":\"lpv2-xxxxxxxxxx\"}\n\n\nprintscreen",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-19T16:32:30.379Z",
                "Answer_upvote_count":1,
                "Answer_body":"SageMaker assumes the AmazonSageMakerServiceCatalogProductsLaunchRole role to access and launch Projects. If you aren't using the AmazonSageMakerFullAccess policy for your Studio or user profile's execution role, make sure your Studio execution role has enough permissions to assume the AmazonSageMakerServiceCatalogProductsLaunchRole.\n\nYou don't need to add Admin access to the launch role, it has the minimum required permissions to launch a service catalog product, including assuming the AmazonSageMakerServiceCatalogProductsUseRole for running the project successfully.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Async Inference Endpoint: Linking input identifier, container log, output identifier?",
        "Question_creation_time":1652877072527,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPN-YPS0cTsWWi4oR3tFMgQ\/sagemaker-async-inference-endpoint-linking-input-identifier-container-log-output-identifier",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":100,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi, based on https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb I've created a docker image which contains the model code implemented in Python. This model image is deployed as async inference endpoint. Everything works well, the endpoint can be invoked via InvokeEndpointAsync, custom attributes are provided to the model code via \"X-Amzn-SageMaker-Custom-Attributes\" header, the input located on S3 is provided as http body. After succesful processing the result is uploaded to S3 with an automatically generated output filename (\"*.out\"). The container behavior is logged to CloudWatch via Python logging module, initialized as:\n\nimport logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger()\n\n\nHowever, the request arriving inside the container neither contains an \"X-Amzn-SageMaker-Inference-Id\" nor \"X-Amzn-SageMaker-InputLocation\" field (mentioned here https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpointAsync.html). Is there a way to access the inference_id\/output filename from inside the container? Or any other opportunity to link input filename\/container log\/output filename? Otherwise it's not possible to fully track and examine requests.\n\nThanks in advance for your advice!",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-26T19:40:23.482Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nThank you for using AWS Sagemaker.\n\nOn investigating the issue that you have shared with us. I got the opportunity to work with the internal team on this issue, and it was identified that at this moment Async Inference is not logging those details in the container.\n\nThat being said, I have raised a feature request with the service team on your behalf. While I am unable to comment on if\/when this feature may get released, I request you to keep an eye on our What's New and Blog pages for any new feature announcements.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"CPU + memory usage missing from SM Studio notebook toolbar",
        "Question_creation_time":1652797037698,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGQfGnTgqQcyNbWVb3U9V8Q\/cpu-memory-usage-missing-from-sm-studio-notebook-toolbar",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon CloudWatch"
        ],
        "Question_upvote_count":0,
        "Question_view_count":327,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I work in SM Studio, and I do not understand why CPU and memory usage do not appear in the notebook toolbar. These metrics should be there, at least given this description:\n\nhttps:\/\/docs.amazonaws.cn\/en_us\/sagemaker\/latest\/dg\/notebooks-menu.html\n\nWhen I open a notebook in SM Studio, I see the same toolbar but without CPU and memory usage listed. Moreover, I see 'cluster' before the kernel's name in my toolbar.\n\nHas anyone experienced sth similar? I assume an alternative for me would be to use CloudWatch.",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-17T14:39:13.413Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi, you should be able to see your CPU and Memory on the bottom toolbar, looks like Kernel: Idle | Instance MEM. You can click on that text to show the kernel and instance usage metrics.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"How to send own failure info in case of failed SageMaker Training Job?",
        "Question_creation_time":1652720071234,
        "Question_link":"https:\/\/repost.aws\/questions\/QUW-6fYh-kQy2bgc5PSkQG3Q\/how-to-send-own-failure-info-in-case-of-failed-sage-maker-training-job",
        "Question_topic":[
            "Machine Learning & AI",
            "Developer Tools",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Monitoring & Logging",
            "Amazon CloudWatch Logs",
            "Amazon SageMaker Model Training",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0,
        "Question_view_count":210,
        "Question_answer_count":4,
        "Question_has_accepted_answer":false,
        "Question_body":"Good day!\n\nMy main purpose:\n\nEasy way to collect information about different failure scenarios in SageMaker TrainingJob.\n\nWhat do I use currently?\n\nSagemaker SKLearn Estimator(TrainingJobs are inside)\n\nWhere will my model train?\n\nDifferent datasets. So, I need control and collect all information about all training processes and their final statuses on different datasets.\n\nWhich failure scenarios do I have?\n\nThere are plenty of them. I have create my own python Errors for them.\nFor example:\n\nThere are labels only for one class.\nToo small dataset(by my own criterions)\nMissing data for crucial columns\ne.t.c.\n\nWhere am I stuck?\n\nAfter failed training I can't get own errors from training job response. All of them are \"ExecuteUserScriptError\" I can't pass my own info in FailureReason or ErrorMessage(always it's empty). I see which error was raised in CloudWatchLogs and TrainingJobTraceback(from SagemakerNotebook). So, bad solution is parse all CloudWatchLogs in case of failure.\n\n**Question: How to provide my own ErrorMessage or FailureReason? **\n\nMay be I am digging in the wrong direction. Anyway, I need your advice. Thank you so much for possibility to ask an advice here)",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-18T02:06:09.805Z",
                "Answer_upvote_count":0,
                "Answer_body":"In: sklearn_estimator.latest_training_job.describe()['FailureReason']\n\nOut:\n\nTraceback (most recent call last):\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_trainer.py\", line 84, in train\n    entrypoint()\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py\", line 39, in main\n    train(environment.Environment())\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/training.py\", line 35, in train\n    runner_type=runner.ProcessRunnerType)\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/entry_point.py\", line 100, in run\n    wait, capture_error\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py\", line 291, in run\n    cwd=environment.code_dir,\n  File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_training\/process.py\", line 208, in check_error\n    info=extra_info,\nsagemaker_training.errors.ExecuteUserScriptError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"\"\nCommand \"\/miniconda3\/bin\/python train.py\"\n\nExecuteUserScriptErro```",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-05-18T09:51:37.844Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello, is the reported problem similar to this issue reported on SageMaker Python SDK project ?\n\nhttps:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1952",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-05-18T10:43:27.396Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, did you try writing to \/opt\/ml\/output\/failure as per the doc here?\n\nIt's worth mentioning that in the past there was a bug that overwrote this file in the base training toolkit that powers \"script mode\" containers. This got resolved at source per the linked issue, but I guess there's a chance older containers or frameworks which customize this tool could still be affected? So may be worth upgrading your framework version if you're using an older one.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-05-16T19:13:54.147Z",
                "Answer_upvote_count":0,
                "Answer_body":"This issue looks like related to my problem) There are 2 sagemaker-scikit-learn versions: 0.23-1, 0.20-0.\nSo, I use: 683313688378.dkr.ecr.us-east-1.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3 sagemaker version: 2.86.0 (tried to upgrade with pip in terminal to 2.90.0 , but there is the previous version 2.86 in notebooks) Python - 3.7\n\nHow can I understand which versions I should use to get away of ErrorMessage problem?",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to fix SageMaker training job error \"SM_CHANNEL_TRAIN\"?",
        "Question_creation_time":1652689524286,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwxIZtOn8Qg6EHGP_Ehi2mQ\/how-to-fix-sage-maker-training-job-error-sm-channel-train",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":46,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am building a ml workflow using step function following this. However, when I start the state machine, I got error\n\nAlgorithmError: framework error ... SM_CHANNEL_TRAIN ...exit code: 1 \n\n\nDoes anyone know how to fix it? or how to set SM_CHANNEL_TRAIN?\n\nThank you",
        "Answers":[
            {
                "Answer_creation_date":"2022-08-29T06:55:20.947Z",
                "Answer_upvote_count":0,
                "Answer_body":"Assuming you are using the sagemaker python sdk, you'll have to specify the train channel.\n\nThe example below shows how to specify 3 channels and their respective paths to S3. In the training container that is started, these will be translated to the environment variable SM_CHANNEL_{channel_name.upper()}. I.e. train channel is translated to SM_CHANNEL_TRAIN, test123 is translated to SM_CHANNEL_TEST123.\n\nfrom sagemaker.estimator import Estimator\n\n\ns3pth = 's3:\/\/mybucket'\n\ndata = {\n\t'train': f'{s3pth}\/train',\n    'validation': f'{s3pth}\/validation',\n    'test': f'{s3pth}\/test',\n}\n\n# starting the train job with our uploaded datasets as input\nestimator.fit(\n    data,\n    wait=False,\n    # job_name = f\"{experiment_name}--{pd.Timestamp.now().strftime('%y%m%d-%H%M%S')}\",\n    # experiment_config = {\n    #     \"TrialName\": trial.trial_name,\n    #     \"TrialComponentDisplayName\": \"Training\",\n    # },\n)",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Built-in Algorithms",
        "Question_creation_time":1652686627960,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDkYruiibS9S05bzFSkLaxg\/sagemaker-built-in-algorithms",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Service Catalog"
        ],
        "Question_upvote_count":0,
        "Question_view_count":76,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I am exploring the Sagemaker Built-in algorithms, and I am curious to learn more about the details of the algorithms. However, I am surprised that it is hard to find any references for the research background and implementation details in the numerous documents and tutorials for particular algorithms. If such information exists somewhere, I would highly appreciate a pointer. Thanks a lot in advance!",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-16T08:11:20.111Z",
                "Answer_upvote_count":0,
                "Answer_body":"thanks for your interest in the built-in algorithms! You can find research papers in the documentation of many of them. And documentation page has a section \"how it works\" explaining the science of every algorithm. For example:\n\nBlazingText: BlazingText: Scaling and Accelerating Word2Vec using Multiple GPUs, Gupta et Khare\nDeepAR DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks, Salinas et al.\nFactorization Machines\nIP Insights\nKMeans\nKNN\nLDA\nLinear Learner\nNTM\nObject2Vec\nObject Detection (it's an SSD model)\nPCA\nRandom Cut Forest: Robust Random Cut Forest Based Anomaly Detection On Streams, Guha et al\nSemantic Segmentation\nSeq2seq\nXGBoost",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"ClientError: An error occurred (UnknownOperationException) when calling the CreateHyperParameterTuningJob operation: The requested operation is not supported in the called region.",
        "Question_creation_time":1652655830031,
        "Question_link":"https:\/\/repost.aws\/questions\/QUkAwy2tG8QreIWmLTGIUAqg\/client-error-an-error-occurred-unknown-operation-exception-when-calling-the-create-hyper-parameter-tuning-job-operation-the-requested-operation-is-not-supported-in-the-called-region",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance",
            "Compute"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Command Line Interface",
            "Machine Learning & AI",
            "Spot Instances"
        ],
        "Question_upvote_count":0,
        "Question_view_count":135,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi Dears,\n\nI am building ML model using DeepAR Algorithm.\n\nI faced this error while i reached to this point : Error :\n\nClientError: An error occurred (UnknownOperationException) when calling the CreateHyperParameterTuningJob operation: The requested operation is not supported in the called region.\n\nCode: from sagemaker.tuner import ( IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner, ) from sagemaker import image_uris\n\ncontainer = image_uris.retrieve(region= 'af-south-1', framework=\"forecasting-deepar\")\n\ndeepar = sagemaker.estimator.Estimator( container, role, instance_count=1, instance_type=\"ml.m5.2xlarge\", use_spot_instances=True, # use spot instances max_run=1800, # max training time in seconds max_wait=1800, # seconds to wait for spot instance output_path=\"s3:\/\/{}\/{}\".format(bucket, output_path), sagemaker_session=sess, ) freq = \"D\" context_length = 300\n\ndeepar.set_hyperparameters( time_freq=freq, context_length=str(context_length), prediction_length=str(prediction_length) )\n\nCan you please help in solving the error? I have to do that in af-south-1 region.\n\nThanks Basem\n\nhyperparameter_ranges = { \"mini_batch_size\": IntegerParameter(100, 400), \"epochs\": IntegerParameter(200, 400), \"num_cells\": IntegerParameter(30, 100), \"likelihood\": CategoricalParameter([\"negative-binomial\", \"student-T\"]), \"learning_rate\": ContinuousParameter(0.0001, 0.1), }\n\nobjective_metric_name = \"test:RMSE\"\n\ntuner = HyperparameterTuner( deepar, objective_metric_name, hyperparameter_ranges, max_jobs=10, strategy=\"Bayesian\", objective_type=\"Minimize\", max_parallel_jobs=10, early_stopping_type=\"Auto\", )\n\ns3_input_train = sagemaker.inputs.TrainingInput( s3_data=\"s3:\/\/{}\/{}\/train\/\".format(bucket, prefix), content_type=\"json\" ) s3_input_test = sagemaker.inputs.TrainingInput( s3_data=\"s3:\/\/{}\/{}\/test\/\".format(bucket, prefix), content_type=\"json\" )\n\ntuner.fit({\"train\": s3_input_train, \"test\": s3_input_test}, include_cls_metadata=False) tuner.wait()",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-20T16:09:16.983Z",
                "Answer_upvote_count":0,
                "Answer_body":"The error message indicates that CreateHyperParameterTuningJob operation is not supported in the region you're currently using. If possible, try the notebook in a region that supports HPO jobs.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker - S3 bucket access when logged into us-east-1",
        "Question_creation_time":1652377507550,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVgOCxivDSgiiHh-HZ9aYYA\/sagemaker-s-3-bucket-access-when-logged-into-us-east-1",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":76,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"My IAM works, but when I login, it logs me into us-east-1. My project is on the S3 instance (sagemaker tool). How can I access buckets in S3 if I am logged into us-east-1 by default.",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-13T13:01:21.591Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can change the region when you are inside a SageMaker notebook or python script. Here you can find many examples of how to access S3 buckets from SageMaker. Please elaborate more on the issue you are facing so we can help you.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker framework processor compatibility with sagemaker pipelines",
        "Question_creation_time":1652344291417,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbY_u2lSORnmomHzZsGOZAA\/sage-maker-framework-processor-compatibility-with-sagemaker-pipelines",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0,
        "Question_view_count":201,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi all,\n\nI am asking if it's possible to use framework processor inside a sagemaker pipeline.\n\nI am asking because the to submit the source_dir for the framework processor, we have to do so when calling the .run() method, when wrapping the processor inside a sagemaker.workflow.steps.ProcessingStep, there isn't an available argument to specify the source_dir.\n\nThank you! Best, Ruoy",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-12T19:17:46.859Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can do this with the latest version of the sagemaker sdk 2.89.0\n\nfrom sagemaker.workflow.pipeline_context import PipelineSession\n\nsession = PipelineSession()\n\ninputs = [\n    ProcessingInput(\n    source=\"s3:\/\/my-bucket\/sourcefile\", \n    destination=\"\/opt\/ml\/processing\/inputs\/\",),\n]\n\nprocessor = FrameworkProcessor(...)\n\nstep_args = processor.run(inputs=inputs, source_dir=\"...\")\n\nstep_sklearn = ProcessingStep(\n    name=\"MyProcessingStep\",\n    step_args=step_args,\n)",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Registered for aws sagemaker studio but not able to create account",
        "Question_creation_time":1652237658633,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSB0wO0OiQ_irUdCx9ppjbg\/registered-for-aws-sagemaker-studio-but-not-able-to-create-account",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":70,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi I received my aws sagemaker studio approval to create an account 1 hr ago When I go to the link to create an account it says my email has not been approved Even though I have an email to the contrary\n\nHow do I contact amazon to sort this out?",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-11T15:42:07.722Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, If you requested a SageMaker Studio Lab free account, then please create an issue on this link.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Studio domain lifecycle configuration for hosting VSCode not working and no error logs",
        "Question_creation_time":1652176660406,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfxuiAWeXShmE1q7BfMdhCw\/sage-maker-studio-domain-lifecycle-configuration-for-hosting-vs-code-not-working-and-no-error-logs",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0,
        "Question_view_count":42,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi all,\n\nI've followed the guide provided in this medium blog by senior AWS ML SA: https:\/\/towardsdatascience.com\/hosting-vs-code-in-sagemaker-studio-f211385e25f7\n\nI've created the lifecycle configuration (jupyterlab app) for the following bash script:\n\n#!\/bin\/bash\n\nmkdir -p vscode\ncd vscode\nsudo su\ncurl -fsSL https:\/\/code-server.dev\/install.sh | sh\nexit\n\n\nHowever, when I attach the configuration to the domain or a specific user's jupyter server in the SM Studio, the JupyterServer App fails to be created with the following message displayed on the loading page of SM Studio's jupyterlab:\n\n\"The JupyterServer app default encountered a problem and was stopped.Details: ConfigurationError: LifecycleConfig execution failed with non zero exit code 1 for script arn:aws:sagemaker:eu-west-3:615740825886:studio-lifecycle-config\/install-vscode-on-jupyterserver. Check https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-lcc-debug.html for debugging instructions.\"\n\nI checked the page for debugging instructions, specifically the part where I am instructed to check the cloudwatch logs for the log group: aws\/sagemaker\/studio The problem is this log group does not exist in CloudWatch (I have admin permission).\n\nI would like to have this set up for the entire team, Any help would be greatly appreciated!\n\nBest, Ruoy",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"XGBoost Error: Allreduce failed - 100GB Dask Dataframe on AWS Fargate ECS cluster dies with 1T of memory.",
        "Question_creation_time":1652113324900,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvXlsdbQ4R9-iX9yAleu-0A\/xg-boost-error-allreduce-failed-100-gb-dask-dataframe-on-aws-fargate-ecs-cluster-dies-with-1-t-of-memory",
        "Question_topic":[
            "Serverless",
            "Containers",
            "Machine Learning & AI",
            "Compute"
        ],
        "Question_tag":[
            "AWS Fargate",
            "Amazon SageMaker",
            "Amazon EC2",
            "Machine Learning & AI",
            "Amazon Elastic Container Service"
        ],
        "Question_upvote_count":0,
        "Question_view_count":101,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Overview: I'm trying to run an XGboost model on a bunch of parquet files sitting in S3 using dask by setting up a fargate cluster and connecting it to a Dask cluster.\n\nTotal dataframe size totals to about 140 GB of data. I scaled up a fargate cluster with properties:\n\nWorkers: 40 Total threads: 160 Total memory: 1 TB So there should be enough data to hold the data tasks. Each worker has 9+ GB with 4 Threads. I do some very basic preprocessing and then I create a DaskDMatrix which does cause the task bytes per worker to get a little high, but never above the threshold where it would fail.\n\nNext I run xgb.dask.train which utilizes the xgboost package not the dask_ml.xgboost package. Very quickly, the workers die and I get the error XGBoostError: rabit\/internal\/utils.h:90: Allreduce failed. When I attempted this with a single file with only 17MB of data, I would still get this error but only a couple workers die. Does anyone know why this happens since I have double the memory of the dataframe?\n\nX_train = X_train.to_dask_array()\nX_test = X_test.to_dask_array()\ny_train = y_train\ny_test = y_test\n\n\ndtrain = xgb.dask.DaskDMatrix(client,X_train, y_train)\n\noutput = xgb.dask.train( client, {\"verbosity\": 1, \"tree_method\": \"hist\", \"objective\": \"reg:squarederror\"}, dtrain, num_boost_round=100, evals=[(dtrain, \"train\")])`",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-11T17:31:53.759Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, Regarding the issue that you are seeing, we would need additional information related to the Fargate tasks that are running and the failures that you are seeing. I would recommend you to open a case with AWS Premium Support ECS Fargate team so that we can discuss more on the specific details of the issue along with the configuration in your use case. You can open a support case with AWS using the link: https:\/\/console.aws.amazon.com\/support\/home?#\/case\/create",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Error on DeleteEndpoint operation: Cannot update in-progress endpoint",
        "Question_creation_time":1652103997301,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2NawO4aWQvmytekX8xJJNQ\/error-on-delete-endpoint-operation-cannot-update-in-progress-endpoint",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":42,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\n\nI have created and endpoint in sagemaker using boto3 but never finishes creation, is stuck in Creating status for few days now. I have tried to delete it using the aws cli api but i get the message:\n\nAn error occurred (ValidationException) when calling the DeleteEndpoint operation: Cannot update in-progress endpoint\n\nUsually endpoint fails after some time and can deleted but this time doesn't fail. Is there any way to force deletion?",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"Amazon SageMaker Data Wrangler now supports additional M5 and R5 instances for interactive data preparation",
        "Question_creation_time":1651874584266,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_YGIMTDDQt61QEJiSLsmtg\/amazon-sage-maker-data-wrangler-now-supports-additional-m-5-and-r-5-instances-for-interactive-data-preparation",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Data Wrangler"
        ],
        "Question_upvote_count":0,
        "Question_view_count":46,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Amazon SageMaker Data Wrangler reduces the time it takes to aggregate and prepare data for machine learning (ML) from weeks to minutes in Amazon SageMaker Studio, the first fully integrated development environment (IDE) for ML. With SageMaker Data Wrangler, you can simplify the process of data preparation and feature engineering, and complete each step of the data preparation workflow, including data selection, cleansing, exploration, and visualization, from a single visual interface. SageMaker Data Wrangler runs on ml.m5.4xlarge by default. SageMaker Data Wrangler includes built-in data transforms and analyses written in PySpark so you can process large data sets (up to hundreds of gigabytes (GB) of data) efficiently on the default instance.\n\nStarting today, you can use additional M5 or R5 instance types with more CPU or memory in SageMaker Data Wrangler to improve performance for your data preparation workloads. Amazon EC2 M5 instances offer a balance of compute, memory, and networking resources for a broad range of workloads. Amazon EC2 R5 instances are the memory optimized instances. Both M5 and R5 instance types are well suited for CPU and memory intensive applications such as running built-in transforms for very large data sets (up to terabytes (TB) of data) or applying custom transforms written in Panda on medium data sets (up to tens of GBs).\n\nTo learn more about the newly supported instances with Amazon SageMaker Data Wrangler, visit the blog or the AWS document, and the pricing page. To get started with SageMaker Data Wrangler, visit the AWS documentation.",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"Data Wrangler Full Outer Join Not Working As Expected Nor Concatenate",
        "Question_creation_time":1651799011602,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4DJAYgTvQQKdl-jplyRtmw\/data-wrangler-full-outer-join-not-working-as-expected-nor-concatenate",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Data Wrangler"
        ],
        "Question_upvote_count":0,
        "Question_view_count":56,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I've got two CSV files that are loaded into Data Wrangler that are intended to augment each other. The tables have some columns that are the same (in name) and some that are not, many of the rows are missing entries for many of the columns. The two tables represent separate datasets. Consider the example below: Table 1:\n\nFilename\tLabelA\tLabelB\n.\/A\/001.dat\t1\t1\n.\/A\/002.dat\t0\t1\n\nTable 2:\n\nFilename\tLabelB\tLabelC\n.\/B\/001.dat\t\t0\n.\/B\/002.dat\t0\t1\n\nI am looking to merge \/ concatenate the two table. The problem is that neither Data Wrangler join nor concatenate seems to work (at least as expected).\n\nDesired result:\n\nFilename\tLabelA\tLabelB\tLabelC\n.\/A\/001.dat\t1\t1\t\n.\/A\/002.dat\t0\t1\t\n.\/B\/001.dat\t\t\t0\n.\/B\/002.dat\t\t0\t1\n\nWhen using a \"Full Outer\" join and ask to combine \"Filename\" and \"LabelB\" columns, it will take all the values from Table 1 OR Table 2 even if Table 1 does not have that entry (for example, some rows will have Filename = <nothing> rather than Filename = .\/B\/001.dat).\n\nWhen using concatenate, Data Wrangler errors on the fact that it cannot match EVERY column between the tables.\n\nNow in my example there are many columns and many rows which precludes a manual process of joining without merging columns and then going through a renaming and merging process one-by-one. How do get these tables to simply merge? I feel I must be missing something obvious. I am about to give up on Data Wrangler and do it all in a python script using pandas, but I thought I should give Data Wrangler a try while learning the MLops process.",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-09T11:29:24.682Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, @b33fcafe\n\nShouldn't you use Concatenate instead of Join because you don't want to join data horizontally but vertically?\n\nAfter importing Table1 and Table2, add the missing columns and then Concatenate to get the expected result.\n\nIf duplicate data occurs, additional processing is possible in subsequent steps.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Service",
        "Question_creation_time":1651712288852,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJXLrO6tMRFm6MCHQf39nbg\/sagemaker-service",
        "Question_topic":[
            "Cloud Financial Management",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Cost and Usage Report",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":42,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am using AWS Sagemaker services and delete any instances that belong to the services. However, i still get charged per day even though I dont use the service anymore. I sent the case to the customer support center but no clear explanation yet. Please any kind of help for my account to stop the cost.",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-05T21:51:06.787Z",
                "Answer_upvote_count":1,
                "Answer_body":"Your Cost explorer can help you find what exactly are you being charged for. You can filter by SageMaker, and then group by 'Usage type'. In general, make sure -\n\nNo notebook instances are running\nNo endpoints are being hosted\nIf you're using SageMaker Studio, all the apps are shut down for every user profile. There might still be EFS charges if you have data over the free tier. See this documentation for shutting down resources - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks-run-and-manage-shut-down.html\n\nAnd, make sure to check all regions, if you've used multiple regions. This should stop recurring costs (if you had training jobs, processing jobs etc - you only pay for the billable seconds, so you won't see recurring costs).",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Service",
        "Question_creation_time":1651712288840,
        "Question_link":"https:\/\/repost.aws\/questions\/QURsZ28QERR3GwqsIKNPhNDw\/sagemaker-service",
        "Question_topic":[
            "Cloud Financial Management",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Cost and Usage Report",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":77,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I am using AWS Sagemaker services and delete any instances that belong to the services. However, i still get charged per day even though I dont use the service anymore. I sent the case to the customer support center but no clear explanation yet. Please any kind of help for my account to stop the cost.",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-05T06:47:26.587Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, As per the AWS pricing Sagemaker pricing follow 2 models i.e. pay as you go & Savings plan. I'm assuming you haven't applied for savings plan? Please do clarify. Refer attached document for detail pricing model, https:\/\/aws.amazon.com\/sagemaker\/pricing\/\n\nIf you can provide the screenshot of your billing then will check and could comment anything.\n\nRegards, Nikhil Shah",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-05-06T21:42:02.345Z",
                "Answer_upvote_count":0,
                "Answer_body":"Overall, a good way to see some of the SageMaker billable resources, is by going to the SageMaker page on AWS Console and click on 'SageMaker dashboard'. The 'Recent Activity' section shows the resources\n\nPS: SageMaker Studio resources are not displayed on that dashboard",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Canvas Integration",
        "Question_creation_time":1651657499501,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSu1172l7SLif49c_4j4PbQ\/sage-maker-canvas-integration",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Canvas"
        ],
        "Question_upvote_count":0,
        "Question_view_count":139,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"How can you use the functions of SageMaker Canvas with python or outside the user interface?",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-04T14:16:43.642Z",
                "Answer_upvote_count":1,
                "Answer_body":"You cannot use Canvas APIs directly outside Canvas. Canvas is no-code ML tool so the intent is not to let the user write any code to do ML. But.. Canvas uses SageMaker Autopilot APIs to train models and you can access all the different training recipes, their code, their hyperparameters, data prep code which was used in Canvas, as jupyter notebooks with in SageMaker Studio.\n\nSo SageMaker Autopilot might be what you are looking for as you can very well access Autopilot APIs.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Why my sagemaker training job slower than notebook from studiolab.sagemaker.aws?",
        "Question_creation_time":1651627134522,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsEj-8jnJRTK3a3gXcH8dRw\/why-my-sagemaker-training-job-slower-than-notebook-from-studiolab-sagemaker-aws",
        "Question_topic":[
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "High Performance Compute",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Studio Lab",
            "Amazon SageMaker Model Training"
        ],
        "Question_upvote_count":0,
        "Question_view_count":84,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I run neural network tensorflow train on studiolab. and I got:\n\nEpoch 145\/4000\n1941\/1941 - 10s - ... - 10s\/epoch - 5ms\/step\n\n\nthen I try to make a train job with script_mode with ml.c5.xlarge\n\nestimator = TensorFlow(entry_point='untitled.py',\n                       source_dir='.\/training\/',\n                       instance_type='ml.c5.xlarge',\n                       instance_count=1,\n                       output_path=\"s3:\/\/sagemaker-[skip]\",\n                       role=sagemaker.get_execution_role(),\n                       framework_version='2.8.0',\n                       py_version='py39',\n                       hyperparameters={...},\n                       metric_definitions=[...],\n                       script_mode=True)\n\n\nand its got:\n\nEpoch 19\/4000\n1941\/1941 - 49s - ... - 49s\/epoch - 25ms\/step\n\n\nWhy is it 5 times slower than studiolab notebook? Is it because instance type?",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-05T23:51:05.130Z",
                "Answer_upvote_count":0,
                "Answer_body":"May I know which instance type you are using for training locally on your notebook instance. Including factors that influence training performance, hardware spec of the training node is very critical. You might be either getting bottlenecked on CPU, Storage or Memory. See here for more details",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Notebook - SSL failed validation when Boto3 session.client(verify=False)",
        "Question_creation_time":1651530941407,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmNuNjDSJSSyOUJ8dxVP-hg\/sagemaker-notebook-ssl-failed-validation-when-boto-3-session-client-verify-false",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":456,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"In a sagemaker notebook with an associated git repository, when I try to create a boto3 session client using verify = False, I get the following : SSLError: SSL validation failed for {service_name } [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)\n\nThe error resolves if I allow the default value of verify = None (meaning SSL certs are verified). My problem is that this session is created as part of a function call on the associated git repository and I don't want to change the behavior in the repo. I don't understand why this error occurs only when I specify not to validate SSL certificates. Any ideas of what explains this behavior?",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-05T18:39:40.758Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nThank you for using AWS SageMaker.\n\nWhen running the below command, it got executed without any error. More over this issue seems to look like a configuration issue, I'd recommend you to reach out to boto3 team via Github or opening a case with Premium Support team to identify if calling the session from Github is possible or not.\n\nimport boto3\nimport boto3.session\n\n# Create your own session\nmy_session = boto3.session.Session()\n\n# Now we can create low-level clients or resource clients from our custom session\ns3 = my_session.client('s3',verify=False)\n\n\n\nOpen a support case with AWS using the link:\n\nhttps:\/\/console.aws.amazon.com\/support\/home?#\/case\/create",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"ECS load container image from sagemaker built-in algorithm docker path",
        "Question_creation_time":1651321815480,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCbp7XzQSSPSH200r45m4Uw\/ecs-load-container-image-from-sagemaker-built-in-algorithm-docker-path",
        "Question_topic":[
            "Serverless",
            "Containers",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Fargate",
            "Amazon SageMaker",
            "Amazon Elastic Container Service"
        ],
        "Question_upvote_count":0,
        "Question_view_count":103,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hello, I'd like to deploy SageMaker's built-in algorithm, BlazingText model on Fargate instead of Sagemaker endpoint. So, I tried to make an ECS task using BlazingText docker path. Here is my CDK code for it.\n\nconst loadBalancedFargateService = new ecsPatterns.ApplicationLoadBalancedFargateService(this, 'Service', { memoryLimitMiB: 1024, desiredCount: 1, cpu: 512, taskImageOptions: { image: ecs.ContainerImage.fromRegistry(\"811284229777.dkr.ecr.us-east-1.amazonaws.com\/blazingtext:1\"), }, });\n\nHowever, I got an error: CannotPullContainerError: inspect image has been retried 1 time(s): failed to resolve ref \"811284229777.dkr.ecr.us-east-1.amazonaws.com\/blazingtext:1\": pulling from host 811284229777.dkr.ecr.us-east-1.amazonaws.com failed with status code [manifests 1]...\n\nIs it impossible to pull docker container of sagemaker built-in algorithm from ECS?",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-30T14:28:20.610Z",
                "Answer_upvote_count":1,
                "Answer_body":"To my knowledge, no - it's not generally possible to pull the built-in algorithm containers outside SageMaker: Your easiest route would probably just be to deploy the model on SageMaker and integrate your other containerized tasks to call the SageMaker endpoint.\n\nIt's maybe worth mentioning that the framework containers for custom\/script-mode modelling (e.g. the AWS DLCs for PyTorch\/HuggingFace\/etc) are not subject to this restriction (can check you should even be able to pull them locally): So if you were to use those to implement a customized text processing model I think you should be able to deploy it on ECS if needed. Of course this'd mean a more initial build and later maintenance effort though.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Does Sagemaker support private GitLab repos?",
        "Question_creation_time":1650995574723,
        "Question_link":"https:\/\/repost.aws\/questions\/QUI8X0yZHTQ1uiS-TxOJe4ww\/does-sagemaker-support-private-git-lab-repos",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":1,
        "Question_view_count":283,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a private GitLab and would like to connect it to Sagemaker Studio and\/or Notebooks. Is that supported? What is it is through a VPC? If so, how can it be done?",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-27T07:23:12.161Z",
                "Answer_upvote_count":1,
                "Answer_body":"Yes, it is possible to connect a private GitLab to SageMaker Studio and Notebooks. You will find the detailed steps that you can use in this blog post.\n\nThe standard documentation on Git repository associations can be found here.\n\nIf you are interested in how to build MLOps workflows with Amazon SageMaker projects, GitLab, and GitLab pipelines this recent blog provides valuable information.\n\nYou can also do the same configuration in a VPC, just ensure you have a route to your private GitLab repository.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Run different notebooks present in same Sagemaker notebook instance using lifecycle configurations based on different lambda triggers",
        "Question_creation_time":1650980304333,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGrSiVuFAS_WuZDVTUmFdlA\/run-different-notebooks-present-in-same-sagemaker-notebook-instance-using-lifecycle-configurations-based-on-different-lambda-triggers",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0,
        "Question_view_count":135,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a sagemaker notebook instance having two jupyter notebook ipynb files. When I had one jupyter notebook, I was able to run it automatically with one lambda function trigger and lifecycle configuration.\n\nNow I have two jupyter notebooks and corresponding two lambda function triggers. How can I run them based on the trigger by changing the lifecycle configuration script.\n\nThe trigger is file uploading into S3. Based on what location the file is added, the corresponding jupyter notebook should run",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-11T01:13:14.965Z",
                "Answer_upvote_count":1,
                "Answer_body":"For the use case of automatically running ML jobs with on-demand infrastructure, with the ability to accept input parameters, I'd recommend SageMaker Processing as a better fit than Notebook Instances + Lifecycle Configs.\n\nWith processing jobs:\n\nYou could still use notebook files if needed, using a tool like Papermill or a more basic pattern like just loading the file and running through the code cells. For example using a FrameworkProcessor, you should be able to upload a bundle of files to S3 (including your notebooks and a plain Python entrypoint to manage running them).\nYou could trigger processing jobs from events just like your current notebook start-up, but could provide many different parameters to control what gets executed.\nThe history of jobs and their parameters will be automatically tracked through the SageMaker Console - with logs and metrics also available for analysis.\nYou wouldn't be limited to the 5 minute time-out of a LCConfig script\n\nIf you really needed to stick with the notebook pattern though, modifying the LCConfig each time seems less than ideal... So maybe I'd suggest bringing in another external state you could use to manage some state: For example have your LCConfig script read a parameter from SSM or DynamoDB to tell it which notebook to execute on the current run?",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Run different notebooks present in same Sagemaker notebook instance with lifecycle configurations based on different lambda triggers",
        "Question_creation_time":1650980301919,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxwrW6VkVQra1tlWA7UtiNQ\/run-different-notebooks-present-in-same-sagemaker-notebook-instance-with-lifecycle-configurations-based-on-different-lambda-triggers",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0,
        "Question_view_count":50,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a sagemaker notebook instance having two jupyter notebook ipynb files. When I had one jupyter notebook, I was able to run it automatically with one lambda function trigger and lifecycle configuration.\n\nNow I have two jupyter notebooks and corresponding two lambda function triggers. How can I run them based on the trigger by changing the lifecycle configuration script.",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"\u00bfHow can we crate a lambda which uses a Braket D-Wave device?",
        "Question_creation_time":1650979594609,
        "Question_link":"https:\/\/repost.aws\/questions\/QUt5r-dbkZT1O4yQM_TszxHw\/how-can-we-crate-a-lambda-which-uses-a-braket-d-wave-device",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Quantum Technologies",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "AWS Lambda",
            "Amazon Braket",
            "Amazon SageMaker",
            "Quantum Technologies"
        ],
        "Question_upvote_count":0,
        "Question_view_count":97,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"We are trying to deploy a Lambda with some code which works in a Notebook. The code is rather simple and uses D-Wave \u2014 DW_2000Q_6. The problem is that when we execute the lambda (container lambda due to size problems), it give us the following error:\n\n{\n  \"errorMessage\": \"[Errno 30] Read-only file system: '\/home\/sbx_user1051'\",\n  \"errorType\": \"OSError\",\n  \"stackTrace\": [\n    \"  File \\\"\/var\/lang\/lib\/python3.8\/imp.py\\\", line 234, in load_module\\n    return load_source(name, filename, file)\\n\",\n    \"  File \\\"\/var\/lang\/lib\/python3.8\/imp.py\\\", line 171, in load_source\\n    module = _load(spec)\\n\",\n    \"  File \\\"<frozen importlib._bootstrap>\\\", line 702, in _load\\n\",\n    \"  File \\\"<frozen importlib._bootstrap>\\\", line 671, in _load_unlocked\\n\",\n    \"  File \\\"<frozen importlib._bootstrap_external>\\\", line 843, in exec_module\\n\",\n    \"  File \\\"<frozen importlib._bootstrap>\\\", line 219, in _call_with_frames_removed\\n\",\n    \"  File \\\"\/var\/task\/lambda_function.py\\\", line 6, in <module>\\n    from dwave.system.composites import EmbeddingComposite\\n\",\n    \"  File \\\"\/var\/task\/dwave\/system\/__init__.py\\\", line 15, in <module>\\n    import dwave.system.flux_bias_offsets\\n\",\n    \"  File \\\"\/var\/task\/dwave\/system\/flux_bias_offsets.py\\\", line 22, in <module>\\n    from dwave.system.samplers.dwave_sampler import DWaveSampler\\n\",\n    \"  File \\\"\/var\/task\/dwave\/system\/samplers\/__init__.py\\\", line 15, in <module>\\n    from dwave.system.samplers.clique import *\\n\",\n    \"  File \\\"\/var\/task\/dwave\/system\/samplers\/clique.py\\\", line 32, in <module>\\n    from dwave.system.samplers.dwave_sampler import DWaveSampler, _failover\\n\",\n    \"  File \\\"\/var\/task\/dwave\/system\/samplers\/dwave_sampler.py\\\", line 31, in <module>\\n    from dwave.cloud import Client\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/__init__.py\\\", line 21, in <module>\\n    from dwave.cloud.client import Client\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/client\/__init__.py\\\", line 17, in <module>\\n    from dwave.cloud.client.base import Client\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/client\/base.py\\\", line 89, in <module>\\n    class Client(object):\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/client\/base.py\\\", line 736, in Client\\n    @cached.ondisk(maxage=_REGIONS_CACHE_MAXAGE)\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/utils.py\\\", line 477, in ondisk\\n    directory = kwargs.pop('directory', get_cache_dir())\\n\",\n    \"  File \\\"\/var\/task\/dwave\/cloud\/config.py\\\", line 455, in get_cache_dir\\n    return homebase.user_cache_dir(\\n\",\n    \"  File \\\"\/var\/task\/homebase\/homebase.py\\\", line 150, in user_cache_dir\\n    return _get_folder(True, _FolderTypes.cache, app_name, app_author, version, False, use_virtualenv, create)[0]\\n\",\n    \"  File \\\"\/var\/task\/homebase\/homebase.py\\\", line 430, in _get_folder\\n    os.makedirs(final_path)\\n\",\n    \"  File \\\"\/var\/lang\/lib\/python3.8\/os.py\\\", line 213, in makedirs\\n    makedirs(head, exist_ok=exist_ok)\\n\",\n    \"  File \\\"\/var\/lang\/lib\/python3.8\/os.py\\\", line 213, in makedirs\\n    makedirs(head, exist_ok=exist_ok)\\n\",\n    \"  File \\\"\/var\/lang\/lib\/python3.8\/os.py\\\", line 223, in makedirs\\n    mkdir(name, mode)\\n\"\n  ]\n}\n\nIt seems that the library tries to write to some files which are not in \/tmp folder.\n\nI'm wondering if is possible to do this, and if not, what are the alternatives.\n\nimports used:\n\nimport boto3\nfrom braket.ocean_plugin import BraketDWaveSampler\nfrom dwave.system.composites import EmbeddingComposite\nfrom neal import SimulatedAnnealingSampler",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-28T00:03:56.649Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, and thank you very much for your question!\n\nAs you noted, AWS Lambda allows for writing in the \/tmp folder and the dwave determines where it will cache data based on this algorithm.\n\nWe currently support running Braket in Notebook Instances you can create through our console, or as Braket Hybrid Jobs, which is more similar to your use of containers.\n\nHowever, we would really appreciate to understand more about your use case to see if that's something we should look into more deeply. Could you please give us more information about your use case, what you're trying to do, and why you think AWS Lambda will work well for it?",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to create (Serverless) SageMaker Endpoint using exiting tensorflow pb (frozen model) file?",
        "Question_creation_time":1650919347687,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZ3v2_JXIRCSbBHVBDJUWgQ\/how-to-create-serverless-sage-maker-endpoint-using-exiting-tensorflow-pb-frozen-model-file",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0,
        "Question_view_count":74,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Note: I am a senior developer, but am very new to the topic of machine learning.\n\nI have two frozen TensorFlow model weight files: weights_face_v1.0.0.pb and weights_plate_v1.0.0.pb. I also have some python code using Tensorflow 2, that loads the model and handles basic inference. The models detect respectively faces and license plates, and the surrounding code converts an input image to a numpy array, and applies blurring to the images in areas that had detections.\n\nI want to get a SageMaker endpoint so that I can run inference on the model. I initially tried using a regular Lambda function (container based), but that is too slow for our use case. A SageMaker endpoint should give us GPU inference, which should be much faster.\n\nI am struggling to find out how to do this. From what I can tell reading the documentation and watching some YouTube video's, I need to create my own docker container. As a start, I can use for example 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-inference:2.8.0-gpu-py39-cu112-ubuntu20.04-sagemaker.\n\nHowever, I can't find any solid documentation on how I would implement my other code. How do I send an image to SageMaker? Who tells it to convert the image to numpy array? How does it know the tensor names? How do I install additional requirements? How can I use the detections to apply blurring on the image, and how can I return the result image?\n\nCan someone here please point me in the right direction? I searched a lot but can't find any example code or blogs that explain this process. Thank you in advance! Your help is much appreciated.",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-26T13:06:11.302Z",
                "Answer_upvote_count":0,
                "Answer_body":"You should package the model files into model.tar.gz file and use TensorFlowModel object to deploy the mode in SageMaker Endpoint.\n\nYou can see and example here.\n\nThis is and example for the same, but with PyTorch.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Getting stuck on SageMaker domain creation through the standard setup wizard",
        "Question_creation_time":1650593854106,
        "Question_link":"https:\/\/repost.aws\/questions\/QUaiMX9ZU3SdqFnjxiNn_XRg\/getting-stuck-on-sage-maker-domain-creation-through-the-standard-setup-wizard",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":122,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I am having difficulty in getting started on SageMaker Studio using the Standard setup wizard of SageMaker Domain creation. The wizard prevents me from moving forward at the RStudio setup step even though RStudio is indicated to be optional and which I don't need or want to pay for. I have tried a lot of things (looked at the videos, searched the web) to no avail. Tried as a root user as well as a IAM user and have the same issue. Thanks for any help.",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-25T22:55:14.358Z",
                "Answer_upvote_count":0,
                "Answer_body":"What is the region you are trying this into? Can you share some screenshots to explain the difficulty in moving forward? Does it give an error, does it keep loading? When you are using standard setup, are you using any vpc to launch studio? Knowing the answer of these questions will help provide a better solution to your problem",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-04-22T09:01:11.976Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi! You'll need to specify a VPC for setting up a SageMaker domain. If you click on the drop down, you'll see a default VPC (in any region) to set up Studio in. The second configuration (public internet or VPC only) is to whether restrict traffic to only the VPC or internet. You don't need an RStudio license to use SageMaker Studio.\n\nSee here for detailed reference - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/onboard-vpc.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Socket closed error when running inference using Greengrass",
        "Question_creation_time":1650474528451,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSJgKY9a2THi1lcmLUoAHdg\/socket-closed-error-when-running-inference-using-greengrass",
        "Question_topic":[
            "Internet of Things (IoT)",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS IoT Greengrass",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Internet of Things (IoT)"
        ],
        "Question_upvote_count":0,
        "Question_view_count":103,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello\n\nWe are using Greengrass v2 with the Sagemaker component in order to do inference on the edge. The following error is seen intermettently while running our application. Could you please advise of anything we can do to avoid it?\n\nFile \"\/usr\/local\/lib\/python3.6\/dist-packages\/grpc\/_channel.py\", line 946, in call return _end_unary_response_blocking(state, call, False, None) File \"\/usr\/local\/lib\/python3.6\/dist-packages\/grpc\/_channel.py\", line 849, in _end_unary_response_blocking raise _InactiveRpcError(state) grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with: status = StatusCode.UNAVAILABLE details = \"Socket closed\" debug_error_string = \"{\"created\":\"@1650459124.460491853\",\"description\":\"Error received from peer unix:\/tmp\/aws.greengrass.SageMakerEdgeManager.sock\",\"file\":\"src\/core\/lib\/surface\/call.cc\",\"file_line\":906,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\"",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-25T23:29:25.162Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nWould you be able to provide the full log at \/greengrass\/v2\/logs\/aws.greengrass.SageMakerEdgeManager.log , along with any other relevant log files?\n\nCould you also turn on the debug mode with the SagemakerEdgeLogVerbose config option specified here? https:\/\/docs.aws.amazon.com\/greengrass\/v2\/developerguide\/sagemaker-edge-manager-component.html#sagemaker-edge-manager-component-log-file\n\nAre there any other applications that could interfere with that socket?\n\nThanks!",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"[errno 28] no space left on disk",
        "Question_creation_time":1650457037741,
        "Question_link":"https:\/\/repost.aws\/questions\/QUInf7H_9bQpCPMROevLa2fA\/errno-28-no-space-left-on-disk",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":57,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to install a python package called rapidsai in aws notebook instance but I am getting this error errno 28 no space left on disk. I am using g4dn.xlarge instance for it. The package is almost 3 GB. While opening the notebook instance I have tried increasing volume size of notebook upto 50 GB, I am still getting this error. Let me know the solution to it. Thanks",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"How to set up a pipe mode in sagemaker?",
        "Question_creation_time":1650141519238,
        "Question_link":"https:\/\/repost.aws\/questions\/QUoMtSoBPGQpabpreLah_Fjg\/how-to-set-up-a-pipe-mode-in-sagemaker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":113,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"what other data input types can be used pipe input mode in sagemaker? an example of implementation is here https:\/\/aws.amazon.com\/blogs\/aws\/sagemaker-nysummit2018\/, and can this be used for inference as well as, similar to training?",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-19T18:16:18.685Z",
                "Answer_upvote_count":0,
                "Answer_body":"Pipe mode support reading data from Amazon S3. Pipe mode can be used for Batch inference as described in the blog.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"how to choose an instance type for a sagemaker testing\/inference?",
        "Question_creation_time":1650126925753,
        "Question_link":"https:\/\/repost.aws\/questions\/QULxw59aBCRfmso_f7-VCjRQ\/how-to-choose-an-instance-type-for-a-sagemaker-testing-inference",
        "Question_topic":[
            "Machine Learning & AI",
            "Compute"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Spot Instances"
        ],
        "Question_upvote_count":0,
        "Question_view_count":225,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"looking at few examples, for training in sagemaker . are there some guidelines based on the model size, data to be trained , what type of instance cpu\/gpu to use? also, can one use spot instances ( may be with multiple gpu cores)?",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-16T20:54:39.046Z",
                "Answer_upvote_count":1,
                "Answer_body":"Yes, you can use spot instances. I recommend it, and always run training on spot instances. If you are using the Python SDK, add the following parameters to your Estimator:\n\n       use_spot_instances=True,\n       max_run={maximum runtime here},\n       max_wait={maximum wait time},\n       checkpoint_s3_uri={URI of your bucket and folder },\n\n\nSee the documentation for more details here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-managed-spot-training.html\n\nAs far as instance types are concerned, the individual algorithms contain some initial recommendations for instances types: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\n\nFor example, see the EC2 Instance Recommendation for the Image Classification Algorithm: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\n\nThere was a presentation at re:Invent 2020 - How to choose the right instance type for ML inference: https:\/\/www.youtube.com\/watch?v=0DSgXTN7ehg\n\nHope this helps",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-05-06T20:32:34.069Z",
                "Answer_upvote_count":1,
                "Answer_body":"And for the selection of instance type for inference, you might want to look at Amazon SageMaker Inference Recommender:\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-recommender.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Visualizing custom Model Monitor metrics in Sagemaker Studio",
        "Question_creation_time":1650041844554,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjbz0DSjCSoujr8vBAJvOOA\/visualizing-custom-model-monitor-metrics-in-sagemaker-studio",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon CloudWatch",
            "Containers",
            "Monitoring"
        ],
        "Question_upvote_count":1,
        "Question_view_count":300,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"We are developing a custom model monitoring container to be used to interact with Sagemaker's model monitoring API, as we require additional custom metrics. One of our requirements is for these metrics to populate and visualize appropriately in Studio visualization tabs, e.g. Data Quality, Model Quality, and Explainability. None of the built-in containers are an option for us, and we are trying to interop with the Monitoring APIs as seamlessly as possible.\n\nThe docs specify the container contracts for output, specifically statistics.json, constraints.json, and constraint_violations.json. However, the docs are not clear on what JSON files to emit for specifically custom Model Quality metrics. There does not appear to be a provided schema.\n\nThings I have tried:\n\nEmitting CloudWatch metrics to a different namespace, sagemaker\/Endpoint\/model-quality-metrics\nAttempting to retrofit the statistics.json file to also include Model Quality metrics.\n\nIs there a separate JSON file I should write to \/opt\/ml\/processing\/resultdata for our custom model quality metrics to populate the Studio visualization tab?\n\nSpecifically, where do the base containers look for this JSON report: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-model-quality-metrics.html",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-22T15:03:22.611Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi Michael,\n\nIf I understand your question correctly, you are trying to build a BYOC Model Monitor container in order to bring in custom metrics to monitor your model. Firstly, when we take that route, the metrics that are monitored by default, lets say for Model Quality, metrics such as mae, r1 for instance does not come out of the box, you have to incorporate logic to calculate the respective metrics listed here. In short, you cannot \"add-on\" a new custom metric to the existing metric OOTB until the SageMaker team exposes the container which does this logic publicly.\n\nSecondly, there is no \"strict\" formatting(json files) to writing your custom metrics per say, it is at the discretion of the customer to implement the the json file as required as long as they can read that in the BYOC container to calculate violations and so on. However, we encourage customers to follow the KLL fashion as show in the example here.\n\nLinking a few samples of BYOC implementations of model monitor for your reference below -\n\nNLP data drift BYOC model monitor - https:\/\/github.com\/aws-samples\/detecting-data-drift-in-nlp-using-amazon-sagemaker-custom-model-monitor\n\nCV BYOC model monitor - https:\/\/aws.amazon.com\/blogs\/machine-learning\/detecting-and-analyzing-incorrect-model-predictions-with-amazon-sagemaker-model-monitor-and-debugger\/",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-04-15T20:28:11.741Z",
                "Answer_upvote_count":0,
                "Answer_body":"Apologies for the delayed response Michael, to answer your question s\n\n1\/ on where and what name the metrics need to be stored - The constraints and violations files need to be written to \/opt\/ml\/processing https:\/\/github.com\/aws-samples\/detecting-data-drift-in-nlp-using-amazon-sagemaker-custom-model-monitor\/blob\/main\/docker\/evaluation.py#L156 and the filename is constraint_violations.json\n\n2\/ I have ran a default ModelQualityMonitor job. I see that this job also produces the 3-set of (constraints.json, constraint_violations.json, statistics.json). How are these differentiated between monitoring job types?\n\nAnswer: Even though the files generated by each job is the same no matter the type of Model monitoring you choose(Data\/Quality), if you closely observe, the model data monitoring for Statistics file will have -\n\n\"mean\" : 0.13082980736646624, \"sum\" : 54.94851909391582, \"std_dev\" : 0.2511377559440087, \"min\" : 0.006144209299236536, \"max\" : 0.989563524723053, \"distribution\" : {\n\nwhereas, statistics for Model Quality will have ( for Binary classification problem)\n\n\"binary_classification_metrics\" : { \"confusion_matrix\" : { \"0\" : { \"0\" : 173, \"1\" : 0 }, \"1\" : { \"0\" : 12, \"1\" : 16 }\n\nTo sum it up, yes if you provide these 3 files i.e statistics.json, constraints.json and constraint_violations.json you should be all set.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Ground truth pdf annotation tool not rendering anything",
        "Question_creation_time":1649874349265,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzYkN4V8kRsWhT3ZSNJy9mQ\/sagemaker-ground-truth-pdf-annotation-tool-not-rendering-anything",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth",
            "Amazon Comprehend"
        ],
        "Question_upvote_count":0,
        "Question_view_count":169,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello, I have followed these docs https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/cer-annotation-pdf.html and have gotten to the point in which I have created the annotation task and I have uploaded several pdf's to a s3 bucket to be used for an annotation task so I can create a comprehend model. I put myself and a co-worker as annotators just so I can verify that I can set up the task properly and I only uploaded 37 pdf's. However when both of us log in and start the task, the webpage loads as the instructions tell us to however there is no pdf rendered (though I think I see it briefly flash on the screen before it goes blank) and there are also no entities to be selected as a part of the ui unlike how the documentation pictures the tool. I am trying to do named entity recognition and created this task with the full 25 entities I want to be able to label and Also another time with only 5 entities to label. However there seems to be something wrong with this native pdf annotation feature.",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-15T22:07:07.140Z",
                "Answer_upvote_count":0,
                "Answer_body":"Dear Customer,\u2028\u2028\u2028\n\nThank you so much for reaching to us. I understand that you followed our AWS Comprehend documentation for annotating PDF\u2019s, in-order to annotate your training PDFs in SageMaker Ground Truth. Further yourself and your team is working as private annotators, in-order to facilitate the task appropriately. However when you and your co-workers login to the page for annotation in Sagemaker Ground truth, the webpage does not list any pdfs and the page is blank.\u2028Hence you were looking for guidance in resolving this issue.\u2028\u2028\n\nThank you so much for providing the details.\u2028\u2028\n\nTo further better assist you on this issue, can you please create a Support Ticket to AWS. Below link will assist you to create the Support Ticket. [+]https:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html\u2028\u2028 [+]https:\/\/console.aws.amazon.com\/support\/home#\/case\/create\n\n\u2014While creating the support ticket, we kindly request you to provide the below information\n\nUse-case description.\nGround Truth Job ARN details\nScreen-shots of the issue you are facing.\nLog Files for the Ground truth Job(This logs from your labeling jobs appear in Amazon CloudWatch under the \/aws\/sagemaker\/LabelingJobs group.).\n\nThe reason behind this ask is this would help us to understand your use-case in a better way, further if we might need to deep dive and access the job created from our internal tools, we will have more visibility through the support ticket.\n\nRest assured we will do everything best in our abilities to assist you on this issue. \u2028\u2028 Thanks.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Without ECR being enabled in AWS account at Organization Level, what's the impact to SageMaker Studio?",
        "Question_creation_time":1649820151027,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXZB0Oki3QamlQ5ijtVSuzQ\/without-ecr-being-enabled-in-aws-account-at-organization-level-whats-the-impact-to-sage-maker-studio",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon Elastic Container Registry (ECR)"
        ],
        "Question_upvote_count":0,
        "Question_view_count":95,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I have a customer already standardized Artifactory as the centralized image registration. They disabled ECR service at Org level. Now we want to understand the potential impact on customer's day-2-day use of SageMaker Studio as a platform to support their full ML lifecycle.\n\n(If customer only use built-in SageMaker algorithm or framework and use prebuilt SageMaker container images)\n\nEspecially when customer trying to deploy the model to endpoint, does that need ECR service to be enabled in customer account?",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-18T16:46:25.539Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi, yes, if you're restricting the user to only built-in algorithms and frameworks, and prebuilt images for Studio, you should be able to use it seamlessly (to deploy endpoints as well). That said, it severely restricts the data scientist from using custom images that could be built to their needs and packages, or bringing their own container for machine learning.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker requirements.txt unable to find certain packages",
        "Question_creation_time":1649797399439,
        "Question_link":"https:\/\/repost.aws\/questions\/QUl7yzFGwfSEauI7yesYeXiw\/sagemaker-requirements-txt-unable-to-find-certain-packages",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":181,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\n\nI'm trying to run a sagemaker job on p3.2xlarge instance using the PyTorch estimator. My script has dependencies on several packages (possibly not pre-installed on the instance) for which I have a requirements.txt file. After loading the instance, it installs several packages but the job fails with following error:\n\nsagemaker-training-toolkit ERROR InstallRequirementsError: Command \"\/opt\/conda\/bin\/python3.6 -m pip install -r requirements.txt\" ERROR: Could not find a version that satisfies the requirement scikit_image==0.18.3 ERROR: No matching distribution found for scikit_image==0.18.3\n\nIt fails for other packages too, like h5py, numpy etc.\n\nAny help is greatly appreciated.\n\nThank you, Aditya",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-20T16:55:57.266Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nThank you for using Amazon SageMaker.\n\nUsually this kind of error is occurred due version mismatch or missing dependencies. As you have mentioned that it worked locally on your system without any issue, we would like to dive deep into this issue for which we will required your script and requirement.txt file and sharing those files with us will help us identify what is the root cause of this issue and help you overcome this issue. Sharing any files on this medium is not encourage, so I recommend you to open a case with SageMaker Premium Support team so that you can share above mentioned details securely.\n\nOpen a support case with AWS using the link:\n\nhttps:\/\/console.aws.amazon.com\/support\/home?#\/case\/create",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Multi Model EndPoint and Inference Data Capture feature",
        "Question_creation_time":1649794559479,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlAvpGSsISyqu0MyebgRJDA\/sage-maker-multi-model-end-point-and-inference-data-capture-feature",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":175,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Does Data Capture feature used for model monitor and analytics work with the multi model endpoint (one container).. we ran into an error. See error \" An error occurred (ValidationException) when calling the CreateEndPointConfig operation: Data Capture Feature is not supported with MultiModel mode\" Theoretically, it should work because it is calling the DataCaptureConfig:\n\nfrom sagemaker.model_monitor import DataCaptureConfig\n\nendpoint_name = 'your-pred-model-monitor-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()) print(\"EndpointName={}\".format(endpoint_name))\n\ndata_capture_config=DataCaptureConfig( enable_capture = True, sampling_percentage=100, destination_s3_uri=s3_capture_upload_path)",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-13T13:39:17.628Z",
                "Answer_upvote_count":0,
                "Answer_body":"SageMaker multi-model endpoints do not have support for SageMaker Model monitor as of writing this answer. So the error is pointing to exactly that.\n\nHowever, if you are looking to implement data drift using sagemaker model monitor then you can do that my mimicking data capture config functionality by capturing inference input and prediction output and storing it in the format supported by Model Monitor. And then setup a customer monitoring container using the instructions listed https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-byoc-containers.html",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Can Sagemaker Git Repositories use ssh secrets (no name and password)?",
        "Question_creation_time":1649790570262,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-P1Hlk4OR6K6kAug-wHT_g\/can-sagemaker-git-repositories-use-ssh-secrets-no-name-and-password",
        "Question_topic":[
            "Machine Learning & AI",
            "Security, Identity, & Compliance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Secrets Manager"
        ],
        "Question_upvote_count":0,
        "Question_view_count":519,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Can Sagemaker Git Repositories use ssh secrets (no name and password)?",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-13T05:47:58.883Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yes, Sagemaker can use SSH for private repos. There are multiple options on how to connect to a repo in Sagemaker.\n\n** Option 1**: Using SSH to work with a private repo You can follow the same steps you do in your local machine to connect to a private repo through SSH, steps to follow:\n\nOpen Terminal and type ssh-keygen to create an SSH key in your Amazon Sagemaker instance.\nAdd the public key to your Git account (Github or Gitlab)\nGet the SSH url of your repo and git clone\n\nOption 2: Using AWS Secret Manager You can follow the steps in AWS official documentation here.\n\nOption 3: Using GitHub with Personal Access Tokens Recommended\n\nLet\u2019s assume you have already generated an Access Tokens through the GitHub\u2019s Settings \/ Developer Settings \/ Personal Access Tokens page.\n\nYou can just simply go ahead and clone the repository using Studio UI. When it asks your username and password, you can provide your GitHub username and the Personal Access Token. If you want to cache your credentials avoiding to type it every time when you\u2019re interacting with the GitHub server, you can cache or store it on your home folder with the following command issued in the Terminal:\n\n$ git config --global git credential.helper [cache|store]\n\nIf you choose to store your credentials, it will be written to the ~\/.git-credentials file located in your home folder. The \u201ccache\u201d helper stores the credential in-memory only and never lands on disk. It also accepts the --timeout <seconds> option, which changes the amount of time its daemon is kept running (the default is \u201c900\u201d, or 15 minutes)\n\nBefore you make your first commit, you still need to configure the git client to use your identity when we\u2019re checking in some new code into the repository. You need to run the following two commands from the terminal:\n\n$ git config --global user.email \u201cuser@email.com\u201d\n$ git config --global user.name \u201cUser Name\u201d\n\n\nSagemaker Studio is fully integrated with git and you can do it through the UI.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"How to debug invocation timeout in sagemaker?",
        "Question_creation_time":1649727502799,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE4UPZjwNQveIG8zuZeXIgA\/how-to-debug-invocation-timeout-in-sagemaker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":432,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am testing inference in sagemaker , by using one of the container listed here -> https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md. the model is zipped up as below and with in inference.py file , i am overwriting functions like model_fn method and predict_fn. I tested this with batch transform and it worked but for few small input files but for other larger files, i keep getting \"Model server did not respond to \/invocations request within 3600 seconds\" . I'm trying to find out what is the cause of it? 3600 is the max we can set for \"invocation timeout in seconds\" parameter and the default input size for batch is 6mb , the input files i'm using are way smaller than that but i still get that error.\n\nDirectory structure\n\nmodel.tar.gz\/\n|- model.pth\n|- code\/\n  |- inference.py\n  |- requirements.txt  \n\n\n\nfile : inference.py\n\nimport torch\nimport os\n\ndef model_fn(model_dir):\n    model = Your_Model()\n    with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:\n        model.load_state_dict(torch.load(f))\n    return model\n\ndef predict_fn():\n    \/\/\n\n\nbased on docs here, https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html#your-algorithms-batch-code-how-containers-should-respond-to-inferences, do we need to install flask and have an \/invocations endpoint , that responds 200 ok , when we are using custom container?",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-15T20:48:32.532Z",
                "Answer_upvote_count":0,
                "Answer_body":"One of the best ways to debug a custom inference script would be to start off with using the SageMaker \"local mode\". Once you are sure that your script is working fine, move over to hosting on the SageMaker endpoint. Here are some of the examples to get started.\n\nExample for a TF serving model that I have a custom Inference script, I would use local mode as shown below for my testing-\n\nfrom sagemaker.tensorflow.model import TensorFlowModel\nfrom sagemaker.local import LocalSession\n\ntensorflow_serving_model = TensorFlowModel(\n    model_data=model_data,\n    role=sagemaker_role,\n    framework_version=\"2.6\",\n  # sagemaker_session=sagemaker_session,\n  sagemaker_session=LocalSession()\n)",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker inference on inf1 no opencv",
        "Question_creation_time":1649704234930,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUG9fHLN3TNGbXjRmuwRBcA\/sagemaker-inference-on-inf-1-no-opencv",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Inferentia"
        ],
        "Question_upvote_count":0,
        "Question_view_count":123,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to deploy Pytorch model on ml.inf1.xlarge instance. Image: 301217895009.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-neo-pytorch:1.5.1-inf-py3 My python code using some oepncv functions, and when I am trying to run the infernce I got the following error: ModuleNotFoundError: No module named 'cv2'\n\nI tried to add opencv-python-headless to requirements.txt, but then I got another error ImportError: libgthread-2.0.so.0: cannot open shared object file\n\nHow I can use opencv with the ml.inf1 instances?",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-11T21:49:47.674Z",
                "Answer_upvote_count":0,
                "Answer_body":"When running Neo inference on Sagemaker, it\u2019s possible to now use the Deep Learning Containers (DLC) provided by AWS. It\u2019s also unnecessary to remain on Pytorch 1.5 when using the DLC images. In our latest released image, OpenCV is pre-installed along with a more recent version of AWS Neuron SDK. Here\u2019s a link to the latest docker image for Pytorch 1.10 + Neuron SDK 1.17: https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.10\/py3\/sdk1.17.1\/Dockerfile.neuron#L83\n\nIf you\u2019re unable to easily move to the latest Pytorch+Neuron DLC, the dockerfile link may help with resolving installation errors of OpenCV into your container.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Is it possible to create Parallel Pipelines in Sagemaker",
        "Question_creation_time":1649666083053,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZtsbNf1GTAOnaTCrif-WJg\/is-it-possible-to-create-parallel-pipelines-in-sagemaker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Pipelines"
        ],
        "Question_upvote_count":0,
        "Question_view_count":373,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I want to bind processing pipeline to multiple training pipeline. I just want to compare algorithm accuracy. Same dataset will be trained by multiple algorithms and will be predicted by them. My goal for the future is consolidate predict results of different algorithms and generate combined\/consolidated resulst. Is is possible to do in SageMaker.\n\nExample Schema:\n\n            - Train_Algo1      \n Process    - Train_Algo2    - Predict Result\n            - Train_AlgoN",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-11T09:48:58.637Z",
                "Answer_upvote_count":0,
                "Answer_body":"I'd recommend checking out SageMaker Pipelines for this - especially if you're able to use SageMaker Studio for the graphical pipeline management UIs.\n\nYou can build your pipeline definition through the SageMaker Python SDK, just like you might normally define Training and Processing jobs. In fact pipeline steps (like TrainingStep) typically just wrap around the standalone constructs (like Estimator) that you might be using already.\n\nPipeline steps are executed in paralllel by default, unless there is an implicit (properties data) or explicit (depends_on) dependency between them.\n\nSM Pipelines can take parameters, so you could expose necessary training hyperparameters or pre-processing parameters up to the pipeline level, and use the pipeline to kick off multiple end-to-end runs with different configurations.\n\nBy turning on step caching, you could prevent your pre-processing from being re-run if the input parameters are unchanged (however, note that caching doesn't look at ongoing executions: So better to trigger one pipeline execution first and wait a bit for the processing step to complete, rather than triggering ~20 all at once so none of them see a cached processing result and all re-run the job).\n\n...And Pipelines automatically tag SageMaker Experiments config (Pipeline = Experiment; Execution = Trial; Step = Trial Component) which you can then use to plot and compare multiple training jobs in the SM Studio UI. So for example your pipeline might just be Pre-process > Train > Evaluate > RegisterModel. If you right click your pipeline's \"Experiment\" in SMStudio Experiments and Trials view, you can open a list of the executed training jobs and select multiple to scatter-plot the final loss\/accuracy vs the hyperparameters.\n\nIf you run your evaluation as a SageMaker Processing Job which outputs a JSON in model quality metrics format, you can even have your pipeline load the model into SM Model Registry tagged with this data. This way, you'd be able to see and compare the metrics between model versions (and even charts e.g. ROC curves in classification case) through the SMStudio Model Registry UI.\n\nSome relevant code samples:\n\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/main\/sagemaker-pipeline-compare-model-versions\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/main\/sagemaker-pipeline-parameterization",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker endpoint does not use latest approved model version",
        "Question_creation_time":1649605467970,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1qWaaww8TiWKj4MRv9DX2Q\/sagemaker-endpoint-does-not-use-latest-approved-model-version",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Pipelines",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0,
        "Question_view_count":50,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm trying to troubleshoot why the sagemaker staging endpoint is not using the latest model version. I did the following steps:\n\nI started a new Sagemaker project and used the build, train, deploy and monitor model template.\nThe pipeline automatically executes upon creating the project and I approved that model. The endpoint was successfully created using this model version 1.\nI cloned the repo, made a inconsequential change (just changed some training hyper params) and pushed the changes. Pipeline executes again successfully creating model version 2.\nI approved the version 2 and deploy script also runs successfully it seems based on the deploy logs but the endpoint keeps using version 1 of the model. (Also for some reason version 1 of the model appears as \"Model-xxxxxxx\" in the Models list while version 2 appears as \"Pipeline-xxxxxx-xxxxxx\"\n\nWould appreciate any help. Thank you!",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"Unauthorized AWS account racked up charges on stolen credit card.",
        "Question_creation_time":1649524594036,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhV-lkkYyS1qaYFvsoPYiWg\/unauthorized-aws-account-racked-up-charges-on-stolen-credit-card",
        "Question_topic":[
            "Compute",
            "Machine Learning & AI",
            "AWS Well-Architected Framework",
            "Security, Identity, & Compliance"
        ],
        "Question_tag":[
            "Amazon Lightsail",
            "Amazon SageMaker",
            "Security",
            "Support Case",
            "Shared Responsibility Model"
        ],
        "Question_upvote_count":0,
        "Question_view_count":280,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"My mother was automatically signed up for an AWS account or someone used her credentials to sign up. She did not know that she had been signed up, and it sat unused for 3 years. Last month, she got an email from AWS for \"unusual activity\" and she asked me to help her look into it. Someone racked up $800+ in charges in 10 days for AWS services she has never heard of, let alone used (SageMaker, LightSail were among the services). The card on the AWS account is a credit card that was stolen years ago and has since been cancelled. So when AWS tried to charge the card, it didn't go through.\n\nMy experience with AWS customer service has been unhelpful so far. Mom changed her AWS password in time so we could get into the account and contact support. I deleted the instances so that the services incurring charges are now stopped. But now AWS is telling me to put in a \"valid payment method\" or else they will not review the fraudulent bill. They also said that I have to set up additional AWS services (Cost Management, Amazon Cloud Watch, Cloud Trail, WAF, security services) before they'll review the bill. I have clearly explained to them that this entire account is unauthorized and we want to close it ASAP, so adding further services and a payment method doesn't make sense.\n\nWhy am I being told to use more AWS services when my goal is to use zero? Why do I have to set up \"preventative services\" when the issue I'm trying to resolve is a PAST issue of fraud? They also asked me to write back and confirm that we have read and understood the AWS Customer Agreement and shared responsibility model.\" Of course we haven't, because we didn't even know the account existed!\n\nAny advice or input into this situation? It's extremely frustrating to be told that AWS won't even look into the issue unless I set up these additional AWS services and give them a payment method. This is a clear case of identity fraud. We want this account shut down.\n\nSupport Case # is xxxxxxxxxx.\n\nEdit- removed case ID -Ann D",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-10T21:14:09.639Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nFirst, I want to apologize for the wait and the frustration this situation has caused.\n\nWhile we can\u2019t discuss account or case details, via this platform, I can assure you that the issue is currently under investigation. Our team takes these matters seriously and will update you with any action needed\/taken.\n\nPlease continue to address any further concerns or questions, with our teams, via your case.\n\nThank you,\n\nRandi S.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SKLearnProcessor Run() ConnectionTime Out Issue",
        "Question_creation_time":1649441115150,
        "Question_link":"https:\/\/repost.aws\/questions\/QUylzH-GAKRj-vmGib16YyGg\/sk-learn-processor-run-connection-time-out-issue",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":70,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"We ran into the following error when trying to call SageMaker SKlearn Container using SklearnProcess run(). Also the current boto3 version is 1.20.23 ( there is a github post indicated an issue with one boto3 release 1.16 with timeout issue). Looks like a missing permission for STS service? Any other suggestions?\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/httpsession.py in send(self, request) 416 raise ProxyConnectionError(proxy_url=proxy_url, error=e) 417 except URLLib3ConnectTimeoutError as e: --> 418 raise ConnectTimeoutError(endpoint_url=request.url, error=e) 419 except URLLib3ReadTimeoutError as e: 420 raise ReadTimeoutError(endpoint_url=request.url, error=e)\n\nConnectTimeoutError: Connect timeout on endpoint URL: \"https:\/\/sts.us-xxxx-x.amazonaws.com\/\"",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-12T20:13:09.826Z",
                "Answer_upvote_count":0,
                "Answer_body":"Issue resolved - it is network set up issue",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"NoCredentialsError: Unable to locate credentials when using dask_ml inside sagemaker",
        "Question_creation_time":1649439202838,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwsmuXhovR3SWjSp51P0Gtw\/no-credentials-error-unable-to-locate-credentials-when-using-dask-ml-inside-sagemaker",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon Simple Storage Service",
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth"
        ],
        "Question_upvote_count":0,
        "Question_view_count":198,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm trying to use dask in sagemaker because I have over 1B+ rows in a single dataset. Creating a dask.dataframe works fine, when I create a client through dask, it also works:\n\nclient = Client(n_workers=6, threads_per_worker=20)\nclient\n\n\nHowever when I try to use dask_ml.preprocessing.Categorizer, I get the error NoCredentialsError: Unable to locate credentials. I understand the issue might be dask distributed client doesn't have authorization to sagemaker client. Its confusing but how do I have them connect somehow? Code below:\n\nimport dask.dataframe as dd\ndf3 = dd.read_parquet('s3:\/\/bucket\/parquetfiles2\/data_*.parquet',\n                     storage_options={'token': 'anon'})\nif __name__ == \"__main__\":\n    obj_df = df3.select_dtypes(include=['object','datetime64'])\n    num_df = df3.select_dtypes(exclude=['object','datetime64'])\n    ce = Categorizer()\n    obj_df_1 = ce.fit_transform(obj_df)",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-13T07:43:36.091Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nThank you for contacting us and for using Amazon Sagemaker.\n\nI understand that you encountered a \"NoCredentialsError: Unable to locate credentials\" when trying to use dask_ml.preprocessing.Categorizer.\n\nIt looks like you're running the code locally on your machine. Please feel free to correct me if I have misunderstood anything here. The error is usually seen when you don\u2019t have AWS credentials correctly configured on your machine.\n\nHowever, I see you're using Dask Distributed client to use Amazon SageMaker Processing. Please have a look here : https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_processing\/feature_transformation_with_sagemaker_processing_dask\/feature_transformation_with_sagemaker_processing_dask.html#Build-a-Dask-container-for-running-the-preprocessing-job where we have built Dask enabled containers for SageMaker Processing.\n\nYou might need to run aws configure so set up IAM Credentials (User) on your machine. [1] However, to be able to replicate and investigate into this further, we'd need your IAM role arn and other details. Hence, for further investigation on this issue, I recommend to cut a support case and provide more detail about your account information and script\/config. Due to security reason, we cannot discuss account specific issue in the public posts.\n\nPlease open a support case with AWS using the link:\n\nhttps:\/\/console.aws.amazon.com\/support\/home?#\/case\/create\n\nThank you\n\nReferences :\n\n[1] https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/s3-locate-credentials-error\/ [2] https:\/\/youtu.be\/UMUQs2PojdE",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Spot instances for inference and sagemaker?",
        "Question_creation_time":1649350121825,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcQU2DOmNQdyI8HWeIMzzdg\/spot-instances-for-inference-and-sagemaker",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Lambda",
            "Amazon SageMaker",
            "Amazon EC2",
            "Spot Instances"
        ],
        "Question_upvote_count":1,
        "Question_view_count":184,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Is it possible to deploy spot inf1 instances on sagemaker? We run an API 24\/7, and it's costly to keep it up, considering we only have 2 hours of peak performance a day.\n\nWe don't shut off those machines because we might have random bursts of traffic during the day that CPU instances can't hold. Alternatively, we could deploy spot EC2 inf machines; however, I'm unsure how I would invoke them from gateway and lambda. Does anybody have a tip or recommendation for our case?\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-07T23:39:13.030Z",
                "Answer_upvote_count":0,
                "Answer_body":"You could possibly integrate EC2 Spot instance fleet with Application Auto Scaling service to spin up or down spot instances when you receive traffic. To scale it down to 0 instances, you will need to configure a queue to hold the requests while you spin up from 0 instances to 1 or more. Then your application would insert the requests in the SQS queue and wait for an instance to be available. Take a look at this link for more information on how to configure application autoscaling with Spot instances: https:\/\/docs.aws.amazon.com\/autoscaling\/application\/userguide\/services-that-can-integrate-ec2.html\n\nTo configure your policy for the autoscaling, you can look at SQS queue length metric. Here is how you can set a target tracking policy for the application autoscaling: https:\/\/docs.aws.amazon.com\/autoscaling\/application\/userguide\/create-target-tracking-policy-cli.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Exporting Sagemaker model to local computer",
        "Question_creation_time":1649274099076,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKgLZZhWVSg2d5XJWwbaTiA\/exporting-sagemaker-model-to-local-computer",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon S3 Glacier",
            "Build & Train ML Models",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":341,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I hyper-tuned an XGBoost model, deployed the model and created an endpoint. Is there a way to export the model to my local computer? That way I can test the model locally.",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-08T09:11:28.188Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nYou can easily do this, either from the console or using the SageMaker SDK.\n\nFrom the console navigate to:\n\nSageMaker > Training > Training Jobs > Select the training job you wish > Scroll near the bottom and find the Output > click on the S3 link > click on download.\n\nIn case you used SageMaker model tuning and you have a lot of training jobs, you can instead go to\n\nSageMaker > Training > Hyperparameter tuning jobs > Select tuning job > Best training job > click on the name link > this takes you to the training job, > Scroll near the bottom and find the Output > click on the S3 link > click on download.\n\nOnce you have downloaded the model, you can use the open-source xgboost package to load the model and perform predictions as you wish on your local system.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker endpoint creation fails for Multi Model",
        "Question_creation_time":1649256875061,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2bUNsPi3Rgautb0ZycrziA\/sage-maker-endpoint-creation-fails-for-multi-model",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":119,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"When using scikit to create multi model, it throws an exception, but when in single model it works.\n\nComplains about model_fn implementation or ping issues, any tips on how to fix this?\n\ne.g container={\\n\", \" 'Image' : image_uri,\", \" 'Mode': 'MultiModel',\", \" 'ModelDataUrl': 's3:\/\/somepatch\/with\/all\/models\/,\", \" 'Environment': {'SAGEMAKER_SUBMIT_DIRECTORY': mme_artifacts_path,\", \" 'SAGEMAKER_PROGRAM': 'inference.py'} \"\n\nFile \"\/miniconda3\/bin\/serve\", line 8, in <module> sys.exit(serving_entrypoint()) File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/serving.py\", line 144, in serving_entrypoint start_model_server() File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/serving_mms.py\", line 124, in start_model_server modules.import_module(serving_env.module_dir, serving_env.module_name) File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_modules.py\", line 263, in import_module six.reraise(_errors.ImportModuleError, _errors.ImportModuleError(e), sys.exc_info()[2]) File \"\/miniconda3\/lib\/python3.7\/site-packages\/six.py\", line 702, in reraise raise value.with_traceback(tb) File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_modules.py\", line 258, in import_module module = importlib.import_module(name) File \"\/miniconda3\/lib\/python3.7\/importlib\/init.py\", line 118, in import_module if name.startswith('.'):",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-08T09:24:37.439Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nWe would need a bit more information to help you, including a bit more detail on how you create the model (code wise) as well as more details on the errors you receive.\n\nI would also suggest first trying out one of the available examples for multi model endpoints, like this one: Amazon SageMaker Multi-Model Endpoints using Scikit Learn and from there modify to your own needs.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"IncompleteSignature error while using Sklearn SDK",
        "Question_creation_time":1649218784916,
        "Question_link":"https:\/\/repost.aws\/questions\/QUtDHnpTdiTE66ph6OV5GDzA\/incomplete-signature-error-while-using-sklearn-sdk",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Model Training"
        ],
        "Question_upvote_count":0,
        "Question_view_count":24,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Currently, we are trying to SK-Learn model from a python script running in a local computer by uploading data to S3 bucket.\n\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\n# container = retrieve(framework='sklearn', region='us-east-1', version=\"0.23-1\")\ncontainer = sagemaker.image_uris.get_training_image_uri('us-east-1', 'sklearn', framework_version='0.23-1') \nsklearn_estimator = SKLearn(\n    entry_point=\"script.py\",\n    # # role=get_execution_role(),\n    role = role_aws,\n    instance_count=1,\n    instance_type=\"ml.m5.4xlarge\",\n    framework_version=FRAMEWORK_VERSION,\n    base_job_name=\"rf-scikit\",\n    metric_definitions=[{\"Name\": \"median-AE\", \"Regex\": \"AE-at-50th-percentile: ([0-9.]+).*$\"}],\n    hyperparameters={\n        \"n-estimators\": 100,\n        \"min-samples-leaf\": 3,\n        \"features\": \"MedInc HouseAge AveRooms AveBedrms Population AveOccup Latitude Longitude\",\n        \"target\": \"target\",\n    },\n    sagemaker_session=session,\n    image_uri=container,\n    image_uri_region='us-east-1',\n    # output_path=model_output_path,\n)\n# launch training job, with asynchronous call\npath_train_test = 's3:\/\/'+bucket_name+'\/'+prefix\nsklearn_estimator.fit({\"train\": path_train_test, \"test\": path_train_test}, wait=False)\n\n\n'ClientError: An error occurred (IncompleteSignature) when calling the GetCallerIdentity operation: Credential must have exactly 5 slash-delimited elements, e.g. keyid\/date\/region\/service\/term, got 'https:\/\/elasticmapreduce.us-east-1b.amazonaws.com\/\/20220406\/us-east-1\/sts\/aws4_request' The access key and the secret key are passed through the session object via a client and passed to the SK-Learn estimator.\n\nclient_sagemaker = boto3.client('sagemaker', \n                  aws_access_key_id=accesskey , \n                  aws_secret_access_key=access_secret,\n                  )\nsession = sagemaker.Session(sagemaker_client =client_sagemaker )\n\n\nThe same access key worked for Xgboost model (already available in sagemaker) Any ideas about the reason ?",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"can sagemaker batch transform process input files with new line character ?",
        "Question_creation_time":1649208896916,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrZKVALwvQjCcoBn4bYUayg\/can-sagemaker-batch-transform-process-input-files-with-new-line-character",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":168,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"example provided in the aws documentation , https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html, see sample input csv can be structured like a sample below. is there a link, to see the actual csv file that is used in an working example , instead of the sample posted in the docs. it is also mentioned that each record is one per line and no end of line character is allowed. what if the input type is other than csv, like json? do the same rules apply to json files as well, one record per line, or can one record be spread across multiple lines ,what about new line character in that case?\n\nRecord1-Attribute1, Record1-Attribute2, Record1-Attribute3, ..., Record1-AttributeM\nRecord2-Attribute1, Record2-Attribute2, Record2-Attribute3, ..., Record2-AttributeM\n...\n...",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-11T17:27:14.573Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yes, you will need to specify SplitType parameter ( Reference)\n\n   Split_type=\"Line\",",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"how to configure ideal value for MaxConcurrentTransforms in setting up a sagemaker batch transform ?",
        "Question_creation_time":1649205572690,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKcUBF0wPQSyerTP2IK53hQ\/how-to-configure-ideal-value-for-max-concurrent-transforms-in-setting-up-a-sagemaker-batch-transform",
        "Question_topic":[
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "High Performance Compute",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":152,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"based on the documentation , https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html, it states that \" The ideal value for MaxConcurrentTransforms is equal to the number of compute workers in the batch transform job.\" how to figure out what the number of compute workers is , i assume this depends on the instance type. also what about the instance count parameter we can set , do we have to take that into account as well?",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-06T12:08:37.948Z",
                "Answer_upvote_count":0,
                "Answer_body":"The ideal value for MaxConcurrentTransforms varies based on instance type as well as based on your specific model.\n\nit could make sense to increase MaxConcurrentTransforms up to the core count of the instance you are using (for cpu based transform), however, you should also take into account the memory utilisation by your model.\n\nThe ultimate answer is it \"depends\" and I would recommend that you experiment with increasing this number gradually from 1 up to instance core count, while monitoring RAM\/cpu utilisation to find the optimal.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Issue: AttributeError: 'AioClientCreator' object has no attribute '_register_lazy_block_unknown_fips_pseudo_regions'",
        "Question_creation_time":1649181155664,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1RBibWGuQSymNwJfXkln6A\/sage-maker-issue-attribute-error-aio-client-creator-object-has-no-attribute-register-lazy-block-unknown-fips-pseudo-regions",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":427,
        "Question_answer_count":3,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi, there\n\nWhen training model using sagemaker and calling panda.read_csv().. we are getting the above error. It appears that this is a known issue and the workaround is to upgrade aiobotocore to the latest 2.20 release. However, the missing link is where to upgrade the aiobotocore .. since we are running model against SageMaker and its fully managed M5 instance... instead of an EC2 instance. Any thoughts?\n\nError message in full: \/opt\/conda\/lib\/python3.7\/site-packages\/aiobotocore\/client.py in create_client(self, service_name, region_name, is_secure, endpoint_url, verify, credentials, scoped_config, api_version, client_config) 43 service_client, endpoint_url, client_config 44 ) ---> 45 self._register_lazy_block_unknown_fips_pseudo_regions(service_client) 46 return service_client 47\n\nAttributeError: 'AioClientCreator' object has no attribute '_register_lazy_block_unknown_fips_pseudo_regions'\n\nSee notes below under this link: https:\/\/github.com\/boto\/botocore\/pull\/2558\/files#",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-07T16:55:34.954Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello, Thank you for contacting us.\n\nI understand that you encountered an AttributeError code and you are looking for where to upgrade the aiobotocore.\n=== Error message === Error: AttributeError: 'AioClientCreator' object has no attribute '_register_lazy_block_unknown_fips_pseudo_regions'\n\nAs you know, Attribute errors in Python are generally raised when an invalid attribute reference is made. It would seem to me that the issue is caused by a change in dependency library here : https:\/\/github.com\/boto\/botocore\/issues\/2568\n\nif the aiobotocore latest 2.20 release did not work, other option is to downgrade its version that is working properly. I did some test around both Studio domains and notebooks . The default Kernal that spins up currently for me had the issue replicable and had these versions of the library installed : aiobotocore 2.0.1 boto 2.49.0 boto3 1.20.23 botocore 1.23.23\n\nTo make the code work, I downgraded the aiobotocore version. The \"1.3.0\" version worked for me : pip install aiobotocore==1.3.0. I was able to make it with keeping the existing botocode version the same. Would you please try this? === Test steps ====\n\nimport pandas df = pandas.read_csv(\"s3:\/\/testdata\/Sales\/year='2010'\/month='feb'\/day='2'\/IOS.csv\") .... AttributeError: 'AioClientCreator' object has no attribute '_register_lazy_block_unknown_fips_pseudo_regions' pip list | grep boto* aiobotocore 2.0.1 boto 2.49.0 boto3 1.20.23 botocore 1.23.23 pip install aiobotocore==1.3.0 ... Succeeded pip list | grep boto* aiobotocore 1.3.0 boto 2.49.0 boto3 1.20.23 botocore 1.20.49 import pandas df = pandas.read_csv(\"s3:\/\/testdata\/Sales\/year='2010'\/month='feb'\/day='2'\/IOS.csv\") ... No error\n\nIf that still does not work, would you please cut a support case with your script? Due to security reason, this post is not suitable for sharing customer's resource.\n\nThank you.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-04-21T15:56:38.384Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks for the feedback. We opened a support ticket and was able to identify a workaround. Thank you. We were told that SageMaker team is aware of this issue and is working on a fix.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-04-08T17:59:51.201Z",
                "Answer_upvote_count":0,
                "Answer_body":"While trying pip install aiobotocore==1.3.0, I ve got:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/secretstorage\/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n\/opt\/conda\/lib\/python3.7\/site-packages\/secretstorage\/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nCollecting aiobotocore==1.3.0\n  Using cached aiobotocore-1.3.0.tar.gz (48 kB)\n  Preparing metadata (setup.py) ... error\n  error: subprocess-exited-with-error\n  \n  \u00d7 python setup.py egg_info did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [20 lines of output]\n      Traceback (most recent call last):\n        File \"<string>\", line 36, in <module>\n        File \"<pip-setuptools-caller>\", line 34, in <module>\n        File \"\/tmp\/pip-install-t0w2jms3\/aiobotocore_f1bc89a0a84449de976bab0e94ab3cb8\/setup.py\", line 68, in <module>\n          include_package_data=True\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/__init__.py\", line 87, in setup\n          return distutils.core.setup(**attrs)\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/core.py\", line 109, in setup\n          _setup_distribution = dist = klass(attrs)\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 466, in __init__\n          for k, v in attrs.items()\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/dist.py\", line 293, in __init__\n          self.finalize_options()\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 885, in finalize_options\n          for ep in sorted(loaded, key=by_order):\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 884, in <lambda>\n          loaded = map(lambda e: e.load(), filtered)\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_vendor\/importlib_metadata\/__init__.py\", line 196, in load\n          return functools.reduce(getattr, attrs, module)\n      AttributeError: type object 'Distribution' has no attribute '_finalize_feature_opts'\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\n\u00d7 Encountered error while generating package metadata.\n\u2570\u2500> See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.\nNote: you may need to restart the kernel to use updated packages.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Is it possible to use Lambda functions along with other services to scale up and scale down(probably to 0 instances) Ec2 Deployed apps",
        "Question_creation_time":1649176043559,
        "Question_link":"https:\/\/repost.aws\/questions\/QUl2PMA3JZRU2QmN7_knI7fQ\/is-it-possible-to-use-lambda-functions-along-with-other-services-to-scale-up-and-scale-down-probably-to-0-instances-ec-2-deployed-apps",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Developer Tools",
            "Machine Learning & AI",
            "Application Integration"
        ],
        "Question_tag":[
            "AWS Lambda",
            "AWS CodeDeploy",
            "Amazon SageMaker",
            "Amazon EC2",
            "Amazon EventBridge"
        ],
        "Question_upvote_count":0,
        "Question_view_count":125,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi there, hope you are fine. Recently I came across Sagemaker Async inference API, there we can scale down even to 0 instances. What I want is that I deploy my solution to EC2 instances using FastAPI, uvicorn and Celery or Rabbit-MQ(as message broker, for queuing). Then I can scale up and scale down instances based on traffic. Also, if that's not the case, then I keep a minimal CPU instance on always and based on that I scale up and scale down GPU instances for handling requests.\n\nThanks , for any help.\n\nBest Regards Muhammad Ali",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-05T17:51:22.995Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, I guess everything is possible in computers world and your case too. But it will take a lot of effort to do it and will not be a best practice. I have not seen somebody scale ec2's with lambda.\n\nI guess all you need is to scale ec2s based on messages in queue (very easy with sqs) or just created a load balancer or use autoscaling group https:\/\/docs.aws.amazon.com\/autoscaling\/ec2\/userguide\/what-is-amazon-ec2-auto-scaling.html\n\nRegards Denys",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-04-21T16:50:07.121Z",
                "Answer_upvote_count":0,
                "Answer_body":"Scaling with EC2 instances with Lambdas is a common pattern. There are a few options available to assist with this. 1\/ Leverage Systems Manager to Start\/Stop EC2 instances on a schedule. If you want to minimize the cost of EC2 during off hours, say at night and weekends, this is a simple solution. 2\/ Cloudwatch events. You can set up CloudWatch events that trigger based upon a cron or usage of EC2. If using cron, that would trigger at a certain interval, say once a day, to run a Lambda that will shut down the EC2 instance. There are several examples you can search online that shows the code for that Lambda. If you want to have more flexibility, using CloudWatch telemetry, such as CPU utilization, to trigger a shutdown, or even start new instances. 3\/ An auto-scaling group is a great way to do the same. You will set up the parameters for scale up\/scale down, and the auto-scaling group will manage that for you. 4\/ If you want to manage the size of all the services as an entire platform, then using containers managed by Fargate can be a solution. --You can combine these solutions to reduce consumption based upon your needs. Let's say you configure an auto-scaling group that will run during your work hours that has a minimum size of 1. At night and weekends, you have a cron that shuts down the auto-scaling group during off hours. An hour or two before work hours, a cron runs to start the auto-scaling group so you have warm instances ready to go.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to automate sagemaker batch transform?",
        "Question_creation_time":1649085888036,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyENAstk3Q_--wYwScAIq-A\/how-to-automate-sagemaker-batch-transform",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS CloudFormation",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_upvote_count":0,
        "Question_view_count":546,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"does cloudformation support sagemaker batch transform? if yes, can the jobs be triggered\/run automatically once the stack is created?",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-04T16:24:21.868Z",
                "Answer_upvote_count":1,
                "Answer_body":"While CloudFormation doesn't currently offer a resource for a SageMaker Batch Transform (resource list here in the docs), there are plenty of other integration points to automate running these jobs.\n\nCloudFormation\n\nI'd actually argue that CloudFormation is probably not a great fit for this anyway because CloudFormation defines resources which can be created, updated, and deleted. I could maybe see a correspondence between \"Create\" = \"Run a job\", maybe \"Delete\" = \"Delete job outputs\", and possibly \"Update\" = \"Re-run the job\"? But these are opinionated choices that might not make sense in every case.\n\nIf you really wanted, you could create a Custom CloudFormation resource backed by an AWS Lambda function using the CreateTransformJob API (via whatever language you prefer e.g. boto3 in Python).\n\nNote that:\n\nIf you wanted to use the SageMaker Python SDK (import sagemaker, Transformer, etc) instead of the low-level boto3 interface in Python - you'd need to install this extra library in your Lambda function. Tools like AWS SAM and CDK can help with this.\nThe maximum Lambda timeout is 15 minutes, you may not want to keep your Lambda function running (billable) just waiting for the transform to complete anyway, and even the overall Custom Resource will have a longer max timeout within which it must stabilize after a create\/update\/delete request... So additional orchestration may be required beyond a single synchronous Lambda function call.\nOther (better?) options\n\nAs mentioned above, you can create, describe and stop SageMaker Batch Transform jobs from any environment where you're able to call AWS APIs \/ use AWS SDKs... And you can even use the high-level open-source sagemaker SDK from anywhere you install it. Interesting options might include:\n\nAmazon SageMaker Pipelines: SageMaker Pipelines have native \"steps\" for a range of SageMaker processes, including transform jobs but also training, pre-processing and more. You can define a multi-step pipeline from the SageMaker Python SDK (in your notebook or elsewhere) and then start it running on-demand (with parameters) by calling the StartPipelineExecution API.\nAWS Step Functions: Step Functions provides general-purpose serverless orchestration so while the orchestration for SageMaker jobs in particular might be a little more complex (one step to start the job, then a polling check to check wait for completion) - the visual workflow editor and range of integrations to other services may be useful.\nAmazon S3 Lambda integrations can trigger an event automatically (to start your transform job) when new data is uploaded to Amazon S3.\nScheduled EventBridge Rules can run actions on a regular schedule (such as calling Lambda functions, kicking off these pipelines, etc) - in case you need a schedule-based execution rather than in response to some event.\n\nThe choice will depend on what the initial trigger for your workflow would be (schedule? Data upload? Some other AWS event? An API call from outside AWS?) and what other steps need to be orchestrated as well as your transform job in the overall flow.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Is it possible to use smddp in notebook?",
        "Question_creation_time":1649002671694,
        "Question_link":"https:\/\/repost.aws\/questions\/QUselhJbg7SAShCfx3-8WU9Q\/is-it-possible-to-use-smddp-in-notebook",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Studio Lab",
            "Amazon SageMaker Model Training"
        ],
        "Question_upvote_count":0,
        "Question_view_count":30,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I recently tried the smddp v1.4.0 on SageMaker notebook instance (not sagemaker studio), using 8-GPU instances ml.p3.16xlarge, by directly using smddp as backend in the training scripts. I launched the estimator by setting instance_type to local_gpu and ended up with smddp error. Corresponding errors are attached below, saying an initialization error.\n\n42u1m0wni0-algo-1-36bbw | Traceback (most recent call last):\n42u1m0wni0-algo-1-36bbw |   File \"true_main_notebook.py\", line 636, in <module>\n42u1m0wni0-algo-1-36bbw | main()\n42u1m0wni0-algo-1-36bbw |   File \"true_main_notebook.py\", line 178, in main\n42u1m0wni0-algo-1-36bbw | dist.init_process_group(backend=args.dist_backend)\n42u1m0wni0-algo-1-36bbw |   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/distributed\/distributed_c10d.py\", line 576, in init_process_group\n42u1m0wni0-algo-1-36bbw | store, rank, world_size = next(rendezvous_iterator)\n42u1m0wni0-algo-1-36bbw |   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/distributed\/rendezvous.py\", line 219, in _env_rendezvous_handler\n42u1m0wni0-algo-1-36bbw | rank = int(_get_env_or_raise(\"RANK\"))\n42u1m0wni0-algo-1-36bbw |   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/distributed\/rendezvous.py\", line 203, in _get_env_or_raise\n42u1m0wni0-algo-1-36bbw |     raise _env_error(env_var)\n42u1m0wni0-algo-1-36bbw | ValueError: Error initializing torch.distributed using env:\/\/ rendezvous: environment variable RANK expected, but not set\n42u1m0wni0-algo-1-36bbw | Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n42u1m0wni0-algo-1-36bbw | Running smdistributed.dataparallel v1.4.0\n42u1m0wni0-algo-1-36bbw | Error in atexit._run_exitfuncs:\n42u1m0wni0-algo-1-36bbw | Traceback (most recent call last):\n42u1m0wni0-algo-1-36bbw |   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/smdistributed\/dataparallel\/torch\/torch_smddp\/__init__.py\", line 51, in at_exit_smddp\n42u1m0wni0-algo-1-36bbw | hm.shutdown()\n42u1m0wni0-algo-1-36bbw | RuntimeError: Was this script started with smddprun? For more info on using smddprun, run smddprun -h\n42u1m0wni0-algo-1-36bbw | 2022-04-03 16:07:30,005 sagemaker-training-toolkit ERROR    Reporting training FAILURE\n42u1m0wni0-algo-1-36bbw | 2022-04-03 16:07:30,005 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\n42u1m0wni0-algo-1-36bbw | ExitCode 1\n42u1m0wni0-algo-1-36bbw | ErrorMessage \"ValueError: Error initializing torch.distributed using env:\/\/ rendezvous: environment variable RANK expected, but not set\n42u1m0wni0-algo-1-36bbw |  Environment variable SAGEMAKER_INSTANCE_TYPE is not set Error in atexit._run_exitfuncs: Traceback (most recent call last):   File \"\/opt\/conda\/lib\/python3.8\/site-packages\/smdistributed\/dataparallel\/torch\/torch_smddp\/__init__.py\", line 51, in at_exit_smddp hm.shutdown() RuntimeError: Was this script started with smddprun? For more info on using smddprun, run smddprun -h\"\n\n\nThe original goal is to launch a single-node smddp for debugging.\n\nDoes the smddp only support launched by AWS python SDK rather than the notebook? Or if something I've done is not correct?",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"Sagemaker studio doesn't load and returns internal failure",
        "Question_creation_time":1648852294743,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQ_rnD4H_REKOnpPfMp8hMg\/sagemaker-studio-doesnt-load-and-returns-internal-failure",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":393,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Trying to open sagemaker studio and keep getting the following error message:\n\nThe JupyterServer app default encountered a problem and was stopped. If you continue to experience issues, please contact Customer Service. Details: InternalFailure\n\nHave restarted the app for couple of times and always the error message is the same with no progress. Any help would be appreciated.",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-04T20:19:30.248Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nThank you for using Amazon SageMaker Studio.\n\nUsually this kind of error is occurred due to internal issues when trying to launch the Studio where the Sagemaker Service is trying to create default JupyterServer App on your behalf. To identify the root cause of this issue, we will need to get the details about the SageMaker Studio domain id or arn along with the user profile name on which the error was encountered. For further investigation on this issue, I'd recommend you to open a case with SageMaker Premium Support team so that you can share above mentioned details securely.\n\nOpen a support case with AWS using the link:\n\nhttps:\/\/console.aws.amazon.com\/support\/home?#\/case\/create",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"how to configure max concurrent transforms and instance count parameter in batch transform ?",
        "Question_creation_time":1648673976146,
        "Question_link":"https:\/\/repost.aws\/questions\/QUWFExTH6UQsaefQYrJ0OHaQ\/how-to-configure-max-concurrent-transforms-and-instance-count-parameter-in-batch-transform",
        "Question_topic":[
            "Management & Governance",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Auto Scaling",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":261,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"configuring a batch job for inference in sagemaker ( sample code below) . for my use case, there are multiple input files and i'm trying to configure such that it can process files one by one or if the instance type allows process them in parallel. if value of max concurrrent transforms is greater than 1, does it processes file in parallel. as things are predefined for the job , before it starts , for exampel the instance_count is already set to 1. how does it do horizontal scaling? can it add add more instances. does the value of instance type or instance count dictate what value we can configure for max concurrent transforms parameter?\n\nfrom sagemaker.transformer import Transformer\n\ntransformer = Transformer(model_name='my-previously-trained-model',\n                          max_concurrent_transforms=0\n                          instance_count=1,\n                          instance_type='ml.m4.xlarge')",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-30T22:19:41.091Z",
                "Answer_upvote_count":1,
                "Answer_body":"To add to it, you can set the BatchStrategy to MultiLine in order to speed up the processing. General guideline is - number of workers\/instances is a multiple of number of files in S3 to process. If MaxConcurrentTransforms is set to 0 or left unset, Amazon SageMaker checks the optional execution-parameters to determine the settings for your chosen algorithm",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-03-30T22:09:38.927Z",
                "Answer_upvote_count":0,
                "Answer_body":"It partitions the Amazon S3 objects in the input by key. Please checkout https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Is OrdinalEncoder on Scikit Learn available on AWS SageMaker SciKit Learn?",
        "Question_creation_time":1648655583589,
        "Question_link":"https:\/\/repost.aws\/questions\/QUnRh2iyYuTYK_EXlYYF6QoQ\/is-ordinal-encoder-on-scikit-learn-available-on-aws-sage-maker-sci-kit-learn",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":56,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"We'd like to use SKLearn.Preprocessing.OrdinalEncoder. However, it seems that the AWS SKLearn package version SageMaker is one that older than the SKLearn release 1.0.2 that have this encoding available. Any recommendations how to work around it?",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-31T19:28:33.747Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can try building your own container - https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/training\/bring_your_own_container.html For ref sagemaker-scikit-learn-container - https:\/\/github.com\/aws\/sagemaker-scikit-learn-container",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-03-31T22:12:12.484Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello\n\nThank you for reaching out to us.\n\nI understand that you would like to use the SKLearn.Preprocessing.OrdinalEncode with the SKLearn release 1.0.2 that have this encoding available. Please correct me If I am missing anything.\n\nThe Scikit-learn versions supported by the Amazon SageMaker Scikit-learn container: 0.20.0, 0.23-1.[1] However you can bring your own docker container with the latest version and the required packages with the Amazon SageMaker.[2]\n\nFor more details please refer to our public examples[4]\n\nReference:\n\n[1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sklearn.html\n\n[2] https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/training\/bring_your_own_container.html\n\n[3] https:\/\/github.com\/aws\/sagemaker-scikit-learn-extension\/blob\/2412131311433addbae9f6ad5aa393a8bdbbe61f\/src\/sagemaker_sklearn_extension\/preprocessing\/encoders.py#L449\n\n[4] https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/main\/advanced_functionality\/scikit_bring_your_own",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How does sagmaker batch inference processes individual files?",
        "Question_creation_time":1648600758921,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKlZAPl2oRtGHPnpQrPcPXA\/how-does-sagmaker-batch-inference-processes-individual-files",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":67,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"based on the documentation provided here , https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html, large dataset can be structured as shown below in a csv file. is it possible to have multiple files in this format for batch inference, is there any configuration that can be set , for it to process multiple files. Also, what other formats , beside csv can the batch inference handle?\n\nRecord1-Attribute1, Record1-Attribute2, Record1-Attribute3, ..., Record1-AttributeM\nRecord2-Attribute1, Record2-Attribute2, Record2-Attribute3, ..., Record2-AttributeM\n...\n...\nRecordN-Attribute1, RecordN-Attribute2, RecordN-Attribute3, ..., RecordN-AttributeM",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-30T22:23:41.588Z",
                "Answer_upvote_count":0,
                "Answer_body":"If you have multiple files in S3 bucket for Batch Inference, general guidelines is set the number of workers\/instances = multiple of number of files in S3 to process. In addition, you can set the BatchStrategy to MultiLine in order to speed up the processing. To enable parallel processing, set the MaxConcurrentTransforms to 0 to start off, Amazon SageMaker checks the optional execution-parameters to determine the settings for your chosen algorithm.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Invoking endpoint outputs empty prediction data",
        "Question_creation_time":1648559198447,
        "Question_link":"https:\/\/repost.aws\/questions\/QU9EktnduxRgmYc0dYxPBD4Q\/invoking-endpoint-outputs-empty-prediction-data",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Pipelines",
            "Amazon SageMaker Model Training"
        ],
        "Question_upvote_count":0,
        "Question_view_count":87,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello,\n\nI am able to invoke my endpoint using the following command template:\n\naws --profile \u2018insert_profile_name\u2019 sagemaker-runtime invoke-endpoint --endpoint-name 'insert_endpoint_name' --body fileb:\/\/'insert_image_file_path' --region \u2018insert_region\u2019 --content-type application\/x-image output.txt\n\nHowever, this produces an output text file that contains the following:\n\n{prediction\": []}\n\nAlso, this appears in the terminal after running the command:\n\n{ \"ContentType\": \"application\/json\", \"InvokedProductionVariant\": \"variant-name-1\" }\n\nThe image I used to invoke my endpoint was also used for training the model.\n\nHere is my training job configuration (values that I've modified or added):\n\nJob Settings:\n\nAlgorithm - Object Detection | Input Mode - Pipe\n\nHyperparameters:\n\nnum_classes - 1 | mini_batch_size - 1 | num_training_samples - 1\n\nInput data configuration:\n\nFirst channel:\n\nName - validation | Input Mode - Pipe | Content Type - application\/x-recordio | Record Wrapper - RecordIO | S3 Data Type - AugmentedManifestFile | Attribute Names - source-ref, bounding-box\n\nSecond channel:\n\nName - train | Input Mode - Pipe | Content Type - application\/x-recordio | Record Wrapper - RecordIO | S3 Data Type - AugmentedManifestFile | Attribute Names - source-ref, bounding-box\n\nAny help would be appreciated. I can provide more information if needed. Thanks!",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-30T20:55:50.167Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello\n\nThank you for contacting us.\n\nI understand that you are using the SageMaker Object Detection Algorithm for training and hosting your model and having issues while Invoking endpoint outputs empty prediction data, Please correct me If I am missing anything here.\n\nFrom the Invocation script I see that you are using content-type as application\/x-image, from the documentation I see that the you can Query a trained model by using the model's endpoint and The endpoint takes .jpg and .png image formats with image\/jpeg and image\/png as content-types.[1]\n\nI would request you to test it out with the ContentType=\"image\/jpeg\" and let us know if you are still facing the issue. As an alternative you can test using the sagemaker runtime API call to invoke the endpoint.[2]\n\nendpoint_response = runtime.invoke_endpoint(EndpointName=ep, ContentType=\"image\/jpeg\", Body=b)\n\nFor more details please refer to our GitHub example[2]\n\nIf you still have difficulties, I recommend to cut a support case and provide more detail about your account information and script\/config. Due to security reason, we cannot discuss account specific issue in the public posts.\n\nThank you\n\nReferences: [1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection-in-formats.html\n\n[2] https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/object_detection_birds\/object_detection_birds.ipynb",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Trying Sagemaker example but getting error: AttributeError: module 'sagemaker' has no attribute 'create_transform_job'",
        "Question_creation_time":1648494191878,
        "Question_link":"https:\/\/repost.aws\/questions\/QUj8LepwTyQkq0ABgtX-nfew\/trying-sagemaker-example-but-getting-error-attribute-error-module-sagemaker-has-no-attribute-create-transform-job",
        "Question_topic":[
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "High Performance Compute",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Batch",
            "Amazon SageMaker Ground Truth"
        ],
        "Question_upvote_count":0,
        "Question_view_count":260,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi, I keep getting this error: AttributeError: module 'sagemaker' has no attribute 'create_transform_job', when using a batch transform example that AWS graciously had in the notebook instances. Code: ***Also, I updated Sagemaker to the newest package and its still not working.\n\n%%time\nimport time\nfrom time import gmtime, strftime\n\nbatch_job_name = \"Batch-Transform-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\ninput_location = \"s3:\/\/{}\/{}\/batch\/{}\".format(\n    bucket, prefix, batch_file\n)  # use input data without ID column\noutput_location = \"s3:\/\/{}\/{}\/output\/{}\".format(bucket, prefix, batch_job_name)\n\nrequest = {\n    \"TransformJobName\": batch_job_name,\n    \"ModelName\": 'xgboost-parquet-example-training-2022-03-28-16-02-31-model',\n    \"TransformOutput\": {\n        \"S3OutputPath\": output_location,\n        \"Accept\": \"text\/csv\",\n        \"AssembleWith\": \"Line\",\n    },\n    \"TransformInput\": {\n        \"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": input_location}},\n        \"ContentType\": \"text\/csv\",\n        \"SplitType\": \"Line\",\n        \"CompressionType\": \"None\",\n    },\n    \"TransformResources\": {\"InstanceType\": \"ml.m4.xlarge\", \"InstanceCount\": 1},\n}\n\nsagemaker.create_transform_job(**request)\nprint(\"Created Transform job with name: \", batch_job_name)\n\n# Wait until the job finishes\ntry:\n    sagemaker.get_waiter(\"transform_job_completed_or_stopped\").wait(TransformJobName=batch_job_name)\nfinally:\n    response = sagemaker.describe_transform_job(TransformJobName=batch_job_name)\n    status = response[\"TransformJobStatus\"]\n    print(\"Transform job ended with status: \" + status)\n    if status == \"Failed\":\n        message = response[\"FailureReason\"]\n        print(\"Transform failed with the following error: {}\".format(message))\n        raise Exception(\"Transform job failed\")\n\n\nEverything else is working well. I've had no luck with this on anyother forum.",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-28T21:45:21.581Z",
                "Answer_upvote_count":1,
                "Answer_body":"Please double check what type is sagemaker object. Check out this example\n\nsagemaker = boto3.client(service_name=\"sagemaker\") sagemaker.create_transform_job(...)",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"In Sagemaker endpoint, pip download fails when connected through VPC",
        "Question_creation_time":1648489585321,
        "Question_link":"https:\/\/repost.aws\/questions\/QU7Q7e634rRAidb_GUu2ZhXw\/in-sagemaker-endpoint-pip-download-fails-when-connected-through-vpc",
        "Question_topic":[
            "Networking & Content Delivery",
            "Machine Learning & AI",
            "AWS Well-Architected Framework"
        ],
        "Question_tag":[
            "Amazon VPC",
            "Amazon SageMaker",
            "Networking & Content Delivery",
            "Security",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0,
        "Question_view_count":90,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Pip download fails in my instance when sagemaker is connected through VPC. It is successful when VPC is not specified. I have internet gateway configured for my public subnets. I was able to pip download successfully in EC2 with same security groups and subnets. Is this any bug in sagemaker side?\n\nAnd also, can we log into EC2 instance that sagemaker created? It is more of like a blackbox testing without it.",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-05T02:01:23.429Z",
                "Answer_upvote_count":0,
                "Answer_body":"When specifying a VPC config, ENIs are launched into your VPC which your Endpoint will use to communicate within your VPC. These ENIs have private IPs. In order to communicate with the internet you would need to make use of a NAT. SageMaker Endpoints are managed and it is currently not possible to SSH into the EC2 instance(s) backing your Endpoint.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker XGBoost Parquet Example Code Fails and Errors out. Bug?",
        "Question_creation_time":1648146766576,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqqbIbodsT42efRxxi1FLzw\/sage-maker-xg-boost-parquet-example-code-fails-and-errors-out-bug",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Model Building"
        ],
        "Question_upvote_count":0,
        "Question_view_count":117,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi, I'm trying to run the SageMaker XGBoost Parquet example linked here. I followed the exact same steps but using my own data. I uploaded my data, converted it to a pandas df. The train_df shape is (15279798, 32) while the test_df shape is (150848, 32). I then converted it to parquet files and uploaded it to an S3 bucket - per example instructions.\n\nMy error is as follows:\n\nFailure reason\nAlgorithmError: framework error: Traceback (most recent call last): File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_xgboost_container\/data_utils.py\", line 422, in _get_parquet_dmatrix_pipe_mode data = np.vstack(examples) File \"<__array_function__ internals>\", line 6, in vstack File \"\/miniconda3\/lib\/python3.7\/site-packages\/numpy\/core\/shape_base.py\", line 283, in vstack return _nx.concatenate(arrs, 0) File \"<__array_function__ internals>\", line 6, in concatenate ValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 32 and the array at index 1 has size 9 During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_trainer.py\", line 84, in train entrypoint() File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_xgboost_container\/training.py\", line 94, in main train(framework.tr\n\n\n\nBut I'm confused because the train and test are the same shape and I added no extra code. My code below:\n\n# requires PyArrow installed\ntrain.to_parquet(\"Xgb_train.parquet\")\ntest.to_parquet(\"Xgb_test.parquet\")\n\n%%time\nsagemaker.Session().upload_data(\n    \"Xgb_train.parquet\", bucket=bucket, key_prefix=prefix + \"\/\" + \"Ptrain\"\n)\n\nsagemaker.Session().upload_data(\n    \"Xgb_test.parquet\", bucket=bucket, key_prefix=prefix + \"\/\" + \"Ptest\"\n)\n\ncontainer = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-2\")\n\n%%time\nimport time\nfrom time import gmtime, strftime\n\njob_name = \"xgboost-parquet-example-training-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nprint(\"Training job\", job_name)\n\n# Ensure that the training and validation data folders generated above are reflected in the \"InputDataConfig\" parameter below.\n\ncreate_training_params = {\n    \"AlgorithmSpecification\": {\"TrainingImage\": container, \"TrainingInputMode\": \"Pipe\"},\n    \"RoleArn\": role,\n    \"OutputDataConfig\": {\"S3OutputPath\": bucket_path + \"\/\" + prefix + \"\/single-xgboost\"},\n    \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.m5.2xlarge\", \"VolumeSizeInGB\": 20},\n    \"TrainingJobName\": job_name,\n    \"HyperParameters\": {\n        \"max_depth\": \"5\",\n        \"eta\": \"0.2\",\n        \"gamma\": \"4\",\n        \"min_child_weight\": \"6\",\n        \"subsample\": \"0.7\",\n        \"objective\": \"reg:linear\",\n        \"num_round\": \"10\",\n        \"verbosity\": \"2\",\n    },\n    \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 3600},\n    \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": bucket_path + \"\/\" + prefix + \"\/Ptrain\",\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                }\n            },\n            \"ContentType\": \"application\/x-parquet\",\n            \"CompressionType\": \"None\",\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": bucket_path + \"\/\" + prefix + \"\/Ptest\",\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                }\n            },\n            \"ContentType\": \"application\/x-parquet\",\n            \"CompressionType\": \"None\",\n        },\n    ],\n}\n\n\nclient = boto3.client(\"sagemaker\", region_name=region)\nclient.create_training_job(**create_training_params)\nprint(client)\nstatus = client.describe_training_job(TrainingJobName=job_name)[\"TrainingJobStatus\"]\nprint(status)\nwhile status != \"Completed\" and status != \"Failed\":\n    time.sleep(60)\n    status = client.describe_training_job(TrainingJobName=job_name)[\"TrainingJobStatus\"]\n    print(status)",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-24T19:11:38.277Z",
                "Answer_upvote_count":0,
                "Answer_body":"I just changed my bucket name and file names. It worked now.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Async Inference not able to process later requests",
        "Question_creation_time":1648126119561,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqAl1qUyYRK-cbY3DGH-X9g\/async-inference-not-able-to-process-later-requests",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":172,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi there, hope all of you are fine.\n\nI am trying to deploy a train-on-inference type model. I am done with BYOC, and it is working completely fine with real-time inference endpoints. Also, I am able to make it work with Async inference, and concurrent requests on the same instance are also being handled. But, the later requests, never get processed, without any logical error. Also once the endpoint gets scaled down to 0 instance, it fails to scales up.\n\nThese are some of error and warning messages which I get intermittently:\n\n\n\ndata-log:\n2022-03-23T11:23:17.723:[sagemaker logs] [5ea751c9-9271-4533-bc09-c117791e1372] Received server error (500) from primary with message \"<!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\">\n\n\n\nwarnings:\n\/usr\/local\/lib\/python3.8\/dist-packages\/numpy\/core\/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n  setattr(self, word, getattr(machar, word).flat[0])\n\n\nKindly help me with this. Thanks.",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-29T06:30:15.076Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hello, I'm running into the exact same issue. I used the same guide and the async endpoint doesn't scale up or down.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-10-14T14:12:50.944Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, hope you are fine. Thanks for getting back to me. This is what I am using:\n\n\n# Configure Autoscaling on asynchronous endpoint down to zero instances\nresponse = client.register_scalable_target(\n    ServiceNamespace=\"sagemaker\",\n    ResourceId=resource_id,\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n    MinCapacity=0,\n    MaxCapacity=4,\n)\n\nresponse = client.put_scaling_policy(\n    PolicyName=\"Invocations-ScalingPolicy\",\n    ServiceNamespace=\"sagemaker\",  # The namespace of the AWS service that provides the resource.\n    ResourceId=resource_id,  # Endpoint name\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",  # SageMaker supports only Instance Count\n    PolicyType=\"TargetTrackingScaling\",  # 'StepScaling'|'TargetTrackingScaling'\n    TargetTrackingScalingPolicyConfiguration={\n        \"TargetValue\": 2.0,  # The target value for the metric. - here the metric is - SageMakerVariantInvocationsPerInstance\n        \"CustomizedMetricSpecification\": {\n            \"MetricName\": \"ApproximateBacklogSizePerInstance\",\n            \"Namespace\": \"AWS\/SageMaker\",\n            \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": endpoint_name}],\n            \"Statistic\": \"Average\",\n        },\n        \"ScaleInCooldown\": 300,  # The cooldown period helps you prevent your Auto Scaling group from launching or terminating\n        # additional instances before the effects of previous activities are visible.\n        # You can configure the length of time based on your instance startup time or other application needs.\n        # ScaleInCooldown - The amount of time, in seconds, after a scale in activity completes before another scale in activity can start.\n        \"ScaleOutCooldown\": 300  # ScaleOutCooldown - The amount of time, in seconds, after a scale out activity completes before another scale out activity can start.\n        # 'DisableScaleIn': True|False - ndicates whether scale in by the target tracking policy is disabled.\n        # If the value is true , scale in is disabled and the target tracking policy won't remove capacity from the scalable resource.\n    },\n)",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Training Metric logging on SageMaker experiment tracking: how to get time-series metrics with visualisation",
        "Question_creation_time":1648058261935,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDNp9HXW9SCqdadORoXUX9g\/training-metric-logging-on-sage-maker-experiment-tracking-how-to-get-time-series-metrics-with-visualisation",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon CloudWatch"
        ],
        "Question_upvote_count":0,
        "Question_view_count":702,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I am using the sagemaker python SDK to train a bespoke model. I have defined my metric_definition regexes and passed them to the estimator like:\n\nnum_re = \"([0-9\\\\.]+)(e-?[[01][0-9])?\"\nmetrics = [\n    {\"Name\": \"learning-rate\", \"Regex\": f\"lr: {num_re}\"},\n    {\"Name\": \"training:loss\", \"Regex\": f\"loss: {num_re}\"},\n    # ...\n]\nestimator = Estimator(\n    image_uri=training_image_uri,\n    # ...\n    metric_definitions=metrics,\n    enable_sagemaker_metrics=True,\n)\n\nWhen I run training, these metrics are visible in my logs and I can also see them in SageMaker Studio in Trial Components > Metrics (tab) as a grid of numbers like:\n\nName | Minimum | Maximum | Standard Deviation | Average | Count | Final value\n\nlearning-rate | 8.889 | 8.907 | 0.010392304845413657 | 8.898 | 4 |8.907\n\n...\n\nWhich suggests that the regexes are correctly matching on the logs\n\nHowever, I am not able to visualise any graphs for my metrics. I have tried all of:\n\nSagemaker Studio > Trial components > charts. It is only possible to plot things like learning-rate_min (i.e. a point value not a time-series metric)\nSageMaker aws console > training > training jobs > <select job> > Scroll to Monitor section. Here I can see metrics like CPUUtilization over time but for my metrics there is just an empty graph for each metric that I have defined that says 'No data available'\nSageMaker aws console > training > training jobs > <select job> > Scroll to Monitor section > View algorithm metrics (opens in CloudWatch) > Browse > select metric (e.g. learning-rate and 'Add to Graph' . I filter by the correct time period and go the Graphed metrics (1) tab, even after updating the period to 1 second I am not able to see anything on the graph.\n\nI'm not sure what the issue is here but any help would be much appreciated",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-25T21:34:45.463Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello, Thank you for contacting us.\n\nI understand you are not able to visualize any graphs for your metrics, even though you see them in \"Trial Components > Metrics (tab)\".\n\nSageMaker parses the Cloudwatch logs for your training job and emits metrics from the parsed logs as defined in the metrics_definition. The Cloudwatch logs for your training job depends on your train.py script. For example, If you wish to have metrics per step (or per 100 steps), your script needs to print the metrics per step (or per 100 steps) so that it is there in the Cloudwatch logs. Please see the documentation linked below for more information.\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/training-metrics.html#define-train-metrics\n\nAnd I am providing you with examples from our official AWS Github Repository in [1] and [2] which provides an Entry Script which emits custom metrics for a Hyper-parameter Tuning Job. Please compare them with your script. [1] https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/hyperparameter_tuning\/tensorflow2_mnist [2] https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/hyperparameter_tuning\/tensorflow_mnist\n\nIf you still have difficulties, I recommend to cut a support case and provide more detail about your account information and script\/config. Due to security reason, we cannot discuss account specific issue in the public posts.\n\nThank you.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-03-29T12:22:40.592Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi - this is the OP:\n\nThanks for your response. Yes I should have stated in the original question: I am logging these metrics to the console every iteration and can see them in view logs in the console. The issue is that I'm not able to:\n\nview the parsed metrics for the period (I can only see the mean\/max\/min\/...)\nget visualisations of these metrics",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"how to use custom_attribute in sagemaker api?",
        "Question_creation_time":1647897988473,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyzl7vjQGSVGdbnkxv8HKxA\/how-to-use-custom-attribute-in-sagemaker-api",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":1,
        "Question_view_count":231,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"based on documentation provided here -> https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker-runtime\/invoke-endpoint-async.html I am passing a custom attributes parameter when calling the invoke-endpoint-async function.\n\n  invoke-endpoint-async\n--endpoint-name <value>\n[--custom-attributes <value>]\n[--inference-id <value>]\n--input-location <value>\n\n\n\nare there any sample\/example on how can i read this in my code before\/after invoking my model for inference.\n\nI am creating a preprocessing file and have input_fn and predict_fn function, can value passed in custom_attributes during api call be read or written to the response here? also, if it can be written to the response like documentation says, where can i see it . as the async endpoint only gives the output location when invoked and later process the request, i don't see the response, i just see the dumped out file in the specified output location. how can i see the full response?\n\nsagemaker_model = TensorFlowModel(model_data = 'model_data_path',\nrole = execution_role,\nframework_version = '1.12',\nsource_dir ='src',\nentry_point = 'preprocessing.py', \n...)\n\n\nfile: preprocessing.py\n\ndef input_fn(request_body, content_type):\n    \/\/read the custom attribute here",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-30T18:45:29.679Z",
                "Answer_upvote_count":0,
                "Answer_body":"Additionally to AWS CLI, you can pass CustomAttributes parameter during invocation of endpoint with boto3-client or SM SDK:\n\nIn case of boto3: runtime_client.invoke_endpoint(CustomAttributes=json.dumps({key1:val1, key2:val2, ...}))\nIn case of SM SDK: predictor.predict(payload, initial_args={'CustomAttributes': json.dumps({key1:val1, key2:val2, ...})})\n\nAnd then you could parse the context-arg in the input-handlers of preprocessing.py, as for example:\n\ndef handler(data, context):\n    processed_input = _process_input(data, context)\n    custom_attrs = json.loads(context.custom_attributes)\n    # logic to parse \/ enact custom attrs ...\n    response = requests.post(context.rest_uri, data=processed_input)\n    return _process_output(response, context)\n\n\nSee also this post: https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-runtime-now-supports-the-customattribute-header\/",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Can AWS Batch jobs use SageMaker FastFile mode?",
        "Question_creation_time":1647671103679,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzNeAEdKTQCKDfDKqGcZojw\/can-aws-batch-jobs-use-sage-maker-fast-file-mode",
        "Question_topic":[
            "Machine Learning & AI",
            "Compute"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Batch"
        ],
        "Question_upvote_count":0,
        "Question_view_count":108,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Can a regular (non-machine learning) Python job submitted via AWS Batch make use of SageMaker FastFile mode to stream data from S3 to the container? Or is there any equivalent of SageMaker FastFile mode in other Python libraries? The key point is about whether it's possible to avoid the need to copy data from S3 to the EBS or instance store volume before processing.",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-31T19:29:39.663Z",
                "Answer_upvote_count":1,
                "Answer_body":"Don't think FastFile mode is available outside of SageMaker Training. Depending on your use-case\/trade-offs, some of the options are\n\nUse SageMaker Training as your processing host\nCould try FSx\/EFS",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Bad RMSE when predicting Price with Linear Regression",
        "Question_creation_time":1647526960709,
        "Question_link":"https:\/\/repost.aws\/questions\/QUly8ruHWYTCy_MEaBQfz4ZA\/bad-rmse-when-predicting-price-with-linear-regression",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Autopilot"
        ],
        "Question_upvote_count":0,
        "Question_view_count":30,
        "Question_answer_count":3,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi. I have a dataset of price data. It looks like this\n\nPrice\tBranch\tItemCode\tDiscount\tDateTimeOfPrice\n10\t002\t52345436\t0.33\t2022-03-24 14:00\n\nThe dataset has about 1M records\n\nI feature engineered it in the following way\n\nPrice\tDiscount\tItemCode\tYear\tMonth\tDay\tHour\tBranch1\tBranch2\tBranch3\n10\t0.33\t52345436\t2022\t03\t24\t14\t0\t1\t0\n\nEach component of the DateTimeOfPrice got a separate column We have 3 branches. To avoid the situation when algorithm may think that \"branch\" column is some kind of priority column, I created 3 new column (we have 3 branches). If the item belongs to branch2, the column will get the value 1, if not - it will be 0\n\nI run Linear Learner, XGBoost build-in algorithms and also SageMaker AutoPilot. In all cases I run , the best RMSE was 60 and prediction\/ validation gives sometimes a result which is far from the actual value. I tried also to run XGBoost from the notebook with the following parameters\n\nhyperparams = {\n    \"max_depth\": \"7\",\n    \"eta\": \"0.2\",\n    \"gamma\": \"4\",\n    \"min_child_weight\": \"6\",\n    \"subsample\": \"0.7\",\n    \"objective\": \"reg:squarederror\",\n    \"num_round\": \"100\",\n    \"eval_metric\":\"rmse\",\n    \"verbosity\": \"2\",\n}\n\n\nStill, the RMSE is arround 60.\n\nPlease advice what can be done to improve the mertic and predication",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-23T08:07:24.157Z",
                "Answer_upvote_count":0,
                "Answer_body":"Since I see you have a timestamp field in your data, would it be fair to assume your use case is mainly aimed at forecasting future prices - rather than estimating missing historical prices at different points in time?\n\nIf so, plain tabular regression (Autopilot regression task type) is probably not a good way to tackle this problem as forecasting techniques would work better instead. You could instead explore:\n\nSageMaker Canvas, which offers a forecasting model (see the docs here to make sure your input timestamp is recognised so that Canvas shows you the forecasting option)\nAmazon Forecast, a dedicated managed forecasting service separate from SageMaker",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-03-20T13:16:02.354Z",
                "Answer_upvote_count":0,
                "Answer_body":"I followed you suggestion and used Sagemaker Canvas\n\nI modified the data structure in the following way\n\nItemPrice\tBranch\tDiscount\tItemCode\tPriceDate\nData\tData\tData\tData\tData\nData\tData\tData\tData\tData\n\nI choose ItemCode as \"id\" and \"grouped\" by \"branch\". However the score of the prediction is very poor score 22%\n\nAccording to the analisys the reason is because of the Discount column. So I removed it and run the process again. And the score was even lower :(",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-03-18T07:48:45.405Z",
                "Answer_upvote_count":0,
                "Answer_body":"I suggest before you start to build your algorithm, do a data exploration. Does your data have a seasonality? Some items are just not seasonal.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"AWS StepFunctions - SageMaker's InvokeEndpoint block throws \"validation error\" when fetching parameters for itself inside iterator of Map block",
        "Question_creation_time":1647503861594,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDc1foN9TQhe3OYkkGzCKhQ\/aws-step-functions-sage-makers-invoke-endpoint-block-throws-validation-error-when-fetching-parameters-for-itself-inside-iterator-of-map-block",
        "Question_topic":[
            "Serverless",
            "Application Integration",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "AWS Step Functions",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":110,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I have a state-machine workflow with 3 following states:\n\nscreenshot-of-my-workflow\n\nA 'Pass' block that adds a list of strings(SageMaker endpoint names) to the original input. (this 'Pass' will be replaced by a call to DynamoDB to fetch list in future.)\nUse map to call SageMaker endpoints dictated by the array(or list) from above result.\nSend the result of above 'Map' to a Lambda function and exit the workflow.\n\nHere's the entire workflow in .asl.json, inspired from this aws blog.\n\n{\n  \"Comment\": \"A description of my state machine\",\n  \"StartAt\": \"Pass\",\n  \"States\": {\n    \"Pass\": {\n      \"Type\": \"Pass\",\n      \"Next\": \"InvokeEndpoints\",\n      \"Result\": {\n        \"Endpoints\": [\n          \"sagemaker-endpoint-1\",\n          \"sagemaker-endpoint-2\",\n          \"sagemaker-endpoint-3\"\n        ]\n      },\n      \"ResultPath\": \"$.EndpointList\"\n    },\n    \"InvokeEndpoints\": {\n      \"Type\": \"Map\",\n      \"Next\": \"Post-Processor Lambda\",\n      \"Iterator\": {\n        \"StartAt\": \"InvokeEndpoint\",\n        \"States\": {\n          \"InvokeEndpoint\": {\n            \"Type\": \"Task\",\n            \"End\": true,\n            \"Parameters\": {\n              \"Body\": \"$.InvocationBody\",\n              \"EndpointName\": \"$.EndpointName\"\n            },\n            \"Resource\": \"arn:aws:states:::aws-sdk:sagemakerruntime:invokeEndpoint\",\n            \"ResultPath\": \"$.InvocationResult\"\n          }\n        }\n      },\n      \"ItemsPath\": \"$.EndpointList.Endpoints\",\n      \"MaxConcurrency\": 300,\n      \"Parameters\": {\n        \"InvocationBody.$\": \"$.body.InputData\",\n        \"EndpointName.$\": \"$$.Map.Item.Value\"\n      },\n      \"ResultPath\": \"$.InvocationResults\"\n    },\n    \"Post-Processor Lambda\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::lambda:invoke\",\n      \"Parameters\": {\n        \"Payload.$\": \"$\",\n        \"FunctionName\": \"arn:aws:lambda:<my-region>:<my-account-id>:function:<my-lambda-function-name>:$LATEST\"\n      },\n      \"Retry\": [\n        {\n          \"ErrorEquals\": [\n            \"Lambda.ServiceException\",\n            \"Lambda.AWSLambdaException\",\n            \"Lambda.SdkClientException\"\n          ],\n          \"IntervalSeconds\": 2,\n          \"MaxAttempts\": 6,\n          \"BackoffRate\": 2\n        }\n      ],\n      \"End\": true\n    }\n  }\n}\n\n\nAs can be seen in the workflow, I am iterating over the list from the previous 'Pass' block and mapping those to iterate inside 'Map' block and trying to access the Parameters of 'Map' block inside each iteration. Iteration works fine with number of iterators, but I can't access the Parameters inside the iteration. I get this error:\n\n{\n  \"resourceType\": \"aws-sdk:sagemakerruntime\",\n  \"resource\": \"invokeEndpoint\",\n  \"error\": \"SageMakerRuntime.ValidationErrorException\",\n  \"cause\": \"1 validation error detected: Value '$.EndpointName' at 'endpointName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9])* (Service: SageMakerRuntime, Status Code: 400, Request ID: ed5cad0c-28d9-4913-853b-e5f9ac924444)\"\n}\n\n\nSo, I presume the error is because \"$.EndpointName\" is not being filled with the relevant value. How do I avoid this.\n\nBut, when I open the failed execution and check the InvokeEndpoint block from graph-inspector, input to that is what I expected and above JSON-Paths to fetch the parameters should work, but they don't.\nscreenshot-of-graph-inspector\n\nWhat's causing the error and How do I fix this?",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-17T10:46:07.263Z",
                "Answer_upvote_count":1,
                "Answer_body":"In general (as mentioned here in the parameters doc), you also need to end the parameter name with .$ when using a JSON Path.\n\nIt looks like you're doing that some places in your sample JSON (e.g. \"InvocationBody.$\": \"$.body.InputData\"), but not in others (\"EndpointName\": \"$.EndpointName\"), so I think the reason you're seeing the validation error here is that Step Functions is trying to interpret $.EndpointName as literally the name of the endpoint (which doesn't satisfy ^[a-zA-Z0-9](-*[a-zA-Z0-9])*!)\n\nSo suggest you change to EndpointName.$ and Body.$ in your InvokeEndpoint parameters",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Inspection of algorithm containers for Sagemaker",
        "Question_creation_time":1647409671693,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPDqf4t6_TGajnssgca9qNA\/inspection-of-algorithm-containers-for-sagemaker",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_upvote_count":0,
        "Question_view_count":45,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"We plan to provide a machine learning algorithm via a container image and are concerned about. Is it possible that other parties download the docker image for local inspection?",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-17T21:56:48.936Z",
                "Answer_upvote_count":0,
                "Answer_body":"As long as you have the image pushed to an ECR repository you should be fine, you will want to check access and control permissions to your repository and if you want it in a VPC or not.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-03-21T00:51:15.753Z",
                "Answer_upvote_count":0,
                "Answer_body":"That is, if the container contains for example Python code that is not open source, would it be possible for unrelated entities to obtain the Python sources?\n\nThird parties who can access or download the image will be able to access the internals, i.e. via docker run -it your_secret_image \/bin\/bash. If you are distributing your container for use, you are distributing the contents of the container for access as well.\n\nSee also a similar question asked and answered elsewhere.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Batch Transform Job Failure: Timeout Issue and Job Restarted Unexpectedly",
        "Question_creation_time":1647373610390,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHEfg4SyhSsO3CYaqLfefUQ\/sagemaker-batch-transform-job-failure-timeout-issue-and-job-restarted-unexpectedly",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Elastic Inference"
        ],
        "Question_upvote_count":0,
        "Question_view_count":35,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I am using the batch transform function in SageMaker for the inference of my PyTorch model. I am using the same structure as https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/main\/advanced_functionality\/scikit_bring_your_own\/container. The error is that my job will start multiple times on different workers if I choose multiple workers. Or it will repeat after finish if I choose 1 worker.\n\nI think it should be some errors in timeout setup. I have tried to increase the keepalive_timeout and proxy_read_timeout in the serve file and tried the SAGEMAKER_MODEL_SERVER_TIMEOUT as an environment variable. But nothing worked. Could some one help? Thanks!",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"how to set up autoscaling for async sagemaker endpoint?",
        "Question_creation_time":1646861290947,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjjLx7h0TR0q1KwubwQjU9A\/how-to-set-up-autoscaling-for-async-sagemaker-endpoint",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Management & Governance",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Lambda",
            "AWS Auto Scaling",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":256,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"working with an example documented here -> https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/async-inference\/Async-Inference-Walkthrough.ipynb. I was able to set up the sagemaker model, config and aync endpoint via lambda, now I'm trying to re-create the stack via terraform. based on the documentation on terraform, i was able to set up the model, config and the endpoint but couldn't find how to go about setting up the auto scaling ( sample code below). is this possible?\n\nclient = boto3.client(    \"application-autoscaling\") \nresource_id = (    \"endpoint\/\" + endpoint_name + \"\/variant\/\" + \"variant1\")  \nresponse = client.register_scalable_target(\n    ServiceNamespace=\"sagemaker\",\n    ResourceId=resource_id,\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n    MinCapacity=0,\n    MaxCapacity=5,\n)\nresponse = client.put_scaling_policy(\n    PolicyName=\"Invocations-ScalingPolicy\",\n    ServiceNamespace=\"sagemaker\", \n    ResourceId=resource_id,  # Endpoint name\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",  \n    PolicyType=\"TargetTrackingScaling\",  # 'StepScaling'|'TargetTrackingScaling'\n    TargetTrackingScalingPolicyConfiguration={\n        \"TargetValue\": 5.0,  \nSageMakerVariantInvocationsPerInstance\n        \"CustomizedMetricSpecification\": {\n            \"MetricName\": \"ApproximateBacklogSizePerInstance\",\n            \"Namespace\": \"AWS\/SageMaker\",\n            \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": endpoint_name}],\n            \"Statistic\": \"Average\",\n        },\n        \"ScaleInCooldown\": 600,\n   ....\n    },\n)\n\n\nclean up\n\nresponse = client.deregister_scalable_target(\n    ServiceNamespace='sagemaker',\n    ResourceId='resource_id',\n    ScalableDimension='sagemaker:variant:DesiredInstanceCount'\n)",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-11T17:05:39.186Z",
                "Answer_upvote_count":1,
                "Answer_body":"You will using the regular autoscaling config outlined in the doc here to configure it for the SageMaker Async endpoint. There are no specifics for SageMaker.\n\nFirst, you define the \"aws_appautoscaling_target\" with minimum and maximum capacities. Then go ahead and define your \"TargetTrackingScaling\" in the autoscaling policy",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Can we run a python script in Sagemaker using boto3 from a local machine?",
        "Question_creation_time":1646819726632,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjnuGv6KCRaS9BCxzVgCYyA\/can-we-run-a-python-script-in-sagemaker-using-boto-3-from-a-local-machine",
        "Question_topic":[
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "DevOps"
        ],
        "Question_upvote_count":0,
        "Question_view_count":808,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Here's what I am trying to do: In my application that resides outside aws, I take some user inputs, and trigger scripts that reside inside Sagemaker notebook instance. I am able to start or create a new instance using boto3, and also use lifecycle configuration to run some starter script while the instance turns on. But I want to run multiple scripts in short intervals based on user inputs, so I don't want to restart my instance each time with a new lifecycle configuration script. I am trying to find if there is a way to execute shell commands in sagemaker using boto3 (or any other way).",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-09T10:41:19.588Z",
                "Answer_upvote_count":1,
                "Answer_body":"It should be possible, but it's probably not a great idea...\n\nThis is not really an intended pattern for SageMaker notebooks today, and it's more likely that you should be using SageMaker Processing Jobs to schedule your regular tasks - taking input and output data direct from S3 rather than relying on local notebook storage.\n\nWith that warning out of the way, a hacky solution:\n\nSageMaker notebooks (both Notebook Instances and Studio) are based on Jupyter and thus today more-or-less conform (with some customizations) to Jupyter's client\/server API model, which has both REST and WebSocket\/ZeroMQ aspects. This means as long as you're able to handle authentication, it's possible to interact with the notebooks from a script using the same interfaces your browser would.\n\nThis automation-style solution would proceed as (assuming Python):\n\nUse boto3 and the SageMaker CreatePresignedNotebookInstanceUrl API to create a presigned notebook instance URL (Granting this IAM permission is what allows a User\/Role\/principal to open the notebook)\nUse a stateful HTTP library like requests to request this URL in a session and and save the cookie data set by the response. Fetching the URL logs your client in to Jupyter, and \"your client\" is the session - need to keep it persistent.\nUse the JupyterServer REST APIs for things like opening terminal or notebook sessions, listing available kernels, listing open sessions, etc.\nWhen you have a session open (terminal or notebook), use a WebSocket client library like websocket-client to interact with it (sending commands, receiving results, etc). Remember you'll need to use your same session for authentication.\n\nI think I only have end-to-end examples of this for SMStudio: The deprecated auto-installer of the official SageMaker Studio Auto-Shutdown Extension used to use this method before SMStudio Lifecycle Configuration Scripts became available, and some rough draft PoCs on GitHub explore the notebook side too but always with ref to Studio. However it should be possible for NBIs too with almost the same process: Just need to use the above mentioned API in place of CreatePresignedDomainUrl, and may need to check whether the REST api_base_url needs to be adjusted.\n\nIt might even be possible to use a higher-level solution like the nbclient library if you can get the authentication to work with it - would be interested to hear if anyone does!",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Usage of Sagemaker Processing Job Manifest File",
        "Question_creation_time":1646687511442,
        "Question_link":"https:\/\/repost.aws\/questions\/QUAKFZ3-NLSIuWWSesoqRPRQ\/usage-of-sagemaker-processing-job-manifest-file",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":322,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a processing Job that uses input files saved in different folders of a S3 Bucket and use the Manifest file within the processing Job to copy it to \/opt\/ml\/processing\/input Folder.\n\nThis works perfectly fine when i have all the files in one folder but wont work when they are under the same prefix but under different folders.\n\nFollowing the steps listed in the url https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_S3DataSource.html\n\n[ {\"prefix\": \"s3:\/\/customer_bucket\/some\/prefix\/\"},\n\n\"relative\/path\/to\/custdata-1\",\n\n\"relative\/path\/custdata-2\",\n\n...\n\n\"relative\/path\/custdata-N\"\n\n]\n\nIf i have all the input files in \"relative\/path1\/custdata-1\" The job works fine but if i add another one \"relative\/path2\/custdata-2\", there is no file copied and my script fails with no such file or directory.\n\nAny suggestions or advise on this will be very helpful.",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-10T04:38:42.624Z",
                "Answer_upvote_count":0,
                "Answer_body":"I've used Processing Job ManifestFile inputs successfully in the past with multiple relative folders (for e.g. the \"Extract clean input images\" section of this notebook - sorry for citing a large\/sprawling sample, there are probably simpler ones out there).\n\nNot sure exactly what could be going wrong here so I'll try to describe using the feature as I think of it and hope that helps:\n\nGiven an S3 bucket containing:\n\ns3:\/\/customer_bucket\/some\/prefix\/relative\/path1\/custdata-1\ns3:\/\/customer_bucket\/some\/prefix\/relative\/path2\/custdata-2\n\n\n...and a manifest file like:\n\n[ { \"prefix\":  \"s3:\/\/customer_bucket\/some\/prefix\/\" },\n  \"relative\/path1\/custdata-1\",\n  \"relative\/path2\/custdata-2\"\n]\n\n...for a processing input something like the below (or equivalent if you're using boto3\/etc instead of the SageMaker Python SDK):\n\nProcessingInput(\n    destination=\"\/opt\/ml\/processing\/input\/mycoolinput\",\n    input_name=\"mycoolinput\",\n    s3_data_type=\"ManifestFile\",\n    source=\"s3:\/\/path-to-your-manifest-file\",\n)\n\n...I'd expect your processing job to see files:\n\n\/opt\/ml\/processing\/input\/mycoolinput\/relative\/path1\/custdata-1\n\/opt\/ml\/processing\/input\/mycoolinput\/relative\/path2\/custdata-2\n\n\nSo in this sense it is possible to have files under the same prefix with different subfolders. In the above mentioned sample, the raw_s3uri prefix contains credit card agreement PDFs categorized into folders by bank\/provider - e.g. {raw_s3uri}\/Bank1\/Card1.pdf, {raw_s3uri}\/CreditUnion2\/Disclosures.pdf, etc.\n\nTo my knowledge it's not possible to have multiple { \"prefix\": \"...\" } entries in your manifest, but as I understood it didn't sound like you were trying to do that.\n\nApart from double-checking this overall setup (and maybe using Python os.walk() to recursively print() out the folder contents as your Processing job sees them), the only other thing I could suggest is to check if your S3 object keys have any special characters in them that could be causing issues when mapping to a local filesystem - such as files\/folders with spaces at the end, or characters that aren't usually allowed in filenames?",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Load balancing is not happening on sagemaker batch transform job",
        "Question_creation_time":1646666388010,
        "Question_link":"https:\/\/repost.aws\/questions\/QUniirbKJST5KVrdCgXlPv0w\/load-balancing-is-not-happening-on-sagemaker-batch-transform-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Deployment"
        ],
        "Question_upvote_count":0,
        "Question_view_count":80,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi All,\n\nGreetings!!\n\nWe have two issues in sagemaker batch transform job.\n\nLoad balancing is not happening with two instances even after CPU utilization = 200% and GPU utilization = 81% of single instance and 2nd instance was complete idle.\n\nTransform job arguments:\n\nMaxConcurrentTransforms: 2, MaxPayloadInMB: 1, BatchStrategy: MultiRecord, InstanceCount=2\n\nBatch transform job is failing after 20 minutes without any errors in cloud watch logs but noticed that CPU utilization = 200% and GPU utilization = 81% of single instance and 2nd instance was complete idle.\n\nCould you please have a look?\n\nThanks, Vinayak",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-11T17:46:28.025Z",
                "Answer_upvote_count":0,
                "Answer_body":"\"If you have one input file but initialize multiple compute instances, only one instance processes the input file and the rest of the instances are idle.\" Kindly see this link for more information. I would suggest confirming you have more than one input file.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Setting up data for DeepAR, targets and categories for simultaneous data?",
        "Question_creation_time":1646522787273,
        "Question_link":"https:\/\/repost.aws\/questions\/QUT2YRXdWxS2-ORoyS5CXF7w\/setting-up-data-for-deep-ar-targets-and-categories-for-simultaneous-data",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Deep Learning AMIs",
            "AWS Deep Learning Containers",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":95,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I would like to try out DeepAR for an engineering problem that I have some sensor datasets for, but I am unsure how to set it up for ingestion into DeepAR to get a predictive model.\n\nThe data is essentially the positions, orientations, and a few other timeseries sensor readings of an assortment of objects (animals, in this case, actually) over time. Data is both noisy and sometimes missing.\n\nSo, in this case, there are N individuals and for each individual, there are Z variables of interest per individual. None of the variables are \"static\" (color, size, etc), they are all expected to be time-varying on the same time scale. Ultimately, I would like to try and predict all Z targets for all N individuals.\n\nHow do I set up the timeseries to feed into DeepAR? The premise is that all these individuals are implicitly interacting in the observed space, so all the target values have some interdependence on each other, which is what I would like to see if DeepAR can take into account to make predictions.\n\nShould I be using a category vector of length 2, such that the first cat variable corresponds to the individual, and the second corresponds to one of the variables associated with the individual? Then there would be N*Z targets in my input dataset, each with cat = [ n , z ], where there are N distinct values for n, and z for Z?",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-07T08:20:30.261Z",
                "Answer_upvote_count":1,
                "Answer_body":"\"Yes, but...\"\n\nI agree it sounds like all your N*Z timeseries are prediction targets: You don't know them into the future, so can't provide them as dynamic_feats. Creating each as a target record with a 2D cat encoding the individual n and the variable z would probably be the \"right\" way to submit this data to SageMaker DeepAR per the algorithm docs.\n\nBut I'm not overly optimistic about the success of the DeepAR algorithm for this task... As described in the original paper, it's mainly an auto-regressive model from past target and dynamic_feats to output, with some global conditioning\/encoding on the cat features. It's true there's scope in there to learn correlations\/interactions between series, but I'm not sure whether there'll be enough bandwidth\/capacity in that coupling to learn your inter-individual interactions if those are really dominant.\n\nTo illustrate what I mean at a high level:\n\nIf we were talking about global position of migratory birds over months, I'd guess (only a guess) that basic auto-regression over what time of year it is and where bird A usually hangs out in March could already tell you quite a lot... Maybe bird A and bird B always travel together - this also seems like a manageable pattern for a primarily forecasting-based model.\nIf we were watching pigs bounce around in a small pen over seconds, I'd guess the position and direction of each pig would be tightly coupled. Lots of information about the current location of other pigs would be important for deducing where pig X goes next. Seems more like the domain of dynamic system identification to me.\n\nOf course absolutes are pretty hard to come by in ML and DeepAR+ is pre-built - so could still be worth trying. You might even consider a custom domain model in Amazon Forecast to see if the automatic ensembling and extra algorithms it offers could help out... But might be that there are other modelling approaches out there if the forecasting angle doesn't produce the results you're looking for.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to prevent disassociating SageMaker LifecycleConfig unintentionally",
        "Question_creation_time":1646363723581,
        "Question_link":"https:\/\/repost.aws\/questions\/QUTvkDhX_yQXyW7WpFinO_vA\/how-to-prevent-disassociating-sage-maker-lifecycle-config-unintentionally",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":28,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"When you go to SageMaker Notebook Instance edit screen in AWS Web Console (to change the Instance Type for example), it is sometimes the case that Lifecycle configuration is popped up as No Configuration even though the configuration is actually set earlier. This results in an unintentional disassociation of the LifecycleConfig because it's easy to save the instance change without noticing the change in Lifecycle Config. This is a serious problem for us. I was able to reproduce this issue in Chrome and Firefox (but you need to try several times to repro the issue).\n\nI am in the position of provisioning different cloud resources for the end users and I need a way to systematically prevent this disassociation to happen. I considered applying an IAM policy that denies the update operation containing the change in the LifecycleConfig of notebooks, but there seems no condition key for LifecycleConfig which makes me think this approach isn't feasible.\n\nWhat can I do?\n\nThanks.",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"what are some ways\/alternative to expose sagemaker endpoints as a HTTP \/REST endpoints?",
        "Question_creation_time":1646086454991,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCR-voc-dSKa9Og24XXs_2A\/what-are-some-ways-alternative-to-expose-sagemaker-endpoints-as-a-http-rest-endpoints",
        "Question_topic":[
            "Serverless",
            "Front-End Web & Mobile",
            "Networking & Content Delivery",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon API Gateway",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":1,
        "Question_view_count":159,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am testing out serverless sagemaker endpoints and was planning to integrate it with api gateway directly, but realized there is a 29 seconds timeout limit in api gateway, which might not work if the endpoints take longer than that for inference. is there any workaround for this apart from adding a lambda in-between? I am trying to avoid lambda as it might add to the latency",
        "Answers":[
            {
                "Answer_creation_date":"2022-06-08T07:51:00.726Z",
                "Answer_upvote_count":0,
                "Answer_body":"Here is a blog post about adding a API Gateway in front of a SageMaker endpoint: https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/ How long does your model take for inference right now? if your model is slower than you expected, you might want to choose a larger instance type or use a GPU instance for models that can use GPU. Take a look at https:\/\/github.com\/aws-samples\/aws-marketplace-machine-learning\/blob\/master\/right_size_your_sagemaker_endpoints\/Right-sizing%20your%20Amazon%20SageMaker%20Endpoints.ipynb\n\nYou can also consider using async inference. When the API Gateway receives a request, trigger a async inference job and return immediately. Then let the endpoint write the result to a S3 bucket, then notify your user either by SNS -> Email or through a polling API etc.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker - All metrics in statistics.json by Model Quality Monitor are \"0.0 +\/- 0.0\", but confusion matrix is built correctly for multi-class classification!!",
        "Question_creation_time":1645965956086,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOOz6SJnzR7-VDglJ1rAW8Q\/sage-maker-all-metrics-in-statistics-json-by-model-quality-monitor-are-0-0-0-0-but-confusion-matrix-is-built-correctly-for-multi-class-classification",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Monitoring"
        ],
        "Question_upvote_count":1,
        "Question_view_count":27,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I have scheduled an hourly model-quality-monitoring job in AWS SageMaker. both the jobs, ground-truth-merge and model-quality-monitoring completes successfully without any errors. but, all the metrics calculated by the job are \"0.0 +\/- 0.0\" while the confustion matrix gets calculated as expected.\n\nI have done everything as mentioned in this notebook for model-quality-monitoring from sagemaker-examples with very few changes and they are:\n\nI have changed the model from xgboost churn to model trained on my data.\nmy input to the endpoint was csv like in the example-notebook, but output was json.\ni have changed the problem-type from BinaryClassfication to MulticlassClassification wherever necessary.\n\nconfustion matrix was built successfully, but all metrics are 0 for some reason. So, I would like the monitoring job to calculate the multi-classification metrics on data properly.\n\nAll Logs\n\nHere's the statistics.json file that model-quality-monitor saved to S3 with confustion matrix built, but with 0s in all the metrics:\n\n{\n  \"version\" : 0.0,\n  \"dataset\" : {\n    \"item_count\" : 4432,\n    \"start_time\" : \"2022-02-23T03:00:00Z\",\n    \"end_time\" : \"2022-02-23T04:00:00Z\",\n    \"evaluation_time\" : \"2022-02-23T04:13:20.193Z\"\n  },\n  \"multiclass_classification_metrics\" : {\n    \"confusion_matrix\" : {\n      \"0\" : {\n        \"0\" : 709,\n        \"2\" : 530,\n        \"1\" : 247\n      },\n      \"2\" : {\n        \"0\" : 718,\n        \"2\" : 497,\n        \"1\" : 265\n      },\n      \"1\" : {\n        \"0\" : 700,\n        \"2\" : 509,\n        \"1\" : 257\n      }\n    },\n    \"accuracy\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"weighted_recall\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"weighted_precision\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"weighted_f0_5\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"weighted_f1\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"weighted_f2\" : {\n      \"value\" : 0.0,\n      \"standard_deviation\" : 0.0\n    },\n    \"accuracy_best_constant_classifier\" : {\n      \"value\" : 0.3352888086642599,\n      \"standard_deviation\" : 0.003252410977346705\n    },\n    \"weighted_recall_best_constant_classifier\" : {\n      \"value\" : 0.3352888086642599,\n      \"standard_deviation\" : 0.003252410977346705\n    },\n    \"weighted_precision_best_constant_classifier\" : {\n      \"value\" : 0.1124185852154987,\n      \"standard_deviation\" : 0.0021869336610830254\n    },\n    \"weighted_f0_5_best_constant_classifier\" : {\n      \"value\" : 0.12965524348784485,\n      \"standard_deviation\" : 0.0024239410000317335\n    },\n    \"weighted_f1_best_constant_classifier\" : {\n      \"value\" : 0.16838092925822584,\n      \"standard_deviation\" : 0.0028615098045768348\n    },\n    \"weighted_f2_best_constant_classifier\" : {\n      \"value\" : 0.24009212108475822,\n      \"standard_deviation\" : 0.003326031863819311\n    }\n  }\n}\n\n\nHere's how couple of lines of captured data looks like(prettified for readability, but each line has no tab spaces as shown below) :\n\n{\n    \"captureData\": {\n        \"endpointInput\": {\n            \"observedContentType\": \"text\/csv\",\n            \"mode\": \"INPUT\",\n            \"data\": \"0,1,628,210,30\",\n            \"encoding\": \"CSV\"\n        },\n        \"endpointOutput\": {\n            \"observedContentType\": \"application\/json\",\n            \"mode\": \"OUTPUT\",\n            \"data\": \"{\\\"label\\\":\\\"Transfer\\\",\\\"prediction\\\":2,\\\"probabilities\\\":[0.228256680901919,0.0,0.7717433190980809]}\\n\",\n            \"encoding\": \"JSON\"\n        }\n    },\n    \"eventMetadata\": {\n        \"eventId\": \"a7cfba60-39ee-4796-bd85-343dcadef024\",\n        \"inferenceId\": \"5875\",\n        \"inferenceTime\": \"2022-02-23T04:12:51Z\"\n    },\n    \"eventVersion\": \"0\"\n}\n{\n    \"captureData\": {\n        \"endpointInput\": {\n            \"observedContentType\": \"text\/csv\",\n            \"mode\": \"INPUT\",\n            \"data\": \"0,3,628,286,240\",\n            \"encoding\": \"CSV\"\n        },\n        \"endpointOutput\": {\n            \"observedContentType\": \"application\/json\",\n            \"mode\": \"OUTPUT\",\n            \"data\": \"{\\\"label\\\":\\\"Adoption\\\",\\\"prediction\\\":0,\\\"probabilities\\\":[0.99,0.005,0.005]}\\n\",\n            \"encoding\": \"JSON\"\n        }\n    },\n    \"eventMetadata\": {\n        \"eventId\": \"7391ac1e-6d27-4f84-a9ad-9fbd6130498a\",\n        \"inferenceId\": \"5876\",\n        \"inferenceTime\": \"2022-02-23T04:12:51Z\"\n    },\n    \"eventVersion\": \"0\"\n}\n\n\nHere's couple of lines from my ground-truths that I have uploaded to S3 look like(prettified for readability, but each line has no tab spaces as shown below):\n\n{\n  \"groundTruthData\": {\n    \"data\": \"0\",\n    \"encoding\": \"CSV\"\n  },\n  \"eventMetadata\": {\n    \"eventId\": \"1\"\n  },\n  \"eventVersion\": \"0\"\n}\n{\n  \"groundTruthData\": {\n    \"data\": \"1\",\n    \"encoding\": \"CSV\"\n  },\n  \"eventMetadata\": {\n    \"eventId\": \"2\"\n  },\n  \"eventVersion\": \"0\"\n},\n\n\nHere's couple of lines from the ground-truth-merged file look like(prettified for readability, but each line has no tab spaces as shown below). this file is created by the ground-truth-merge job, which is one of the two jobs that model-quality-monitoring schedule runs:\n\n{\n  \"eventVersion\": \"0\",\n  \"groundTruthData\": {\n    \"data\": \"2\",\n    \"encoding\": \"CSV\"\n  },\n  \"captureData\": {\n    \"endpointInput\": {\n      \"data\": \"1,2,1050,37,1095\",\n      \"encoding\": \"CSV\",\n      \"mode\": \"INPUT\",\n      \"observedContentType\": \"text\/csv\"\n    },\n    \"endpointOutput\": {\n      \"data\": \"{\\\"label\\\":\\\"Return_to_owner\\\",\\\"prediction\\\":1,\\\"probabilities\\\":[0.14512373737373732,0.6597074314574313,0.1951688311688311]}\\n\",\n      \"encoding\": \"JSON\",\n      \"mode\": \"OUTPUT\",\n      \"observedContentType\": \"application\/json\"\n    }\n  },\n  \"eventMetadata\": {\n    \"eventId\": \"c9e21f63-05f0-4dec-8f95-b8a1fa3483c1\",\n    \"inferenceId\": \"4432\",\n    \"inferenceTime\": \"2022-02-23T04:00:00Z\"\n  }\n}\n{\n    \"eventVersion\": \"0\",\n    \"groundTruthData\": {\n        \"data\": \"1\",\n        \"encoding\": \"CSV\"\n    },\n    \"captureData\": {\n        \"endpointInput\": {\n            \"data\": \"0,2,628,5,90\",\n            \"encoding\": \"CSV\",\n            \"mode\": \"INPUT\",\n            \"observedContentType\": \"text\/csv\"\n        },\n        \"endpointOutput\": {\n            \"data\": \"{\\\"label\\\":\\\"Adoption\\\",\\\"prediction\\\":0,\\\"probabilities\\\":[0.7029623691085284,0.0,0.29703763089147156]}\\n\",\n            \"encoding\": \"JSON\",\n            \"mode\": \"OUTPUT\",\n            \"observedContentType\": \"application\/json\"\n        }\n    },\n    \"eventMetadata\": {\n        \"eventId\": \"5f1afc30-2ffd-42cf-8f4b-df97f1c86cb1\",\n        \"inferenceId\": \"4433\",\n        \"inferenceTime\": \"2022-02-23T04:00:01Z\"\n    }\n}\n\n\nSince, the confusion matrix was constructed properly, I presume that I fed the data to sagemaker-model-monitor the right-way. But, why are all the metrics 0.0, while confustion-matrix looks as expected?\n\nEDIT 1:\nLogs for the job are available here.",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"MLOps Query regarding SageMaker Project template and Pipelines",
        "Question_creation_time":1645816119455,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbOWLdrrNStmzDzTMOcdG3Q\/ml-ops-query-regarding-sage-maker-project-template-and-pipelines",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "ML Ops with Amazon SageMaker and Kubernetes"
        ],
        "Question_upvote_count":0,
        "Question_view_count":93,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi MLOps Gurus,\n\nI'd like to seek guidance on my below situation.\n\nThis is regarding Sagemaker Project creation in AWS. The use case is to take final model (built by DS team) from S3 and do all sorts of Bias analysis using Clarify and upon acceptance of its response(Bias & Explainability reports) by Data Scientists, deploy to Model Monitor. Now, as there is no prebuilt Sagemaker template that caters to my use case, how shall I initiate this whole process and how shall I create a ML project at first place.\n\nAlso, how shall I initiate a notebook from within my cloudformation template for DS Team to kick off from Service Catalogue to automate the above process?\n\nAny pointers would be greatly appreciated.\n\nRegards, Nikhil",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-27T13:02:12.192Z",
                "Answer_upvote_count":0,
                "Answer_body":"To learn more about building custom SageMaker projects templates, I usually suggest a read of Build Custom SageMaker Project Templates \u2013 Best Practices, and also to check if the use case of interest is already present in this collection of custom templates: Custom Project Templates in SageMaker.\n\nI'm not really sure I understand:\n\nAlso, how shall I initiate a notebook from within my cloudformation template for DS Team to kick off from Service Catalogue to automate the above process?",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"API Gateway Resource Policy Not Working - IP Address Allow List",
        "Question_creation_time":1645811510068,
        "Question_link":"https:\/\/repost.aws\/questions\/QUTc-jSxqlSXCueAqrINQF-Q\/api-gateway-resource-policy-not-working-ip-address-allow-list",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Security, Identity, & Compliance",
            "Front-End Web & Mobile",
            "Networking & Content Delivery",
            "Machine Learning & AI",
            "Storage"
        ],
        "Question_tag":[
            "AWS Lambda",
            "AWS Identity and Access Management",
            "Amazon API Gateway",
            "Amazon SageMaker",
            "presigned URL"
        ],
        "Question_upvote_count":0,
        "Question_view_count":411,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"API Gateway Experts, I want to invoke an API that triggers a lambda function to create a SageMaker instance pre-signed URL. They would like to deny access to the API to only a particular user\u2019s source IP address. I have provide the YAML used to deploy this, but it is still allowing all other IP addresses to interact with the API even with the resource policy. What am I missing?\n\n  ApiGatewayRestApi:\n    Type: AWS::ApiGateway::RestApi\n    Properties:\n      ApiKeySourceType: HEADER\n      Description: An API Gateway with a Lambda Integration\n      EndpointConfiguration:\n        Types:\n          - EDGE\n      Name: lambda-sagemaker-presigned-url-api\n      Policy: !Sub |\n        {\n          \"Version\": \"2012-10-17\",\n          \"Statement\": [\n            {\n              \"Effect\": \"Deny\",\n              \"Principal\": \"*\",\n              \"Action\": \"execute-api:Invoke\",\n              \"Resource\": \"arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:*\/${APIGatewayStageName}\/*\/*\",\n              \"Condition\": {\n                \"NotIpAddress\": {\n                  \"aws:SourceIp\": \"${YourIPAddress}\"\n                }\n              }\n            },\n            {\n              \"Effect\": \"Allow\",\n              \"Principal\": \"*\",\n              \"Action\": \"execute-api:Invoke\",\n              \"Resource\": \"arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:*\/${APIGatewayStageName}\/*\/*\"\n            }\n          ]\n        }",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-28T12:59:34.912Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hey Derek, It looks like the resource you're limiting has a Stage in it. Can you please try the policy with a * in place of ${APIGatewayStageName}?",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"what is the model(transformer) size limitation in sagemaker serverless endpoint deployment?",
        "Question_creation_time":1645726803899,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsU6_idiTQIWu2Poovfpfdw\/what-is-the-model-transformer-size-limitation-in-sagemaker-serverless-endpoint-deployment",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":160,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"is there a limitation on the size of the model that we can create a model and then eventually serverless endpoint? any documentation? I did some research and ran into something similar here. https:\/\/discuss.huggingface.co\/t\/sagemaker-serverless-inference-for-layoutlmv2-model\/14186\/3 i as a solution , it is advised to set MMS_DEFAULT_WORKERS_PER_MODEL=1. I'm not sure what exactly does this do? is there any aws documentation around this?",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-03T15:33:01.342Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hello,\n\nThere is not a hard limit on the size of a model used on a serverless endpoint as such, but there certain limits that you will reach in practice.\n\n1st a serverless endpoint can have a limited amount of RAM memory (6GB) so even with MMS_DEFAULT_WORKERS_PER_MODEL=1, if your model requires more RAM to make a prediction then you will encounter errors.\n\n2nd is that when you deploy the endpoint it needs to be up+running within 180s (3mins). Part of this time includes loading your model into memory. The larger the model the longer it will take to load into the memory. Unfortunately, I don't have any specific benchmarks on how long this might be against size of model.\n\n3rd you need to consider the cold start issue of your endpoint. This is connected to 2nd point but wanted to mention separately as well. Part of the greatness of serverless endpoints is that when not needed they are completely shut down (cold) and upon request they get spun up and make a prediction. The larger the model, the longer it will take for it be loaded into memory and the longer your cold start time will be.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to create a serverless endpoint in sagemaker?",
        "Question_creation_time":1645674841041,
        "Question_link":"https:\/\/repost.aws\/questions\/QULRy50Vd7SW6KT0MMzk4NeQ\/how-to-create-a-serverless-endpoint-in-sagemaker",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":277,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am recreating an endpoint currently working in sagemaker for inference to a serverless endpoint. I am using one of the images ( huggingface-pytorch-inference:1.9.1-transformers4.12.3-cpu-py38-ubuntu20.04) found here -> https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md.\n\neverything works when i choose non serverless, i.e. provisioned option for endpoint configuration , but when i try to create one with serverless option it fails. error messages are below ( from the logs in cloudwatch) . starting with python and log4j error at the end.\n\n'python: can't open file '\/user\/local\/bin\/deep_learning_container.py': [Errno 13] permission denied. Requirement already satisfied: transformers in \/opt\/conda\/lib\/pythong3.6 ..... ..... Warning: MMS is using non-default JVM parameters: -XX: -UseContainersupport Failed to reap children process log4j: ERROR setfile(null,true) call failed. java.io.FileNotFoundException: logs\/mms_log.log (No Such file or directory)\n\nwhy am i getting this error ???\n\nFYI - i have set memory to maximum allowed memory size of 6gb. for the serverless option.",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-24T08:28:43.776Z",
                "Answer_upvote_count":0,
                "Answer_body":"The cause might be that your SageMaker Python SDK is not updated to the latest version. Please make sure you update it to the latest version as well as the AWS SDK for Python (boto3). You can use pip:\n\npip install --upgrade boto3\npip install --upgrade sagemaker\n\n\nFor a sample notebook you can have a look here. More information on the documentation page.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to serve a model in sagemaker?",
        "Question_creation_time":1645625592189,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmmN8fILJTqyJz8b0-MqoXw\/how-to-serve-a-model-in-sagemaker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":102,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"based on documentation provided here , https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#model-directory-structure, the model file saved from training is model.pth. I also read that it can be .pt extension or even bin extension. I have seen a example of pytorch_model.bin, but when i tried to serve the model with the pytorch_model.bin, it warns me that .pt or .pth file needs to exist. has anyone run into this?",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-23T14:42:36.693Z",
                "Answer_upvote_count":1,
                "Answer_body":"Depending on the version of the PyTorch framework container that you're using (since they go back quite a long way now), there may be some differences in what formats the default model loader expects & supports.\n\nIn general, if you're not finding that the default loader works for you, you should also be able to provide a model_fn() implementation (usually in code\/inference.py inside your model.tar.gz, although again the exact specifics here may vary if you're using older versions) to override the behaviour: Loading the model how you want and returning it from the function.\n\nIf you have issues getting your model_fn recognised:\n\nI believe there was an issue affecting specific framework images 1.6.0, 1.7.0 and 1.8.0 where a custom model_fn may be ignored. While the major and minor versions of these framework containers correspond to PyTorch versions, the final build number is for tracking AWS DLC changes... So if you're currently pinning to these specific versions e.g. 1.6.0 and finding this issue, you may find upgrading to a later build or just specifying 1.6 would help.\n\nAlso, when deploying via PyTorchModel you can specify a new source_dir (containing your inference.py) and a new model.tar.gz will be created to add in this code... But when deploying directly from estimator.deploy(), you may need to make sure your training job sets everything up by:\n\nCopying\/creating the required code file(s) into ${SM_MODEL_DIR}\/code (to add the inference code to model.tar.gz)\n(In some versions) making sure your original training script name (e.g. train.py) is also a valid entrypoint under ${SM_MODEL_DIR}\/code and exposes your required functions. This is because in some cases the entrypoint seems to get carried over from training, instead of defaulting to inference.py.\n\nMy usual (lazy) solution to this is to make my PyTorch training jobs recursively copy the whole src folder contents from training into ${SM_MODEL_DIR}\/code (as shown here), and to from inference import * in my training entrypoint script (as shown here). I think I've seen this approach work for at least v1.4-1.8, but it's been a while now since I used the older end of that range.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"What is required in HumanLoopInput.InputContent for start_human_loop",
        "Question_creation_time":1645624147204,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJast4QKSTjCYWe-pcQFyHw\/what-is-required-in-human-loop-input-input-content-for-start-human-loop",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon Textract",
            "Amazon Augmented AI",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":33,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have built a custom key-value extraction workflow that leverages textract Tables and Forms. It does a whole heap of post processing using the output of Textract to extract a small number of highly important fields from documents that are of very poor quality.\n\nMy client would like a human-in-the-loop to make minor changes to the results where certain fields are missing. I think that the sagemaker_a2i_runtime.start_human_loop is the perfect tool for this.\n\nI want to send the human reviewer, the input image and the current Key-Value pairs that I have extracted and have them find any that are missing, or mark them as not there. I have setup and tested the textract.analyse_image workflow with a human reviewer and like the results.\n\nWhat structure and data fields do I need to set in the HumanLoopInput field of sagemaker_a2i_runtime.start_human_loop to get this to work. I assume that it will look something like a dictionary of current K-V pairs and the s3 image file but I cannot find any documentation on how to do this.",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-01T02:18:34.042Z",
                "Answer_upvote_count":0,
                "Answer_body":"The HumanLoopInput field accepts JSON.\n\nhttps:\/\/docs.aws.amazon.com\/augmented-ai\/2019-11-07\/APIReference\/API_HumanLoopInput.html\n\nPlease take a look at the following documentation on how to get started with A2I. Specifically, the Create a Human Loop -> Custom Integration section has an example for \"sagemaker_a2i_runtime.start_human_loop\".\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/a2i-get-started-api.html#a2i-get-started-api-create-human-loop",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SM Elastic Inference Accelerators are not available during inference",
        "Question_creation_time":1645620490604,
        "Question_link":"https:\/\/repost.aws\/questions\/QUi78wfQkWTuaxYOjA8jGZaw\/sm-elastic-inference-accelerators-are-not-available-during-inference",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Elastic Inference"
        ],
        "Question_upvote_count":0,
        "Question_view_count":37,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi Team,\n\nGreetings!!\n\nWe are able to deploy on real-time endpoint with elastic inference accelerators. But SM Elastic Inference Accelerators are not available during inference, could you please have a look?\n\nNote:\n\nWe are bringing our own trained model using Pytorch = 1.10.2 on SM.\nWe don't find any errors in cloud watch logs.\nConverted our trained model to TorchScript and able to load that model as well during inference.\nTried both Torchscript's trace and script modes but no luck.\n\nCode\n\nfrom sagemaker.pytorch import PyTorchModel\n\nfrom sagemaker import get_execution_role\n\nendpoint_name = 'ner-bert'\n\npytorch = PyTorchModel(entry_point='deploy_ei.py', source_dir='code', model_data=model_data, role=get_execution_role(), framework_version='1.3.1', py_version='py3', sagemaker_session=sagemaker_session)\n\npredictor = pytorch.deploy(initial_instance_count=1, instance_type='ml.m5.large', accelerator_type='ml.eia2.xlarge', endpoint_name=endpoint_name)\n\nThanks, Vinayak",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"How to grant permission to create a log file in sagemaker?",
        "Question_creation_time":1645550175217,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1Cfn6H1kQMeuWAhtGctNiA\/how-to-grant-permission-to-create-a-log-file-in-sagemaker",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":111,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I created a serverless endpoint in sagemaker based on the documentation provided. I have my model (artifactory) zipped up and uploaded to s3, created a model and then created an endpoint configuration , then finally created the endpoint\n\nresponse = client.create_endpoint(\n    EndpointName=\"serverless-endpoint\",\n    EndpointConfigName=\"serverless-endpoint-config\"\n)\n\n\non the endpoint invocation, cloudwatch logs show error below - log4j:ERROR setFile(null, true) called failed. java.io.FileNotFoundException: logs\/ts_metrics.log (No such file or directory) at java.base\/java.io.FileOutputStream.open0 (Native Method) at java.base\/java.io.File......\n\nthis looks like permission error , but i'm not sure how to give access\/permission to create file in sagemaker.",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-11T17:58:25.643Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello, does the execution role associated with the model have the proper CloudWatch permissions ?",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Using SageMaker as a backend for a web app",
        "Question_creation_time":1645453626658,
        "Question_link":"https:\/\/repost.aws\/questions\/QUA06r5EiFR0ifBd3KixHHLA\/using-sage-maker-as-a-backend-for-a-web-app",
        "Question_topic":[
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_upvote_count":0,
        "Question_view_count":266,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Continued\/simplified from this post.\n\nLet's say I have a web app UI and I want to connect it to a SageMaker notebook using the Churn Predictor model in SageMaker Sudio. Think of it like using that model hosted in SageMaker as a backend to my web app.\n\nI could create an API endpoint but my web app has a very simple solution for opening a web socket which I'd like to use. Question: Whats the simplest\/best way to deploy the SageMaker notebook so that it keeps the kernel running and connection open?\n\nThe websocket means I don't need to set up and configure individual API endpoints which is what all the documentation I can find suggests.\n\nThe desired workflow is:\n\nConfigure model in SageMaker notebook environment\nAdd a cell that establishes a websocket connection\nExport notebook as a Python script (Maybe?)\nHost & Run Python script on an AWS service\n\nWith the web socket I can simply establish the connection:\n\nanvil.server.connect(app_key)\n\nAnd then give each function that I want my web app to call a decorator. i.e:\n\n@anvil.server.callable\ndef function(foo):\n  return foo",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-22T02:04:42.714Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello, According to the description, I assume you are going to use the Sagemaker notebooks for the development of the inference code and then export the code as a script.\n\nIf you model is light weight, you can cache the model inside you server application and call the predictions in it.\n\nmodel = load_model()\n\n@anvil.server.callable\ndef function(foo):\n  return model.predict(\"\")\n\n\nAlternatively you can create a SageMaker endpoint and inside the same function above you can call the Sagemaker endpoint and pass it back to the client as a response to the web-socket invocation.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Unsupported pytorch version 1.10.0 with SM Elastic Inference Accelerators",
        "Question_creation_time":1645246550155,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-LyE6PRQSbq3mXEouDuGjQ\/unsupported-pytorch-version-1-10-0-with-sm-elastic-inference-accelerators",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Elastic Inference"
        ],
        "Question_upvote_count":0,
        "Question_view_count":39,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi Team,\n\nGreetings!!\n\nWe are not able to deploy on real-time endpoint with elastic inference accelerators. Could you please have a look?\n\nSageMaker version: 2.76.0\n\nCode: from sagemaker.pytorch import PyTorchModel from sagemaker import get_execution_role\n\nendpoint_name = 'ner-bert'\n\nmodel = PyTorchModel(entry_point='deploy_ei.py', source_dir='code', model_data=model_data, role=get_execution_role(), framework_version='1.10.0', py_version='py38')\n\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge', accelerator_type='ml.eia2.medium', endpoint_name=endpoint_name)\n\nError details: Unsupported pytorch version: 1.10.0. You may need to upgrade your SDK version (pip install -U sagemaker) for newer pytorch versions. Supported pytorch version(s): 1.3.1, 1.5.1, 1.3, 1.5.\n\nNote: We are able to deploy without elastic accelerator in above code and want to use Python 3.8 version because we have some dependency libraries which supports only Python 3.8 version.\n\nI looked at \"Available DL containers\" at https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md and by looking at this section \"SageMaker Framework Containers (SM support only)\", SM support PyTorch 1.10.0 with Python 3.8 version.\n\nBut we would like to deploy on Elastic Inference and by looking at this section \"Elastic Inference Containers\" in above URL, EI containers supports only PyTorch 1.5.1 with Python 3.6. Why these containers are so outdated?\n\nWhat could be the solution?\n\nCan we specify the latest version of Python in requirements.txt file and get it installed?\n\nThanks, Vinayak",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"Can I use glue interactive sessions with pythonshell?",
        "Question_creation_time":1645223131500,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjwZtVPYTSCG96fKOGn1k4w\/can-i-use-glue-interactive-sessions-with-pythonshell",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Glue"
        ],
        "Question_upvote_count":0,
        "Question_view_count":138,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Can I use glue interactive sessions with pythonshell?\n\nIn the docs, it hints at choosing the %job_type or is it only available for glueetl. I am looking for a simple way to develop by requiring myself to use a sage maker notebook because it is overkill on half the ETL more of the time. Can I just get the 0.0625 DPU by default with a notebook? And no extras like I get with sagemaker.",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-20T17:42:10.047Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, Glue Interactive Sessions allows you to install Glue PySpark and Glue Scala kernels to any Jupyter Notebook and Glue Studio Notebook is a new Glue Studio experience powered by Interactive Sessions. As such, they are aimed ad interactive Glue ETL (Spark) development.\n\nAs your question is about development with Glue Python Shell jobs, which is the right choice for small data sets, this can be developed with the Glue Studio Python Shell Editor (no DPUs paid before you run the job) or in any Python IDE environment using compatible Python version.\n\nUsing the Glue Studio Python Shell editor you can develop and submit the job to run. For the results you can open the logs directly from the Runs tabs.\n\nAlternatively you could use the legacy script editor, that when you run the PythonShell job in the bottom part of the windows it shows you the logs as the job is running.\n\nhope this helps,",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Notebook stopped working. Cannot access and shows in notebook instances only intermittently.",
        "Question_creation_time":1645192253179,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfLJaDrf8R9KsDSp1QMZ91g\/notebook-stopped-working-cannot-access-and-shows-in-notebook-instances-only-intermittently",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":148,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I was working on a Sagemaker notebook when suddenly it stopped working. I stopped and notebook and started it again, but now I cannot access it, getting the following error:\n\nAccessDeniedException User: arn:aws:sts::755460267215:assumed-role\/voclabs\/user1672114=9540446637 is not authorized to perform: sagemaker:ListNotebookInstances with an explicit deny in an identity-based policy\n\nCan somebody please help? I should note that I am pretty new to the Sagemaker environment.",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-21T09:54:20.420Z",
                "Answer_upvote_count":0,
                "Answer_body":"It looks like the AWS IAM identity that you're logged in to the AWS Console with (going by your error message, the assumed Role voclabs) does not have permission to list SageMaker notebook instances.\n\nIf you use multiple Roles (or a combination of your own IAM User and assumed Roles), you may need to switch roles back to the identity you were previously using to access SageMaker.\n\nIf you always use the same role, it may be that the attached access policies have been changed (reference list of SageMaker IAM actions and resources here). You'll need sagemaker:ListNotebookInstances to list notebook instances in the console and sagemaker:CreatePresignedNotebookInstanceUrl to open Jupyter\/JupyterLab on an instance. From the error message it looks like there's a permission policy attached to your role that's explicitly denying this access. Your administrator should be able to help with this, or can refer to the AWS IAM documentation and IAM service console if you manage the account yourself.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Invoke the SageMaker model endpoint directly from PostMan without Lambda and API gateway setup",
        "Question_creation_time":1645172179513,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2IGM0IeOSb-6elPnkRpd-Q\/invoke-the-sage-maker-model-endpoint-directly-from-post-man-without-lambda-and-api-gateway-setup",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":768,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi All,\n\nGood day!!\n\nHow to invoke the SageMaker model endpoint directly from Postman without Lambda and API gateway setup?\n\nWe have deployed the real-time endpoint on SM and data scientist (has AWS account) would like to invoke that endpoint via Postman from their local machine and test?\n\nMethod: Post\n\nURL: https:\/\/runtime.sagemaker.ap-southeast-1.amazonaws.com\/endpoints\/my-endpoint-name\/invocations\n\nWe know that we can invoke SM endpoint using SM boto3 SDK like below, but we would like to invoke the endpoint directly via Postman.\n\nimport boto3 smr = boto3.client('sagemaker-runtime')\n\nresp = smr.invoke_endpoint(EndpointName=endpoint_name, Body=b'.345,0.224414,.131102,0.042329,.279923,-0.110329,-0.099358,0.0', ContentType='text\/csv')\n\nThanks, Vinayak",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-18T09:20:31.413Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello Vinayak,\n\nSageMaker endpoints are not publicly available and are scoped to an individual account (see note in doc: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpoint.html) therefore you need to use the right credentials in order for you to be able to access the endpoint.\n\nAre you using temporary AWS credentials? If so, you need to set the \"X-Amz-Security-Token\" header in the request. Once set, you should be able to send a POST request.\n\nIn case you have not seen already, I also suggest having a look at the below link from PostMan documentation, and specifically on the part regarding AWS Signature: https:\/\/learning.postman.com\/docs\/sending-requests\/authorization\/#aws-signature\n\nFinally, this thread on aws forums may also help you troubleshoot further: https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=274401\n\nI hope this helps,\n\nGeorgios",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-02-23T14:48:10.097Z",
                "Answer_upvote_count":0,
                "Answer_body":"It is indeed possible to invoke sagemaker endpoints from sagemaker without using any other AWS services and that is also manifested by the fact that they have invocation URLs.\n\nHere's how you set it up:\n\ncreate an user with only programmatic access and attach this policy json looking something like below:\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"sagemaker:InvokeEndpoint\",\n            \"Resource\": \"arn:aws:sagemaker:<region>:<account-id>:endpoint\/<endpoint-name>\"\n        }\n    ]\n} \n\n\nyou can replace <endpoint-name> with * to let this user invoke all endpoints.\n2. use the ACCESS-KEY and SECRET-ACCESS-KEY to configure authorisation in postman like shown in this screenshot. also add the parameters in advanced tab like shown in the screenshot: https:\/\/i.stack.imgur.com\/cYkTf.png\n3. then fill up your body with the relevant content type.\n4. then add or remove additional headers like variant-name or model-name, if you have them set up and the headers should look like shown in this screenshot: https:\/\/i.stack.imgur.com\/NLqkV.png\n5. send the request to receive reponse like this: https:\/\/i.stack.imgur.com\/uA4kF.png\n\nURL and credentials in the above screenshots doesn't work anymore, duh!\n\nIt has actually been a pain in the rear for me to set up code in node.js and dot-net to sign requests(make headers like postman did) to invoke the model is in production. I have put my endpoints behing an API in AWS API Gateway(without Lambda) for invocations for ease of use.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to invoke the SageMaker model endpoint directly from PostMan without Lambda and API gateway setup",
        "Question_creation_time":1645077384113,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-Ef8noXsSt-hPgBubJK88A\/how-to-invoke-the-sage-maker-model-endpoint-directly-from-post-man-without-lambda-and-api-gateway-setup",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":834,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi All,\n\nGood day!!\n\nHow to invoke the SageMaker model endpoint directly from Postman without Lambda and API gateway setup?\n\nWe know that we can invoke SM endpoint using SM boto3 SDK like below, but we would like to invoke the endpoint directly via Postman.\n\nimport boto3 smr = boto3.client('sagemaker-runtime')\n\nresp = smr.invoke_endpoint(EndpointName=endpoint_name, Body=b'.345,0.224414,.131102,0.042329,.279923,-0.110329,-0.099358,0.0', ContentType='text\/csv')\n\nThanks, Vinayak",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-17T07:20:06.174Z",
                "Answer_upvote_count":0,
                "Answer_body":"As I would frame it, there are 2 main things your client needs to directly invoke a SageMaker endpoint via AWS APIs (that it wouldn't need via API Gateway):\n\nCredentials for some kind of AWS IAM identity (for example an IAM User or Role) which is authorized to call that API (via IAM policies)\nTo sign the request via the process expected via AWS APIs, which is more complex than simply passing an auth token\n\nHow you achieve (1) will depend on your actual use case... For example, is this just for an individual developer (who maybe already has an AWS IAM user) testing on their local machine? Are you building an application which will run somewhere you could store AWS credentials? Or are you trying to expose the model out to untrusted clients (such as users' web browsers) where directly sharing AWS credentials to the client would be inappropriate?\n\nSpecifically for individual developer testing, you might refer to the guide on using Amazon Pinpoint with Postman - which goes via an IAM user.\n\nAs you may see from the doc linked above, building (2) from scratch is also non-trivial, which is one good reason why it's usually best to use the AWS SDK for your target language\/environment of choice (many more are available besides Python's boto3) rather than building raw API requests. There are other good reasons too, like simplified configuration for loading your credentials and setting up best practices like retries.\n\nHowever, assuming you validate that you really do have a good use case for Postman here:\n\nThe specific API you want is SageMaker's InvokeEndpoint. On that page you'll find details about the path structure and different supported parameters... And also note that it's under the SageMakerRuntime service which is separate from the SageMaker administrative APIs\nEach service's base endpoint (host\/domain name) is documented online, and SageMaker's are listed here. Since you're looking for the runtime, you'll be targeting something like (https:\/\/) runtime.sagemaker.ap-southeast-1.amazonaws.com\nPostman actually provides native support for AWS request signing: Look for AWS Signature under the Authorization options... So no need to try and manually sign the request content yourself.\n\nPutting it all together, I believe you'll be making a POST request to something like https:\/\/runtime.sagemaker.ap-southeast-1.amazonaws.com\/endpoints\/my-endpoint-name\/invocations. You'll see the other options documented for InvokeEndpoint correspond closely to boto3's invoke_endpoint() method.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-02-17T08:37:29.811Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi @Alext_T,\n\nI have deployed the real-time endpoint on SM and data scientist (has AWS account) would like to invoke that endpoint via Postman from their local machine?\n\nMethod: Post\n\nURL: https:\/\/runtime.sagemaker.ap-southeast-1.amazonaws.com\/endpoints\/my-endpoint-name\/invocations\n\nHow can we achieve that? Please put it in simple words.\n\nThanks, Vinayak",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to define concurrency in SageMaker real-time inference",
        "Question_creation_time":1645076515894,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOW_MzWx4QWaQeIqVYMEyeQ\/how-to-define-concurrency-in-sage-maker-real-time-inference",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":241,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi Team,\n\nGreetings!!\n\nCould you please confirm, how can we define concurrency in SageMaker real-time inference?\n\nThis is how we define concurrency in SageMaker serverless inference.\n\n\u201cServerlessConfig\u201d: { \u201cMemorySizeInMB\u201d: 4096, \u201cMaxConcurrency\u201d: 10, }\n\nThanks, Vinayak",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-11T17:53:44.363Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Vinayak,\n\nMaxConcurrency is a concept for Serverless Endpoints and not real-time Endpoints.\n\nIn terms of a real-time Endpoint, the concurrency is defined by what ever client is making the requests to your Endpoint (InvokeEndpoint API). I.e Assuming your Model container can handle concurrent request, you define the concurrency by Invoking the Endpoint.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to create a serverless endpoint configuration?",
        "Question_creation_time":1645067206226,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfmAxh_aDQiS2nk0gbDicsg\/how-to-create-a-serverless-endpoint-configuration",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":129,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"based on the sample code provided here , https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config\n\nI created a model via lambda, now when i try to create a serverless endpoint config (sample code below) , i keep getting -> parameter validation failed unknown parameter in ProductVariants [ 0 ]: \"ServerlessConfig\", must be one of : VairantName, ModelName, InitialInstanceCount , Instancetype...\n\nresponse = client.create_endpoint_config(\n   EndpointConfigName=\"endpoint-new\",\n   ProductionVariants=[\n        {\n            \"ModelName\": \"MyModel\",\n            \"VariantName\": \"AllTraffic\",\n            \"ServerlessConfig\": {\n                \"MemorySizeInMB\": 2048,\n                \"MaxConcurrency\": 10\n            }\n        } \n    ]\n)",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-17T09:30:12.370Z",
                "Answer_upvote_count":1,
                "Answer_body":"The cause might be that your SageMaker Python SDK is not updated to the latest version. Please make sure you update it to the latest version as well as the AWS SDK for Python (boto3). You can use pip:\n\npip install --upgrade boto3\npip install --upgrade sagemaker\n\n\nFor a sample notebook you can have a look here. More information on the documentation page.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2022-02-17T09:37:24.602Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi,\n\nCan you confirm the version of boto3 that you are using? This error is likely to be caused because of an older version of boto3 that does not include capability for serverless inference.\n\nServerless inference was introduced in version 1.20.18\n\nHope this helps,\n\nGeorgios",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to check\/determine image\/container size for aws managed images ?",
        "Question_creation_time":1645022076554,
        "Question_link":"https:\/\/repost.aws\/questions\/QU35dVp2D9SKKUnnVYGw9Z7A\/how-to-check-determine-image-container-size-for-aws-managed-images",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":1,
        "Question_view_count":108,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm using one of the images listed here https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md, to create an model such that I can tie that up with a sagemaker serverless endpoint , but I keep getting \"failed reason: Image size 15136109518 is greater that suppported size 1073741824\" . this work when the endpoint configuration is not serverless. is there any documentation around image\/container size for aws managed images?",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-16T19:28:16.051Z",
                "Answer_upvote_count":1,
                "Answer_body":"It sounds like you set up a serverless endpoint with 1GB of memory and the image is larger than that. You can increase the memory size of your endpoint with the MemorySizeInMB parameter, more info in this documentation: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config\n\nIf you pick a larger value for that (e.g. 4096 MB) then it should hopefully work.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"If I open a web socket connection with one of my SageMaker's notebook cells how long will the connection last?",
        "Question_creation_time":1644937024950,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-iZHXP55T_6BLKhHPET_0Q\/if-i-open-a-web-socket-connection-with-one-of-my-sage-makers-notebook-cells-how-long-will-the-connection-last",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":164,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"If I open a web socket connection with one of my SageMaker's notebook cells how long will the connection last?\n\nI realise the time frame may not be exact and I cannot expect 100% uptime. It's more about whether there is a set cut off time i.e. every 24hrs the kernel shuts down or restarts.\n\nA secondary question: What is the best way to deploy the notebook as a web service that maintains the web socket connection indefinitely?",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-16T05:53:20.159Z",
                "Answer_upvote_count":1,
                "Answer_body":"SageMaker has a few different notebook environments, so the answer might depend a bit:\n\nIf you're using SageMaker Studio Lab - the new free notebook service not tied to an AWS Account - \"sessions\" are limited in duration and the environment will automatically shut down when your limit expires (currently 4hrs for GPU, 12hrs for CPU)\nIn SageMaker Studio, your running kernel's lifespan is tied to the \"app\" (container) it runs in. If one user opens multiple notebooks on the same kernel type and instance type (e.g. commonly Python 3 (Data Science) and ml.t3.medium), then those notebooks will share a container. Your user's Jupyter server itself runs in a different container.\nIn SageMaker Notebook Instances (NBIs), one \"instance\" runs both the Jupyter server and running notebook kernels (a bit more similar to using Jupyter on local machine).\n\nIn both Studio and NBIs (barring availability interruptions etc), there's usually no automatic restart of your kernels unless you explicitly set one up. Of course, stopping and re-starting your NBIs (and deleting\/re-creating your Studio apps) regularly is a good practice - to consume security and functionality updates.\n\nFor example, I've observed notebooks in both Studio and NBIs successfully maintain their state for ~months without any intervention - so if your running kernel was keeping the websocket connection alive - it could persist. Anecdotally, I think I've also observed the kernel<->notebook UI connection being a bit more stable in Studio than NBIs occasionally... I guess because the more formal separation between kernel and notebook server in separate containers means Jupyter handles broken connections a bit more robustly?\n\nMaybe the second part of your question is the more interesting one: As it sounds like what you're trying to isn't really a great fit for notebook environments anyway. I think the answer here will depend on what exactly you're trying to host\/serve (e.g. a full Jupyter-like notebook experience; Just read-only view of the cell inputs and outputs; something different and more UI-like backed by notebook data; an ML model) so maybe worth raising as a separate question with a few more details?\n\nDepending on what exactly the use case is, you might like to check out SageMaker model deployment, this blog and sample using Streamlit on AWS, or maybe something different.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Optimal notebook instance type for DeepAR in AWS Sagemaker",
        "Question_creation_time":1644862112974,
        "Question_link":"https:\/\/repost.aws\/questions\/QUnYV-WoO2R3KY4sNEq-Dshw\/optimal-notebook-instance-type-for-deep-ar-in-aws-sagemaker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Model Training"
        ],
        "Question_upvote_count":0,
        "Question_view_count":96,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I am currently utilizing an ml.c4.2xlarge instance type for a DeepAR use case to run an Automated Model Tuning job. The data consists of 7157 time series with 152 timesteps in the training set and 52 timesteps in the test set respectively. I estimate the run time for the tuning job on this specific instance type to take about 4-5 days. Looking to find out if DeepAR is engineered to take advantage of GPU computing for training and if it would be advisable to use a 'p' or 'g' compute instance instead for faster results. Also would be great for recommendations as to which Accelerated Computing instance would be optimal for this scenario.",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-15T02:29:47.263Z",
                "Answer_upvote_count":1,
                "Answer_body":"(As detailed further on the algorithm details page), yes, the SageMaker DeepAR algorithm implementation is able to train on GPU-accelerated instances to speed up more challenging jobs. There's also a handy reference table here listing all the SageMaker built-in algorithms and whether they're likely to be accelerated with GPU.\n\nHowever, to be clear, it shouldn't be the notebook instance type that affects this... Typically when training models on SageMaker, the notebook would provide your interactive compute environment but you'd run training in training jobs - for example using the SageMaker Python SDK Estimator class as shown in the sample notebooks for DeepAR electricity and synthetic. The instance type you select for training is independent of the instance type you use for your notebook - for example in the electricity notebook it's set as follows:\n\nestimator = sagemaker.estimator.Estimator(\n    image_uri=image_name,\n    sagemaker_session=sagemaker_session,\n    role=role,\n    train_instance_count=1,  # <-- Setting training instance count\n    train_instance_type=\"ml.c4.2xlarge\",  # <-- Setting training instance type\n    base_job_name=\"deepar-electricity-demo\",\n    output_path=s3_output_path,\n)\n\nSo normally I wouldn't expect you to need to change your notebook instance type to speed up training - just edit the configuration of your training job from within the notebook.\n\nSuggesting a particular type is tricky because DeepAR hyperparameters like context_length, embedding_dimension, and mini_batch_size will affect how much GPU capacity is needed for a particular run. Since you're coming from CPU-only baseline, I'd maybe suggest to start small with trying out single-GPU g4dn.xlarge, g5.xlarge or p3.2xlarge instances, perhaps starting with the lowest cost-per-hour? You can keep an eye on your jobs' GPUUtilization and GPUMemoryUtilization metrics to check whether utilization is low on instances like p3 with \"bigger\" GPUs. Increasing mini_batch_size should help fill extra capacity on these and complete your job faster, but it will probably affect model convergence - so may need to tune other parameters like learning_rate to try and compensate. So considering all of this, you may find trade-offs between speed and total cost, or speed and accuracy, for good hyperparameter combinations on your dataset. Of course you could also scale up to multi-GPU instance types if you'd like to accelerate further.\n\nIf I understood right you're also using SageMaker Automatic Hyperparameter Tuning to search these parameters, something like this XGBoost notebook with the HyperparameterTuner class?\n\nIn that case would also mention:\n\nIncreasing the max_parallel_jobs parameter may accelerate the overall run time (by running more of the individual training jobs in parallel) - with a trade-off on how much information is available when each training job in the budget is kicked off.\nIf you're planning to run this training regularly on a dataset which evolves over time, you probably don't need to run HPO each time: Will likely see good results using your previously-optimized hyperparameters, unless something materially changes in the nature of the data and patterns.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Does the pre and post-processing need to be incorporate in SageMaker?",
        "Question_creation_time":1644776794437,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMhy15ZuZTNeZFajUWrFXbw\/does-the-pre-and-post-processing-need-to-be-incorporate-in-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":85,
        "Question_answer_count":3,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi All,\n\nGreetings!!\n\nPlease address my below queries.\n\nDoes the pre and post-processing need to be incorporate in SM?\nIsn't SM supposed to be used for inference only?\n\nWhy I'm asking these questions because I am using PyTorch model server for pre-processing, predictions and post-processing for NER use cases.\n\nWe use utokenize for creating tokens which supports >= Python 3.8 but PyTorch model server supports <= 3.6, what would be the solution? How to install Python 3.8 in PyTorch model server?\n\nDon't we have basic word piece tokenization as pre-processing in SM?\n\nThanks, Vinayak",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-02T17:25:22.518Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Vinayak\n\nI just want to make sure I understand the context: Are you referring to pre and postprocessing for an inference request or for training a model? I'm asking because SageMaker offers both:\n\nProcessing jobs which are used to process data before training a model: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/processing-job.html\nProcessing data before sending it to an endpoint for inference: https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#write-an-inference-script\n\nCould you help clarify the question?\n\nThanks Heiko",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-02-17T05:57:42.469Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello @Heiko,\n\nI am talking about inference pipeline only.\n\nDon't we have pre and post-processing in-built feature for NER in SM PyTorch or HuggingFace model servers?\n\nWe thought that pre and post-processing taken care by SM and we just need to bring our fine-tuned model on our dataset.\n\nThanks, Vinayak",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-02-15T16:29:03.191Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Vinayak - apologies for the late response, I was OOTO.\n\nYes, you are correct, pre- and post-processing for Huggingface inference is \"built in\" via the SageMaker Hugging Face Inference Toolkit. The documentation on the Github repo shows how this is being done.\n\nHope that helps!\n\nThanks Heiko",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Where should I report to when encounter a trouble at SageMaker Canvas?",
        "Question_creation_time":1644763229468,
        "Question_link":"https:\/\/repost.aws\/questions\/QUpsGPPvV7SbuofE1fnwC5AA\/where-should-i-report-to-when-encounter-a-trouble-at-sage-maker-canvas",
        "Question_topic":[
            "Machine Learning & AI",
            "AWS Well-Architected Framework",
            "Microservices"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Well-Architected Framework",
            "Microservices",
            "Amazon SageMaker Canvas"
        ],
        "Question_upvote_count":0,
        "Question_view_count":63,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"When I was building model for analyzing in SageMaker Canvas, it just run for 1h 2m and then I got this notification:\n\nModel building failed: Failed to run Neo compilation or generate explainability report. client_request_id is f76d5bf7-6780-4257-9631-500101632b1e\n\nWhere should I contact the admin for this issue? Thank you so much!",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-14T18:28:44.414Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Ailee, you can submit a ticket here and engineering will get back to you: https:\/\/t.corp.amazon.com\/create\/templates\/20b56bae-3fca-4281-9b94-69b6e50128cd. Thanks!",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Host a fine-tuned BERT Multilingual model on SageMaker with Serverless inference",
        "Question_creation_time":1644677512763,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIVk2l_3iQ1Ob4kKZq-Qw5A\/host-a-fine-tuned-bert-multilingual-model-on-sage-maker-with-serverless-inference",
        "Question_topic":[
            "Serverless",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Serverless",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Elastic Inference"
        ],
        "Question_upvote_count":0,
        "Question_view_count":88,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi All,\n\nGood day!!\n\nKey point to note here is, we have pre-processing script for the text document, deserialize which is required for prediction then we have post-processing script for generating NER (entitites).\n\nI went through SageMaker material and decided to try following options.\n\nOption 1: Bring our own model, write a inference script and deploy it on SM real-time endpoint using Pytorch container. I went through Suman video (https:\/\/www.youtube.com\/watch?v=D9Qo5OpG4p8) which is really good, need to try with our pre-processing and post-processing scripts then see if it works fine or not.\n\nOption 2: Bring our own model, write a inference script and deploy it on SM real-time endpoint using Huggingface container. I went through Huggingface docs (https:\/\/huggingface.co\/docs\/sagemaker\/inference#deploy-a-%F0%9F%A4%97-transformers-model-trained-in-sagemaker) but there is no reference for how to use own pre and post-processing scripts to setup inference pipeline.\n\nIf you know any examples on using our own pre and post-processing scripts using Huggingface container then please share it.\n\nOption 3: Bring our own model, write a inference script and deploy it on SM Serverless inference\/endpoint using Huggingface container. I went through Julien video (https:\/\/www.youtube.com\/watch?v=cUhDLoBH80o&list=PLJgojBtbsuc0E1JcQheqgHUUThahGXLJT&index=35) which is excellent but he has not shown how to use our own pre and post-processing scripts using Huggingface container.\n\nPlease share if you know any examples.\n\nCould you please help?\n\nThanks, Vinayak",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-15T16:23:39.670Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Vinayak, thanks for opening this thread.\n\nThis is the official documentation and describes how the inference script for pre and postprocessing should be structured: https:\/\/huggingface.co\/docs\/sagemaker\/inference#user-defined-code-and-modules.\n\nAnd here a simple example of an inference script with pre and postprocessing: https:\/\/github.com\/marshmellow77\/text-summarisation-project\/blob\/main\/inference_code\/inference.py\n\nHope that helps, please reach out in case of questions.\n\nThanks Heiko",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Uncaught exception in ZMQStream callback -- trying to run Jupyter notebook with Julia kernel in SageMaker",
        "Question_creation_time":1644454296572,
        "Question_link":"https:\/\/repost.aws\/questions\/QUM2y8-rjDS1Wp5LUkdLMhyA\/uncaught-exception-in-zmq-stream-callback-trying-to-run-jupyter-notebook-with-julia-kernel-in-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":1,
        "Question_view_count":172,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I want to run a Jupyter notebook with Julia kernel in Amazon SageMaker. The Julia 1.7.1 icon shows up in the Jupyterlab launcher and accepts the kernel on launch, but then the kernel dies immediately after launch (it never works). I have posted about this here\n\nhttps:\/\/www.repost.aws\/questions\/QU2PXu3tbpQ7-V2OTlD_I07Q\/make-julia-notebooks-work-in-sage-maker\n\nand Alex_T has made some very good suggestions, but even they get stumped at this point. Log info:\n\n[E 00:25:58.760 NotebookApp] Uncaught exception in ZMQStream callback\n    Traceback (most recent call last):\n      File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/zmq\/eventloop\/zmqstream.py\", line 431, in _run_callback\n        callback(*args, **kwargs)\n      File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/notebook\/services\/kernels\/kernelmanager.py\", line 391, in record_activity\n        msg = session.deserialize(fed_msg_list)\n      File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/site-packages\/jupyter_client\/session.py\", line 929, in deserialize\n        raise ValueError(\"Invalid Signature: %r\" % signature)\n    ValueError: Invalid Signature: b'ec1d1093f3b6505658469b860c203f696bc39cf8fcea1672cb55802fc57592eef57b8db0b5cb603d1bcada6f41060f0819f6002a7a31f309fbaa1a701cc13f5b'\n\n\nThere are some posts from 2018 recommending updating tornado and ipykernel, but 4 years later this should hardly apply (and I tried it, and it didn't work). Any suggestions? What could be going wrong here? Everything else seems to be in place. There is a white paper on running a Jupyter notebook with Julia kernel in SageMaker here [https:\/\/d1.awsstatic.com\/whitepapers\/julia-on-sagemaker.pdf?did=wp_card&trk=wp_card], but it is incomplete, as you can see in the other post.",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-10T03:52:32.909Z",
                "Answer_upvote_count":1,
                "Answer_body":"Thanks for raising this again!\n\nSo as mentioned on the other question, I tentatively believe this to be caused by this open IJulia issue... And there I see one participant commented they managed to resolve the issue by not installing Julia inside conda.\n\nHappy to share I seem to have been able to reproduce this success with the following steps... :D\n\nInstead of creating a conda env and installing Julia through it as advised by the whitepaper (which would of course be a good practice for proper environment separation, if it weren't for this issue), wget the generic Linux x86 64-bit (glibc) tarball from the Julia downloads page https:\/\/julialang.org\/downloads\/ (for example, https:\/\/julialang-s3.julialang.org\/bin\/linux\/x64\/1.7\/julia-1.7.2-linux-x86_64.tar.gz)\n(Because uname -a shows us that our NBI is running in x86 64-bit Linux, and a couple of hacky checking methods I found online seemed to suggest it's a glibc setup rather than musl)\nDon't forget to cd SageMaker in terminal before your wget, so the file actually downloads to somewhere you can see it in JupyterLab\n(Verify the checksum of the downloaded file first, as good security practice and then) Untar the bundle with tar -xvf {YourFileName.tar.gz}. You should see a folder something like julia-1.7.2 containing (amongst other things) a bin subfolder with a julia executable in it.\nMake julia executable visible in your system PATH\nThere are many different ways you might want to set this up, depending what you want on persistent EBS storage vs ephemeral, whether you want to use multiple versions of julia in parallel, and whether you have preferences about where in the filesystem the application should reside.\nFor this particular test I chose to cp -R .\/julia-1.7.2 \/usr\/local\/julia (copy Julia to a folder where I wouldn't accidentally edit it) and then sudo ln -s \/usr\/local\/julia\/bin\/julia \/bin\/julia (create a julia symlink to the main executable inside the \/bin folder, which is already on PATH).\nStart julia from your terminal (julia) and then install & configure IJulia as per the original instructions:\nusing Pkg\nPkg.add(\"IJulia\")\nusing IJulia\njupyterlab(detached=true)\n# I got an error on this last command but it still seemed to work?\n\nVerify that you should now have a julia entry in your Jupyter kernels folder. I copied out the contents into my JupyterLab working folder to inspect it: cp -R ~\/.local\/share\/jupyter\/kernels\/julia-1.7 .\/julia-1.7-copy. It should contain a kernel.json.\nIf all is well (and it was for me) you should now see a \"Julia 1.7.2\" (or similar version) kernel on your launcher and be able to launch a notebook successfully and run commands (\ud83c\udf89)\nWhat about productionization?\n\nSo of course this installation is a bit of a hacky workaround for whatever underlying issue is causing Julia to not run properly inside conda... I'd really prefer to see a direct resolution for that problem if possible. The procedure described here won't persist between Notebook Instance stop\/starts (at which time everything outside of \/home\/ec2-user\/SageMaker EBS mount is reset), so you'd probably want to explore automating the setup with a lifecycle configuration script.\n\nI see by default that Julia packages (including IJulia) are installed to \/home\/ec2-user\/.julia\/packages\/ (which is outside the SageMaker mount), so may be interesting to explore configuration options for moving .julia under the SageMaker folder for persistence. Files and folders beginning with a dot are hidden in JupyterLab file tree, which is worth remembering if you explore customizing the setup.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Possible quota issue",
        "Question_creation_time":1644304787619,
        "Question_link":"https:\/\/repost.aws\/questions\/QUnEGxZsbiQPe4oHO0CkRaRg\/possible-quota-issue",
        "Question_topic":[
            "Serverless",
            "Application Integration",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Step Functions",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":200,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I opened up an AWS support case to increase my quota based on the error below, but AWS support states this is not a quota issue. They were unwilling to help me in support but directed me to this forum.\n\nI'm getting the following error when running some AWS SageMaker batch transform jobs on m5.large instances. I'm running 8 transforms in parallel against 8 different models in a step function map. A few of the transforms succeed, but some fail with the following error: { \"resourceType\": \"sagemaker\", \"resource\": \"createTransformJob.sync\", \"error\": \"SageMaker.AmazonSageMakerException\", \"cause\": \"Rate exceeded (Service: AmazonSageMaker; Status Code: 400; Error Code: ThrottlingException; Request ID: 49a80dc1-df06-4b88-a462-24e517d13531; Proxy: null)\" }\n\nWhat is going on here? Am I doing something wrong? What kind of throttling exception is this (what is being throttled?)",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-09T15:37:04.872Z",
                "Answer_upvote_count":0,
                "Answer_body":"I have seen this with batch transform when I try to trigger too many jobs too quickly (usually like 4xjobs within 1-2 seconds).\n\nWhat I did as a workaround was handle and recognize this quota error on my service, sleep for 5 seconds and retry the same job.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Host multiple TensorFlow computer vision models using Amazon SageMaker multi-model endpoints",
        "Question_creation_time":1644300080795,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxAPkO75GTnSERkxpABFgSQ\/host-multiple-tensor-flow-computer-vision-models-using-amazon-sage-maker-multi-model-endpoints",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Computer Vision",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":112,
        "Question_answer_count":3,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi All,\n\nGreetings!!\n\nCould you please clarify on two questions below which are related to this post.\n\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/host-multiple-tensorflow-computer-vision-models-using-amazon-sagemaker-multi-model-endpoints\/\n\nAs per AWS documentation (https:\/\/docs.aws.amazon.com...] \"Multi-model endpoints are not supported on GPU instance types\". I see we are using 'instance_type': 'ml.m5.2xlarge' to train both image classification and sign language digit classification models in this post.\n\nAs per instance configuration we don't have any GPU cores and GPU memory available in 'ml.m5.2xlarge' instance type. But we all know that we need a GPU instance for deep learning model training, we are not getting how we are able to train both classification models using 'ml.m5.2xlarge' instance type.\n\nDoes this image - IMAGE_URI = '763104351884.dkr.ecr.us-eas... has GPU cores?\n\nml.m5.2xlarge - Instance details:\n\nCompute Type: Standard Instances V CPU: 8 Memory: 32 GiB Clock Speed: undefined GPU: 0 Network Performance: Up to 10 Gigabit Storage: EBS only GPU Memory: 0\n\nAs mentioned in this post, I believe we have one production variant of both CIFAR and sign-language models meaning we have v1 models. Let's assume that we have a new set of images for classification then we have decided to train a new variant CIFAR model and want to create that model.\n\nSo we have 2 versions of the CIFAR model and 1 version of the sign-language model.\n\nHow can we get the inference from two production variants of CIFAR models? How can we do A\/B testing for CIFAR models?\n\nThanks in advance.",
        "Answers":[
            {
                "Answer_creation_date":"2022-11-16T18:43:01.669Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Vinayak & hope I can help:\n\nLet's take (1) GPU vs CPU first:\n\nYes, you're correct that Multi-Model Endpoints don't currently support GPU instance types (as documented here, and a lot of ML frameworks automatically reserve full GPU capacity which would make this tricky in practice anyway).\nI see from over on the blog post that the full container image URI you linked (which got chopped off here on rePost) was: 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-inference:2.3.1-cpu-py37-ubuntu18.04 - which is a -cpu- image so is set up for CPU-only instance types - excluding GPU tools like CUDA, etc.\nFrom the notebook of this sample that it's actually training the model on CPU-only instances too (using ml.m5.2xlarge in the Estimator constructor)\n\nTraining small DL models on CPU only, without GPU acceleration, is perfectly possible... Just often much slower because of the nature of the workload - especially as models and datasets get bigger. It should also be fine (for faster training speed) to train on GPU-accelerated instances (setting a different instance type in the Estimator) but then deploy for CPU-only inference (keeping m5 or similar in the MultiDataModel).\n\nGenerally this could be as easy as just changing the instance types in the notebook, but there's a couple of subtle things you might need to bear in mind if you see problems:\n\nYour SageMaker \"Model\" will reference a container image, which will be optimized for either CPU or GPU instances. This is why the SageMaker Python SDK Estimator.fit() API that runs a \"Training Job\" doesn't automatically register the output in the SageMaker \"Models\" list - it doesn't know whether the image will be the same or different until you define target infrastructure (e.g. with mme.deploy() in the notebook). I think you'll see that even creating the mme MultiDataModel in the notebook doesn't create a \"Model\" in SageMaker console until that deploy call.\nIf you're providing any custom inference scripts e.g. input_handler that reference CUDA devices or other GPU-specific tools, you'll need to check these scripts are set up to work okay on either GPU or CPU-only deployments. The example you linked is already CPU-only so that should be fine.\n\nNow for (2) A\/B testing:\n\nBetween variants, MME, multi-container endpoints, or even separate SageMaker endpoints with some proxy in front - you have multiple options for A\/B testing models depending what's easiest for your overall deployment, monitoring, integration and rollback requirements.\n\nSome tips I would suggest that might guide your choice:\n\nFor MCE, MME and separate-endpoints, your client logic would always need to explicitly choose one of the options\/versions to invoke. Using ProductionVariants, you can either explicitly specify a Target-Variant from the client side or let the configured variant weights do their thing and weight invocations automatically.\nMME supports a potentially large number of available models at a time (e.g. more than memory), but doesn't (yet?) offer a way to explicitly purge cached models from endpoint memory. This means you can't really overwrite the \"same\" model name on S3 within one MME - You'd need to add new versions in as new models on S3, and make sure your clients start requesting the new model names.\nI believe yes it should be possible to set up an endpoint with multiple variants, each of which is a MultiModel... Each variant would run on separate infrastructure instance(s). I guess you could probably use a new variant pointing to the same S3 models folder as a way to force clear an MME cache - because you could spin up new serving containers, push traffic over to them, and shut the old variant down? But this would be a pretty niche case.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-02-17T05:58:36.671Z",
                "Answer_upvote_count":0,
                "Answer_body":"@Alex_T\n\nThanks for your reply.\n\nLet's talk about MME only.\n\nAs mentioned in this post, I believe we have one production variant of both CIFAR and sign-language models meaning we have v1 models. Let's assume that we have a new set of images for classification then we have decided to train a new variant CIFAR model and want to create that model. So we have 2 versions of the CIFAR model and 1 version of the sign-language model.\n\nBoth CIFAR and sign-language models v1 models will be running in one MME then new production variant (version) of CIFAR model will be running in new MME?\n\nIn general, how can we do A\/B testing in MME? If possible then please clarify it with simple example.\n\nThanks",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-02-08T07:26:55.384Z",
                "Answer_upvote_count":0,
                "Answer_body":"MME now Support GPU instances as well. Checkout \u2013\n\nWhats New- https:\/\/aws.amazon.com\/about-aws\/whats-new\/2022\/10\/amazon-sagemaker-cost-effectively-host-1000s-gpu-multi-model-endpoint\/\n\nLaunch Blog-- https:\/\/aws.amazon.com\/blogs\/machine-learning\/run-multiple-deep-learning-models-on-gpu-with-amazon-sagemaker-multi-model-endpoints\/",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Turn our endpoints off when not in use with Lambda",
        "Question_creation_time":1644299069251,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGU0QdOahS3SrmdOSaNOj7Q\/turn-our-endpoints-off-when-not-in-use-with-lambda",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Lambda",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":145,
        "Question_answer_count":4,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi Team,\n\nGreetings!!\n\nIn this video (https:\/\/www.youtube.com\/watch?v=KFuc2KWrTHs&list=PLhr1KZpdzukcOr_6j_zmSrvYnLUtgqsZz&index=6&t=374s) Emily Webber mentioned that we can turn our endpoints (instances) off when not in use with Lambda.\n\nCould anyone please explain how can we do that using Lambda functions?\n\nI checked it in AWS console (Amazon SageMaker --> Endpoints), looks like we can only delete the endpoints but there is no option for turning off the endpoints.\n\nThanks, Vinayak",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-17T05:59:40.912Z",
                "Answer_upvote_count":0,
                "Answer_body":"Deleting == turning off in this case.\n\nYou can definitely delete endpoints when you don't need them. Be aware that creating the endpoints will take some time - it isn't instantaneous. So if your Lambda function is executing and you create the endpoint at the same time it (most probably) won't be ready in time for the Lambda function to use it.\n\nSo this is useful if you're using the endpoint at particular times during the day and you can create them before you need them.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-02-08T05:57:04.470Z",
                "Answer_upvote_count":0,
                "Answer_body":"@vinayak - sorry for taking the discussion elsewhere but based on your question and comment above I thought this might be helpful...\n\nSince cold start is not an issue with you and you were looking to spin up the endpoint right at run time and then potentially shut it down, I would suggest looking into asynchronous endpoints that can scale to 0 instances. This blog could be a good example: https:\/\/aws.amazon.com\/blogs\/machine-learning\/improve-high-value-research-with-hugging-face-and-amazon-sagemaker-asynchronous-inference-endpoints\/\n\nAdditionally, we recently released serverless inference for SageMaker endpoints. These could also be a good solution for models with intermediate access patterns where cold start times is not an issue. However, please note that this feature is in Preview mode at the moment and has some limitations compared to the asynchronous option (model needs to be a good fit for the service, payload limitations, execution time limitations etc.). More on this here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints.html",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-02-08T09:51:17.303Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks for your reply.\n\n@Brettski@AWS\n\nSo this is useful if you're using the endpoint at particular times during the day and you can create them before you need them. --> In this case we may experience a cold start problem which is acceptable.\n\nBut how can we create the endpoints before or at the time of inference request using Lambda function?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-02-17T05:59:18.962Z",
                "Answer_upvote_count":0,
                "Answer_body":"@georgios_s\n\nThanks for your reply. Yes, you're diverting the discussion.\n\nWe are interested in real time inference which has low latency requirements. The purpose of turning off the real time endpoints when not invoking through Lambda is to save the cost for on-demand instances we used to host the model.\n\nAs you know, we can't scale the instances to 0 in case of real-time inference.\n\nAs suggested by @Brettski, we want to know, how can we create the endpoints before or at the time of inference request using Lambda function?",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Deploy an model trained using Sagemaker's built-in k-NN algorithm for AWS Panorama",
        "Question_creation_time":1644293159320,
        "Question_link":"https:\/\/repost.aws\/questions\/QUx37woHPgTamp-7MjnpYqcA\/deploy-an-model-trained-using-sagemakers-built-in-k-nn-algorithm-for-aws-panorama",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Panorama"
        ],
        "Question_upvote_count":0,
        "Question_view_count":76,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi, I'm trying to use a k-Nearest Neighbour model for deployment to an Edge device (AWS Panorama). I understand I need to create an optimized model to suit the target device. As I understand it the built-in algorithms all include a SageMaker Neo 'compile_model' function which I am running over my trained model -\n\nBuild model..\n\nknn = sagemaker.estimator.Estimator(container,\n                                       role, \n                                       instance_count=1, \n                                       instance_type='ml.m5.2xlarge',\n                                       output_path=output_location,\n                                       sagemaker_session=sess,\n                                       input_mode='Pipe'\n                                   )\n# Setup the hyperparameters\nknn.set_hyperparameters(**hyperparams)\n\nknn.fit(fit_input, job_name=job_name)\n\n\nBuild optimised model...\n\noptimized_ic = knn.compile_model(\n    target_instance_family=\"ml_c5\",\n    target_platform_os=\"LINUX\",\n    target_platform_arch=\"ARM64\",\n    input_shape={\"data\": [1,3,512,512]},\n    output_path=s3_optimized_output_location,\n    framework=\"mxnet\",\n    framework_version=\"1.8\",\n)\n\n\nWhile the initial model builds fine I get an error running the optimised model \/ compile_model function:\n\nFailed. Reason: ClientError: InputConfiguration: No valid Mxnet model file -symbol.json found. Please make sure the framework you select is correct.\n\nI can find a few google references for attempts at something similar with KMeans algorithm, but is k-NN algorithm dramatically different? Do I just have the wrong settings? I understand all of the built-in algorithms use were originally mxnet trained? When I extract my model I only have three files:\n\nmodel_algo-1\nmodel_algo-1.json\nmodel_algo-1-labels.npy\n\nI am struggling to work out where I am going wrong in porting this model!",
        "Answers":[
            {
                "Answer_creation_date":"2022-05-11T02:24:33.213Z",
                "Answer_upvote_count":0,
                "Answer_body":"The default Panorama service leverages NEO to compile and run the models. Unfortunately, the 1P Sagemaker algorithms are not compatible with NEO. Also, the Panorama service leverages Nvidia Jetson hardware, which is not compatible with the 'ml.m5.2xlarge' hardware. With that said, Panorama now supports an option to \"bring your own runtime\" to Panorama that has recently been introduced that would allow you to leverage 1P Sagemaker models. If you were to train your model with a GPU enabled instance (p3 or g4dn), the process would involve building your own custom container for Panorama with the MXNET 1.4 version installed. If this is still something you are pursuing, reach out and I can help walk you through the process.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"make julia notebooks work in SageMaker",
        "Question_creation_time":1644167031317,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2PXu3tbpQ7-V2OTlD_I07Q\/make-julia-notebooks-work-in-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":1,
        "Question_view_count":95,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I want to run a Jupyter notebook in SageMaker with a Julia kernel. There is very little documentation about this. There is this:\n\nhttps:\/\/d1.awsstatic.com\/whitepapers\/julia-on-sagemaker.pdf?did=wp_card&trk=wp_card\n\nI followed all the instructions, and Julia shows up in the JupyterLab launcher; but when I run it, Julia 1.17.1 shows up as the kernel and then dies. It appears to be trying, but then gives up and says \"No Kernel\" instead of \"Julia 1.17.1\" in the status line.\n\nIf I run the R kernel, all goes well. If I run the Julia kernel (which shows up in the list of available kernels!), I get the following error message:\n\nConnection failed\n\nA connection to the notebook server could not be established.\n\nThe notebook will continue trying to reconnect.\n\nCheck your network connection or notebook server configuration.",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-10T18:11:39.365Z",
                "Answer_upvote_count":1,
                "Answer_body":"Sorry this is not a full working solution, but too much for a comment & hopefully will still be useful to you:\n\n(Assuming you're talking here about SageMaker Notebook Instances rather than SageMaker Studio, same as the whitepaper; that you're trying to install the latest version of Julia from conda rather than v1.0.3 explicitly specified in the whitepaper, currently 1.7.1)\n\nThe first thing to point out is that you should be able to debug this via the notebook's logs: Either click the \"View logs\" link on the notebook's detail page in Amazon SageMaker console, or look in the Amazon CloudWatch console for log group \/aws\/sagemaker\/NotebookInstances, stream {YOUR-NBI-NAME}\/jupyter.log.\n\nFollowing through the whitepaper instructions myself (on an notebook-al2-v1 notebook instance), the errors I saw preventing the kernel from loading were like:\n\nArgumentError: Package IJulia not found in current path:\n- Run `import Pkg; Pkg.add(\"IJulia\")` to install the IJulia package.\n\n\nLooking at ~\/.local\/share\/jupyter\/kernels\/julia-1.7\/kernel.json, I saw the created kernel was defined as follows:\n\n{\n  \"display_name\": \"Julia 1.7.1\",\n  \"argv\": [\n    \"\/home\/ec2-user\/anaconda3\/envs\/julia\/bin\/julia\",\n    \"-i\",\n    \"--color=yes\",\n    \"--project=@.\",\n    \"\/home\/ec2-user\/anaconda3\/envs\/julia\/share\/julia\/packages\/IJulia\/e8kqU\/src\/kernel.jl\",\n    \"{connection_file}\"\n  ],\n  \"language\": \"julia\",\n  \"env\": {},\n  \"interrupt_mode\": \"signal\"\n}\n\nIf we run julia from within the julia conda environment, using IJulia works no problem... However, if you source activate JupyterSystemEnv from the terminal - you can still run \/home\/ec2-user\/anaconda3\/envs\/julia\/bin\/julia from the terminal but the interpreter will not think the IJulia package is installed... I think this is closer to what the above kernel definition is doing (NBI JupyterServer itself runs in this system conda env).\n\nI tried a couple of hacky solutions:\n\nSimply start Julia in JupyterSystemEnv as above and Pkg.add(\"IJulia\") to install it there\nCopy the setup you'll see in the R kernel, \/home\/ec2-user\/.local\/share\/jupyter\/kernels\/ir - where kernel.json points to a run.sh script which first activates the target conda environment and then runs the interpreter\n\nkernel.json (with a different display name & file path to visually confirm JupyterLab has picked the new one up):\n\n{\n  \"display_name\": \"Julia Fix\",\n  \"argv\": [\n    \"\/home\/ec2-user\/.local\/share\/jupyter\/kernels\/julia-fix\/run.sh\",\n    \"{connection_file}\"\n  ],\n  \"language\": \"julia\",\n  \"env\": {},\n  \"interrupt_mode\": \"signal\"\n}\n\n\nrun.sh (remember to chmod +x this file to avoid 500 server errors as Jupyter can't execute it)\n\n#!\/bin\/bash\n\nsource activate julia\n\/home\/ec2-user\/anaconda3\/envs\/julia\/bin\/julia -i --project=@. \/home\/ec2-user\/anaconda3\/envs\/julia\/share\/julia\/packages\/IJulia\/e8kqU\/src\/kernel.jl $1\n\n\nHOWEVER, unfortunately both approaches yield a ZMQStream Invalid Signature error, which looks to me like this open IJulia issue. I see speculation there that Julia isn't playing nice with conda, but am not deep enough with Julia to know if that's really the root cause or how best to mitigate it if so.\n\nMaybe you could install Julia itself outside of conda (conda deactivate in terminal to check you're in base environment) and, if still getting the signature error, use a run.sh kernel script to ensure Jupyter also runs the command outside of conda rather than in the JupyterSystemEnv? Or perhaps there's some other cause for the signature issue that can be resolved while keeping conda environments set up...\n\nYou could also look into the SageMaker Studio Custom Image Samples, instead of Notebook Instances, since Studio kernels are isolated by full container images rather than conda? There is an example image there for Julia (v1.5), although it's not been updated in some time so of course could have some issues of its own.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-02-07T03:18:25.173Z",
                "Answer_upvote_count":0,
                "Answer_body":"Note that Alex_T found a (hacky) solution to the ZMQStream problem here:\n\nhttps:\/\/www.repost.aws\/questions\/QUM2y8-rjDS1Wp5LUkdLMhyA\/uncaught-exception-in-zmq-stream-callback-trying-to-run-jupyter-notebook-with-julia-kernel-in-sage-maker\n\nand got the Jupyter notebook with Julia kernel to work in SageMaker.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Project Run Time does not start on Sagemaker Studio Lab",
        "Question_creation_time":1643885518208,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXiCETmDfRje5zL4WArbhRA\/project-run-time-does-not-start-on-sagemaker-studio-lab",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":24,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"That is the case as of two days ago. This does not work for neither CPU nor GPU \"compute type\".\n\nBasically, after pressing the \"Start runtime\" button, it says \"Preparing project runtime...\" for about ten minutes and then stops. It shows the following error, \"There was a problem when starting the project runtime. This should be resolved shortly. Please try again later.\"\n\nI have now tried it about a dozen or more times throughout the period.\n\nThere is no way to even access the work that is saved there. The \"project\" will not boot up.\n\nBasically it is a dud at this point.\n\nIs anyone else experiencing similar issues? What does one do? Is there a way to reset the environment to original and restart (e.g., a factory reboot)?",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"AWS SageMaker - Upload our own docker image on Amazon SageMaker",
        "Question_creation_time":1643870744946,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYIfUo3-0Qqyr4XunexHJXw\/aws-sage-maker-upload-our-own-docker-image-on-amazon-sage-maker",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Containers"
        ],
        "Question_upvote_count":0,
        "Question_view_count":111,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am new to AWS SageMaker and i am using this technology for building and training the machine learning models. I have now developed a docker image which contains our custom code for tensorflow. I would like to upload this custom docker image to AWS SageMaker and make use of it.\n\nI have searched various links but could not find proper information on how to upload our own custom docker image.\n\nCan you please suggest me the process of uploading our own docker image to AWS SageMaker?",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-03T08:16:26.646Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi, You can find detailed instructions in the SageMaker Immersion Day. In Lab3 there is a sample called \"Option 2: Bring your own Container\". And the corresponding notebook is here. The content of the sample Docker container is inside this zip file and showcases the decision tree algorithm from scikit-learn package. Hope this helps.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Processing environmental data with netCDF files",
        "Question_creation_time":1643796556447,
        "Question_link":"https:\/\/repost.aws\/questions\/QU9sduyLViQxepK_RcYuhPwQ\/processing-environmental-data-with-net-cdf-files",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon Simple Storage Service",
            "Amazon SageMaker",
            "AWS Glue"
        ],
        "Question_upvote_count":0,
        "Question_view_count":42,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm looking for some experience, reference architecture, best practices about the processing of environmental data stored in netCDF files.",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"AWS Sagemaker: UnexpectedStatusException: Compilation job Failed. Reason: ClientError: InputConfiguration: Please make sure input config is correct - Input 1 of node StatefulPartitionedCall was passed",
        "Question_creation_time":1643795001851,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvCjrBvqAQzGwEyZjXDwE7g\/aws-sagemaker-unexpected-status-exception-compilation-job-failed-reason-client-error-input-configuration-please-make-sure-input-config-is-correct-input-1-of-node-stateful-partitioned-call-was-passed",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":28,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to convert a pre-trained (NASNETMobile) model into AWS Neo Optimized model. I am flowing [https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/] this link. Only difference is that I am using Tensorflow 2.6 and Python 3.7.10. Other dependencies are boto 2.49.0 boto3 1.20.30 botocore 1.23.30 keras 2.7.0 Keras-Preprocessing 1.1.2 numpy 1.19.5 safety 1.10.3 sagemaker 2.74.0 sagemaker-pyspark 1.4.2 tensorflow 2.6.0 tensorflow-cpu 2.6.0 tensorflow-estimator 2.6.0 tensorflow-gpu 2.6.0 tensorflow-io-gcs-filesystem 0.23.1 tensorflow-serving-api 2.6.0\n\nAs per link, I got stuck in step 7. Getting error->\n\nfrom tensorflow.keras.applications import nasnet\nfrom keras.preprocessing import image\nimport h5py\nimport numpy as np\nfrom keras.models import load_model\n\n# Load h5 file\nloaded_model = load_model('myy_model.h5')\n\nmodel_version = '2'\nexport_dir = 'export\/Servo\/' + model_version\n\n# Save in pb format\nloaded_model.save(export_dir)\n\n# Save as tar file\nimport tarfile\nmodel_archive = 'model.tar.gz'\nwith tarfile.open(model_archive, mode='w:gz') as archive:\n    archive.add('export', recursive=True)\n\nfrom sagemaker import get_execution_role\nfrom sagemaker import Session\n\nrole = get_execution_role()\nsess = Session()\nregion = sess.boto_region_name\nbucket = sess.default_bucket()\n\nmodel_data = sess.upload_data(path=model_archive, key_prefix='model')\n\nfrom sagemaker.tensorflow import TensorFlowModel\n\nsagemaker_model = TensorFlowModel(model_data=model_data, \n                      framework_version=tf_framework_version,\n                      role=role)\npredictor = sagemaker_model.deploy(initial_instance_count=1,\n                                   instance_type='ml.m4.xlarge')\n\n#Load  jpeg image from local and set target size to 224 x 224\nimg = image.load_img('eagle.jpg', target_size=(224, 224))\n\n#convert image to array\ninput_img = image.img_to_array(img)\ninput_img = np.expand_dims(input_img, axis=0)\ninput_img = nasnet.preprocess_input(input_img)\n\n\npredict_img = predictor.predict(input_img)\npredict_img\n\n\/\/ Till this point every thing is working\n\ninstance_family = 'ml_c5'\nframework = 'tensorflow'\ncompilation_job_name = 'keras-compile-16'\n# output path for compiled model artifact\ncompiled_model_path = 's3:\/\/{}\/{}\/output'.format(bucket, compilation_job_name)\n\ndata_shape = {'input_2':[1,224,224,3]}\n\n\/\/ Failing in below line\noptimized_estimator = sagemaker_model.compile(target_instance_family=instance_family,\n                                         input_shape=data_shape,\n                                         job_name=compilation_job_name,\n                                         role=role,\n                                         framework=framework,\n                                         framework_version=tf_framework_version,\n                                         output_path=compiled_model_path\n                                        )\n\n\n\ndata_shape = {'input_2':[1,224,224,3]} \"input_2\" is right name. have checked in model plus https:\/\/netron.app\/\n\nFull Error Logs:\n\nUnexpectedStatusException Traceback (most recent call last) <ipython-input-140-c16ae7ba24de> in <module> 5 framework=framework, 6 framework_version=tf_framework_version, ----> 7 output_path=compiled_model_path 8 )\n\n~\/anaconda3\/envs\/tensorflow2_p37\/lib\/python3.7\/site-packages\/sagemaker\/model.py in compile(self, target_instance_family, input_shape, output_path, role, tags, job_name, compile_max_run, framework, framework_version, target_platform_os, target_platform_arch, target_platform_accelerator, compiler_options) 659 ) 660 self.sagemaker_session.compile_model(**config) --> 661 job_status = self.sagemaker_session.wait_for_compilation_job(job_name) 662 self.model_data = job_status[\"ModelArtifacts\"][\"S3ModelArtifacts\"] 663 if target_instance_family is not None:\n\n~\/anaconda3\/envs\/tensorflow2_p37\/lib\/python3.7\/site-packages\/sagemaker\/session.py in wait_for_compilation_job(self, job, poll) 3224 \"\"\" 3225 desc = _wait_until(lambda: _compilation_job_status(self.sagemaker_client, job), poll) -> 3226 self._check_job_status(job, desc, \"CompilationJobStatus\") 3227 return desc 3228\n\n~\/anaconda3\/envs\/tensorflow2_p37\/lib\/python3.7\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name) 3341 ), 3342 allowed_statuses=[\"Completed\", \"Stopped\"], -> 3343 actual_status=status, 3344 ) 3345\n\nUnexpectedStatusException: Error for Compilation job keras-compile-14: Failed. Reason: ClientError: InputConfiguration: Please make sure input config is correct - Input 1 of node StatefulPartitionedCall was passed float from stem_conv1\/kernel:0 incompatible with expected resource.\n\nThanks in Advance",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-05T15:00:55.100Z",
                "Answer_upvote_count":0,
                "Answer_body":"It is hard to say if the input tensor name is wrong without having access to the notebook and the other artifacts but I guess the name of the input tensor might be wrong. TF uses dynamic names for the tensors when you don't specify and the sequence is incremented each time you load the model using the same runtime session. It starts with input_1 then input_2 and so on. Another point is that you're using the estimator to invoke Neo. This library is sometimes outdated. I prefer to use boto3 instead. I wrote a sample code to test the compilation. The following code is working perfectly with your model:\n\n## Load a pre-trained Keras model and export in Tensorflow format\nimport tensorflow as tf\nfrom tensorflow.keras.applications import nasnet\nimport numpy as np\nmodel = tf.keras.applications.nasnet.NASNetMobile(weights='imagenet')\nx = tf.random.uniform((1, 224, 224, 3))\ny = model(x)\nexport_dir = 'export\/1'\ntf.saved_model.save(model, export_dir)\n\n## Put the model into a tar ball and send to S3\nimport sagemaker\nimport tarfile\nimport io\n\nmodel_archive = 'model.tar.gz'\nmodel_name='nasnet-mobile'\nimg_size=224\nsagemaker_session = sagemaker.Session()\ndefault_bucket = sagemaker_session.default_bucket()\n\nwith io.BytesIO() as f:\n    with tarfile.open(fileobj=f, mode=\"w:gz\") as tar:\n        tar.add('export')\n        tar.list()\n    f.seek(0)\n    s3_uri = sagemaker_session.upload_string_as_file_body(f.read(), default_bucket, f\"models\/{model_name}\/model.tar.gz\")\n    print(s3_uri)\n\n## Now kick-off a compilation job\nimport time\nimport boto3\nimport sagemaker\n\nrole = sagemaker.get_execution_role()\nsm_client = boto3.client('sagemaker')\n\nframework='tensorflow'\nimg_size=224\ninput_shape=f\"1,{img_size},{img_size},3\"\ncompilation_job_name = f'{model_name}-{framework}-{int(time.time()*1000)}'\n\nsm_client.create_compilation_job(\n    CompilationJobName=compilation_job_name,\n    RoleArn=role,\n    InputConfig={\n        'S3Uri': s3_uri,\n        'DataInputConfig': f'{{\"{model.layers[0].name}\": [{input_shape}]}}',\n        'Framework': framework.upper(),\n        'FrameworkVersion': '2.4'\n    },\n    OutputConfig={\n        'S3OutputLocation': f's3:\/\/{default_bucket}\/{model_name}-{framework}\/optimized\/',\n        'TargetDevice': 'ml_c5',\n        # Comment or change the following line depending on your edge device\n        # Jetson Xavier: sm_72; Jetson Nano: sm_53\n        #'CompilerOptions': '{\"trt-ver\": \"7.1.3\", \"cuda-ver\": \"10.2\", \"gpu-code\": \"sm_72\"}' # Jetpack 4.4.1\n    },\n    StoppingCondition={ 'MaxRuntimeInSeconds': 18000 }\n)\nwhile True:\n    resp = sm_client.describe_compilation_job(CompilationJobName=compilation_job_name)    \n    if resp['CompilationJobStatus'] in ['STARTING', 'INPROGRESS']:\n        print('Running...')\n    else:\n        print(resp['CompilationJobStatus'], compilation_job_name)\n        break\n    time.sleep(5)\n\n\nJust notice that I'm using FrameworkVersion = 2.4. There is not information about that in your sample.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Multi-model, Multi-container and Variants - what are the possible combinations?",
        "Question_creation_time":1643789695844,
        "Question_link":"https:\/\/repost.aws\/questions\/QUw1jokF5cQLmWHpj0hBGgtg\/multi-model-multi-container-and-variants-what-are-the-possible-combinations",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_upvote_count":0,
        "Question_view_count":126,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"This question is mostly for educational purposes, but the current SageMaker documentation does not describe whether these things are allowed or not.\n\nLets suppose I have:\n\na XGBoost_model_1 (that needs a XGBoost container)\na KMeans_model_1 and a KMeans_model_2 (both require a KMeans container)\n\n1. Here's the first question - can I do the following:\n\ncreate a Model with InferenceExecutionConfig.Mode=Direct and specify two cointainers (XGBoost and KMeans with Mode: MultiModel)\n\nThat would enable the client:\n\nto call invoke_endpoint(TargetContainer=\"XGBoost\") to access the XGBoost_model_1\nto call invoke_endpoint(TargetContainer=\"KMeans\", TargetModel=\"KMeans_model_1\") to access the KMeans_model_1\nto call invoke_endpoint(TargetContainer=\"KMeans\", TargetModel=\"KMeans_model_2\") to access the KMeans_model_2\n\nI don't see a straight answer in the documentation whether combining Multi-Model containers with Multi-container endpoint is possible.\n\n2. The second question - how does the above idea work with ProductionVariants. Can I create something like this:\n\nVariant1 with XGBoost serving XGBoost_model_1 having a weight of 0.5\nVariant2 with a Multi-container having both XGBoost and KMeans (with a MultiModel setup) having a weight of 0.5\n\nSo that the client could:\n\ncall invoke_endpoint(TargetVariant=\"Variant2\", TargetContainer=\"KMeans\", TargetModel=\"KMeans_model_1\") to access the KMeans_model_1\ncall invoke_endpoint(TargetVariant=\"Variant2\", TargetContainer=\"KMeans\", TargetModel=\"KMeans_model_2\") to access the KMeans_model_2\ncall invoke_endpoint(TargetVariant=\"Variant1\") to access the XGBoost_model_1\ncall invoke_endpoint(TargetVariant=\"Variant2\", TargetContainer=\"XGBoost\") to access the XGBoost_model_1\n\nIs that combination even possible?\n\nIf so, what happens when the client calls the invoke_endpoint without specifying the variant? For example:\n\nwould invoke_endpoint(TargetContainer=\"KMeans\", TargetModel=\"KMeans_model_2\") fail 50% of the time (if it hits the right variant then it works just fine, if it hits the wrong one it would most likely result with a 400\/500 error (\"incorrect payload\")?",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-16T12:04:45.094Z",
                "Answer_upvote_count":0,
                "Answer_body":"Well, I've checked that myself.\n\nTurns out NONE of these combinations are possible. :)\n\nMulti-model + Multi-container is NOT possible\nVariants + Multi-container is NOT possible\nVariants + Multi-model is NOT possible\n\nIn all cases, you get a corresponding error while invoking create_endpoint_configuration:\n\nMultiple ProductionVariants is currently not supported when a Model uses a Direct InferenceExecutionMode.\nDirect InferenceExecutionMode is not supported when a Container uses MultiModel mode.\nMultiModel mode is not supported with the current model specification.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Auto rollback with Guardrails if model accuracy is not good",
        "Question_creation_time":1643721370893,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVgggpE9JTPuwbhCmJUKPFA\/auto-rollback-with-guardrails-if-model-accuracy-is-not-good",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon CloudWatch"
        ],
        "Question_upvote_count":1,
        "Question_view_count":50,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"While deploying model with guardrails for SageMaker Inference Endpoint::\n\nWe can create CloudWatch alarms to monitor Endpoint performance for metrics like Invocation5XXErrors, ModelLatency and we can rollback the model to previous version if the threshold is matched. [Different metrics available][https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/monitoring-cloudwatch.html]\n\nPlease let me know is there any way to achieve any one of the following requirement:\n\nI want to the use metrics like \"accuracy\" returned by endpoint and rollback to previous version if the model performance is consistently not good in the real time.\nUse the metrics like \"accuracy\" returned by version2 and compare it with version1 or with the ground truth and rollback model if necessary",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-11T17:51:49.797Z",
                "Answer_upvote_count":0,
                "Answer_body":"You should take a look at SageMaker Model Monitor capabilities and its integration with SageMaker Pipelines to achieve this.\n\nIn this method, you will continuously \"monitor the model\" for model drift and take downstream actions when you detect the drift. The action could be to send a notification(SNS) or re-trigger the Model training.\n\nSome examples for using SageMaker Model Monitor can be found here",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"[Help\/ideas wanted] Serverless Inference: Optimize cold start time",
        "Question_creation_time":1643639465200,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlakvrCXORXyNh7KZehiXKQ\/help-ideas-wanted-serverless-inference-optimize-cold-start-time",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":1,
        "Question_view_count":359,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"We are using Sagemaker Serverless Inference, where the endpoint is wrapped with a Lambda that has a 30sec timeout (this timeout is not adjustable). Our cold start time of the model is quite above that (around 43sec). We load a model using Huggingface transformers and have a FLASK API for serving the model. The model size is around 1.75GB.\n\nAre there any guides on how to improve cold start and model loading time? Could we compile the weights differently beforehand for faster loading?",
        "Answers":[
            {
                "Answer_creation_date":"2022-01-31T15:26:17.484Z",
                "Answer_upvote_count":0,
                "Answer_body":"instead of loading model object from a zip file in lambda session. you can load the model object to elastic-cache upfront and load it in lambda instance from elastic-cache. you might need to serialize and deserialize but I think it would still be faster.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-01-31T15:11:04.633Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi! Thanks for your answer. In theory, that'd be a good idea and could work. However, my other question in this forum then comes into play :D\n\nhttps:\/\/repost.aws\/questions\/QU0JnCsfMHRrSUosWjOiOM9g\/feature-request-serverless-inference-with-vpc-config\n\nServerless Inference currently does not support a VPC configuration. Redis clusters, however, need to be in a VPC.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"[Feature Request] Serverless Inference with VPC Config",
        "Question_creation_time":1643639169189,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0JnCsfMHRrSUosWjOiOM9g\/feature-request-serverless-inference-with-vpc-config",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":1,
        "Question_view_count":181,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I would like to use a Sagemaker Model with a custom VPC Configuration, which is currently not possible with Serverless Inference. Is this feature planned? More generally: Is there a roadmap somewhere for Serverless Inference?",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-11T17:55:31.963Z",
                "Answer_upvote_count":0,
                "Answer_body":"SageMaker Serverless Inference is currently in preview and VPC support is not available but as the feature you are asking for is an important one and is on the roadmap( unfortunately I cannot share the exact details of the timelines here)",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Studio notebook instances restricted to 64 megabytes not allow to train Pytorch multiprocess",
        "Question_creation_time":1643418800912,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4A4gYrPSQWuh71w2shardQ\/sagemaker-studio-notebook-instances-restricted-to-64-megabytes-not-allow-to-train-pytorch-multiprocess",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":174,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Sagemaker Studio notebook instances restricted to 64 megabytes not allow to train Pytorch multiprocess with the default dataloaders. How can I add more capacity to \/dev\/shm or what kernel can I use to train with Pytorch multiprocess?\n\nuname -a\nLinux tensorflow-2-3-gpu--ml-g4dn-xlarge-33edf42bcb5531c041d8b56553ba 4.14.231-173.361.amzn2.x86_64 #1 SMP Mon Apr 26 20:57:08 UTC 2021 x86_64 x86_64 x86_64 GNU\/Linux\ndf -h | grep -E 'shm|File'\nFilesystem Size Used Avail Use% Mounted on\nshm 64M 0 64M 0% \/dev\/shm",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-02T00:47:34.297Z",
                "Answer_upvote_count":0,
                "Answer_body":"This is being tracked in the GitHub issue linked below.\n\nhttps:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/937\n\nA possible workaround is to use a regular Notebook Instance instead of a Studio Notebook Instance. On a regular Notebook Instance of the same size (ml.g4dn.xlarge), \/dev\/shm is 7.7G\n\ndf -h | grep -E 'shm|File'\nFilesystem      Size  Used Avail Use% Mounted on\ntmpfs           7.7G     0  7.7G   0% \/dev\/shm",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Inference endpoint not responding when invoked by lambda",
        "Question_creation_time":1643379618036,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxDOb4CEnT1qEzTkgcOLRog\/inference-endpoint-not-responding-when-invoked-by-lambda",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Lambda",
            "Amazon SageMaker",
            "Amazon Elastic Inference"
        ],
        "Question_upvote_count":0,
        "Question_view_count":44,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi fellow AWS users,\n\nI am working on an inference pipeline on AWS. Simply put, I have trained a PyTorch model and I deployed it (and created an inference endpoint) on Sagemaker from a notebook.\n\nOn the other hand, I have a lambda that will be triggered whenever there is a new audio that gets uploaded to my S3 bucket and pass the name of that audio to the endpoint. The endpoint downloads the audio, performs some pre-processing (super-quick) and returns predictions. The lambda then sends these predictions by email.\n\nAudios get uploaded on the S3 bucket on a non regular basis, like around 10 audios a day, at irregular intervals.\n\nThis morning, I tried manually uploading a test audio to the bucket to check if the pipeline was working. It turns out that my endpoint is correctly invoked by my lambda but looking at the endpoint logs nothing happens (and I don't get any email). I tried a couple of times, without any more success. The lambda just ends up timing out after 300ms (what I set). However, invoking the endpoint from my sagemaker notebook worked perfectly fine on the first try and seemed to unblock the endpoint. After that, the endpoint was responsive to the lambda invokation. Was that because the endpoint was not \"cold\" anymore and it was a coincidence, I couldn't tell.\n\nMy questions are:\n\nAre there any differences in endpoint invokations between the two scenarios (from the lambda or from the Sagemaker notebook)?\nHow can we see how much time after an invokation the endpoint will become \"cold\" again? Please correct me If I am wrong using the term cold here. I know it applies to lambdas as well. To what I understood, the endpoint is basically calling my inference script on a ECR container.\nAccording to my use case (number of inferences a day, pre-proccesing lightness, ...), what would be the best option for my endpoint? (async, batch, ...)\nMy lambda seems to try invokation twice in total (invoke 1 - timeout 1 - invoke 2 - timeout 2). Can that be set differently?\nShall I increase the timeout of my lambda and let it try more times until the ECR is \"warm\"? or is there such a setting that can be modified on the endpoint side?\n\nThank you so much in advance for your support.\n\nCheers\n\nAntoine",
        "Answers":[
            {
                "Answer_creation_date":"2022-01-31T16:43:49.398Z",
                "Answer_upvote_count":0,
                "Answer_body":"I had a similar issue and in my case my lambda function was not transforming the input data into the right format for my inference endpoint to digest. Assuming your lambda function takes an input of bucket\/key for the location of the audio file try and mock-up a test directly in lambda to see what errors it is throwing.\n\nPS: Yes there is a difference in how SageMaker Studio calls the inference endpoint and lambda. With lambda the invoke_endpoint API is used and in SageMaker Studio (using MXNET\/Gluon framework) the predict\/predictor method is called.\n\nPS2: Initially to rule out IAM you may want to give Lambda function AmazonS3FullAccess and AmazonSageMakerFullAccess policies.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Bug in the Sagemaker Studio!!",
        "Question_creation_time":1643281342627,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcXnGbPcOSP-sOuXCZIdSRA\/bug-in-the-sagemaker-studio",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":165,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm trying to open the Projects or Model Registry in AWS Sagemaker Studio from the Components and registries icon in the left sidebar. But when I choose either Projects or Model Registry from the dropdown menu it shows nothing except an error message: Response not successful: Received status code 400\n\nThe region I'm working in is Frankfurt (eu-central-1).\n\nThey used to work fine, but it's been a few days that I see this error. Please someone help me with this, I need to approve some models as soon as possible.",
        "Answers":[
            {
                "Answer_creation_date":"2022-01-27T17:57:29.271Z",
                "Answer_upvote_count":2,
                "Answer_body":"Please update to latest Studio version (3.21.1 or greater) and then do Application Reset. Please let us know if you continue to see the issue.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Using R model in SageMaker ML pipelines",
        "Question_creation_time":1643230196748,
        "Question_link":"https:\/\/repost.aws\/questions\/QU17aS4s7uSRqmiLuveuchBw\/using-r-model-in-sage-maker-ml-pipelines",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":97,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi there,\n\nIs it possible to use R model training and serving in SageMaker ML Pipelines? Looked in examples here. And it doesn't look that R is fully supported currently by ML Pipelines. Any examples and success stories are very welcome.\n\nThanks.",
        "Answers":[
            {
                "Answer_creation_date":"2022-01-28T21:07:43.709Z",
                "Answer_upvote_count":1,
                "Answer_body":"In general it is possible to use the SageMaker python SDK and boto3 using the reticulate package in R, However do not have direct examples of SageMaker Pipelines using R.\n\nIt is possible to orchestrate the production pipeline using the R Containers for training and serving and setting up the DAG can be done with reticulate and SageMaker Python SDK and can be achieved using the AWS Step Functions. Please refer to the following example for reference.\n\nhttps:\/\/github.com\/aws-samples\/reinvent2020-aim404-productionize-r-using-amazon-sagemaker https:\/\/www.youtube.com\/watch?v=Zpp0nfvqDCA",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"how to choose ml.g4dn.* instances in sagemaker processing jobs",
        "Question_creation_time":1643215786791,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXqikCZktSFywwXL14PWcYg\/how-to-choose-ml-g-4-dn-instances-in-sagemaker-processing-jobs",
        "Question_topic":[
            "Machine Learning & AI",
            "Compute"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon EC2"
        ],
        "Question_upvote_count":1,
        "Question_view_count":302,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have to perform some data manipulation for which the sagemaker \"processing job\" would fit perfectly. Such jobs would benefit from GPU and thus I was looking to use instances from the ml.g4dn family for cost efficiency. Unfortunately, I cant see them available in the dropdown when creating a processing job from the aws dashboard, only when creating training jobs. I previously requested the limit increase to the aws support, and i was told it was not necessary and up to 20 instances could be run in the chosen region.\n\nAm I missing anything? do i have to enable the instance family somewhere else?\n\nthanks",
        "Answers":[
            {
                "Answer_creation_date":"2022-01-26T22:19:03.162Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi there, thanks for reaching out about your issue. Are you able to use the AWS CLI or SDKs to start Processing jobs with the ml.g4dn instance types? Also, could you clarify whether AWS support increased your ml.g4dn quota for both Processing and Training or not?",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How can we connect a Sagemaker Studio user to a gitlab repo within a private VPN?",
        "Question_creation_time":1643132865842,
        "Question_link":"https:\/\/repost.aws\/questions\/QURGs7VOVlTzKCG7H2AFLWww\/how-can-we-connect-a-sagemaker-studio-user-to-a-gitlab-repo-within-a-private-vpn",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":115,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"We have a gitlab repo within a private VPN and would like to setup Studio to clone that repo and to push and pull updates. Is that possible yet from within Studio?",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-26T17:41:40.956Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thank you for your response. For those looking to do the same thing, according to AWS Support AWS SageMakers does NOT support GitLab yet and there is no ETA for that feature.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2022-02-24T18:06:03.844Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nThank you for using AWS services.\n\nAWS provides different VPN options like Client VPN and Site-to-Site VPN which might required different configuration and setup to get associated with other AWS resources like SageMaker Studio.\n\nTo answer your question in precise way and provide better assistance, we would like to understand the architecture at your end while implementing this configuration. The best way to understand the architecture is by opening a support case with networking team and discussed more about the options available to connect the Gitlab repo within private VPN to SageMaker Studio.\n\nhttps:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html\n\nFor more reference on AWS VPN:\n\nClient VPN: AWS Client VPN is a managed client-based VPN service that enables you to securely access your AWS resources and resources in your on-premises network. With Client VPN, you can access your resources from any location using an OpenVPN-based VPN client.\n\nSite-to-Site VPN: AWS Virtual Private Network solutions establish secure connections between your on-premises networks, remote offices, client devices, and the AWS global network.\n\nhttps:\/\/docs.aws.amazon.com\/vpn\/latest\/clientvpn-admin\/what-is.html\n\nhttps:\/\/docs.aws.amazon.com\/vpn\/latest\/s2svpn\/VPC_VPN.html\n\nhttps:\/\/docs.aws.amazon.com\/vpn\/index.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How much GPU memory are available on the g4, g5, p3d, and p4d series instances?",
        "Question_creation_time":1643029354176,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvPdBv2rwTEiYKKHDPLUTWA\/how-much-gpu-memory-are-available-on-the-g-4-g-5-p-3-d-and-p-4-d-series-instances",
        "Question_topic":[
            "Machine Learning & AI",
            "Compute"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon EC2",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":398,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Is there a link that shows how much GPU memory is available on the following GPU instances on AWS?\n\ng4-series instances (NVidia T4)\ng5-series instances (NVidia A10)\np3d-series instances (NVidia V100)\np4d-series instances (NVidia A100)\n\nUpdate: the information is available for the p3d series and g5 series, though not for the g4 series or the p4 series instances. Is it possible to retrieve the information for the latter two instances anywhere (without having to launch the instances)?",
        "Answers":[
            {
                "Answer_creation_date":"2022-01-24T13:17:36.730Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello Nathaniel, You can find this information on the launch blogs here:\n\nfor G4 series: 16GB GPU Memory https:\/\/aws.amazon.com\/blogs\/aws\/now-available-ec2-instances-g4-with-nvidia-t4-tensor-core-gpus\/\n\nfor P4 ultraclusters: 320GB GPU Memory https:\/\/aws.amazon.com\/blogs\/aws\/new-gpu-equipped-ec2-p4-instances-for-machine-learning-hpc\/\n\nHope this helps",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Forecast - how to handle missing values in the dataset",
        "Question_creation_time":1642832665715,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEBKd6SqjSJuVY3xYuMgC5w\/forecast-how-to-handle-missing-values-in-the-dataset",
        "Question_topic":[
            "Analytics",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Analytics",
            "AWS Data Pipeline",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Forecast"
        ],
        "Question_upvote_count":0,
        "Question_view_count":31,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a few questions regarding data preparation for Forecast.\n\nI have a dataset with about 3,000 item_id's, the data is recorded on weekdays only (no row for weekends\/holidays), and the forecast horizon is 1 day. For example:\n[item_id | timestamp | target_value]\nitem_A | 2022-01-19 (Wed) | 100\nitem_A | 2022-01-20 (Thurs) | 101\nitem_A | 2022-01-21 (Fri) | 99\nitem_A | 2022-01-24 (Mon) | 98\nitem_A | 2022-01-25 (Tues) | 102\n\nQ1. Is it recommended that the weekends (1\/22, 1\/23) row is inserted to the dataset with NaN as the target_value?\n\nQ2. If target_value for a timestamp is NaN, do the RTS attributes get ignored regardless of what value it is?\n\nQ3. When the Forecast is training, does Forecast recognize that on Friday, the next value to predict is on Monday rather than on Saturday?\n\nQ4. If an item_id has small time series data points (for instance, global startend date ranges from 2018 to 2021, but a particular item_id only has data recorded for a couple months in 2020), should I front-fill & back-fill with NaN to match the global startend range? (I only intend to use the data for training, not creating a forecast)",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"Use crowd-textract-analyze-document with start_human_loop",
        "Question_creation_time":1642783549682,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuEnsW0WnRI6HU8TvW7FEWA\/use-crowd-textract-analyze-document-with-start-human-loop",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth"
        ],
        "Question_upvote_count":0,
        "Question_view_count":26,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm trying to use the crowd-textract-analyze-document widget on a custom task with sagemaker.\n\nThe problem is call the human loop from \"start_human_loop\" function not from analyze_document.\n\nThe human loop tasks is not rendered, the error can be reproduced using \"render_ui_template\" function\n\nresponse = sagemaker_client.render_ui_template( UiTemplate={\"Content\": template_content}, Task={\"Input\": input_json}, RoleArn=role, )\n\nI'm using the default template from the documentation. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/a2i-crowd-textract-detection.html With the minimal ranges to read the data from task.input, so I assume than the error is on the input data.\n\nThe error raised is\n\ncrowd-html-elements-without-ce-polyfill.js:689 Uncaught TypeError: Cannot read properties of undefined (reading 'text')\n\nThe entire json used is this one that includes the \"text\" attribute, so I don't have any idea of why means the error:\n\n{\n  \"TaskObject\": \"s3:\/\/foo_bar\/foo_bar.pdf\",\n  \"Keys\": [\n    {\n      \"importantFormKey\": \"Foo bar\"\n    }\n  ],\n  \"Blocks\": [\n    {\n      \"blockType\": \"KEY_VALUE_SET\",\n      \"confidence\": 93.0,\n      \"geometry\": {\n        \"boundingBox\": {\n          \"width\": 0.09730120003223419,\n          \"height\": 0.009636270813643932,\n          \"left\": 0.5012893676757812,\n          \"top\": 0.3701384961605072\n        },\n        \"polygon\": [\n          {\n            \"x\": 0.5012893676757812,\n            \"y\": 0.3701384961605072\n          },\n          {\n            \"x\": 0.5985905528068542,\n            \"y\": 0.3701384961605072\n          },\n          {\n            \"x\": 0.5985905528068542,\n            \"y\": 0.3797747790813446\n          },\n          {\n            \"x\": 0.5012893676757812,\n            \"y\": 0.3797747790813446\n          }\n        ]\n      },\n      \"id\": \"6231be31-5f56-41db-95cf-2dad8f765cca\",\n      \"relationships\": [\n        {\n          \"type\": \"VALUE\",\n          \"ids\": [\n            \"b7697bbd-c5f5-4d28-a345-92c1f53daef7\"\n          ]\n        },\n        {\n          \"type\": \"CHILD\",\n          \"ids\": [\n            \"57c87916-0636-4d51-8d3b-8d15f4e93d73\",\n            \"f38b7a0f-2750-46e0-960f-8d6b58dcab3d\"\n          ]\n        }\n      ],\n      \"entityTypes\": [\n        \"KEY\"\n      ],\n      \"text\": \"Foo bar\",\n      \"page\": 1\n    },\n    {\n      \"blockType\": \"KEY_VALUE_SET\",\n      \"confidence\": 93.0,\n      \"geometry\": {\n        \"boundingBox\": {\n          \"width\": 0.2203546017408371,\n          \"height\": 0.019548991695046425,\n          \"left\": 0.6025875210762024,\n          \"top\": 0.3636907935142517\n        },\n        \"polygon\": [\n          {\n            \"x\": 0.6025875210762024,\n            \"y\": 0.3636907935142517\n          },\n          {\n            \"x\": 0.8229421377182007,\n            \"y\": 0.3636907935142517\n          },\n          {\n            \"x\": 0.8229421377182007,\n            \"y\": 0.3832397758960724\n          },\n          {\n            \"x\": 0.6025875210762024,\n            \"y\": 0.3832397758960724\n          }\n        ]\n      },\n      \"id\": \"b7697bbd-c5f5-4d28-a345-92c1f53daef7\",\n      \"relationships\": [\n        {\n          \"type\": \"CHILD\",\n          \"ids\": [\n            \"d803e2ba-c238-4af1-8745-0dbb17e74420\"\n          ]\n        }\n      ],\n      \"entityTypes\": [\n        \"VALUE\"\n      ],\n      \"text\": \"Foo bar\",\n      \"page\": 1\n    }\n  ]\n}",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"Pros and cons of restricting user access to certain regions",
        "Question_creation_time":1642700804560,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxt7fqO9HQrKWDfi4V4Lagg\/pros-and-cons-of-restricting-user-access-to-certain-regions",
        "Question_topic":[
            "Security, Identity, & Compliance",
            "Machine Learning & AI",
            "Management & Governance"
        ],
        "Question_tag":[
            "AWS Identity and Access Management",
            "Amazon SageMaker",
            "AWS Account Management"
        ],
        "Question_upvote_count":0,
        "Question_view_count":43,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hello, Are there any drawbacks I should be aware of if we restrict user access to only a single region?\n\nWe use a variety of AWS services but mainly S3 and Sagemaker Studio. Our team is located in various locations so their default regions are different. It has been a challenge to keep track of studio instances when they are created in different regions so we are now considering restricting access to a single region. Are there issues that we may face in that case? Any services we may miss?",
        "Answers":[
            {
                "Answer_creation_date":"2022-01-20T19:09:44.219Z",
                "Answer_upvote_count":4,
                "Answer_body":"I would take a look at this for some potential edge cases. In summary, you may need to allow us-east-1 and us-west-2 in addition to whatever regions your team is in since they host some of the global service endpoints (like IAM, Route 53, Global Accelerator, and a few others). For STS, I would use the regional endpoints if you aren't already.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Serverless Inference - Limit number of workers",
        "Question_creation_time":1642602434394,
        "Question_link":"https:\/\/repost.aws\/questions\/QUWYP78UdYQseoErcj4kjiug\/serverless-inference-limit-number-of-workers",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":161,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"We've deployed a HuggingFace model to Sagemaker as a serverless endpoint. We set memory to be 6GB and max concurrency to be 1. With these settings, we keep getting errors when we call invoke_endpoint. Not all the time, but about 60% of the time...\n\nWhen we check the logs and metrics, we see that the memory has gone up to almost 100%. We also see that, since the machine has 6 CPUs, if starts 6 workers. We believe this could be the cause of the problem. How can se set the number of workers?\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_date":"2022-01-19T17:44:50.854Z",
                "Answer_upvote_count":0,
                "Answer_body":"From \u201csagemaker.pytorch.model.PyTorchModel\u201d documentation:\n\nmodel_server_workers (int) \u2013 Optional. The number of worker processes used by the inference server. If None, server will use one worker per vCPU.\n\nYou can see this example on how to set \u201cMODEL_SERVER_WORKERS\u201d environment variable to set number of workers.\n\nenv={\n    \"MODEL_SERVER_WORKERS\":\"2\"\n    }\n\nlocal_regressor = Estimator(\n    image,\n    role,\n    instance_count=1,\n    instance_type=\"local\")\n\ntrain_location = 'file:\/\/'+local_train\nvalidation_location = 'file:\/\/'+local_validation\nlocal_regressor.fit({'train':train_location, 'validation': validation_location}, logs=True)\n\npredictor = local_regressor.deploy(1, 'local', serializer=csv_serializer, env=env)\n\n\nHope it helps.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-01-19T16:04:33.089Z",
                "Answer_upvote_count":0,
                "Answer_body":"Eitan, thanks for replying.\n\nI'm not sure if this worked or not, as not the cloudwatch logs are not showing the number of workers anymore! The performance seems to be the same, however. It's failing more often than it's responding. And still reaching almost 100% memory.\n\nInstead of your code, I used the following, as I'm deploying a Hugging Face model:\n\nhuggingface_model = HuggingFaceModel(\n    name=model_name,\n    model_data=os.path.join(\"s3:\/\/\" + tar_bucket_name, tarfile_name),\n    env={\n        'HF_TASK': 'text-classification',\n        'MODEL_SERVER_WORKERS': '1',\n        'MODEL_SERVER_TIMEOUT': '300'\n    },\n    role=sagemaker.get_execution_role(),\n    entry_point='inference.py',\n    transformers_version='4.12.3',\n    pytorch_version='1.9.1',\n    py_version='py38'\n)\n\n\nTwo follow up questions then, if you don't mind:\n\nHow can I see if the serverless function actually created only one worker per instance?\nWhere can I find all the different environment variables accepted by SageMaker?\n\nMany thanks!\n\nRogerio",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"sagemaker online and offline store question",
        "Question_creation_time":1642578480105,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxxGzLRsPQk6iAlG-yEv4VQ\/sagemaker-online-and-offline-store-question",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":1,
        "Question_view_count":245,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I looked at SM feature store documentation and see a flag named is_online_Enable for ingestion. My queries are -\n\nIf i make it false, will it only store the data in offline store ?\nIf I make it true will it store data both in online and offline store? if yes , both the store will have same data right?\nIs there any way to store my data only in online or only in offline store ? if yes, what will be the configuration?\n\nAlso is there any scenario while ingesting data there is some syn happens between online and offline store or vice versa. I see documentation mentioning it will be available after some minutes and for our use case we want to know is there any sync happens( is there any scenario where data goes to online store first and then sync up with offline store or vice versa and some latency associated with the later store)?",
        "Answers":[
            {
                "Answer_creation_date":"2022-01-19T13:20:33.916Z",
                "Answer_upvote_count":0,
                "Answer_body":"SageMaker Feature Store allows feature groups that are online-only, offline-only, or both online and offline. When your feature group supports both, any features ingested to the online store, are also replicated to the offline store in append-only fashion, enabling a full history for training, batch scoring, row-level time travel, etc.. Yes, you set EnableOnlineStore to True to indicate your feature group should have online enabled. You provide an OfflineStoreConfig to indicate that it should have an offline store as well.\n\nThis blog post provides an overview of the capabilities and concepts.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"I am not able to find this solution in sagemaker jumpstart",
        "Question_creation_time":1642562655390,
        "Question_link":"https:\/\/repost.aws\/questions\/QULbPt5pRtT1q8sV-CK5wPhA\/i-am-not-able-to-find-this-solution-in-sagemaker-jumpstart",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Build & Train ML Models",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon SageMaker Ground Truth"
        ],
        "Question_upvote_count":0,
        "Question_view_count":111,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Link for the post: https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-custom-amazon-sagemaker-pytorch-models-for-real-time-handwriting-text-recognition\/ I am unable to find this in sagemaker jumpstart. Please guide me through this.",
        "Answers":[
            {
                "Answer_creation_date":"2022-03-15T10:51:30.251Z",
                "Answer_upvote_count":0,
                "Answer_body":"I can confirm that the solution is available in JumpStart. Can I ask what steps you have undertaken to find the solution?",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker error: \"unexpected EOF\"",
        "Question_creation_time":1642551770380,
        "Question_link":"https:\/\/repost.aws\/questions\/QULdleKKPDQpO1CPT_VHpl-A\/sage-maker-error-unexpected-eof",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":44,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"We are trying to run a SageMaker batch transform job and we're getting some errors:\n\n2022-01-18T23:29:00.980:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD 2022-01-18T23:34:21.071:[sagemaker logs]: <<<path to csv file in s3>>>: Unable to get response from algorithm: unexpected EOF\n\nWe do not understand what \"Unable to get response from algorithm: unexpected EOF\" means. How can we get more details about this error?\n\nIt would help if we could get the full request and full response from the endpoint. Is this information recorded somewhere in SageMaker?\n\nWe have added extra logging in our docker image and we are not able to find an issue on that end. We also tried to log the request and response but those got truncated in CloudWatch.\n\nWe would be grateful for any pointers that you can provide. Thanks.\n\nSebastien",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"java.lang.IllegalArgumentException in SageMaker",
        "Question_creation_time":1642538521034,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXl66qTr3TBWJjO5td_K0jw\/java-lang-illegal-argument-exception-in-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":43,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm unable to invoke the my SageMaker endpoint. I'm seeing this error in the endpoint logs\n\njava.lang.IllegalArgumentException: reasonPhrase contains one of the following prohibited characters: \\r\\n: tokenizers>=0.10.1,<0.11 is required for a normal functioning of this module, but found tokenizers==0.11.2.\n\n\nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git\n\n\nMy Sagemaker endpoint is invoked through a lambda function. The code that calls the sagemaker endpoint is:\n\nSM_ENDPOINT_NAME = \"pytorch-inference-2021-xx-xx\"\nsm_runtime= boto3.client('runtime.sagemaker')\ntxt = \"Canon SELPHY CP1300 Compact Photo Printer\"\nresponse = sm_runtime.invoke_endpoint(EndpointName=SM_ENDPOINT_NAME, ContentType='text\/plain', Body=txt)\n\n\nThe response is supposed to contain a vector.\n\nIt's been working fine previously but I started seeing this exception today.\n\nIs this a bug in SageMaker? If not, how do I fix it?",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"How do I check my current SageMaker service quotas?",
        "Question_creation_time":1642480202619,
        "Question_link":"https:\/\/repost.aws\/questions\/QUweO83CSlTu-3Zn2RxdESWg\/how-do-i-check-my-current-sage-maker-service-quotas",
        "Question_topic":[
            "Management & Governance",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Management Console",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":533,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"How do I check my current service quotas for Amazon SageMaker?\n\nIn the case of Amazon EC2, service quotas can be checked here: https:\/\/console.aws.amazon.com\/servicequotas\/home\/services\/ec2\/quotas\n\nFor SageMaker, the default quotas are listed here: https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html but there isn't a link to where one can find the current region-specific quotas for an account, which could have changed after a request for a service quota increase.",
        "Answers":[
            {
                "Answer_creation_date":"2022-01-18T22:33:29.893Z",
                "Answer_upvote_count":1,
                "Answer_body":"Amazon SageMaker has now been integrated with Service Quotas. You should be able to find current SageMaker quotas for your account in the Service Quotas console. You can also request for a quota increase right from the Service Quotas console itself. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/regions-quotas.html#regions-quotas-quotas",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2022-06-15T18:32:09.862Z",
                "Answer_upvote_count":1,
                "Answer_body":"Unfortunately, AWS Sagemaker is not supported for direct visibility into the service quotas. We have an existing feature request for SageMaker Integration with the Service Quotas page as below. However, I currently done have an ETA for the same as it would depend on feasibility and further integration.\n\nHaving said that, the below documentations provides the default values as you mentioned: [+] https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html\n\nI would suggest to contact the AWS Support when ever you wish to get to get insights on current quotas currently.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Use instance (NVMe) storage in SageMaker Studio notebooks",
        "Question_creation_time":1642415387301,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfsg0pVUAQge5kFrXBC6sZQ\/use-instance-nv-me-storage-in-sage-maker-studio-notebooks",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":23,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm exploring and pre-processing some raw data in SageMaker Studio which is split across many (20k+++) small files, and running into slow speed because SMStudio's main user storage is backed by EFS rather than an EBS volume as used on SageMaker Notebook Instances (NBIs). Navigating and manipulating this dataset is slower in Studio because of the extra metadata introduced by the communication being at filesystem level, rather than just a block storage device.\n\nI know there's a little ephemeral block storage available to notebooks under \/tmp which can help with these issues (as used here, in fact), but thought it would be more scalable to make use of the proper NVMe instance storage available with ml.m5d.* instances in Studio to work with bigger datasets.\n\nOnly trouble is, I'm not sure how to use these instance storage volume(s) from notebooks? When I run !df -aTh, the NVMe device only seems to be mounted on some very specific points as shown below:\n\nFilesystem        Type      Size  Used Avail Use% Mounted on\n[...]\n127.0.0.1:\/200015 nfs4      8.0E   57G  8.0E   1% \/root\n\/dev\/nvme0n1p1    xfs       124G   14G  111G  11% \/opt\/.sagemakerinternal\n\/dev\/nvme0n1p1    xfs       124G   14G  111G  11% \/etc\/resolv.conf\n\/dev\/nvme0n1p1    xfs       124G   14G  111G  11% \/etc\/hostname\n\/dev\/nvme0n1p1    xfs       124G   14G  111G  11% \/etc\/hosts\n\/dev\/nvme0n1p1    xfs       124G   14G  111G  11% \/var\/log\/studio\n\/dev\/nvme0n1p1    xfs       124G   14G  111G  11% \/var\/log\/apps\n\/dev\/nvme0n1p1    xfs       124G   14G  111G  11% \/opt\/ml\/metadata\/resource-metadata.json\n[...]\n\n\nShould I be creating a new mount somehow to access the storage? Any particular best-practices to follow?",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"SageMaker AutoML generates ExpiredTokenException",
        "Question_creation_time":1642293400372,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPU-nfYYIRbmM-tJpkG6XqA\/sage-maker-auto-ml-generates-expired-token-exception",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":21,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\n\nI can train models using different AWS SageMaker estimators, but when I use SageMaker AutoML Python SDK the following error occurs about 15 minutes into the model training process:\n\n\"botocore.exceptions.ClientError: An error occurred (ExpiredTokenException) when calling the DescribeAutoMLJob operation: The security token included in the request is expired\"\n\nThe role used to create the AutoML object is associated with the following AWS pre-defined policies as well as one inline policy. Can you please let me know what I\u2019m missing that's causing this ExpiredTokenException error?\n\nAmazonS3FullAccess AWSCloud9Administrator AWSCloud9User AmazonSageMakerFullAccess\n\nInline policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"iam:PassRole\" ], \"Resource\": \"\", \"Condition\": { \"StringEquals\": { \"iam:PassedToService\": \"sagemaker.amazonaws.com\" } } }, { \"Effect\": \"Allow\", \"Action\": [ \"sagemaker:DescribeEndpointConfig\", \"sagemaker:DescribeModel\", \"sagemaker:InvokeEndpoint\", \"sagemaker:ListTags\", \"sagemaker:DescribeEndpoint\", \"sagemaker:CreateModel\", \"sagemaker:CreateEndpointConfig\", \"sagemaker:CreateEndpoint\", \"sagemaker:DeleteModel\", \"sagemaker:DeleteEndpointConfig\", \"sagemaker:DeleteEndpoint\", \"cloudwatch:PutMetricData\", \"logs:CreateLogStream\", \"logs:PutLogEvents\", \"logs:CreateLogGroup\", \"logs:DescribeLogStreams\", \"s3:GetObject\", \"s3:PutObject\", \"s3:ListBucket\", \"ecr:GetAuthorizationToken\", \"ecr:BatchCheckLayerAvailability\", \"ecr:GetDownloadUrlForLayer\", \"ecr:BatchGetImage\" ], \"Resource\": \"\" } ] }\n\nThanks, Stefan",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"How to specify instance type when training models using SageMaker AutoML Python SDK",
        "Question_creation_time":1642292454744,
        "Question_link":"https:\/\/repost.aws\/questions\/QUClX3PJmFSX-aIaTBlaGidw\/how-to-specify-instance-type-when-training-models-using-sage-maker-auto-ml-python-sdk",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":62,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nIs there a way to specify the instance type when training models using the SageMaker AutoML Python SDK? The AutoML.deploy method takes an instance_type argument, but the AutoML.fit method does not take an instance_type argument.\n\nThanks, Stefan",
        "Answers":[
            {
                "Answer_creation_date":"2022-01-17T00:59:16.641Z",
                "Answer_upvote_count":0,
                "Answer_body":"As I understand it's not currently possible to customize this instance type selection - the underlying CreateAutoMLJob API doesn't offer any instance type controls so there's no way for the Python SDK AutoML class to expose them.\n\nAlthough it might be nice to have some more user control over this in future, it's worth mentioning that the options might be non-trivial: For example different stages of the autoML process encapsulated within fit() (like pre-processing, training, explainability analysis) might have different infrastructure needs, and different tested algorithms running in parallel might also have different optimal choices for a given dataset.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Deploying large scale ML model",
        "Question_creation_time":1642203662068,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzS24kMebSh2QCg4bXDvX6Q\/deploying-large-scale-ml-model",
        "Question_topic":[
            "Machine Learning & AI",
            "Database"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Database"
        ],
        "Question_upvote_count":0,
        "Question_view_count":41,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi, I am deploying an ML model with a retrieval component from AWS and it had two parts:\n\nML model that is deployed using Sagemaker. The model isn't big, so this is simple.\nRetrieval: The ML model first retrieves information from a database using ANN algorithm(like Annoy or Scann). The database needs to be loaded into memory at all times for really fast inference. However, the database is big(around 500GB). What is the best way to deploy this database? Is Sagemaker the best bet?",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"Importing externally-trained TensorFlow v2 models to SageMaker deployment",
        "Question_creation_time":1642131421895,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmbEwNJTBTkSPKQvzc6R7eg\/importing-externally-trained-tensor-flow-v-2-models-to-sage-maker-deployment",
        "Question_topic":[
            "TensorFlow on AWS",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "TensorFlow on AWS",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":35,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Can anybody suggest nice, reasonably up-to-date example(s) for importing a previously trained TFv2 model to SageMaker? i.e. tarballing the artifact to S3, and configuring the Model & Endpoint (preferably via SageMaker Python SDK).\n\nMost of the examples I've come across so far are for TFv1. Thanks!",
        "Answers":[
            {
                "Answer_creation_date":"2022-01-14T18:33:31.366Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi! Assuming you already have a trained model on an Amazon S3 location in a tar.gz format then you should be able to load that using the TensorFlowModel class and deploy from there (using the SDK) as shown in the documentation here: https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#deploying-directly-from-model-artifacts\n\nIn the constructor of TensorFlowModel you may also want to pass the keyword argument \"framework_version\" (more on this here: https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/sagemaker.tensorflow.html)\n\nCurrently the supported versions of TFv2 are \"2.0.4\", \"2.1.3\", \"2.2.2\", \"2.3.2\", \"2.4.3\", \"2.5.1\" and \"2.6.0\".\n\nLet me know if this helped answer your question",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Data Wrangler UI Features",
        "Question_creation_time":1641943985816,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcsIt78jnSTW8Ta9__kUm-w\/sage-maker-data-wrangler-ui-features",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":1,
        "Question_view_count":61,
        "Question_answer_count":3,
        "Question_has_accepted_answer":true,
        "Question_body":"The SageMaker Data Wrangler UI in SageMaker Studio doesn't seem to support all the features that the API does. When will the UI support:\n\nLoading all s3 objects under a prefix? https:\/\/aws-data-wrangler.readthedocs.io\/en\/stable\/stubs\/awswrangler.s3.read_csv.html#awswrangler.s3.read_csv\nLoading JSON objects in addition to CSV and Parquet files? https:\/\/aws-data-wrangler.readthedocs.io\/en\/stable\/stubs\/awswrangler.s3.read_json.html#awswrangler.s3.read_json",
        "Answers":[
            {
                "Answer_creation_date":"2022-01-12T20:59:06.281Z",
                "Answer_upvote_count":2,
                "Answer_body":"As mentioned by Tulio Alberto in comments, Amazon SageMaker Data Wrangler (the graphical data preparation feature inside Amazon SageMaker) is separate from AWS Data Wrangler (an open-source data prep utility published by AWS Labs): The two tools are based on different technologies and don't necessarily aim for full feature parity - they just happen to share similar names.\n\nTo my knowledge there's no committed timeline we can share at the moment for when these particular features will make it to SageMaker Data Wrangler, but I think as feature requests they make sense and the reasoning for both is pretty clear: I'm aware that both have been discussed to some extent internally already, and I'd personally like to see them launch too!\n\nThanks for sharing the feedback, and apologies for the naming confusion!",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2022-02-12T01:48:49.303Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Tulio, thanks for the clarification. But doesn't SageMaker Data Wrangler generate code that complies with\/uses AWS Data Wrangler? Isn't there some (if tenuous) connection between the two?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-01-13T05:22:27.466Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nSageMaker Data Wrangler in Studio just launched the JSON\/ORC support and we support import files under a prefix already. Please see the following links\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/data-wrangler-import.html#data-wrangler-import-s3\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/prepare-and-analyze-json-and-orc-data-with-amazon-sagemaker-data-wrangler\/",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to load large amount of data from S3 onto Sagemaker?",
        "Question_creation_time":1641932571368,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4m2DyyJQSSCL1QqclXS6ZA\/how-to-load-large-amount-of-data-from-s-3-onto-sagemaker",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon S3 Glacier",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":1033,
        "Question_answer_count":3,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a notebook on Sagemaker Studio, I want to read data from S3, I am using the code bellow:\n\ns3_client = boto3.client('s3') bucket = 'bucket_name' data_key = 'file_key.csv' obj = s3_client.get_object(Bucket=bucket, Key=data_key) df = pd.read_csv(io.BytesIO(obj['Body'].read())) df.head()\n\nIt works for small datasets but fails along the way with the dataset I'm trying to load which is 15GB. I changed the instance to ml.g4dn.xlarge ( accelerated computing, 4vCPU + 16GiB + 1 GPU), still fails. what am I missing here? Is is about the instance type, or about the code? What is the best way to import large datasets from S3 to sagemaker?\n\nThank you",
        "Answers":[
            {
                "Answer_creation_date":"2022-01-12T15:20:57.834Z",
                "Answer_upvote_count":1,
                "Answer_body":"What is the need to load large dataset onto the notebook? If you are pre-processing then there are better ways to do this - Sagemaker Spark processing job, or have your own spark cluster and process or even possibly Glue. If you are exploring the data, you should just use a smaller data set. If you are loading the data for training, Sagemaker supports different modes to read the data and data doesnt have to be downloaded on the notebook.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-01-11T21:00:32.086Z",
                "Answer_upvote_count":1,
                "Answer_body":"If you want to download the data onto the notebook so that you don't have to load it from S3 each time you want it in Pandas, you should confirm that the volume size is sufficient for the data. It is set to 5 GB by default, which lines up with your scenario.\n\nTo change this, you'll need to edit your notebook instance, expand the \"additional configuration\" drop down and look for the \"Volume size in GB\" field.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-01-11T21:31:44.046Z",
                "Answer_upvote_count":0,
                "Answer_body":"I have used a much simpler approach to reading a single data file into S3 using pandas. For example:\n\nimport pandas as pd\n\nbucket = 'bucket_name'\n\ndata_key = 'file_key.csv'\n\ndf = pd.read_csv( 's3:\/\/{}\/{}'.format(bucket,data_key) )\n\ndf.head()\n\nMaybe this will perform better?",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Train machine learning model using reserved instance",
        "Question_creation_time":1641871148701,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsy3vkTMkSA2ojA1bmafDSA\/train-machine-learning-model-using-reserved-instance",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":151,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi.\n\nIs it possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance every time which is somewhat time consuming? I'm familiar with local mode, but I understand this is not supported when using AWS SageMaker machine learning estimators.\n\nAppreciate any suggestions for how to make the model training process in SageMaker go faster when using AWS SageMaker machine learning estimators.\n\nThanks, Stefan",
        "Answers":[
            {
                "Answer_creation_date":"2022-01-14T08:13:53.228Z",
                "Answer_upvote_count":0,
                "Answer_body":"As of today, it's not possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance. The service team is currently working on it, unfortunately I don't have an ETA as to when the feature will be released.\n\nLocal Mode is supported for frameworks images (TensorFlow, MXNet, Chainer, PyTorch, and Scikit-Learn) and images you supply yourself.\n\nUsing the SageMaker Python SDK \u2014 sagemaker 2.72.3 documentation\n\nIf you want to train Built-in algorithm models simply faster, you should check the recommendation in the SageMaker document.\n\nExample Blazingtext-instances, Deepar-instances\n\nIf the algorithm supports it, one can also try using Pipe mode or FastFile mode. These offer some fast training job startup time. Accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Canvas failed to import the Redshift Data",
        "Question_creation_time":1641655668805,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCUdYWY0gSV6W60g7ArukXw\/sage-maker-canvas-failed-to-import-the-redshift-data",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon Redshift",
            "Amazon SageMaker Canvas"
        ],
        "Question_upvote_count":0,
        "Question_view_count":103,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Actions:\n\nThe Redshift connection has been setup on SageMaker Canvas.\nThe Redshift already load the sample data (users, sales, etc)\nDrag and drop table 'users' to import pane.\nCheck the import preview can show the data of 'users'\nClick Import\n\nExpected result:\n\nThe dataset can be imported successfully\n\nActual result: Import failed with below details:\n\n{'message': \"Variable '$input' got invalid value None at 'input.uri'; Expected non-nullable type 'String!' not to be None.\", 'locations': [{'line': 1, 'column': 8}], 'path': None}\n\nPlease contact your admin. Request ID: 8b849887-b067-46fc-9be9-dd122e9c8874",
        "Answers":[
            {
                "Answer_creation_date":"2022-02-16T15:57:31.443Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi there @AWS-User-8556114,\n\nThis could be a mishap in the configuration of the Redshift connector on the Canvas service side. Can you try to forcefully close the Canvas app by deleting the connector first, then logging out of Canvas and\/or by deleting the app (by going into the AWS Management Console, SageMaker, your domain, your profile, and deleting the app with type Canvas), then creating it again?\n\nAlternatively, I'd have to ask you to reach out to Support so that they can help you troubleshoot this by looking into your configuration.\n\nThanks!",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Canvas connect Redshift failed",
        "Question_creation_time":1641573857407,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJvjatAJaQv-Ist96WT1IIw\/sage-maker-canvas-connect-redshift-failed",
        "Question_topic":[
            "Security, Identity, & Compliance",
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "AWS Identity and Access Management",
            "Amazon SageMaker",
            "Amazon Redshift",
            "Amazon SageMaker Canvas"
        ],
        "Question_upvote_count":0,
        "Question_view_count":135,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Try to add the Redshift connection on SageMaker Canvas to import the data\n\nThe cluster identify: redshift-cluster-1\ndatabase name: dev\ndatabase user: awsuser\nunload IAM Role: my-reshift-role\nconnection name: redshift\ntype: IAM\n\nmy-reshift-role trust-relationship is trust the \"redshift.amazonaws.com\" and \"sagemaker.amazonaws.com\"\n\nExpectation: create connection successfully\n\nActually result: RedshiftCreateConnectionError Unable to validate connection. An error occurred when trying to list schema from Redshift",
        "Answers":[
            {
                "Answer_creation_date":"2022-01-08T09:30:21.918Z",
                "Answer_upvote_count":0,
                "Answer_body":"The sagemaker canvas using sagemaker domain user, so need add the Redshift permission to the IAM Role attached to domain user. After add the permission, the connection can be setup",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"How to set model custom metadata in Sagemaker ML pipeline",
        "Question_creation_time":1641356351060,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFtq-bSeiRDisaQaEzHr7sQ\/how-to-set-model-custom-metadata-in-sagemaker-ml-pipeline",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":491,
        "Question_answer_count":3,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi there,\n\nI am interested in using model custom metadata. Looks like it got released recently.\nhttps:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/12\/sagemaker-model-registry-endpoint-visibility-custom-metadata-model-metrics\/\n\nMetadata can be set and read successfully via cli aws sagemaker describe-model-package --model-package-name \"arn:aws:sagemaker:us-east-1:ACCOUNT:model-package\/MODEL_PACKAGE_NAME\/1\"\n\naws --profile dev sagemaker describe-model-package --model-package-name \"arn:aws:sagemaker:us-east-1:ACCOUNT:model-package\/MODEL_PACKAGE_NAME\/1\" | jq .CustomerMetadataProperties { \"KeyName1\": \"string2\", \"KeyName2\": \"string2\" }\n\nHowever, it is not clear how custom metadata can be set in Sagemaker ML pipeline when model is train and registered using RegisterModel\n\nThanks in advance.",
        "Answers":[
            {
                "Answer_creation_date":"2022-01-05T21:24:48.071Z",
                "Answer_upvote_count":1,
                "Answer_body":"Have you tried using this parameter on the RegisterModel step?",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2022-01-06T15:52:34.925Z",
                "Answer_upvote_count":0,
                "Answer_body":"That is correct. Figured that. Looks like one to call that API is to have LambaStep and chain it after RegisterModel step . Here is lambda implementation. Feels a bit heavy workaround for this objective. So let me know if there is a better \/lighter way to achieving the same. Thanks.\n\nsagemaker_client = client = boto3.client(\"sagemaker\")\ndef lambda_handler(event, context):\n\n    print(boto3.__version__)\n    logger.info(f\"Received Event: {event}\")\n    logger.info(f\"Boto version: {boto3.__version__}\")\n    \n    model_arn = event['model_arn']\n    model_arn = 'arn:aws:sagemaker:us-east-1:ACCOUNT:model-package\/model_package_name\/version'\n    response = client.describe_model_package(\n        ModelPackageName=model_arn\n    )\n    help(client.update_model_package)\n    response = client.update_model_package(\n        ModelPackageArn=model_arn,\n        ModelApprovalStatus=response['ModelApprovalStatus'],\n        CustomerMetadataProperties={'string': 'string'},\n    )\n\n    return {\"statusCode\": 200, \"body\": json.dumps(\"Model metadata updated successfully\")}",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-02-23T20:40:05.226Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nYou can use the UpdateModelPackage API to update custom metadata properties on an already registered model package whether it's in a pipeline or not.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"XGBOOST inference prediction error with type",
        "Question_creation_time":1640805146200,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-Q8JXVUvSZaEdFwWPHmKJg\/xgboost-inference-prediction-error-with-type",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":152,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to make a prediction in Sagemaker:\n\n#array is a nympy array variable\n\nxgb_predictor.predict(array)\n\nand I am getting this error:\n\nParamValidationError: Parameter validation failed: Invalid type for parameter Body, value: [[0.71028037 0.7866242 0.16398714 0.88787879 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n\n    0.         0.         0.         0.         0.\n\n    0.         1.         0.         0.         0.\n\n    0.         0.         0.         1.         0.\n\n    0.         0.         0.         0.         0.\n\n    0.         0.         0.         0.         0.\n\n    0.         0.         0.         1.         0.\n\n    0.         0.         0.         0.         0.\n\n    0.         0.         0.        ]], type: <class 'numpy.ndarray'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object\n\n\nPlease your help.",
        "Answers":[
            {
                "Answer_creation_date":"2022-01-04T13:56:58.098Z",
                "Answer_upvote_count":0,
                "Answer_body":"What is 'xgb_predictor'? Is this a SageMaker built-in? Or are you in script mode?\n\nConsider using 'array.tobytes' to convert you numpy array to bytes",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-01-04T15:27:59.750Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello and happy new year!\n\nWithout seeing any of your code it is hard to be certain but judging from the error message it seems that you need to change the type\/format of the data that you send through the request. The easiest way to do this can be by utilising one of the bult-in serialisers.\n\nIn this case, since you are using (I assume) an XGBoost model, you can serialise the input into csv format as one of the supported formats for the XGBoost container. To do so, before you call the predict method, set the serialiser to the CSVSerializer as per the below:\n\nxgb_predictor.serializer = sagemaker.serializers.CSVSerializer()\n\nthen you can use the xgb_predictor.predict(array) which should then return the prediction.\n\n*Please note that the array variable should not contain the variable you are trying to predict.\n\n*Here is an example of training & deploying an XGBoost model that you can use as reference https:\/\/github.com\/aws-samples\/amazon-sagemaker-immersion-day\/blob\/master\/xgboost_direct_marketing_sagemaker.ipynb\n\nLet me know if this solves your issue. If not, could you please provide some more details as to the type of model you are training and how you reach to the point you are seeing the error?",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Amazon SageMaker Local Mode raised boto3.exceptions.RetriesExceededError: Max Retries Exceeded",
        "Question_creation_time":1640615850316,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVLhj-0JzTUCLcs7yH5Kznw\/amazon-sage-maker-local-mode-raised-boto-3-exceptions-retries-exceeded-error-max-retries-exceeded",
        "Question_topic":[
            "Machine Learning & AI",
            "Containers"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Elastic Container Registry (ECR)"
        ],
        "Question_upvote_count":0,
        "Question_view_count":83,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I try to run the SageMaker local mode example without any modification at https:\/\/github.com\/aws-samples\/amazon-sagemaker-local-mode\/tree\/main\/pytorch_nlp_script_mode_local_model_inference on my local machine.\n\nHowever I encountered the **boto3.exceptions.RetriesExceededError: Max Retries Exceeded ** exception when the example tries to deploy the inference endpoint to 'local' instance type.\n\nI checked with\n\ndocker images -a \n\n\ncommand and it does not pull the expected pre-built SageMaker deep learning container image from ECR. The code example is using a dummy role for the local SageMaker session. I need help as I am blocked at this point as the exception error message is not helpful to pinpoint the actual root cause of this issue. Thanks in advance.\n\nBelow are my configurations:\n\nUbuntu: 20.04.3 LTS\nAWS CLI version: 2.4.7\nPython: 3.8.12\nDocker: 20.10.12\nDocker Compose: 1.29.2\nboto3: 1.20.26\nsagemaker: 2.72.1",
        "Answers":[
            {
                "Answer_creation_date":"2021-12-27T15:10:36.523Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, if you aren't able to pull the expected pre-built SageMaker deep learning container image, I would check your network settings. From the local mode sample documentation, \"you'll need to be able to access a public Amazon ECR repository from your local environment.\"\n\nFor more detailed information from boto3, you can enable logging and debug mode as well, which could help pinpoint the exact cause of the error. More information on Boto3 Retries: https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/guide\/retries.html.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"GroundTruth text labelling - hide data columns, and methods of quality control",
        "Question_creation_time":1640605742768,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1PUIO8wSSnqbr_9ZF_oQNQ\/ground-truth-text-labelling-hide-data-columns-and-methods-of-quality-control",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Text processing & Analytics",
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth",
            "Amazon Mechanical Turk",
            "Amazon SageMaker Ground Truth Plus"
        ],
        "Question_upvote_count":1,
        "Question_view_count":44,
        "Question_answer_count":0,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a csv of sentences which I'd like labelled, and have identified GroundTruth labelling jobs as a way to do this. Having spent some time exploring the service, I have some questions:\n\n**1) **I can't find a way to display only particular columns to the labellers - e.g. if the dataset has a column of IDs for each sentence, this ideally shouldn't be shown to labellers\n\n2) There is either single labelling or multi labelling, but I would like a way to have two sets of single-selection labels, where one captures difficulty of assigning the label:\n\nSelect one for binary classification a) Yes, b) No\n\nSelect one for difficulty of classification c) Easy, d) Medium, e) Hard\n\nCan this be done using custom HTML? Is there a guide to writing this - the template it gives you doesn't seem to render as-is.\n\n3) There appears to be a maximum of $1.20 payment per task. Is this the case, and why?\n\n4) Having not used mechanical turk before, are there ways of ensuring people take the work seriously and don't just select random answers? I can see there's an option to have x number of people answer the same question, but is there also a way to put in unambiguous questions to which we already have a 'pre_agreed_label' every nth question, and remove people from the task if they get them wrong?\n\nThanks!",
        "Answers":[

        ]
    },
    {
        "Questiont_title":"Sagemaker Studio notebook - no module named 'tensorflow' when chosen image type \"Tensorflow 2.6 Python 3.8 GPU optimized\"",
        "Question_creation_time":1640585801507,
        "Question_link":"https:\/\/repost.aws\/questions\/QUN63fjMWsT5uVUNn2AIsmhw\/sagemaker-studio-notebook-no-module-named-tensorflow-when-chosen-image-type-tensorflow-2-6-python-3-8-gpu-optimized",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":100,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nI have created a simple notebook on SageMaker Studio using the Image \"Tensorflow 2.6 Python 3.8 GPU optimized\". But when I try to run simple statement viz. \"import tensorflow\", I am getting the error \"no module named 'tensorflow'\".\n\nI tried to install 'tensorflow' package using pip from the terminal attached to the image. But it shows the message \"requirement already satisfied\".\n\nAm I missing anything here? Please help.\n\nThanks in advance, Praveen",
        "Answers":[
            {
                "Answer_creation_date":"2021-12-28T19:55:32.675Z",
                "Answer_upvote_count":1,
                "Answer_body":"From the question, I understand that you are trying to use a TensorFlow 2.6 Python 3.8 kernel in SageMaker Studio, but you are unable to import tensorflow.\n\nThe service team are aware of this issue and are actively working on a fix.\n\nMitigation Option\n\nA) If your use case is version flexible, version other than 2.6 should work.\n\nB) If not, you can try the following as workaround\n\nOpen a notebook using a Tensorflow 2.6 Python 3.8 kernel\nExecute the following line in a notebook cell: !sed -i 's|^ *\"python\",| \"\/usr\/local\/bin\/python\",|g' \/usr\/local\/share\/jupyter\/kernels\/python3\/kernel.json\nStop the kernel\nRe-attach the kernel to your notebook.\n\nHope it helps!",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Closing up a Sagemaker user profile - intended behavior?",
        "Question_creation_time":1640092026826,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmSJa7T1nRm6PQkiXDxZJqA\/closing-up-a-sagemaker-user-profile-intended-behavior",
        "Question_topic":[
            "Machine Learning & AI",
            "Cloud Financial Management"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Billing"
        ],
        "Question_upvote_count":0,
        "Question_view_count":529,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I had a Sagemaker user I wasn't using, so I tried to delete it and initially came across this tutorial: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-delete-domain.html . As part of the larger process for how to delete a domain, it shows how to delete any user profiles within that domain. The steps are:\n\nChoose the user.\nOn the User Details page, for each non-failed app in the Apps list, choose Delete app.\nOn the Delete app dialog, choose Yes, delete app, type delete in the confirmation field, and then choose Delete.\nWhen the Status for all apps show as Deleted, choose Delete user.\n\nThe problem comes on the final step: I wasn't able to find a \"Delete user\" button. This feels like a bug, because without such a button the only way to stop charges on a Sagemaker user is to use the CLI, which I eventually did. You can only delete the domain if you have deleted all users, meaning it only works using the CLI for that as well. For every other AWS service I've used, there is an easy way to delete everything from the GUI.",
        "Answers":[
            {
                "Answer_creation_date":"2021-12-21T13:51:27.751Z",
                "Answer_upvote_count":0,
                "Answer_body":"Try clicking on the user, then Edit, and then Delete? I don't remember if that is the exact flow, but I do know that you can do it in the GUI. I've done it a few times.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Unable to use the same HumanWorkflow within textract for more than 1 file\/call",
        "Question_creation_time":1639923120530,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDgM77ZgnTbWjW57_v9rCGw\/unable-to-use-the-same-human-workflow-within-textract-for-more-than-1-file-call",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon Textract",
            "Amazon Augmented AI",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":50,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"I created the private team from the Amazon SageMaker console for labeling tasks followed by the creation of the human review workflow, which I later integrated with the Amazon Textract for Key-Value pair extraction.\n\nWhile I called the analyze_document (along with HumanLoop configuration) to extract key-value pairs for the first time it worked as expected and I was able to see the Job in the labeling project console. However, when I called it again (irrespective of the same or different file) the HumanLoop started giving the below error.\n\n\"[ERROR] InvalidParameterException: An error occurred (InvalidParameterException) when calling the AnalyzeDocument operation: HumanLoop 'textractworkflow1' already exists and it is associated with a different InputContent. Please use a new HumanLoopName and try your request again.\"\n\nDo we have to create a new Human Review Loop each time we trigger analyze_document with another file?",
        "Answers":[
            {
                "Answer_creation_date":"2021-12-29T06:21:48.043Z",
                "Answer_upvote_count":1,
                "Answer_body":"I think you are using the same human loop name for multiple tasks and that is causing this issue. You have to make sure that within the HumanLoopConfig configuration, the HumanLoopName should be unique for each task. You can also refer to this video",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2021-12-27T14:03:50.873Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hello,\n\nI'm from the AWS Textract team. Thanks for using Textract.\n\nMost likely as per the details of the exception mentioned suggests there could be a need of using a new HumanLoopName. I believe someone from sageMaker team will shortly respond to this.\n\nHowever to check if it is due to Textract calls, could you please help us out with the following questions:\n\nCould you please share the entire stack trace of the exception ?\nIs this exception being thrown at the time Textract is called ? If yes, could you check and compare the i\/p parameters passed at the initial time when it passes and the next call when it fails ? Also please share the i\/p parameters for both.\n\nThanks,\n\nNitish",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Giving weights to event types in amazon personalize",
        "Question_creation_time":1639825094332,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSogRKFlfRzC5b8afIwPybQ\/giving-weights-to-event-types-in-amazon-personalize",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Personalize"
        ],
        "Question_upvote_count":0,
        "Question_view_count":174,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"For the VIDEO_ON_DEMAND domain, some use cases include multiple event types. For example, the 'Top picks for you' use case includes two event types 'watch' and 'click'. Is 'watch' given more weight than 'click' when training the model? In general, when there is more than one event type, do domain recommenders give more weight to some event types?\n\nIn our use case, we have a platform that recommends video content. However, we have multiple event types, and some events need to be given more weight than others. Below is the list of our event types in the order of their importance:\n\nSHARE > LIKE > WATCH_COMPLETE > WATCH_PARTIAL > STARTED > SKIP\n\nSo when training the model, we would want 'SHARE' to have more weight than 'LIKE', and 'LIKE' to have more weight than 'WATCH_COMPLETE' and so on.\n\nI was looking into custom solutions. It looks like there is no way to give weights when using Personalize's custom solutions as mentioned in this post...\n\nSo when using Amazon Personalize, should we use domain recommenders or build custom solutions for our use case?\n\n**If we cannot give weights to different event types using Personalize, then what are alternatives? **Should we use Amazon SageMaker and build models from scratch? Open to any and all suggestions.\n\nThank you!",
        "Answers":[
            {
                "Answer_creation_date":"2021-12-21T19:25:06.908Z",
                "Answer_upvote_count":0,
                "Answer_body":"The VOD recommender for \"Top picks for you\" uses the same underlying HRNN-based algorithm noted in the StackOverflow answer you linked. Therefore the answer still applies with respect to weighting event types. That is, Personalize does not support weighting specific event types or specific interactions more than others. Instead, the Top picks for you recommender (as well as the underlying user-personalization recipe) builds sequence models from user sessions which are used to learn each user's interest based on a sequence of events rather than specific event types.\n\nGiven your event taxonomy, including interactions for SHARE, LIKE, and WATCH_COMPLETE in your interactions dataset are good choices since they indicate positive intent by the user. It may make sense to include WATCH_PARTIAL interactions as well (particularly if they represent the user watching the majority of the content, there is not a subsequent WATCH_COMPLETE for the user for the video, and\/or you do not have a sufficient number of WATCH_COMPLETE events across your user base). Otherwise, use WATCH_COMPLETE. If using one of the VOD recommenders, you will need to map your WATCH_COMPLETE events to the required Watch type and you could map the STARTED events to View. The SKIP events could be used as impressions if they can be correlated to a WATCH_COMPLETE or WATCH_PARTIAL event for a video that the user eventually watched (e.g., the user skips through the first 3 videos in a sequence and watches the 4th video could be expressed in a PutEvents call with the 4 videos as impressions and the 4th video as the ItemId that the user Watched).\n\nI suggest not basing the choice of whether to use Personalize or a custom SageMaker model on whether event type weighting is supported. Rather, the choice should be based on the approach that drives the most impact to your business metric (CTR, watch time, etc) with online testing.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Aamazon SageMaker feature store throughput and latency",
        "Question_creation_time":1639718714901,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDX9mkJlNQzaR8lF50Z4H2Q\/aamazon-sage-maker-feature-store-throughput-and-latency",
        "Question_topic":[
            "Machine Learning & AI",
            "AWS Well-Architected Framework"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Performance Efficiency"
        ],
        "Question_upvote_count":0,
        "Question_view_count":272,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"What is the maximum throughput of AWS feature store. Also what is the P99 value of latency of AWS feature store (online store) ?",
        "Answers":[
            {
                "Answer_creation_date":"2021-12-17T08:05:14.816Z",
                "Answer_upvote_count":0,
                "Answer_body":"There are soft limits on Feature Store TPS, feature number etc. But soft limits can be increased based on your need.\n\nMaximum number of feature groups per AWS account: Soft limit of 100.\nMaximum number of feature definitions per feature group: 2500.\nMaximum Transactions per second (TPS) per API per AWS account: Soft limit of 10000 TPS per API excluding the BatchGetRecord API call, which has a soft limit of 500 TPS.\nMaximum size of a record: 350KB.\nMaximum size of a feature value: 350KB.\nMaximum number of concurrent feature group creation workflows: 4.\nBatchGetRecord API: Can contain as many as 100 records and can query up to 10 feature groups.\n\nYou can check Limits and Quotas in the SageMaker Developer Guide.\n\nDepending on your data ingestion use cases, your requirements and runtime context might be different:\n\nExperimenting with new ML features\nStreaming feature ingestion\n*Bulk feature ingestion as part of a batch pipeline **\n\nFor more details on use cases and Feature ingestion APIs, see this blogpost",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Multi-file source_dir bundle with SM Training Compiler (distributed)",
        "Question_creation_time":1639669045329,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwcM0XER5TcOggtQ_5cfVPw\/multi-file-source-dir-bundle-with-sm-training-compiler-distributed",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Natural Language Processing",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":29,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I'm hoping to use SageMaker Training Compiler with a (Hugging Face Trainer API, PyTorch) program split across multiple .py files for maintainability. The job needs to run on multiple GPUs (although at the current scale, multi-device single-node would be acceptable).\n\nFollowing the docs, I added the distributed_training_launcher.py launcher script to my source_dir bundle, and passed in the true training script via a training_script hyperparameter.\n\n...But when the job tries to start, I get:\n\nTraceback (most recent call last):\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\nreturn _run_code(code, main_globals, None,\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 90, in <module>\nmain()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 86, in main\nxmp.spawn(mod._mp_fn, args=(), nprocs=args.num_gpus)\nAttributeError: module 'train' has no attribute '_mp_fn'\n\n\nAny ideas what might be causing this? Is there some particular limitation or additional requirement for training scripts that are written over multiple files?\n\nI also tried running in single-GPU mode (p3.2xlarge) instead - directly calling the train script instead of the distributed launcher - and saw the below error which seems to originate within TrainingArguments itself? Not sure why it's trying to call a 'tensorflow\/compiler' compiler when running in PT..?\n\nEDIT: Turns out the below error can be solved by explicitly setting n_gpus as mentioned on the troubleshooting doc - but that takes me back to the error message above\n\nFile \"\/opt\/ml\/code\/code\/config.py\", line 124, in __post_init__\nsuper().__post_init__()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 761, in __post_init__\nif is_torch_available() and self.device.type != \"cuda\" and (self.fp16 or self.fp16_full_eval):\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 975, in device\nreturn self._setup_devices\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1754, in __get__\ncached = self.fget(obj)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 918, in _setup_devices\ndevice = xm.xla_device()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 231, in xla_device\ndevices = get_xla_supported_devices(\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 137, in get_xla_supported_devices\nxla_devices = _DEVICES.value\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/utils\/utils.py\", line 32, in value\nself._value = self._gen_fn()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 19, in <lambda>\n_DEVICES = xu.LazyProperty(lambda: torch_xla._XLAC._xla_get_devices())\nRuntimeError: tensorflow\/compiler\/xla\/xla_client\/computation_client.cc:273 : Missing XLA configuration",
        "Answers":[
            {
                "Answer_creation_date":"2022-07-15T08:01:47.440Z",
                "Answer_upvote_count":0,
                "Answer_body":"Ahh I solved this a while ago and forgot to update -\n\nYes, the training script needs to define a _mp_fn (which can just execute the same code as gets run if __name__ == \"__main__\") and number of GPUs (at least the last time I checked - hopefully this could change in future) needs to be explicitly configured.\n\nFor my particular project the fix to enable SMTC on the existing job is available online here. For others would also suggest referring to the official SMTC example notebooks & scripts!",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Studio PyTorch 1.8 kernel has no PyTorch, Numpy, or Matplotlib module",
        "Question_creation_time":1639384597526,
        "Question_link":"https:\/\/repost.aws\/questions\/QUns1rahq-ShmzYk0GJoLGWA\/sage-maker-studio-py-torch-1-8-kernel-has-no-py-torch-numpy-or-matplotlib-module",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":210,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm working with SageMaker studio with the following options:\n\nkernel: PyTorch 1.8 Python 3.6 GPU optimized.\ninstance: ml.g4dn.xlarge\n\nWhen running import torch numpy, matplotlib or PIL, I'm getting the No module named 'X' error. No matter when using pip install in a cell above, it will not be imported. Is this a problem only I am encountering with the new PyTorch 1.8 kernel? It also happens with the CPU-optimized version. However, PyTorch 1.6 kernel does not throw an error.\n\nWhen running conda list, I get the output without any of the previously mentioned modules.",
        "Answers":[
            {
                "Answer_creation_date":"2021-12-13T08:55:47.690Z",
                "Answer_upvote_count":0,
                "Answer_body":"I have followed your explanation to recreate the No module named 'X'. After instance lunch, I could import numpy, matplotlib and PIL but could not import torch because torch is not initially installed.\n\nI followed these steps to install and import torch:\n\nShut down your instance completely.\nAttach the kernel to your notebook again\nLunch your ml.g4dn.xlarge instance again\npip3 install torch\nimport torch",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2021-12-14T20:32:06.803Z",
                "Answer_upvote_count":0,
                "Answer_body":"No module named 'X' should not be the expected behavior. Thanks for reporting this issue and sorry for the inconvenience it caused.\n\nIs your use case flexible to use other PyTorch versions prior to 1.8? If yes, please try other versions. If PT1.8 is the only choice, please try following workaround for unblocking(while service team is working on the fix).\n\nTwo options:\n1 - executed following in a notebook cell\n\n# switch the python execution to \/usr\/local\/bin\/python in the kernel.json file. \n!sed 's|^ *\"python\",|  \"\/usr\/local\/bin\/python\",|g' \/usr\/local\/share\/jupyter\/kernels\/python3\/kernel.json>\/tmp\/kernel.json; cp -f \/tmp\/kernel.json \/usr\/local\/share\/jupyter\/kernels\/python3\/kernel.json;\n\n\n2 - directly execute following shell command in kernel image specific terminal(not the global terminal).\n\nsed 's|^ *\"python\",|  \"\/usr\/local\/bin\/python\",|g' \/usr\/local\/share\/jupyter\/kernels\/python3\/kernel.json>\/tmp\/kernel.json; cp -f \/tmp\/kernel.json \/usr\/local\/share\/jupyter\/kernels\/python3\/kernel.json;`\n\n\nThe above command is only needed once per kernel gateway app. After above, please restart the kernel. You can verify using following command in a notebook cell. The '\/usr\/local\/bin\/python' should be shown as python executable.\n\nimport sys\nprint(sys.executable)",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Do I have to redownload dataset to training job every time I run a Sagemaker Estimator training job?",
        "Question_creation_time":1639259390804,
        "Question_link":"https:\/\/repost.aws\/questions\/QUleNfBVthSaGI7rAT2wsKWQ\/do-i-have-to-redownload-dataset-to-training-job-every-time-i-run-a-sagemaker-estimator-training-job",
        "Question_topic":[
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "High Performance Compute",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":157,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi, Over the coming weeks I'll be running some deep learning experiments using the PyTorch Sagemaker estimator, and I was wondering if it would be possible to avoid re-downloading my dataset every time I call estimator.fit()?\n\nIs there a way to do this without using FastFile mode - ie downloading the dataset once and using the same docker image?\n\nIf it's not possible to do it with online instances, would it be possible to re-use the docker instance used if I was to run it in local mode (ie instance_type='local_gpu') - if so, how?\n\nAnd just to add, I am using S3 for the input data.\n\nMany thanks, Tim",
        "Answers":[
            {
                "Answer_creation_date":"2021-12-12T11:16:41.095Z",
                "Answer_upvote_count":1,
                "Answer_body":"Hi Tim, SageMaker training job will need to download\/stream in data from S3. Currently by default, the training job's input data config is file mode which means the data will be downloaded from s3. We have launched a new mode called fast file mode which will stream data in while the job runs. If you are aware of the pipe mode, the fast file mode is a combination of file mode and pipe mode, which streams data in to the training instance without any code change. Please refer to the what's new doc https:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/10\/amazon-sagemaker-fast-file-mode\/. To use the fast file mode, you just simply change the configuration of your estimator according (https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/dev\/src\/sagemaker\/estimator.py#L151). Additionally, SageMaker training job does support other data storage source other than S3. You can use EFS or FSx for Lustre to speed up your training by eliminating the need to download data as used in file mode. You can refer to the blog here https:\/\/aws.amazon.com\/blogs\/machine-learning\/speed-up-training-on-amazon-sagemaker-using-amazon-efs-or-amazon-fsx-for-lustre-file-systems\/",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"AWS SageMaker Endpoint Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check",
        "Question_creation_time":1639162238458,
        "Question_link":"https:\/\/repost.aws\/questions\/QUV4VuYCKHTEedPanW-keHTQ\/aws-sage-maker-endpoint-failed-reason-the-primary-container-for-production-variant-all-traffic-did-not-pass-the-ping-health-check",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":983,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Links to the AWS notebooks for reference https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/xgboost_bring_your_own_model\/xgboost_bring_your_own_model.ipynb\n\nhttps:\/\/github.com\/aws-samples\/amazon-sagemaker-local-mode\/blob\/main\/xgboost_script_mode_local_training_and_serving\/code\/inference.py\n\nI am using the example from the notebooks to create and deploy an endpoint to AWS SageMaker Cloud. I have passed all the checks locally and when I attempt to deploy the endpoint I run into the issue.\n\nCode\n\nIn my local notebook (my personal machine NOT sagemaker notebook):\n\n    import pandas\n    import xgboost\n    from xgboost import XGBRegressor\n    import numpy as np\n    from sklearn.model_selection import train_test_split, RandomizedSearchCV\n    \n    print(xgboost.__version__)\n    1.0.1\n\n    # Fit model\n    r.fit(X_train.toarray(), y_train.values)\n\n    xgbest = r.best_estimator\n\n\n\nAWS SageMaker Endpoint code\n\nimport boto3\nimport pickle\nimport sagemaker\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\nfrom time import gmtime, strftime\n\nregion = boto3.Session().region_name\n\nrole = 'arn:aws:iam::111:role\/xxx-sagemaker-role'\n\nbucket = 'ml-model'\nprefix = \"sagemaker\/xxx-xgboost-byo\"\nbucket_path = \"https:\/\/s3-{}.amazonaws.com\/{}\".format('us-west-1', 'ml-model')\n\nclient = boto3.client(\n    's3',\n    aws_access_key_id=xxx\n    aws_secret_access_key=xxx\n)\nclient.list_objects(Bucket=bucket)\n\n\n\nSave the model\n\n# save the model, either xgbest \nmodel_file_name = \"xgboost-model\"\n\n# using save_model\n# xgb_model.save_model(model_file_name)\n\npickle.dump(xgbest, open(model_file_name, 'wb'))`\n\n!tar czvf xgboost_model.tar.gz $model_file_name\n\n\n\nUpload to S3\n\nkey = 'xgboost_model.tar.gz'\n\nwith open('xgboost_model.tar.gz', 'rb') as f:\n    client.upload_fileobj(f, bucket, key)\n\n\nImport model\n\n# Import model into hosting\ncontainer = get_image_uri(boto3.Session().region_name, \"xgboost\", \"0.90-2\")\nprint(container)\n\nxxxxxx.dkr.ecr.us-west-1.amazonaws.com\/sagemaker-xgboost:0.90-2-cpu-py3\n\n%%time\n\nmodel_name = model_file_name + datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\nmodel_url = \"https:\/\/s3-{}.amazonaws.com\/{}\/{}\".format(region, bucket, key)\n\nfrom sagemaker.xgboost import XGBoost, XGBoostModel\nfrom sagemaker.session import Session\nfrom sagemaker.local import LocalSession\n\n\nsm_client = boto3.client(\n                         \"sagemaker\",\n                         region_name=\"us-west-1\",\n                         aws_access_key_id='xxxx',\n                         aws_secret_access_key='xxxx'\n                        )\n\n# Define session\nsagemaker_session = Session(sagemaker_client = sm_client)\n\nmodels3_uri = \"s3:\/\/ml-model\/xgboost_model.tar.gz\"\n\nxgb_inference_model = XGBoostModel(\n                                   model_data=models3_uri,\n                                   role=role,\n                                   entry_point=\"inference.py\",\n                                   framework_version=\"0.90-2\",\n                                   # Cloud\n                                   sagemaker_session = sagemaker_session\n                                   # Local\n                                   # sagemaker_session = None\n           \n)\n\n#serializer = StringSerializer(content_type=\"text\/csv\")\npredictor = xgb_inference_model.deploy(\n                                       initial_instance_count = 1,\n                                       # Cloud\n                                       instance_type=\"ml.t2.large\",\n                                       # Local\n                                       # instance_type = \"local\",\n                                       serializer = \"text\/csv\"\n)\n\n\nif xgb_inference_model.sagemaker_session.local_mode == True:\n    print('Deployed endpoint in local mode')\nelse:\n    print('Deployed endpoint to SageMaker AWS Cloud')\n\n\n\/Applications\/Anaconda\/anaconda3\/lib\/python3.9\/site-packages\/sagemaker\/session.py in wait_for_endpoint(self, endpoint, poll)\n   3354         if status != \"InService\":\n   3355             reason = desc.get(\"FailureReason\", None)\n-> 3356             raise exceptions.UnexpectedStatusException(\n   3357                 message=\"Error hosting endpoint {endpoint}: {status}. Reason: {reason}.\".format(\n   3358                     endpoint=endpoint, status=status, reason=reason\n\nUnexpectedStatusException: Error hosting endpoint sagemaker-xgboost-xxxx: Failed. Reason:  The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..",
        "Answers":[
            {
                "Answer_creation_date":"2021-12-13T18:36:06.840Z",
                "Answer_upvote_count":0,
                "Answer_body":"Please make sure that the trained model used was trained on same version of XGBoost that is used while deploying the endpoint.\n\nAlso verify there are no typo's in your script while deploying the endpoint.\n\nI'd also check CloudWatch logs to find any information on the error encountered. If you are still not able to identify the issue, I'd recommend you to reach out to AWS Support for further investigation of the issue:\n\nOpen a support case with AWS using the link: https:\/\/console.aws.amazon.com\/support\/home?#\/case\/create",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to access file system in Sagemaker notebook instance from outside of that instance (ie via Python Sagemaker Estimator training call)",
        "Question_creation_time":1638914293851,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3yXAL7d7Sl--kKO3TTZf1g\/how-to-access-file-system-in-sagemaker-notebook-instance-from-outside-of-that-instance-ie-via-python-sagemaker-estimator-training-call",
        "Question_topic":[
            "Machine Learning & AI",
            "Storage"
        ],
        "Question_tag":[
            "Build & Train ML Models",
            "Amazon SageMaker",
            "Storage"
        ],
        "Question_upvote_count":0,
        "Question_view_count":690,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nI have large image dataset stored in a Sagemaker notebook instance, in the file system. I was hoping to learn how I could access this data from outside of that particular notebook instance. I have done quite a bit of researching but can't seem to find much - I am relatively new to this.\n\nI want to be able to access the data in that notebook in a fast manner as I will be using the data to train an AI model. Is there any recommended way to do this?\n\nI originally uploaded the data within that notebook instance to train a model within that instance in exactly the same file system. Note that it is a reasonably large dataset which I had to do some preprocessing on within Sagemaker.\n\nWhat is the best way to store data when using the Sagemaker estimators from training AI models?\n\nMany thanks\n\nTim",
        "Answers":[
            {
                "Answer_creation_date":"2021-12-07T22:53:56.668Z",
                "Answer_upvote_count":2,
                "Answer_body":"Hi Tim, when you create a sagemaker training job using the estimator, the general best practice is to store your data on S3 and the training job will launch instances as requested by the training job configuration. As now we support fast file mode, which allows faster training job start compared to the file mode (which downloads the data from s3 to the training instance). But when you say you used sagemaker notebook instance to train the model, I assume you were not using SageMaker Training jobs but rather running the notebook (.ipynb) on the SageMaker notebook instance. Please note that as SageMaker is a fully managed service, the notebook instance (also training instances, hosting instances etc.) are launched in the service account, so you will not have directly access to those instance. The SageMaker notebook instance use EBS to store data and the EBS volume is mounted to the \/home\/ec2-user\/SageMaker. Please note that the EBS volume used by a SageMaker notebook instance can only be increased but not decrease. If you want to reduce the EBS volume, you need to create a new notebook instance with a smaller volume and move your data from the previous instance via s3. You will not be able to access that EBS volume from outside of the SageMaker notebook instance. The general best practice is to store large dataset on s3 and only use sample data on the SageMaker notebook instance (reduce the storage). Then use that small amount of sample data to test\/build your code. Then when you are ready to train on the whole dataset, you can launch a SageMaker training job and use the whole dataset stored on s3. Note that, running the training on the whole dataset on a SageMaker notebook instance will require you to use a big instance with enough computing power and also will not be able to perform distributed training with multiple instances. Comparatively, if you run the training job use SageMaker training instances, it gives you more flexibility of choosing the instance type and allow you to run on multiple instances for distributed training. Lastly, once the SageMaker training job is done, all the resources will be terminated which will save cost compared to continue using the big instance with a SageMaker notebook instance. Hope this has helped answer your question",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"How to use categorical data without converting?",
        "Question_creation_time":1638858572453,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_mbjTldXQY2DHVM7-rMb_g\/how-to-use-categorical-data-without-converting",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":69,
        "Question_answer_count":4,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to develop a model in SageMaker, but my data is contained categorical type of data, and I would not want to convert it. In the old Machine Learning, I can use back the same data and train the model without any issue. However, when I tried the built-in algorithm, I got the error message that wanted me to convert the data. Is there anyway to do the same as the old Machine Learning without converting? Thank you.",
        "Answers":[
            {
                "Answer_creation_date":"2021-12-07T08:29:10.669Z",
                "Answer_upvote_count":0,
                "Answer_body":"Have you tried SageMaker Autopilot? https:\/\/aws.amazon.com\/sagemaker\/autopilot\/",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2021-12-13T16:33:38.957Z",
                "Answer_upvote_count":0,
                "Answer_body":"What kind of ML problem are you working with? If it's regression or classification, you could try SageMaker Autopilot to avoid performing feature engineering steps yourself. If you're using Jupyter notebooks on Sagemaker Studio to do your model training, adding a preprocessing step to perform categorical encoding isn't too difficult. We have plenty of examples on Github.\n\nLet us know in more detail what kind of algorithm are you dealing with and how you're doing ML on AWS today and I can give you some pointers.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2021-12-07T17:22:15.133Z",
                "Answer_upvote_count":0,
                "Answer_body":"In addition to SageMaker Autopilot other folks mentioned, I'd suggest you to try SageMaker Canvas:\n\nhttps:\/\/aws.amazon.com\/sagemaker\/canvas\/\nhttps:\/\/aws.amazon.com\/blogs\/aws\/announcing-amazon-sagemaker-canvas-a-visual-no-code-machine-learning-capability-for-business-analysts\/\n\nSageMaker Canvas offers a similar yet more powerful experience when compared to the previous-generation Amazon Machine Learning service.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2021-12-07T06:36:23.653Z",
                "Answer_upvote_count":0,
                "Answer_body":"There are a couple of approaches that might be suitable, feel free to pick the one that fits your use case best.\n\nSageMaker Canvas supports the ability to do machine learning on datasets in a no-code fashion, and performs this pre-processing for you. As described in that page, Canvas is suited for business analysts who may not have any coding experience.\n\nSageMaker AutoPilot allows for AutoML where AutoPilot takes care of feature engineering and hyperparameter optimization. Autopilot is suited for developers who want to perform AutoML without doing the pre-processing themselves.\n\n*You can preprocess the input prior to using prebuilt SageMaker algorithms, a basic example of that is shown here in the section \"Data_Transformation\". This approach is suited for data scientists who want fined-grained control over the transformation steps.\n\nIn your context, it seems to me that Canvas or AutoPilot might be most appropriate.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"passing a numpy array to predict_fn when making inference for xgboost model",
        "Question_creation_time":1638724249630,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-0wEAMBoQaK4s-Bsbp0qHA\/passing-a-numpy-array-to-predict-fn-when-making-inference-for-xgboost-model",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":421,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a model that's trained locally and deployed to SageMaker to make inferences \/ invoke endpoint. When I try to make predictions, I get the following exception.\n\nraise ValueError('Input numpy.ndarray must be 2 dimensional')\nValueError: Input numpy.ndarray must be 2 dimensional\n    \n\n\nMy model is a xgboost model with some pre-processing (variable encoding) and hyper-parameter tuning. Here's what model object looks like:\n\nXGBRegressor(colsample_bytree=xxx, gamma=xxx,\n             learning_rate=xxx, max_depth=x, n_estimators=xxx,\n             subsample=xxx)\n\n\nMy test data is a string of float values which is turned into an array as the data must be passed as numpy array.\n\ntestdata = [........., 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 2000, 200, 85, 412412, 123, 41, 552, 50000, 512, 0.1, 10.0, 2.0, 0.05]\n\n\nI have tried to reshape the numpy array from 1d to 2d, however, that doesn't work as the number of features between test data and trained model do not match.\n\nMy question is how do I pass a numpy array same as the length of # of features in trained model? I am able to make predictions by passing test data as a list locally.\n\nMore info on inference script here: https:\/\/github.com\/aws-samples\/amazon-sagemaker-local-mode\/blob\/main\/xgboost_script_mode_local_training_and_serving\/code\/inference.py\n\nTraceback (most recent call last):\nFile \"\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_containers\/_functions.py\", line 93, in wrapper\nreturn fn(*args, **kwargs)\nFile \"\/opt\/ml\/code\/inference.py\", line 75, in predict_fn\nprediction = model.predict(input_data)\nFile \"\/miniconda3\/lib\/python3.6\/site-packages\/xgboost\/sklearn.py\", line 448, in predict\ntest_dmatrix = DMatrix(data, missing=self.missing, nthread=self.n_jobs)\nFile \"\/miniconda3\/lib\/python3.6\/site-packages\/xgboost\/core.py\", line 404, in __init__\nself._init_from_npy2d(data, missing, nthread)\nFile \"\/miniconda3\/lib\/python3.6\/site-packages\/xgboost\/core.py\", line 474, in _init_from_npy2d\nraise ValueError('Input numpy.ndarray must be 2 dimensional')\nValueError: Input numpy.ndarray must be 2 dimensional",
        "Answers":[
            {
                "Answer_creation_date":"2021-12-06T21:17:24.244Z",
                "Answer_upvote_count":0,
                "Answer_body":"Try converting your list to a numpy 2d array like so:\n\na = np.array([1, 2, 3])\n\nand replace [1, 2, 3] with your list.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2021-12-10T09:23:39.023Z",
                "Answer_upvote_count":0,
                "Answer_body":"XGBoost, similar to scikit-learn, expects X as 2D data (n_samples, n_features). In order to predict one sample, you need to reshape your list or feature vector to a 2D array.\n\nimport numpy as np\n\nlst = [1, 2, 3]\nlst_reshaped = np.array(lst).reshape((1,-1))\nclf.predict(lst_reshaped)",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"mxnet error encountered in Lambda Function",
        "Question_creation_time":1638553590411,
        "Question_link":"https:\/\/repost.aws\/questions\/QUW1vSlOVxRC2pGlTEXs0Z2w\/mxnet-error-encountered-in-lambda-function",
        "Question_topic":[
            "Storage",
            "Serverless",
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon Simple Storage Service",
            "AWS Lambda",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":69,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I trained and deployed a semantic segmentation network (mlp2.xlarge) using SageMaker. I wanted to use an AWS Lambda function to send an image to this endpoint and get a mask in return however when I use invoke_endpoint it gives an mxnet error in the logs. Funnily when I use the deployed model from a transformer object from inside the SageMaker notebook the mask is returned properly. Here is my Lambda function code:\n\nimport json\nimport boto3\n\ns3r = boto3.resource('s3')\n\ndef lambda_handler(event, context):\n    # TODO implement\n    \n    bucket = event[\"body\"]\n    key = 'image.jpg'\n    local_file_name = '\/tmp\/'+key\n    s3r.Bucket(bucket).download_file(key, local_file_name)\n\n    runtime = boto3.Session().client('sagemaker-runtime')\n\n    with open('\/tmp\/image.jpg', 'rb') as imfile:\n        imbytes = imfile.read()\n\n    # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given\n    response = runtime.invoke_endpoint(\n    EndpointName='semseg-2021-12-03-10-05-58-495', \n    ContentType='application\/x-image', \n    Body=bytearray(imbytes))                       # The actual image\n\n    # The response is an HTTP response whose body contains the result of our inference\n    result = response['Body'].read()\n    \n    return {\n        'statusCode': 200,\n        'body': json.dumps(result)\n    }\n\n\nHere are the errors I see in the logs: mxnet.base.MXNetError: [10:26:14] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.4.x.4276.0\/AL2_x86_64\/generic-flavor\/src\/3rdparty\/dmlc-core\/src\/recordio.cc:12: Check failed: size < (1 << 29U) RecordIO only accept record less than 2^29 bytes",
        "Answers":[
            {
                "Answer_creation_date":"2021-12-03T20:37:40.178Z",
                "Answer_upvote_count":0,
                "Answer_body":"If you are using the built-in Sagemaker algorithm for Semantic Segmentation , the content type must be \"image\/jpeg\" for inference to accept images. For more details, see https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/semantic-segmentation.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Error Invoking endpoint deployed locally using SageMaker SDK for a xgboost model",
        "Question_creation_time":1638547297863,
        "Question_link":"https:\/\/repost.aws\/questions\/QUnHPDXJNyTzm85yaJUFJYaA\/error-invoking-endpoint-deployed-locally-using-sage-maker-sdk-for-a-xgboost-model",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":157,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I am deploying a SageMaker endpoint locally for xgboost model and running to some issues when invoking the endpoint.\n\nI am able to successfully deploy the endpoint in local model using the following code sample:\n\nfrom sagemaker.xgboost import XGBoost, XGBoostModel\nfrom sagemaker.session import Session\n\nxgb_inference_model = XGBoostModel(\n                                   model_data=models3_uri,\n                                   role=role,\n                                   entry_point=\"inference.py\",\n                                   framework_version=\"0.90-2\",\n                                   sagemaker_session = None # sagemaker_session if cloud \/ prod mode     \n)\n\nprint('Deploying endpoint in local mode')\npredictor = xgb_inference_model.deploy(\n                                       initial_instance_count = 1,\n                                       #instance_type=\"ml.m5.xlarge\",\n                                       instance_type = \"local\",\n)\n\n\n\nI have the inference.py that includes the functions for accepting input, making predictions and output. Link here: https:\/\/github.com\/aws-samples\/amazon-sagemaker-local-mode\/blob\/main\/xgboost_script_mode_local_training_and_serving\/code\/inference.py\n\nThe issue I am running into is with type of data that input_fn accepts. I have tried passing passing a numpy array \/ dataframe \/ bytes object as input data, but still get the error.\n\ndef input_fn(request_body, request_content_type):\n    \"\"\"\n    The SageMaker XGBoost model server receives the request data body and the content type,\n    and invokes the `input_fn`.\n    \n    Return a DMatrix (an object that can be passed to predict_fn).\n    \"\"\"\n    # Handle numpy array type\n    if request_content_type == \"application\/x-npy\":\n        \n        print(type(request_body))\n        \n        array = np.load(BytesIO(request_body))\n        \n        return xgb.DMatrix(request_body)\n    \n    if request_content_type == \"text\/csv\":\n        \n        print(\"request body\", request_body)\n        \n        # change to request_body to Pandas DataFrame\n \n        return xgb_encoders.libsvm_to_dmatrix(request_body)\n        #perform encoding on the input data here\n    \n    else:\n        raise ValueError(\"Content type {} is not supported.\".format(request_content_type))\n\n\n\nEncoder object. Training data is encoded before fit. Doing the same with test data. Posting here for reference.\n\nI tried making predictions using NumpySerializer and CSVSerializer [1]. Both don't work -\n\nfrom sagemaker.serializers import NumpySerializer\npredictor.serializer = NumpySerializer()\n\ntestpoint = encoder.transform(df).toarray()\n\nprint(testpoint)\n\n[[0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n  0.000e+00 0.000e+00 0.000e+00.........1.000e+01 2.000e+00 5.312e-02]]\n\n\nTraceback with CSVSerializer(), when passing body of type text\/csv\n\nException on \/invocations [POST]\n | Traceback (most recent call last):\n\nFile \"\/opt\/ml\/code\/inference.py\", line 35, in input_fn\nreturn xgb_encoders.libsvm_to_dmatrix(request_body)\npackages\/sagemaker_xgboost_container\/encoder.py\", line 65, in libsvm_to_dmatrix\nTypeError: a bytes-like object is required, not 'str'\n\n\nTraceback with NumpySerializer() when passing <class 'numpy.ndarray'> type body\n\nException on \/invocations [POST]\nTraceback (most recent call last):\nraise TypeError('no supported conversion for types: %r' % (args,))\nTypeError: no supported conversion for types: (dtype('O'),)\n\n\n[1] https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html",
        "Answers":[
            {
                "Answer_creation_date":"2021-12-05T14:02:55.710Z",
                "Answer_upvote_count":0,
                "Answer_body":"Since the error is occuring in the call to libsvm_to_dmatrix(), the content type is set to \"text\/csv\". Have you tried passing in a csv string?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2021-12-03T16:57:56.502Z",
                "Answer_upvote_count":0,
                "Answer_body":"You appear to have conflicting data types in your input_fn:\n\n    if request_content_type == \"text\/csv\":\n        \n        print(\"request body\", request_body)\n        \n        # change to request_body to Pandas DataFrame\n \n        return xgb_encoders.libsvm_to_dmatrix(request_body)\n        #perform encoding on the input data here\n\nProbably the logic you want is:\n\n    if request_content_type == \"text\/csv\":\n        return xgb_encoders.csv_to_dmatrix(request_body)\n    elif request_content_type == \"text\/libsvm\":\n        return xgb_encoders.libsvm_to_dmatrix(request_body)",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Model Registry - how to set the Stage column of a Model Package?",
        "Question_creation_time":1638530817367,
        "Question_link":"https:\/\/repost.aws\/questions\/QUI07MHIlPSbSwALreeEJ28g\/sage-maker-model-registry-how-to-set-the-stage-column-of-a-model-package",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":1,
        "Question_view_count":435,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Each version of a model (i.e. versioned Model Package) in a Model Package Group in SageMaker Model Registry has some metadata attached to it. You can see its Status, Short description, Modified by as well as Stage it is currently deployed on.\n\nThis can be seen in the docs here on the screenshot below the 5. point on that list.\n\nMy question is - how do we explicitly (or implictly) set the Stage of a given Model Package? I know that changing the approval status can be done by calling update_model_package method. What about the Stage?\n\nIt magically happens when we use provided MLOps templates, but even after a thorough browsing of every generated file (SM Pipelines, CodePipeline, CodeBuild etc.) I could not find the exact API call. It probably has something to do with the sagemaker:deployment-stage Tag but setting it up on Endpoint, Endpoint Config or Model (the old construct of pre-2020 SageMaker that is still used by SageMaker Inference) did not work. You can't set tags on a Model Package.",
        "Answers":[
            {
                "Answer_creation_date":"2021-12-03T13:23:21.229Z",
                "Answer_upvote_count":1,
                "Answer_body":"You are right that the way to reference the stage is to propagate the tags all the way down to Model, EndpointConfig and Endpoint. When you do that through the MLOps template, CodeBuild and CloudFormation take care of the tagging.\n\nYou might need to create_model from the model package as well, here's a working example:\n\nimport boto3\nclient = boto3.client('sagemaker')\n\nrole = '<my_role_arn>'\nmodel_package_arn = '<my_model_package_arn>'\nmy_tags = [\n    {'Key': 'sagemaker:deployment-stage', 'Value': 'my_stage'},\n    {'Key': 'sagemaker:project-id', 'Value': 'my_project_id'},\n    {'Key': 'sagemaker:project-name', 'Value': 'my_project_name'},\n]\n\nclient.create_model(\n    ModelName='testing-stage-model',\n    PrimaryContainer={\n        'ModelPackageName': model_package_arn,\n    },\n    ExecutionRoleArn=role,\n    Tags=my_tags\n)\n\nclient.create_endpoint_config(\n    EndpointConfigName='testing-stage-endpoint-config',\n    ProductionVariants=[\n        {\n            'VariantName': 'AllTraffic',\n            'ModelName': 'testing-stage-model',\n            'InitialInstanceCount': 1,\n            'InstanceType': 'ml.t2.medium',\n        },\n    ],\n    Tags=my_tags\n)\n\nclient.create_endpoint(\n    EndpointName='testing-stage-endpoint',\n    EndpointConfigName='testing-stage-endpoint-config',\n    Tags=my_tags\n)",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to import postgresql or nosql datasets in Amazon Sagemaker?",
        "Question_creation_time":1638524627821,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFyL4V0gFQ3WJsCwd8ZZgrA\/how-to-import-postgresql-or-nosql-datasets-in-amazon-sagemaker",
        "Question_topic":[
            "Database",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "PostgreSQL",
            "Amazon SageMaker",
            "Database"
        ],
        "Question_upvote_count":0,
        "Question_view_count":172,
        "Question_answer_count":3,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi, I am new to Amazon Sagemaker. I noticed that available options to import datasets are through local files, S3, Redshift, Snowflake and Athena only. Is there a way to import PostgreSQL or MySQL or any other NoSQL datasets?",
        "Answers":[
            {
                "Answer_creation_date":"2021-12-03T09:49:36.076Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi! SageMaker is based on JupyterLab and you are completely free to leverage and install the appropriate libraries. For install, to load data from an SQLite database, you could write a code similar to this one:\n\nimport sqlite3\nimport pandas as pd\n\ndb_client = sqlite3.connect(\"path_to_database.db\")\nquery = \"SELECT * FROM table WHERE 1;\"\ndf = pd.read_sql_query(query, db_client)\nprint(df.shape)\ndf.head()\n\n\nNote that in this case, SQLite is already included in the standard Python library. You will have a similar approach to connect to MySQL, PostgreSQL, DynamoDB (using the boto3 API), etc. But may need to install additional libraries in your JupyterLab environment before you can do so (you can use !pip install lib from within a JupyterLab cell to install a library in the current Kernel).\n\nIf you want to leverage SageMaker Data Wrangler you may have to go through this step to then, upload your data to one of the natively support data source that you mentioned (Amazon S3, Redshift...).\n\nHope this helps!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2021-12-03T14:07:13.873Z",
                "Answer_upvote_count":0,
                "Answer_body":"Depending on the Sagemaker Service you are using there could be different options.\n\nIf you are looking at one of the low code options, and exporting the data to the Data Lake (Amazon S3) is not an option, you could consider Athena Federated Query.\n\nThis will allow you to import the other sources through the Athena connection. Please review the considerations and limitation on Athena Federated Queries in the link above.\n\nFor more information on Athena Federated Queries you can also read this blog post.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2021-12-06T21:25:47.852Z",
                "Answer_upvote_count":0,
                "Answer_body":"Another option would be to not import the data and handle it via Aurora PG integration with Sagemaker. Here is a blog that may help.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Using SageMaker SDK to deploy a open source xgboost model locally",
        "Question_creation_time":1638503327094,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEH97qD5dSjS93XkXTdel8w\/using-sage-maker-sdk-to-deploy-a-open-source-xgboost-model-locally",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":141,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a locally trained model that I am trying to debug locally on docker container before deploying \/ creating endpoint on SageMaker. I am following the documentation that AWS customer service provided, however, I am running into issue with Creating Endpoint Config.\n\nHere's the code snippet:\n\nfrom sagemaker.xgboost import XGBoost, XGBoostModel\nfrom sagemaker.session import Session\n\nsm_client = boto3.client(\n                         \"sagemaker\",\n                         aws_access_key_id='xxxxxx',\n                         aws_secret_access_key='xxxxxx'\n                        )\n\nsagemaker_session = Session(sagemaker_client = sm_client)\n\nxgb_inference_model = XGBoostModel(\n                                   model_data=model_url,\n                                   role=role,\n                                   entry_point=\"inference.py\",\n                                   framework_version=\"0.90-2\",\n                                   sagemaker_session = sagemaker_session     \n)\n\nprint('Deploying endpoint in local mode')\npredictor = xgb_inference_model.deploy(\n                                       initial_instance_count = 1,\n                                       instance_type = \"local\"\n)\n\n\nTraceback:\n\n20 print('Deploying endpoint in local mode')\n21 predictor = xgb_inference_model.deploy(\n22                                        initial_instance_count = 1,\n\nClientError: An error occurred (ValidationException) when calling the CreateEndpointConfig operation: 1 validation error detected: Value 'local' at 'productionVariants.1.member.instanceType' failed to satisfy constraint: Member must satisfy enum value set: [ml.r5d.12xlarge, ml.r5.12xlarge, ml.p2.xlarge, ml.m5.4xlarge, ml.m4.16xlarge, ml.r5d.24xlarge,\n\n\nHere's the documentation link: https:\/\/github.com\/aws-samples\/amazon-sagemaker-local-mode\/blob\/main\/xgboost_script_mode_local_training_and_serving\/xgboost_script_mode_local_training_and_serving.py",
        "Answers":[
            {
                "Answer_creation_date":"2021-12-03T04:54:22.627Z",
                "Answer_upvote_count":4,
                "Answer_body":"Can you try using from sagemaker.local import LocalSession instead of sagemaker.session import Session as specified in the documentation?\n\nhttps:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"XGBoost Reports Not Generated",
        "Question_creation_time":1638475570231,
        "Question_link":"https:\/\/repost.aws\/questions\/QUx_M71_2nQJSDp-I1mgbjDg\/xg-boost-reports-not-generated",
        "Question_topic":[
            "Machine Learning & AI",
            "AWS Well-Architected Framework"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Performance Efficiency"
        ],
        "Question_upvote_count":0,
        "Question_view_count":169,
        "Question_answer_count":3,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi!\n\nI have been trying to create a model using XGBoost, and was able to successfully run\/train the model. However, I have not been able to generate the training reports. I have included the rules parameter as follows: \"rules=[Rule.sagemaker(rule_configs.create_xgboost_report())]\".\n\nI am following this tutorial, but I am using objective: \"multi:softmax\" instead of the \"binary:logistic\" used in the example.\n\nWhen I run the model everything is fine but only the Profiler Report gets generated and I do not see the XGBoostReport under the rule-output folder. According to the tutorial it should be under the same file path.\n\nHere is my code for the model if it helps any:\n\ns3_output_location='s3:\/\/{}\/{}\/{}'.format(bucket, prefix, 'xgboost_model')\ncontainer = sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"latest\")\n\ntrain_input = TrainingInput(\n    \"s3:\/\/{}\/{}\/{}\".format(bucket, prefix, \"data\/train.csv\"), content_type=\"csv\"\n)\nvalidation_input = TrainingInput(\n    \"s3:\/\/{}\/{}\/{}\".format(bucket, prefix, \"data\/validation.csv\"), content_type=\"csv\"\n)\n\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\n\nxgb = sagemaker.estimator.Estimator(\n    image_uri=container,\n    role=sagemaker.get_execution_role(),\n    instance_count=1,\n    instance_type=\"ml.c5.2xlarge\",\n    volume_size=5,\n    output_path=s3_output_location,\n    sagemaker_session=sagemaker.Session(),\n    rules=rules\n)\n\nxgb.set_hyperparameters(\n    max_depth=6,\n    objective='multi:softmax',\n    num_class=num_classes,\n    gamma=800,\n    num_round=250\n)\n\n\nAny help is appreciated! Thanks!",
        "Answers":[
            {
                "Answer_creation_date":"2021-12-03T14:44:54.672Z",
                "Answer_upvote_count":0,
                "Answer_body":"I believe the issue here is the rules parameter is receiving a URL, not an array of RuleBase objects, as required by the Estimator documentation. Try re-writing your estimator accordingly, as suggested by the SageMaker Developer Guide:\n\nxgb = sagemaker.estimator.Estimator(\n   image_uri=container, \n   role=sagemaker.get_execution_role(), \n   instance_count=1, \n   instance_type=\"ml.c5.2xlarge\", \n   volume_size=5, \n   output_path=s3_output_location, \n   sagemaker_session=sagemaker.Session(), \n   rules=[\n      Rule.sagemaker(\n         rule_configs.create_xgboost_report()\n      )  \n   ]\n)\n\nFor more information on using Rules, check out the SageMaker Debugger documentation.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-11-14T11:17:17.194Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello jughead, Has your problem of your code been resolve? Let us know. Also, remember to click on the \"Accept\" button when an answer provided in the community helped you. This allows other community members to also benefit from it. Thank you for your participation.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2022-02-13T22:12:23.421Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nI had the same problem. To be able to generate the xgboost report, make sure that you use xgboost version 1.2-1 in your image_uri and sklearn version 1.0-1 as your estimator_cls in FrameworkProcessor. Furthermore, set header to False in train, validation and test files in your preprocessing script so that you exclude them before you run estimator.fit(). This worked for me.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Location of Huggingface SageMaker Dockerfile.",
        "Question_creation_time":1636035334158,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUuhJq-uDQRmUoyuagoHPsQ\/location-of-huggingface-sage-maker-dockerfile",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":81,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Where is the github repository of the Dockerfile for Huggingface training with SageMaker? I see this repository for inference, but do not see one for training.",
        "Answers":[
            {
                "Answer_creation_date":"2021-11-04T14:18:56.693Z",
                "Answer_upvote_count":2,
                "Answer_body":"There are a bunch of Dockerfiles in the DLC repo. Here's the HuggingFace training Dockerfile for PyTorch 1.9.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2021-12-01T00:14:34.416Z",
                "Answer_upvote_count":0,
                "Answer_body":"HuggingFace Dockerfiles are located",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Specify a subnet for SageMaker endpoints",
        "Question_creation_time":1634219209733,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_fdJIdbDTuSL60rOV-Bv8g\/specify-a-subnet-for-sage-maker-endpoints",
        "Question_topic":[
            "Networking & Content Delivery",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon VPC",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":1,
        "Question_view_count":323,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Similar to setting subnet configuration for training jobs, is it possible to specify a subnet for SageMaker endpoints? Are they deployed in any default subnets, and is there a way to find that through SDK\/CLI?",
        "Answers":[
            {
                "Answer_creation_date":"2021-10-15T14:14:48.095Z",
                "Answer_upvote_count":2,
                "Answer_body":"Yes it is possible. Checkout this link for further information into vpc and endpoints. You can also create a VPC endpoint url and connect through CLI as mentioned in this link. In order to obtain the information using CLI you can use the describe-endpoint functionality. Based on the CLI documentation, VPC information might not be available using that.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2021-10-14T18:49:14.015Z",
                "Answer_upvote_count":0,
                "Answer_body":"yes it is possible. please take a look at this link Give SageMaker Access to your VPC . You will need to specify a VPC model with security group for sagemaker to gain access to your VPC by providing at least 2 subnets in different AZ's. To connect sagemaker to an interface endpoint, please look at Connect SageMaker to VPC Interface Endpoints.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"ClientError: Data download failed:Unable to create download dir",
        "Question_creation_time":1633390841000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhODEUiiMT2y0cBJixQ3ofA\/client-error-data-download-failed-unable-to-create-download-dir",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":194,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello,\nI searched for this error in the forum read through the first 6 or 7 forum pages with no luck. Im able to build models in Sagemaker Studio, but when I try in Sagemaker I get this error:\n\nClientError: Data download failed:Unable to create download dir \/opt\/ml\/checkpoints\/tc19\/preprocessed-data\/header\n\nAnybody know how to clear this ?\n\nThank in advance.",
        "Answers":[
            {
                "Answer_creation_date":"2021-10-05T21:41:00.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"The problem is that Sagemaker was trying to use a sub-directory that already contained a download. Not sure how that happened. I cleared the error by creating a sub-dir in S3 and referencing that location for the data download.\n\nHope this helps some one.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Why do I get an error that Sagemaker Endpoint does not have multiple models",
        "Question_creation_time":1629115147000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUA_yXPjW3TUOinot9BJe5GA\/why-do-i-get-an-error-that-sagemaker-endpoint-does-not-have-multiple-models",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":302,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Invoking a multimodel Sagemaker Endpoint, I get an error that it is not multimodel. I create it like this.\n\n    create_endpoint_config_response = client.create_endpoint_config(\r\n        EndpointConfigName=endpoint_config_name,\r\n        ProductionVariants=[\r\n            {\r\n                \"InstanceType\": \"ml.m5.large\",\r\n                \"InitialVariantWeight\": 0.5,\r\n                \"InitialInstanceCount\": 1,\r\n                \"ModelName\": model_name1,\r\n                \"VariantName\": model_name1,\r\n            },\r\n             {\r\n                \"InstanceType\": \"ml.m5.large\",\r\n                \"InitialVariantWeight\": 0.5,\r\n                \"InitialInstanceCount\": 1,\r\n                \"ModelName\": model_name2,\r\n                \"VariantName\": model_name2,\r\n            }\r\n        ]\r\n    )\n\n\nI confirm in the GUI that it in fact has multiple models. I invoke it like this:\n\n    response = client.invoke_endpoint(\r\n        EndpointName=endpoint_name, \r\n        TargetModel=model_name1,\r\n        ContentType=\"text\/x-libsvm\", \r\n        Body=payload\r\n    )\n\n\nand get this error:\n\nValidationError: An error occurred (ValidationError) when calling the\nInvokeEndpoint operation: Endpoint\nmy-endpoint1 is not a multi-model endpoint\nand does not support target model header.\n\nThe same problem was discussed at https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1026 with no resolution.\n\nHow can I invoke a multimodel endpoint?",
        "Answers":[
            {
                "Answer_creation_date":"2021-08-18T10:50:59.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"The answer is discussed here https:\/\/stackoverflow.com\/questions\/68802388\/\n\nTwo recommended improvements:\n\nThe error message is factually wrong. It should be amended.\nThe details should be in the documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/invoke-multi-model-endpoint.html and it would be good to further clarify whether ModelDataUrl is meant to be a prefix onto which model_filename.tar.gz is appended or whether it is a full path to a file (as appears to be the case in some Notebooks).\n\nEdited by: JoshuaFox on Aug 18, 2021 3:52 AM",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"GPU not detected by tensorflow in SM Studio",
        "Question_creation_time":1627628146000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYk1rHeoARsSfMsuiL-0JgQ\/gpu-not-detected-by-tensorflow-in-sm-studio",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":184,
        "Question_answer_count":3,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm currently using SageMaker Studio with kernel \"Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)\".\nTensorflow doesn't detect the GPU on both ml.g4dn.xlarge and ml.g4dn.2xlarge instances (with 1 GPU).\nI would appreciate any advice.\n\nimport tensorflow as tf\ntf.config.list_physical_devices('GPU')\n: []\n\nEdited by: haganHL on Jul 29, 2021 11:55 PM",
        "Answers":[
            {
                "Answer_creation_date":"2021-08-18T14:52:01.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Downgrading to the Python 3 (TensorFlow 2.1 Python 3.6 GPU Optimized) kernel allowed the GPU to be recognized. However, I had to recode my notebook to support that kernel. What a shame. What a pain.\n\nEdited by: jmlineb on Aug 18, 2021 8:00 AM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2021-08-18T14:46:52.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I am having the very same problem with the very same kernel using the very same instance types. It worked perfectly a month ago. Attempting to downgrade my kernel to see if my notebook will still work.\n\nEdited by: jmlineb on Aug 18, 2021 7:47 AM\n\nEdited by: jmlineb on Aug 18, 2021 8:11 AM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2021-08-24T11:50:30.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, I'm having the same problem here. I can't believe that Amazon hasn't fixed this - we could really use support for GPUs on new Tensorflow versions!",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"API Gateway returns TARGET_MODEL_HEADER_MISSING",
        "Question_creation_time":1627461635000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMJ7lKY6CRl2oJOgdGVakdg\/api-gateway-returns-target-model-header-missing",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":32,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi everybody\n\nI have a deployed endpoint for a Sagemaker MultiDataModel. I can call it succesfully from my local computer using boto3.\nI've set up an API Gateway to this Sagemaker MultiDataModel and I am trying to retrieve predictions from the model using a https request. But I keep getting a TARGET_MODEL_HEADER_MISSING-error.\n\nMy https request looks like this:\nheaders = {\n'X-Amzn-SageMaker-Target-Model':'\/jobtitles-exact'\n}\nresponse = requests.request(\"POST\"\n, \"https:\/\/XXXXXXXXXX.execute-api.eu-north-1.amazonaws.com\/v1\/predicted-job-titles\"\n, headers = headers\n, data = data\n)\n\nAccording to the documentation here:\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpoint.html\nand the source code here:\nhttps:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/local\/local_session.py\nit seems like I am providing the header with the target model correctly. But this is obvously not the case.\n\nHow am I supposed to provide the target model in the header with the https-request?\n\nBest regards\nElias",
        "Answers":[
            {
                "Answer_creation_date":"2021-07-28T11:46:45.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Found a solution and posted it here:\nhttps:\/\/stackoverflow.com\/questions\/68557263\/aws-api-gateway-returns-target-model-header-missing-even-though-target-model-pas\/68559729#68559729",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"API Gateway returns TARGET_MODEL_HEADER_MISSING",
        "Question_creation_time":1627461625000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3daqTW9fR5m2U-HV0U3f6A\/api-gateway-returns-target-model-header-missing",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":31,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi everybody\n\nI have a deployed endpoint for a Sagemaker MultiDataModel. I can call it succesfully from my local computer using boto3.\nI've set up an API Gateway to this Sagemaker MultiDataModel and I am trying to retrieve predictions from the model using a https request. But I keep getting a TARGET_MODEL_HEADER_MISSING-error.\n\nMy https request looks like this:\nheaders = {\n'X-Amzn-SageMaker-Target-Model':'\/jobtitles-exact'\n}\nresponse = requests.request(\"POST\"\n, \"https:\/\/XXXXXXXXXX.execute-api.eu-north-1.amazonaws.com\/v1\/predicted-job-titles\"\n, headers = headers\n, data = data\n)\n\nAccording to the documentation here:\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpoint.html\nand the source code here:\nhttps:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/local\/local_session.py\nit seems like I am providing the header with the target model correctly. But this is obvously not the case.\n\nHow am I supposed to provide the target model in the header with the https-request?\n\nBest regards\nElias",
        "Answers":[
            {
                "Answer_creation_date":"2021-07-28T11:47:23.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Found a solution and posted it here:\nhttps:\/\/stackoverflow.com\/questions\/68557263\/aws-api-gateway-returns-target-model-header-missing-even-though-target-model-pas\/68559729#68559729",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Multidatamodels and API Gateway",
        "Question_creation_time":1627382816000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUV-bD_HPIQ8ua0Il7RGOaPQ\/multidatamodels-and-api-gateway",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":39,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi forum!\n\nI have set up a multidatamodel with two models on it, model1 and model2.\nI can call the model from a Sagemaker notebook or locally using boto3 and invoke_endpoint. Everything works like a charm there.\n\nBut I would like to set up an API with API Gateway - and that is proving troublesome. I keep getting a 403 \"Unable to determine service\/operation name to be authorized\"-error.\n\nIs API Gateway supporting MultiDataModels? And is there any documentation or guides on how to make them interact?\n\nBest regards\nElias",
        "Answers":[
            {
                "Answer_creation_date":"2021-07-28T11:50:27.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yes, it is possible. There is no documentation of specifically working with API Gateway and SageMaker MultiDataModels. The trick is to pass the model as a header.\nSee the documentation here:\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpoint.html\nand the source code here:\nhttps:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/local\/local_session.py\n\nAnd the answer to another difficulty I encountered here:\nhttps:\/\/stackoverflow.com\/questions\/68557263\/aws-api-gateway-returns-target-model-header-missing-even-though-target-model-pas\/68559729#68559729\n\nOr direct message my user here on the forums.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Studio: Syntax highlighting does not work",
        "Question_creation_time":1625245313000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUwwXalEUSsiJBm3joeEtnQ\/studio-syntax-highlighting-does-not-work",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":49,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"On my jupyter notebook syntax highlighting does not work. Its also greyed out in the dropdown menu.\nIt does work on raw python files.\n\nDo I need to enable a setting to turn this on?",
        "Answers":[
            {
                "Answer_creation_date":"2021-07-02T17:22:01.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I answered my own question. Kernel needs to be running for syntax highlighting to work",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Create endpoint from Python",
        "Question_creation_time":1625083671000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUTyUMHH4QRDaMa6L24rhOMg\/create-endpoint-from-python",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":83,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"Hello,\n\nI have trained my model on sagemaker. I have deleted the endpoint, but I am keeping the model and the endpoint configuration which points to the model.\n\nFrom the sagemaker dashboard I am able to recreate the endpoint using the existing endpoint configuration. However I don't want to keep the endpoint on all the time, as I will use it only once a day for a few minutes.\n\nIs it possible to create in on demand from a Python script? I would assume that it is possible, but can't find how. Can someone point me in the right direction?\n\nRegards.",
        "Answers":[
            {
                "Answer_creation_date":"2021-07-22T16:36:17.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello hugoflores,\n\nYou can use SageMaker APIs - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DeleteEndpoint.html to delete the endpoint and https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpoint.html. to create an endpoint. This an be automated either using SageMaker Pipelines or a Lambda function.\n\nHere are a few resources towards that:\n\nhttps:\/\/awsfeed.com\/whats-new\/machine-learning\/build-a-ci-cd-pipeline-for-deploying-custom-machine-learning-models-using-aws-services\nhttps:\/\/github.com\/aws-samples\/aws-lambda-layer-create-script\nhttps:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1200\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\nhttps:\/\/www.sagemakerworkshop.com\/step\/deploymodel\/\n\nHTH,\n\nChaitanya",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2021-08-17T16:35:53.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks Chaitanya,\n\nI was able to create the endpoint using the \"create_endpoint\" method and following one of the links provided.\n\ndef create_endpoint(endpoint_name, config_name):\r\n    \"\"\" Create SageMaker endpoint with input endpoint configuration.\r\n    Args:\r\n        endpoint_name (string): Name of endpoint to create.\r\n        config_name (string): Name of endpoint configuration to create endpoint with.\r\n    Returns:\r\n        (None)\r\n    \"\"\"\r\n    try:\r\n        sagemaker.create_endpoint(\r\n            EndpointName=endpoint_name,\r\n            EndpointConfigName=config_name\r\n        )\r\n    except Exception as e:\r\n        print(e)\r\n        print('Unable to create endpoint.')\r\n        raise(e)\r\n\r\nname = 'name-of-the-endpoint'\r\nconfig = 'name-of-the-endpoint-config' #this one has to exist on the Endpoint configuration list on sagemaker\r\ncreate_endpoint(name, config)\n\nRegards\n\nEdited by: hugoflores on Aug 17, 2021 9:36 AM",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to pass data to an endpoint",
        "Question_creation_time":1625081705000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUpamBayk2RT6c6KuNop0DQQ\/how-to-pass-data-to-an-endpoint",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":277,
        "Question_answer_count":3,
        "Question_has_accepted_answer":true,
        "Question_body":"Hello,\nI have followed the DeepAR Chicago Traffic violations notebook example. The Model and Endpoint has been created and the forecasting is working.\n\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/deepar_chicago_traffic_violations\/deepar_chicago_traffic_violations.ipynb\n\nHowevr, I haven't deleted the model nor the endpoint in order to use it externally. I have created a Python script on an EC2 that tries to load the endpoint and passes the data to it to get a prediction, and here is what I am doing:\n\nLoading the CSV exactly the way I did it on the notebook\nParsing the CSV the same way I did on the notebook for the \"predictor.predict\" command\nInstead of using the \"predictor.predict\", I am using \"invoke_endpoint\" to load the endpoint and passing the data from the previous point\nInstead of getting the same response I got on the notebook, I am getting the following message:\n\"type: <class 'list'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object\"\n\nNot sure what the issue is, seems that it requires a byte data... I guess I cannot send the data as a list to the endpoint and I need to serialize it or to encode it? convert to to JSON? to Bytes?\n\nAny help will be appreciated.\nRegards",
        "Answers":[
            {
                "Answer_creation_date":"2021-12-15T13:25:47.348Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nSo the issue here is the predictor.predict command converts the data to the format necessary for the endpoint to understand, thus you need to serialize or encode the payload by yourself. To do this you can work with something like json.dumps(payload) or for a byte array json.dumps(payload).encode().\n\nIf you want to use the predictor class this is taken care of by the serializer option. The serializer encodes\/decodes the data for us and lets you simply call the endpoint through the predictor class. An example of this is the following code snippet:\n\nfrom sagemaker.serializers import IdentitySerializer\nfrom sagemaker.deserializers import JSONDeserializer\nserializer=IdentitySerializer(content_type=\"application\/json\")\n\nHope this helps!\n\nTo check out the various serializer options that can work for your different use cases check the following link.\nSerializers: https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\n\nEdited by: rvegira-aws on Jul 22, 2021 9:22 AM\n\nEdited by: rvegira-aws on Jul 22, 2021 9:24 AM",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2021-07-22T16:21:57.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks rvegira-aws,\n\nI changed the approach, instead of using the \"invoke_endpoint\" method, I have used the predictor class as you suggested and this has fixed the issue.\n\nRegards.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2021-08-17T16:50:37.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I faced the exact problem when building models for my website, thanks for the question",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"An error occurred (ModelError) when calling the InvokeEndpoint operation",
        "Question_creation_time":1623820480000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU68gR5B3JRdqOEGlwE2-pnA\/an-error-occurred-model-error-when-calling-the-invoke-endpoint-operation",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":831,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello,\nI received the following error message when I tried to send an array to my model:\n\nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from container-1 with message \"<!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\">\n\n<title>500 Internal Server Error<\/title> <h1>Internal Server Error<\/h1> <p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.<\/p> \". See https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group\n\nI have created inference pipeline containing preprocessing and autoencoder model and deployed it to a single endpoint. Am trying to send raw data in text\/csv format. EX: \"39, 4, 9, 8, contact\"\n\nPlease help me out in this.\n\nMuch appreciated,\nKarthik",
        "Answers":[
            {
                "Answer_creation_date":"2021-07-22T16:37:09.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nSo the issue here is most likely with your inference code and how you are parsing\/transforming the data coming in. Your endpoint is up and running but the format in which you are feeding it data is confusing it. The endpoint is expecting encoded data thus you need to convert your payload into the appropriate data format, there are two manners in which you can approach this.\n\nUse a serializer, when creating your endpoint with the predictor class you want to use the SageMaker Serializer to automatically encode\/decode your data, this is configured while creating your endpoint. Look at the following code snippet below.\n\nfrom sagemaker.predictor import csv_serializer\nrf_pred = rf.deploy(1, \"ml.m4.xlarge\", serializer=csv_serializer)\n\n#for prediction, decode the data properly\nprint(rf_pred.predict(payload).decode('utf-8'))\n\nIf you choose not to use the serializer you want to encode the data on your own using something such as json.dumps(payload) to encode your data properly before sending the data to the endpoint.\n\nExtra Resources:\nSageMaker Serializers: https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\n\nHope this helps!",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sage Maker Notebook - overcommit_memory settings - permission denied",
        "Question_creation_time":1621878351000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPG-W9iG_RteLMwkPbpvEZg\/sage-maker-notebook-overcommit-memory-settings-permission-denied",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":273,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am doing some NLP work using spaCy in a ml.p2.xlarge notebook instance in sagemaker.\n\nHowever, I get the following error:\n\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 3.10 GiB for an array with shape (2889884, 288) and data type float32\n\nTo fix the issue, I have tried the following commands:\n\n!echo 1 | sudo tee \/proc\/sys\/vm\/overcommit_memory\n!echo 1 > \/proc\/sys\/vm\/overcommit_memory\n\nNeither of them work and they both return:\n\/bin\/sh: \/proc\/sys\/vm\/overcommit_memory: Permission denied\n\nAny suggestions? I even added all policies with \"admin\" in them to the notebook IAM with no luck.",
        "Answers":[
            {
                "Answer_creation_date":"2021-05-24T18:57:06.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Okay, it seems that I needed to restart my notebook after adding the admin policies to it. It's working now and I can overcommit the memory!",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Can Sagemaker-trained models be deployed to non-Sagemaker endpoints?",
        "Question_creation_time":1619959658000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVfuRLH2cSIifokrvWNaWdA\/can-sagemaker-trained-models-be-deployed-to-non-sagemaker-endpoints",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":81,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am looking for a way to train model which will be deployed for inferencing, but Google AI Platform or an on-premises VM, not to a Sagemaker endpoint. Are Sagemaker-trained generic enough to be used for that? If so, could you point me to documentation?",
        "Answers":[
            {
                "Answer_creation_date":"2021-07-22T17:06:19.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello JoshuaFox,\n\nYes, you can deploy a SageMaker trained model outside of SageMaker. How to do that exactly and the documentation depends on the framework and framework version.\n\nFor example lets say you want to deploy a TensorFlow 2.X model outside of SageMaker, you would use something https:\/\/www.tensorflow.org\/tfx\/tutorials\/serving\/rest_simple.\n\nHTH,\n\nChaitanya",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Tracking model artifacts used in machine learning",
        "Question_creation_time":1619803212000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUp5EvOvmhTkqTM-NYhYAJZA\/tracking-model-artifacts-used-in-machine-learning",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":149,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"What are effective working patterns and tools to ensure we can easily reproduce the model artifacts deployed in production? (Customer is using DVC and Github to get version control on all key aspects: data, training scripts, model specification, hyper parameters, etc.) I'd like to share AWS best practices and recommendations with them.",
        "Answers":[
            {
                "Answer_creation_date":"2021-04-30T20:25:28.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"There are a few good practices that would ensure a robust model governance and tracking strategies in place. This is usually required by regulators especially in the financial industry and defined by frameworks like SR 11-7:\n\n\u2022 Identify models, owners, and associated usage: This is usually controlled by having a controlled landing zone for data scientists with clear authentication and authorization strategies to keep track of user activities and model owners.\n\n\u2022 Cover all aspects of the model life cycle and MLOps: By using an experimentation, proper documentation, feature tagging, testing, deployment environment and pipelining so that models can be independently validated by model validators without having to get back to model developers.\n\n\u2022 maintain a centralized model inventory, and track the current validation status: by keeping track of different model versions along with its associated risk and validation processes.\n\nOn-going monitoring for production models: by implementing mechanisms to continuously assess accuracy, drift, building constraints on data feed used for inference and outcome analysis for different model versions.\n\nNow saying that, there are many tools out there to help in implementing and achieving all the above, which are scattered and can become challenge in implementation and integration. However, SageMaker have different modules to cover model of the practices mentioned above.\n\nSageMaker Experiments: tracks all different steps of ML lifecycle in a construct called trial component. A bunch of trial components can form a trial and a trial belongs to an experiment.\n\nSageMaker Pipelines: Help in building a re-producible experiment. Each stage of the ML Lifecycle fits in the pipeline and can automatically be tracked as Trial Components and tracked by its execution history.\n\nSageMaker Model Registry: Builds a centralized catalog for models to manage different model versions, associate meta data with different version of models and manage approvals to enforce ownership.\n\nSageMaker Model Monitoring: for managing data feed constraints, model performance analysis and drift detection.\n\nSageMaker Feature Store: with proper tagging for feature groups and why certain features have been engineered by who is also a must.\n\nSageMaker ML Lineage: which - from my point of view - is the most important component for model tracking, auditing and governance. SM ML Lineage is the glue that builds a graph to trace a certain model back to its origins. It can tell what Artifact contributed to which Trial Component and what data produced which model.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Waiting for SageMaker CreateTrainingJob to Finish",
        "Question_creation_time":1618441857000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKgs9xRKGRYaibqIaX7sdzg\/waiting-for-sage-maker-create-training-job-to-finish",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":128,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm invoking the following task in my step function workflow:\n\nhttps:\/\/docs.aws.amazon.com\/cdk\/api\/latest\/docs\/@aws-cdk_aws-stepfunctions-tasks.SageMakerCreateTrainingJob.html\n\nSageMakerCreateTrainingJob starts the task in an asynchronous manner, i.e. the task returns immediately after I request it to start, even if the training job may take hours to finish.\n\nI want to have another task execute after the training job truly and completely finishes. What is an easy way to accomplish this without having to use the two techniques below?\n\nI know there are two clunky ways to do this:\n\nUsing a combination of a Lambda function that calls the SageMaker API to poll for the status of a training job - i.e. to detect when it is finished - and a wait state to force a poll every X number of minutes or so.\nAnother possibility is using step function activities, but that would probably preclude me from using Lambdas as a Lambda activity worker would need to keep waiting (doing a thread wait) and constantly polling. I would also need to worry about the 15 minute Lambda time limit.",
        "Answers":[
            {
                "Answer_creation_date":"2021-04-14T23:42:24.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Oops, I guess I didn't look at the integrationPattern property. Anyway, I set it to RUN_JOB and now the call is synchronous.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Studio projects in VpcOnly mode without internet access",
        "Question_creation_time":1615480055000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcyhpq1pxRTmtjkDRAh_MDA\/sage-maker-studio-projects-in-vpc-only-mode-without-internet-access",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":323,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"A customer is using SageMaker Studio in VpcOnly mode (VPC, protected subnets without internet access, NO NAT gateways). The all functionality is fine. However, when I try create a SageMaker projects - as described here, SageMaker Studio is unable to list the project templates (timeout and unspecified error) resulting in empty list of the available project templates.\n\nProjects are enabled for the users - as described here. The problem is with project creation.\n\nIs internet access (e.g. via NAT gateways) is needed for SageMaker projects?",
        "Answers":[
            {
                "Answer_creation_date":"2021-04-10T20:10:40.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Figured it out. SageMaker Studio projects need Service Catalog access and VPCE for com.amazonaws.${AWS::Region}.servicecatalog",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Failing to create multi-model endpoint",
        "Question_creation_time":1613660927000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcIYmAjUlRn-b_PqcIjk08A\/failing-to-create-multi-model-endpoint",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":35,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I have been trying to create a multi-model endpoint with my own container, using the instructions here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html.\n\nFollowing the instructions here, I am able to successfully create a model and endpoint configuration: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/create-multi-model-endpoint-sdk.html\n\nHowever, when I try to create the endpoint itself, it shows the status of \"Creating\" for over 2 hours, before finally stopping with the status, \"Failed\". It gives no reason for the failure or any other help.\n\nDoes anyone have any ideas?\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_date":"2021-03-19T18:41:32.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi gdaley,\n\nThanks for using Amazon SageMaker!\n\nWe are seeing the following exception when creating your endpoint:\nOCI runtime create failed: container_linux.go:370: starting container process caused: exec: \"python\": executable file not found in $PATH: unknown\"\n\nThis is related to the python path ENTRYPOINT setup in Dockerfile. Please check your Python path setup is the same as https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\n\nTo help you resolve the issue faster, you can create a support case with AWS at https:\/\/console.aws.amazon.com\/support\/home#\/\n\nThanks,\nAmazon SageMaker",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2021-03-22T07:25:33.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thank you!",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Studio Enterprise Deployment guidelines",
        "Question_creation_time":1612447142000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0QiBS-GdSIKZdXxSf1NUYA\/sage-maker-studio-enterprise-deployment-guidelines",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":32,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Do we have guidelines on requirements gathering\/designing the provisioning of SageMaker Studio domains across large global enterprises with many business units?\n\nI've seen discussions where topics like number of users\/domain, org\/team structure, collaboration patterns, resource needs, classes of ML problems, framework\/library usage, security and others were raised when defining requirements and boundaries. Customer is starting their first Studio deployment and they are asking for guidance on how to scope and design that so that they can have a scalable process.",
        "Answers":[
            {
                "Answer_creation_date":"2021-02-23T14:34:46.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"You should guide your customer based on the general principles of multi account best practices that we provide for other services.\n\nHere are some high level boundaries.\n\nOne studio domain per account and region. No cross region AWS SSO configuration provided.\n\nMaximum numbers of users allowed in studio vary between 60 - 200 users. Although AWS SSO can support many more users, there are some considerations around other dependencies such as EFS among others.\n\nIf you need to isolate any model artifacts produced by SageMaker, you may want to have them use a separate account. Even if you use tag based access control, you can still technically list those artifacts.\n\nSageMaker feature store should follow the data lake pattern closely. As a general rule, you want to write in one account and can read from many other accounts perhaps using Lake formation to expose datasets into other accounts. Teams can create their own offline \/ online feature store for non production use cases.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"SageMaker ValidationException",
        "Question_creation_time":1612307749000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-bXA28SLQryx60CbXdb9wg\/sage-maker-validation-exception",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":460,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi All\nI am brand new to AWS and find it all very overwhelming and confusing.\nI am trying to use SageMaker and Forecast - however I am having issues with trying to get into SageMaker Studio. I am getting the following error:\nValidationException\n1 validation error detected\nValue '[]' at 'subnetIds' failed to satisfy constraint: Member must have length greater than or equal to 1\n\nI have asked out internal team and they are not sure, and opened a support ticket. The support person said that I should be using the Standard setup rather than the Quick setup. However, this is still giving the same issue. To which he said that this is an advanced problem and he can't assist with it.\nIs anyone able to help me? give me an idea of what is wrong?",
        "Answers":[
            {
                "Answer_creation_date":"2022-04-21T07:59:07.109Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello\n\nyou can go for standard and then fill the VPC and subnet in the Network and Storage block",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2021-10-25T04:08:30.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I can get the same error (Value '[]' at 'subnetIds' failed to satisfy constraint: Member must have length greater than or equal to 1) if:\n\nI start with an AWS account which has been customized (for example, no default VPC)\nChoose \"Quick Setup\" when setting up SageMaker studio\n\nAs @ArturoT mentioned, choosing \"Standard\" when setting up SageMaker Studio and selecting VPC and subnets does work for me.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"botocore.exceptions.ClientError: An error occurred (ValidationException)",
        "Question_creation_time":1611849385000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSzifL4nASM60UV1hVI6e_A\/botocore-exceptions-client-error-an-error-occurred-validation-exception",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":819,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\nI want to deploy an MLflow image to an AWS Sagemaker endpoint that contains a machine learning model. I executed the following code, which I found in https:\/\/towardsdatascience.com\/deploying-models-to-production-with-mlflow-and-amazon-sagemaker-d21f67909198 .\n\nimport mlflow.sagemaker as mfs  \n  \nrun_id = run_id # the model you want to deploy - this run_id was saved when we trained our model  \nregion = \"us-east-1\" # region of your account  \naws_id = \"XXXXXXXXXXX\" # from the aws-cli output  \narn = \"arn:aws:iam::XXXXXXXXXXX:role\/your-role\"  \napp_name = \"iris-rf-1\"  \nmodel_uri = \"mlruns\/%s\/%s\/artifacts\/random-forest-model\" % (experiment_id,run_id) # edit this path based on your working directory  \nimage_url = aws_id _ \".dkr.ecr.\" _ region + \".amazonaws.com\/mlflow-pyfunc:1.2.0\" # change to your mlflow version  \n  \nmfs.deploy(app_name=app_name,   \n           model_uri=model_uri,   \n           region_name=region,   \n           mode=\"create\",  \n           execution_role_arn=arn,  \n           image_url=image_url)  \n\n\nBut I got the following error. I checked all policies and permissions attached to the IAM role. They all comply with what the error message complains about. I don't know what to do next. I'd appreciate your help. Thanks.\n\nbotocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not access model data at https:\/\/s3.amazonaws.com\/mlflow-sagemaker-us-east-1-xxx\/mlflow-xgb-demo-model-eqktjeoit5mxhmjn-abpanw\/model.tar.gz. Please ensure that the role \"arn:aws:iam::xxx:role\/mlflow-sagemaker-dev\" exists and that its trust relationship policy allows the action \"sts:AssumeRole\" for the service principal \"sagemaker.amazonaws.com\". Also ensure that the role has \"s3:GetObject\" permissions and that the object is located in us-east-1.",
        "Answers":[
            {
                "Answer_creation_date":"2021-01-28T16:36:18.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I found the root cause. I had to go to \"Trust relationship\" section of the IAM role and then add \"sagemaker.amazonaws.com\" to the service principal.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Processing Job automatically created when I start a training job",
        "Question_creation_time":1611570309000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmy7drPKBRH6hs3Pw_yj3tw\/processing-job-automatically-created-when-i-start-a-training-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":38,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\nI haven't used sagemaker for a while and today I started a training job (with the same old settings I always used before), but this time I noticed that a processing job has been automatically created and it's running while my training job runs (I don't even know what a processing job is). I also checked in the dashboard to be sure, this was not happening before, it's the second time (first time was in December) but I've been using sagemaker for the last two years..\nIs this a wanted behaviour? I didn't find anything related in the documentation, but it's important to know because I don't want extra costs..\nThis is the image used by the processing job, with a instance type of ml.m5.2xlarge which I didn't set anywhere..\n\n929884845733.dkr.ecr.eu-west-1.amazonaws.com\/sagemaker-debugger-rules:latest  \n\n\nAnd this is how I launch my training job (the entrypoint script is basically Keras code for a MobileNetV3)\n\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\nfrom sagemaker import get_execution_role\n\nbucket = 'mybucket'\n\ntrain_data = 's3:\/\/{}\/{}'.format(bucket,'train')\n\nvalidation_data = 's3:\/\/{}\/{}'.format(bucket,'test')\n\ns3_output_location = 's3:\/\/{}'.format(bucket)\n\nhyperparameters = {'epochs': 130, 'batch-size' : 512, 'learning-rate' : 0.0002}\n\nmetrics = .. some regex here\n\ntf_estimator = TensorFlow(entry_point='train.py',\nrole=get_execution_role(),\ntrain_instance_count=1,\ntrain_instance_type='ml.p2.xlarge',\ntrain_max_run=172800,\noutput_path=s3_output_location,\nframework_version='2.3.0',\npy_version='py37',\nmetric_definitions = metrics,\nhyperparameters = hyperparameters,\nsource_dir=\"data\")\n\ninputs = {'train': train_data, 'test': validation_data}\nmyJobName = 'myname'\ntf_estimator.fit(inputs=inputs, job_name=myJobName)\n\nEdited by: rokk07 on Jan 25, 2021 2:55 AM",
        "Answers":[
            {
                "Answer_creation_date":"2021-01-25T11:14:01.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I can answer myself. It's described https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/use-debugger-built-in-rules.html , must be a recent feature. The documentation explain also how to disable the debugger.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"API definition for ModelBiasMonitor and ModelExplainabilityMonitor",
        "Question_creation_time":1611507553000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU9SPQKzelSUu3dr-D4zaXHQ\/api-definition-for-model-bias-monitor-and-model-explainability-monitor",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":25,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Where can I find the actual references to API definitions and descriptions for ModelBiasMonitor and ModelExplainabilityMonitor Classes?\n\nI can a find a few mentions in the Amazon SageMaker documentation in the following links. https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model_monitor.html https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_model_monitor\/fairness_and_explainability\/SageMaker-Model-Monitor-Fairness-and-Explainability.html\n\nWhere can I find the actual reference and the code implementation for these Classes?",
        "Answers":[
            {
                "Answer_creation_date":"2021-01-25T10:47:51.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"The actual reference to the classes can be found here: https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/model_monitor\/clarify_model_monitoring.py\nIt encapsulates the definitions and descriptions for all of SageMaker Clarify related monitoring classes.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Using Hyperparameter Tuning Jobs over Training and Preprocessing",
        "Question_creation_time":1610658074000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3xcWKDPHR8ylWSaX83lNKQ\/using-hyperparameter-tuning-jobs-over-training-and-preprocessing",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":65,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Some data science teams want to tune the hyperparameters of their preprocessing jobs alongside ML model training jobs.\n\nDoes AWS have a recommended approach to establish this using Sagemaker Hyperparameter tuning?",
        "Answers":[
            {
                "Answer_creation_date":"2021-01-15T20:17:42.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"It depends on the dataset and the question for ML to answer.\n\nYes, it is feasible to do HPO with preprocessing. However, to run a HPO job, it is required to define to a specific target to achieve, e.g. maximize\/minimize certain values during the whole HPO process. Thus, it is important to understand what is the target during preprocessing. If the answer is yes, they should be able to leverage Hyperparameter Tuning Jobs.\n\nHere is how HPO works in SageMaker. Firstly, we define each training Job with output in a container and specify the hyperparameters in \/opt\/ml\/input\/config\/hyperparameters.json. When we run the pipeline using HyperparameterTuner in SageMaker, the initial Job can pass the hyperparameters to the Pipeline for HPO, and return the model with highest score.\n\nOption 1, if there is a clear defined target for preprocessing to achieve, we can also do HPO separately for data preprocessing through defining the function and outputs in a container and use HyperparameterTuner fit to tune the preprocessing.\n\nOption 2. include the preprocessing + training code in the whole SageMaker Training Job. But then you can't use separate infrastructure for training and preprocessing.\n\nSo it depends on what exactly they are looking for, but they can likely use SageMaker HPO.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"SageMaker PyTorch Hosting 1.6 works only with artifacts named model.pth ?",
        "Question_creation_time":1610118101000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUn-IyC9nySDKKibLL3Lvw3A\/sage-maker-py-torch-hosting-1-6-works-only-with-artifacts-named-model-pth",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":174,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi, does SageMaker PyTorch Hosting 1.6 works only with artifacts named model.pth ? I'm trying this sample with 1.6 and the deployment fails with error\n\nFileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/model\/model.pth'\n\n\nthe documentation doesn't mention such a constraint",
        "Answers":[
            {
                "Answer_creation_date":"2021-01-08T16:02:15.000Z",
                "Answer_upvote_count":1,
                "Answer_body":"Yes - from the thread on this open issue: https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/issues\/86\n\nIn the issue thread they note that the new Pytorch 1.6 image requires that the model filename is model.pth, linking to the relevant code where this default is set: https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/9a6869e\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L121\n\nAlso noted in the thread is that users have successfully adapted their code to use torchserve in Pytorch 1.6 by changing it to save their model in a file named model.pth. Once renamed, they were still able to use custom inference scripts to load their model by defining a custom model_fn: https:\/\/github.com\/data-science-on-aws\/workshop\/blob\/374329adf15bf1810bfc4a9e73501ee5d3b4e0f5\/09_deploy\/wip\/pytorch\/code\/inference.py",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Redshift ML \/ SageMaker - Deploy an existing model artifact to a Redshift Cluster",
        "Question_creation_time":1609954586000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCMYCx28qRe-MOCIfj91Y2g\/redshift-ml-sage-maker-deploy-an-existing-model-artifact-to-a-redshift-cluster",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Redshift"
        ],
        "Question_upvote_count":0,
        "Question_view_count":68,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Is it possible to deploy an existing model artifact from SageMaker to Redshift ML?\n\nFor example, with an Aurora ML you can reference a SageMaker endpoint and then use it as a UDF in a SELECT statement. Redshift ML works a bit differently - when you call CREATE MODEL - the model is trained with SageMaker Autopilot and then deployed to the Redshift Cluster.\n\nWhat if I already have a trained model, can i deploy it to a Redshift Cluster and then use a UDF for Inference?",
        "Answers":[
            {
                "Answer_creation_date":"2021-01-06T17:52:12.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"As of January 30 2021, you can't deploy an existing model artifact from SageMaker to Redshift ML directly with currently announced Redshift ML preview features. But you can reference sagemaker endpoint through a lambda function and use that lambda function as an user defined function in Redshift.\n\nBelow would be the steps:\n\nTrain and deploy your SageMaker model in a SageMaker Endpoint.\nUse Lambda function to reference sagemaker endpoint.\nCreate a Redshift Lambda UDF referring above lambda function to run predictions.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Anybody ever successfully ran multi-node gradient boosting on Amazon SageMaker?",
        "Question_creation_time":1607968009000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGJGFHP76S0izgf1xfM6aIg\/anybody-ever-successfully-ran-multi-node-gradient-boosting-on-amazon-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":44,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Anybody ever successfully ran multi-node gradient boosting on Amazon SageMaker? I'm looking for a SageMaker-compatible multi-node training solution for either Catboost, LightGBM or XGBoost. Knowing if it's ever been done would be nice, having a public demo link would be even better :)",
        "Answers":[
            {
                "Answer_creation_date":"2020-12-14T18:06:33.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"We do have an example of distributed training of XGBoost in the sagemaker-examples repo. You can find it here: https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone_dist_script_mode.ipynb",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Custom Amazon SageMaker container registration and deployment tracking",
        "Question_creation_time":1607710961000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdJqWN_WJQeuYrkZMikkibQ\/custom-amazon-sage-maker-container-registration-and-deployment-tracking",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":34,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"My customer asks that:\n\nContainer images must be registered and deployments tracked\n\nContainers must be registered within a private customer-owned registry prior to deployment\n\nOnly registered containers are to be deployed.\n\nPart of the registration process must include verification the containers have comes from a trusted source and that they have been scanned and found to be free of malware and vulnerabilities.\n\nAn inventory of all deployed containers must be maintained at all times.\n\nThe inventory must include: Software installed within the container version of all software and patch level . Where the container has been deployed . Owner of the container\n\nDo we do any of these? Please provide documentation on AWS\/SageMaker vs custom container provider's responsibilities.",
        "Answers":[
            {
                "Answer_creation_date":"2020-12-18T15:43:38.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Amazon Elastic Container Registry (Amazon ECR) enables customers to store images, secure their images using AWS Identity and Access Management (IAM), and scan their containers for vulnerabilities. Open Policy Agent (OPA) is an open-source project focused on codifying policy such as the approved image registries. OPA is integrated with Kubernetes via Gatekeeper, an admission controller that checks if the image is from an approved registry prior to allowing it to be deployed on the cluster. For more details see: https:\/\/aws.amazon.com\/blogs\/containers\/designing-a-secure-container-image-registry",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Custom container not running under root account?",
        "Question_creation_time":1607710724000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYAkZepq4SgyArKZCC7gT_A\/custom-container-not-running-under-root-account",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":76,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"A customer wants to enforce these rules in their custom SageMaker containers:\n\n\u2022\tProcesses running inside a container must run with a known UID\/GUID and never as root.\n\u2022\tAvoid using privilege escalation methods that grant root access (e.g. sudo)\n\n\nHow do we ensure this?",
        "Answers":[
            {
                "Answer_creation_date":"2020-12-18T15:45:37.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"SageMaker requires that Docker containers run without privileged access. See: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-toolkits.html SageMaker Docker containers do not run in Privileged mode and have the following Linux capabilities removed: SETPCAP, SETFCAP, NET_RAW, MKNOD",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Can you configure Amazon ECR containers to be immutable?",
        "Question_creation_time":1607710511000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUPiBylRCSe6_ax_u_4g-oA\/can-you-configure-amazon-ecr-containers-to-be-immutable",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":27,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Is there a way to configure Amazon ECR containers so that they can't be changed once they're created? Here are our requirements:\n\nContainers can't be changed after their built.\nContainers can't receive updates.\nChanges in the containerized application must require the building and deployment of a new container image.\nRuntime data and configurations must be stored outside of the container environment.",
        "Answers":[
            {
                "Answer_creation_date":"2020-12-18T15:46:52.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yes, you can configure Amazon ECR containers to be immutable. Amazon ECR uses resource-based permissions to control access to repositories. The resource-based permissions let you specify which IAM users or roles have access to a repository and what actions they can perform on it. By default, only the repository owner has access to a repository.\n\nFor more information, see Repository policies and Image tag mutability in the Amazon ECR user guide.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Do Amazon SageMaker manifest files enable dataset versioning?",
        "Question_creation_time":1607681930000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq44kZCYWTiOnwXblHSQSTA\/do-amazon-sage-maker-manifest-files-enable-dataset-versioning",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":65,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Some Amazon SageMaker algorithms can train with a manifest JSON file that stores the mapping between images and their Amazon S3 ARNs and metadata, such as labels. This is a great option, because the manifest file is much smaller than the dataset itself. Because the manifest files are small, they can be used easily in versioning tools or saved as part of the model artifact. This appears to be the best construct enabling exact dataset versioning within SageMaker. i.e., if we exclude the creation of a unique training set hard copy per training job that can't be scaled to large datasets. Is my understanding accurate?",
        "Answers":[
            {
                "Answer_creation_date":"2020-12-11T10:56:42.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"If you create the conditions for immutability of the assets the manifest points to, then manifest enables exact dataset versioning with SageMaker. You can have a data store in Amazon S3 with all versions of the data assets and use the manifest files for creating and versioning datasets for specific usage.\n\nIf you don't guarantee immutability for the assets that the manifest points to, then your manifest becomes invalid.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Attaching custom image to user (not domain) in SageMaker Studio",
        "Question_creation_time":1607576593000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhP1jmxpAQi6X0eDIUI2JKA\/attaching-custom-image-to-user-not-domain-in-sage-maker-studio",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":157,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Is there a way to attach a custom image to just the user (not the domain) in SageMaker Studio.\n\nDocumentation states 'To make a custom SageMaker image available to all users within a domain, you attach the image to the domain. To make an image available to a single user, you attach the image to the user's profile.' https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-byoi.html\n\nWhen I 'edit user', I dont see a way to attach a custom image. Is there a way to do this?",
        "Answers":[
            {
                "Answer_creation_date":"2020-12-10T09:36:50.000Z",
                "Answer_upvote_count":1,
                "Answer_body":"You can attach custom images to user profiles via the APIs to create\/update user profiles.\n\nMore info:\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateUserProfile.html https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateUserProfile.html",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"AWS AI\/ML integration with Power BI",
        "Question_creation_time":1607495476000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4VexAnfiSFi4Jf5i9RyO_A\/aws-ai-ml-integration-with-power-bi",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon QuickSight",
            "Amazon Comprehend"
        ],
        "Question_upvote_count":0,
        "Question_view_count":148,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"Customer wants to know if AWS AI\/ML services integrate with Power BI. The customer currently uses Power BI that integrates with Azure ML for sentiment analysis, opinion mining, etc. Customer is looking for a push button solution where the business analyst can do text analytics on the response from the model. Is there a way to do this on AWS or a marketplace solution?",
        "Answers":[
            {
                "Answer_creation_date":"2020-12-09T15:40:38.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"PowerBI can connect to Amazon Redshift and leverage the new SQL based ML capability in Redshift that uses Sagemaker under the hood.\n\nAs an alternative thought the customer can integrate Amazon Sagemaker Model with Amazon Quicksight to achieve functionality very similar to PowerBI with Azure ML. Quicksight does have some embedded ML capability like forecasting and anomaly detection but Opinion mining is not one of them yet.\n\nYou should be able to leverage Blazing Text Algorithm in Sagemaker or some market place solution like Twinword sentiment model in sagemaker for sentiment analysis for Text mining after the integration.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2022-03-21T18:11:56.219Z",
                "Answer_upvote_count":0,
                "Answer_body":"Please take a look at this AWS solution - https:\/\/aws.amazon.com\/solutions\/implementations\/text-analysis-with-amazon-opensearch-service-and-amazon-comprehend\/\n\nComprehend also has an integration with AWS Aurora - https:\/\/docs.aws.amazon.com\/AmazonRDS\/latest\/AuroraUserGuide\/mysql-ml.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Debugger and forecasting",
        "Question_creation_time":1607461696000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhc2VF00VTGy1VonBHiyd9Q\/sagemaker-debugger-and-forecasting",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon Forecast"
        ],
        "Question_upvote_count":0,
        "Question_view_count":42,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"How to detect overfitting with Amazon Forecast. Does SageMaker Debugger work with Amazon Forecast?",
        "Answers":[
            {
                "Answer_creation_date":"2020-12-09T05:15:32.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Answer #1: Based on the same technology used for time-series forecasting at Amazon.com, Forecast provides state-of-the-art algorithms to predict future time-series data based on historical data, and requires no machine learning experience. Amazon SageMaker Debugger profiles and debugs your training jobs to improve the performance of machine learning models on compute resource utilization and model predictions. So Debugger is available for your SageMaker training jobs but not for Amazon Forecast which is a fully managed service of its own.\n\nAnswer #2 - The way to detect overfitting with forecasting is using \"backtests\", which are the \"train\/valid\" equivalent of traditional machine learning. The reason Backtesting is used in forecasting is to keep the time order of the train\/valid data. With careful analysis of backtest windows, you can have more confidence the model will generalize well with unseen data.\n\nAmazon Forecast now supports export of backtest forecasts as .csv files. Customer can take the backtest forecasts and calculate whatever metric they want and\/or visualize.\n\nLink to documentation: https:\/\/docs.aws.amazon.com\/forecast\/latest\/dg\/metrics.html\nLink to blog and notebook about using Backtests in Amazon Forecast: https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-forecast-now-supports-accuracy-measurements-for-individual-items\/\n\nOne common fear is backtesting will increase overfitting - it does not since each validation dataset is different from the previous backtest validation dataset. To have more certainty about robustness of the model, use more than 1 backtest window. If the Forecast Horizon is long, you may get by with just 1-2 backtest windows; otherwise use the maximum which is 5 backtest windows.\n\nAnother concern is the increased training time since each backtest window is in fact another train\/forecast iteration. It's a trade-off, if you want more model validation or not.\n\nIdeally, the wQL's of each backtest window are similar. If you see all backtest forecasts with similar metrics (except maybe a few windows where underlying data had anomalies), that is good sign your model is not overfit.\n\nBoth under-fitting and over-fitting for now are left to human judgement to examine backtest forecasts and decide whether model is acceptable or not.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"[AI\/ML] Data acquisition and preprocessing",
        "Question_creation_time":1607357917000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUebPx1UeWSGOb_3i0TXlBWA\/ai-ml-data-acquisition-and-preprocessing",
        "Question_topic":[
            "Serverless",
            "Compute",
            "Machine Learning & AI",
            "Analytics",
            "Database"
        ],
        "Question_tag":[
            "AWS Lambda",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "AWS Glue",
            "Amazon DynamoDB",
            "Amazon Redshift"
        ],
        "Question_upvote_count":0,
        "Question_view_count":38,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nCustomer who loads the e-bike data to S3 wants to get AI\/ML insight from sensor data. The e-bike sensor data are size about 4KB files each and posted in S3 buckets. The sensor data is put into format like this\n\ntimestamp1, sensorA, sensorB, sensorC, ..., sensorZ timestamp2, sensorA, sensorB, sensorC, ..., sensorZ timestamp3, sensorA, sensorB, sensorC, ..., sensorZ ...\n\nThen these sensor data are put into one file about 4KB size.\n\nThe plan I have is to\n\nRead S3 objects\nParse S3 object with Lambda. I thought about Glue but wanted to put data in DynamoDB where Glue does not seem to support. Also, Glue seems to be more expensive.\nPut the data in DynamoDB with bike ID as primary key and timestamp as sort key.\nUse SageMaker to learn with the DynamoDB data. There will be separate discussion on choosing which model and making time-series inferencing.\nIf we need to re-learn, it will use the DynamoDB data, not from S3. I think it will be faster to get data from DynamoDB instead from the raw S3 data.\nAlso, I think we can filter out some bad input or apply little modification to DynamoDB data (shifting time stamps to the correct time, etc.)\nMake inferencing output based on the model.\n\nWhat do you think? Would you agree? Would you approach the problem differently? Would you rather learn from S3 directly via Athena or direct S3 access? Or would you rather use Glue and Redshift? But the data about 100MB would be sufficient to train the model we have in mind. Glue and Redshift maybe overkill. Currently, Korea region does not support Timestream database. So, time series database closest in Korea could be DynamoDB.\n\nPlease share your thoughts.\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_date":"2020-12-07T17:41:59.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thoughts about DynamoDB\n\nPer GB, DynamoDB is around 5X more cost per GB of data stored. On top of that, you have RCU\/WCU cost.\n\nI would recommend keeping data in S3. Not only is it more cost effective, but with S3, you do not have to worry about RCU\/WCU cost or throughput of DynamoDB.\n\nSageMaker notebooks and training instances can read directly from S3, and S3 has high-throughput. I don't think you will have a problem with 100 MB datasets.\n\nIf you need to prep\/transform your data, you can do the transformations \"in place\" in S3 using Glue, Athena, Glue DataBrew, GlueStudio, etc.\n\nGlue and DynamoDB\n\nI thought about Glue but wanted to put data in DynamoDB where Glue does not seem to support.\n\nGlue supports both Python and Spark jobs. If you use a Glue Python job, you can import the boto3 (AWS SDK) library and write to DynamoDB.\n\nOther strategies\n\nHow is your customer ingesting the sensor data \/ how is it being written to S3? Are they using AWS IoT Core?\n\nRegardless, the pattern you've described thus far is:\n\nDevice -> Sensor data in S3 -> Transform with Lambda -> store data in DynamoDB\n\nAn alternative approach you could consider is using Kinesis Firehose with Lambda transformations. This will allow you to do \"in-line\" parsing \/ transformation of your data before it is ever written to S3, this removing the need to re-read the data from S3 and apply transformations after the fact. Firehose also allows you to write the stored data in formats such as Parquet, which can help with cost and subsequent query performance.\n\nIf you want to store both raw data and transformed data, you can use a \"fanout\" pattern with Kinesis Streams\/Firehose, where one output is raw data to S3 and the other is a transformed stream.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Create or update Sagemaker Endpoint via CloudFormation",
        "Question_creation_time":1607356793000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXiLSnlxkQHKzQVFj6GKT7w\/create-or-update-sagemaker-endpoint-via-cloud-formation",
        "Question_topic":[
            "Machine Learning & AI",
            "Management & Governance",
            "DevOps"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS CloudFormation",
            "DevOps"
        ],
        "Question_upvote_count":0,
        "Question_view_count":242,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"A wants to manage Sagemaker resources (such as models and endpoints) via CloudFormation. As part of their model deployment pipeline, they'd like to be able to create or update existing Sagemaker Endpoint with new model data. Customers wants to re-use the same endpoint name for a given workload.\n\nQuestion:\n\nHow to express in CF a following logic:\n\nIf Sagemaker endpoint with name \"XYZ\" doesn't exist in customer account, then create a new endpoint;\nIf Sagemaker endpoint with name \"XYZ\" already exist, then update existing endpoint with new model data.",
        "Answers":[
            {
                "Answer_creation_date":"2020-12-07T16:06:33.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"This functionality of \"UPSERT\" type does not exist in CFn natively. You would need to use a Custom Resource to handle this logic. One alternative that is not exactly what you asked for but might be a decent compromise is to use a Parameter to supply the endpoint if it does exist. Then use a condition to check the value. If the paramter is blank then create an endpoint if not use the value supplied. I know this is not what you asked for but it allows you to avoid the custom resource solution.\n\nSample of similiar UPSERT example for a VPC:\n\nParameters :\n\n  Vpc:\n    Type: AWS::EC2::VPC::Id\n\nConditions:\n\n  VpcNotSupplied: !Equals [!Ref Vpc, '']\n\nResources:\n\n  NewVpc:\n    Type: AWS::EC2::VPC\n    Condition: VpcNotSupplied\n    Properties:\n      CidrBlock: 10.0.0.0\/16\n\n  SecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupDescription: Sample\n      GroupName: Sample\n      VpcId: !If [VpcNotSupplied, !Ref NewVpc, !Ref Vpc ]\n\n\nHere the Vpc input parameter can be supplied if the VPC you wish to use already exists, left blank if you want to create a new one. The NewVPC resource uses the Condition to only create if the supplied Vpc parameter value is blank. The Security group then uses the same condition to decide whetehr to use and existing Vpc or the newly created one.\n\nHope this makes sense.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"What is the difference between SageMaker Pipelines and SageMaker Step Function SDK?",
        "Question_creation_time":1606994100000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2iheeTzhSTmWw4aqVEeqOQ\/what-is-the-difference-between-sage-maker-pipelines-and-sage-maker-step-function-sdk",
        "Question_topic":[
            "Serverless",
            "Application Integration",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Step Functions",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":673,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"What is the difference between SageMaker Pipelines and SageMaker Step Function SDK?\n\nThe official Pipelines notebook is basically only doing a workflow - pretty much a copy cat of what step functions has been doing for years. In the nice video from Julien Simon I see CICD capacities mentioned, where are those? any demos?",
        "Answers":[
            {
                "Answer_creation_date":"2020-12-03T17:04:25.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hey, that demo is missing the project part of Pipelines and therefore the SM provided project templates. Go to SM studio and on the Studio summary hit edit settings and then enable access and provisioning of Service Catalog Portfolio of products in SM Studio. Then check your service catalog portfolios. Haven't tried it out yet though.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2022-08-29T14:23:35.610Z",
                "Answer_upvote_count":0,
                "Answer_body":"both StepFunctions and SageMaker pipeline can be used to build a pipeline for ML. In the end, we need a SageMaker DAG StepFunction can integrate with different services, and good when one already used it. SageMaker pipeline is natively integrated with SageMaker system.\n\nThis is my example using both to build a ML Pipeline GitHub",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"In SageMaker Studio, how to decide on which instance to open a terminal?",
        "Question_creation_time":1606945520000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUo5ycye8jQ7Cw8dgSAfE9RQ\/in-sage-maker-studio-how-to-decide-on-which-instance-to-open-a-terminal",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":495,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nI was writing some notebook on a t2.Medium Studio Notebook. Now I just switched to an m5.8xlarge. However, when I launch a terminal, it still shows up only 2 CPUs, not the 32 I expected. How to open a terminal on that m5.8xlarge instance?",
        "Answers":[
            {
                "Answer_creation_date":"2020-12-03T11:09:16.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Where do you launch the terminal from? If you use the launcher window, it would start on the t2.medium as you are experiencing.\n\nHowever, if you use the launch terminal button in the toolbar that is displayed at the top of your notebook, it will launch the image terminal on the new instance the notebook's kernel is running on (your m5.8xlarge instance).",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Jupyter notebook",
        "Question_creation_time":1606707121000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIzWlfNVTSIWIqkVsIaNv2A\/sagemaker-jupyter-notebook",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":62,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"What are the advantages of using SageMaker jupyter instance instead of running it locally? Is there a special integration with SageMaker that we lose it if we do not use Sagemaker jupyer instance?",
        "Answers":[
            {
                "Answer_creation_date":"2020-11-30T04:05:24.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Some useful points:\n\nThe typical arguments of cloud vs local will apply (as with e.g. Cloud9, Workspaces, etc): Can de-couple your work from the lifetime of your laptop, keep things running when your local machine is shut down, right-size the environment for what workloads you need to do on a given day, etc.\nSageMaker notebooks already run in an explicit IAM context (via assigned execution role) - so you don't need to log in e.g. as you would through the CLI on local machine... Can just run sagemaker.get_execution_role()\nPre-built environments for a range of use-cases (e.g. generic data science, TensorFlow, PyTorch, MXNet, etc) with libraries already installed, and easy wiping\/reset of the environment by stopping & starting the instance - no more \"environment soup\" on your local laptop.\nLinux-based environments, which typically makes for a shorter path to production code than Mac\/Windows.\nIf you started using SageMaker Studio, then yes there are some native integrations such as the UIs for experiment tracking and endpoint management\/monitoring; easy sharing of notebook snapshots; and whatever else might be announced over the next couple of weeks.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Receiving consistent AccessDenied errors",
        "Question_creation_time":1606179025000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQ3KwMxUsREup-lSqlA44Hg\/receiving-consistent-access-denied-errors",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":55,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to use SageMaker Notebook Instances, but consistently receive AccessDenied errors for commands that my IAM role should have access to (and for commands that worked the last time I tried several weeks ago). For example:\n\naws s3 ls results in An error occurred (AccessDenied) when calling the ListBuckets operation: Access Denied despite my role having the AmazonS3FullAccess policy attached.\n\nAlso aws ecr describe-repositories --repository-names \"sagemaker-decision-trees\" results in An error occurred (AccessDeniedException) when calling the DescribeRepositories operation: User: arn:aws:sts::XXXXXXXXXX:assumed-role\/AmazonSageMaker-ExecutionRole-20201123T151452\/SageMaker is not authorized to perform: ecr:DescribeRepositories on resource: arn:aws:ecr:us-east-2:XXXXXXXXXX:repository\/sagemaker-decision-trees with an explicit deny despite my role having the AmazonEC2ContainerRegistryFullAccess policy attached.\n\nOne thing that seems new is that \"SageMaker\" is appended to my user ARN. I can't remember seeing errors with this appended before.\n\nNote: I've replicated these errors with several combinations of configurations:\n\na new IAM role (which I created in the SageMaker console to have AmazonSageMakerFullAccess to any S3 bucket)\nfresh notebook instance\nwith (and without) a VPC\nAlso, these commands all work when run outside of a notebook instance (i.e. when run locally from my laptop).\n\nI'm guessing there's some problem with my account setup, but not sure what to try next.\nThanks.\n\nEdited by: DJAIndeed on Nov 24, 2020 8:35 AM",
        "Answers":[
            {
                "Answer_creation_date":"2020-12-22T22:39:21.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I am sorry to hear that you are not able to access required services from Sagemaker.\nYou can run below command to check the execution role that is getting used and then verify that required permission are present.\n\nimport sagemaker  \nsagemaker.get_execution_role()  \n\n\nOther useful links -\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html\nhttps:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/s3-troubleshoot-403\/\n\nEdited by: amitsur on Dec 22, 2020 2:41 PM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2020-12-23T00:15:18.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks, @amitsur. We had confirmed that the SageMaker Notebook Instance was using the desired execution role and that that role had the required permissions. The issue appears to have resolved itself though, since we're no longer receiving these errors. So it must have been a configuration elsewhere? We appreciate the help anyway.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"What value should I set for directory_path for the Amazon SageMaker SDK with FSx as data source?",
        "Question_creation_time":1605283057000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUHaScKqcfRu-aZ1Cwza63NQ\/what-value-should-i-set-for-directory-path-for-the-amazon-sage-maker-sdk-with-f-sx-as-data-source",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon FSx for Lustre",
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":1,
        "Question_view_count":120,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"What value should I set for the directory_path parameter in FileSystemInput for the Amazon SageMaker SDK?\n\nHere is some information about my Amazon FSx for Lustre file system:\n\nMy FSx ID is fs-0684xxxxxxxxxxx.\nMy FSx has the mount name lhskdbmv.\nThe FSx maps to an Amazon S3 bucket with files (without extra prefixes in their keys)\n\nMy attempts to describe the job and the results are the following:\n\nAttempt 1:\n\nfs = FileSystemInput(\n    file_system_id='fs-0684xxxxxxxxxxx',\n    file_system_type='FSxLustre',\n    directory_path='lhskdbmv',\n    file_system_access_mode='ro')\n\n\nResult:\n\nestimator.fit(fs) returns ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: FileSystem DirectoryPath 'lhskdbmv' for channel 'training' is not absolute or normalized. Please ensure you don't have a trailing \"\/\", and\/or \"..\", \".\", \"\/\/\" in the path.\n\nAttempt 2:\n\nfs = FileSystemInput(\n    file_system_id='fs-0684xxxxxxxxxxx',\n    file_system_type='FSxLustre',\n    directory_path='\/',\n    file_system_access_mode='ro')\n\n\nResult:\n\nClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: The directory path for FSx Lustre file system fs-068406952bf758bac is invalid. The directory path must begin with mount name of the file system.\n\nAttempt 3:\n\nfs = FileSystemInput(\n    file_system_id='fs-0684xxxxxxxxxxx',\n    file_system_type='FSxLustre',\n    directory_path='fsx',\n    file_system_access_mode='ro')\n\n\nResult:\n\nClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: FileSystem DirectoryPath 'fsx' for channel 'training' is not absolute or normalized. Please ensure you don't have a trailing \"\/\", and\/or \"..\", \".\", \"\/\/\" in the path.",
        "Answers":[
            {
                "Answer_creation_date":"2020-11-13T19:41:48.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"The directory_path parameter must point to \/mountname\/path\/to\/specific\/folder\/in-file-system. The value of mountname is returned in the CreateFileSystem API operation response. It is also returned in the response of the describe-file-systems AWS Command Line Interface (AWS CLI) command and the DescribeFileSystems API operation.\n\nFor your use case, the response might look similar to the following: mountName = lhskdbmv",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"How do I achieve the least-access secure networking for SageMaker Training on Amazon FSx for Lustre?",
        "Question_creation_time":1605279993000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrTkxH_kIT-a_LJSGYS5SXA\/how-do-i-achieve-the-least-access-secure-networking-for-sage-maker-training-on-amazon-f-sx-for-lustre",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI",
            "Networking & Content Delivery"
        ],
        "Question_tag":[
            "Amazon FSx for Lustre",
            "Amazon SageMaker",
            "Networking & Content Delivery",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":56,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I'm trying to figure out a minimally permissive yet operational network configuration for Amazon SageMaker training to train on data from Amazon FSx for Lustre. My understanding is that both the file system and the SageMaker instance can have their own security groups and that FSx uses TCP on ports 988 and 1021-1023. Therefore, I think a good network configuration for using SageMaker with FSx is the following:\n\nSageMaker EC2 equipped with the security group SM-SG that allows Inbound only with TCP on 988 and 1021-1023 from FSX-SG only.\nAmazon FSx equipped with the security group FSX-SG that allows outbound only with TCP on 988 and 1021-1023 towards SM-SG only. Is this configuration enough for the training to work? Do FSx and SageMaker need other ports and sources to be opened to operate normally?",
        "Answers":[
            {
                "Answer_creation_date":"2020-11-13T15:26:19.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"For the security group for Amazon FSx (Example: FSx-SG), you need to add the following additional rules:\n\nFSx-SG needs inbound access from the security group for SageMaker (Example: SM-SG). The SageMaker instance needs to initiate a connection to the Amazon FSx file system, which is an inbound TCP packet to FSx.\nFSx-SG needs inbound and outbound access to itself. This is because, Amazon FSx for Lustre is a clustered file system, where each file system is typically powered by multiple file servers, and the file servers need to communicate with one another.\n\nFor more information on the minimum set of rules required for FSx-SG, see [File system access control with Amazon VPC][1]. [1]: https:\/\/docs.aws.amazon.com\/fsx\/latest\/LustreGuide\/limit-access-security-groups.html",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"How to deploy N models on N Greengrass devices with a unique Lambda for inference logic?",
        "Question_creation_time":1605018964000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlVJHC1NaTTOquvDqs444oQ\/how-to-deploy-n-models-on-n-greengrass-devices-with-a-unique-lambda-for-inference-logic",
        "Question_topic":[
            "Internet of Things (IoT)",
            "Serverless",
            "Compute",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS IoT Greengrass",
            "AWS Lambda",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Internet of Things (IoT)"
        ],
        "Question_upvote_count":0,
        "Question_view_count":25,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nLet's consider an ML edge inference use-case on Greengrass-managed device. The model is unique to each device, however its architecture and invocation logic are the same for all devices. In other words, the same invocation Lambda could be the same for all devices, only the model parameters would need to change across devices. We'd like to deploy a unique inference Lambda to all devices, and load device-specific artifact to each device.\n\nCan this be achieved with Greengrass ML Inference? It seems that GG MLI requires each model to be associated with a specific Lambda.\n\nOtherwise, is the recommended pattern to self-manage the inference in Lambda? E.g. by loading a specific model from S3 unique a local config file or some env variable?",
        "Answers":[
            {
                "Answer_creation_date":"2020-11-10T14:55:38.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"In IoT Greengrass 1.x, the configuration is unique to each Greengrass Group. This includes Connectors, Lambdas and ML Resources.\n\nThe same Lambda can be referenced by multiple groups as a Greengrass function, which is likely what you want. This is similar to using one of the GG ML connectors (Object Detection or Image Classification).\n\nIn addition to your inference code, you'll also need to configure an ML Resource, which has a local name and a remote model. The local name would be the same for all Greengrass Groups, but in each group you will refer to a different remote object (the model) - either S3 or SageMaker job.\n\nEvery time a model changes, you will need to redeploy the corresponding Greengrass group for the changes to be deployed locally.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Relevancy of SageMaker Model Monitor for NLP?",
        "Question_creation_time":1605005518000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxCKLg-eiQ1mwZvzFyczBEg\/relevancy-of-sage-maker-model-monitor-for-nlp",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":61,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nCan SageMaker Model Monitor be applied in NLP models? Is it necessary to do some preprocessing of the data? How can we use SageMaker Model Monitor? sentence length, unseen words, language etc. Any thoughts or experience on that?",
        "Answers":[
            {
                "Answer_creation_date":"2020-11-10T11:24:02.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yes, You can use model monitor for data capture and scheduling in your own custom container with the relevant monitoring for NLP use case.\nFor example, there's a blog post for model monitor for computer vision classification prediction with defined *alert * of predict more than expected.\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/automated-monitoring-of-your-machine-learning-models-with-amazon-sagemaker-model-monitor-and-sending-predictions-to-human-review-workflows-using-amazon-a2i\/?nc1=b_rp",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"SageMaker training with FSx: what is \"directory_path\"",
        "Question_creation_time":1604678225000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCaemzfoDRIy9AgLRW8suqw\/sage-maker-training-with-f-sx-what-is-directory-path",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon FSx for Lustre",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":105,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"SageMaker can train on FSx data. One SageMaker SDK parameter for FSx training is directory_path. Where do we find that?",
        "Answers":[
            {
                "Answer_creation_date":"2020-11-06T16:13:42.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"FSx for Lustre is a file system that you can use to provide high performance for ML training workloads. The directory_path should point to the location on your file system where your dataset is stored.\n\nIn the example in the docs: directory_path='\/fsx\/tensorflow',\n\n\/fsx is the directory you define on your compute instances where you are mounting the file system \/tensorflow would represent a folder within the fsx directory\n\nIf you are using an S3-linked FSx for Lustre file system \/tensorflow would be a prefix within your S3-linked bucket.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Why does my kernal keep dying when I try to import Hugging Face BERT models to Amazon SageMaker?",
        "Question_creation_time":1604517955000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsO3sfUGpTKeHiU8W9k1Kwg\/why-does-my-kernal-keep-dying-when-i-try-to-import-hugging-face-bert-models-to-amazon-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":458,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"When I try to import Hugging Face BERT models to the conda_pytorch_p36 kernal of my Amazon SageMaker Notebook instance using the following pip command, the kernal always dies:\n\n! pip install transformers\n\n\nThe result is the same for Hugging Face BERT, RoBERTa, and GPT2 models on ml.c5.2xlarge and ml.c5d.4xlarge Amazon SageMaker instances.\n\nWhy is this happening, and how do I resolve the issue?",
        "Answers":[
            {
                "Answer_creation_date":"2020-11-04T22:05:35.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"This issue occurs when the latest sentence piece breaks. The workaround is to force install sentencepiece==0.1.91.\n\npip install sentencepiece==0.1.91",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Running a request against all variants in an endpoint",
        "Question_creation_time":1604486652000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6bm-EMtOQV6robgbTXClLQ\/running-a-request-against-all-variants-in-an-endpoint",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":14,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I have a customer asking me about the Rendezvous architecture. What I'm thinking is, we could implement this in a number of ways, all using endpoint variants:\n\nLambda (and probably SQS) around the endpoint;\nA custom monitoring job;\nStep Functions\n\nWithout going into details of the above options or of how the evaluation and SLA check will be done, it looks like the several models would fit very well as variants of an endpoint. The thing is, the architecture expects to call them all. Is there a way to directly call all variants of a model, or will a wrapper to identify the variants, call them all and process the results be needed?",
        "Answers":[
            {
                "Answer_creation_date":"2020-11-04T16:22:44.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"When I last looked into it, it was not possible to query all versions\/variants of the model automatically. You can specify what variant to use when using the invoke_endpoint method. I would therefore write a lambda function to invoke each of the endpoints one-by-one (see here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpoint.html). To be especially rigorous about it, you can add a function in your lambda code that first retrieves all the endpoint variants (see here: https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.describe_endpoint) then queries them one-by-one, and returns all the results.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Can Amazon SageMaker endpoints be fitted with multiple Amazon Elastic Inference accelerators?",
        "Question_creation_time":1604477936000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwU8IHcSVQ3eH9-fGx0KZCA\/can-amazon-sage-maker-endpoints-be-fitted-with-multiple-amazon-elastic-inference-accelerators",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Elastic Inference"
        ],
        "Question_upvote_count":0,
        "Question_view_count":39,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Can Amazon SageMaker endpoints be fitted with multiple Amazon Elastic Inference accelerators?\n\nI see that in EC2 it's possible, however I don't see it mentioned in Amazon SageMaker documentation.",
        "Answers":[
            {
                "Answer_creation_date":"2020-11-04T16:17:19.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"No, they cant be; multi-attach is only supported with EC2.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Can I limit the type of instances that data scientists can launch for training jobs in SageMaker?",
        "Question_creation_time":1603454458000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUd77APmdHTx-2FZCvZfS6Qg\/can-i-limit-the-type-of-instances-that-data-scientists-can-launch-for-training-jobs-in-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":426,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"We want to limit the types of instances that our data scientists can launch for running training jobs and hyperparameter tuning jobs in SageMaker. Is it possible to limit the instance size options available through SageMaker by using IAM policies, or another method? For example: Could we remove the ability to launch ml.p3.16xlarge instances?",
        "Answers":[
            {
                "Answer_creation_date":"2020-10-23T12:15:48.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yes, you can limit the types of instances that are available for your data scientists to launch in SageMaker by using an IAM policy similar to the following one:\n\nNote: This example IAM policy allows SageMaker users to launch only Compute Optimized (ml.c5)-type training jobs and hyperparameter tuning jobs.\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"EnforceInstanceType\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:CreateTrainingJob\",\n                \"sagemaker:CreateHyperParameterTuningJob\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"ForAllValues:StringLike\": {\n                    \"sagemaker:InstanceTypes\": [\"ml.c5.*\"]\n                }\n            }\n        }\n\n     ]\n}",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Where can I find guidance for getting a customer started with SageMaker sizing and cost?",
        "Question_creation_time":1603285551000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq-Kaj1bLStK6Bs2gCUZ1Iw\/where-can-i-find-guidance-for-getting-a-customer-started-with-sage-maker-sizing-and-cost",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":33,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"A customer wants to use SageMaker, but doesn't know how to get started with instance sizes or how to forecast the cost for it. I've looked at the SageMaker TCO PDF we have online, but that appears more marketing than helpful, i.e. more price comparison than guidance.\n\nI know that the SageMaker cost is really the underlying EC2 and storage pieces, not SageMaker itself. However, I feel it is incorrect to say that they start with (say) t3.medium and see if that fits and scale up if they need more power behind it. As well, that doesn't help them to forecast either.\n\nAny thoughts here?",
        "Answers":[
            {
                "Answer_creation_date":"2020-10-21T13:22:02.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"See the performance efficiency and cost optimization pillars in Machine Learning Lens. Additionally this is an EC2 based right sizing best practices guide.\nOverall, it's better to start small, then increase instance size as needed (as those that start large, never bother reduce the size), or apply auto scaling for SageMaker hosting.\nAssuming a CPU ML predictions: When choosing ml.t2.medium instances the customer will need to keep an eye on the instance CPU credits. If they lack the knowledge, just start with M5.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"What is the cluster manager in SageMaker Spark Processing?",
        "Question_creation_time":1602770746000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUShPm0t4vR4S8XBKMiAcA6g\/what-is-the-cluster-manager-in-sage-maker-spark-processing",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":81,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"SageMaker Processing can launch multi-instance jobs. What is the underlying cluster manager? Yarn? Mesos? Something custom?",
        "Answers":[
            {
                "Answer_creation_date":"2020-10-15T14:12:23.000Z",
                "Answer_upvote_count":1,
                "Answer_body":"The Spark container uses YARN - for ref the bootstrap script on github: https:\/\/github.com\/aws\/sagemaker-spark-container\/blob\/master\/src\/smspark\/bootstrapper.py and the Dockerfile with hadoop-yarn dependencies",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Please validate: SageMaker Endpoint URL Authentication\/Authorization",
        "Question_creation_time":1602169065000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFlHNZ7JxTFGIkPHQ75u44w\/please-validate-sage-maker-endpoint-url-authentication-authorization",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":282,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Need validation:\n\nOnce the SageMaker endpoint is deployed. It can be invoked with the Sagemaker Runtime API InvokeEndpoint OR it can be invoked using the endpoint URL+HTTP AZ headers (below).\n\nSuccessful deployment also exposes a URL (on the console) that has the format:\n\nhttps:\/\/runtime.sagemaker.us-east-1.amazonaws.com\/endpoints\/ENDPOINT-NAME\/invocations\n\nWhat is the purpose of this URL (shown on console)?\n\nIn my understanding this URL Cannot be invoked w\/o appropriate headers as then there will be a need to have globally unique endpoint name!! THAT IS to invoke this URL it needs to have the \"HTTP Authorization headers\" (refer: https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/API\/sig-v4-authenticating-requests.html)\n\nI have a customer who is concerned that anyone can invoke the URL even from the internet. Tried to do it and received the <MissingTokenException> so I know it can't be done but just want to ensure I have the right explanation. (Test with HTTP\/AZ headers pending)",
        "Answers":[
            {
                "Answer_creation_date":"2020-10-08T15:38:33.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Your understanding is correct. From the docs:\n\nAmazon SageMaker strips all POST headers except those supported by the API. Amazon SageMaker might add additional headers. You should not rely on the behavior of headers outside those enumerated in the request syntax.\n\nCalls to InvokeEndpoint are authenticated by using AWS Signature Version 4.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Tracking the lineage between Amazon SageMaker endpoint model and Model Monitor captured data",
        "Question_creation_time":1602088286000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPWBH_xFoS4aq5i41Za5qaQ\/tracking-the-lineage-between-amazon-sage-maker-endpoint-model-and-model-monitor-captured-data",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":84,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"I have an Amazon SageMaker endpoint with A1, a model with data capture activated, and I want to update the endpoint with A2, a new model.\n\nHow do I track the Model Monitor Data Capture that captured data in Amazon S3, and identify which data referred to model A1 and which data referred to model A2?",
        "Answers":[
            {
                "Answer_creation_date":"2020-10-08T08:01:12.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"using boto3:\nwhen you update the model endpoint, you need to create a new EndpointConfig where you specify a new s3 uri where data capture will be stored and thats how you can see different data captures from different versions of the model.\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateEndpoint.html\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpointConfig.html",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2021-09-23T19:58:35.449Z",
                "Answer_upvote_count":0,
                "Answer_body":"When you update the model endpoint (using boto3), you must to create a new EndpointConfig where you specify a new Amazon S3 URI. The new Amazon S3 URI is where the data capture will be stored so that you can see different data captures from different versions of the model.\n\nFor more information, see the following: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateEndpoint.html https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpointConfig.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Can I use SageMaker Autopilot for my time-series modeling?",
        "Question_creation_time":1601671292000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxaXaoPpqRyGdkh6aKK9uew\/can-i-use-sage-maker-autopilot-for-my-time-series-modeling",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":166,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I'm working on time-series modeling. I'm comparing battle-of-algorithms against the autopilot machine learning approach to identify the model that best fits my use case. I understand that Amazon SageMaker Autopilot doesn't work with time series. Is there an alternative library or algorithm in the AWS ecosystem that implements battle-of-algorithms for time series?",
        "Answers":[
            {
                "Answer_creation_date":"2020-10-05T07:31:28.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Amazon SageMaker Autopilot currently supports regression, binary classification, and multi-class classification. SageMaker supports only tabular data formatted in files with comma-separated values. For more information, see Automate model development with Amazon SageMaker Autopilot.\n\nFor your use case, you can use Amazon Forecast. Amazon Forecast includes the AutoML feature that trains different models with your target time series, related time series, and item metadata. Amazon Forecast then uses the model with the best accuracy metrics.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Using AWS services to perform pet face recognition",
        "Question_creation_time":1601645649000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2S-z85hhSr-XVecDxE3ihw\/using-aws-services-to-perform-pet-face-recognition",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS DeepLens",
            "Amazon SageMaker",
            "Amazon Rekognition"
        ],
        "Question_upvote_count":0,
        "Question_view_count":132,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I want to build a camera that automatically recognizes a specific pet with image or video recognition. What AWS service can I use to identify an individual pet (not just the pet type). I've tried to use AWS Rekognition, but it can only differentiate between animal types, race, or color. Amazon SageMaker could be another option to create a completely new mode, but is very costly. What other AWS services can I use to identify specific pets?",
        "Answers":[
            {
                "Answer_creation_date":"2020-10-02T14:29:04.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can use Amazon Rekognition Custom Labels to use single class object detection to identify or classify a specific animal. However, note that Amazon Rekognition Custom Labels do not perform animal face recognition. It only classifies the image or object.\n\nFor example, you can train your detection model to identify an animal based on the images you provide for that label. For more information about using Amazon Rekognition Custom Labels, see this blog: https:\/\/aws.amazon.com\/blogs\/machine-learning\/training-a-custom-single-class-object-detection-model-with-amazon-rekognition-custom-labels\/.\n\nTo use frames from a video with Custom Labels, see Video analysis .",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"How do you analyze Autopilot results in Amazon SageMaker Studio?",
        "Question_creation_time":1601332827000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU7p9B6zcpSIeTG4MbzrSjKA\/how-do-you-analyze-autopilot-results-in-amazon-sage-maker-studio",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":38,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I launched an Autopilot job in SageMaker Studio, and now I'm trying to figure out how to compare autoML iterations. Is there a way to list them, see their metrics, and see the configuration of the best job?",
        "Answers":[
            {
                "Answer_creation_date":"2020-09-29T00:33:01.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Watch the Choose and deploy the best model video tutorial in the SageMaker developer guide. The video shows how to use SageMaker Autopilot to visualize and compare model metrics.\n\nFor more SageMaker Autopilot tutorials, see Videos: Use Autopilot to automate and explore the machine learning process.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Is it possible to test locally SageMaker Inference Pipelines?",
        "Question_creation_time":1600158011000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU8R_MjbU1QPm66SCgld4spQ\/is-it-possible-to-test-locally-sage-maker-inference-pipelines",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":202,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Is it possible to test locally SageMaker Inference Pipelines? I would like to be able to easily troubleshoot and find the appropriate serialization between containers",
        "Answers":[
            {
                "Answer_creation_date":"2020-09-25T12:39:21.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"If you are referring to using local mode via the SM PySDK, then pipeline deployment is not supported.\n\nAs an alternative, given your three inference containers, you could manually run the services locally and then implement a kind of facade function that invokes the three services in pipeline and manages input\/output accordingly.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"xgboost sagemaker batch transform job output in multiple lines",
        "Question_creation_time":1599771185000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYz7Bz_5sTmG0uBaqlt7J_g\/xgboost-sagemaker-batch-transform-job-output-in-multiple-lines",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":199,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hello,\n\nI've just trained a churn prediction model with XGBoost algorithm, based on the SageMaker example notebooks. I've created SageMaker batch transformation jobs using this model using input from CSV file with multiple records, however the output file is a single record CSV containing all the inferences in a single comma separated row. The result is that I'm not able to use the \"Join source\" feature with \"Input - Merge input data with job output\" since the input and output files must match the number of records. I've tried with different batch job configurations but I always get the same single line output file.\n\nDo you know if is there any configuration that allows me to merge input and output in order to have a direct association between an input column with its inference result? Is this a restriction from the XGBoost algorithm built-in implementation?",
        "Answers":[
            {
                "Answer_creation_date":"2020-09-11T02:38:30.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Sounds like a configuration issue, this algorithm should be able to output proper output CSVs.\n\nAre you using accept=\"text\/csv\" and assemble_with=\"Line\" on your Transformer? Is your strategy set to SingleRecord or MultiRecord?\n\nAnd split_type=\"Line\", content_type=\"text\/csv\" on the .transform() call?\n\nI have had custom algorithms accidentally output row vectors instead of column vectors for multi-record batches in the past (because they gave a 1D output which the default serializer interpreted as a row), but not built-in algorithms.\n\nDropping to SingleRecord could be a last resort (forcing Batch Transform itself to handle the serialization), but would decrease efficiency\/speed.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Automated streaming integration and multiple requests for SageMaker endpoint",
        "Question_creation_time":1599656695000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6E1eYARES123bRYAe2B0Ag\/automated-streaming-integration-and-multiple-requests-for-sage-maker-endpoint",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics",
            "Media Services"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI",
            "Amazon Kinesis Data Analytics",
            "Amazon Kinesis"
        ],
        "Question_upvote_count":0,
        "Question_view_count":88,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"A data scientist is looking to host a Tensorflow model in SageMaker and process low volume streaming event data (~2-3 per second) to collect inferences about each event. Data scientist is looking at having the SageMaker inference model plugged in as a Kinesis Data Analytics Application but Kinesis Data Analytics currently only supports SQL or Flink.\n\nOne option to set up an ECS or Lambda service to consume data from Kinesis or SNS and invoke the SageMaker inference endpoint per message, but if there is a more automated and optimal solution available for these kind of workflows.\n\nIt is not possible to pass multiple requests currently to a SageMaker endpoint, yet Tensorflow models tend to perform much better on batches of data rather than multiple single invocations so some windowing would be beneficial. Ideally the client would want to react to an inference within 10-15 seconds of the event being processed so an S3 based batch approach is probably too slow.\n\nIs there anything you can recommend for handling this sort of workload?",
        "Answers":[
            {
                "Answer_creation_date":"2020-09-15T05:59:33.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"To build integration between SageMaker endpoints and Kinesis Data Application use this blog - https:\/\/aws.amazon.com\/blogs\/architecture\/realtime-in-stream-inference-kinesis-sagemaker-flink\/. It help to setup serverless service to invoke the SageMaker inference endpoint.\n\nTo use batching. The Tensorflow documentation mentions the following:\n\nThis link mentions that you can include multiple instances in your predict request (or multiple examples in classify\/regress requests) to get multiple prediction results in one request to your Endpoint.\nThis link mentions that you can configure SageMaker TensorFlow Serving Container to batch multiple records together before performing an inference\n\nYou would still have to handle the logic internally in ECS\/Lambda to control how many records you consume from your stream in one batch, but at least you will be able to infer on the whole batch on the SageMaker endpoint end based on the above.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"ModuleNotFoundError when starting a training job on Sagemaker",
        "Question_creation_time":1598912648000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJMd_4s52RpWXDXITXFsQdw\/module-not-found-error-when-starting-a-training-job-on-sagemaker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":357,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I want to submit a training job on sagemaker. I tried it on notebook and it works. When I try the following I get ModuleNotFoundError: No module named 'nltk'\n\nMy code is\n\nimport sagemaker  \nfrom sagemaker.pytorch import PyTorch\n\nJOB_PREFIX   = 'pyt-ic'\nFRAMEWORK_VERSION = '1.3.1'\n\nestimator = PyTorch(entry_point='finetune-T5.py',\n                   source_dir='..\/src',\n                   train_instance_type='ml.p2.xlarge' ,\n                   train_instance_count=1,\n                   role=sagemaker.get_execution_role(),\n                   framework_version=FRAMEWORK_VERSION, \n                   debugger_hook_config=False,  \n                   py_version='py3',\n                   base_job_name=JOB_PREFIX)\n\nestimator.fit()\n\n\nfinetune-T5.py have many other libraries that are not installed. How can I install the missing library? Or is there a better way to run the training job?",
        "Answers":[
            {
                "Answer_creation_date":"2020-08-31T22:47:15.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Check out this link (Using third-party libraries section) on how to install third-party libraries for training jobs. You need to create requirement.txt file in the same directory as your training script to install other dependencies at runtime.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"AWS SageMaker Studio pricing. How does the billing work?",
        "Question_creation_time":1598441345000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUp_yTquDhTgiweq85PA1oWg\/aws-sage-maker-studio-pricing-how-does-the-billing-work",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":1199,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello,\n\nI am currently considering using SageMaker Studio (moved from plain SageMaker Notebook Instances to try the functionality of the Studio) and am currently trying to figure out how the billing part works.\n\nBasically my questions are:\n\nDo we start to pay after Notebook is initialized (in Studio) or when we initialize Notebook and Kernel (e.g. Python3) is performing tasks?\nOr is the billing based upon \"Running Apps\" (if that's the case, could you please describe nuances; in particular I am interested in \"default\" App and whether we pay for it or not)\nIf not like in questions 1. and 2., then how?\n\nI would appreciate any help in this matter :)",
        "Answers":[
            {
                "Answer_creation_date":"2020-09-16T05:04:40.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thank you for your interest in SageMaker Studio notebook.\n\nTo answer your questions:\n\nDo we start to pay after Notebook is initialized (in Studio) or when we initialize Notebook and Kernel (e.g. Python3) is performing tasks?\n\nWhen you initialize a notebook & kernel, a compute resource is launched to run the notebook, so you will start getting billed. You don't get charged per notebook. You can create and run multiple notebooks on the same compute instance. You pay only for the compute that you use, not for individual items.\n\nOr is the billing based upon \"Running Apps\" (if that's the case, could you please describe nuances; in particular I am interested in \"default\" App and whether we pay for it or not)\n\nYou are billed by compute instance, not running apps. You can have up to 4 apps running on the same compute instance.\n\nIf not like in questions 1. and 2., then how?\n\nYou pay for both compute and storage when you use SageMaker Studio notebooks. See Amazon SageMaker Pricing (https:\/\/aws.amazon.com\/sagemaker\/pricing\/) for charges by compute instance type. Your notebooks and associated artifacts such as data files and scripts are persisted on Amazon EFS. See Amazon EFS Pricing (https:\/\/aws.amazon.com\/efs\/pricing\/) for storage charges. You can find more FAQs on SageMaker Studio notebook pricing here: https:\/\/aws.amazon.com\/sagemaker\/faqs\/#Build_Models\n\nEdited by: HuongNguyen on Sep 15, 2020 10:05 PM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2020-09-16T05:04:46.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thank you for your interest in SageMaker Studio notebook.\n\nTo answer your questions:\n\nDo we start to pay after Notebook is initialized (in Studio) or when we initialize Notebook and Kernel (e.g. Python3) is performing tasks?\n\nWhen you initialize a notebook & kernel, a compute resource is launched to run the notebook, so you will start getting billed. You don't get charged per notebook. You can create and run multiple notebooks on the same compute instance. You pay only for the compute that you use, not for individual items.\n\nOr is the billing based upon \"Running Apps\" (if that's the case, could you please describe nuances; in particular I am interested in \"default\" App and whether we pay for it or not)\n\nYou are billed by compute instance, not running apps. You can have up to 4 apps running on the same compute instance.\n\nIf not like in questions 1. and 2., then how?\n\nYou pay for both compute and storage when you use SageMaker Studio notebooks. See Amazon SageMaker Pricing (https:\/\/aws.amazon.com\/sagemaker\/pricing\/) for charges by compute instance type. Your notebooks and associated artifacts such as data files and scripts are persisted on Amazon EFS. See Amazon EFS Pricing (https:\/\/aws.amazon.com\/efs\/pricing\/) for storage charges. You can find more FAQs on SageMaker Studio notebook pricing here: https:\/\/aws.amazon.com\/sagemaker\/faqs\/#Build_Models\n\nEdited by: HuongNguyen on Sep 15, 2020 10:05 PM",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker python sdk installation troubles",
        "Question_creation_time":1597398744000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZe9xubsHTuyRPCUGXvi40Q\/sagemaker-python-sdk-installation-troubles",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":145,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nI've tried following a tutorial to use AWS sagemaker in script mode from my local linux VM, but I can't even get the basics working.\n\nSteps:\n\n\u279c ~> python3 --version\nPython 3.6.9\n\n\u279c ~> pip3 install sagemaker\n...\nSuccessfully installed boto3-1.14.42 botocore-1.17.42 importlib-metadata-1.7.0 packaging-20.4 protobuf3-to\n-dict-0.1.5 s3transfer-0.3.3 sagemaker-2.3.0 smdebug-rulesconfig-0.1.4 zipp-3.1.0\n\n\u279c ~> cat sagemaker.py\nimport sagemaker\nimport boto3\n\nsess = sagemaker.Session()\n\n\u279c > python3 sagemaker.py\nTraceback (most recent call last):\nFile \"sagemaker.py\", line 1, in <module>\nimport sagemaker\nFile \"\/sagemaker.py\", line 4, in <module>\nsess = sagemaker.Session()\nAttributeError: module 'sagemaker' has no attribute 'Session'\n\nI also have the aws cli (version 2) installed, and configured using IAM credentials that have full rights, so that's not related.\n\nWhat is the problem with my python sdk install? TIA",
        "Answers":[
            {
                "Answer_creation_date":"2020-08-14T17:11:58.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"You should name your script something else than sagemaker.py, since python will look in the current directory first for a module when doing an import, so the import sagemaker will not import the Sagemaker SDK, but your script.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2020-08-15T12:15:01.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Well, that one is for my list of top 10 most stupid programming things I've ever done... Thanks!",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Which Amazon SageMaker algorithms can only use GPU for training?",
        "Question_creation_time":1597251203000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdTLbPM2STGelSj1g3TIjpA\/which-amazon-sage-maker-algorithms-can-only-use-gpu-for-training",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":121,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I read somewhere that some Amazon SageMaker's built-in algorithms can only be trained using GPU, whereas some can use either GPU or CPU, and some can only be used on CPU.\n\nIs there any official documentation explicitly stating which algorithms can only use GPU or both?",
        "Answers":[
            {
                "Answer_creation_date":"2020-08-12T17:02:17.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Documentation for Amazon SageMaker built-in algorithms provides recommendations around choice of Amazon EC2 instances and whether given algorithm supports GPU or CPU devices.\n\nLet's take Image Classification as an example. Here is a excerpt from online documentation:\n\nFor image classification, we support the following GPU instances for training: ml.p2.xlarge, ml.p2.8xlarge, ml.p2.16xlarge, ml.p3.2xlarge, ml.p3.8xlargeand ml.p3.16xlarge. We recommend using GPU instances with more memory for training with large batch sizes. However, both CPU (such as C4) and GPU (such as P2 and P3) instances can be used for the inference. You can also run the algorithm on multi-GPU and multi-machine settings for distributed training.\n\nFor more complex scenarios, such as Script or BYO Container modes, customers have flexibility to choose which device (GPU or CPU) to utilize for which operation. This is configured as part of their training scripts.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"SageMaker AutoPilot Regions",
        "Question_creation_time":1596620138000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_7jk19ozQSeyjTAR_D_hEA\/sage-maker-auto-pilot-regions",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":34,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Is there official documentation showing the regions in which SageMaker AutoPilot is supported? From my understanding, it should work with the SDK wherever SageMaker is supported, while in the no-code mode only where SageMaker Studio is available. Is this true?\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_date":"2020-08-05T13:44:15.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"SageMaker Autopilot works in all the regions where Amazon SageMaker is available today as noted in this blog post \"Amazon SageMaker Autopilot \u2013 Automatically Create High-Quality Machine Learning Models With Full Control And Visibility\". In addition, Autopilot is also integrated with Amazon SageMaker Studio, which is available in us-east-1, us-east-2, us-west-2 and eu-west-1. For a current list of available regions, please check the AWS Regional Services List.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Has SAS code ever been successfully ran on SageMaker?",
        "Question_creation_time":1596105364000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqMU2EBqGTDCMTPsB5rjNoQ\/has-sas-code-ever-been-successfully-ran-on-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":94,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Has SAS code ever been successfully ran on SageMaker?",
        "Answers":[
            {
                "Answer_creation_date":"2020-07-30T14:02:45.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I\u2019ve helped customers run SAS on a notebook through a kernel and that was their main use case, but we also showed them how they can containerize SAS. Worked well",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"SageMaker GroundTruth Interface - option to skip a task and then return",
        "Question_creation_time":1596055479000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_S8ylg4UQdKh76o3zp3dWQ\/sage-maker-ground-truth-interface-option-to-skip-a-task-and-then-return",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth"
        ],
        "Question_upvote_count":1,
        "Question_view_count":68,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Customer wants to configure the SageMaker Ground Truth interface seen by the workers such that the labeler can navigate to previous or next tasks. For example, if one is labelling images, they could skip the current image, label the next one, and then return to the skipped image. The Ground Truth interface does not seem to have this capability. Is there an option for it that I missed? I could not find anything about it here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-labeling.html.",
        "Answers":[
            {
                "Answer_creation_date":"2020-07-29T21:28:39.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Currently, there is no functionality to skip a task and go back to it later. However, you could add a field like\n\n[ ] this task was skipped\n\nwhere the annotator could check the box for those items to be reviewed and processed at another time.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"What is SageMaker Autopilot doing when in state \"InProgress - AnalyzingData\" ?",
        "Question_creation_time":1596036787000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU8QUiTTSMQ2W2uOgHXC7lqA\/what-is-sage-maker-autopilot-doing-when-in-state-in-progress-analyzing-data",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":24,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nI'm trying this nice SageMaker Autopilot demo https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/autopilot\/autopilot_customer_churn_high_level_with_evaluation.ipynb\n\nAt the beginning of the job, the status is \"InProgress - AnalyzingData\" for several minutes. This is long enough that I'd like to know more about it: what is Autopilot doing when at that status?",
        "Answers":[
            {
                "Answer_creation_date":"2020-08-20T01:25:08.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"There are some metrics begin collected in this stage. To understanding what is doing is the same as what happens when you're using tensorflow autoML. There's a deep explanation what is does in our Science page https:\/\/www.amazon.science\/publications\/amazon-sagemaker-autopilot-a-white-box-automl-solution-at-scale",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Does Ground Truth Support Circles?",
        "Question_creation_time":1595625000000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqVY0A3PIQsuJpcce1tjJcA\/does-ground-truth-support-circles",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon SageMaker Ground Truth"
        ],
        "Question_upvote_count":0,
        "Question_view_count":27,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Customer has circular objects in their data. Does Ground Truth support drawing circles rather than boxes out of the box (no pun intended)? I know that it supports semantic segmentation, but that is overkill in this case.",
        "Answers":[
            {
                "Answer_creation_date":"2020-07-24T21:37:59.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"As of July 2020, we currently have Crowd HTML Element support for bounding box and polygons.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"AutoPilot for Forecasting",
        "Question_creation_time":1595154416000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUu_IodOxiQ6eTu010AYb8pQ\/auto-pilot-for-forecasting",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":40,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi there,\n\nIHAC who asked a question regarding the possible use of AutoPilot for solving Forecasting problems. They don't have the knowledge to play with DeepAR and they are running tests in parallel with Amazon Forecast. Their questions are:\n\nIs it possible to use AutoPilot for Forecasting problems? (my answer would be yes, since regression problems can be solved by XGBoost, which also won a bunch of competitions on Forecasting)\nWhich kind of pre-processing should the customer do and which pre-processing is done by AutoPilot which could simplify transformation of data for solving forecasting? In particular: are there any transformation to be done on the timestamp column? Should we introduce lagged entries - or is it done by AutoPilot?\n\nThanks to those taking the time to answer these questions :)\n\nBest, Davide Gallitelli",
        "Answers":[
            {
                "Answer_creation_date":"2020-07-20T08:24:30.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Although it is possible to model the forecasting as a regression problem in Autopilot, there is no time-series capability built in Autopilot. So, you need to do the preprocessing tasks such as time-series windowing, lag differencing, etc. in order to generate the training\/test datasets for the autopilot experiment.\n\nAdditionally, time series forecasting usually requires a model which can detect the pattern in a sequence of features. So, services such as DeepAR or Amazon forecast provide better capabilities to address this challenge.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Did the SageMaker PyTorch deployment process change?",
        "Question_creation_time":1594979513000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFIru4hJ2TcWLi7CYt3mnuw\/did-the-sage-maker-py-torch-deployment-process-change",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":134,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Did the SageMaker PyTorch deployment process change?\n\nIt use to be the case that people needed to have a model.tar.gz in s3, and an inference script locally or in git. Now, it seems that the inference script must also be part of the model.tar.gz. This is new, right?\n\nFrom the docs, https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#for-versions-1-2-and-higher:\n\n*For PyTorch versions 1.2 and higher, the contents of model.tar.gz should be organized as follows:\n\nModel files in the top-level directory\nInference script (and any other source files) in a directory named code\/ (for more about the inference script, see The SageMaker PyTorch Model Server)\nOptional requirements file located at code\/requirements.txt (for more about requirements files, see Using third-party libraries)*\n\nThis may be confusing, because this new mode of deployment means that people creating the model artifact need to know in advanced how the inference is going to look like. The previous design, with separation of artifact and inference code, was more agile.",
        "Answers":[
            {
                "Answer_creation_date":"2020-07-17T10:23:38.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"When AWS Sample - BERT sample using torch 1.4 was published, advance knowledge of the inference seems to be necessary. If you use the PyTorch SageMaker SDK to create or deploy the model after it is trained, it automatically re-packages the model.tar.gz to include the code files and the inference files. As an example, when you use the following script, the model.tar.gz is repackaged so the contents of the src directory is automatically added to the code directory model.tar.gz, which initially only contains model files. You don't need to know the inference code in advance.\n\nfrom sagemaker.pytorch import PyTorchModel\nfrom sagemaker import get_execution_role\nrole = get_execution_role()\n\nmodel_uri = estimator.model_data\n\nmodel = PyTorchModel(model_data=model_uri,\n                     role=role,\n                     framework_version='1.4.0',\n                     entry_point='serve.py',\n                     source_dir='src')\n\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge')\n\nFor the older versions, you couldn't include additional files \/dependencies during inference unless you built a custom container. The source.tar.gz was only used during training.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Rekognition Custom Labels and Ground Truth integration question",
        "Question_creation_time":1594736742000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUH-j5YmdZTiSEvLnm1ghztw\/rekognition-custom-labels-and-ground-truth-integration-question",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":68,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello!\n\nI've created a semantic segmentation (SS) job in Ground Truth (GT), and it shows all objects are labeled, and the status is Complete. The manifest file appears to be syntactically correct.\n\nWithin Custom Labels (CL), I've created a dataset by choosing \"Import images labeled by Ground Truth\" option and using the manifest file created in GT. It parses correctly. However, when I look at the available dataset in the console, I see that there are zero labeled images. Viewing the images also displays zero labels.\n\nI have done the same with bounding box (BB) jobs in GT, and they import into CL correctly.\n\nDo CL datasets not support import of SS GT manifest files? I can't find anything in the documentation that states either way.\n\nThanks for your help.",
        "Answers":[
            {
                "Answer_creation_date":"2020-07-14T16:18:36.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hey lkoivu\n\nThank you for using CustomLabels!\n\nUnfortunately we do not support semantic segmentation jobs. Currently we support 2 groundtruth formats - Classification Job Output & Bounding Box Job Output.\n\nYou can find the formats documented here:\n(a) Sagemaker GT - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-output.html\n- See section \"Classification Job Output\" for image level labels\n- See section \"Bounding Box Job Output\" for bounding box format\n(b) Custom Labels\n- Image level labels - https:\/\/docs.aws.amazon.com\/rekognition\/latest\/customlabels-dg\/cd-manifest-files-classification.html\n- Bounding boxes - https:\/\/docs.aws.amazon.com\/rekognition\/latest\/customlabels-dg\/cd-manifest-files-object-detection.html\n\nI have passed on feedback to the team to make this more clear in our documentation as well as on our console.\n\nWe do have roadmap items to expand our support to other ground truth formats.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Failed ping healthcheck after deploying TF2.1 model with TF-serving-contain",
        "Question_creation_time":1594297614000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUT86tGF5tRB2hC168_n5MAQ\/failed-ping-healthcheck-after-deploying-tf-2-1-model-with-tf-serving-contain",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":141,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello,\n\nI would appreciate any input to the following issue.\nWe want to deploy a trained Tensorflow Model to AWS Sagemaker for inference with a tensorflow-serving-container. Tensorflow version is 2.1. Following the guide at https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container the following steps have been taken:\n\nBuild TF 2.1 AMI and publish it to AWS ECR after sucessful local testing\nSetting Sagemaker Execution Role Permissions for S3 and ECR.\nPack saved TF model folder (saved_model.pb, assets, variables) into model.tar.gz\n4a. Created endpoint with realtime predictor:\nimport os\r\nimport sagemaker\r\nfrom sagemaker.tensorflow.serving import Model\r\nfrom sagemaker.tensorflow.model import TensorFlowModel\r\nfrom sagemaker.predictor import json_deserializer, json_serializer, RealTimePredictor\r\nfrom sagemaker.content_types import CONTENT_TYPE_JSON\r\n\r\ndef create_tfs_sagemaker_model():\r\n    sagemaker_session = sagemaker.Session()\r\n    role = 'arn:aws:iam::XXXXXXXXX:role\/service-role\/AmazonSageMaker-ExecutionRole-XXXXXXX\r\n    bucket = 'XXXXXXX-tf-serving'\r\n    prefix = 'sagemaker\/tfs-test'\r\n    s3_path = 's3:\/\/{}\/{}'.format(bucket, prefix)\r\n    image = 'XXXXXXXX.dkr.ecr.eu-central-1.amazonaws.com\/sagemaker-tensorflow-serving:2.1.0-cpu'\r\n    model_data = sagemaker_session.upload_data('model.tar.gz', bucket, os.path.join(prefix, 'model'))\r\n    endpoint_name = 'tf-serving-ep-test-1'\r\n    tensorflow_serving_model = Model(model_data=model_data, role=role, sagemaker_session=sagemaker_session, image=image, framework_version='2.1')\r\n    tensorflow_serving_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)\r\n    rt_predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sagemaker_session, serializer=json_serializer, content_type=CONTENT_TYPE_JSON, accept=CONTENT_TYPE_JSON)\n\n\n4b. Create batch-transform job:\n\ndef create_tfs_sagemaker_batch_transform():\r\n    sagemaker_session = sagemaker.Session()\r\n    print(sagemaker_session.boto_region_name)\r\n    role = 'arn:aws:iam::XXXXXXXXXXX:role\/service-role\/AmazonSageMaker-ExecutionRole-XXXXXXXX'\r\n    bucket = 'XXXXXXX-tf-serving'\r\n    prefix = 'sagemaker\/tfs-test'\r\n    image = 'XXXXXXXXXX.dkr.ecr.eu-central-1.amazonaws.com\/sagemaker-tensorflow-serving:2.1.0-cpu'\r\n    s3_path = 's3:\/\/{}\/{}'.format(bucket, prefix)\r\n    model_data = sagemaker_session.upload_data('model.tar.gz', bucket, os.path.join(prefix, 'model'))\r\n    tensorflow_serving_model = Model(model_data=model_data, role=role, sagemaker_session=sagemaker_session, image=image, name='deep-net-0', framework_version='2.1')\r\n    print(tensorflow_serving_model.model_data)\r\n    out_path = 's3:\/\/XXXXXX-serving-out\/'\r\n    input_path = \"s3:\/\/XXXXXX-serving-in\/\"    \r\n    tensorflow_serving_transformer = tensorflow_serving_model.transformer(instance_count=1, instance_type='ml.c4.xlarge', accept='application\/json', output_path=out_path)\r\n    tensorflow_serving_transformer.transform(input_path, content_type='application\/json')\n\n\nBoth steps 4a and 4b are running and in the AWS Cloudwatch logs we see successful starting of the instances, loading of the model and TF-Serving entering the event loop \u2013 see below:\n\n_2020-07-08T17:07:16.156_02:00+\nINFO:main:starting services\n_2020-07-08T17:07:16.156_02:00+\nINFO:main:nginx config:\n_2020-07-08T17:07:16.156_02:00+\nload_module modules\/ngx_http_js_module.so;\n_2020-07-08T17:07:16.156_02:00+\nworker_processes auto;\n_2020-07-08T17:07:16.156_02:00+\ndaemon off;\n_2020-07-08T17:07:16.156_02:00+\npid \/tmp\/nginx.pid;\n_2020-07-08T17:07:16.157_02:00+\nerror_log \/dev\/stderr error;\n_2020-07-08T17:07:16.157_02:00+\nworker_rlimit_nofile 4096;\n_2020-07-08T17:07:16.157_02:00+\nevents { worker_connections 2048;\n_2020-07-08T17:07:16.157_02:00+\n}\n_2020-07-08T17:07:16.162_02:00+\nhttp { include \/etc\/nginx\/mime.types; default_type application\/json; access_log \/dev\/stdout combined; js_include tensorflow-serving.js; upstream tfs_upstream { server localhost:10001; } upstream gunicorn_upstream { server unix:\/tmp\/gunicorn.sock fail_timeout=1; } server { listen 8080 deferred; client_max_body_size 0; client_body_buffer_size 100m; subrequest_output_buffer_size 100m; set $tfs_version 2.1; set $default_tfs_model None; location \/tfs { rewrite ^\/tfs\/(.*) \/$1 break; proxy_redirect off; proxy_pass_request_headers off; proxy_set_header Content-Type 'application\/json'; proxy_set_header Accept 'application\/json'; proxy_pass http:\/\/tfs_upstream; } location \/ping { js_content ping; } location \/invocations { js_content invocations; } location \/models { proxy_pass http:\/\/gunicorn_upstream\/models; } location \/ { return 404 '{\"error\": \"Not Found\"}'; } keepalive_timeout 3; }\n_2020-07-08T17:07:16.162_02:00+\n}\n_2020-07-08T17:07:16.162_02:00+\nINFO:tfs_utils:using default model name: mode\n_2020-07-08T17:07:16.162_02:00+\nINFO:tfs_utils:tensorflow serving model config:\n_2020-07-08T17:07:16.162_02:00+\nmodel_config_list: { config: { name: \"model\", base_path: \"\/opt\/ml\/model\", model_platform: \"tensorflow\" }\n_2020-07-08T17:07:16.162_02:00+\n}\n_2020-07-08T17:07:16.162_02:00+\nINFO:main:using default model name: model\n_2020-07-08T17:07:16.162_02:00+\nINFO:main:tensorflow serving model config:\n_2020-07-08T17:07:16.163_02:00+\nmodel_config_list: { config: { name: \"model\", base_path: \"\/opt\/ml\/model\", model_platform: \"tensorflow\" }\n_2020-07-08T17:07:16.163_02:00+\n}\n_2020-07-08T17:07:16.163_02:00+\nINFO:main:tensorflow version info:\n_2020-07-08T17:07:16.163_02:00+\n_TensorFlow ModelServer: 2.1.0-rc1_dev.sha.075ffcf+\n_2020-07-08T17:07:16.163_02:00+\nTensorFlow Library: 2.1.0\n_2020-07-08T17:07:16.163_02:00+\nINFO:main:tensorflow serving command: tensorflow_model_server --port=10000 --rest_api_port=10001 --model_config_file=\/sagemaker\/model-config.cfg --max_num_load_retries=0\n_2020-07-08T17:07:16.163_02:00+\nINFO:main:started tensorflow serving (pid: 13)\n_2020-07-08T17:07:16.163_02:00+\nINFO:main:nginx version info:\n_2020-07-08T17:07:16.163_02:00+\nnginx version: nginx\/1.18.0\n_2020-07-08T17:07:16.163_02:00+\nbuilt by gcc 7.4.0 (Ubuntu 7.4.0-1ubuntu1~18.04.1)\n_2020-07-08T17:07:16.163_02:00+\nbuilt with OpenSSL 1.1.1 11 Sep 2018\n_2020-07-08T17:07:16.163_02:00+\nTLS SNI support enabled\n_2020-07-08T17:07:16.163_02:00+\nconfigure arguments: --prefix=\/etc\/nginx --sbin-path=\/usr\/sbin\/nginx --modules-path=\/usr\/lib\/nginx\/modules --conf-path=\/etc\/nginx\/nginx.conf --error-log-path=\/var\/log\/nginx\/error.log --http-log-path=\/var\/log\/nginx\/access.log --pid-path=\/var\/run\/nginx.pid --lock-path=\/var\/run\/nginx.lock --http-client-body-temp-path=\/var\/cache\/nginx\/client_temp --http-proxy-temp-path=\/var\/cache\/nginx\/proxy_temp --http-fastcgi-temp-path=\/var\/cache\/nginx\/fastcgi_temp --http-uwsgi-temp-path=\/var\/cache\/nginx\/uwsgi_temp --http-scgi-temp-path=\/var\/cache\/nginx\/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fdebug-prefix-map=\/data\/builder\/debuild\/nginx-1.18.0\/debian\/debuild-base\/nginx-1.18.0=. -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'\n_2020-07-08T17:07:16.163_02:00+\nINFO:main:started nginx (pid: 15)\n_2020-07-08T17:07:16.163_02:00+\n2020-07-08 15:07:15.075708: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\n_2020-07-08T17:07:16.163_02:00+\n2020-07-08 15:07:15.075760: I tensorflow_serving\/model_servers\/server_core.cc:573] (Re-)adding model: model\n_2020-07-08T17:07:16.163_02:00+\n2020-07-08 15:07:15.180755: I tensorflow_serving\/util\/retrier.cc:46] Retrying of Reserving resources for servable: {name: model version: 1} exhausted max_num_retries: 0\n_2020-07-08T17:07:16.163_02:00+\n2020-07-08 15:07:15.180887: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: model version: 1}\n_2020-07-08T17:07:16.163_02:00+\n2020-07-08 15:07:15.180919: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: model version: 1}\n_2020-07-08T17:07:16.163_02:00+\n2020-07-08 15:07:15.180944: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: model version: 1}\n_2020-07-08T17:07:16.163_02:00+\n2020-07-08 15:07:15.180995: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/1\n_2020-07-08T17:07:16.163_02:00+\n2020-07-08 15:07:15.205712: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\n_2020-07-08T17:07:16.164_02:00+\n2020-07-08 15:07:15.205825: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:264] Reading SavedModel debug info (if present) from: \/opt\/ml\/model\/1\n_2020-07-08T17:07:16.164_02:00+\n2020-07-08 15:07:15.208599: I external\/org_tensorflow\/tensorflow\/core\/common_runtime\/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n_2020-07-08T17:07:16.164_02:00+\n2020-07-08 15:07:15.328057: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:203] Restoring SavedModel bundle.\n_2020-07-08T17:07:17.165_02:00+\n2020-07-08 15:07:16.578796: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:152] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/1\n_2020-07-08T17:07:17.165_02:00+\n2020-07-08 15:07:16.626494: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:333] SavedModel load for tags { serve }; Status: success: OK. Took 1445495 microseconds.\n_2020-07-08T17:07:17.165_02:00+\n2020-07-08 15:07:16.630443: I tensorflow_serving\/servables\/tensorflow\/saved_model_warmup.cc:105] No warmup data file found at \/opt\/ml\/model\/1\/assets.extra\/tf_serving_warmup_requests\n_2020-07-08T17:07:17.165_02:00+\n2020-07-08 15:07:16.632461: I tensorflow_serving\/util\/retrier.cc:46] Retrying of Loading servable: {name: model version: 1} exhausted max_num_retries: 0\n_2020-07-08T17:07:17.165_02:00+\n2020-07-08 15:07:16.632484: I tensorflow_serving\/core\/loader_harness.cc:87] Successfully loaded servable version {name: model version: 1}\n_2020-07-08T17:07:17.165_02:00+\n2020-07-08 15:07:16.634727: I tensorflow_serving\/model_servers\/server.cc:362] Running gRPC ModelServer at 0.0.0.0:10000 ...\n_2020-07-08T17:07:17.165_02:00+\nwarn getaddrinfo: address family for nodename not supported\n_2020-07-08T17:07:17.165_02:00+\n2020-07-08 15:07:16.635747: I tensorflow_serving\/model_servers\/server.cc:382] Exporting HTTP\/REST API at:localhost:10001 ...\n_2020-07-08T17:07:17.165_02:00+\nevhttp_server.cc : 238 NET_LOG: Entering the event loop \u2026\n\nBut both (endpoint and batch transform) fail the Sagemaker Ping Health check with:\n_2020-07-08T17:07:32.169_02:00+\n2020\/07\/08 15:07:31 error 16#16: *1 js: failed ping{ \"error\": \"Could not find any versions of model None\" }\n_2020-07-08T17:07:32.170_02:00+\n_169.254.255.130 - -08\/Jul\/2020:15:07:31 _0000 \"GET \/ping HTTP\/1.1\" 502 157 \"-\" \"Go-http-client\/1.1\"+\n\nAlso, when tested locally with self build tf-serving-container the model is running without problems and can be queried with curl.\nWhat could be the issue?\nThank you for your help.\nBest regards\n\nDominik\n\nEdited by: DominikPanther on Jul 9, 2020 5:27 AM",
        "Answers":[
            {
                "Answer_creation_date":"2020-07-16T10:35:45.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"The solution to the problem is as follows:\n\nEnvironmentvariable \"SAGEMAKER_TFS_DEFAULT_MODEL_NAME\" needs to be set to correct model name e.g. \"model\"\n\nimport os\r\nimport sagemaker\r\nfrom sagemaker.tensorflow.serving import Model\r\nfrom sagemaker.tensorflow.model import TensorFlowModel\r\nfrom sagemaker.predictor import json_deserializer, json_serializer, RealTimePredictor\r\nfrom sagemaker.content_types import CONTENT_TYPE_JSON\r\n\r\ndef create_tfs_sagemaker_model():\r\n    sagemaker_session = sagemaker.Session()\r\n    role = 'arn:aws:iam::XXXXXXXXX:role\/service-role\/AmazonSageMaker-ExecutionRole-XXXXXXX\r\n    bucket = 'tf-serving'\r\n    prefix = 'sagemaker\/tfs-test'\r\n    s3_path = 's3:\/\/{}\/{}'.format(bucket, prefix)\r\n    image = 'XXXXXXXX.dkr.ecr.eu-central-1.amazonaws.com\/sagemaker-tensorflow-serving:2.1.0-cpu'\r\n    model_data = sagemaker_session.upload_data('model.tar.gz', bucket, os.path.join(prefix, 'model'))\r\n    endpoint_name = 'tf-serving-ep-test-1'\r\n    env = {\"SAGEMAKER_TFS_DEFAULT_MODEL_NAME\": \"model\"}\r\n    tensorflow_serving_model = Model(model_data=model_data, role=role, sagemaker_session=sagemaker_session, image=image, name='model', framework_version='2.1', env=env)\r\n    tensorflow_serving_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)\r\n    rt_predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sagemaker_session, serializer=json_serializer, content_type=CONTENT_TYPE_JSON, accept=CONTENT_TYPE_JSON)\n\n\nThis creates the endpoint correctly and passes the ping health check with:\n\n2020-07-16T12:08:20.654_02:00 10.32.0.2 - -[16\/Jul\/2020:10:08:20 _0000 \"GET \/ping HTTP\/1.1\" 200 0 \"-\" \"AHC\/2.0\"",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to connect a Sagemaker Notebook to Glue Catalog",
        "Question_creation_time":1594231696000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUoiI3L85FT6OmPewooCH4lQ\/how-to-connect-a-sagemaker-notebook-to-glue-catalog",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Glue"
        ],
        "Question_upvote_count":0,
        "Question_view_count":519,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"A customer wants to connect a Sagemaker notebook to Glue Catalog, but is not allowed to use developer endpoints because of security constraints.\n\nI can't seem to find documentation on the Glue Catalog API that would allow this, or examples of how this might be done. Any links or pointers would be greatly appreciated.",
        "Answers":[
            {
                "Answer_creation_date":"2020-07-08T18:19:07.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"So there is the catalog API which allows you to describe databases, tables, etc. Documentation regarding the calls and data structures can be found here:\n\nhttps:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/aws-glue-api-catalog-tables.html\n\nBoto3 for get_table\n\nhttps:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/glue.html#Glue.Client.get_table\n\nIf they have a restrictive security posture (as suggested by the avoidance of Dev Endpoints) you may also suggest a Glue VPC-E: https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/vpce-interface.html\n\nI would ask what are they accessing the catalog for, as the Dev Endpoint isn't entirely about the Glue Catalog, but about the compute resources andSparkMagic.\n\nAlso, think about steering them towards AWS Data Wrangler for interacting with Glue Catalog if they are using Pandas. Helpful snippets can be found here:\n\nhttps:\/\/github.com\/awslabs\/aws-data-wrangler\/blob\/master\/tutorials\/005%20-%20Glue%20Catalog.ipynb",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"What is value and use case for Deep Learning AMI (DLAMI)?",
        "Question_creation_time":1594209626000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQInSlgeCS6mIe4DJv3KwnQ\/what-is-value-and-use-case-for-deep-learning-ami-dlami",
        "Question_topic":[
            "Machine Learning & AI",
            "Compute"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon EC2",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":77,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"What is value and use case for Deep Learning AMI (DLAMI)?\n\nIt seems that customers often pack ML dependencies at the docker level (themselves, or with DL containers or with SageMaker containers), instead of the AMI level. So what is the value and use-case of DL AMI ?",
        "Answers":[
            {
                "Answer_creation_date":"2020-07-08T13:58:25.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"The value of the DLAMI (https:\/\/docs.aws.amazon.com\/dlami\/latest\/devguide\/what-is-dlami.html) is ease of use and saving time to get up to speed in a development environment. If you are developing code for ML there is a huge variety of frameworks and software that you might need to install. The DLAMI includes the more popular ones, so you may quickly deploy a machine complete with common dependencies. This results in a reduction of the time needed for installing and configuring things. It speeds up experimentation and evaluation. If you want to try a new framework, it is already there.\n\nThe second reason is that AWS keeps the AMI up to date, so you may just deploy a new AMI periodically rather than having to patch. Again, this saves you time and lets you concentrate on the underlying development and business activities.\n\nAll that said, for running in production and at volume you might want to use a different tool, I would imagine that for most cases creating docker images to your specific requirements would make a lot of sense. No need to go over the good and bad points of containers here.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Amazon SageMaker Built-in algorithms and Spot checkpointing",
        "Question_creation_time":1593596016000,
        "Question_link":"https:\/\/repost.aws\/questions\/QURbWeXcwDT8i4dvXKE4HZXg\/amazon-sage-maker-built-in-algorithms-and-spot-checkpointing",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":32,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Which Amazon SageMaker built-in algorithms support checkpointing? In the documentation it says that:\n\nSageMaker built-in algorithms and marketplace algorithms that do not checkpoint are currently limited to a MaxWaitTimeInSeconds of 3600 seconds (60 minutes).\n\nHowever, in the algorithms I don't find any pointer to \"checkpoint\" or \"spot\". Can you help me out?",
        "Answers":[
            {
                "Answer_creation_date":"2020-07-01T13:49:48.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"This is the best resource that I've found to clarify this:\n\nhttps:\/\/aws.amazon.com\/blogs\/aws\/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs\/\n\nBuilt-in algorithms: computer vision algorithms support checkpointing (Object Detection, Semantic Segmentation, and very soon Image Classification). As they tend to train on large data sets and run for longer than other algorithms, they have a higher likelihood of being interrupted. Other built-in algorithms do not support checkpointing for now.\n\nAlso:\n\nPlease note that TensorFlow uses checkpoints by default. For other frameworks, you\u2019ll find examples in our sample notebooks and in the documentation.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Is there a way to automate failure handling and retries when using Amazon SageMaker batch transform?",
        "Question_creation_time":1593595381000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE10OtSwDRCiB-0pP6wflYQ\/is-there-a-way-to-automate-failure-handling-and-retries-when-using-amazon-sage-maker-batch-transform",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":120,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"How does Amazon SageMaker batch transform handle failures? Is there a way to automate failure handling and retries built into the service?",
        "Answers":[
            {
                "Answer_creation_date":"2020-07-01T15:34:51.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can use the ModelClientConfig API to configure the timeout and maximum number of retries for processing a transform job invocation. The maximum number of automated retries is three.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Does Amazon SageMaker RL support heterogenous clusters?",
        "Question_creation_time":1593179712000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE59_Oro0SGKaOIdZNmySiw\/does-amazon-sage-maker-rl-support-heterogenous-clusters",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":27,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Does SageMaker RL support heterogenous clusters? I'd like to have our training to run on GPU and and SageMaker RL, and our inferences to run on CPUs.",
        "Answers":[
            {
                "Answer_creation_date":"2020-06-28T20:35:03.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yes, Amazon SageMaker RL allows you to define training workers separately from inference workers.\n\nFor more information, see Amazon SageMaker RL \u2013 Managed reinforcement learning with Amazon SageMaker on the AWS News Blog. Also, the Build and Train Reinforcement Models with Amazon SageMaker RL AWS online tech talk (see minute 26).",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Notebook Instance Types for SageMaker Studio",
        "Question_creation_time":1593107198000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOd5vfn4FRjGvGjac4d00PQ\/notebook-instance-types-for-sage-maker-studio",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":343,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Within SageMaker Studio, you can change instance types (see screenshots here:https:\/\/aws.amazon.com\/blogs\/machine-learning\/learn-how-to-select-ml-instances-on-the-fly-in-amazon-sagemaker-studio\/). However, this seems to only support changing to: ml.t3.medium, ml.g4dn.xlarge, ml.m5.large, and ml.c5.large.\n\nIs there a way to change to other instance types for SageMaker Studio? For SageMaker Notebook Instances, I know you can change to many other types of instances, but I am not sure how to do it for SageMaker Studio.",
        "Answers":[
            {
                "Answer_creation_date":"2020-06-25T18:02:17.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"The instance types you are seeing are Fast Launch Instances ( which are instance types designed to launch in under two minutes).\n\nIn order to see all the types of instances, click on the switch on top of the instance type list that says \"Fast Launch\", that should display the rest of available instances.\n\nHere is additional info about fast launch instances: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks.html\n\nHope it helps!",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Is there a solution for multi-user Notebook on SageMaker?",
        "Question_creation_time":1592989945000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4mxRvXy2QYmkYCvdqNVa2g\/is-there-a-solution-for-multi-user-notebook-on-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":457,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Is there a solution for multi-user Notebook on Studio Notebook or Notebook Instances? Eg if we want several developers to interact on the same notebook at the same time",
        "Answers":[
            {
                "Answer_creation_date":"2020-06-24T14:42:53.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Notebook instances are not connected to the user. So if two users has the same access rights they will see and will be able to access the same instance (even in the same time).\n\nThe issue is - Jupyter Notebook is not ready for that, both users will have the same privileges, no tracking who did what, ... And working on the same notebook on the same time - basically they will overwrite each other saves.\n\nI had a need for similar thing (pair programming - data scientist and software engineer) - the only viable solution we were able to find was desktop sharing (like TeamViewer, ...)",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"How to install Phyton package in Jupyter Notebook instance in SageMaker?",
        "Question_creation_time":1592823369000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4pvReJNZS6eDLxhd4pK-tQ\/how-to-install-phyton-package-in-jupyter-notebook-instance-in-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":636,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nI want to use awswrangler package in my Jupyter Notebook instance of SageMaker.\n\nI understand that we have to use Lifecycle configuration. I tried to do it using the following script:\n\n#!\/bin\/bash\n\npip install awswrangler==0.2.2\n\n\nBut when I import that package into my Notebook:\n\nimport boto3                                      # For executing native S3 APIs\nimport pandas as pd                               # For munging tabulara data\nimport numpy as np                                # For doing some calculation\nimport awswrangler as wr\nimport io\nfrom io import StringIO\n\n\nI still get the following error:\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n<ipython-input-1-f3d85c7dd0f6> in <module>()\n      2 import pandas as pd                               # For munging tabulara data\n      3 import numpy as np                                # For doing some calculation\n----> 4 import awswrangler as wr\n      5 import io\n      6 from io import StringIO\n\nModuleNotFoundError: No module named 'awswrangler'\n\n\nAny documentation or reference on how to install certain package for Jupyter Notebook in SageMaker?",
        "Answers":[
            {
                "Answer_creation_date":"2020-06-22T13:32:04.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nexample how to use lifecycle config to install python package in one environment : https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-pip-package-single-environment\/on-start.sh\n\nand to all conda env - https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-pip-package-all-environments\/on-start.sh",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Can you share success stories of AWS customers performing ML CI\/CD?",
        "Question_creation_time":1592577511000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwLq6HNRZSOK7ODKKc_lC3Q\/can-you-share-success-stories-of-aws-customers-performing-ml-ci-cd",
        "Question_topic":[
            "Developer Tools",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_tag":[
            "AWS CodePipeline",
            "Amazon SageMaker",
            "Machine Learning & AI",
            "DevOps"
        ],
        "Question_upvote_count":0,
        "Question_view_count":32,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I want to create simple templates for scientists so that they can fit their models easily into a continuous integration\/continuous delivery (CI\/CD) pipeline. I want to know about success stories of AWS customers performing CI\/CD on machine learning pipelines.",
        "Answers":[
            {
                "Answer_creation_date":"2020-06-19T15:15:42.000Z",
                "Answer_upvote_count":1,
                "Answer_body":"Amazon has released the [Amazon SageMaker Pipelines][1] that are the first purpose-built CI\/CD service for machine learning: [1]: https:\/\/aws.amazon.com\/sagemaker\/pipelines\/\n\nFor more information, see [New \u2013 Amazon SageMaker Pipelines brings DevOps capabilities to your machine learning projects] [2] [2]: https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-pipelines-brings-devops-to-machine-learning-projects\/\n\nAdditionally, we have a case-study where a customer created one on their own for model development using Airflow. For more information, see [NerdWallet uses machine learning on AWS to power recommendations platform][3] and [Using Amazon SageMaker to build a machine learning platform with just three engineers][4]. [3]: https:\/\/aws.amazon.com\/solutions\/case-studies\/nerdwallet-case-study\/ [4]: https:\/\/www.nerdwallet.com\/blog\/engineering\/machine-learning-platform-amazon-sagemaker\/",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Custom packages in Sagemaker studio",
        "Question_creation_time":1592469976000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZZFjMw_gS5Cz8sh-TK4J3w\/custom-packages-in-sagemaker-studio",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":266,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi everyone,\n\nhow can i install custom OS libraries on Sagemaker studio? When I open a terminal it states:\n\nroot@0f04278e59cf:~\/# yum install unzip\n\nbash: yum: command not found\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_date":"2020-06-18T09:18:20.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Short answer: [Studio UI] > File > New > Terminal > sudo yum install unzip\nThen unzip away...\n\nLong answer:\nYou can open a terminal in two different types of compute environment:\n\nOn a specific kernel you're running: [Studio UI] > kernal tab > Terminial icon.\nOn the compute studio (jupyter) itself: [Studio UI] > File > New > Terminal\n\nIn both options your personal files folder is accessible. In a kernel terminal: \/root. In a Jupyter terminal: \/home\/sagemaker-user.\nWhen opening a kernel terminal you'll have access to the software that is part of the kernel's container (say tensorflow container). Which in your case is missing yum. You can of course try apt-get, and such to install more tools.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Model Spend",
        "Question_creation_time":1592312639000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlNS8ujYmQqePwWS-mgso3Q\/sage-maker-model-spend",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":71,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"If I deploy a SageMaker model, am I incurring hosting charges even while no one is accessing my model?",
        "Answers":[
            {
                "Answer_creation_date":"2020-06-16T13:24:24.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"When you deploy a SageMaker model, it deploys it behind a SageMaker endpoint for real-time inference. You are charged by the second for on-demand ML hosting. Check the model deployment section of each region on the SageMaker Pricing page. In some use cases, you can save on inference cost by hosting several models behind the same endpoint (check this blog post).",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Provide Autopilot own data",
        "Question_creation_time":1591879538000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfEXvtHQmR0egOGgOSG-b0A\/provide-autopilot-own-data",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":25,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Is there any way to provide your own (already labeled) data to sagemaker ground truth?\nMy plan is to use sagemaker autopilot with my own data that I have already labeled such that the folder structure represents the labels (rekognition provides such functionality).\nThe problem is that AutoPilot can only accept manifest files or csv,txt, ect. Well I have images with labels, so my only option is to use a manifest file. HOWEVER GroundTruth create manifest file option only accepts images from only 1 folder without any option to give it labels by default, therefore forcing me to repeat the labeling on an already labeled dataset.\n\nAny idea how to use already labeled data as input???",
        "Answers":[
            {
                "Answer_creation_date":"2020-06-13T11:35:14.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello... How are your images labeled? What format are the labels in?\n\nIf you have multiple folders each containing images, and the folder is the label then you could create a quick python (etc) script that will create a manifest file, using the folders as labels. You can then upload all the folders and the manifest file to S3 and your set.\n\nGroundtruth is a labeling pipeline tool. Its designed to organize people with domain knowledge to label data. Also its pretty pricey so you definitely don't want to use it to re-label anything thats already labeled.\n\nHope that helps.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Code running slow on Sagemaker notebook instance for the first time it runs",
        "Question_creation_time":1591811233000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeQJN4BFCTsikAct_m9BeZw\/code-running-slow-on-sagemaker-notebook-instance-for-the-first-time-it-runs",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":615,
        "Question_answer_count":7,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello!\n\nI've an issue running the code on SageMaker. I am running my code on SageMaker, which runs my code slowly for the first time, but runs with proper speed, the second time around (I guess there's something getting stored in the cache). Few days back, it was running with the same speed all the time. Whatever I run, be it a model (The model which took just 5 minutes for one epoch when it worked fine estimates 3 hours of running time) \/ just a code reading the data present in my files, it runs too slow. What could be a possible solution for this? I tried changing the notebook instance types as well, but in vain. I've been struggling for 2 days. It'll be great if someone could help me out a bit soon so that I progress ahead in my project. Thanks in advance!\n\nEdited by: vbsrinivasan on Jun 10, 2020 10:47 AM",
        "Answers":[
            {
                "Answer_creation_date":"2020-06-26T10:12:46.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Can confirm the speed issues. Migrated yesterday to Sagemaker and code runs very slowly the first time, the second time is way faster but is slowing down again mid training. With the same code and training data, training on a K80 is way slower than on Colabs K80.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2020-08-18T10:07:41.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yeah, you are right. Also, there's one update. Like, whenever I turn off the notebook instance and turn on once again, the code runs very very slowly. But, after running the code once, if I don't turn off the notebook, it runs faster. But, this incurs a lot of cost for me. It would be great if someone from AWS or an expert responds to this!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2020-06-13T23:33:54.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Can you confirm if this is the notebook taking time to spin up the kernel, then load the libraries (at the start of your script presumably) or if this is all cells taking longer to run?\n\nEdited by: MikeChambers on Jun 13, 2020 4:20 AM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2020-08-18T04:56:07.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello!\n\nThe notebook is taking time to run every cell. Not just libraries and stuff.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2020-06-11T11:25:54.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Exact same experience here. Seeing substantial variation in runtimes between instance restarts. Running the exact same code can take up to a factor of 3 longer (regardless of whether this is just I\/O, model training or something else entirely). I had originally attributed this to slow EBS I\/O (which by experience has been patchy in the past) but doesn't seem to be related. Real showstopper for sagemaker at this point.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2020-06-13T11:20:32.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Having the same problem too. But for me it is mainly disk I\/O. So every time I stop and restart the notebook instance, I need to re-download the data even though they are sitting right there on disk, because if I don't, then it takes a insane amount of time to load the data (even slower than re-download the data and load them). Quite annoying but have not idea how to fix it.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2020-06-12T17:25:56.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yeah, I've narrowed it down to disk I\/O. Extremely slow on first read -- as if the files aren't on the EBS volume but downloaded from elsewhere. Moving away from Sagemaker NBs now for interactive work",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Glue + SageMaker Pip Packages",
        "Question_creation_time":1591210245000,
        "Question_link":"https:\/\/repost.aws\/questions\/QULN3fro-LQ1umpDN831VlZg\/glue-sage-maker-pip-packages",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Glue"
        ],
        "Question_upvote_count":0,
        "Question_view_count":89,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"My customer is looking to use Glue dev endpoints along with a SageMaker notebook. What I've noticed is that in Glue, a package, in this case scipy, will be listed as 1.4.1, but this will or won't match what you get in a sagemaker notebook dependent on kernel.\n\nconda_python3:\n\n!pip show scipy\nName: scipy\nVersion: 1.1.0\nSummary: SciPy: Scientific Library for Python\nHome-page: https:\/\/www.scipy.org\nAuthor: None\nAuthor-email: None\nLicense: BSD\nLocation: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\nRequires: \nRequired-by: seaborn, scikit-learn, sagemaker\n\n\nconda_tensorflow_p36:\n\n!pip show scipy\nName: scipy\nVersion: 1.4.1\nSummary: SciPy: Scientific Library for Python\nHome-page: https:\/\/www.scipy.org\nAuthor: None\nAuthor-email: None\nLicense: BSD\nLocation: \/home\/ec2-user\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\nRequires: numpy\nRequired-by: seaborn, scikit-learn, sagemaker, Keras\n\n\nIs there some sort of best practice to use a kernel that corresponds directly to what's installed on Glue?\n\nSeparate not very useful question. I wasn't able activate the venv that Jupyter notebooks do via shell. Is it using a venv? How come I can't find the right activate script?",
        "Answers":[
            {
                "Answer_creation_date":"2020-06-03T20:20:18.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"conda_python3 and conda_tensorflow_p36 are local kernels on the SageMaker notebook instance while the Spark kernels execute remotely in the Glue Spark environment.\n\nHence you are seeing different versions. The Glue Spark environment comes with 1.4.1 version of scipy. So when you use the PySpark (python) or Spark (scala) kernels and you will get the 1.4.1 version of scipy.\n\nIf you use the default LifeCycle script that Glue SageMaker notebooks already come with, the connectivity to the Glue Dev endpoint should be in place. Note that the Glue SageMaker notebooks has a tag called 'aws-glue-dev-endpoint' that is used to identify which Glue Dev endpoint that particular notebook instance communicates with.\n\nThe Spark kernels cannot be replicated via the python shell. Those kernels relay Spark commands via the Livy service to Spark on the Glue Dev endpoint using a Jupyter module called Sparkmagic.\n\nRef: https:\/\/github.com\/jupyter-incubator\/sparkmagic",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"SparkR not working",
        "Question_creation_time":1591029652000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqyiKb_XvRhGxm1RxwjXhJQ\/spark-r-not-working",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Amazon Elastic MapReduce"
        ],
        "Question_upvote_count":0,
        "Question_view_count":44,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I am trying to control a Spark cluster (using SparkR) from a Sagemaker notebook. I followed these instructions closely: https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/ and got it to work.\n\nToday when I try to run the SageMaker notebook (using the exact same code as before) I inexplicably get this error:\n\nAn error was encountered:\n[1] \"Error in callJMethod(sparkSession, \\\"read\\\"): Invalid jobj 1. If SparkR was restarted, Spark operations need to be re-executed.\"\n\n\nDoes anyone know why this is? I terminated the SparkR kernel and am still getting this error.",
        "Answers":[
            {
                "Answer_creation_date":"2020-06-01T19:56:15.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"You cannot have multiple SparkContexts in one JVM. The issue is resolved as WON'T FIX. You have to stop the spark session which spawned the sparkcontext (which you have already done).\n\nsparkR.session.stop()\n\nhttps:\/\/issues.apache.org\/jira\/browse\/SPARK-2243",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Running concurrent sessions from SageMaker notebooks on Glue Dev Endpoints.",
        "Question_creation_time":1591020062000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIDitlJMgTlGai61w_Zvqdg\/running-concurrent-sessions-from-sage-maker-notebooks-on-glue-dev-endpoints",
        "Question_topic":[
            "Machine Learning & AI",
            "Analytics",
            "Database"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "AWS Glue",
            "Extract Transform & Load Data"
        ],
        "Question_upvote_count":0,
        "Question_view_count":157,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Customer who has created a AWS glue dev endpoint and want to run two Sagemaker notebooks in parallel on same single Dev endpoint but its not working .\n\nThe one which is invoked first is only able to run the job, while another one fails. what could be possible reasons and fix for it?",
        "Answers":[
            {
                "Answer_creation_date":"2020-06-01T16:52:06.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"SageMaker notebooks are Jupyter notebooks that uses the SparkMagic module to connect to a local Livy setup. The local Livy does an SSH tunnel to Livy service on the Glue Spark server. Apache Livy binds to post 8998 and is a RESTful service that can relay multiple Spark session commands at the same time so multiple port binding conflicts cannot happen. So yes, you can have multiple sessions as long as the backend cluster has resources to serve that many sessions.\n\nYou can run the following command in a notebook to check the defaults for Spark sessions:\n\nspark.sparkContext.getConf().getAll()\n\n\nI see the following defaults in my Spark session. You can easily override them from the config file at ~\/.sparkmagic\/config.json or by using the %%configure magic from within the notebook.\n\nspark.executor.cores 4\nspark.executor.memory 5g\nspark.driver.memory 5g\n\n\nNote that spark.executor.instances is not set and spark.dynamicAllocation.enabled is not overridden which means that it is true, so if you have a demanding Spark job in one notebook, it can take over all resources in the cluster and prevent other Spark sessions from starting. The recommendation when sharing a single Glue Dev endpoint is to limit each session to a few executors so that multiple sessions can acquire resources from the cluster e.g.:\n\n%%configure -f\n{\"executorMemory\": \"5G\", \"executorCores\":4,\"numExecutors\":2}\n\n\n(Note: Tested on multiple SageMaker PySpark notebooks in single SageMaker notebook instances as well as multiple SageMaker notebook instances.)",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Does SageMaker Multi-Model Endpoint support SageMaker Model Monitor?",
        "Question_creation_time":1590501108000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq2z-BEt7TnmZ8vFYs-Hu7g\/does-sage-maker-multi-model-endpoint-support-sage-maker-model-monitor",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":130,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Does SageMaker Multi-Model Endpoint support SageMaker Model Monitor?",
        "Answers":[
            {
                "Answer_creation_date":"2020-05-26T13:58:28.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Amazon SageMaker Model Monitor currently supports only endpoints that host a single model and does not support monitoring multi-model endpoints. For information on using multi-model endpoints, see Host Multiple Models with Multi-Model Endpoints . https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor.html",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Pipe Mode",
        "Question_creation_time":1590161458000,
        "Question_link":"https:\/\/repost.aws\/questions\/QURbsBp9m5TsqKWWDdP8VJyw\/sage-maker-pipe-mode",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":33,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Does SageMaker pipe mode serve as a cost saving measure? Or is is just faster than file mode but generally not much cheaper? The cost savings of it might be 1. no need to copy data to training instances and 2. training instances need less space. Are these savings generally significant for customers?",
        "Answers":[
            {
                "Answer_creation_date":"2020-05-22T16:44:47.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"To the best of my understanding, pipe mode decreases startup times, but frequently increases the bill.\n\nThe SageMaker billing starts after the data has been copied onto the container in File mode and control is transferred to the user script.\n\nReading the data in pipe mode starts after control is transferred, so the data transfer happens during the billable time.\n\nFurther the data is, to the best of my knowledge, not hitting the disk (EBS). This is fast, but also means that if you pass over your data multiple times, you have to re-read it again, on your dime (S3 requests and container wait times).\n\nPipe mode is still a good idea. For example if you have only few passes over the data and the data is rather large, so that it would not fit on an EBS volume.\n\nAlso, in PyTorch for example, data loading can happen in parallel. So while the GPU is chucking away on one batch, the CPUs load and prepare the data for the next batch.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"confusion about PIPE mode when using S3 shard key",
        "Question_creation_time":1589363811000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU31DdUqtuQziixKTkPasZKw\/confusion-about-pipe-mode-when-using-s-3-shard-key",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":25,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nI am a little confused about whether S3 Shard key would work when using PIPE mode, here is a example:\n\nAssume I have:\n\n2 instance, each instance have 4 worker;\n\ndata: total 8 files with total size 8GB, each file is 1GB. Put them into 4 different S3 path, that means, each path has 2 files (2GB in total)\n\nIf I use PIPE mode, and s3_input using distribution='ShardedByS3Key', and create 4 channel (each channel mapping a s3 path, 2 files)\n\ntrain_s3_input_1 = sagemaker.inputs.s3_input(channel_1, distribution='ShardedByS3Key')\n\nQuestion:\n\nHow much data of each worker get to train, 1 file or 2 files? thanks",
        "Answers":[
            {
                "Answer_creation_date":"2020-05-13T22:11:20.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi, SageMaker will replicate a subset of data (1\/n ML compute instances) on each ML compute instance that is launched for model training when you specify ShardedByS3Key. If there are n ML compute instances launched for a training job, each instance gets approximately 1\/n of the number of S3 objects. This applies in both File and Pipe modes. Keep this in mind when developing algorithms.\n\nTo answer your question: How much data of each worker get to train, 1 file or 2 files? 1 file each from the training channel.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"tensorboard with custom docker image without notebook",
        "Question_creation_time":1588924703000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdIJGIVDEQSm-9eX3jo0ubA\/tensorboard-with-custom-docker-image-without-notebook",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":61,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\nIs it possible to use tensorboard with a custom docker image without using a notebook ? Is there any other method to monitor the training process ? I'm using the tensorflow object detection API and currently exposing metrics (only loss) from cloudwatch using a regex but I'd like a more detailed way like tensorboar.. is that possible ??\nThanks",
        "Answers":[
            {
                "Answer_creation_date":"2020-05-11T21:19:16.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yes, it is possible to use tensorboard outside of the SageMaker notebooks.\nHere is an example https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/keras_script_mode_pipe_mode_horovod\/tensorflow_keras_CIFAR10.ipynb that uses TensorBoard to compare the training jobs.\nAll of it can be run locally.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2021-01-25T10:26:28.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"For anyone interested I made a tutorial for this:\n\nhttps:\/\/github.com\/roccopietrini\/TFSagemakerDetection\n\nEdited by: rokk07 on Jan 25, 2021 2:28 AM",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Training a classifier on parquet with SageMaker ?",
        "Question_creation_time":1588841008000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCqvDUq4hSQqRT97tBUvE8Q\/training-a-classifier-on-parquet-with-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":188,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nWhat parquet data loading logic is known to work well to train with SageMaker on parquet? ml-io? pyarrow? any examples? That would be to train a classifier, either logistic regression, XGBoost or custom TF.",
        "Answers":[
            {
                "Answer_creation_date":"2020-05-07T09:21:42.000Z",
                "Answer_upvote_count":1,
                "Answer_body":"XGBoost as a framework container (v0.90+) can read parquet for training (see example notebook).\nThe full list of valid content types are CSV, LIBSVM, PARQUET, RECORDIO_PROTOBUF (see source)\n\nAdditionally:\nUber Petastorm for reading parquet into Tensorflow, Pytorch, and PySpark inputs.\nAs XGBoost accepts numpy, you can convert from PySpark to numpy\/pandas using the mentioned PyArrow.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"SageMaker with multiple models",
        "Question_creation_time":1587366119000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfmnWJIIZQs6_2K1uIH9stQ\/sage-maker-with-multiple-models",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":254,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Customer wants to host multiple DNN models on same SageMaker container due to latency concerns. Customer does not want to spin-up different containers for each model due to network adding additional latency. Thus, my customer asked me a question below -\n\nCan one SageMaker host more than one model? Each model then share the same input and produce different outputs concatenated together?\n\nI answered as below -\n\nYes. Amazon SageMaker supports you hosting multiple models in several different ways \u2013\n\nUsing Multi-model Inference endpoints: Amazon SageMaker supports serving multiple models from same Inference endpoint. Details can be found here. The sample code can be found here. Currently, this feature do not support Elastic Inference or serial inference pipelines. Multi-model endpoints also enable time-sharing of memory resources across your models. This works best when the models are fairly similar in size and invocation latency. When this is the case, multi-model endpoints can effectively use instances across all models. If you have models that have significantly higher transactions per second (TPS) or latency requirements, we recommend hosting them on dedicated endpoints. Multi-model endpoints are also well suited to scenarios that can tolerate occasional cold-start-related latency penalties that occur when invoking infrequently used models\n\nUsing Bring your own algorithm on SageMaker You can also bring your own container with your own libs and runtime\/programming language for serving and training. See the example notebook on how you can bring your own algorithm\/container image on sagemaker here\n\nUsing Multi-model serving container by using multi-model archive file You can find a sample example here [4] for tensorflow serving\n\nIf models are called sequentially, the SageMaker inference pipeline allows you to chain up to 5 models called one after the other on the same endpoint Sagemaker endpoints include optimizations that will save costs, such as (1) 1-click deploy to pre-configured environments for popular ML frameworks with a managed serving stack, (2) autoscaling, (3) model compilation, (4) cost-effective hardware acceleration via Elastic Inference, (5) multi-variant model deployment for testing and overlapped model replacement, (6) multi-AZ backend. It is not necessarily a good idea to have multiple models on same endpoint (unless you have the reasons and requirements I mentioned in Option A above). Having one model per endpoint creates an isolation which has positive benefits on fault tolerance, security and scalability. Please keep in mind that SageMaker works on containers that runs on top of EC2.\n\n[1]https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\n\n[2]https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\n\n[3]https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\n\n[4]https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst#deploying-more-than-one-model-to-your-endpoint\n\n[5]https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\n\nAm I missing anything? Any other suggestions in terms of other approaches?",
        "Answers":[
            {
                "Answer_creation_date":"2020-07-02T08:12:08.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Customer does not want to spin-up different containers for each model due to network adding additional latency.\n\nI am assuming this is a pipeline scenario where different models need to be chained. If so, it's important to keep in mind that all containers in pipeline run on the same EC2 instance so that \"inferences run with low latency because the containers are co-located on the same EC2 instances.\"[1]\n\nHope this is useful.\n[1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Training Job Failed \"ClientError: Mask image is not a 8-bit\"",
        "Question_creation_time":1587051280000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUkxj6OqkTRFaImGZYRl-ISA\/training-job-failed-client-error-mask-image-is-not-a-8-bit",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":52,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello,\n\nI am working on a Semantic Segmentation training job. I have used the Ground Truth semantic segmentation labeling GUI and have labeled about 900 images. Everything with the Ground Truth Labeling job seemed to work just fine. However I noticed the output images in the s3 output path are of type .png.\n\nI am now trying to build a training job using the Pipe - augmented manifest input format. When I kick off the job, after a few minutes I see this error: \"ClientError: Mask image is not a 8-bit single channel image. Please check the label image dataset\"\n\nI downloaded the output images or \"masked images\" to photoshop and they do appear to be 8bit \"index color\".\n\nIf I used the built in Semantic Segmentation - Ground Truth Labeling job, how would I end up with the incorrect output format?\nI don't remember specifying or having the option of output image format. Did I miss that somewhere?\n\nHere are a couple line items from the output.manifest file\n\n{\"source-ref\":\"s3:\/\/rigbypotato-2\/segmentation-validation\/MyImage - 20190201122850904.png\",\"potato-features-validation-ref\":\"s3:\/\/rigbypotato-2\/segmentation-validation\/output\/potato-features-validation\/annotations\/consolidated-annotation\/output\/0_2020-04-14T02:38:26.487862.png\",\"potato-features-validation-ref-metadata\":{\"internal-color-map\":{\"0\":{\"class-name\":\"BACKGROUND\",\"hex-color\":\"#ffffff\",\"confidence\":0.79952},\"1\":{\"class-name\":\"pressure-bruise\",\"hex-color\":\"#2ca02c\",\"confidence\":0.79952},\"2\":{\"class-name\":\"potato\",\"hex-color\":\"#1f77b4\",\"confidence\":0.79952},\"3\":{\"class-name\":\"old-bruise\",\"hex-color\":\"#ff7f0e\",\"confidence\":0.79952},\"4\":{\"class-name\":\"black-skin\",\"hex-color\":\"#d62728\",\"confidence\":0.79952},\"5\":{\"class-name\":\"green\",\"hex-color\":\"#9467bd\",\"confidence\":0.79952},\"6\":{\"class-name\":\"scab\",\"hex-color\":\"#8c564b\",\"confidence\":0.79952}},\"type\":\"groundtruth\/semantic-segmentation\",\"human-annotated\":\"yes\",\"creation-date\":\"2020-04-14T02:38:26.563488\",\"job-name\":\"labeling-job\/potato-features-validation\"}}\r\n{\"source-ref\":\"s3:\/\/rigbypotato-2\/segmentation-validation\/MyImage - 20190201122852921.png\",\"potato-features-validation-ref\":\"s3:\/\/rigbypotato-2\/segmentation-validation\/output\/potato-features-validation\/annotations\/consolidated-annotation\/output\/1_2020-04-14T02:21:27.433381.png\",\"potato-features-validation-ref-metadata\":{\"internal-color-map\":{\"0\":{\"class-name\":\"BACKGROUND\",\"hex-color\":\"#ffffff\",\"confidence\":0.82409},\"1\":{\"class-name\":\"pressure-bruise\",\"hex-color\":\"#2ca02c\",\"confidence\":0.82409},\"2\":{\"class-name\":\"potato\",\"hex-color\":\"#1f77b4\",\"confidence\":0.82409},\"3\":{\"class-name\":\"old-bruise\",\"hex-color\":\"#ff7f0e\",\"confidence\":0.82409},\"4\":{\"class-name\":\"black-skin\",\"hex-color\":\"#d62728\",\"confidence\":0.82409},\"5\":{\"class-name\":\"green\",\"hex-color\":\"#9467bd\",\"confidence\":0.82409},\"6\":{\"class-name\":\"scab\",\"hex-color\":\"#8c564b\",\"confidence\":0.82409}},\"type\":\"groundtruth\/semantic-segmentation\",\"human-annotated\":\"yes\",\"creation-date\":\"2020-04-14T02:21:30.556262\",\"job-name\":\"labeling-job\/potato-features-validation\"}}\n\n\nThanks\n\nEdited by: tetontech on Apr 16, 2020 7:07 PM",
        "Answers":[
            {
                "Answer_creation_date":"2020-04-29T15:11:39.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Total user error.\n\nMy problem was that I was only giving the semantic segmentation algorithm 1 attribute from my output.manifest file which was \"source-ref\". This algorithm of course needs 2 attributes from the the output.manifest file which are the source images and masked images. Once I provided the second attribute \"potato-features-validation-ref\", everything worked fine.\n\nFrom the few lines of the output.manifest file I posted earlier, you will notice these two attributes.\n\nhope this helps someone else.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Model Monitor Missing Columns Constraint Violation",
        "Question_creation_time":1586974280000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU8Xkelo1ARA2zcn4rHuk09w\/sage-maker-model-monitor-missing-columns-constraint-violation",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":150,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I have an Endpoint inference pipeline model deployed from an AutoPilot training job. Now that this is successful, I want to add model monitor. I have a script for online validation of the endpoint, and the F1 score is ~99%. This indicates that the endpoint interprets the call correctly.\n\nModel Monitor is recognizing the data in my jsonl files as the data not being CSV formatted. When my Model Monitor processing job runs, I receive the following constraint violation: \"There are missing columns in current dataset. Number of columns in current dataset: 1, Number of columns in baseline constraints: 225\".\n\nGiven the results from the Endpoint and this Model Monitor constraint violation, I perceive there is a conflict between how the Endpoint is storing the data and how the Model Monitor Processing Job wants to consume the data.\n\nHere is one sample prediction from the jsonl file. The data value is comma separated.\n\n{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text\/csv\",\"mode\":\"INPUT\",\"data\":\"JHB,44443000.0,-0.0334,,44264000.0,,,,-2014000.0,,-2014000.0,,,,,,,-0.04,-0.04,55872000.0,,,0.996,,,,,,,,-0.0453,,2845000.0,,2845000.0,11636000.0,,,,,,,,,,,,190000000.0,,,,,,,,-18718000.0,,,,,,,,29000000.0,,,,,,,,-33000000.0,,-4000000.0,,,,,,,,,,,,,,,0.0,,,0.995972369102,1.0,-0.045316472785366,0.0,,,,,,,0.0,,,,,,,,,95.5638,,,,,,1.0,1.0,,0.15263157894737,,,,,,0.65252120693923,0.0,0.15263157894737,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18606500.0,,,95.5638,,,2.3886,,,,,-0.0326,,-1.0449,,-1.05,-1.05,,0.0,,-0.1471,,,,,,,,,,,,,,,,,-0.5451,,,,,,,Financial Services,16.67890010036862\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text\/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"1\\n\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"c97df615-0a2e-414d-9be3-bf3a14eb6363\",\"inferenceTime\":\"2020-04-15T16:26:46Z\"},\"eventVersion\":\"0\"}\n\n\nHere is the point within the log that the processing job recognizes a column mismatch. I see that it pulls down the data to store locally, pulls down the statistics and constraints files, errors with this constraint, and then gracefully ends the Processing Job. If more logs are needed to analyze, I have the Processing Job logs in CloudWatch Logs.\n\n2020-04-15 17:11:49 INFO  FileUtil:66 - Read file from path \/opt\/ml\/processing\/baseline\/constraints\/constraints.json.\n2020-04-15 17:11:50 INFO  FileUtil:66 - Read file from path \/opt\/ml\/processing\/baseline\/stats\/statistics.json.\n2020-04-15 17:11:50 ERROR DataAnalyzer:65 - There are missing columns in current dataset. Number of columns in current dataset: 1, Number of columns in baseline constraints: 225\nSkipping further processing because of column count mismatch.\n\n\nI could not find Model Monitor documentation on how to deal with column mismatch constraint violations.",
        "Answers":[
            {
                "Answer_creation_date":"2020-04-15T18:43:05.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"That violation fires when, for example, input to your endpoint has fewer columns than baseline input does. This is helpful to flag data quality issues. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-interpreting-violations.html\n\nIn this case, however, this is an artifact of how we perform the analysis. We concatenate output and input CSVs into a single CSV to analyze the whole thing in one go. E.g. it would look like:\n\noutput_col,input_col_1,input_col_2,...,input_col_n\n\n\nIn this case, however, your output has a trailing newline which means that after concatenating this looks like:\n\noutput_col # embedded newline in your output\n,input_col_1,input_col_2,...,input_col_n\n\n\nTriggering the code to think there is only one column in dataset and hence failing the job.\n\nWe have a fix flowing through the pipeline now, while that goes out you can add a preprocessing script to your schedule to strip out the trailing newline from the output. We will create a sample notebook for this, in the meantime docs are at https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-pre-and-post-processing.html#model-monitor-pre-processing-script",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker Studio - create domain error",
        "Question_creation_time":1586796156000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyWQfPusnSHG6Ujfzx27o1w\/sagemaker-studio-create-domain-error",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":1,
        "Question_view_count":792,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"A customer is trying to setup Sagemaker studio. He is following our published instructions to set up using IAM: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/onboard-iam.html\n\nBut is getting an error: User: arn:aws:iam:xxxx:user\/user1 is not authorized to perform: sagemaker:CreateDomain on resource: arn:aws:sagemaker: us-east-2:xxxx:domain\/yyyy\n\nHe has admin priviledges on the account and AmazonSageMakerFullAccess. We noticed that the AmazonSageMakerFullAccess policy actually has a limitation. You can perform all sagemaker actions, but not on a resource with arn \u201carn:aws:sagemaker:::domain\/*\u201d. We confirmed there are no other domains in that region with the CLI as you are only allowed one \u2013 so that isn\u2019t blocking. And aws sagemaker list-user-profiles returns no user profiles.\n\nHas anyone seen that error before or know the workaround? Should he create a custom policy to enable creating domains or would there be any implications of that? Are there specific permissions he should have so as to onboard using IAM?",
        "Answers":[
            {
                "Answer_creation_date":"2020-04-13T19:51:10.000Z",
                "Answer_upvote_count":1,
                "Answer_body":"A user with admin privileges would have access to \"iam:CreateServiceLinkedRole\" and \"sagemaker:CreateDomain\" actions, unless SCPs or permissions boundaries are involved. However, for the purpose of onboarding Amazon SageMaker Studio with limited permissions, I would grant the user least privilege by reviewing Control Access to the Amazon SageMaker API by Using Identity-based Policies and Actions, Resources, and Condition Keys for Amazon SageMaker documentation:\n\n{\n    \"Effect\": \"Allow\",\n    \"Action\": \"sagemaker:CreateDomain\",\n    \"Resource\": \"arn:aws:sagemaker:<REGION>:<ACCOUNT-ID>:domain\/*\"\n}\n\n\nNOTE: An AWS account is limited to one Domain, per region, see CreateDomain.\n\n{\n    \"Effect\": \"Allow\",\n    \"Action\": \"iam:CreateServiceLinkedRole\",\n    \"Resource\": \"*\",\n    \"Condition\": {\n        \"StringEquals\": {\n            \"iam:AWSServiceName\": \"sagemaker.amazonaws.com\"\n        }\n    }\n}\n\n\nCheers!",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"How to checkpoint SageMaker model artifact during a training job?",
        "Question_creation_time":1586331915000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrXX2MIygS5igas27GrAhHw\/how-to-checkpoint-sage-maker-model-artifact-during-a-training-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":182,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nIs there a way to regularly checkpoint model artifact in a SageMaker training job for BYO training container?",
        "Answers":[
            {
                "Answer_creation_date":"2020-04-08T15:22:36.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"If you specify a checkpoint configuration (regardless of managed spot training) when starting a training job, checkpointing will work. You can provide a local path and S3 path as follows (API reference):\n\n\"CheckpointConfig\": { \n  \"LocalPath\": \"string\",\n  \"S3Uri\": \"string\"\n}\n\n\nThe local path defaults to \/opt\/ml\/checkpoints\/, and then you specify the target path in S3 with S3Uri.\n\nGiven this configuration, SageMaker will configure an output channel with Continuous upload mode to Amazon S3. At the time being, this results in running an agent on the hosts that watches the file system and continuously uploads data to Amazon S3. Similar behavior is applied when debugging is enabled, for delivering tensor data to Amazon S3.\n\nAs commented, sagemaker-containers implements its own code to save intermediate outputs and watching files on the file system, but I would rather rely on the functionality offered by the service to avoid dependencies on specific libraries where possible.\n\nNote: when using SageMaker Processing, which in my view can be considered an abstraction over training or, from another perspective, the foundation for training, you can configure an output channel to use continuous upload mode; further info here.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"How to verify that checkpoints work for SageMaker Spot Training?",
        "Question_creation_time":1584346040000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbvA_lGXgQ3CdPuEoImWQVw\/how-to-verify-that-checkpoints-work-for-sage-maker-spot-training",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":51,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nHow can we know that checkpoint works before launching a sagemaker spot training job? Is there a way to force a regular checkpoint to s3 instead of waiting for the SIGTERM?\n\ncheers",
        "Answers":[
            {
                "Answer_creation_date":"2020-03-16T08:20:40.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi olivier, If you enable Sagemaker checkpointing , it periodically saves a copy of the artifacts into S3. I have used this in pytorch and it works by checkpointing periodically and the blog on Managed Spot Training: Save Up to 90% On Your Amazon SageMaker Training Jobs also mentions the same\n\nTo avoid restarting a training job from scratch should it be interrupted, we strongly recommend that you implement checkpointing, a technique that saves the model in training at periodic intervals",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"permissions issue with Sagemaker endpoint deployment",
        "Question_creation_time":1583528380000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUAL3DB8x9R7ysr8KjxY4PXg\/permissions-issue-with-sagemaker-endpoint-deployment",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":528,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi there,\n\nI am having permission issue deploying a SageMaker Endpoint, if someone could help me out here.\n\nIf I run this code in Sagemaker's Jupyter Notebook without providing credentials for boto3 client, it works. However if I try to invoke it with my credentials on my local machine, it gives me permission error, even though my account already was given all Sagemaker permissions.\n\nClientError: An error occurred (AccessDeniedException) when calling the InvokeEndpoint operation: User: arn:aws:iam::XXXXXXXXXXXX:user\/jenny.lien is not authorized to perform: sagemaker:InvokeEndpoint on resource: arn:aws:sagemaker:us-east-1:249707424405:endpoint\/XXXXXXXX with an explicit deny\n\nThis stackoverflow post has exactly the same issue as what I've experienced, I tried disable MFA for my IAM user but it is still not working...does it mean I will also have to create a special IAM user to make it work? thank you\nhttps:\/\/stackoverflow.com\/questions\/54172907\/amazon-sagemaker-accessdeniedexception-when-calling-the-invokeendpoint-operatio",
        "Answers":[
            {
                "Answer_creation_date":"2020-03-18T21:46:46.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hey there,\n\nOdd that you are still running into this issue after turning off MFA. I usually debug issues with IAM by creating a new user that is cleared from all other settings. Have you tried that approach yet?\n\nAlso, if you're looking for a no-code option to create an HTTP API for a Sagemaker endpoint, checkout https:\/\/Booklet.ai. If you point Booklet.ai to your endpoint, it will handle these invoke calls. From there, it will wrap your model endpoint in an HTTP API, provide a responsive web demo form, and more. You can bypass this invoke function.\n\nHope this helps!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2020-03-20T22:03:08.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"just wanted to update that I didn't turn of MFA the proper way. After doing so it works now. Thanks!",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Does Amazon SageMaker XGBoost support parallel training across multiple machines?",
        "Question_creation_time":1583496984000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOKZq2V_RQaaFzQkapcWpsA\/does-amazon-sage-maker-xg-boost-support-parallel-training-across-multiple-machines",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":51,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I'd like to set up Amazon SageMaker XGBoost to train datasets on multiple machines. Is that possible? If so, how?",
        "Answers":[
            {
                "Answer_creation_date":"2020-03-08T08:06:27.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Yes, using Amazon SageMaker hosting with XGBoost allows you to train datasets on multiple machines.\n\nFor more information, see Docker registry paths and example code in the Amazon SageMaker developer guide.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Available training metrics for built-in algorithms SageMaker",
        "Question_creation_time":1581632698000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0kvMbwPeRcyYcMAnnPxkng\/available-training-metrics-for-built-in-algorithms-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":47,
        "Question_answer_count":3,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello,\n\nI use built-in algorithms SageMaker and I search about the training metrics for each algorithms in SageMaker, is there a list the training metrics built-in algorithms SageMaker where I can review?\n\nThank you.\nRegards.",
        "Answers":[
            {
                "Answer_creation_date":"2020-02-20T23:48:46.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi msvm,\n\nThanks for your interest in SageMaker algorithms. You can find a list of metrics published by each algorithm on the \"Model Tuning\" page of respective algorithms.\n\nFor example:\nLinear learner: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/linear-learner-tuning.html\nNTM: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ntm-tuning.html\netc.\n\nThanks,\nSageMaker Algorithms Team",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2020-02-21T01:20:46.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\nthank you for your answer. I have other doubt, can I customize the metrics for built-in algorithms, for example Object Detection during training??\n\nThank you for your attention.\nRegards",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2020-02-18T19:13:29.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"The set of metrics emitted by built-in algorithms during training is fixed and cannot be customized by customers.\n\nRegards,\nSageMaker Algorithms team",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker PIPE Mode vs FSx ?",
        "Question_creation_time":1579692074000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyS6bjxG4R4qtnrXzA3uSeg\/sage-maker-pipe-mode-vs-f-sx",
        "Question_topic":[
            "Storage",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon FSx for Lustre",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":124,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi, SageMaker supports training data streaming via PIPE mode, and also reading from FSx distributed file system. Those options seem to provide same value: low latency, high throughput.\n\nWhat are the reasons for using one or the other?\nDo we have any benchmark of PIPE vs FSx for SageMaker, in terms of costs and speed?",
        "Answers":[
            {
                "Answer_creation_date":"2020-01-22T22:29:08.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I can think of the following scenarios\n\nPipemode cons\n\n** UPDATED**\n\nData Shuffling - In pipe mode you are working with streaming data and hence you cannot perform data shuffle operations unless you are prepared to shuffle within batches (as in wait to read a batch of records and shuffle within the batch in Pipe mode). Of if your data is distributed across multiples files, then you could use Sagemaker data shuffle to perform file level shuffle\n\nData readers - There are default data readers for pipemode that come with Tensorflow for formats like csv, tfrecord etc. But if you have custom data formats or using a different deep leaning framework, yYou would have to use custom data readers to deal with the raw bytes and understand the logical end of record. You could also use ml-io to see if any of the built-in pipe mode readers work for your usecase\n\nPIPE mode streams the data for each epoch from S3 and hence will be slower than FSX when you run a few epochs\n\nFSX:\n\nFSX works by lazy loading the s3 file and hence it has a start up delay but gets faster during repeated training.\n\nThere is no dependency on the framework and your existing code will work as is..\n\nThe only con of using FSX is the additional storage costs, but I would almost prefer FSX to pipe mode in most cases.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"SageMaker metrics persistence",
        "Question_creation_time":1578643279000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU197giXXuRn-4Hz56HZZpSw\/sage-maker-metrics-persistence",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":103,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Quick questions on ML metrics persistence from sagemaker training tasks. The SageMaker regexp-over-CloudWatch is an attractive option, yet the metric retention in Cloudwatch seems to be restricted to 15 days.\n\nHow to persist those metrics longer? Is it common to extract them out of Cloudwatch regularly to persist them somewhere else, eg S3 or an RDS? what is the best practice for long-term persistence of those metrics?\nWould SageMaker Experiments allow a collection of similar data (customer-defined training metrics) over a longer retention?",
        "Answers":[
            {
                "Answer_creation_date":"2020-01-10T18:32:10.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can now persist algorithm metrics from SageMaker Training Jobs (the ones you can collect with regexes or the ones available from built-in algorithms by default) by setting EnableSageMakerTimeSeriesMetrics through the AWS SDK or enable_sagemaker_metrics=true in the SageMaker Python SDK. These metrics are persisted long term, and available through Amazon SageMaker Studio. (Go to \"Metrics\" -> \"Add Chart\" from the detail page of a training job). These are available at no additional cost.\n\nYes, SageMaker Experiments allow collection of similar data\n\nNote that system metrics (CPU\/GPU\/Memory\/Disk) are still available only through CloudWatch.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"how to increase the storage of host instance",
        "Question_creation_time":1576741179000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeVh4VvD6R-eIhRYY6K8dsw\/how-to-increase-the-storage-of-host-instance",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":308,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"The parameters of a big neural network model can be huge. But the largest storage size of a host instance is only 30G, according to https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/host-instance-storage.html. Is there a way to increase the storage volume? I have a model (embeddings) that is very close to 30G and caused a no space error when deploying.\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_date":"2020-01-24T05:55:47.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"The disk size is currently not configurable for SageMaker Endpoints with EBS backed volumes. As a workaround, please use instances with ephemeral storage for your SageMaker endpoint.\n\nExample instance types with ephemeral storage:\n\nm5d instances: https:\/\/aws.amazon.com\/ec2\/instance-types\/m5\/\nc5d instances: https:\/\/aws.amazon.com\/ec2\/instance-types\/c5\/\nr5d instances: https:\/\/aws.amazon.com\/ec2\/instance-types\/r5\/\n\nThe full list of Amazon SageMaker instance types can be accessed here: https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2019-12-30T23:03:17.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks! Using a x5d instance solves the issue.\n\nAnd a quick note: even though I could download the big model to endpoint now, I got timeout error when loading the model in the endpoint. After some trial and error, I solved it by increasing the timeout value and reducing the number of worker by setting the environment variables. To do it, pass this dict\n\nenv={\"SAGEMAKER_MODEL_SERVER_WORKERS\":\"1\",\n\"SAGEMAKER_MODEL_SERVER_TIMEOUT\":\"1800\"}\n\nwhen creating the model object.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How should a custom SageMaker algorithm determine if checkpoints are enabled?",
        "Question_creation_time":1576724302000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmOFbuaH9RPSY4kH2t1dFcQ\/how-should-a-custom-sage-maker-algorithm-determine-if-checkpoints-are-enabled",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":93,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Per the SageMaker Environment Variables doc, algorithms should save model artifacts to the folder prescribed by SM_MODEL_DIR.\n\nThe SageMaker Containers doc describes additional environment variables, including SM_OUTPUT_DATA_DIR to write non-model training artifacts.\n\n...But how should the algorithm determine if checkpointing has been requested?\n\nThe Using Checkpoints in Amazon SageMaker doc only specifies a default local path to save them to, and I can't see any environment variables that would indicate whether or not to checkpoint. I've seen one piece of code checking for the existence of that default local path, but not convinced anybody's actually checked to see whether it works (is present when checkpointing is requested and absent when not).\n\nIt's good to parameterize Checkpointing to avoid wasting EBS space (and precious seconds of IO) in jobs when it's not needed; and by the conventions for other I\/O like model and data folders I would assume SageMaker to have a specific mechanism to pass this instruction, rather than just defining an algo hyperparameter?",
        "Answers":[
            {
                "Answer_creation_date":"2019-12-19T04:48:06.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nFor custom Sagemaker containers or deep learning frameworks, I tend to do this..and it works\n\nThis example is for pytorch I have tried\n\nentry point file:\n\n# 1. Define a custom argument, say checkpointdir\n parser.add_argument(\"--checkpointdir\", help=\"The checkpoint dir\", type=str,\n                     default=None)\n# 2. You can additional params for checkpoint frequency etc\n\n# 3. Code for checkpointing\nif checkpointdir is not None:\n   #TODO: save mode\n \n\nSagemaker estimator in Jupyter notebook, for. e.g.\n\n# 1. Define local and remote variables for checkpoints\ncheckpoint_s3 = \"s3:\/\/{}\/{}t\/\".format(bucket, \"checkpoints\")\nlocalcheckpoint_dir=\"\/opt\/ml\/checkpoints\/\"\n\nhyperparameters = {\n\n    \"batchsize\": \"8\",\n    \"epochs\" : \"1000\",\n    \"learning_rate\":.0001,\n    \"weight_decay\":5e-5,\n    \"momentum\":.9,\n    \"patience\": 20,\n    \"log-level\" : \"INFO\",\n    \"commit_id\":commit_id,\n    \"model\" :\"FasterRcnnFactory\",\n    \"accumulation_steps\": 8,\n# 2.  define hp for checkpoint dir\n    \"checkpointdir\": localcheckpoint_dir\n}\n\n# In the Sagemaker estimator fit, specify the local and remote path\nfrom sagemaker.pytorch import PyTorch\n\nestimator = PyTorch(\n     entry_point='experiment_train.py',\n                    source_dir = 'src',\n                    dependencies =['src\/datasets', 'src\/evaluators', 'src\/models'],\n                    role=role,\n                    framework_version =\"1.0.0\",\n                    py_version='py3',\n                    git_config= git_config,\n                    image_name= docker_repo,\n                    train_instance_count=1,\n                    train_instance_type=instance_type,\n# 3. The entrypoint file will pick up the checkpoint location from here\n                    hyperparameters =hyperparameters,\n                    output_path=s3_output_path,\n                    metric_definitions=metric_definitions,\n                    train_use_spot_instances = use_spot,\n                    train_max_run =  train_max_run_secs,\n                    train_max_wait = max_wait_time_secs,   \n                    base_job_name =\"object-detection\",\n# 4. Sagemaker knows that the checkpoints will need to be periodically copied from the localcheckpoint_dir to s3 pointed to by checkpoint_s3\n                    checkpoint_s3_uri=checkpoint_s3,\n                    checkpoint_local_path=localcheckpoint_dir)",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Studio will not load",
        "Question_creation_time":1576685163000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxoSA7eTzQbK-T4OWjJvSmQ\/sage-maker-studio-will-not-load",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":904,
        "Question_answer_count":8,
        "Question_has_accepted_answer":false,
        "Question_body":"Morning of 12\/17 I loaded SageMaker Studio, and created an Autopilot experiment. It ran for 2 hours and was successful.\n\nAfterwards, I exited SageMaker Studio. Ever since that point, I have been unable to re-enter Studio. It is now 24 hours later.\n\nI either receive a response from Chrome:\n\nThis page isn\u2019t working\nd-*************.studio.us-east-2.sagemaker.aws didn\u2019t send any data.\nERR_EMPTY_RESPONSE\n\nOr, I get an error message:\nThe JupyterServer app default encoutered a problem and was stopped.\nDetails: InternalFailure\n\nI get the option to \"Restart Now,\" but it never works.",
        "Answers":[
            {
                "Answer_creation_date":"2020-01-06T22:02:27.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello. I ran into a similar problem. I think SageMaker Studio, since it's in preview, is not yet a super-stable platform. To resolve this issue I just kept trying to launch the sagemaker studio from the aws console ('open amazon sagemaker studio' button from within the amazon SageMaker > Amazon SageMaker Stuiod > d-***** men). I received the same error about ten times but eventually I was able to get through to see the notebook I had previously created.\n\nHope this helps!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2021-07-21T17:45:56.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I see the same issue. SM Studio was working on the evening of 1\/6\/2020. I shut down the Juypter Lab and haven't been able to access it since. I've tried accessing via the SSO dashboard, as well as an admin user via the AWS console.\n\nCHROME returns:\n\nThis page isn\u2019t working d-*********.studio.us-east-2.sagemaker.aws didn\u2019t send any data.\nERR_EMPTY_RESPONSE\n\nFIREFOX returns:\n\nSecure Connection Failed\nAn error occurred during a connection to d-*********.studio.us-east-2.sagemaker.aws.\nThe page you are trying to view cannot be shown because the authenticity of the received data could not be verified.\nPlease contact the website owners to inform them of this problem.\n\nSAFARI returns:\n\nSafari Can't Open the Page\nSafari can't open the page \"https:\/\/d-*******.studio.us-east-2.sagemaker.aws\/jupyter\/default\" because the server unexpectedly dropped the connection. This sometimes occurs when the server is busy. Wait for a few minutes, and then try again.\n\nI tried for a few hours last night and again this morning. The problem persists. Any suggestions or support with this would be greatly appreciated. Thanks.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2020-01-08T16:49:47.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Still having the same issue, trying about once a day since the original errors.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2020-01-13T11:10:40.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I have the same issue. I have shut it down last night and could not find a way to open it or restart the whole amazon sagemaker studio service.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2020-04-20T02:21:05.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I was able to fix this by completely deleting my Studio instance with the AWS CKI. Of course you'll use any ephemaral data. You can then start over from scratch. Here is the sequence of commands.\n\naws sagemaker delete-app --domain-id yourDomainID --user-profile-name yourProfileName --app-type KernelGateway --app-name base-python\naws sagemaker delete-app --domain-id yourDomainID --user-profile-name yourProfileName --app-type JupyterServer --app-name default\naws sagemaker delete-user-profile --domain-id yourDomainID --user-profile-name yourProfileName\naws sagemaker delete-domain --domain-id yourDomainID",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2020-01-17T17:05:49.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I am facing a similar problem for one of my SageMaker users. The default app, which is the Jupyter server, keeps failing while trying to launch SageMaker studio. I am receiving the following message: \"The JupyterServer app default encountered a problem and was stopped.\". I click on the Restart button, but the message appears again. Does anyone know how can I solve this issue? Or do you recommend paying for AWS support?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2020-10-04T23:40:41.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Had the same issue and managed to fix it (and preserve my data).\n\nIn AWS console navigate to SageMaker Studio\nIn the users section at the top, open the user (user details page) that is experiencing this issue.\nAt the bottom of the page, delete all apps for this user. There will be the default app (which is jupyter server) and sometimes others like KernelGateway. DO NOT DELETE THE ACTUAL USER PROFILE.\nWhen all apps for this user is deleted, go back to SageMaker Studio and click on the \"Open Studio\" link for that user.\nIt will take some time, but SageMaker Studio will reinitialize that Jupyter Notebook.\n\nThe notebook will open and you should have all your data as it was before.\n\nEdited by: Noobie on Oct 4, 2020 4:41 PM\n\nEdited by: Noobie on Oct 4, 2020 4:43 PM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2020-01-07T14:18:00.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"This method works, thanks for sharing.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to tune SageMaker Studio Notebooks hardware config?",
        "Question_creation_time":1576675818000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUp5wKTB0URcCPyBgUcAWMww\/how-to-tune-sage-maker-studio-notebooks-hardware-config",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":70,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"How does one choose or tune the hardware backend of a Sagemaker Studio Notebook?",
        "Answers":[
            {
                "Answer_creation_date":"2019-12-18T22:33:24.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"At the top right of a notebook (near the kernel ) there will be a resource configurations button, you'll be able to choose the instance you want to run the notebook on.\n\nA nice feature of that is that all the instances shares the same EFS mount (SageMaker studio uses EFS for notebook storage) if you save a dataframe to the local disk (EFS) you can change instance type during your work and continue from the place you've been in (Move from a GPU instance to a CPU instance for cost effectiveness \/ back to GPU for performance)",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Is there any way to automatically stop the labeling job?",
        "Question_creation_time":1575516599000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU13ldoQNRT06TJu2wWWnE0A\/is-there-any-way-to-automatically-stop-the-labeling-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":30,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello all,\nI have a labeling job with about 40k images for labeling which I requested a vendor to work on, to control cost to fit within my budget, I wanted to automatically stop the job every 3k images being labeled, and may come back to get more labels more later when new budjet comes in, is there any way I can automatically stop the labeling job? or will I have to constantly monitor the labeling progress?\n\nThank you!\n\nJenny",
        "Answers":[
            {
                "Answer_creation_date":"2019-12-12T22:06:28.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Please pass the required MaxHumanLabeledObjectCount as a stopping condition. Refer to the documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_LabelingJobStoppingConditions.html#SageMaker-Type-LabelingJobStoppingConditions-MaxHumanLabeledObjectCount\n\nAlso refer to this documentation for starting a chained job after you receive more budget and re-start from where you left off last time https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-reusing-data.html#sms-reusing-data-console",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Incremental training with custom keras code in script mode",
        "Question_creation_time":1574940710000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU31VWC7mdRMCVfiCnAKiIXg\/incremental-training-with-custom-keras-code-in-script-mode",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":61,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\nI'm moving my first steps in sagemaker. I'm using script mode to train a classification algorithm. Training is fine, however I'm not able to do incremental training. I want to train again the same model with new data. Here what I did. This is my script\n\nimport sagemaker\r\nfrom sagemaker.tensorflow import TensorFlow\r\nfrom sagemaker import get_execution_role\r\n\r\nbucket = 'sagemaker-blablabla'\r\ntrain_data = 's3:\/\/{}\/{}'.format(bucket,'train')\r\nvalidation_data = 's3:\/\/{}\/{}'.format(bucket,'test')\r\n\r\ns3_output_location = 's3:\/\/{}'.format(bucket)\r\n\r\ntf_estimator = TensorFlow(entry_point='main.py', \r\n                          role=get_execution_role(),\r\n                          train_instance_count=1, \r\n                          train_instance_type='ml.p2.xlarge',\r\n                          framework_version='1.12', \r\n                          py_version='py3',\r\n                          output_path=s3_output_location)\r\n\r\ninputs = {'train': train_data, 'test': validation_data}\r\ntf_estimator.fit(inputs)\n\n\nThe entry point is my custom keras code, which I adapted to receive arguments from the script.\nNow the training is successfully completed and I have in my s3 bucket the model.tar.gz. I want to train again, but it's not clear to me how to do it.. I tried this\n\ntrained_model = 's3:\/\/sagemaker-blablabla\/sagemaker-tensorflow-scriptmode-2019-11-27-12-01-42-300\/output\/model.tar.gz'\r\n\r\ntf_estimator = sagemaker.estimator.Estimator(image_name='blablabla-west-1.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.12-gpu-py3', \r\n                                              role=get_execution_role(),\r\n                                              train_instance_count=1, \r\n                                              train_instance_type='ml.p2.xlarge',\r\n                                              output_path=s3_output_location,\r\n                                              model_uri = trained_model)\r\n\r\ninputs = {'train': train_data, 'test': validation_data}\r\n\r\ntf_estimator.fit(inputs)\n\n\nDoesn't work.. first I don't know how to retrieve the training image name (for this I looked for it in the aws console, but I guess there should be a smarter solution), second this code throws an exception about the entry point.. but it is my understanding that I shouldn't need it when I do incremental learning with a ready image..\nI'm surely missing something important, any help? Thank you",
        "Answers":[
            {
                "Answer_creation_date":"2019-12-03T11:48:13.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Found by myself, here the answer: https:\/\/stackoverflow.com\/a\/59096925\/4267439",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"By default, does Sagemaker endpoint handles parallel requests?",
        "Question_creation_time":1573757044000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfOh3ije6SsGZ-xrzRz0xzg\/by-default-does-sagemaker-endpoint-handles-parallel-requests",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":319,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"When there are multiple concurrent InvokeEndpoint requests being called to a deployed AWS Sagemaker endpoint, how is it being handled?\n\nI have deployed an endpoint with a P3.2xlarge instance. Currently one job takes around ~45 seconds to process. I have tried sending 4 different InvokeEndpoint requests at the same time and I noticed from CloudWatch logs that the jobs are being done serially depending on which request arrives first. --I suspect there is some sort of queue internally within the server model itself.--\n\nI am aware of automatic scaling as described here: https:\/\/aws.amazon.com\/\/blogs\/machine-learning\/load-test-and-optimize-an-amazon-sagemaker-endpoint-using-automatic-scaling\/ but my question is by default does aws sagemaker not allow concurrent requests being handled at the same time?\n\nUPDATE\nUpon further investigation and testing here are some additional information.\nI have deployed an ml.m4xlarge instance that simply sleeps for 45 seconds inside the transform function. Looks somewhat like\n\ndef transform_fn(model, request_body, content_type, accept_type):       \r\n    request_body_dict = json.loads(request_body)\r\n    time.sleep(45)\r\n    ...\n\n\nFurthermore, I have set the server timeout to be 420 seconds like so.\n\nsagemaker_model = MXNetModel(model_data = 's3:\/\/' + sagemaker_session.default_bucket() + '\/model\/yolo_object_person_detector.tar.gz',\r\n                             role = role, \r\n                             entry_point = 'load_testing_entrypoint.py',\r\n                             py_version='py3',\r\n                             framework_version='1.4.1',\r\n                             sagemaker_session = sagemaker_session,\r\n                            env = {'SAGEMAKER_MODEL_SERVER_TIMEOUT' : '420' })\r\n\r\npredictor = sagemaker_model.deploy(\r\n                            initial_instance_count=1,\r\n                            instance_type='ml.m4.xlarge',\r\n                            endpoint_name='load-testing')\n\n\nI tried sending 9 consecutive requests and monitored how they are being executed and what I've found is that there is no specific order in which the requests are being handled.\n\nA few questions I have from this experiment is:\n\nDoes AWS Sagemaker not process requests concurrently? Meaning, I would expect the server being able to handle two requests at the same time?\nFrom the client's side, how is it handling the case when the server is busy? I notice that it internally does retries for about 3 times after every 60 seconds if the request is not being handled\nWithin each of the 60 seconds time window, how is the client code calling the Endpoint? Is it constantly calling after every 1,2,4,6,8 seconds ?\n\nHere is the client side code\n\nsagemaker_client = boto3.client('sagemaker-runtime')\r\nresponse = sagemaker_client.invoke_endpoint(EndpointName='load-testing',Body=request_body)\n\n\nEdited by: ptanugraha on Nov 14, 2019 10:44 AM",
        "Answers":[
            {
                "Answer_creation_date":"2020-05-21T19:01:30.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nThanks for trying SageMaker and our apologies for late response.\n\nDoes AWS Sagemaker not process requests concurrently? Meaning, I would expect the server being able to handle two requests at the same time?\nAnswer: SageMaker does process requests concurrently. We keep sending the requests to model container as we get them and does not enqueue. However, we do have throttling in place which can kick in if there are too many requests coming which the endpoint is not able to handle. In case of throttling you will get the error response immediately. Here it is possible that your model is processing the requests sequentially. I suggest please test your model container locally with concurrent requests.\n\nFrom the client's side, how is it handling the case when the server is busy? I notice that it internally does retries for about 3 times after every 60 seconds if the request is not being handled\n\nWithin each of the 60 seconds time window, how is the client code calling the Endpoint? Is it constantly calling after every 1,2,4,6,8 seconds ?\nAnswer: For these you can refer to aws sdk client configuration:\nhttps:\/\/botocore.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/config.html\nhttps:\/\/docs.aws.amazon.com\/AWSJavaSDK\/latest\/javadoc\/com\/amazonaws\/ClientConfiguration.html\nTo answer your question, api call will wait for the response. If it gets any exception or timeout then it will do the retry depending on the retry policy you set for the sdk client configuration.\n\nThanks\n\nEdited by: harishataws on May 21, 2020 12:01 PM",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Batch Transform local mode?",
        "Question_creation_time":1571055107000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUtNtH0LyFSLCXc0xCV4hYkw\/sage-maker-batch-transform-local-mode",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":151,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nA customer is experimenting with SageMaker batch transform with parquet and is interested is some form of local development to speedup iteration. Does SageMaker Batch Transform support local mode?",
        "Answers":[
            {
                "Answer_creation_date":"2019-10-17T09:18:46.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"You can do local testing by running the container in serve mode as a docker. Then using Curl\/Postman to send an HTTP request and inspecting the response.\n\nThe request can be CSV\/JSON or binary (a parquet file in your case).\n\nIf you're able to run the Pytorch model in serve mode locally, then this local testing provides a lot of coverage before running in Batch Transform itself.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Fail to create Endpoints in SageMaker--JNI and NoClassDefFound error",
        "Question_creation_time":1569795079000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUECV9A3Q2RK6pqVpSvhRpYw\/fail-to-create-endpoints-in-sage-maker-jni-and-no-class-def-found-error",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":16,
        "Question_answer_count":3,
        "Question_has_accepted_answer":false,
        "Question_body":"I was trying to deploy a model(logged as a Mleap model in Databricks and saved in a s3 bucket) to SageMaker, and got stuck at the Endpoint creation:\n\n\"The primary container for production variant [xxx] did not pass the ping health check. Please check CloudWatch logs for this endpoint.\"\n\nIn the log I found the following block repeating over and over again until some time later the creating Endpoint process just stopped and the status turned 'failed' in the SageMaker UI:\n\nError: A JNI error has occurred, please check your installation and try again\r\nException in thread \"main\" java.lang.NoClassDefFoundError: org\/eclipse\/jetty\/util\/thread\/ThreadPool\r\n#011at java.lang.Class.getDeclaredMethods0(Native Method)\r\n#011at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)\r\n#011at java.lang.Class.privateGetMethodRecursive(Class.java:3048)\r\n#011at java.lang.Class.getMethod0(Class.java:3018)\r\n#011at java.lang.Class.getMethod(Class.java:1784)\r\n#011at sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544)\r\n#011at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526)\r\nCaused by: java.lang.ClassNotFoundException: org.eclipse.jetty.util.thread.ThreadPool\r\n#011at java.net.URLClassLoader.findClass(URLClassLoader.java:382)\r\n#011at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n#011at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)\r\n#011at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n#011... 7 more\r\nGot sigterm signal, exiting.\n\n\nI coded in Python in Databricks and as far as I could tell this is a Java error so I have no idea what went wrong, anyone has any experience deploying an 'outside' model to SageMaker? Any help or tips would be much appreciated!\n\nFYI: my whole s3 setup, ECR setup were in us-west-2(Oregon); the integration of AWS IAM role and Databricks Role are properly set up; in my s3, I could see the Databricks distributed filesystem in one bucket and the trained and pickled model in another; the docker image that is supposed to hold the model is successfully registered in ECR. I also tried to change the instance type under 'Production variants' in the Endpoint creation settings, I set it to the same instance type (ml.m5.large) as the one I used to initiate the Databricks runtime cluster but it did not seem to work.\n\nUpdate: I successfully trained, logged and deployed a sklearn model, but still have the same issue with spark ML model; for the container, I used a image built by mlflow:\n\nmlflow sagemaker build-and-push-container\n\n\nEdited by: ShumZZ on Sep 29, 2019 3:12 PM\n\nEdited by: ShumZZ on Oct 2, 2019 2:42 PM",
        "Answers":[
            {
                "Answer_creation_date":"2019-10-03T23:06:21.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi ShumZZ,\n\nI'm assuming your model container is built through Mleap's Databrick runtime integration. From the code base (https:\/\/github.com\/combust\/mleap\/tree\/master\/mleap-databricks-runtime-fat), it seems that the underlying implementation is in Scala, which would require JNI bindings to interact with your Python code.\n\nHave you tried running your model container locally? If the error persists when in your local environment, I would suggest reaching out to the Mleap community for assistance. To run your container locally, please follow the commands in\nSageMaker documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html\n\nThank you very much for trying out Amazon SageMaker! Please let us know if you have additional questions.\n\nBest Regards,\nYijie",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-10-03T20:30:13.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Yijie,\nThanks so much for the reply! I followed your advice and saw indeed that the local test also failed with the same JNI error...\n\nYet I am a bit confused since I actually used Mlflow's CLI mlflow sagemaker build-and-push-container to build the container. I am quite new to all these concepts (this is actually the first time I've ever worked on an end-to-end ML project), so correct me if I am wrong, should I reach out to Mlflow community instead of Mleap? Or is it the case that under the hood the container is indeed built through Mleap's Databrick runtime integration? Any help\/ clarifications\/ advices would be much appreciated:)\n\nLinks that might be useful but I do not quite understand...\nhttps:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/sagemaker\/cli.py\nhttps:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/models\/docker_utils.py",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-10-10T19:03:58.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"After some discussion with the Mlflow community, we confirmed the bug, where Java dependencies are not correctly installed in the docker image that Mlflow uses by default. I posted a bug report (https:\/\/github.com\/mlflow\/mlflow\/issues\/1906) on Github and a temporary fix has been provided by @smurching (https:\/\/github.com\/mlflow\/mlflow\/pull\/1913).\n\nEdited by: ShumZZ on Oct 10, 2019 12:04 PM",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"not able to add sagemaker dependencies as external dependencies to lambda",
        "Question_creation_time":1568641875000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUg5l3Jjl4SISDvnjIVYcqaA\/not-able-to-add-sagemaker-dependencies-as-external-dependencies-to-lambda",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":134,
        "Question_answer_count":3,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi Team,\nI'm trying to package sagemaker dependencies as external dependcies to upload to lambda.\nBut I'm getting the max size limit error. Package size is more than allowed size limit i.e.. deployment package size is 50 MB.\nAnd the reason I'm trying to do this is, 'get_image_uri' api is not accessible with boto3.\nsample code for this api :\n#Import the get_image_url utility function Amazon SageMaker Python SDK and get the location of the XGBoost container.\n\nimport sagemaker\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\ncontainer = get_image_uri(boto3.Session().region_name, 'xgboost')\n\nAny reference would be of great help. Thank you.",
        "Answers":[
            {
                "Answer_creation_date":"2019-09-16T13:56:24.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Could you explain in more detail why do you want to have sagemaker inside of a lambda please?",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2019-09-18T17:22:05.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Requirement is to train sagemaker model from lambda. So in the trainingconfiguration we have to pass the container for algorithm image.\nso container definition can either have hardcoded image uris and regions mapping, or, as below code snipper where you can get the latest image for specified region and algorithm.\n\ncontainer = get_image_uri(boto3.Session().region_name, 'xgboost')\n\nso to access the above api 'get_image_uri' I need to do below import.\nimport sagemaker\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\n\nThis is not supported by default in lambda.so I'm trying to package sagemaker dependencies as external dependencies and deploy in lambda.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-09-18T08:23:53.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"As s3 allows more size than direct upload, I even tried with uploading from s3 path.\nBut since sagemaker has numpy dependency and default numpy doesn't work with python containers, I had to add customized numpy packages for aws lambda, which raised the compressed file size to 105MB.\nNow that's also not working as it supports only upto 100MB and uncompressed size of less than 250MB.\nand reference or suggestion would be of great help.Thank you.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"New features of XGBoost",
        "Question_creation_time":1564402802000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIfvyse5TT8qN0Y0lyommFQ\/new-features-of-xg-boost",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":23,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have realized that the original XGBoost has several features which are not supported in the preloaded XGBoost in Sagemaker (or at least I have not been able to find them). For instance, the new metrics as aucr are not available in sagemaker. Is there any plan to add such features? Thank you",
        "Answers":[
            {
                "Answer_creation_date":"2019-08-01T15:45:29.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello, thank you for your using SageMaker! We are currently working to support the new features available in the updated version of XGBoost, which should allow you to use the new metrics as well.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Ground Truth job stuck in \"Stopping\" state",
        "Question_creation_time":1564177526000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUttmAZMiTT8yjLovuVkwI2g\/ground-truth-job-stuck-in-stopping-state",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":31,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I've got a job that I tried to stop, that has been stuck in \"Stopping\" for several days. This is problematic because it shows up on the worker portal, which will confuse my workers. Anything I can do to stop it completely?\n\nI can provide the full ARN if needed.",
        "Answers":[
            {
                "Answer_creation_date":"2019-07-30T18:42:29.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"This issue finally resolved itself after about a week, so I'll close this as resolved. Still happy to provide additional details if anyone on the AWS team want to debug.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"loading and deploying a previously trained sagemaker xgboost model",
        "Question_creation_time":1563308650000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6Cm7BTSlQ1GtLEzqVeGftQ\/loading-and-deploying-a-previously-trained-sagemaker-xgboost-model",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":475,
        "Question_answer_count":3,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to write an inference pipeline where I load a previously trained sagemaker xgboost model stored in s3 as a tar.gz file (following sagemaker tutorial) and deploy it as an endpoint for prediction. Here is my code:\n\ntrainedmodel = sagemaker.model.Model(    \r\n    model_data='data-path-to-my-model-in-s3\/model.tar.gz',\r\n    image=container,  \r\n    role=role)  \r\n\r\nxgb_predictor = trainedmodel.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n\n\nThe code runs fine but after that when I try to call predict() on xgb_predictor I get an error saying 'NoneType' object has no attribute 'predict'. I followed the example here to train the xgboost model:\n\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/simplify-machine-learning-with-xgboost-and-amazon-sagemaker\/\n\nWhy am I getting this error? What's the correct way to load a previously trained model? Help would be appreciated.",
        "Answers":[
            {
                "Answer_creation_date":"2020-10-30T20:06:16.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"thanks for using SageMaker! you're on the right path - you'll need to pass in an argument for \"predictor_cls\" when creating your Model instance in order for a predictor object to be returned after calling deploy(), e.g.\n\nfrom sagemaker.model import Model\r\nfrom sagemaker.predictor import RealTimePredictor, csv_serializer, csv_deserializer\r\n\r\nclass Predictor(RealTimePredictor):\r\n    def __init__(self, endpoint_name, sagemaker_session=None):\r\n        super(Predictor, self).__init__(\r\n            endpoint_name, sagemaker_session, csv_serializer, csv_deserializer\r\n        )\r\n\r\ntrainedmodel = Model(..., predictor_cls=Predictor)\r\nxgb_predictor = trainedmodel.deploy(...)\r\n\r\nxgb_predictor.predict(...)\n\n\nAPI reference:\n\nhttps:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html\nhttps:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\n\nhope that helps!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-09-09T17:50:51.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thank you. This solution worked!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-07-19T21:57:48.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Any special reason for using csv serializer\/deserializer? In my case I reload a model to analyze videos (frames in numpy array actually). What serializer\/deserializer should I use?\nActually, any doc regarding how to properly use the argument predictor_cls would be highly appreicated.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Received server error (0) from model when hosting",
        "Question_creation_time":1562557558000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUut1Vw-EPQUWVOy6cxmOmXw\/received-server-error-0-from-model-when-hosting",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":440,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\n\nI was trying to deploy my trained model to the endpoint and I was given a ModelError.\n\"ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from model with message \"Your invocation timed out while waiting for a response from container model. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See Link:https:\/\/ap-southeast-2.console.aws.amazon.com\/cloudwatch\/home?region=ap-southeast-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/ss-notebook-treemapping-2019-07-08-00-56-39-825 in account 125017970330 for more information.\"\n\nI'm not sure what caused this issue and couldn't figure out how latency metrics in the CloudWatch would be useful in this case. Does anyone know what the approach is to solve this issue? It would also be great to know why this happens. Thanks in advance for any help!",
        "Answers":[
            {
                "Answer_creation_date":"2019-07-10T20:17:00.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"ModelLatency is helpful because Sagemaker requires the container to respond within 60 seconds [1]: if you see the ModelLatency at or above 60 seconds that confirms the container isn't responding fast enough. At that point you'll need to debug why your container isn't fast enough: if it's a custom container you wrote you'll need to debug it; if it's a built-in container you should reach out to AWS support for assistance.\n\n[1] Timeout documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-code-container-response",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Uploading a Dataframe to AWS S3 Bucket from SageMaker",
        "Question_creation_time":1562042043000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfoMiB7A8SFOpr5uklZZuNg\/uploading-a-dataframe-to-aws-s-3-bucket-from-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":807,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"After successfully uploading CSV files from S3 to SageMaker notebook instance, I am stuck on doing the reverse.\n\nI have a dataframe and want to upload that to S3 Bucket as CSV or JSON. The code that I have is below:\n\nbucket='bucketname'\ndata_key = 'test.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\ndf.to_csv(data_location)\nI assumed since I successfully used pd.read_csv() while loading, using df.to_csv() would also work but it didn't. Probably it is generating error because this way I cannot pick the privacy options while uploading a file manually to S3. Is there a way to upload the data to S3 from SageMaker?",
        "Answers":[
            {
                "Answer_creation_date":"2019-07-02T04:34:23.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"One way to solve this would be to save the CSV to the local storage on the SageMaker notebook instance, and then use the S3 API's via boto3 to upload the file as an s3 object. S3 docs for upload_file() available here.\n\nNote, you'll need to ensure that your SageMaker hosted notebook instance has proper ReadWrite permissions in its IAM role, otherwise you'll receive a permissions error.\n\ncode you already have, saving the file locally to whatever directory you wish\n\nfile_name = \"mydata.csv\"\ndf.to_csv(file_name)\n\ninstantiate S3 client and upload to s3\n\nimport boto3\n\ns3 = boto3.resource('s3')\ns3.meta.client.upload_file(file_name, 'YOUR_S3_BUCKET_NAME', 'DESIRED_S3_OBJECT_NAME')\nAlternatively, upload_fileobj() may help for parallelizing as a multi-part upload.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"AWS Sagemaker - Either the training channel is empty or the mini-batch size",
        "Question_creation_time":1559545390000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq0KSPPCBT1qrotLu0NJyBw\/aws-sagemaker-either-the-training-channel-is-empty-or-the-mini-batch-size",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":38,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to train a linear learner model in Sagemaker. My training set is 422 rows split into 4 files on AWS S3. The mini-batch size that I set is 50.\n\nI keep on getting this error in Sagemaker.\n\nCustomer Error: No training data processed. Either the training\nchannel is empty or the mini-batch size is too high. Verify that\ntraining data contains non-empty files and the mini-batch size is less\nthan the number of records per training host.\n\nI am using this InputDataConfig\n\nInputDataConfig=\\[  \n            {  \n                'ChannelName': 'train',  \n                'DataSource': {  \n                    'S3DataSource': {  \n                        'S3DataType': 'S3Prefix',  \n                        'S3Uri': 's3:\/\/MY_S3_BUCKET\/REST_OF_PREFIX\/exported\/',  \n                        'S3DataDistributionType': 'FullyReplicated'  \n                    }  \n                },  \n                'ContentType': 'text\/csv',  \n                'CompressionType': 'Gzip'  \n            }  \n        ],  \n\n\nI am not sure what I am doing wrong here. I tried increasing the number of records to 5547495 split across 6 files. The same error. That makes me think that somehow the config itself has something missing. Due to which it seems to think training channel is just not present. I tried changing 'train' to 'training' as that is what the erorr message is saying. But then I got\n\nCustomer Error: Unable to initialize the algorithm. Failed to validate\ninput data configuration. (caused by ValidationError)\n\nCaused by: {u'training': {u'TrainingInputMode': u'Pipe',\nu'ContentType': u'text\/csv', u'RecordWrapperType': u'None',\nu'S3DistributionType': u'FullyReplicated'}} is not valid under any of\nthe given schemas\n\nI went back to train as that seems to be what is needed. But what am I doing wrong with that?\n\nEdited by: anshbansal on Jun 3, 2019 12:06 AM",
        "Answers":[
            {
                "Answer_creation_date":"2019-06-03T13:03:18.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Found the problem. The CompressionType was mentioned as 'Gzip' but I had changed the actual file to be not compressed when doing the exports. As soon as I changed it to be 'None' the training went smoothly.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Ground truth labeling job - unable to submit annotations",
        "Question_creation_time":1559512661000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOw2BnRGWTVmLfC-zcNN-RA\/ground-truth-labeling-job-unable-to-submit-annotations",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":93,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\nI've been experiencing this issue when I tried to submit a response on the worker portal. The browser kept popping up a message saying \"something went wrong\" and instructed me to refresh. However, nothing changed after refreshing. Does anyone know what might be causing this? Thanks!",
        "Answers":[
            {
                "Answer_creation_date":"2019-06-22T18:44:40.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"The issue turned out to be that I used a template from Mturk and some of the crowd-form attributes were not compatible with sage maker",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker Ground Truth notification after annotation job considered \"done\"?",
        "Question_creation_time":1558877970000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuT3kd2CJRcanme8YzDa73Q\/sage-maker-ground-truth-notification-after-annotation-job-considered-done",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":29,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi all,\n\nIs it possible to generate a notification after an annotation job is done? Couldn't find this info anywhere in the API docs, or by trawling the web. Being able to receive such notifications, be it SNS or something else, would be lot better than having to periodically poll the job's status.\n\nJoni",
        "Answers":[
            {
                "Answer_creation_date":"2019-07-22T01:38:35.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"We apologize for the delay in reply. Unfortunately, today we do not have such mechanisms to get notification for the completion of annotation. However, we recognize the importance of this feature and its usability and are working towards it. Watch out this space for updates. Thank you being our valued customer.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Mxnet default version?",
        "Question_creation_time":1558282489000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGtnafC3BQnCcDoZXeGVB4g\/mxnet-default-version",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":18,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Am struggling to deploy a SageMaker trained object detection model on DeepLens. When I attempt to use mo.optimize on DeepLens, I get error status 2 \"inconsistent platform versions.\". According to the SageMaker documention I found, the default version of mxnet is 1.2.1. Is this info current, and is it therefore safe to assume that an mxnet model trained and created in SageMaker console will use that version?\n\nEdited by: BigEd on May 19, 2019 9:41 AM",
        "Answers":[
            {
                "Answer_creation_date":"2019-05-31T01:08:19.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Error messages during model optimizing indicated SageMaker had saved the model with mxnet 1.4.0, so I upgraded to that on DeepLens and it solved my problem.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-05-24T03:59:28.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Apologies for the late response.\n\nGlad to see you were able to work around the problem.\n\nPlease let us know if there is anything we can assist with.\n\nThanks!",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Trouble deploying SageMaker trained model in DeepLens",
        "Question_creation_time":1558110894000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOklHGvFVQRiq6f24aBMWuA\/trouble-deploying-sage-maker-trained-model-in-deep-lens",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":39,
        "Question_answer_count":6,
        "Question_has_accepted_answer":false,
        "Question_body":"Had no problem deploying sample models to DeepLens, but really struggling to get a SageMaker trained model to deploy successfully. Here is what I have done so far:\n\nExtracted images from video with AWS MediaConnect\nCropped images to 540x540 with PIL and stored in S3\nRan labeling job with Mechanical Turk to draw bounding box around object\nRan training job in SageMaker console using Object Detection algorithm and input size 540\nCreated model in SageMaker console from training job\nImported model into DeepLens console\nCopied and modified Object Detection lambda with call to mo.optimize('model_algo_1',540,540)\nCreated and deployed project to two separate DeepLens cameras\n\nNot getting an output stream on either DeepLens. One device is about a year old. I have run all the software updates and mo.optimize steps recommended in the troubleshooting guide. The other device is brand new out of the box, updated to latest version via DeepLens console. I have not monkeyed with the new device, so all further information will pertain to the older device. I have connected a monitor and keyboard to the older device and am working in a terminal window.\n\nWhen I ran mo.optimize in terminal, I got a warning about having MXNET version 1.4.0 and 1.0.0 being required, along with list range errors. I rolled back MXNET to 1.0.0 per the troubleshooting guide. I did this with both PIP and PIP3.\n\nWhen I import mo in Python3, I get an import error message saying \"no module named mo\".\n\nWhen I import mo and run mo.optimize in Python, I get an access error about the current user lacking write permission to opt\/aws\/artifacts. When I launch Python as sudo and run mo.optimize, I get warnings about symbols saved with MXNET 1.04, and a List Index out of Range error.\n\nHere is my error code for mo.optimize in Python console:\n\naws_cam@Deepcam:~$ python\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import mo\r\n>>> error, model_path = mo.optimize('model_algo_1',540,540)\r\nDEBUG:mo:DLDT command: python3 \/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/mo_mxnet.py --input_model \/opt\/awscam\/artifacts\/model_algo_1-0000.params --data_type FP16 --scale 1 --model_name model_algo_1 --output_dir \/opt\/awscam\/artifacts --reverse_input_channels  --input_shape [1,3,540,540]\r\nModel Optimizer arguments\r\n\tBatch: \t1\r\n\tPrecision of IR: \tFP16\r\n\tEnable fusing: \tTrue\r\n\tEnable gfusing: \tTrue\r\n\tNames of input layers: \tinherited from the model\r\n\tPath to the Input Model: \t\/opt\/awscam\/artifacts\/model_algo_1-0000.params\r\n\tInput shapes: \t[1,3,540,540]\r\n\tLog level: \tERROR\r\n\tMean values: \t()\r\n\tIR output name: \tmodel_algo_1\r\n\tNames of output layers: \tinherited from the model\r\n\tPath for generated IR: \t\/opt\/awscam\/artifacts\r\n\tReverse input channels: \tTrue\r\n\tScale factor: \t1.0\r\n\tScale values: \t()\r\n\tVersion: \t0.3.31.d8b314f6\r\n\tPrefix name for args.nd and argx.nd files: \t\r\n\tName of pretrained model which will be merged with .nd files: \t\r\nERROR:mo:[ ERROR ]  Output directory \/opt\/awscam\/artifacts is not writable for current user. For more information please refer to Model Optimizer FAQ.\n\n\nHere is my error code for mo.optimize when running Python as sudo:\n\naws_cam@Deepcam:~$ sudo python\r\n[sudo] password for aws_cam: \r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import mo\r\n>>> error, model_path = mo.optimize('model_algo_1\",540,540)\r\n  File \"<stdin>\", line 1\r\n    error, model_path = mo.optimize('model_algo_1\",540,540)\r\n                                                          ^\r\nSyntaxError: EOL while scanning string literal\r\n>>> error, model_path = mo.optimize('model_algo_1',540,540)\r\nDEBUG:mo:DLDT command: python3 \/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/mo_mxnet.py --input_model \/opt\/awscam\/artifacts\/model_algo_1-0000.params --data_type FP16 --scale 1 --model_name model_algo_1 --output_dir \/opt\/awscam\/artifacts --reverse_input_channels  --input_shape [1,3,540,540]\r\nModel Optimizer arguments\r\n\tBatch: \t1\r\n\tPrecision of IR: \tFP16\r\n\tEnable fusing: \tTrue\r\n\tEnable gfusing: \tTrue\r\n\tNames of input layers: \tinherited from the model\r\n\tPath to the Input Model: \t\/opt\/awscam\/artifacts\/model_algo_1-0000.params\r\n\tInput shapes: \t[1,3,540,540]\r\n\tLog level: \tERROR\r\n\tMean values: \t()\r\n\tIR output name: \tmodel_algo_1\r\n\tNames of output layers: \tinherited from the model\r\n\tPath for generated IR: \t\/opt\/awscam\/artifacts\r\n\tReverse input channels: \tTrue\r\n\tScale factor: \t1.0\r\n\tScale values: \t()\r\n\tVersion: \t0.3.31.d8b314f6\r\n\tPrefix name for args.nd and argx.nd files: \t\r\n\tName of pretrained model which will be merged with .nd files: \t\r\nERROR:mo:[13:54:52] src\/nnvm\/legacy_json_util.cc:204: Warning: loading symbol saved by MXNet version 10400 with lower version of MXNet v10000. May cause undefined behavior. Please update MXNet if you encounter any issue\r\n[13:54:52] src\/nnvm\/legacy_json_util.cc:204: Warning: loading symbol saved by MXNet version 10400 with lower version of MXNet v10000. May cause undefined behavior. Please update MXNet if you encounter any issue\r\n\/usr\/local\/lib\/python3.5\/dist-packages\/mxnet\/module\/base_module.py:53: UserWarning: You created Module with Module(..., label_names=['softmax_label']) but input with name 'softmax_label' is not found in symbol.list_arguments(). Did you mean one of:\r\n\trelu4_3_scale\r\n\tdata\r\n\tlabel\r\n  warnings.warn(msg)\r\n[ ERROR ]  -------------------------------------------------\r\n[ ERROR ]  ----------------- INTERNAL ERROR ----------------\r\n[ ERROR ]  Unexpected exception happened.\r\n[ ERROR ]  Please contact Model Optimizer developers and forward the following information:\r\n[ ERROR ]  list index out of range\r\n[ ERROR ]  Traceback (most recent call last):\r\n  File \"\/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/mo\/main.py\", line 222, in main\r\n    return driver(argv)\r\n  File \"\/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/mo\/main.py\", line 208, in driver\r\n    mean_scale_values=mean_scale)\r\n  File \"\/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/mo\/pipeline\/mx.py\", line 80, in driver\r\n    graph = symbol2nx(model_nodes, model_params, argv.input)\r\n  File \"\/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/mo\/front\/mxnet\/loader.py\", line 70, in symbol2nx\r\n    model_nodes = MxnetSsdPatternMatcher.remove_and_change_layers(model_nodes)\r\n  File \"\/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/mo\/front\/mxnet\/mxnet_ssd_pattern_matcher.py\", line 132, in remove_and_change_layers\r\n    MxnetSsdPatternMatcher.ssd_pattern_remove_reshape(json_layers)\r\n  File \"\/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/mo\/front\/mxnet\/mxnet_ssd_pattern_matcher.py\", line 92, in ssd_pattern_remove_reshape\r\n    if l2['inputs'][0][0] != i or l2['op'] != 'Reshape':\r\nIndexError: list index out of range\r\n\r\n[ ERROR ]  ---------------- END OF BUG REPORT --------------\n\n\nEdited by: BigEd on May 17, 2019 9:37 AM\n\nEdited by: BigEd on May 17, 2019 11:34 AM",
        "Answers":[
            {
                "Answer_creation_date":"2019-05-24T03:16:49.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Possibly making some progress. Cloned incubator-mxnet to home directory and was able to run deploy.py on the model files with no error messages. Now when I run mo.optimize, I get the following output:\n\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import mo\r\n>>> mo.optimize('model_algo_1',540,540)\r\nDEBUG:mo:DLDT command: python3 \/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/mo_mxnet.py --input_model \/opt\/awscam\/artifacts\/model_algo_1-0000.params --data_type FP16 --scale 1 --model_name model_algo_1 --output_dir \/opt\/awscam\/artifacts --reverse_input_channels  --input_shape [1,3,540,540]\r\nModel Optimizer arguments\r\n        Batch:  1\r\n        Precision of IR:        FP16\r\n        Enable fusing:  True\r\n        Enable gfusing:         True\r\n        Names of input layers:  inherited from the model\r\n        Path to the Input Model:        \/opt\/awscam\/artifacts\/model_algo_1-0000.params\r\n        Input shapes:   [1,3,540,540]\r\n        Log level:      ERROR\r\n        Mean values:    ()\r\n        IR output name:         model_algo_1\r\n        Names of output layers:         inherited from the model\r\n        Path for generated IR:  \/opt\/awscam\/artifacts\r\n        Reverse input channels:         True\r\n        Scale factor:   1.0\r\n        Scale values:   ()\r\n        Version:        0.3.31.d8b314f6\r\n        Prefix name for args.nd and argx.nd files: \r\n        Name of pretrained model which will be merged with .nd files: \r\nERROR:mo:[ WARNING ]  \r\nDetected not satisfied dependencies:\r\n        mxnet: installed: 1.4.0, required: 1.0.0\r\n\r\nPlease install required versions of components or use install_prerequisites script\r\n\/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/install_prerequisites\/install_prerequisites_mxnet.sh\r\nNote that install_prerequisites scripts may install additional components.\r\n\/usr\/local\/lib\/python3.5\/dist-packages\/mxnet\/module\/base_module.py:56: UserWarning: You created Module with Module(..., label_names=['softmax_label']) but input with name 'softmax_label' is not found in symbol.list_arguments(). Did you mean one of:\r\n        data\r\n  warnings.warn(msg)\r\n[ ERROR ]  No or multiple placeholders in the model, but only one shape is provided, can not set it.\r\n[ ERROR ]  The following error happened while processing input shapes: . For more information please refer to Model Optimizer FAQ.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-05-19T14:07:48.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Further progress. Identified mismatch between model hyperparams and deploy.py inputs. Trained new model with resnet50 and image size 512, deployed, and ran deploy.py and mo.optimize.\n\n>>> mo.optimize('model_algo_1',512,512)\r\nDEBUG:mo:DLDT command: python3 \/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/mo_mxnet.py --input_model \/opt\/awscam\/artifacts\/model_algo_1-0000.params --data_type FP16 --scale 1 --model_name model_algo_1 --output_dir \/opt\/awscam\/artifacts --reverse_input_channels  --input_shape [1,3,512,512]\r\nModel Optimizer arguments\r\n        Batch:  1\r\n        Precision of IR:        FP16\r\n        Enable fusing:  True\r\n        Enable gfusing:         True\r\n        Names of input layers:  inherited from the model\r\n        Path to the Input Model:        \/opt\/awscam\/artifacts\/model_algo_1-0000.params\r\n        Input shapes:   [1,3,512,512]\r\n        Log level:      ERROR\r\n        Mean values:    ()\r\n        IR output name:         model_algo_1\r\n        Names of output layers:         inherited from the model\r\n        Path for generated IR:  \/opt\/awscam\/artifacts\r\n        Reverse input channels:         True\r\n        Scale factor:   1.0\r\n        Scale values:   ()\r\n        Version:        0.3.31.d8b314f6\r\n        Prefix name for args.nd and argx.nd files: \r\n        Name of pretrained model which will be merged with .nd files: \r\nERROR:mo:[ WARNING ]  \r\nDetected not satisfied dependencies:\r\n        mxnet: installed: 1.4.0, required: 1.0.0\r\n\r\nPlease install required versions of components or use install_prerequisites script\r\n\/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/install_prerequisites\/install_prerequisites_mxnet.sh\r\nNote that install_prerequisites scripts may install additional components.\r\n\/usr\/local\/lib\/python3.5\/dist-packages\/mxnet\/module\/base_module.py:56: UserWarning: You created Module with Module(..., label_names=['softmax_label']) but input with name 'softmax_label' is not found in symbol.list_arguments(). Did you mean one of:\r\n        data\r\n  warnings.warn(msg)\r\n[ ERROR ]  Cannot infer shapes or values for node \"cls_prob\".\r\n[ ERROR ]  There is no registered \"infer\" function for node \"cls_prob\" with op = \"softmax\". Please implement this function in the extensions. For more information please refer to Model Optimizer FAQ.\r\n[ ERROR ]  \r\n[ ERROR ]  It can happen due to bug in custom shape infer function <UNKNOWN>.\r\n[ ERROR ]  Or because the node inputs have incorrect values\/shapes.\r\n[ ERROR ]  Or because input shapes are incorrect (embedded to the model or passed via --input_shape).\r\n[ ERROR ]  Run Model Optimizer with --log_level=DEBUG for more information.\r\n[ ERROR ]  Stopped shape\/value propagation at \"cls_prob\" node. For more information please refer to Model Optimizer FAQ.\n\n\nI've run the install_prerequisites script several times, but get the following error:\n\n#details omitted\r\n+ python3 pip3 install -r ..\/..\/..\/requirements_mxnet.txt\r\nWARNING: The directory '\/home\/aws_cam\/.cache\/pip\/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\r\nWARNING: The directory '\/home\/aws_cam\/.cache\/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\r\nERROR: Could not open requirements file: [Errno 2] No such file or directory: '..\/..\/..\/requirements_mxnet.txt'\n\n\nAny ideas?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-05-19T02:23:22.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Figured there might be some issues with my model, so I went back and ran thru the SageMaker example notebook for Object Detection, then deployed it. Initially got the same error as above, but then I rolled back MXNET to 1.3.0 for both PIP3 and PIP. Then when I ran deploy.rb and mo.optimize, I got the following result:\n\n>>> mo.optimize('model_algo_1',512,512)\r\nDEBUG:mo:DLDT command: python3 \/opt\/awscam\/intel\/deeplearning_deploymenttoolkit\/deployment_tools\/model_optimizer\/mo_mxnet.py --input_model \/opt\/awscam\/artifacts\/model_algo_1-0000.params --data_type FP16 --scale 1 --model_name model_algo_1 --output_dir \/opt\/awscam\/artifacts --reverse_input_channels  --input_shape [1,3,512,512]\r\nModel Optimizer arguments\r\n        Batch:  1\r\n        Precision of IR:        FP16\r\n        Enable fusing:  True\r\n        Enable gfusing:         True\r\n        Names of input layers:  inherited from the model\r\n        Path to the Input Model:        \/opt\/awscam\/artifacts\/model_algo_1-0000.params\r\n        Input shapes:   [1,3,512,512]\r\n        Log level:      ERROR\r\n        Mean values:    ()\r\n        IR output name:         model_algo_1\r\n        Names of output layers:         inherited from the model\r\n        Path for generated IR:  \/opt\/awscam\/artifacts\r\n        Reverse input channels:         True\r\n        Scale factor:   1.0\r\n        Scale values:   ()\r\n        Version:        0.3.31.d8b314f6\r\n        Prefix name for args.nd and argx.nd files: \r\n        Name of pretrained model which will be merged with .nd files: \r\nERROR:mo:\r\n(2, '')\n\n\nAccording to the model optimizer documentation, a return of 2 for status means \"Model optimization failed because you are using inconsistent platform versions.\" It recommends installing mxnet. Sigh.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-05-20T14:00:03.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I assume the \"inconsistent platform versions\" in mo.optimize status code 2 refers to the mxnet versions used to train and optimize the model. Is that correct?\n\nAs best as I can tell from Sagemaker documentation, the default mxnet version for training is 1.2.1.\n\nI ran \"sudo pip3 install mxnet==1.2.1\" in deeplens, but still get error code 2.\n\nDoes anyone have more current or accurate info on mxnet versions in Sagemaker and DeepLens?\n\nIs it possible the version conflict could be with deploy.py?\n\nEdited by: BigEd on May 19, 2019 9:44 AM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-05-18T05:06:02.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Executed git reset command to set incubator-mxnet to the repo specified in the documentation. According to the README, that version of deploy.rb is compatible with mxnet 1.3.0 and earlier. Updated awscam version of mxnet to 1.3.0. Still getting return of status code 2 - inconsistent platforms from mo.optimize.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-05-18T20:30:23.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Believe I have resolved it. I updated awscam, rebooted, and installed mxnet version 1.4.0.\n\nDocumentation on how to use deploy.py is poor. For anyone else who is looking for the steps, do this:\n\nSSH to your DeepLens or open a terminal on it, clone the incubator-mxnet\/example\/ssd repository to the home directory, then run the following line to roll the version back to the right one:\ngit reset --hard 73d88974f8bca1e68441606fb0787a2cd17eb364\n\n\nMove files model_algo_1-0000.params, model_algo_1-symbol.json, and hyperparams.json from \/opt\/awscam\/artifacts\/ to \/home\/incubator-mxnet\/example\/ssd\/.\n\nRun deploy.py as follows:\n\nsudo python deploy.py --network='resnet50' --epoch=0 --prefix='model_algo_1' --data-shape=512 --num-class=80\n\n\nwhere shape is the x & y dimension of the images you trained the model on, num-class is the number of classes you trained it to identify, and network should match the network you trained to (VGG16 or resnet50). If you are successful, you should get this:\n\nSaved model: model_algo_1-0000.params\r\nSaved symbol: model_algo_1-symbol.json\n\n\nMove the 3 model files back to \/opt\/awscam\/artifacts\/. You'll probably need to use sudo mv.\n\nLaunch sudo python, import mo, and run the optimizer.\n\nEdited by: BigEd on May 23, 2019 8:17 PM\n\nEdited by: BigEd on May 23, 2019 8:17 PM\n\nEdited by: BigEd on May 23, 2019 8:19 PM",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Ground Truth - Active Learning Status",
        "Question_creation_time":1558037074000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBS3WNK9fQtOBQ4k3PowXLQ\/ground-truth-active-learning-status",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":29,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I currently have a Ground Truth labeling job in progress for which I enabled the active learning feature. I see the activelearning folder in the output directory within s3, but all that's in there right now is a file called active_learning_info.json. The job is on its 4th batch of images, which at 250 for each batch, means it is at almost 1,000 objects labeled by Mechanical Turk workers at this point. Based on that as well as reading https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-output.html and https:\/\/aws.amazon.com\/blogs\/machine-learning\/annotate-data-for-less-with-amazon-sagemaker-ground-truth-and-automated-data-labeling\/ , I was expecting to see additional contents within the activelearning folder. Is that not accurate? How do I know if active learning is truly being (at least) attempted within this labeling job?\n\nThank you!\n\nEdited by: naquent on May 16, 2019 1:05 PM",
        "Answers":[
            {
                "Answer_creation_date":"2019-05-17T13:18:23.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Answering my own question: I checked again this morning, and now I see all of the expected folders and files. I guess it just takes a while for it to show up. In this case, the lowest numbered folder in the activelearning folder is 8. My guess is that this is corresponding to the 8th batch, which would mean it took 8 batches of 250 images for it to start attempting active learning. Is that the correct understanding? Either way, I'll mark this thread as answered.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-07-22T20:44:30.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Apologize for the delay in the reply.\nThat is correct. The lowest number of the folder would be when we started to \"attempt\" active learning. You should see input\/output files that might throw some light on to which data objects were tried in those batches. But, this does not necessarily mean that it got auto annotated. The best way to check the objects that got auto annotated is the final output file where it would be marked by a metadata field \"human-annotated\":\"no\" after the job is done. Or, you can check the folder \"intermediate\" where we store the output of each batch.\n\nWe hope this answers your question. Thank you for being a valued customer.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to create Keras's encoder-decoder model's endpoint?",
        "Question_creation_time":1557413472000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUa1eV25XxRjKhLPHVXe9TKQ\/how-to-create-kerass-encoder-decoder-models-endpoint",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":31,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\nI'm just started to use sagemaker.\nNow, I'm testing encoder-decoder model for regression.\nThe model is coded in Sagemaker's script mode, and finished learning on a Jupyter notebook.\n\nThe learning code is written by keras + tensorflow, and the model is based on the encoder decoder model. It is similar to the Keras's seq2seq example linked below.\nBelow, I will take this as an example.\nhttps:\/\/keras.io\/examples\/lstm_seq2seq\/\n\nIn the above model, there are \"encoder_model\" and \"decoder_model\" apart from \"model\" to be trained, and in the inference, \"encoder_model\" and \"decoder_model\" are used to generate the prediction by the function \"decode_sequence (input_seq)\".\nI would like to deploy this function \"decode_sequence (input_seq)\" as an endpoint, but it doesn't work as usual with estimator.deploy () and I don't know how to implement it.\n\nIs there any sample code or resources to solve this?\nThanks in advance.",
        "Answers":[
            {
                "Answer_creation_date":"2019-05-20T00:36:29.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks, the problem was self resolved.\nWhat should I do was to code \"decode sequence\" withint the \"keras_model_fn\".",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to enable automated data labeling for custom labeling job?",
        "Question_creation_time":1557318279000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-JeOVDZER4uGxiY0l_geKg\/how-to-enable-automated-data-labeling-for-custom-labeling-job",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":38,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\nI need to label large dataset which has around 200000 records and this doesn't fall into any of the prebuilt labeling job categories. So trying to create custom labeling job with automated data labeling enabled. But the configuration to enable the automated data labeling doesn't appear for custom labelling job. Any reference to proceed further asap would be of great help. Thank you.",
        "Answers":[
            {
                "Answer_creation_date":"2019-05-08T23:13:42.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi there. I'm an engineer at AWS.\n\nAutomatic data labeling is only available for the first-party workflows: Image Classification, Text Classification, and Object Detection. As an alternative, you might consider orchestrating a series of labeling jobs with an intermediate step in which you train a machine learning model and perform inference to carry out an auto labeling step. We have a blog post that describes this \"active learning\" process to give you some ideas as to how you might accomplish this (https:\/\/aws.amazon.com\/blogs\/machine-learning\/annotate-data-for-less-with-amazon-sagemaker-ground-truth-and-automated-data-labeling\/).\n\nThank you for being a valued customer and for bringing this feature request to our attention.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"SageMaker notebook instance in VPC failed to connect to local database",
        "Question_creation_time":1557130614000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUUm8LxKZTzixOCI_IovK1A\/sage-maker-notebook-instance-in-vpc-failed-to-connect-to-local-database",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":260,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"hi there,\n\nI am setting up a jupyter notebook in SageMaker within the VPC and using the jdbc jars to connect to the local database. But it shows the following error messages.\n\": com.microsoft.sqlserver.jdbc.SQLServerException: The TCP\/IP connection to the host xxx.xxx.xxx.xxx, port 21000 has failed. Error: \"connect timed out. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP\/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.\".\"\n\nI used the exactly the same VPC, subnet, security group as what i used in a glue job to extract the data from the local db. While the glue job works but the SageMaker notebook failed. I am sure the firewalls are opened.\nCould anyone tell me how to solve it?\nI also came across the following articles, but i am not sure if it is the root cause.\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options\/",
        "Answers":[
            {
                "Answer_creation_date":"2019-05-06T22:04:11.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\nThe principle here is that there much be network connectivity between the Notebook Instance and the DB Instance, and the security groups on the DB Instance should allow in-bound traffic from the Notebook Instance\n\nOne example of such as setup is\n\nRDS DB Instance is VPC vpc-a and Subnet subnet-b.\n\nSageMaker Notebook is launched in VPC vpc-a, Subnet subnet-b, with Security Group sg-c with DirectIntenetAccess \"Disabled\"\n\nIn the RDS DB Instance's Security Group rules, you can add an Inbound Rule to allow inbound traffic from the SageMaker Notebook security group \"sg-c\"\n-- Type - Protocol - Port Range - Source\n-- MYSQL\/Aurora - TCP - 3306 - sg-c\n\nSample Code:\n\n! pip install mysql-connector\r\n\r\nimport mysql.connector\r\nmydb = mysql.connector.connect(\r\nhost=\"$RDS_ENDPOINT\",\r\nuser=\"$RDS_USERNAME\",\r\npasswd=\"$RDS_PASSWORD\"\r\n)\r\ncursor = mydb.cursor()\r\ncursor.execute(\"SHOW DATABASES\") \n\n\nThanks for using Amazon SageMaker and let us know if there's anything else we can help with!\n\nEdited by: JaipreetS-AWS on May 6, 2019 3:04 PM",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Private Marketplace for SageMaker algorithms",
        "Question_creation_time":1556295545000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCu2f_chpRL2mDtFiGqwVNg\/private-marketplace-for-sage-maker-algorithms",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":52,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"We would like to make algorithms shareable and re-usable across teams. Is it possible to create a private Amazon SageMaker algorithm marketplace?",
        "Answers":[
            {
                "Answer_creation_date":"2019-04-26T16:28:58.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Private Marketplace is a feature of the AWS Marketplace platform that AWS customers can use to create a private catalog of products containing both algorithms and models which are available in AWS Marketplace. To create a private catalog containing algorithms which have not been published in AWS Marketplace, you can use AWS Service Catalog. AWS Service Catalog lets users create and share algorithms packaged via a CloudFormation template. However, you do have to make the container image and model artifacts available in the destination account outside AWS Service Catalog. Here is a sample template for sharing a model - https:\/\/github.com\/aws-samples\/aws-service-catalog-reference-architectures\/blob\/master\/sagemaker\/sagemaker_vend_endpoint.yml",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Deploy SageMaker model to IoT Greengrass in different account?",
        "Question_creation_time":1556295446000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJha7KbxOTXuhMRGbMYGC0g\/deploy-sage-maker-model-to-io-t-greengrass-in-different-account",
        "Question_topic":[
            "Internet of Things (IoT)",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS IoT Greengrass",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":38,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Is it possible to deploy a model created by SageMaker in one account to an IoT Greengrass device in a different account?",
        "Answers":[
            {
                "Answer_creation_date":"2019-04-29T09:20:54.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"For IoT Greengrass 1.x, this is possible but not trivial. From the console this is not possible, as you can only select buckets or SageMaker jobs from the same account, but you can refer to resources in other accounts if you use the CLI or the API.\n\nYou have to create a new Resource Definition Version with the correct data specifying the model resource and then add it to your group definition. For permissions in the source account, you must set up the S3 bucket policy to allow access from the destination account. For permissions in the destination account, you must update the IoT Greengrass service role policy to access the model resource in the source account.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Greengrass for data processing and ML model training",
        "Question_creation_time":1556295399000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2FEvRboNRL2Ipn1yE7IBvg\/greengrass-for-data-processing-and-ml-model-training",
        "Question_topic":[
            "Internet of Things (IoT)",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS IoT Greengrass",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":38,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Is it possible to train and deploy ML models in Greengrass? Or is Greengrass limited to inference while training is done using SageMaker in cloud?",
        "Answers":[
            {
                "Answer_creation_date":"2019-04-26T16:21:21.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"As a native service offering, Greengrass has support for deploying models to the edge and running inference code against those models. Nothing prevents you from deploying your own code to the edge that would train a model, but I suspect you wouldn't be able to store it as a Greengrass local resource for later inferences without doing a round trip to the cloud and redeploying to GG.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Ground Truth Text Format",
        "Question_creation_time":1556263122000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzhqhRbWOReCIYtX925_VPw\/ground-truth-text-format",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":1,
        "Question_view_count":125,
        "Question_answer_count":3,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello\nWe are trying to setup labeling for text with ground truth. Is it possible to format the source text to be labeled in some way e.g. html, markdown? A new line would already help a lot. I could not find any documentation on this here https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html\n\nThanks\nNicolas",
        "Answers":[
            {
                "Answer_creation_date":"2019-04-29T06:56:34.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"When you post tasks to Ground Truth they are displayed to workers using a web interface so HTML is the best bet for formatting the text you want annotated. For example, you could simply use the following to create a multi-line input:\n\n{\"source\": \"Lorem ipsum <br\/>dolor sit amet\"}\n\n\nUnfortunately, by default, inputs are HTML escaped to prevent confusion between your variable text and HTML. As a result if you use the Text Classification widget with the text above will just be displayed to workers as Lorem ipsum <br\/>dolor sit amet. To pass those values without escaping them you'll need to create a custom template that includes a filter on the variable to prevent it from being escaped.\n\nTo setup a custom task, start by creating Lambdas to handle for the pre and post processing required. Information on setting these up can be found at https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step3.html\n\nI've included simple pre and post Lambda examples below that I find are a good place to start for text annotation. Note that the post Lambda doesn't do any answer consolidation, just simply passes back all of the answers provided by Workers.\n\nCreate a custom template in Ground Truth and use the Sentiment Analysis template to create a starter task for text annotation. To prevent it from escaping HTML values, update the Liquid variables to include the skip_autoescape filter.\n\n{{ task.input.text | skip_autoescape }}\n\n\nYou can find more info on using Liquid template values here:\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step2.html#sms-custom-templates-step2-automate\n\nPre\n\ndef lambda_handler(event, context):\r\n    print(event)\r\n    source = event['dataObject'].get('source')\r\n\r\n    if source:\r\n        print(\"text is {}\".format(source))\r\n    else:\r\n        print(\"Missing text in dataObject\")\r\n        return {}\r\n    \r\n    response = {\r\n        \"taskInput\": {\r\n            \"text\": source\r\n        }\r\n    }\r\n    print(response)\r\n    return response\n\n\nPost\n\nimport json\r\nimport boto3\r\nfrom urllib.parse import urlparse\r\n\r\n\r\ndef lambda_handler(event, context):\r\n    print(json.dumps(event))\r\n\r\n    payload = get_payload(event)\r\n    print(json.dumps(payload))\r\n\r\n    consolidated_response = []\r\n    for dataset in payload:\r\n        annotations = dataset['annotations']\r\n        responses = []\r\n        for annotation in annotations:\r\n            response = json.loads(annotation['annotationData']['content'])\r\n            if 'annotatedResult' in response:\r\n                response = response['annotatedResult']\r\n\r\n            responses.append({\r\n                'workerId': annotation['workerId'],\r\n                'annotation': response\r\n            })\r\n\r\n        consolidated_response.append({\r\n            'datasetObjectId': dataset['datasetObjectId'],\r\n            'consolidatedAnnotation' : {\r\n                'content': {\r\n                    event['labelAttributeName']: {\r\n                        'responses': responses\r\n                    }\r\n                }\r\n            }\r\n        })\r\n\r\n    print(json.dumps(consolidated_response))\r\n    return consolidated_response\r\n\r\n\r\ndef get_payload(event):\r\n    if 'payload' in event:\r\n        parsed_url = urlparse(event['payload']['s3Uri'])\r\n        s3 = boto3.client('s3')\r\n        text_file = s3.get_object(Bucket=parsed_url.netloc, Key=parsed_url.path[1:])\r\n        return json.loads(text_file['Body'].read())\r\n    else:\r\n        return event.get('test_payload',[])",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2021-06-18T16:24:52.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Ah, great. That was exactly what I was missing. I simply changed the template to {{ task.input.taskObject | skip_autoescape }} and left out the pre-processing lambda.\n\nEdited by: nicolasdoodle on Apr 28, 2019 11:56 PM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-04-29T04:49:25.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I'm trying to do the same thing. But couldn't make it work. Did you still use the post-processing lambda? Or did you just change the template.liquid file\n\nEdited by: apoorvsrivastava on Jun 18, 2021 9:28 AM",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Ground Truth - Label Consolidation, NN Models, and Accuracy Levels",
        "Question_creation_time":1554731637000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVR_6Xq0sQKaRrK65qA2VOg\/ground-truth-label-consolidation-nn-models-and-accuracy-levels",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":36,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hello,\n\n(1)\nIn https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-annotation-consolidation.html, it says: \"Multi-class annotation consolidation for image and text classification uses a variant of the Expectation Maximization approach to annotations. It estimates parameters for each worker and uses Bayesian inference to estimate the true class based on the class annotations from individual workers.\" Is there anywhere I can get more information and detail on this consolidation process? For now, I am trying to understand what this looks like when I have only 2 labelers per object, but I would also want to get a deeper understanding of how this works in general.\n\n(2)\nIs there anywhere I can get more information on the neural networks used in the automatic labeling component of Ground Truth? Is there a way to access and customize them? Also, is there a way to export\/pickle the models and use them outside of Ground Truth?\n\n(3)\nLastly, in https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-automated-labeling.html, it says: \"The potential benefit of automated data labeling also depends on the accuracy that you require. Higher accuracy levels generally reduce the number of data objects that are automatically labeled.\" Where is this accuracy level set?\n\nThank you!",
        "Answers":[
            {
                "Answer_creation_date":"2019-04-10T12:58:31.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi naquent,\n\nI'm an engineer at AWS, and I wanted to offer a response to your three questions.\n\nWe have a blog post that describes the active learning process and annotation consolidation methodology in a little bit more detail, which might be helpful in understanding the approach - https:\/\/aws.amazon.com\/blogs\/machine-learning\/annotate-data-for-less-with-amazon-sagemaker-ground-truth-and-automated-data-labeling\/. The exact nature of the annotation consolidation algorithm used in the service is proprietary. However, there is a large body of work that describes this type of method that might give you a better sense for what is involved [1].\n\nI should add that we typically recommend at least three annotators for all scenarios for exactly the reason indicated in the question. That is, when you only have two annotators, it is difficult to determine the preference that should be given to any individual worker in the presence of disagreements.\n\nThe neural networks used by Ground Truth are the same as those available elsewhere on the SageMaker platform. When you run an image classification labeling job, you are leveraging a SageMaker image classification model. There are many writeups of these algorithms in various locations, but a useful starting point would be https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html.\nYou can definitely access and customize these models! As I mentioned above, when you run Ground Truth labeling jobs, machine learning models will be trained over the course of the labeling job. You will find these in your SageMaker console, e.g., the \"Training Jobs\" console will show you the training jobs that have been run, and the \"Models\" console will list the model artifacts that have been created. You can use these to run inference, or retrain \/ fine tune them as desired depending on your circumstances. Please see one of our sample notebooks for a demonstration of how you can use this model for further training, inference, and hosting - https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/from_unlabeled_data_to_deployed_machine_learning_model_ground_truth_demo_image_classification\/from_unlabeled_data_to_deployed_machine_learning_model_ground_truth_demo_image_classification.ipynb.\n\nThis accuracy level is set by the service. You can always assess the labeled data after a labeling job in the console. Every label provided by Ground Truth is associated with a \"confidence score\" which can be helpful in filtering the output of a labeling job. The sample notebook linked above illustrates a couple of workflows in this regard.\n\nThanks for being a valued AWS customer. Please don't hesitate to reach back out to us.\n\n, e.g., A. P. Dawid and A. M. Skene. 1979. Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm. J. Royal Stat. Soc. Series C 28, 1 (1979), 20\u201328. http:\/\/www.jstor.org\/stable\/2346806",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-04-10T00:10:54.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thank you, Jonathan! These resources look to be very helpful. I will look through them and follow up if I have any further questions.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Lifecycle scripts to access the notebook instance git repository",
        "Question_creation_time":1554297310000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhTwl-BZURCGatI4Rm9fFqg\/lifecycle-scripts-to-access-the-notebook-instance-git-repository",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":132,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi there,\nIs it possible for the lifecycle scripts to access the content of the checkout-ed git repository? A use case would be to access the already available in the repository pip requirements file and to populate the notebook instance with the required python modules on start up.\nI guess the answer to this question depends on the order of the executed events when a notebook is created. Are you executing the lifecycle scripts first and then checkout the repository or vice versa.\n\nThanks!\n\nEdit:\n\nWell, I did my experiment as follows.\n\nI added a simple 'ls -al SageMaker' in the start and create lifecycle scripts and inspected the logs.\nIt seems that on initial notebook instance creation the git repository is checked out after the execution of the start and create scripts.\nOn subsequent notebook starts, the start script is executed and the repository folder is present in the SageMaker folder with a timestamp indicating that the repository folder was created after the initial start\/create scripts executions.\n\nSo, can someone confirm that this is what's expected and that we can access the repository only on subsequent notebook starts?\n\nEdited by: ainkov on Apr 3, 2019 7:25 AM",
        "Answers":[
            {
                "Answer_creation_date":"2019-04-11T18:41:49.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi and thank you for using SageMaker!\n\nCurrently, Git repositories are checked out after Lifecycle Configuration scripts are executed, so unfortunately they will not have access to them. We are always considering new features and functionality, so I've added this as a feature request.\n\nIn the mean time, I have two recommendations to workaround this limitation:\n\nManually clone or download the requirements.txt files from within your Lifecycle Configuration.\nFrom your Lifecycle Configuration, create a cron job or similar background process that waits until the Git Clone operation is complete.\n\nLet me know if that helps!\n\nBest,\nKevin",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-10-01T12:18:00.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hey there, sorry for this late reply.\nThanks much, the workarounds would help!",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Linear Learner - Multiclass classification - Prediction Scores",
        "Question_creation_time":1553707649000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBCgmx93mT92w6lw03c8Sqg\/linear-learner-multiclass-classification-prediction-scores",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":26,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\nI am using the linear learner algo for multiclass classification with 5 classes and 9 features\n\nI'm trying to figure out what the scores relate to? After getting the predicted_label result I get 5 scores....eg....\n\n{\"score\": [0.0003054474655073136, 0.8110334873199463, 0.0521857813000679, 0.1320497989654541, 0.004425559192895889], \"predicted_label\": 1.0}\n\nor\n\n{\"score\": [0.0001415203878423199, 0.11968196928501129, 0.8732749223709106, 0.006143826059997082, 0.0007577905780635774], \"predicted_label\": 2.0}\n\nDoes each score relate to a class? I have tried searching the internet for an explanation but so far cannot find anything, hopefully someone can enlighten me\n\nRegards",
        "Answers":[
            {
                "Answer_creation_date":"2019-03-29T17:37:24.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks for using SageMaker. The scores returned by multiclass models are the predicted probabilites for each class.\n\nIn your first example, the model is predicting class 1 with 81% confidence, and in your second example, the model is predicting class 2 with 87% confidence.\n\nIn general, to interpret the score from classification models, you have to consider the loss function. If the loss is logistic for binary classification or softmax_loss for multiclass classification, then the score can be interpreted as a probability. These are the default losses used by Linear Learner. But if the loss is hinge_loss, then the score cannot be interpreted as a probability. This is because hinge loss corresponds to a Support Vector Classifier, which does not produce probability estimates.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-03-30T21:39:37.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Great, many thanks for the explanation",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Unable to create endpoint",
        "Question_creation_time":1553516561000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUySs_fgNpSE6wuY-6W7MwqQ\/unable-to-create-endpoint",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":301,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi,\n\nI am new to SageMaker and I am trying to deploy my model to an endpoint but am getting the following error:\n\nFailure reason\nUnable to locate at least 2 availability zone(s) with the requested instance type ml.t2.medium that overlap with SageMaker subnets\n\nI have tried using different instance types but always the same error\n\nI was under the impression that SageMaker will create the required instances for me and I do not need to create the instances first? I am using the EU-WEST-1 zone and using the console to setup the endpoint",
        "Answers":[
            {
                "Answer_creation_date":"2019-03-25T23:31:36.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nSagemaker engineer here. I looked at the VpcConfig of your model and found only one subnet configured.\n\nThe error message \"Unable to locate at least 2 availability zone(s) with the requested instance type XYZ that overlap with SageMaker subnets\" usually indicates misconfigured VPCs. Sagemaker imposes mandatory requirement for at least 2 availability zones in your VPC subnets even if you only request one instance, to account for the potential use of auto-scaling in the future.\n\nIn order to create the endpoint, the number of subnets in your model needs to be at least 2 in distinct availability zones, and ideally as close to the total number of availability zones as possible in the region.\n\nHope it helps,\nWenzhao",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Ground Truth - Error loading Image for Task",
        "Question_creation_time":1552494750000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfa0f2oL7SY2hRFJ05dRC6Q\/ground-truth-error-loading-image-for-task",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":82,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\n\nI am creating a custom labeling job using boto3. It creates the job and the user is able to access the job using the front end tool but it does not load the images. I am using the default pre labeling lambda functions (arn:aws:lambda:us-east-1:432418664414:function:PRE-SemanticSegmentation) and the default for the annotation consolidation lambda (arn:aws:lambda:us-east-1:432418664414:function:ACS-SemanticSegmentation). I think the issue is with the name I am using for the LabelAttributeName (custom-ref) but I am not sure.\n\nFull error: There was an error loading the image for the task\nThis task was created incorrectly and cannot be completed at this time. Please return\/stop this task.\n\nAny help would be appreciated.\n\nThanks,\nHumberto",
        "Answers":[
            {
                "Answer_creation_date":"2019-03-13T18:15:54.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"The template I was using was missing the grant_read_access to the task.input.taskObject.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Image Classification Algorithm Class Activation Map",
        "Question_creation_time":1552316346000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUdCD7nXuTJegFm_lGunPzw\/image-classification-algorithm-class-activation-map",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":29,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hey!\n\nI am using SageMaker's built in image classification algorithm and currently have a trained model that can make predictions. I am wondering if there is an easy way to use this model to make a class activation map to better debug my classifier and see why it is making the predictions it is?\n\nThanks!",
        "Answers":[
            {
                "Answer_creation_date":"2019-03-14T17:05:21.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Steveleo,\n\nThanks for trying out the image classification algorithm. The trained model from SageMaker built-in image classification is a standard MXNet model. Here is a notebook https:\/\/github.com\/dmlc\/mxnet-notebooks\/blob\/master\/python\/how_to\/predict.ipynb that you can follow to extract features.\n\nThanks,\nXiong",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-03-14T20:59:33.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"For the class activation map, please check this http:\/\/mxnet.incubator.apache.org\/versions\/master\/tutorials\/vision\/cnn_visualization.html .",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to deploy pre-trained model?",
        "Question_creation_time":1551517462000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_grAySy_Ra6hAsekjf_tlQ\/how-to-deploy-pre-trained-model",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":51,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I have a pre-trained model that will translate text from English to Marathi. You can find it here...\ngit clone https:\/\/github.com\/shantanuo\/Word-Level-Eng-Mar-NMT.git\n\nClone and Run the notebook. I am looking for a way to deploy it so that users can use it as an API\n\nThe guidelines for deploying the model can be found here...\nhttps:\/\/gitlab.com\/shantanuo\/dlnotebooks\/blob\/master\/sagemaker\/01-Image-classification-transfer-learning-cifar10.ipynb\n\nI will like someone to suggest me the steps to follow to deploy the model.",
        "Answers":[
            {
                "Answer_creation_date":"2019-03-05T20:49:32.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Shantanu Oak,\n\nThe basic idea to deploy model to SageMaker is: 1) Containerize your model. 2) Publish your model to ECR repository and grant SageMaker necessary permissions. 3) Call CreateModel, CreateEndpointConfig and CreateEndpoint to deploy your model to SageMaker.\n\nPer your notebook of training the model, you didn't use any SageMaker sdk to containerize your model automatically, so it is more complicated to start from scratch. You may consider use any of the following sample notebooks with keras to containerize your model first: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"invoke_endpoint error in Lambda: StreamingBody is not JSON serializable",
        "Question_creation_time":1550644604000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6wwr5n1XTuWtQjkbCGbCOA\/invoke-endpoint-error-in-lambda-streaming-body-is-not-json-serializable",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":763,
        "Question_answer_count":5,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm writing a Lambda function that invokes an endpoint:\n\nruntime= boto3.Session().client('runtime.sagemaker')\r\npayload = {\"data\": [\"McDonalds\"]}\r\nresponse = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\r\n                                       ContentType='application\/json',\r\n                                       Body=json.dumps(payload))\n\n\nIt returns this error\n\nAn error occurred during JSON serialization of response: <botocore.response.StreamingBody object at 0x7f59e40acc50> is not JSON serializable\n\n\nI tried this exact function in SageMaker notebook and it works but it doesn't work in Lambda. Can someone please help me?\n\nEdited by: aurelius on Feb 19, 2019 10:38 PM",
        "Answers":[
            {
                "Answer_creation_date":"2019-12-20T09:29:36.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Was this because I didn't attach the necessary SageMaker policies to the IAM role? I only added this policy:\n\n{\r\n    \"Version\": \"2012-10-17\",\r\n    \"Statement\": [\r\n        {\r\n            \"Sid\": \"VisualEditor0\",\r\n            \"Effect\": \"Allow\",\r\n            \"Action\": \"sagemaker:InvokeEndpoint\",\r\n            \"Resource\": \"*\"\r\n        }\r\n    ]\r\n}",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-12-20T11:03:09.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi aurelius,\nDo you have further code after the response = runtime.invoke_endpoint(..) line? the error message says problem with serialization of the response object.\n\nYour role permission looks fine.\n\nThank you,\nArun",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-12-18T12:10:27.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I am facing similar issue, in sagemaker jupyter notebook instance the endpoint is invoked successfully and I am able to get back the inference results.\n\nBelow is my Lambda function that i used to invoke the endpoint, but I am facing the following error,\n\nimport json \r\nimport io\r\nimport boto3 \r\n\r\nclient = boto3.client('runtime.sagemaker')\r\n\r\ndef lambda_handler(event, context):\r\n    print(\"Received event: \" + json.dumps(event, indent=2))\r\n    \r\n    #data = json.loads(json.dumps(event))\r\n    #payload = data['data']\r\n    print(json.dumps(event))\r\n    \r\n    response = client.invoke_endpoint(EndpointName='linear-learner-2019-12-12-16-20-56-788',\r\n                                  ContentType='application\/json',\r\n                                  Body=(json.dumps(event)))\r\n    return response\n\n\nOutput:\n\nFunction Logs:\r\nSTART RequestId: 7f4c7589-b70f-4af8-834c-89a1a1fbe5e5 Version: $LATEST\r\nReceived event: {\r\n  \"instances\": [\r\n    {\r\n      \"features\": [\r\n        0.1,\r\n        0.2\r\n      ]\r\n    }\r\n  ]\r\n}\r\n{\"instances\": [{\"features\": [0.1, 0.2]}]}\r\nAn error occurred during JSON serialization of response: <botocore.response.StreamingBody object at 0x7f53918e2828> is not JSON serializable\n\n\nPlease help me resolve the issue, you can see the JSON input passed to the function. Not sure what is going wrong here. I even checked the cloudwatch logs not able to identify the origin of the issue.\n\nThanks in advance,\nArun\n\nEdited by: NMAK on Dec 18, 2019 5:54 AM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-02-22T01:07:19.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I call Sagemaker from Lambda using a slightly different approach in terms of data structures:\n\nfinal_data = ','.join(ordered_data.iloc[0].astype(str).values.tolist())\r\nruntime = boto3.client('runtime.sagemaker')\r\nresponse = runtime.invoke_endpoint(EndpointName='whatever-endpoint', \r\n                                           ContentType='text\/csv',\r\n                                           Body=final_data)\r\nresult = json.loads(response['Body'].read().decode())\n\n\nI start with a dataframe of one row containing all the data.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-02-20T22:36:48.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"hello Javierlopez,\n\nThanks for your response, I really appreciate it.\n\nI tried the below code and still it gives me same error\n\n    payload = ','.join(str(item) for item in data['instances'][0]['data']['features'])\r\n    #payload=bytearray(payload)\r\n    print(payload)\r\n    \r\n    response = client.invoke_endpoint(EndpointName='linear-learner-2019-12-12-16-20-56-788',\r\n                                  ContentType='text\/csv',\r\n                                  Body=payload)\r\n    return response\n\n\nbelow is output\/error for the above code.\n\nFunction Logs:\r\nSTART RequestId: 73daa03e-dec2-4d37-b779-72c1f70a7142 Version: $LATEST\r\nReceived event: {\r\n  \"instances\": [\r\n    {\r\n      \"data\": {\r\n        \"features\": [\r\n          0.1,\r\n          0.2\r\n        ]\r\n      }\r\n    }\r\n  ]\r\n}\r\n0.1,0.2 # this is the payload sent for inference.\r\nAn error occurred during JSON serialization of response: <botocore.response.StreamingBody object at 0x7f77555e59e8> is not JSON serializable\n\n\nI really don't understand the logic behind this on what format the function expects the input, my colleague ran a different model with same JSON format I used and it works fine. I believe you used this is inference format for XGBoost. I am not able to find any documentation on linear-learner sample inference requests using lambda.\n\nCould you please point me in the right direction.\n\nRegards,\nArun\n\nEdited by: NMAK on Dec 20, 2019 3:04 AM",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"What factors affect Sagemaker endpoint response time?",
        "Question_creation_time":1550614607000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUraUOZYBITMGxNh339cctOA\/what-factors-affect-sagemaker-endpoint-response-time",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":319,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"I'm using Sagemaker as part of a planned deployment of a XGBoost model to production where it will be called by a customer facing mobile app (via another back end service that we also have hosted in AWS).\n\nI would like to understand how to improve response times. I have tested the response time of my model both when it resides locally on my own dev machine, as well as when its running in Sagemaker.\n\nThe wall time for local atomic predictions takes about 1 ms at 50p and 7 ms at 99p.\n\nThe wall time for atomic predictions (using the Python client SDK in a sagemaker notebook) takes about 20 ms at 50p and 25 ms at 99p. However, there are outliers that take as long as ~300ms.\n\nI am curious to know what factors affect the performance of Sagemaker calls (other than the complexity of the model itself). And I would be very grateful for any tips to get our outliers lower (preferably around 50 ms if possible).",
        "Answers":[
            {
                "Answer_creation_date":"2019-02-19T23:48:45.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi bradmitchell,\nSageMaker Endpoints are managed hosted solution which has layered routing internally. You could gain some additional insights into the system with the OverheadLatency metric. You will also see the ModelLatency metric which will show what the time taken by customer model itself -- https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/monitoring-cloudwatch.html.\n\nWhat kind of TPS are you driving against your Endpoint? (In a low request rate situation, it is possible that the caches on our side are not kept warm and you might observe increased latencies). I believe the above metrics will help you with more details.\n\nThank you,\nArun",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-02-20T15:40:25.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Arun,\n\nThank you for suggesting the Cloudwatch metrics. I just checked the Overhead Latency and it lines up pretty closely with the average timings I got in a Sagemaker Jupyter Notebook using the Python client SDK to invoke the endpoint.\n\nCurrently, I am expecting about 2.5 transactions per second during peak usage and 0.5 TPS during low traffic hours. The peak number will hopefully go up to around 10 TPS later this year.\n\nThe cache explanation makes a lot of sense. I've experimented with a few different TPS settings. There is pretty consistently some spiking of timings to start off but then it levels off to around 20 ms for higher TPS. In lower TPS experiments I've noticed that the timings remain a little unpredictable.\n\nThank you again for your help!\n\nBrad",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Ground Truth - Progress Not Updating?",
        "Question_creation_time":1550529412000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUf9EnD7gzRlu86yy1Zlry7g\/ground-truth-progress-not-updating",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":1,
        "Question_view_count":117,
        "Question_answer_count":9,
        "Question_has_accepted_answer":false,
        "Question_body":"I've just configured my first Ground Truth labelling job, 1737 images assigned to a private workforce.\n\nWhen I first logged in as a worker I was able to label 10 images as a test run, and the Ground Truth \"Labeling job summary\" in the AWS Console shows 10\/1737 images labelled. So far so good.\n\nI then returned to the labelling job and worked through approx 50 images and then used the 'Stop working' button to finish my labelling session.\n\nDespite that work, the progress shown in \"Labelling job summary\" has not been updated with those additional labeled images, even 48+ hours after the work was done.\n\nHow can I confirm whether the image labelling is happening correctly? I don't see any error messages that might help debug the situation, and I certainly don't want to get other workers to label 1700+ images without being sure that the labelling data is being saved.",
        "Answers":[
            {
                "Answer_creation_date":"2019-02-20T21:01:35.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"To update my own question:\n\nI concluded that the job had somehow become corrupted, so I stopped it with the intention of starting a new one and seeing if the issue went away.\n\nDuring the \"stopping\" process the progress counter updated to show that the images I'd labelled had actually been saved, but by then it was too late and the job was queued for termination.\n\nI started a new job with the same settings (rebuilt the image manifest from scratch), and had the same issue with the initial 10 images being labelled and updated in the summary progress counter, but the images labelled in subsequent launches do not update the progress counter so there's no way of knowing how far long we are in the process and\/or checking the labelled areas of any images already labelled.\n\nIs this expected behaviour, or is this a bug?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-02-19T04:12:21.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Bump\n\nCan anyone using Ground Truth for labelling shed any light on whether this lack of status update is normal?\n\nHow do we keep track of progress on a job?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2021-06-28T16:02:52.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"BUMP AGAIN - I'll answer my own question incase anyone has the same issue.\n\nIt seems that Ground Truth just takes a very long time to update the progress of any labelling tasks. A job we created on Feb 23 has just shown progress on the Ground Truth console, so it seems to take about 5 days for any labelling work to be reflected in the console.\n\nI have no idea why this is the case, and I've had no response from AWS to my query, but at least we can be confident that our work is being saved and the labels are being generated.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-04-08T13:38:56.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi jamesatfish,\n\nI'm an AWS engineer working on SageMaker Ground Truth. I appreciate that you've been stuck with this for over a week, and I want to personally help make it better. It shouldn't take 5 days for a labeling task to be reflected in the console. We'd very much like to understand how you set up your labeling job so we can investigate it on our end. I sent you a private message on February 21st, but I'm replying here in the main thread in the hopes that you'll see this. Please feel free to send me a private message at your earliest convenience so we can discuss the particulars of your labeling job.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-04-08T22:42:06.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks Jonathan!\n\nApologies for not seeing your reply or PM earlier, I don't seem to have received a notification email to let me know of the reply.\n\nI have replied to your PM this morning with the ARN details, let me know if you need further background.\n\nAs an update to the thread, we spent a week labelling images in our most recently created task on the assumption that the updates to the task progress were just delayed. Unfortunately that was not the case, and we discovered this morning that Ground Truth has automatically cancelled our labelling job because \"no progress had been made\", causing us to lose most of the labelled images we had processed..",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-03-01T00:12:48.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi jamesatfish,\n\nI'm currently setting up a labeling job in Ground Truth and was hoping to avoid any pitfalls you came across through this process. Do you mind sharing what ended up being the resolutions to the issues you faced?\n\nThank you!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-03-01T21:28:59.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Naquent,\n\nHere's a quick summary of what we learned:\n\nGround Truth jobs are split into image batches. The first batch size in any job is 10 images, but after that the default for each batch is 1000 images. You need to get through a batch in entirety before Ground Truth will prepare the next batch.\n\nBy default Ground Truth gives you 4 days to complete each batch. If you fail to complete the batch within 4 days the entire Ground Truth job will be failed and any progress lost.\n\nYou cannot control those 2 variables via the AWS Admin Console, but if you create the Ground Truth job via the CLI you can control both, up to a maximum of 10 days for each batch to be valid. You can only set these at the creation of the job, not once it has been created.\n\nUsing the above, you'll need to find a suitable batch size that's a compromise between ensuring you get the batch completed and not being delayed at the end of each batch. If you're working on the labelling tasks as a full time endeavour or with a dedicated team then set a large batch size so the team can label as many images as possible before waiting for the next batch to generate. If you're labelling on an ad-hoc basis then set a smaller batch size to ensure you get through the batch before the timeout.\n\nAlways set the maximum 10 day timeout for each batch, there appears to be no penalty for doing so.\n\nAWS have changed the Ground Truth console output to more frequently update the progress of your labelling so you can see how many images have been labelled, but there's no way to see where you are up to in a batch or how long you have left to complete the current batch. If you've got a large labelling job you'll want to keep track of that yourself somehow.\n\nI hope that helps!\n\nJames\n\nEdited by: jamesatfish on Apr 9, 2019 8:42 AM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-03-07T21:47:02.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks so much for sharing, James! This is really good information to have.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-04-09T12:55:52.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I am facing the same issue. I created the labeling job and after submitting almost 24+ hours of tagged images the status did not change. then when I stooped the labeling job it updated the progress. How do I resolve this?",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Which CUDA version exists on p2.xlarge when I use it in Sagemaker?",
        "Question_creation_time":1550235092000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUL74ZyThTRlKKMEN4lYGJJQ\/which-cuda-version-exists-on-p-2-xlarge-when-i-use-it-in-sagemaker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":276,
        "Question_answer_count":5,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\nFrom my Host machine: Nvidia GPU GeForce GTX 1080 Ti, CUDA 9.0, Ubuntu 16.04:\n\nI have compiled my docker image with:\n\nnvidia cuda 9.0 cudnn 7 devel (base image)\ninstalled python2.7, darknet yolo (from PJReddie site)\nbuilt the darknet from source with options GPU=1, OPENMP=1, CUDNN=1\ndeployed Flask based RESTful service that gives prediction from Images (object labels and bounding boxes)\n\nOn this docker image:\n\ntrained the Darknet model with my images data and created model weights file (yolov2.backup)\ntested this model with test images, and it works\nensured that the darknet YOLO model utilizes the host machine's GPU\n\nThen to use this custom image in Sagemaker, for inference:\n\npushed docker image to ECR, created Model, Endpoint Config and Endpoint in Sagemaker\nconfig = p2.xlarge, min instance=1, max instance=1\nThe \/ping and \/invocations requests ran without error E2E.\nThe RESTful client code was Python code from Sagemaker's Jupyter Notebook\n\nIssue I am facing is:\n\nThere are no predictions generated during execution and my webservice returns an Image in response, when requested by Python's 'requests' API. The image that I get in response, does not show any object's bounding box. I feel that the CUDA version on P2 instance is not matching the CUDA9.0 that is in my docker image. For CUDA to run properly, it is required that the host machine and docker's CUDA version should be same.\n\nWhen I test my webservice and Darknet predictions inside my docker on my local Host machine, it runs fine, but not on Sagemaker!!\n\nQuestions:\n\nCan you please tell me which CUDA version exists on the p2.xlarge instance? So that I can build my docker accordingly.\nPlease suggest some way to print debug information in CloudWatch logs from my docker image so that I am aware what is going on inside the Docker on Sagemaker env. The Flask webservice code doesn't allow any print statements. Hence each time I need to debug, I am creating a new docker image. A good way could save my time.\n\nThanks",
        "Answers":[
            {
                "Answer_creation_date":"2019-02-22T06:07:25.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello SachinAWS,\n\nI believe that the CUDA version on the p2s should be CUDA 9.0. Our deep learning containers currently utilize CUDA 9.0 and I believe they run fine on the p2. Here is our GPU TensorFlow image that uses CUDA 9.0: https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/docker\/1.12.0\/final\/py2\/Dockerfile.gpu#L2\n\nI will reach out to the team that manages the host for your inferences on SageMaker to confirm the CUDA version.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-02-15T20:32:34.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"According to the team, the CUDA version is determined at the container level, while the drivers are installed on the host itself.\n\nAs for debugging, please consider using the Python SDK with local mode. This will spin up your docker container similar to that of serving within SageMaker. The benefit is that you will be able to utilize docker logs and also has the benefit of lower latency due to not waiting for your serving instances to be provisioned.\n\nhttps:\/\/github.com\/aws\/sagemaker-python-sdk#local-mode\nhttps:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_distributed_mnist\/tensorflow_local_mode_mnist.ipynb\n\nPlease let me know if there is anything to clarify.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-02-20T13:30:01.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Daniel,\nThank you for your response. Even though my issue is still unresolved, I am now trying out Model Training inside Sagemaker itself and use the generated model for inference. I strongly hope that it fixes the problem during the inference.\n\nFYI, a sample inference of Darknet YOLO model from Sagemaker is this:\n(Notice that it uses the model binary file, and neither prints prediction nor any error. The prediction time of 0.035261 seconds also indicates that the GPU has been utilized, further indicating not a Sagemaker problem):\n\nsample output:\n\nlayer     filters    size              input                output\r\n    0 conv     32  3 x 3 \/ 1   608 x 608 x   3   ->   608 x 608 x  32  0.639 BFLOPs\r\n    1 max          2 x 2 \/ 2   608 x 608 x  32   ->   304 x 304 x  32\r\n    2 conv     64  3 x 3 \/ 1   304 x 304 x  32   ->   304 x 304 x  64  3.407 BFLOPs\r\n    3 max          2 x 2 \/ 2   304 x 304 x  64   ->   152 x 152 x  64\r\n    4 conv    128  3 x 3 \/ 1   152 x 152 x  64   ->   152 x 152 x 128  3.407 BFLOPs\r\n    5 conv     64  1 x 1 \/ 1   152 x 152 x 128   ->   152 x 152 x  64  0.379 BFLOPs\r\n    6 conv    128  3 x 3 \/ 1   152 x 152 x  64   ->   152 x 152 x 128  3.407 BFLOPs\r\n    7 max          2 x 2 \/ 2   152 x 152 x 128   ->    76 x  76 x 128\r\n    8 conv    256  3 x 3 \/ 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs\r\n    9 conv    128  1 x 1 \/ 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs\r\n   10 conv    256  3 x 3 \/ 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs\r\n   11 max          2 x 2 \/ 2    76 x  76 x 256   ->    38 x  38 x 256\r\n   12 conv    512  3 x 3 \/ 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs\r\n   13 conv    256  1 x 1 \/ 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs\r\n   14 conv    512  3 x 3 \/ 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs\r\n   15 conv    256  1 x 1 \/ 1    38 x  38 x 512   ->    38 x  38 x 256  0.379 BFLOPs\r\n   16 conv    512  3 x 3 \/ 1    38 x  38 x 256   ->    38 x  38 x 512  3.407 BFLOPs\r\n   17 max          2 x 2 \/ 2    38 x  38 x 512   ->    19 x  19 x 512\r\n   18 conv   1024  3 x 3 \/ 1    19 x  19 x 512   ->    19 x  19 x1024  3.407 BFLOPs\r\n   19 conv    512  1 x 1 \/ 1    19 x  19 x1024   ->    19 x  19 x 512  0.379 BFLOPs\r\n   20 conv   1024  3 x 3 \/ 1    19 x  19 x 512   ->    19 x  19 x1024  3.407 BFLOPs\r\n   21 conv    512  1 x 1 \/ 1    19 x  19 x1024   ->    19 x  19 x 512  0.379 BFLOPs\r\n   22 conv   1024  3 x 3 \/ 1    19 x  19 x 512   ->    19 x  19 x1024  3.407 BFLOPs\r\n   23 conv   1024  3 x 3 \/ 1    19 x  19 x1024   ->    19 x  19 x1024  6.814 BFLOPs\r\n   24 conv   1024  3 x 3 \/ 1    19 x  19 x1024   ->    19 x  19 x1024  6.814 BFLOPs\r\n   25 route  16\r\n   26 conv     64  1 x 1 \/ 1    38 x  38 x 512   ->    38 x  38 x  64  0.095 BFLOPs\r\n   27 reorg              \/ 2    38 x  38 x  64   ->    19 x  19 x 256\r\n   28 route  27 24\r\n   29 conv   1024  3 x 3 \/ 1    19 x  19 x1280   ->    19 x  19 x1024  8.517 BFLOPs\r\n   30 conv     75  1 x 1 \/ 1    19 x  19 x1024   ->    19 x  19 x  75  0.055 BFLOPs\r\n   31 detection\r\nmask_scale: Using default '1.000000'\r\nLoading weights from backup\/yolov2.backup...Done!\r\n\/root\/darknet\/data2\/Test.png: Predicted in 0.035261 seconds.\n\n\nExpected output (addition to above o\/p):\n\nobject1: 66%\r\nobject1: 65%\r\nobject2: 56%\r\nobject3: 74%\r\nobject3: 61%\r\nobject4: 92%\n\n\nEdited by: SachinAws on Feb 20, 2019 5:31 AM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-03-04T07:14:16.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thank you for the answers",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-02-19T19:51:02.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Sharing my observations:\n\nThe Darknet Yolo Model is pretty dependent on the GPU env where it is trained. Means - the model generally gives inference\/predictions where it is trained.\nThe Darknet's yolo.cfg file contains few parameters that have to be enabled during training and testing (inference). The docker image should contain appropriate config parameters enabled during training and testing. Else the model does not give out any predictions, and neither prints any error.\n\nThank you.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to pass null\/Nan values into the dataframe passed into batch transform",
        "Question_creation_time":1549531218000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUtKq2ZetyTUmvDwS7i3ot2Q\/how-to-pass-null-nan-values-into-the-dataframe-passed-into-batch-transform",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":92,
        "Question_answer_count":1,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to make inference using my Xgboost model on a dataset which has NaN values, now inherently Xgboost handles the NaN values, it does not throw any error while training with NaN values whereas the batch transform job gives the error ''could not convert string to float'' when it encounters NaN values into the dataset that is to be transformed.\nCan anyone help me as to how can I pass NaN values into my input dataset for the batch transform job?\nThanks a ton!",
        "Answers":[
            {
                "Answer_creation_date":"2019-03-12T17:50:37.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi bharat-patidar,\n\nThanks for your interest in Amazon SageMaker XGBoost. We recently made a change to address empty values in the dataset. Now you should be able to pass empty strings or \"NaN\" strings with CSV data. Can you please verify and let us know if the change addresses the issue at your end?\n\nThanks!\nRegards,\nAmazon SageMaker Team",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"how to version step functions for ML?",
        "Question_creation_time":1549456954000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdG1tanW-TXy-vY0YrCsVeg\/how-to-version-step-functions-for-ml",
        "Question_topic":[
            "Serverless",
            "Application Integration",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "AWS Step Functions",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":230,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi, Step Functions can be used to create ML workflows. What is the best practice to version the code creating those workflows? boto3 code in CodeCommit? Something else?\n\nCheers Olivier",
        "Answers":[
            {
                "Answer_creation_date":"2019-02-06T18:38:37.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"A Step Functions state machine usually doesn't come alone and typically relies on other resources such as Lambda, EC2, DynamoDB, etc. You might want to package these dependent artifacts\/resources altogether within a version otherwise you might have a state machine that doesn't fully work (eg, state machine version doesn't match Lambda version). I guess the simplest way to achieve this is to provision these resources together as code (eg, CDK or CloudFormation) and store them in a Git repo. You could then use Git tags for versioning.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"S3 Dataset versioning with SageMaker?",
        "Question_creation_time":1549396058000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhYC1EJQuSWqpwTByAtB_fg\/s-3-dataset-versioning-with-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":386,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"Is there any standard for ML S3 dataset tracking or versioning? Basically, what setup allows to track a given model training execution to a given dataset? Interested to hear about proven or state-of-the-art ideas",
        "Answers":[
            {
                "Answer_creation_date":"2019-02-06T07:18:29.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Unfortunately, managing versions of datasets and which models used them is not embedded in SageMaker. But, you can use SageMaker search to manage the differences in data location between experiments. In that case, if your dataset isn't too big, my recommendation will be to create a standard for data structure in S3. i.e. for each new dataset, create a new prefix in S3 with your logic. Using SageMaker search you'll be able to find all your jobs and compare between datasets.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2022-09-28T12:51:12.007Z",
                "Answer_upvote_count":0,
                "Answer_body":"Nowadays, there are 3rd party tool that can be used alongside SageMaker. One example is Data Version Control (DVC), and we have discussed it how to integrate within SageMaker Processing jobs and SageMaker Training Jobs in this blogpost. As an alternative, you can leverage SageMaker Pipelines when your data preparation step is executed as a processing step within a pipeline execution. Pipelines allows you to achieve data versioning in a programmatic way by using execution-specific variables like ExecutionVariables.PIPELINE_EXECUTION_ID, which is the unique ID of a pipeline run. We can, for example, create a unique key for storing the output datasets in S3 that ties them to a specific pipeline run. We have also discussed this possibility as part of this blogpost.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Neo compilation load error",
        "Question_creation_time":1548810043000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUW8F63wxUTSCCdXf0MESXIg\/neo-compilation-load-error",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":24,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\n\nI tried to follow the documentation to run a Neo compilation job on AWS console. I downloaded the model from http:\/\/download.tensorflow.org\/models\/mobilenet_v1_2018_02_22\/mobilenet_v1_1.0_224.tgz and uploaded it to S3. I did everything else the same as the documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo-job-compilation-console.html. But I got this error:\n\"Load Error: InputConfiguration: Exactly one .pb file is allowed for Tensorflow models.\"\nI checked the tar.gz file that there is only one .pn file. What caused the error?\nThank you very much!",
        "Answers":[
            {
                "Answer_creation_date":"2019-08-04T20:30:32.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Well. My bad. \"Exactly one pb file\" means there can be only one file in the gz compressed file and that single file must have the format pb. I hope the example can make it more clear in the future.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-02-15T17:21:13.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks ac4289 ,\n\nI was facing the same error and then removing unnecessary files from tar file helped me solve my issue.",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Free tier usage Sagemaker",
        "Question_creation_time":1548335489000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUWzc5HuoeQZOuo79E8SQwxg\/free-tier-usage-sagemaker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":116,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi, I have a question about the free tier usage of AWS Sagemaker.\nIt says on the free tier page:\n\nAmazon SageMaker\n250 Hours\nper month of t2.medium notebook usage for the first two months\n\nDoes this mean that you have to start using Sagemaker right away when you start your account or your two months will be void? Or does this mean that the first two months you're using Sagemaker, up to 250 hours per month of t2.medium notebook usage is free?\n\nI really just want to test something but I have to know if I can still do this for free. I've had my account since september 2018 so >2 months.\n\nThe FAQ page that's referred to about expiring offers FREE TRIAL (What's the difference between expiring and non-expiring offers? VIEW FAQs >> https:\/\/aws.amazon.com\/free\/faqs\/?ft=n) doesn't give an answer to my question.\n\nThanks in advance.",
        "Answers":[
            {
                "Answer_creation_date":"2019-01-28T22:33:22.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-01-28T08:59:09.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello,\n\nSorry for any confusion caused!\n\nThe SageMaker trial begins after your first use of SageMaker - if you have not yet launched any SageMaker resources you will still be eligible for the 2 month trial period.\n\nWe can take a look at your account and advise directly if you create a support case: http:\/\/amzn.to\/aws-support-center.\n\nI hope this helps!\n\nWarmest Regards,\n-RachaelAWS",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Maximising multi-instance and multi-GPU utilisation",
        "Question_creation_time":1548267645000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJdZhEsN_QTWOcvWD7tfcLA\/maximising-multi-instance-and-multi-gpu-utilisation",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":9,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi,\n\nI've been trying to distribute the MNIST example across instances and GPUs with SageMaker Tensorflow, but I'm not seeing the kind of benefit that I was hoping for. I'm not sure whether I'm just setting the job up incorrectly or whether this example is just not suited to distribution and was wondering if anyone has any ideas which it might be please?\n\nI'm using TensorFlow 1.9 because I think MPI\/Horovod doesn't work with 1.12 which I was using originally?\n\nIn my test I get these results with a batch size of 512:\n1 Instance 1 GPU 13.5 global_step\/sec\n2 Instance 1 GPU 7.3 global_step\/sec\n1 Instance 8 GPU 18.8 global_step\/sec\n\nIf I reduce the batch size I get lower overall throughput, and see little benefit when increasing beyond 512.\n\nMy job specification looks as below and mnist.py is the file which comes with the examples, though I changed batch_size=100 to batch_size=512 in the script:\n\nestimator = TensorFlow(entry_point='mnist.py',\r\n                  role=role,\r\n                  framework_version='1.9.0',\r\n                  training_steps=1250, \r\n                  evaluation_steps=10,\r\n                  train_max_run=5*60,    \r\n                  output_path=output_location,\r\n                  checkpoint_path=output_location,\r\n                  code_location=output_location,\r\n                  model_dir=output_location,\r\n                  train_instance_count=1,\r\n                  train_instance_type='ml.p3.16xlarge',\r\n                  base_job_name='PerformanceTest-p3-16xlarge-1-instance',\r\n                  distributions={\r\n                    'mpi': {\r\n                      'enabled': True,\r\n                      'processes_per_host': 8,\r\n                      'custom_mpi_options': '--NCCL_DEBUG INFO'\r\n                    }\r\n                  })\n\n\nWhat I was hoping to see was a single 8-GPU instance hitting global_step\/sec of 70.2-97.2. Based on 8x the global_step\/sec of a single instance, scaled with 60-90% efficiency. Any help or clarification on this would be greatly appreciated!\n\nThanks,\n\nCarl",
        "Answers":[
            {
                "Answer_creation_date":"2019-01-24T21:50:04.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi,\n\nSorry that we are not clear in the doc.\n\nMPI\/Horovod support is only available since 1.12 in script mode. So according to your codes, if you use 1.9, then the mpi related parameter doesn't work. We will add version check in the codes.\n\nSo my recommendation for your test is,\n\nYou need to use 1.12 as explained above.\nMNIST may not be a good example for performance testing. It's pretty small.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-01-30T17:31:09.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Thanks for the reply. I'll try to rerun the experiment, I'll also try with a different network.\n\nCheers,\n\nCarl",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"ClientError: object_detection_augmented_manifest_training template",
        "Question_creation_time":1547563664000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjk2-AZ6VQl68Zdm1Owq-_A\/client-error-object-detection-augmented-manifest-training-template",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":52,
        "Question_answer_count":4,
        "Question_has_accepted_answer":true,
        "Question_body":"Hello,\n\nMy aim is to create a model for garden birds. I have 293 photos of birds that I have put through 2 custom labelling jobs in ground truth for training and validation. The issue I encountered was being able to have multiple labels on the bounding box which I managed to do via creating a custom labelling job with the following labels:\n\n<crowd-bounding-box\r\n    name=\"annotatedResult\"\r\n    labels=\"['Blackbird', 'Blue tit', 'Coal tit', 'Dunnock', 'Great tit', 'Long-tailed tit', 'Nuthatch', 'Pigeon', 'Robin']\" .....\n\n\nI now have 2 output manifest files with many lines of this:\n\n{\"source-ref\":\"s3:\/\/XXXXX\/Blackbird_1.JPG\",\"BirdLabel\":{\"workerId\":\"privateXXXXX\",\"imageSource\":{\"s3Uri\":\"s3:\/\/XXXXX\/Blackbird_1.JPG\"},\"boxesInfo\":{\"annotatedResult\":{\"boundingBoxes\":[{\"width\":1619,\"top\":840,\"label\":\"Blackbird\",\"left\":1287,\"height\":753}],\"inputImageProperties\":{\"width\":3872,\"height\":2592}}}},\"BirdLabel-metadata\":{\"type\":\"groundtruth\/custom\",\"job-name\":\"birdlabel\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-01-10T15:41:52+0000\"}}\n\n\nAfter this job was successful, I made an ml.p3.2xlarge instance, using the object_detection_augmented_manifest_training template.\n\nI have filled in the necessary sections, I then run it and received this error when I have the Content Type to 'application\/x-image' with Record wrapper type:RecordIO : 'ClientError: train channel is not specified.'\n\nI then changed the channel to train_annotation instead of train and I receive this error message: \"ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)\\n\\nCaused by: u'train' is a required property\n\nAdditional information can be provided if neccessary.\nAny help would be much apreciated! Thank you.\n\nEdited by: LuciA on Jan 16, 2019 1:12 PM\n\nEdited by: LuciA on Jan 16, 2019 1:18 PM\n\nEdited by: LuciA on Jan 16, 2019 1:19 PM",
        "Answers":[
            {
                "Answer_creation_date":"2019-02-07T18:20:27.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi LuciA - I'm an engineer at AWS. Thanks for continuing to try the service in the face of some difficulties. Can you please cross-reference your augmented manifest against the samples shown in https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-ground-truth-build-highly-accurate-datasets-and-reduce-labeling-costs-by-up-to-70\/, https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/object_detection_augmented_manifest_training\/object_detection_augmented_manifest_training.ipynb, and https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb?\n\nIt looks like your format is a little different, e.g., the algorithm expects to see keys called \"annotations\" and \"image_size\". Can you please check the syntax and let us know if your results change?",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2019-01-29T22:57:02.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi LuciA,\n\nThanks for trying out the SageMaker object detection algorithm. When using the Pipe mode with an AugmentedManifestFile, you need to specify the RecordWrapperType as RecordIO and ContentType as application\/x-recordio. Can you please retry with these suggestions and revert back if you continue to see issues.\n\nThanks!\nRegards,\nAmazon SageMaker Team",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-02-10T12:15:37.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi vrkhareataws\n\nThank you for getting back to me. I have tried the changes suggested and now recieve a new error message, 'InternalServerError: We encountered an internal error. Please try again.' I have added a snippet of code below for more insight:\n\ntraining_params = \\\r\n{\r\n    \"AlgorithmSpecification\": {\r\n        \"TrainingImage\": training_image, \r\n        \"TrainingInputMode\": \"Pipe\"\r\n    },\r\n    \"RoleArn\": role,\r\n    \"OutputDataConfig\": {\r\n        \"S3OutputPath\": s3_output_path\r\n    },\r\n    \"ResourceConfig\": {\r\n        \"InstanceCount\": 1,   \r\n        \"InstanceType\": \"ml.p3.2xlarge\",\r\n        \"VolumeSizeInGB\": 50\r\n    },\r\n    \"TrainingJobName\": job_name,\r\n    \"HyperParameters\": { \r\n         \"base_network\": \"resnet-50\",\r\n         \"use_pretrained_model\": \"1\",\r\n         \"num_classes\": \"1\",\r\n         \"mini_batch_size\": \"1\",\r\n         \"epochs\": \"5\",\r\n         \"learning_rate\": \"0.001\",\r\n         \"lr_scheduler_step\": \"3,6\",\r\n         \"lr_scheduler_factor\": \"0.1\",\r\n         \"optimizer\": \"rmsprop\",\r\n         \"momentum\": \"0.9\",\r\n         \"weight_decay\": \"0.0005\",\r\n         \"overlap_threshold\": \"0.5\",\r\n         \"nms_threshold\": \"0.45\",\r\n         \"image_shape\": \"300\",\r\n         \"label_width\": \"350\",\r\n         \"num_training_samples\": str(num_training_samples)\r\n    },\r\n    \"StoppingCondition\": {\r\n        \"MaxRuntimeInSeconds\": 86400\r\n    },\r\n    \"InputDataConfig\": [\r\n        {\r\n            \"ChannelName\": \"train\",\r\n            \"DataSource\": {\r\n                \"S3DataSource\": {\r\n                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\r\n                    \"S3Uri\": s3_train_data_path,\r\n                    \"S3DataDistributionType\": \"FullyReplicated\",\r\n                    \"AttributeNames\": [\"source-ref\",\"Bird-Label-Train\"] \r\n                }\r\n            },\r\n            \"ContentType\": \"application\/x-recordio\",\r\n            \"RecordWrapperType\": \"RecordIO\",\r\n            \"CompressionType\": \"None\"\r\n        },\r\n        {\r\n            \"ChannelName\": \"validation\",\r\n            \"DataSource\": {\r\n                \"S3DataSource\": {\r\n                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\r\n                    \"S3Uri\": s3_validation_data_path,\r\n                    \"S3DataDistributionType\": \"FullyReplicated\",\r\n                    \"AttributeNames\": [\"source-ref\",\"Bird-Label\"] \r\n                }\r\n            },\r\n            \"ContentType\": \"application\/x-recordio\",\r\n            \"RecordWrapperType\": \"RecordIO\",\r\n            \"CompressionType\": \"None\"\r\n        }\r\n    ]\r\n}\n\n\nI had run 2 seperate labelling jobs to get my training and validation set which have different names. \"Bird-Label\" for my validation and \"Bird-Label-Train\" for training. As you can see in the \"AttributeNames\" fields.\n\nI would also like to point out that my decision for using the image content type was due to this sentence, \"However you can also train in pipe mode using the image files (image\/png, image\/jpeg, and application\/x-image), without creating RecordIO files, by using the augmented manifest format.\" from this guide (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html).\n\nPlease let me know if you understand what is causing this error. Thank You!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-01-30T10:05:35.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi JonathanB-AWS\n\nThank you for sending me really useful links.\n\nHaving sorted through the information, I understood that my JSON was slightly different. Having gone back to basics, I created another labelling job, but used the 'Bouding Box' type as opposed to the 'Custom - Bounding box template'. My output matched what was expected. This ran with no errors!\n\nAs my purpose was to have multiple labels, I was able to edit the files and mapping of my output manifests, which also worked!\n\ni.e.\n\n{\"source-ref\":\"s3:\/\/xxxxx\/Blackbird_15.JPG\",\"ValidateBird\":{\"annotations\":[{\"class_id\":0,\"width\":2023,\"top\":665,\"height\":1421,\"left\":1312}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"ValidateBird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:23:51.174131\",\"type\":\"groundtruth\/object-detection\"}}\r\n{\"source-ref\":\"s3:\/\/xxxx\/Pigeon_19.JPG\",\"ValidateBird\":{\"annotations\":[{\"class_id\":2,\"width\":784,\"top\":634,\"height\":1657,\"left\":1306}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"ValidateBird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"2\":\"Pigeon\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:23:51.074809\",\"type\":\"groundtruth\/object-detection\"}}\n\n\nThe original mapping was 0:'Bird' for all images through the labelling job.\n\nThank you for your help!",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"How to use an Augmented Manifest File for AWS SageMaker Ground Truth?",
        "Question_creation_time":1546562747000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1LLbT-AYQDO-XXrjPUFl9w\/how-to-use-an-augmented-manifest-file-for-aws-sage-maker-ground-truth",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":98,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hey,\n\nI'm trying to use Ground Truth to do image classification but with a different set of label options for each image. I have the custom labeling task template and pre-\/post-labeling Lambda functions set up and I figured I could pass in the labels through the manifest file.\n\nMy issue is that the Ground Truth job ignores the attributes in the manifest file that are not \"source-ref\" (or \"source\"). This causes the pre-processing Lambda function to fail because the request it is passed only contains the \"source-ref\" attribute, but the Lambda function also references a different attribute. Are augmented manifest files supported for Ground Truth and if they are, how can I make use of the extra attributes?\n\nReferences:\nGround Truth Input Data: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html\nSageMaker Augmented Manifest Files: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/augmented-manifest.html\n\nExample:\n\nA normal Ground Truth manifest file:\n\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img1.png\"}\r\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img2.png\"}\r\n...\n\n\nWhat I want to be able to use:\n\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img1.png\",\"labels\":[\"pen\",\"pencil\",\"stick\"]}\r\n{\"source-ref\":\"s3:\/\/some_bucket\/images\/img2.png\",\"labels\":[\"tv\",\"laptop\",\"phone\"]}\r\n...",
        "Answers":[
            {
                "Answer_creation_date":"2019-01-07T19:20:40.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi sageuser, I'm an engineer at AWS. Augmented manifests are not supported for custom workflows, and so it is not possible to pass through additional parameters, e.g., \"labels\" in your example. We appreciate that you are using the service and welcome customer feedback. We can always be reached at https:\/\/aws.amazon.com\/contact-us\/.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"why SageMaker Ground Truth automated labelling?",
        "Question_creation_time":1544611168000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUp6wm80kUT0GZJp8meQtgTg\/why-sage-maker-ground-truth-automated-labelling",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":52,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi, I'm confused with the automated labelling feature of SM. Usually when people label things it is to train their own models afterwards. Is the goal of this feature to replace the downstream ML model that would use the labelled dataset? some sort of code free computer vision system?",
        "Answers":[
            {
                "Answer_creation_date":"2018-12-12T16:28:01.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Automated data labeling is labeling of data using machine learning. Amazon SageMaker Ground Truth will first select a random sample of data and send it to humans to be labeled. The results are then used to train a labeling model that attempts to label a new sample of raw data automatically. The labels are committed when the model can label the data with a confidence score that meets or exceeds a high threshold. Where the confidence score falls below this threshold, the data is sent to human labelers. Some of the data labeled by humans is used to generate a new training dataset for the labeling model, and the model is automatically retrained to improve its accuracy. This process repeats with each sample of raw data to be labeled. The labeling model becomes more capable of automatically labeling raw data with each iteration, and less data is routed to humans.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Accessing SageMaker Notebooks without accessing the console",
        "Question_creation_time":1543947347000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUp9lMw9-ESm-27BWY_RgCSg\/accessing-sage-maker-notebooks-without-accessing-the-console",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":240,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Is it possible to access SageMaker Notebooks without accessing the console?\n\nDo we have a best practice for that? In the create-presigned-notebook-instance-url command, what is the --session-expiration-duration-in-seconds: is it the validity duration of the URL or the max session duration once the URL has been clicked?",
        "Answers":[
            {
                "Answer_creation_date":"2018-12-04T19:28:56.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"I have experimented with CreatePresignedNotebookInstanceUrl a number of times. It returns an \"AuthorizedUrl\" string in the form: https:\/\/<notebook_instance_name>.notebook.<region>.sagemaker.aws?authToken=<a_very_long_string>\n\nI used the URL in another browser with no AWS console's session cookies (not logged in to the console) and it worked (could access my notebooks).\n\nThe parameter SessionExpirationDurationInSeconds is... well, exactly what it says, The number of seconds the presigned url is valid for. The API accepts a range of [1800, 43200] in seconds, which is equivalent to : 30 minutes to 12 hours .\n\nI hope this helps",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Which connection method when using SageMaker Notebook through VPC Interface Endpoint?",
        "Question_creation_time":1541494962000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5z-7bQ9zQOi_NrVlHy_5oA\/which-connection-method-when-using-sage-maker-notebook-through-vpc-interface-endpoint",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":280,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi, I see in this page of documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-interface-endpoint.html that:\n\n\"You can connect to your notebook instance from your VPC through an interface endpoint in your Virtual Private Cloud (VPC) instead of connecting over the internet. When you use a VPC interface endpoint, communication between your VPC and the notebook instance is conducted entirely and securely within the AWS network.\"\n\nHow would customer interact on their laptop with the UI of a notebook instance sitting in a VPC?",
        "Answers":[
            {
                "Answer_creation_date":"2018-11-06T15:02:57.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"If you are trying to access from within VPC, you'll have a direct connection. Otherwise, you'll need a configuration in place, such as Amazon VPN or AWS Direct Connect, to connect to your notebooks. Here is the blog post where we tried to explain how to set up AWS PrivateLink for Amazon SageMaker notebooks: https:\/\/aws.amazon.com\/blogs\/machine-learning\/direct-access-to-amazon-sagemaker-notebooks-from-amazon-vpc-by-using-an-aws-privatelink-endpoint\/",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker taking an unexpectedly long time to download training data",
        "Question_creation_time":1540384039000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPpqUS0ckRXCHW0BXgxV5wQ\/sagemaker-taking-an-unexpectedly-long-time-to-download-training-data",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":654,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"My customer's 220 Gb of training data took 54 minutes for Sagemaker to download. This is a rate of only 70 MB\/s, which is unexpectedly slow. He is accessing the data in S3 from his p3.8xlarge instance through a private VPC endpoint, so the theoretical maximum bandwidth is 25 Gbps. Is there anything that can be done to speed up the download?\n\nHe started the Sagemaker training with the following function:\n\nestimator = Estimator(image_name, role=role, output_path=output_location, train_instance_count=1, train_instance_type='ml.p3.8xlarge', train_volume_size=300, train_max_run = 52460*60 , security_group_ids='sg-00f1529adc4076841')\n\nThe output was: 2018-10-18 23:27:15 Starting - Starting the training job... Launching requested ML instances...... Preparing the instances for training... 2018-10-18 23:29:15 Downloading - Downloading input data............ .................................................................... .................................................................... .................................................................... 2018-10-19 00:23:50 Training - Downloading the training image..\n\nDataset download took ~54mins",
        "Answers":[
            {
                "Answer_creation_date":"2018-10-27T06:48:20.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"How are they connect to S3? are they using a VPC endpoint \/ NAT? If they are using a VPC endpoint, My recommendation will be the open a support ticket, it's possible that support will be able to look at the network logs.\n\nAnother option for the customer is to use pipe input, pipe mode is recommended for large datasets, and it'll shorter their startup time because the data is being streamed instead of being downloaded to your training instances.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"CloudFormation with SageMaker LifeCycleConfig without leaving the instance running",
        "Question_creation_time":1539775195000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU43NLxohAQvmSL3aH-KpPaw\/cloud-formation-with-sage-maker-life-cycle-config-without-leaving-the-instance-running",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":306,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I have a use case with SageMaker in which I want to create a notebook instance using CloudFormation. I have some initialization to do at creation time (clone a github repo, etc.). That all works fine. The only problem is that I would like to do this ahead of time in a set of accounts, and there doesn't appear to be any way to leave the newly-created instance in a Stopped state. A property in the CFT would be helpful in this regard.\n\nI tried using the aws cli to stop the instance from the lifecycle create script, but that fails as shown in the resulting CloudWatch logs:\n\nAn error occurred (ValidationException) when calling the StopNotebookInstance operation: Status (Pending) not in ([InService]). Unable to transition to (Stopping) for Notebook Instance (arn:aws:sagemaker:us-east-1:147561847539:notebook-instance\/birdclassificationworkshop).\n\n\n\nInterestingly, when I interactively open a notebook instance, open a terminal in the instance, and execute a \"stop-notebook-instance\" command, SageMaker is happy to oblige. I would have thought it would let me do the same in the lifecycle config. Unfortunately, SageMaker still has the notebook in the Pending state at that point, so \"stop\" is not permitted.\n\nAre there other hooks or creative options anyone can provide for me?",
        "Answers":[
            {
                "Answer_creation_date":"2018-10-17T11:53:11.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"One solutions will be to create a CFN custom resource backed by lambda. You can configure to run this resource only when the notebook resource completed. and use the lambda function to stop the notebook using one of our SDKs.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"SageMaker MXNet local mode not working",
        "Question_creation_time":1537534843000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQu1fDak6RL2wmivZ5UJwUw\/sage-maker-mx-net-local-mode-not-working",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":186,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi, I am trying to fit an MXNet model locally. I am adapting this https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/ and doing the following:\n\nbucket = 'XXXXXXXXXXX'\nprefix = 'sagemaker\/cifar-bench\/data'\n\ninputs = sagemaker_session.upload_data(\n    path='data',\n    bucket=bucket, \n    key_prefix=prefix)\n\nprint('data sent to ' + inputs)\n\n\nInception = MXNet('gluon_cifar_net.py', \n          role=role, \n          train_instance_count=1, \n          train_instance_type='local_gpu',\n          framework_version='1.2.1',\n          base_job_name='cifar10-inception-',\n          hyperparameters={'batch_size': 256, \n                           'optimizer': 'sgd',\n                           'epochs': 100, \n                           'learning_rate': 0.1, \n                           'momentum': 0.9})\n\n\nInception.fit(inputs)\n\n\nwhich returns an OSError: [Errno 2] No such file or directory\n\nIn the error log I can see that there seems to be error at self.latest_training_job = _TrainingJob.start_new(self, inputs) and self.sagemaker_client.create_training_job(**train_request)\n\nHow can I make the local mode work?",
        "Answers":[
            {
                "Answer_creation_date":"2018-09-21T17:24:55.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"It is very likely that you don't have docker-compose (or docker) installed in the box, that is why you are getting a No such file or directory.\n\nIf you want to use the GPU setup I would recommend running on a sagemaker notebook instance. Navigate to one of the example notebooks such as: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_cifar10\/mxnet_cifar10_local_mode.ipynb\n\nAnd run the setup.sh cell. This will install and configure all the docker dependencies correctly and then you should be able to use MXNet locally on GPU without any issue.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Problem Sagemaker and Spark",
        "Question_creation_time":1536801534000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3gi1LMvEQNiiGlYooXnUQA\/problem-sagemaker-and-spark",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":792,
        "Question_answer_count":2,
        "Question_has_accepted_answer":false,
        "Question_body":"Hi there,\n\nI followed this tutorial to set up Sagemaker Notebook with Spark (EMR): https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/\n\nI launched a notebook with sparkmagic (pyspark3) and tried to call the Spark context but got the following error:\n\"\"\"\nThe code failed because of a fatal error:\nInvalid status code '400' from http:\/\/xxx.xx.xx.xx:8998\/sessions with error payload: \"Invalid kind: pyspark3 (through reference chain: org.apache.livy.server.interactive.CreateInteractiveRequest[\"kind\"])\".\n\nSome things to try:\na) Make sure Spark has enough available resources for Jupyter to create a Spark context.\nb) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\nc) Restart the kernel.\n\"\"\"\n\nAnyone encountered the same issue?",
        "Answers":[
            {
                "Answer_creation_date":"2018-10-01T22:17:14.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hey,\n\nThanks for using SageMaker! This is an issue in pyspark3 with latest Livy. Starting with version 0.5.0-incubating, session kind \u201cpyspark3\u201d is removed, instead users require to set PYSPARK_PYTHON to python3 executable[1].\n\nSo there're two options:\n\nYou can switch to use pyspark kernel instead of pyspark3.\nYou can set PYSPARK_PYTHON variable in EMR's config file for spark: spark-env.sh\n[\n{\n\"Classification\": \"spark-env\",\n\"Configurations\": [\n{\n\"Classification\": \"export\",\n\"Configurations\": [],\n\"Properties\": {\n\"PYSPARK_PYTHON\": \"\/usr\/bin\/python3\"\n}\n}\n],\n\"Properties\": {}\n}\n]\n\nLet us know if you have any other question.\n\nThanks,\nHan\n\n[1]https:\/\/livy.incubator.apache.org\/docs\/latest\/rest-api.html#pyspark\n[2]https:\/\/docs.aws.amazon.com\/emr\/latest\/ReleaseGuide\/emr-spark-configure.html",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2019-01-31T21:37:35.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Submitting these EMR configuration options at the time of cluster creation worked for me:\n[\n{\n\"Classification\": \"spark-env\",\n\"Configurations\": [\n{\n\"Classification\": \"export\",\n\"Properties\": {\n\"PYSPARK_PYTHON\": \"\/usr\/bin\/python3\"\n}\n}\n]\n},\n{\n\"Classification\": \"yarn-env\",\n\"Properties\": {},\n\"Configurations\": [\n{\n\"Classification\": \"export\",\n\"Properties\": {\n\"PYSPARK_PYTHON\": \"\/usr\/bin\/python3\",\n}\n}\n]\n}\n]",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker and Data on Databases",
        "Question_creation_time":1533317474000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUh_P30-iXTKmzZv0D4vtLOA\/sagemaker-and-data-on-databases",
        "Question_topic":[
            "Storage",
            "Analytics",
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon Simple Storage Service",
            "AWS Data Pipeline",
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":208,
        "Question_answer_count":2,
        "Question_has_accepted_answer":true,
        "Question_body":"A customer has a question about data sources\n\n\u201cmost of our data is stored in SQL databases, while the SageMaker docs say that I have to put it all in S3. It\u2019s not obvious what the best way to do this is. I can think for example of splitting my analysis code in two; one pre-processing step to go from SQL queries to tabular data, and e.g. store that as Parquet files. For high-dimensional tensor data it\u2019s even less obvious.\u201d\n\nCan someone comment on that?",
        "Answers":[
            {
                "Answer_creation_date":"2018-08-03T17:52:46.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"We have an example notebook for interacting from Redshift data from a SageMaker managed notebook, which I believe is suitable for an Exploratory Data Analysis (EDA) use-case: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/working_with_redshift_data\/working_with_redshift_data.ipynb\n\nFor production purposes, the customer should consider separating the job of first extracting data from relational databases to S3 (to build out a data lake), and then using that for downstream processing\/machine learning (including SageMaker, EMR, Athena, Spectrum, etc.). Customers can build extraction pipelines from popular relational databases using AWS Glue, EMR, or their preferred ETL engines like those on the AWS Marketplace.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_date":"2022-02-12T01:50:00.007Z",
                "Answer_upvote_count":0,
                "Answer_body":"I'd recommend using SageMaker Data Wrangler to connects the dots of different SageMaker services. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/data-wrangler-import.html",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker local deployment: \"RuntimeError: Giving up, endpoint: didn't launch correctly\"",
        "Question_creation_time":1533133401000,
        "Question_link":"https:\/\/repost.aws\/questions\/QU54PWM3V9QoybCiO5GvB03g\/sagemaker-local-deployment-runtime-error-giving-up-endpoint-didnt-launch-correctly",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":311,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi, I am trying to launch an endpoint locally, to do couple inferences from my dev notebook (without having to wait for instanciation time of actual endpoint or batch training). I am running the following code:\n\n# get trained model from s3\ntrained_S2S = SM.model.Model(\n    image=seq2seq,\n    model_data=('s3:\/\/XXXXXXXXXXXXX\/'\n        + 'output\/seq2seq-2018-07-30-16-55-12-521\/output\/model.tar.gz'),\n    role=role) \n\nS2S = trained_S2S.deploy(1, instance_type='local')    \n\n\nI get the following error (several hundreds of lines repeated):\n\nWARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa5c61eaac8>: Failed to establish a new connection: [Errno 111] Connection refused',)': \/ping \n\n\nRuntimeError: Giving up, endpoint: seq2seq-2018-08-01-14-18-06-555 didn't launch correctly",
        "Answers":[
            {
                "Answer_creation_date":"2018-08-01T15:04:23.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Currently, SageMaker local is supported only for SageMaker framework containers (MXNet, TensorFlow, PyTorch, Chainer and Spark) and not for the Builtin algorithms",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker batch transform 415 error",
        "Question_creation_time":1532625720000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUr4Vq95ScROqSguzxNQYDOg\/sagemaker-batch-transform-415-error",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":424,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi, I need to run XGBoost inferences on 15MM samples (3.9Gb when stored as csv). Since Batch transform does not seem to work on such large batches (max payload 100MB) I split my input file into 646 files, each around 6Mb, stored in S3. I am running the code below:\n\ntransformer = XGB.transformer(\n    instance_count=2, instance_type='ml.c5.9xlarge',\n    output_path='s3:\/\/xxxxxxxxxxxxx\/sagemaker\/recsys\/xgbtransform\/',\n    max_payload=100)\n\ntransformer.transform(\n    data='s3:\/\/xxxxxxxxxxxxx\/sagemaker\/recsys\/testchunks\/',\n    split_type='Line')\n\n\nBut the job fails - Sagemaker tells \"ClientError: Too many objects failed. See logs for more information\" and cloudwatch logs show:\n\nBad HTTP status returned from invoke: 415\n'NoneType' object has no attribute 'lower'\n\n\nDid I forget something in my batch transform settings?",
        "Answers":[
            {
                "Answer_creation_date":"2018-07-26T19:42:05.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"This indicates that the algorithm thinks it has been passed bad data. Perhaps a problem with your splitting?\n\nI would suggest two things:\n\nTry running the algorithm on the original data using the \"SplitType\": \"Line\" and \"BatchStrategy\": \"MultiRecord\" arguments and see if you have better luck.\nLook in the cloudwatch logs for your run and see if there's any helpful information about what the algorithm didn't like. You can find these in the log group \"\/aws\/sagemaker\/TransformJobs\" in the log stream that begins with your job name.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Sagemaker batch transform",
        "Question_creation_time":1532619204000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlefH1ni4QOaulUT4870D5g\/sagemaker-batch-transform",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":254,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi, it seems that Sagemaker Batch Transform is limited to 100MB payloads I'd like to run preds against a 5GB csv file, what the recommended way to do so?",
        "Answers":[
            {
                "Answer_creation_date":"2018-07-26T18:02:46.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"SageMaker Batch Transform will automatically split your input file into whatever payload size is specified if you use \"SplitType\": \"Line\" and \"BatchStrategy\": \"MultiRecord\". There's no need to split files yourself or to use large payload sizes unless you have very large single records.\n\nHope that helps!",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Is there a way to install R libraries in SageMaker that receive a non-zero exit status?",
        "Question_creation_time":1527798496000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE-0c9SwxRViGhd-lAYtsuw\/is-there-a-way-to-install-r-libraries-in-sage-maker-that-receive-a-non-zero-exit-status",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":464,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"Hi all, I'm having an issue with an R kernel\/Jupyter notebook. I've come across two different libraries that result in the following error:\n\nWarning message in install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\"):\n\u201cinstallation of package \u2018XML\u2019 had non-zero exit status\u201dUpdating HTML index of packages in '.Library'\nMaking 'packages.html' ... done\n\n\nXML is the second package that I have run into this issue with. The other is rJava.\n\nI found a workaround that could work if I had root access, which is installing via the command line in a terminal. Which involves commands such as:\n\nyum install r-cran-rjava\n\n\nHowever, I don't have root access and cannot install as I get the message \"You need to be root to perform this command.\" So this workaround hasn't been possible.\n\nAfter checking the documentation for rJava and XML, I am running the requirements for JDK and other system requirements in SageMaker. This issue wasn't reproducible on a local RStudio environment. XML is a dependency for multiple R libraries (as is rJava). Is there a way that I can still install these packages?",
        "Answers":[
            {
                "Answer_creation_date":"2018-06-01T02:52:15.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"For Amazon SageMaker notebook Instances, you have the ability to assume root privileges, so instead of:\n\n$ yum install r-cran-rjava\n\nyou can try:\n\n$ sudo yum install r-cran-rjava\n\nwhich will allow you to impersonate the superuser (ie. root) for that command 1\n\nBut I don't believe that package exists in the available repos (ie. may be valid for another distro of Linux, but does not appear to be available in the yum repos -- running yum search 'r-cran-rjava' returned no results 2)\n\nInstead, from a prompt, install R + the necessary development files for later installation of R packages:\n\n$ sudo yum install -y R-java-devel.x86_64\n\nAnd finally, install the necessary XML libraries to support the XML package in R:\n\n$ sudo yum install -y libxml2-devel 3\n\nAfter which you can then open R (either as root user...)\n\n$ sudo R\n\nor personal\/local user\n\n$ R\n\nand execute the package installation:\n\n> install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\")\n\nEDITED TO FIX RJAVA PACKAGE INSTALLATION\n\nIt looks like the installation is requiring libgomp.spec\/libgomp.a files, so you can first find that file:\n\n$ sudo find \/ -iname libgomp.spec\n\nwhich should be located at \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec -- if so, you can manually create symlinks to fix this:\n\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec \/usr\/lib64\/\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.a \/usr\/lib64\/\n\n\nIf that ran correctly, you should now see both files in \/usr\/lib64 path:\n\n$ ls \/usr\/lib64\/libgomp*\n\nOnce confirmed, you can run the install.package('rJava') command.",
                "Answer_has_accepted":true
            }
        ]
    },
    {
        "Questiont_title":"Waiter TrainingJobCompletedOrStopped failed: Waiter encountered a terminal",
        "Question_creation_time":1526422450000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeYBDZTVwQlWovkq2eE1CkQ\/waiter-training-job-completed-or-stopped-failed-waiter-encountered-a-terminal",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker"
        ],
        "Question_upvote_count":0,
        "Question_view_count":401,
        "Question_answer_count":5,
        "Question_has_accepted_answer":false,
        "Question_body":"I am trying to launch a job using the low level api in boto3 sagemaker client. After calling sagemaker.create_training_job(**params) I try to get a waiter. This code is directly from the documentation for creating a training job (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html)\nI get this error:\n\nTraceback (most recent call last):\r\n  File \"traindeploy.py\", line 97, in create_training_job\r\n    sagemaker.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\r\n  File \"\/path\/to\/lib\/Python\/3.6\/lib\/python\/site-packages\/botocore\/waiter.py\", line 53, in wait\r\n    Waiter.wait(self, **kwargs)\r\n  File \"\/path\/to\/lib\/Python\/3.6\/lib\/python\/site-packages\/botocore\/waiter.py\", line 323, in wait\r\n    last_response=response,\r\nbotocore.exceptions.WaiterError: Waiter TrainingJobCompletedOrStopped failed: Waiter encountered a terminal failure state\n\n\nThese are my job params:\n\n{\r\n  \"AlgorithmSpecification\": {\r\n    \"TrainingImage\": \"<image-url-from-ecr>\",\r\n    \"TrainingInputMode\": \"File\"\r\n  },\r\n  \"RoleArn\": \"<role-arn>\",\r\n  \"OutputDataConfig\": {\r\n    \"S3OutputPath\": \"s3:\/\/path-to-bucket\/some-folder-output\/\"\r\n  },\r\n  \"ResourceConfig\": {\r\n    \"InstanceCount\": 2,\r\n    \"InstanceType\": \"ml.c4.8xlarge\",\r\n    \"VolumeSizeInGB\": 50\r\n  },\r\n  \"TrainingJobName\": \"some-jobname\",\r\n  \"HyperParameters\": {},\r\n  \"StoppingCondition\": {\r\n    \"MaxRuntimeInSeconds\": 3600\r\n  },\r\n  \"InputDataConfig\": [\r\n    {\r\n      \"ChannelName\": \"train\",\r\n      \"DataSource\": {\r\n        \"S3DataSource\": {\r\n          \"S3DataType\": \"S3Prefix\",\r\n          \"S3Uri\": \"s3:\/\/path-to-bucket\/some-folder-input\/\",\r\n          \"S3DataDistributionType\": \"FullyReplicated\"\r\n        }\r\n      },\r\n      \"CompressionType\": \"None\",\r\n      \"RecordWrapperType\": \"None\"\r\n    }\r\n  ]\r\n}\n\n\nCan someone please advise what is causing this and how will I get a waiter on a training job?",
        "Answers":[
            {
                "Answer_creation_date":"2018-05-17T23:27:35.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hello rks,\n\nYou received that error message because the waiter you created was expecting the training job to finish in either a Completed or Stopped state, but it reached the Failed state instead. This means there was an issue with your job and it could not be completed correctly.\n\nTo understand what caused your training job to fail, you can follow these steps:\n\nGo to the Amazon SageMaker console in your account\nClick on \"Jobs\" in the navigation bar to the left\nSearch for your training job in the list. You can use the \"Search jobs\" text form to quickly find the job by its name, or filter them by status.\nThe training job details should explain why it failed.\n\nIf you need more help with this issue, please don't hesitate to contact us.\n\nBest regards,\nAmazon SageMaker team",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2018-05-18T01:37:08.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Rodrigo,\n\nThanks for looking into this. Actually the training job does eventually succeed. I tried putting time.wait(seconds=N) before I get call get_waiter, but no matter what wait time I chose the waiter still failed giving the same exception. I waited 5 seconds, I waited 2 minutes, I even waited 3 minutes which was more than the time my training job took to complete successfully, but I always got the same exception.\n\nSo, I want to emphasize that there is nothing wrong with the training jobs I create. They succeed. But the get_waiter method always fails for me. What am I doing wrong here?\n\nEdited by: rks on May 17, 2018 4:34 PM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2018-05-17T22:39:01.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi rks,\n\nI'm sorry for the confusion, I misunderstood your issue. I attempted to reproduce it by running the example code myself in a SageMaker notebook, but I was able to run the job and the waiter worked correctly. For the record, I used the low-level KMeans MNIST sample notebook that comes bundled with all SageMaker notebooks. You can find it in \"\/sample-notebooks\/sagemaker-python-sdk\/1P_kmeans_lowlevel\/kmeans_mnist_lowlevel.ipynb\". That notebook should be very similar to the code you tried to run.\n\nCould you tell us more about how the environment you're running the job in? In particular, we'd like to know the version of Python and BOTO you're using. You can run the following commands to get them:\n\nimport sys, boto3\r\nprint(\"boto version = \" + boto3.__version__)\r\nprint(\"python version = \" + sys.version)\n\n\nThank you for your patience.\n\nRodrigo",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2018-06-12T02:41:29.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi Roberto,\n\nThank you for helping me out. This is the version of boto and python I am running:\n\n16:30 $ python3.6\r\nPython 3.6.4 (v3.6.4:d48ecebad5, Dec 18 2017, 21:07:28) \r\n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\nimport sys, boto3\r\nprint(\"boto version = \" + boto3.__version__)\r\n    boto version = 1.5.18\r\nprint(\"python version = \" + sys.version)\r\n    python version = 3.6.4 (v3.6.4:d48ecebad5, Dec 18 2017, 21:07:28) \r\n    [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]\n\n\nI'll review my code based on the notebook you've pointed out.\n\nEdited by: rks on May 18, 2018 4:35 PM\n\nEdited by: rks on May 18, 2018 4:35 PM\n\nEdited by: rks on May 18, 2018 4:36 PM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_date":"2018-05-18T23:34:52.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"Hi rks,\n\nWe have tried to troubleshoot this issue but it doesn't seem like it's reproducible. It's likely that the training-job name was invalid. Is this issue still occurring?\n\nThanks,\nIngrid",
                "Answer_has_accepted":false
            }
        ]
    },
    {
        "Questiont_title":"LightGBM on SageMaker",
        "Question_creation_time":1516632842000,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPwkZcylKQR6-u0pghgrseA\/light-gbm-on-sage-maker",
        "Question_topic":[
            "Machine Learning & AI"
        ],
        "Question_tag":[
            "Amazon SageMaker",
            "Machine Learning & AI"
        ],
        "Question_upvote_count":0,
        "Question_view_count":388,
        "Question_answer_count":1,
        "Question_has_accepted_answer":true,
        "Question_body":"I have a customer who wants to install LightGBM on SageMaker notebooks, as they are currently using it outside of SageMaker.\n\nRight now, they are interested in the ability to SSH into the instance, but it would be great if we could provide them a way to install LightGBM right now.\n\nCheers",
        "Answers":[
            {
                "Answer_creation_date":"2018-01-22T14:58:17.000Z",
                "Answer_upvote_count":0,
                "Answer_body":"It's possible to do, I have used it myself for gradient boosting, from within Jupyter you can simply run:\n\n!conda install -y -c conda-forge lightgbm\n\n\nWithin a selected conda environment. No terminal access is needed, however it must be done, On the top right of the Jupyter notebook you can choose a terminal environment which will give you a shell to the backend instance and you can install there.\n\nHowever if you want the notebook to be immutable\/transferable you can do the install within the notebook .\n\nThanks",
                "Answer_has_accepted":true
            }
        ]
    }
]