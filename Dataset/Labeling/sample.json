[
    {
        "Question_title":"Error while defining sampling algorithm in hyper parameter tuning using random sampling - Version V1",
        "Question_body":"<p>I am trying to perform the random sampling to accomplish the hyper parameter tuning and tuning parameter version 1 (v1). I would like to get the chance to define the algorithm as sampling algorithm explicitly.<\/p>\n<p>Currently using the below code block and is there any chance of implementing explicitly defining sampling in V1? If not, any specific procedure to solve the issue is helpful.<\/p>\n<pre><code>from azureml.train.hyperdrive import RandomParameterSampling\nfrom azureml.train.hyperdrive import normal, uniform, choice\nparam_sampling = RandomParameterSampling( {\n        &quot;learning_rate&quot;: normal(10, 3),\n        &quot;keep_probability&quot;: uniform(0.05, 0.1),\n        &quot;batch_size&quot;: choice(16, 32, 64, 128)\n    }\n)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1659096794043,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":34.0,
        "Answer_body":"<p>There is an explicit procedure called a <strong>sweep job<\/strong>. This sweep job in <strong>hyperparameter value<\/strong>. We can mention the random sampling using the sweep job explicitly.<\/p>\n<p>From azure.ai.ml.sweep import Normal, Uniform, RandomParameterSampling<\/p>\n<pre><code>Command_job_for_sweep = command_job(\n    learning_rate = Normal(mu=value, sigma=value),\n    keep_probability=Uniform(min_value= your value, max_value= value),\n    batch_size = Choice(value=[.your values in list]),\n)\n\nSweep_job = command_job_sweep.sweep(\n    Computer =\u201dcluster\u201d,\n    sampling_algorithm=\u201drandom\u201d,\n    ....\n)\n<\/code><\/pre>\n<p>This will be available in <strong>version 2 (v2)<\/strong> of hyperparameter tuning in random sampling.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73166561",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1659100027590,
        "Question_original_content":"error defin sampl algorithm hyper paramet tune random sampl version try perform random sampl accomplish hyper paramet tune tune paramet version like chanc defin algorithm sampl algorithm explicitli current code block chanc implement explicitli defin sampl specif procedur solv issu help train hyperdr import randomparametersampl train hyperdr import normal uniform choic param sampl randomparametersampl learn rate normal probabl uniform batch size choic",
        "Question_preprocessed_content":"error defin sampl algorithm hyper paramet tune random sampl version try perform random sampl accomplish hyper paramet tune tune paramet version like chanc defin algorithm sampl algorithm explicitli current code block chanc implement explicitli defin sampl specif procedur solv issu help",
        "Question_gpt_summary_original":"The user is encountering challenges while defining a sampling algorithm in hyper parameter tuning using random sampling. They are trying to explicitly define the algorithm as a sampling algorithm in version V1 but are facing errors. They have provided a code block and are seeking help to solve the issue.",
        "Question_gpt_summary":"user encount challeng defin sampl algorithm hyper paramet tune random sampl try explicitli defin algorithm sampl algorithm version face error provid code block seek help solv issu",
        "Answer_original_content":"explicit procedur call sweep job sweep job hyperparamet valu mention random sampl sweep job explicitli azur sweep import normal uniform randomparametersampl command job sweep command job learn rate normal valu sigma valu probabl uniform min valu valu max valu valu batch size choic valu valu list sweep job command job sweep sweep cluster sampl algorithm random avail version hyperparamet tune random sampl",
        "Answer_preprocessed_content":"explicit procedur call sweep job sweep job hyperparamet valu mention random sampl sweep job explicitli import normal uniform randomparametersampl avail version hyperparamet tune random sampl",
        "Answer_gpt_summary_original":"Solution: The discussion suggests using a sweep job in hyperparameter tuning to explicitly define the random sampling algorithm. The code block provided includes importing the necessary modules and defining the sampling algorithm using Normal, Uniform, and Choice functions. However, it is mentioned that this solution will only be available in version 2 (v2) of hyperparameter tuning in random sampling.",
        "Answer_gpt_summary":"solut discuss suggest sweep job hyperparamet tune explicitli defin random sampl algorithm code block provid includ import necessari modul defin sampl algorithm normal uniform choic function mention solut avail version hyperparamet tune random sampl"
    },
    {
        "Question_title":"Import error when using remote Azure Automated Machine Learning model in Azure notebook",
        "Question_body":"<p>I have trained an automated machine learning model on an Azure ML compute cluster.<\/p>\n\n<p>I am trying to use that remote model in my Azure hosted Jupyter notebook. <\/p>\n\n<p>This is the code in the workbook that tries to load the remote model:<\/p>\n\n<pre><code>remote_run = AutoMLRun(experiment = experiment, run_id = '... Experiment id ...')\nremote_best_run, remote_fitted_model = remote_run.get_output()\n<\/code><\/pre>\n\n<p>This code fails with the following error:<\/p>\n\n<blockquote>\n  <p>ModuleNotFoundError                       Traceback (most recent call\n  last)  in \n        2 # remote_run.wait_for_completion(show_output = True)\n        3 import pandas as pd\n  ----> 4 remote_best_run, remote_fitted_model = remote_run.get_output()\n        5 #!pip list<\/p>\n  \n  <p>~\/anaconda3_501\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/run.py\n  in get_output(self, iteration, metric)\n      406 \n      407         with open(model_local, \"rb\") as model_file:\n  --> 408             fitted_model = pickle.load(model_file)\n      409         return curr_run, fitted_model\n      410 <\/p>\n  \n  <p>ModuleNotFoundError: No module named 'pandas._libs.tslibs.timestamps'<\/p>\n<\/blockquote>\n\n<p>Presumably there is a version difference between what is installed on the Azure ML compute cluster vs what is installed in the kernel of the Jupyter notebook, or I have a package missing. <\/p>\n\n<p>How can I make this remote model work?<\/p>\n\n<p>For additional reference, I am following this tutorial: <a href=\"https:\/\/notebooks.azure.com\/NileshA\/projects\/GlobalAI\" rel=\"nofollow noreferrer\">https:\/\/notebooks.azure.com\/NileshA\/projects\/GlobalAI<\/a><\/p>\n\n<p><strong>Note 1<\/strong> I can also reproduce this error by running the following code in my jupyter notebook: <\/p>\n\n<pre><code>import pickle\n\nwith open('model.pkl', 'rb') as p_f:\n    data = pickle.load(p_f)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1544869470737,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1544870483248,
        "Question_score":0.0,
        "Question_view_count":353.0,
        "Answer_body":"<p>I emailed the Auto ML helpdesk and they solved the problem.<\/p>\n\n<p>Quote from them: <\/p>\n\n<blockquote>\n  <p>We have a bug where the AutoML inferencing fails because the pandas\n  version is 0.22.0 which doesn\u2019t have some API support.<\/p>\n<\/blockquote>\n\n<p>I upgraded pandas on my hosted notebook to version 0.23.4, and after this the model unpickles and works successfully<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53791461",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1545041889310,
        "Question_original_content":"import error remot azur autom machin learn model azur notebook train autom machin learn model comput cluster try us remot model azur host jupyt notebook code workbook tri load remot model remot run automlrun experi experi run experi remot best run remot fit model remot run output code fail follow error modulenotfounderror traceback recent remot run wait complet output true import panda remot best run remot fit model remot run output pip list anaconda lib python site packag train automl run output self iter metric open model local model file fit model pickl load model file return curr run fit model modulenotfounderror modul name panda lib tslib timestamp presum version differ instal comput cluster instal kernel jupyt notebook packag miss remot model work addit refer follow tutori http notebook azur com nilesha project globalai note reproduc error run follow code jupyt notebook import pickl open model pkl data pickl load",
        "Question_preprocessed_content":"import error remot azur autom machin learn model azur notebook train autom machin learn model comput cluster try us remot model azur host jupyt notebook code workbook tri load remot model code fail follow error modulenotfounderror traceback true import panda pip list iter metric return modulenotfounderror modul name presum version differ instal comput cluster instal kernel jupyt notebook packag miss remot model work addit refer follow tutori note reproduc error run follow code jupyt notebook",
        "Question_gpt_summary_original":"The user is encountering an import error when trying to use a remote Azure Automated Machine Learning model in their Azure hosted Jupyter notebook. The error message suggests a version difference or missing package between the Azure ML compute cluster and the Jupyter notebook kernel. The user is seeking a solution to make the remote model work.",
        "Question_gpt_summary":"user encount import error try us remot azur autom machin learn model azur host jupyt notebook error messag suggest version differ miss packag comput cluster jupyt notebook kernel user seek solut remot model work",
        "Answer_original_content":"email auto helpdesk solv problem quot bug automl inferenc fail panda version doesnt api support upgrad panda host notebook version model unpickl work successfulli",
        "Answer_preprocessed_content":"email auto helpdesk solv problem quot bug automl inferenc fail panda version doesnt api support upgrad panda host notebook version model unpickl work successfulli",
        "Answer_gpt_summary_original":"Solution: The user emailed the Auto ML helpdesk and they suggested upgrading pandas on the hosted notebook to version 0.23.4, which solved the problem.",
        "Answer_gpt_summary":"solut user email auto helpdesk suggest upgrad panda host notebook version solv problem"
    },
    {
        "Question_title":"Migration tutorials",
        "Question_body":"<p>is there any tutorial how to migrate from git lfs to dvc?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1608660663563,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":463.0,
        "Answer_body":"<p>I don\u2019t think so but that would be great to have! In case you want to make one.<\/p>\n<p>I can think of 2 strategies:<\/p>\n<p>i. Brute-force approach:<br>\n<a href=\"https:\/\/github.com\/git-lfs\/git-lfs\/issues\/910\">Remove Git-LFS<\/a> (or even Git itself) completely and start over with DVC.<\/p>\n<p>ii. Conversion<\/p>\n<ol>\n<li>Translate the Git-LFS pointer files into <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files\"><code>.dvc<\/code> files<\/a> (empty hash values)<\/li>\n<li>Download all the data from the Git-LFS server into the workspace<\/li>\n<li>\n<a href=\"https:\/\/dvc.org\/doc\/command-reference\/commit\"><code>dvc commit<\/code><\/a> and <code>git remove\/add\/commit<\/code> everything.<\/li>\n<\/ol>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/migration-tutorials\/601",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-22T20:30:24.457Z",
                "Answer_body":"<p>I don\u2019t think so but that would be great to have! In case you want to make one.<\/p>\n<p>I can think of 2 strategies:<\/p>\n<p>i. Brute-force approach:<br>\n<a href=\"https:\/\/github.com\/git-lfs\/git-lfs\/issues\/910\">Remove Git-LFS<\/a> (or even Git itself) completely and start over with DVC.<\/p>\n<p>ii. Conversion<\/p>\n<ol>\n<li>Translate the Git-LFS pointer files into <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files\"><code>.dvc<\/code> files<\/a> (empty hash values)<\/li>\n<li>Download all the data from the Git-LFS server into the workspace<\/li>\n<li>\n<a href=\"https:\/\/dvc.org\/doc\/command-reference\/commit\"><code>dvc commit<\/code><\/a> and <code>git remove\/add\/commit<\/code> everything.<\/li>\n<\/ol>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"migrat tutori tutori migrat git lf",
        "Question_preprocessed_content":"migrat tutori tutori migrat git lf",
        "Question_gpt_summary_original":"The user is seeking a tutorial on how to migrate from Git LFS to DVC.",
        "Question_gpt_summary":"user seek tutori migrat git lf",
        "Answer_original_content":"dont think great case want think strategi brute forc approach remov git lf git complet start convers translat git lf pointer file file hash valu download data git lf server workspac commit git remov add commit",
        "Answer_preprocessed_content":"dont think great case want think strategi approach remov complet start convers translat pointer file file download data server workspac",
        "Answer_gpt_summary_original":"Two strategies were suggested in the discussion for migrating from Git LFS to DVC. The first strategy is a brute-force approach which involves removing Git LFS completely and starting over with DVC. The second strategy involves translating the Git LFS pointer files into .dvc files, downloading all the data from the Git LFS server into the workspace, and using dvc commit and git remove\/add\/commit commands.",
        "Answer_gpt_summary":"strategi suggest discuss migrat git lf strategi brute forc approach involv remov git lf complet start second strategi involv translat git lf pointer file file download data git lf server workspac commit git remov add commit command"
    },
    {
        "Question_title":"Dvc error \"file not owned by user\"",
        "Question_body":"<p>Hi,<br>\nI am configuring dvc on shared system.<br>\nThe ubuntu system has multiple user ids as logins.<\/p>\n<p>The folder dataset1 is copied by one user  \u201cuser1\u201d.<br>\nNow when I am doing \u201cdvc add dataset1\u201d, its showing as<br>\n\u201cfile not owned by user\u201d.<\/p>\n<p>I have rwx permissions on the dataset1. I verified using getfacl.<br>\nCan anyone please guide how can i do \u201cdvc add\u201d without getting error. I dont want to do as root user as later during git push we would need username for tracking.<\/p>\n<p>Kindly help please.<\/p>\n<p>Thanks &amp; Regards<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1612845083215,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":261.0,
        "Answer_body":"<p><a class=\"mention\" href=\"\/u\/ruchita\">@Ruchita<\/a>, could you please share the traceback by running <code>dvc add -v<\/code>? Thanks.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-error-file-not-owned-by-user\/661",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-09T10:38:52.813Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/ruchita\">@Ruchita<\/a>, could you please share the traceback by running <code>dvc add -v<\/code>? Thanks.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"error file own user configur share ubuntu multipl user id login folder dataset copi user user add dataset show file own user rwx permiss dataset verifi getfacl guid add get error dont want root user later git push need usernam track kindli help thank regard",
        "Question_preprocessed_content":"error file own user configur share ubuntu multipl user id login folder dataset copi user user add dataset show file own user rwx permiss dataset verifi getfacl guid add get error dont want root user later git push need usernam track kindli help thank regard",
        "Question_gpt_summary_original":"The user is encountering an error while configuring dvc on a shared Ubuntu system with multiple user IDs. The error message \"file not owned by user\" appears when trying to add a folder copied by another user. The user has rwx permissions on the folder and wants to avoid using root user for tracking purposes during git push. The user is seeking guidance on how to resolve the error.",
        "Question_gpt_summary":"user encount error configur share ubuntu multipl user id error messag file own user appear try add folder copi user user rwx permiss folder want avoid root user track purpos git push user seek guidanc resolv error",
        "Answer_original_content":"ruchita share traceback run add thank",
        "Answer_preprocessed_content":"share traceback run thank",
        "Answer_gpt_summary_original":"Solutions provided: No solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut provid solut mention discuss"
    },
    {
        "Question_title":"How to deploy ML Designer pipeline as real-time inference pipeline using N-Gram",
        "Question_body":"Hi,\ni deployed a real-time inference pipeline using ML Designer. Training and deploying works fine. But when I'm consuming\/testing my API it doesn't work. Postman gives me Errorcode 500 and \"Internal Server Error. Run: Server internal error is from Module Extract N-Gram Features from Text\".\n\nThis is my training pipeline:\n\n\nI read this: https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/algorithm-module-reference\/extract-n-gram-features-from-text.md#score-or-publish-a-model-that-uses-n-grams\n\nBut I don't know how to achieve this.\n\nThanks in advance.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1606307286153,
        "Question_favorite_count":6.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Once you create a real-time inference pipeline, please make the further modifications below:\n\nFind the output Result_vocabulary dataset from Extract N-Gram Features from Text module.\n\n\n\nRegister the dataset as with a name\n\n\n\nUpdate real-time inference pipeline like below:\n\n\n\n\n\nWe will improve the documentation accordingly. Thanks for reporting the issue!",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/175242\/how-to-deploy-ml-designer-pipeline-as-real-time-in.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-26T05:06:13.387Z",
                "Answer_score":0,
                "Answer_body":"Once you create a real-time inference pipeline, please make the further modifications below:\n\nFind the output Result_vocabulary dataset from Extract N-Gram Features from Text module.\n\n\n\nRegister the dataset as with a name\n\n\n\nUpdate real-time inference pipeline like below:\n\n\n\n\n\nWe will improve the documentation accordingly. Thanks for reporting the issue!",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-04-28T06:35:33.063Z",
                "Answer_score":0,
                "Answer_body":"Hi @LuZhang-4441 do not see Output datasets to select for registering them. How should I proceed? I have also attached screenshot.\n\nInput datasets\nNone\n\nOutput datasets\nNone",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":1606367173387,
        "Question_original_content":"deploi design pipelin real time infer pipelin gram deploi real time infer pipelin design train deploi work fine consum test api work postman give errorcod intern server error run server intern error modul extract gram featur text train pipelin read http github com microsoftdoc azur doc blob master articl machin learn algorithm modul refer extract gram featur text score publish model us gram know achiev thank advanc",
        "Question_preprocessed_content":"deploi design pipelin infer pipelin deploi infer pipelin design train deploi work fine api work postman give errorcod intern server error run server intern error modul extract featur text train pipelin read know achiev thank advanc",
        "Question_gpt_summary_original":"The user is facing challenges in deploying a real-time inference pipeline using ML Designer. Although training and deploying the pipeline works fine, the API fails to work when tested, resulting in an internal server error. The error is related to the module \"Extract N-Gram Features from Text,\" and the user is seeking guidance on how to resolve the issue.",
        "Question_gpt_summary":"user face challeng deploi real time infer pipelin design train deploi pipelin work fine api fail work test result intern server error error relat modul extract gram featur text user seek guidanc resolv issu",
        "Answer_original_content":"creat real time infer pipelin modif output result vocabulari dataset extract gram featur text modul regist dataset updat real time infer pipelin like improv document accordingli thank report issu",
        "Answer_preprocessed_content":"creat infer pipelin modif output dataset extract featur text modul regist dataset updat infer pipelin like improv document accordingli thank report issu",
        "Answer_gpt_summary_original":"Solution: The discussion does not provide a clear solution to the issue. However, the user is advised to find the output Result_vocabulary dataset from the \"Extract N-Gram Features from Text\" module, register it with a name, and update the real-time inference pipeline accordingly. The discussion also mentions that the documentation will be improved based on the reported issue.",
        "Answer_gpt_summary":"solut discuss provid clear solut issu user advis output result vocabulari dataset extract gram featur text modul regist updat real time infer pipelin accordingli discuss mention document improv base report issu"
    },
    {
        "Question_title":"How to patch a multi-run downstream operation, for example tensorboard:multi-run",
        "Question_body":"From slack\n\nHi, simple Q. I want to launch a tensorboard with the tensorboard profiler pip installed from the GUI. At the moment I am using this:\n\nversion: 1.1\nkind: operation\nhubRef: tensorboard:multi-run\njoins:\n- query: \"uuid: XX\"\n  params:\n    uuids: {value: \"globals.uuid\"}\n\nHowever I want to runPatch it so that it installs:\n\npip install -U tensorboard-plugin-profile\n\nIs there a way to easily do this?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651747273000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"You can add the following runPatch:\n\n...\npatchStrategy: replace\nrunPatch:\n  container:\n    command: [\"bash\", \"-c\"]\n    args:\n      - \"pip install -U tensorboard-plugin-profile && tensorboard --logdir={{globals.artifacts_path}} --port={{globals.ports[0]}} --path_prefix={{globals.base_url}} --host=0.0.0.0\"\n\nFor the specific case of tensorboard, we will add a new input plugins of type List[str] to the tensorboard component versions , so instead of patching the component, users can pass a parameter:\n\nparams:\n  plugins: { value: [tensorboard-plugin-profile, tensorboard-plugin-custom, ...] }",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1502",
        "Tool":"Polyaxon",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-05T10:43:55Z",
                "Answer_score":1,
                "Answer_body":"You can add the following runPatch:\n\n...\npatchStrategy: replace\nrunPatch:\n  container:\n    command: [\"bash\", \"-c\"]\n    args:\n      - \"pip install -U tensorboard-plugin-profile && tensorboard --logdir={{globals.artifacts_path}} --port={{globals.ports[0]}} --path_prefix={{globals.base_url}} --host=0.0.0.0\"\n\nFor the specific case of tensorboard, we will add a new input plugins of type List[str] to the tensorboard component versions , so instead of patching the component, users can pass a parameter:\n\nparams:\n  plugins: { value: [tensorboard-plugin-profile, tensorboard-plugin-custom, ...] }"
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"patch multi run downstream oper exampl tensorboard multi run slack simpl want launch tensorboard tensorboard profil pip instal gui moment version kind oper hubref tensorboard multi run join queri uuid param uuid valu global uuid want runpatch instal pip instal tensorboard plugin profil wai easili",
        "Question_preprocessed_content":"patch downstream oper exampl slack simpl want launch tensorboard tensorboard profil pip instal gui moment version kind oper hubref join queri uuid param uuid want runpatch instal pip instal wai easili",
        "Question_gpt_summary_original":"The user is facing a challenge in patching a multi-run downstream operation, specifically in launching a tensorboard with the tensorboard profiler pip installed from the GUI. They are currently using a code but want to run a patch to install \"pip install -U tensorboard-plugin-profile\". The user is seeking advice on how to easily do this.",
        "Question_gpt_summary":"user face challeng patch multi run downstream oper specif launch tensorboard tensorboard profil pip instal gui current code want run patch instal pip instal tensorboard plugin profil user seek advic easili",
        "Answer_original_content":"add follow runpatch patchstrategi replac runpatch contain command bash arg pip instal tensorboard plugin profil tensorboard logdir global artifact path port global port path prefix global base url host specif case tensorboard add new input plugin type list str tensorboard compon version instead patch compon user pass paramet param plugin valu tensorboard plugin profil tensorboard plugin custom",
        "Answer_preprocessed_content":"add follow runpatch patchstrategi replac runpatch contain command arg pip instal tensorboard specif case tensorboard add new input plugin type list tensorboard compon version instead patch compon user pass paramet param plugin",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to the challenge by suggesting two possible ways to install \"pip install -U tensorboard-plugin-profile\". The first solution is to add a runPatch to the code, which includes the command to install the plugin and launch the tensorboard. The second solution is to add a new input plugin of type List[str] to the tensorboard component versions, so users can pass a parameter instead of patching the component.",
        "Answer_gpt_summary":"solut discuss provid solut challeng suggest possibl wai instal pip instal tensorboard plugin profil solut add runpatch code includ command instal plugin launch tensorboard second solut add new input plugin type list str tensorboard compon version user pass paramet instead patch compon"
    },
    {
        "Question_title":"Aws Sagemaker - ModuleNotFoundError: No module named 'cv2'",
        "Question_body":"<p>I am trying to run a object detection code in Aws. Although opencv is listed in the requirement file, i have the error &quot;no module named cv2&quot;. I am not sure how to fix this error. could someone help me please.<\/p>\n<p>My requirement.txt file has<\/p>\n<ul>\n<li>opencv-python<\/li>\n<li>numpy&gt;=1.18.2<\/li>\n<li>scipy&gt;=1.4.1<\/li>\n<li>wget&gt;=3.2<\/li>\n<li>tensorflow==2.3.1<\/li>\n<li>tensorflow-gpu==2.3.1<\/li>\n<li>tqdm==4.43.0<\/li>\n<li>pandas<\/li>\n<li>boto3<\/li>\n<li>awscli<\/li>\n<li>urllib3<\/li>\n<li>mss<\/li>\n<\/ul>\n<p>I tried installing &quot;imgaug&quot; and &quot;opencv-python headless&quot; as well.. but still not able to get rid of this error.<\/p>\n<pre><code>sh-4.2$ python train_launch.py \n[INFO-ROLE] arn:aws:iam::021945294007:role\/service-role\/AmazonSageMaker-ExecutionRole-20200225T145269\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_count has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\n2021-04-14 13:29:58 Starting - Starting the training job...\n2021-04-14 13:30:03 Starting - Launching requested ML instances......\n2021-04-14 13:31:11 Starting - Preparing the instances for training......\n2021-04-14 13:32:17 Downloading - Downloading input data...\n2021-04-14 13:32:41 Training - Downloading the training image..WARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n2021-04-14 13:33:03,970 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\n2021-04-14 13:33:05,030 sagemaker-containers INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {\n        &quot;training&quot;: &quot;\/opt\/ml\/input\/data\/training&quot;\n    },\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;unfreezed_epochs&quot;: 2,\n        &quot;freezed_batch_size&quot;: 8,\n        &quot;freezed_epochs&quot;: 1,\n        &quot;unfreezed_batch_size&quot;: 8,\n        &quot;model_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {\n        &quot;training&quot;: {\n            &quot;TrainingInputMode&quot;: &quot;File&quot;,\n            &quot;S3DistributionType&quot;: &quot;FullyReplicated&quot;,\n            &quot;RecordWrapperType&quot;: &quot;None&quot;\n        }\n    },\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;yolov4-2021-04-14-15-29&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;train_indu&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 8,\n    &quot;num_gpus&quot;: 1,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;train_indu.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2}\nSM_USER_ENTRY_POINT=train_indu.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[&quot;training&quot;]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=train_indu\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=8\nSM_NUM_GPUS=1\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{&quot;training&quot;:&quot;\/opt\/ml\/input\/data\/training&quot;},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;yolov4-2021-04-14-15-29&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;train_indu&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:8,&quot;num_gpus&quot;:1,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;train_indu.py&quot;}\nSM_USER_ARGS=[&quot;--freezed_batch_size&quot;,&quot;8&quot;,&quot;--freezed_epochs&quot;,&quot;1&quot;,&quot;--model_dir&quot;,&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;--unfreezed_batch_size&quot;,&quot;8&quot;,&quot;--unfreezed_epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_CHANNEL_TRAINING=\/opt\/ml\/input\/data\/training\nSM_HP_UNFREEZED_EPOCHS=2\nSM_HP_FREEZED_BATCH_SIZE=8\nSM_HP_FREEZED_EPOCHS=1\nSM_HP_UNFREEZED_BATCH_SIZE=8\nSM_HP_MODEL_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/lib\/python36.zip:\/usr\/lib\/python3.6:\/usr\/lib\/python3.6\/lib-dynload:\/usr\/local\/lib\/python3.6\/dist-packages:\/usr\/lib\/python3\/dist-packages\n\nInvoking script with the following command:\n\n\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2\n\n\nWARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n[name: &quot;\/device:CPU:0&quot;\ndevice_type: &quot;CPU&quot;\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 4667030854237447206\n, name: &quot;\/device:XLA_CPU:0&quot;\ndevice_type: &quot;XLA_CPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 3059419181456814147\nphysical_device_desc: &quot;device: XLA_CPU device&quot;\n, name: &quot;\/device:XLA_GPU:0&quot;\ndevice_type: &quot;XLA_GPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 6024475084695919958\nphysical_device_desc: &quot;device: XLA_GPU device&quot;\n, name: &quot;\/device:GPU:0&quot;\ndevice_type: &quot;GPU&quot;\nmemory_limit: 14949928141\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 13034103301168381073\nphysical_device_desc: &quot;device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5&quot;\n]\nTraceback (most recent call last):\n  File &quot;train_indu.py&quot;, line 12, in &lt;module&gt;\n    from yolov3.dataset import Dataset\n  File &quot;\/opt\/ml\/code\/yolov3\/dataset.py&quot;, line 3, in &lt;module&gt;\n    import cv2\nModuleNotFoundError: No module named 'cv2'\n2021-04-14 13:33:08,453 sagemaker-containers ERROR    ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n\n2021-04-14 13:33:11 Uploading - Uploading generated training model\n2021-04-14 13:33:54 Failed - Training job failed\nTraceback (most recent call last):\n  File &quot;train_launch.py&quot;, line 41, in &lt;module&gt;\n    estimator.fit(s3_data_path, logs=True, job_name=job_name) #the argument logs is crucial if you want to see what happends\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 535, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 1210, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 3365, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 2957, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job yolov4-2021-04-14-15-29: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1618407986697,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1218.0,
        "Answer_body":"<p>Make sure your estimator has<\/p>\n<ul>\n<li>framework_version = '2.3',<\/li>\n<li>py_version = 'py37',<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67093041",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1618410091900,
        "Question_original_content":"modulenotfounderror modul name try run object detect code aw opencv list requir file error modul name sure fix error help requir txt file opencv python numpi scipi wget tensorflow tensorflow gpu tqdm panda boto awscli urllib mss tri instal imgaug opencv python headless abl rid error python train launch info role arn aw iam role servic role amazon executionrol train instanc type renam http readthedoc stabl html detail train instanc count renam http readthedoc stabl html detail train instanc type renam http readthedoc stabl html detail start start train job start launch request instanc start prepar instanc train download download input data train download train imag warn tensorflow usr local lib python dist packag tensorflow core init estim input deprec us compat estim input instead contain info import framework tensorflow contain train contain info invok user script train env addit framework paramet channel input dir train opt input data train current host algo framework modul tensorflow contain train main host algo hyperparamet unfreez epoch freez batch size freez epoch unfreez batch size model dir dataset dataset yolo result yolov small yolov model input config dir opt input config input data config train traininginputmod file sdistributiontyp fullyrepl recordwrappertyp input dir opt input master true job yolov log level master hostnam algo model dir opt model modul dir dataset dataset yolo result yolov smal yolov sourc sourcedir tar modul train indu network interfac eth num cpu num gpu output data dir opt output data output dir opt output output intermedi dir opt output intermedi resourc config current host algo host algo network interfac eth user entri point train indu environ variabl host algo network interfac eth hp freez batch size freez epoch model dir dataset dataset yolo result yolov small yolov model unfreez batch size unfreez epoch user entri point train indu framework param resourc config current host algo host algo network interfac eth input data config train recordwrappertyp sdistributiontyp fullyrepl traininginputmod file output data dir opt output data channel train current host algo modul train indu log level framework modul tensorflow contain train main input dir opt input input config dir opt input config output dir opt output num cpu num gpu model dir opt model modul dir dataset dataset yolo result yolov smal yolov sourc sourcedir tar train env addit framework paramet channel input dir train opt input data train current host algo framework modul tensorflow contain train main host algo hyperparamet freez batch size freez epoch model dir dataset dataset yolo result yolov small yolov model unfreez batch size unfreez epoch input config dir opt input config input data config train recordwrappertyp sdistributiontyp fullyrepl traininginputmod file input dir opt input master true job yolov log level master hostnam algo model dir opt model modul dir dataset dataset yolo result yolov smal yolov sourc sourcedir tar modul train indu network interfac eth num cpu num gpu output data dir opt output data output dir opt output output intermedi dir opt output intermedi resourc config current host algo host algo network interfac eth user entri point train indu user arg freez batch size freez epoch model dir dataset dataset yolo result yolov small yolov model unfreez batch size unfreez epoch output intermedi dir opt output intermedi channel train opt input data train unfreez epoch freez batch size freez epoch unfreez batch size model dir dataset dataset yolo result yolov small yolov model pythonpath opt code usr local bin usr lib python zip usr lib python usr lib python lib dynload usr local lib python dist packag usr lib python dist packag invok script follow command usr bin python train indu freez batch size freez epoch model dir dataset dataset yolo result yolov small yolov model unfreez batch size unfreez epoch warn tensorflow usr local lib python dist packag tensorflow core init estim input deprec us compat estim input instead devic cpu devic type cpu memori limit local incarn devic xla cpu devic type xla cpu memori limit local incarn physic devic desc devic xla cpu devic devic xla gpu devic type xla gpu memori limit local incarn physic devic desc devic xla gpu devic devic gpu devic type gpu memori limit local bu link incarn physic devic desc devic tesla pci bu comput capabl traceback recent file train indu line yolov dataset import dataset file opt code yolov dataset line import modulenotfounderror modul name contain error executeuserscripterror command usr bin python train indu freez batch size freez epoch model dir dataset dataset yolo result yolov small yolov model unfreez batch size unfreez epoch upload upload gener train model fail train job fail traceback recent file train launch line estim fit data path log true job job argument log crucial want happend file home user anaconda env jupytersystemenv lib python site packag estim line fit self latest train job wait log log file home user anaconda env jupytersystemenv lib python site packag estim line wait self session log job self job wait true log type log file home user anaconda env jupytersystemenv lib python site packag session line log job self check job statu job descript trainingjobstatu file home user anaconda env jupytersystemenv lib python site packag session line check job statu actual statu statu except unexpectedstatusexcept error train job yolov fail reason algorithmerror executeuserscripterror command usr bin python train indu freez batch size freez epoch model dir dataset dataset yolo result yolov small yolov model unfreez batch size unfreez epoch",
        "Question_preprocessed_content":"modulenotfounderror modul name try run object detect code aw opencv list requir file error modul name sure fix error help file panda boto awscli urllib mss tri instal imgaug headless abl rid error",
        "Question_gpt_summary_original":"The user is encountering a \"ModuleNotFoundError\" error while trying to run an object detection code in AWS SageMaker. The error message indicates that the \"cv2\" module is missing, even though it is listed in the requirement file. The user has tried installing \"imgaug\" and \"opencv-python headless\" but the error persists. The error occurs during the training job and causes it to fail.",
        "Question_gpt_summary":"user encount modulenotfounderror error try run object detect code error messag indic modul miss list requir file user tri instal imgaug opencv python headless error persist error occur train job caus fail",
        "Answer_original_content":"sure estim framework version version",
        "Answer_preprocessed_content":"sure estim",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to the \"ModuleNotFoundError\" error encountered while running an object detection code in AWS SageMaker. The solution suggests ensuring that the estimator has the following parameters: framework_version = '2.3' and py_version = 'py37'.",
        "Answer_gpt_summary":"solut discuss provid solut modulenotfounderror error encount run object detect code solut suggest ensur estim follow paramet framework version version"
    },
    {
        "Question_title":"Switching between virtual Python environments within `cmd`",
        "Question_body":"<p>Hello,<\/p>\n<p>We are using Poetry to manage Python environments. It is generally possible to invoke a python script B to run in a given Poetry environment from within python program A. I have so far done this for instance via issuing shell commands from within a Python program. Like This:<\/p>\n<p>Given this structure with two differnt envs:<\/p>\n<p>folder_a<\/p>\n<ul>\n<li>prog_a.py<\/li>\n<li>pyproject.toml<\/li>\n<li>poetry.lock<\/li>\n<\/ul>\n<p>folder_b<\/p>\n<ul>\n<li>prog_b.py<\/li>\n<li>pyproject.toml<\/li>\n<li>poetry.lock<\/li>\n<\/ul>\n<pre><code class=\"lang-auto\"># within prog_a.py\nimport subprocess\ncmd = \"cd folder_b; poetry run python prog_b.py\"\nsubprocess.run(cmd, shell=True, check=True)                  \n<\/code><\/pre>\n<p>This worked without problem.<\/p>\n<p>Now, when I try the same from DVC it unfortunately does not work. Say I now have the following setup:<\/p>\n<p>folder_a<\/p>\n<ul>\n<li>dvc.yaml<\/li>\n<li>pyproject.toml   # has DVC installed<\/li>\n<li>poetry.lock<\/li>\n<\/ul>\n<p>folder_b<\/p>\n<ul>\n<li>prog_b.py<\/li>\n<li>pyproject.toml<\/li>\n<li>poetry.lock<\/li>\n<\/ul>\n<p>If I specify a <code>cmd<\/code> in a <code>dvc.yaml<\/code> like this: <code>cmd : \"cd abs\/path\/to\/folder_b; poetry run python prog_b.py\"<\/code>, then upon <code>dvc repro ...<\/code> I expected prog_b to be run in the Poetry env from folder_b. Instead it is run in the env from folder_a (where DVC is installed).<\/p>\n<p>There seems to be no way to escape this. I have even tried instead calling a wrapper through <code>cmd<\/code> that does the exact same subprocess call as in the first example (meaning it gets the full rest of the command as string, including the cd\u2026). Still, no luck, the env stays the same.<\/p>\n<p>Does anyone have any clue why this is the case? How does DVC execute <code>cmd<\/code>? In some special way that prevents running a process in a new env? I have tried a lot of variants of DVC \/ Poetry configs, to no avail.<\/p>\n<p>Alternatively I appreciate suggestions how else to deal with the need to execute different stages in different environments.<\/p>\n<p>Thanks lots and lots. <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>Best<\/p>\n<p>Jonas<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1639444412795,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":228.0,
        "Answer_body":"<p>After some more experimentation it appears that it has probably nothing to do with DVC, so please disregard the question about how DVC calls <code>cmd<\/code>.<\/p>\n<p>I\u2019d still be interested in alternative suggestions how to handle separate virtual environments for the stages in the context of DVC.<\/p>. <p>Hello <a class=\"mention\" href=\"\/u\/jonas\">@Jonas<\/a>!<\/p>\n<p>I am glad you found solution to your problem.<br>\nRegarding the execution from different envs in your stages:<br>\nI think there are few ways you could approach this problem:<\/p>\n<ol>\n<li>\n<p>Use particular python interpreter directly, eg:<br>\n<code>~\/.virtualenvs\/my_env\/bin\/python my_script.py<\/code><\/p>\n<\/li>\n<li>\n<p>You could leverage subshell, example for UNIX:<br>\n<code>(source ~\/.virtualenvs\/my_env\/bin\/activate &amp;&amp; which python)<\/code> - putting parentheses around command launches subshell which will be exited after execution<\/p>\n<\/li>\n<\/ol>\n<p>One thing to keep in mind is that we need a mechanism verifying if a particular env changed and run might need retriggering. You can specify the python directory as a dependency, but with bulky setups calculating checksums for that might take some time. I presume that you probably don\u2019t want to keep a virtualenv in DVC cache, so that might be a potential drawback.<\/p>. <p>Hi <a class=\"mention\" href=\"\/u\/paffciu\">@Paffciu<\/a>,<\/p>\n<p>thank you! I will try this for sure.<\/p>\n<p>Regarding the change monitoring of envs. I address this by automatically cloning version-tagged commits of the pipeline components, and defining the code as dependency. The code includes a poetry.lock file which defines the environment. So it is detected when this file changes (which incidentally happens only when the version changes).<\/p>. <p><a class=\"mention\" href=\"\/u\/paffciu\">@Paffciu<\/a> Hi! Just to wrap this up and report back. I am now calling the interpreter directly as you suggested in your first point. It works very nicely. It also feels more robust thann <code>cd<\/code>-ing to the dir and then running <code>poetry run ...<\/code>. Thanks again!<\/p>\n<p>(Side note: I saw that DVC also simply uses the subprocess library to call the commands, so there should be no difference in principle between DVC and calling processes from other Python programs.)<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/switching-between-virtual-python-environments-within-cmd\/1004",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-14T10:07:24.054Z",
                "Answer_body":"<p>After some more experimentation it appears that it has probably nothing to do with DVC, so please disregard the question about how DVC calls <code>cmd<\/code>.<\/p>\n<p>I\u2019d still be interested in alternative suggestions how to handle separate virtual environments for the stages in the context of DVC.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-14T12:14:14.860Z",
                "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/jonas\">@Jonas<\/a>!<\/p>\n<p>I am glad you found solution to your problem.<br>\nRegarding the execution from different envs in your stages:<br>\nI think there are few ways you could approach this problem:<\/p>\n<ol>\n<li>\n<p>Use particular python interpreter directly, eg:<br>\n<code>~\/.virtualenvs\/my_env\/bin\/python my_script.py<\/code><\/p>\n<\/li>\n<li>\n<p>You could leverage subshell, example for UNIX:<br>\n<code>(source ~\/.virtualenvs\/my_env\/bin\/activate &amp;&amp; which python)<\/code> - putting parentheses around command launches subshell which will be exited after execution<\/p>\n<\/li>\n<\/ol>\n<p>One thing to keep in mind is that we need a mechanism verifying if a particular env changed and run might need retriggering. You can specify the python directory as a dependency, but with bulky setups calculating checksums for that might take some time. I presume that you probably don\u2019t want to keep a virtualenv in DVC cache, so that might be a potential drawback.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-14T14:13:45.308Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/paffciu\">@Paffciu<\/a>,<\/p>\n<p>thank you! I will try this for sure.<\/p>\n<p>Regarding the change monitoring of envs. I address this by automatically cloning version-tagged commits of the pipeline components, and defining the code as dependency. The code includes a poetry.lock file which defines the environment. So it is detected when this file changes (which incidentally happens only when the version changes).<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-20T16:21:52.670Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/paffciu\">@Paffciu<\/a> Hi! Just to wrap this up and report back. I am now calling the interpreter directly as you suggested in your first point. It works very nicely. It also feels more robust thann <code>cd<\/code>-ing to the dir and then running <code>poetry run ...<\/code>. Thanks again!<\/p>\n<p>(Side note: I saw that DVC also simply uses the subprocess library to call the commands, so there should be no difference in principle between DVC and calling processes from other Python programs.)<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"switch virtual python environ cmd hello poetri manag python environ gener possibl invok python script run given poetri environ python program far instanc issu shell command python program like given structur differnt env folder prog pyproject toml poetri lock folder prog pyproject toml poetri lock prog import subprocess cmd folder poetri run python prog subprocess run cmd shell true check true work problem try unfortun work follow setup folder yaml pyproject toml instal poetri lock folder prog pyproject toml poetri lock specifi cmd yaml like cmd ab path folder poetri run python prog repro expect prog run poetri env folder instead run env folder instal wai escap tri instead call wrapper cmd exact subprocess exampl mean get rest command string includ luck env stai clue case execut cmd special wai prevent run process new env tri lot variant poetri config avail altern appreci suggest deal need execut differ stage differ environ thank lot lot best jona",
        "Question_preprocessed_content":"switch virtual python environ hello poetri manag python environ gener possibl invok python script run given poetri environ python program far instanc issu shell command python program like given structur differnt env work problem try unfortun work follow setup yaml instal specifi like expect run poetri env instead run env wai escap tri instead call wrapper exact subprocess exampl luck env stai clue case execut special wai prevent run process new env tri lot variant poetri config avail altern appreci suggest deal need execut differ stage differ environ thank lot lot best jona",
        "Question_gpt_summary_original":"The user is facing challenges in switching between virtual Python environments within `cmd`. They are using Poetry to manage Python environments and have successfully invoked a python script to run in a given Poetry environment from within a Python program. However, when they try to do the same from DVC, it does not work. The user has tried various configurations of DVC\/Poetry but has not been successful. They are seeking suggestions on how to deal with the need to execute different stages in different environments.",
        "Question_gpt_summary":"user face challeng switch virtual python environ cmd poetri manag python environ successfulli invok python script run given poetri environ python program try work user tri configur poetri success seek suggest deal need execut differ stage differ environ",
        "Answer_original_content":"experiment appear probabl disregard question call cmd interest altern suggest handl separ virtual environ stage context hello jona glad solut problem execut differ env stage think wai approach problem us particular python interpret directli virtualenv env bin python script leverag subshel exampl unix sourc virtualenv env bin activ python put parenthes command launch subshel exit execut thing mind need mechan verifi particular env chang run need retrigg specifi python directori depend bulki setup calcul checksum time presum probabl dont want virtualenv cach potenti drawback paffciu thank try sure chang monitor env address automat clone version tag commit pipelin compon defin code depend code includ poetri lock file defin environ detect file chang incident happen version chang paffciu wrap report call interpret directli suggest point work nice feel robust thann ing dir run poetri run thank note saw simpli us subprocess librari command differ principl call process python program",
        "Answer_preprocessed_content":"experiment appear probabl disregard question call interest altern suggest handl separ virtual environ stage context hello glad solut problem execut differ env stage think wai approach problem us particular python interpret directli leverag subshel exampl unix put parenthes command launch subshel exit execut thing mind need mechan verifi particular env chang run need retrigg specifi python directori depend bulki setup calcul checksum time presum probabl dont want virtualenv cach potenti thank try sure chang monitor env address automat clone commit pipelin compon defin code depend code includ file defin environ detect file chang wrap report call interpret directli suggest point work nice feel robust thann ing dir run thank note saw simpli us subprocess librari command differ principl call process python",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Use a particular Python interpreter directly.\n2. Leverage subshell.\n\nThe user also mentions addressing the change monitoring of environments by automatically cloning version-tagged commits of the pipeline components and defining the code as a dependency.",
        "Answer_gpt_summary":"possibl solut mention discuss us particular python interpret directli leverag subshel user mention address chang monitor environ automat clone version tag commit pipelin compon defin code depend"
    },
    {
        "Question_title":"How to automate DVC push",
        "Question_body":"<p>Hello<\/p>\n<p>I have started using DVC for version control in one of my projects and it\u2019s awesome.<\/p>\n<p>Currently, the model training job is executed from an amazon ec2 machine and the output is downloaded to my local machine which is pushed to the remote storage using the DVC push command.<\/p>\n<p>My doubt is, is it possible to programmatically push the model into the remote DVC storage (Amazon S3)? I couldn\u2019t find any helping articles online. Could you please suggest the right way of doing the same?<\/p>\n<p>Thanks in advance<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1612185766792,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":516.0,
        "Answer_body":"<p>You can automate this process with git hooks, provided by <code>dvc install<\/code>. See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/install\">https:\/\/dvc.org\/doc\/command-reference\/install<\/a><\/p>\n<p>So when you install this pre-push hook, it will run <code>dvc push<\/code> before every <code>git push<\/code>.<\/p>. <blockquote>\n<p>Note: alternative discussion in <a href=\"https:\/\/groups.google.com\/u\/1\/a\/iterative.ai\/g\/support\/c\/Bk43iyFOq4M\/m\/Y290XdCMDgAJ\">https:\/\/groups.google.com\/u\/1\/a\/iterative.ai\/g\/support\/c\/Bk43iyFOq4M\/m\/Y290XdCMDgAJ<\/a> (in case needed). Cheers<\/p>\n<\/blockquote>. <p>For the record, it looks like the <code>dvc import-url --to-remote<\/code> may satisfy this need. (WIP, currently in <a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/5198\">https:\/\/github.com\/iterative\/dvc\/pull\/5198<\/a>).<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-automate-dvc-push\/649",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-01T13:31:47.059Z",
                "Answer_body":"<p>You can automate this process with git hooks, provided by <code>dvc install<\/code>. See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/install\">https:\/\/dvc.org\/doc\/command-reference\/install<\/a><\/p>\n<p>So when you install this pre-push hook, it will run <code>dvc push<\/code> before every <code>git push<\/code>.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-01T19:59:27.315Z",
                "Answer_body":"<blockquote>\n<p>Note: alternative discussion in <a href=\"https:\/\/groups.google.com\/u\/1\/a\/iterative.ai\/g\/support\/c\/Bk43iyFOq4M\/m\/Y290XdCMDgAJ\">https:\/\/groups.google.com\/u\/1\/a\/iterative.ai\/g\/support\/c\/Bk43iyFOq4M\/m\/Y290XdCMDgAJ<\/a> (in case needed). Cheers<\/p>\n<\/blockquote>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-02T19:25:50.050Z",
                "Answer_body":"<p>For the record, it looks like the <code>dvc import-url --to-remote<\/code> may satisfy this need. (WIP, currently in <a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/5198\">https:\/\/github.com\/iterative\/dvc\/pull\/5198<\/a>).<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"autom push hello start version control project awesom current model train job execut amazon machin output download local machin push remot storag push command doubt possibl programmat push model remot storag amazon help articl onlin suggest right wai thank advanc",
        "Question_preprocessed_content":"autom push hello start version control project awesom current model train job execut amazon machin output download local machin push remot storag push command doubt possibl programmat push model remot storag help articl onlin suggest right wai thank advanc",
        "Question_gpt_summary_original":"The user is facing a challenge in automating the process of pushing their model into remote DVC storage (Amazon S3) programmatically. They are seeking guidance on how to achieve this as they have not found any helpful articles online.",
        "Question_gpt_summary":"user face challeng autom process push model remot storag amazon programmat seek guidanc achiev help articl onlin",
        "Answer_original_content":"autom process git hook provid instal http org doc command refer instal instal pre push hook run push git push note altern discuss http group googl com iter support bkiyfoqm yxdcmdgaj case need cheer record look like import url remot satisfi need wip current http github com iter pull",
        "Answer_preprocessed_content":"autom process git hook provid instal hook run note altern discuss cheer record look like satisfi need",
        "Answer_gpt_summary_original":"Solution: The user can automate the process of pushing their model into remote DVC storage (Amazon S3) programmatically using git hooks provided by \"dvc install\". They can install the pre-push hook, which will run \"dvc push\" before every \"git push\". Another possible solution is to use \"dvc import-url --to-remote\", which is currently a work in progress and can be found in the GitHub pull request.",
        "Answer_gpt_summary":"solut user autom process push model remot storag amazon programmat git hook provid instal instal pre push hook run push git push possibl solut us import url remot current work progress github pull request"
    },
    {
        "Question_title":"Does Amazon SageMaker XGBoost support parallel training across multiple machines?",
        "Question_body":"I'd like to set up Amazon SageMaker XGBoost to train datasets on multiple machines. Is that possible? If so, how?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1583496984000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":51.0,
        "Answer_body":"Yes, using Amazon SageMaker hosting with XGBoost allows you to train datasets on multiple machines.\n\nFor more information, see Docker registry paths and example code in the Amazon SageMaker developer guide.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOKZq2V_RQaaFzQkapcWpsA\/does-amazon-sage-maker-xg-boost-support-parallel-training-across-multiple-machines",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-03-08T08:06:27.000Z",
                "Answer_score":0,
                "Answer_body":"Yes, using Amazon SageMaker hosting with XGBoost allows you to train datasets on multiple machines.\n\nFor more information, see Docker registry paths and example code in the Amazon SageMaker developer guide.",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1583654787000,
        "Question_original_content":"xgboost support parallel train multipl machin like set xgboost train dataset multipl machin possibl",
        "Question_preprocessed_content":"xgboost support parallel train multipl machin like set xgboost train dataset multipl machin possibl",
        "Question_gpt_summary_original":"The user is seeking information on whether Amazon SageMaker XGBoost supports parallel training across multiple machines and how to set it up if possible.",
        "Question_gpt_summary":"user seek inform xgboost support parallel train multipl machin set possibl",
        "Answer_original_content":"ye host xgboost allow train dataset multipl machin inform docker registri path exampl code develop guid",
        "Answer_preprocessed_content":"ye host xgboost allow train dataset multipl machin inform docker registri path exampl code develop guid",
        "Answer_gpt_summary_original":"Solution: Yes, Amazon SageMaker hosting with XGBoost supports parallel training across multiple machines. The Amazon SageMaker developer guide provides information on Docker registry paths and example code to set it up.",
        "Answer_gpt_summary":"solut ye host xgboost support parallel train multipl machin develop guid provid inform docker registri path exampl code set"
    },
    {
        "Question_title":"Why does the Optuna CSV file only display 1 item per parameter when I have multiple?",
        "Question_body":"<p>I have created the following piece of code in Python in order to optimize my network using Optuna.<\/p>\n<pre><code>    activations_1 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_2 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_3 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_4 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu'])\n\nmodel = Sequential([\n            layers.Conv2D(filters=dict_params['num_filters_1'],\n                          kernel_size=dict_params['kernel_size_1'],\n                          activation=dict_params['activations_1'],\n                          strides=dict_params['stride_num_1'],\n                          input_shape=self.input_shape),\n            layers.BatchNormalization(),\n            layers.MaxPooling2D(2, 2),\n\n            layers.Conv2D(filters=dict_params['num_filters_2'],\n                          kernel_size=dict_params['kernel_size_2'],\n                          activation=dict_params['activations_2'],\n                          strides=dict_params['stride_num_2']),\n<\/code><\/pre>\n<p>As you can see, I made multiple activation trials instead of one because I wanted to see if the model produced better results when each layer had a different activation function. I did the same with other parameters as you can see. My confusion begins when I return the study.bestparams object:<\/p>\n<pre><code>{&quot;num_filters&quot;: 32, &quot;kernel_size&quot;: 4, &quot;strides&quot;: 1, &quot;activation&quot;: &quot;selu&quot;, &quot;num_dense_nodes&quot;: 64, &quot;batch_size&quot;: 64}\n<\/code><\/pre>\n<p>The best parameters from the trials produced only one parameter. It does not tell me where the parameter was used and also doesn't display the other 3 activation functions I used (or the other parameters for that matter). Is there a way to precisely display the best settings my model used and at which layers? (I am aware of saving the best model and model summary but this does not help me too much)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630949196040,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1631055883776,
        "Question_score":0.0,
        "Question_view_count":82.0,
        "Answer_body":"<p>The problem  is you used the same parameter name  for all activations. Instead of :<\/p>\n<pre><code>    activations_1 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_2 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_3 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_4 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu'])\n<\/code><\/pre>\n<p>Try:<\/p>\n<pre><code>    activations_1 = trial.suggest_categorical('activation1', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_2 = trial.suggest_categorical('activation2', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_3 = trial.suggest_categorical('activation3', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_4 = trial.suggest_categorical('activation4', ['relu', 'sigmoid', 'tanh', 'selu'])\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69078338",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1631093676416,
        "Question_original_content":"csv file displai item paramet multipl creat follow piec code python order optim network activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu model sequenti layer convd filter dict param num filter kernel size dict param kernel size activ dict param activ stride dict param stride num input shape self input shape layer batchnorm layer maxpoolingd layer convd filter dict param num filter kernel size dict param kernel size activ dict param activ stride dict param stride num multipl activ trial instead want model produc better result layer differ activ function paramet confus begin return studi bestparam object num filter kernel size stride activ selu num dens node batch size best paramet trial produc paramet tell paramet displai activ function paramet matter wai precis displai best set model layer awar save best model model summari help",
        "Question_preprocessed_content":"csv file displai item paramet multipl creat follow piec code python order optim network multipl activ trial instead want model produc better result layer differ activ function paramet confus begin return object best paramet trial produc paramet tell paramet displai activ function wai precis displai best set model layer",
        "Question_gpt_summary_original":"The user encountered a challenge with the Optuna CSV file as it only displayed one item per parameter despite having multiple activation trials. The best parameters from the trials produced only one parameter, which did not tell the user where the parameter was used and also did not display the other activation functions used or the other parameters. The user is seeking a way to precisely display the best settings the model used and at which layers.",
        "Question_gpt_summary":"user encount challeng csv file displai item paramet despit have multipl activ trial best paramet trial produc paramet tell user paramet displai activ function paramet user seek wai precis displai best set model layer",
        "Answer_original_content":"problem paramet activ instead activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu try activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu",
        "Answer_preprocessed_content":"problem paramet activ instead try",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to use different parameter names for each activation function instead of using the same parameter name for all activations. By doing so, the user can precisely display the best settings the model used and at which layers.",
        "Answer_gpt_summary":"solut solut suggest discuss us differ paramet name activ function instead paramet activ user precis displai best set model layer"
    },
    {
        "Question_title":"Azure endpoint in decimal notation",
        "Question_body":"Hello, I've set up an Azure endpoint and I'm trying to communicate with it using some old software that can only read decimal notation. The scientific notation the endpoint occasionally delivers is breaking it. Is there a way to configure the endpoint to return only decimal notation? Ideally just with the correct header like \"application\/jsonlegacy\" or something?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639603918890,
        "Question_favorite_count":9.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@JonathanHorton-4850 Returning a decimal value from an endpoint should be possible. I think this depends on the training of the experiment if the ML studio is used. I have an experiment which returns decimals. You can use a similar setup with Apply transformation module or Apply Math operation if using the newer version of the studio.\n\n\n\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/665285\/azure-endpoint-in-decimal-notation.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-16T06:58:54.83Z",
                "Answer_score":1,
                "Answer_body":"@JonathanHorton-4850 Returning a decimal value from an endpoint should be possible. I think this depends on the training of the experiment if the ML studio is used. I have an experiment which returns decimals. You can use a similar setup with Apply transformation module or Apply Math operation if using the newer version of the studio.\n\n\n\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1639637934830,
        "Question_original_content":"azur endpoint decim notat hello set azur endpoint try commun old softwar read decim notat scientif notat endpoint occasion deliv break wai configur endpoint return decim notat ideal correct header like applic jsonlegaci",
        "Question_preprocessed_content":"azur endpoint decim notat hello set azur endpoint try commun old softwar read decim notat scientif notat endpoint occasion deliv break wai configur endpoint return decim notat ideal correct header like",
        "Question_gpt_summary_original":"The user is facing a challenge with an Azure endpoint that occasionally delivers scientific notation, which is causing issues with their old software that can only read decimal notation. They are seeking a solution to configure the endpoint to return only decimal notation, preferably with the correct header.",
        "Question_gpt_summary":"user face challeng azur endpoint occasion deliv scientif notat caus issu old softwar read decim notat seek solut configur endpoint return decim notat prefer correct header",
        "Answer_original_content":"jonathanhorton return decim valu endpoint possibl think depend train experi studio experi return decim us similar setup appli transform modul appli math oper newer version studio answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"return decim valu endpoint possibl think depend train experi studio experi return decim us similar setup appli transform modul appli math oper newer version studio answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"Solution: @JonathanHorton-4850 suggests that returning a decimal value from an endpoint should be possible. He recommends using a similar setup with Apply transformation module or Apply Math operation if using the newer version of the studio.",
        "Answer_gpt_summary":"solut jonathanhorton suggest return decim valu endpoint possibl recommend similar setup appli transform modul appli math oper newer version studio"
    },
    {
        "Question_title":"split dataframe column header and values into multiple columns",
        "Question_body":"<p>I've uploaded my <code>csv<\/code> file on Azure, but for some reason it became like this<\/p>\n\n<pre><code> nominal;data;curs;cdx         Column 1\n0          1;21.06.2000;28  2300;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n1          1;22.06.2000;28  2200;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n2          1;23.06.2000;28  1900;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n3          1;24.06.2000;28  1700;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n4          1;27.06.2000;28  1300;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n5          1;28.06.2000;28  1100;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n<\/code><\/pre>\n\n<p>Basically instead of four columns <code>nominal<\/code>, <code>data<\/code>, <code>curs<\/code>, <code>cdx<\/code> I got two columns with one having all the values and the last one (it is empty or something because the last column has encoding issue) - no idea what.<\/p>\n\n<p>I have deleted the column <code>Column 1<\/code> like this<\/p>\n\n<pre><code>import pandas as pd\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    dataframe1.drop(['Column 1'], axis = 1, inplace = True)\n    print('Input pandas.DataFrame #1:\\r\\n\\r\\n{0}'.format(dataframe1))\n    return dataframe1,\n<\/code><\/pre>\n\n<p>How to split the first column into multiple now? To get 4 separate columns<\/p>\n\n<p>I am using pandas 0.18<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_time":1532029428013,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1532029712492,
        "Question_score":0.0,
        "Question_view_count":1086.0,
        "Answer_body":"<p>You need to split the column with:<\/p>\n\n<pre><code>dataframe1['nominal;data;curs;cdx'].str.split(';',expand=True)\n<\/code><\/pre>\n\n<p>Then change the headers with:<\/p>\n\n<pre><code>dataframe1.columns = 'nominal;data;curs;cdx'.split(';')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51430645",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1532031297043,
        "Question_original_content":"split datafram column header valu multipl column upload csv file azur reason like nomin data cur cdx column basic instead column nomin data cur cdx got column have valu column encod issu idea delet column column like import panda def main datafram datafram datafram drop column axi inplac true print input panda datafram format datafram return datafram split column multipl separ column panda",
        "Question_preprocessed_content":"split datafram column header valu multipl column upload file azur reason like basic instead column got column have valu idea delet column like split column multipl separ column panda",
        "Question_gpt_summary_original":"The user uploaded a CSV file on Azure, but the column headers and values got merged into two columns instead of four. The user deleted the extra column but now needs to split the first column into four separate columns using pandas 0.18.",
        "Question_gpt_summary":"user upload csv file azur column header valu got merg column instead user delet extra column need split column separ column panda",
        "Answer_original_content":"need split column datafram nomin data cur cdx str split expand true chang header datafram column nomin data cur cdx split",
        "Answer_preprocessed_content":"need split column chang header",
        "Answer_gpt_summary_original":"Solution: The user can split the first column into four separate columns using pandas 0.18 by using the following code: \n\ndataframe1['nominal;data;curs;cdx'].str.split(';',expand=True)\n\nThen, the user can change the headers using the following code:\n\ndataframe1.columns = 'nominal;data;curs;cdx'.split(';')",
        "Answer_gpt_summary":"solut user split column separ column panda follow code datafram nomin data cur cdx str split expand true user chang header follow code datafram column nomin data cur cdx split"
    },
    {
        "Question_title":"dvc.exceptions.CyclicGraphError: Pipeline has a cycle involving: load_extract_save",
        "Question_body":"<pre><code>stages:\n  load_extract_save: \n    cmd: python src\/stage_01_load_extract_save.py --config=config\/config.yaml\n    deps:\n      - config\/config.yaml\n      - src\/utils\/all_utils.py\n      - src\/stage_01_load_extract_save.py\n      - artifacts\/data\n    outs:\n      - artifacts\/data\n      - artifacts\/clean_data\/X.npy\n      - artifacts\/clean_data\/Y.npy\n\n  train_test_split_save:\n    cmd: python src\/stage_02_train_test_split_save.py --config=config\/config.yaml --params=params.yaml\n    deps:\n      - artifacts\/clean_data\/X.npy\n      - artifacts\/clean_data\/Y.npy\n      - src\/utils\/all_utils.py\n      - params.yaml\n      - config\/config.yaml\n      - src\/stage_02_train_test_split_save.py\n    outs:\n      - artifacts\/train_data\/X_train.npy\n      - artifacts\/train_data\/Y_train.npy\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n  \n  train_model:\n    cmd:  python src\/stage_03_train.py --config=config\/config.yaml --params=params.yaml\n    deps:\n      - artifacts\/train_data\/X_train.npy\n      - artifacts\/train_data\/Y_train.npy\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n      - src\/stage_03_train.py\n      - src\/utils\/all_utils.py\n      - config\/config.yaml\n      - params.yaml\n    outs:\n      - artifacts\/checkpoints\n      - artifacts\/model\n  \n  metrics:\n    cmd: python src\/stage_04_metrics.py --config=config\/config.yaml\n    deps:\n      - src\/stage_04_metrics.py\n      - config\/config.yaml\n      - src\/utils\/all_utils.py\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n      - artifacts\/checkpoints\n      - artifacts\/model\n    outs:\n      - confusion_matrix.png\n<\/code><\/pre>\n<p>This is my DVC.yaml.<\/p>\n<p>I have created Github workflow to reproduce it, but whenever I run it it gives me the following error - <code>... ERROR: Pipeline has a cycle involving: load_extract_save.<\/code><\/p>\n<p>The error looks <a href=\"https:\/\/i.stack.imgur.com\/1mt1Y.png\" rel=\"nofollow noreferrer\">like this<\/a>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655502527920,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1655524559503,
        "Question_score":2.0,
        "Question_view_count":38.0,
        "Answer_body":"<p>Stage <code>load_extract_save<\/code> both outputs and depends on the same path (<code>artifacts\/data<\/code>). That's a cycle.<\/p>\n<p>Pipeline structures should be <a href=\"https:\/\/dvc.org\/doc\/command-reference\/dag#directed-acyclic-graph\" rel=\"nofollow noreferrer\">directed <strong>acyclical<\/strong> graphs<\/a>, otherwise <code>dvc repro<\/code> could execute that stage over and over forever.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72665109",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1655524277716,
        "Question_original_content":"except cyclicgrapherror pipelin cycl involv load extract save stage load extract save cmd python src stage load extract save config config config yaml dep config config yaml src util util src stage load extract save artifact data out artifact data artifact clean data npy artifact clean data npy train test split save cmd python src stage train test split save config config config yaml param param yaml dep artifact clean data npy artifact clean data npy src util util param yaml config config yaml src stage train test split save out artifact train data train npy artifact train data train npy artifact test data test npy artifact test data test npy train model cmd python src stage train config config config yaml param param yaml dep artifact train data train npy artifact train data train npy artifact test data test npy artifact test data test npy src stage train src util util config config yaml param yaml out artifact checkpoint artifact model metric cmd python src stage metric config config config yaml dep src stage metric config config yaml src util util artifact test data test npy artifact test data test npy artifact checkpoint artifact model out confus matrix png yaml creat github workflow reproduc run give follow error error pipelin cycl involv load extract save error look like",
        "Question_preprocessed_content":"pipelin cycl involv yaml creat github workflow reproduc run give follow error error look like",
        "Question_gpt_summary_original":"The user has encountered a challenge with their DVC.yaml file, specifically with a pipeline involving the \"load_extract_save\" stage. When attempting to reproduce the workflow using a Github workflow, the user receives an error message stating that the pipeline has a cycle involving the \"load_extract_save\" stage.",
        "Question_gpt_summary":"user encount challeng yaml file specif pipelin involv load extract save stage attempt reproduc workflow github workflow user receiv error messag state pipelin cycl involv load extract save stage",
        "Answer_original_content":"stage load extract save output depend path artifact data cycl pipelin structur direct acycl graph repro execut stage forev",
        "Answer_preprocessed_content":"stage output depend path cycl pipelin structur direct acycl graph execut stage forev",
        "Answer_gpt_summary_original":"Solution: The error message is caused by a cycle in the pipeline structure, which violates the requirement of a directed acyclic graph. To resolve the issue, the user needs to modify the pipeline structure to remove the cycle.",
        "Answer_gpt_summary":"solut error messag caus cycl pipelin structur violat requir direct acycl graph resolv issu user need modifi pipelin structur remov cycl"
    },
    {
        "Question_title":"Parallel *apply in Azure Machine Learning Studio",
        "Question_body":"<p>I have just started to get myself acquainted with parallelism in R. <\/p>\n\n<p>As I am planning to use <a href=\"https:\/\/studio.azureml.net\/\" rel=\"nofollow\">Microsoft Azure Machine Learning Studio<\/a> for my project, I have started investigating what <a href=\"https:\/\/mran.revolutionanalytics.com\/documents\/rro\/multithread\/\" rel=\"nofollow\">Microsoft R Open<\/a> offers for parallelism, and thus, I found <a href=\"https:\/\/mran.revolutionanalytics.com\/documents\/rro\/multithread\/\" rel=\"nofollow\">this<\/a>, in which it says that parallelism is done under the hood that leverages the benefit of all available cores, without changing the R code. The article also shows some performance benchmarks, however, most of them demonstrate the performance benefit in doing mathematical operations.<\/p>\n\n<p>This was good so far. In addition, I am also interested to know whether it also parallelize the <code>*apply<\/code> functions under the hood or not. I also found these 2 articles that describes how to parallelize <code>*apply<\/code> functions in general:<\/p>\n\n<ol>\n<li><a href=\"https:\/\/www.r-bloggers.com\/quick-guide-to-parallel-r-with-snow\/\" rel=\"nofollow\">Quick guide to parallel R with snow<\/a>: describes facilitating parallelism using <a href=\"https:\/\/cran.r-project.org\/web\/packages\/snow\/snow.pdf\" rel=\"nofollow\"><code>snow<\/code><\/a> package, <code>par*apply<\/code> function family, and <code>clusterExport<\/code>.<\/li>\n<li><a href=\"http:\/\/www.win-vector.com\/blog\/2016\/01\/parallel-computing-in-r\/\" rel=\"nofollow\">A gentle introduction to parallel computing in R<\/a>: using <code>parallel<\/code> package, <code>par*apply<\/code> function family, and binding values to environment.<\/li>\n<\/ol>\n\n<p>So my question is when I will be using <code>*apply<\/code> functions in Microsoft Azure Machine Learning Studio, will that be parallelized under the hood by default, or I need to make use of packages like <code>parallel<\/code>, <code>snow<\/code> etc.?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1476002256777,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":501.0,
        "Answer_body":"<p>Personally, I think we could have marketed MRO a bit differently, without making such a big deal about parallelism\/multithreading. Ah well.<\/p>\n\n<p>R comes with an Rblas.dll\/.so which implements the routines used for linear algebra computations. These routines are used in various places, but one common use case is for fitting regression models. With MRO, we replace the standard Rblas with one that uses the <a href=\"https:\/\/software.intel.com\/en-us\/intel-mkl\" rel=\"noreferrer\">Intel Math Kernel Library<\/a>. When you call a function like <code>lm<\/code> or <code>glm<\/code>, MRO will use multiple threads and optimized CPU instructions to fit the model, which can get you dramatic speedups over the standard implementation.<\/p>\n\n<p>MRO isn't the only way you can get this sort of speedup; you can also compile\/download other BLAS implementations that are similarly optimized. We just make it an easy one-step download.<\/p>\n\n<p>Note that the MKL only affects code that involves linear algebra. It isn't a general-purpose speedup tool; any R code that doesn't do matrix computations won't see a performance improvement. In particular, it won't speed up any code that involves <em>explicit<\/em> parallelism, such as code using the parallel package, SNOW, or other cluster computing tools.<\/p>\n\n<p>On the other hand, it won't <em>degrade<\/em> them either. You can still use packages like parallel, SNOW, etc to create compute clusters and distribute your code across multiple processes. MRO works just like regular CRAN R in this respect. (One thing you might want to do, though, if you're creating a cluster of nodes on the one machine, is reduce the number of MKL threads. Otherwise you risk contention between the nodes for CPU cores, which will degrade performance.)<\/p>\n\n<p>Disclosure: I work for Microsoft.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/39941622",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1476024698776,
        "Question_original_content":"parallel appli studio start acquaint parallel plan us studio project start investig microsoft open offer parallel sai parallel hood leverag benefit avail core chang code articl show perform benchmark demonstr perform benefit mathemat oper good far addit interest know parallel appli function hood articl describ parallel appli function gener quick guid parallel snow describ facilit parallel snow packag par appli function famili clusterexport gentl introduct parallel comput parallel packag par appli function famili bind valu environ question appli function studio parallel hood default need us packag like parallel snow",
        "Question_preprocessed_content":"parallel appli studio start acquaint parallel plan us studio project start investig microsoft open offer parallel sai parallel hood leverag benefit avail core chang code articl show perform benchmark demonstr perform benefit mathemat oper good far addit interest know parallel function hood articl describ parallel function gener quick guid parallel snow describ facilit parallel packag function famili gentl introduct parallel comput packag function famili bind valu environ question function studio parallel hood default need us packag like",
        "Question_gpt_summary_original":"The user is exploring parallelism in R and is interested in whether the *apply functions will be parallelized under the hood in Microsoft Azure Machine Learning Studio or if they need to use packages like parallel or snow. They have found articles that describe how to parallelize *apply functions in general using these packages.",
        "Question_gpt_summary":"user explor parallel interest appli function parallel hood studio need us packag like parallel snow articl parallel appli function gener packag",
        "Answer_original_content":"person think market mro bit differ make big deal parallel multithread come rbla dll implement routin linear algebra comput routin place common us case fit regress model mro replac standard rbla us intel math kernel librari function like glm mro us multipl thread optim cpu instruct fit model dramat speedup standard implement mro isn wai sort speedup compil download bla implement similarli optim easi step download note mkl affect code involv linear algebra isn gener purpos speedup tool code matrix comput won perform improv particular won speed code involv explicit parallel code parallel packag snow cluster comput tool hand won degrad us packag like parallel snow creat comput cluster distribut code multipl process mro work like regular cran respect thing want creat cluster node machin reduc number mkl thread risk content node cpu core degrad perform disclosur work microsoft",
        "Answer_preprocessed_content":"person think market mro bit differ make big deal come implement routin linear algebra comput routin place common us case fit regress model mro replac standard rbla us intel math kernel librari function like mro us multipl thread optim cpu instruct fit model dramat speedup standard implement mro isn wai sort speedup bla implement similarli optim easi download note mkl affect code involv linear algebra isn speedup tool code matrix comput won perform improv particular won speed code involv explicit parallel code parallel packag snow cluster comput tool hand won degrad us packag like parallel snow creat comput cluster distribut code multipl process mro work like regular cran respect disclosur work microsoft",
        "Answer_gpt_summary_original":"There are no solutions provided in the discussion for parallelizing *apply functions in Microsoft Azure Machine Learning Studio. The discussion focuses on the benefits of using the Intel Math Kernel Library (MKL) for linear algebra computations and how it can improve the performance of functions like lm and glm. However, it is noted that the MKL only affects code that involves linear algebra and does not provide a general-purpose speedup tool for explicit parallelism. The discussion also mentions that packages like parallel and SNOW can still be used to create compute clusters and distribute code across multiple processes.",
        "Answer_gpt_summary":"solut provid discuss parallel appli function studio discuss focus benefit intel math kernel librari mkl linear algebra comput improv perform function like glm note mkl affect code involv linear algebra provid gener purpos speedup tool explicit parallel discuss mention packag like parallel snow creat comput cluster distribut code multipl process"
    },
    {
        "Question_title":"How to select a target column in a Vertex AI AutoML time series model",
        "Question_body":"<p>I am testing out Google Cloud Vertex AI with a time series AutoML model.<\/p>\n<p>I have created a dataset, from a Biguery table, with 2 columns, one of a timestamp and another of a numeric value I want to predict:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/t5R7S.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/t5R7S.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><code>salesorderdate<\/code> is my <code>TIMESTAMP<\/code> column and <code>orders<\/code> is the value I want to predict.<\/p>\n<p>When I proceed to the next step I cannot select <code>orders<\/code> as my value to predict, there are no available options for this field:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/HOed3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/HOed3.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What am I missing here? Surely the time series value <em>is<\/em> the target value in this case? Is there an expectation of more fields here, and can one actually add additional features as columns to a time series model in this way?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624627510520,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1624971874476,
        "Question_score":1.0,
        "Question_view_count":606.0,
        "Answer_body":"<p>I guess from your question that you are using &quot;forecasting models&quot;. Please note that it is in &quot;Preview&quot; <a href=\"https:\/\/cloud.google.com\/products#product-launch-stages\" rel=\"nofollow noreferrer\">Product launch stage<\/a> with all consequences of that fact.<\/p>\n<p>In the documentation you may find <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-tabular#data-structure\" rel=\"nofollow noreferrer\">Training data structure<\/a> following information:<\/p>\n<blockquote>\n<ul>\n<li>There must be at least two and no more than 1,000 columns.<\/li>\n<\/ul>\n<p>For datasets that train AutoML models, one column must be the target,\nand there must be at least one feature available to train the model.\nIf the training data does not include the target column, Vertex AI\ncannot associate the training data with the desired result.<\/p>\n<\/blockquote>\n<p>I suppose you are using AutoML models so in this situation you need to have 3 columns in the data set:<\/p>\n<ul>\n<li>Time column - used to place the observation represented by that row in time<\/li>\n<li>time series identifier column as &quot;Forecasting training data usually includes multiple time series&quot;<\/li>\n<li>and target column which is value that model should learn to predict.<\/li>\n<\/ul>\n<p>If you want to predict <code>orders<\/code> this should be target column. But before you are choosing this target this &quot;time series identifier column&quot; is already chosen in previous step, so you do not have available column to choose.<\/p>\n<p>So you need to add to your BigQuery table at least one additional column with will be used as time series column. You can add to your data set column with the same value in each row. This concept is presented in <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/bp-tabular#data_preparation_best_practices\" rel=\"nofollow noreferrer\">Forecasting data preparation best practices<\/a>:<\/p>\n<blockquote>\n<p>You can train a forecasting model on a single time series (in other\nwords, the time series identifier column contains the same value for\nall rows). However, Vertex AI is a better fit for training data that\ncontains two or more time series. For best results, you should have at\nleast 10 time series for every column used to train the model.<\/p>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1624960598496,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68131742",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1624889338667,
        "Question_original_content":"select target column automl time seri model test googl cloud time seri automl model creat dataset bigueri tabl column timestamp numer valu want predict salesorderd timestamp column order valu want predict proce step select order valu predict avail option field miss sure time seri valu target valu case expect field actual add addit featur column time seri model wai",
        "Question_preprocessed_content":"select target column automl time seri model test googl cloud time seri automl model creat dataset bigueri tabl column timestamp numer valu want predict column valu want predict proce step select valu predict avail option field miss sure time seri valu target valu case expect field actual add addit featur column time seri model wai",
        "Question_gpt_summary_original":"The user is facing challenges in selecting a target column in a Vertex AI AutoML time series model. They have created a dataset with a timestamp and a numeric value they want to predict, but when they proceed to the next step, they cannot select the value they want to predict. The user is unsure if they are missing something and if they can add additional features as columns to a time series model.",
        "Question_gpt_summary":"user face challeng select target column automl time seri model creat dataset timestamp numer valu want predict proce step select valu want predict user unsur miss add addit featur column time seri model",
        "Answer_original_content":"guess question forecast model note preview product launch stage consequ fact document train data structur follow inform column dataset train automl model column target featur avail train model train data includ target column associ train data desir result suppos automl model situat need column data set time column place observ repres row time time seri identifi column forecast train data usual includ multipl time seri target column valu model learn predict want predict order target column choos target time seri identifi column chosen previou step avail column choos need add bigqueri tabl addit column time seri column add data set column valu row concept present forecast data prepar best practic train forecast model singl time seri word time seri identifi column contain valu row better fit train data contain time seri best result time seri column train model",
        "Answer_preprocessed_content":"guess question forecast model note preview product launch stage consequ fact document train data structur follow inform column dataset train automl model column target featur avail train model train data includ target column associ train data desir result suppos automl model situat need column data set time column place observ repres row time time seri identifi column forecast train data usual includ multipl time seri target column valu model learn predict want predict target column choos target time seri identifi column chosen previou step avail column choos need add bigqueri tabl addit column time seri column add data set column valu row concept present forecast data prepar best practic train forecast model singl time seri better fit train data contain time seri best result time seri column train model",
        "Answer_gpt_summary_original":"Solution:\n- The user needs to have at least three columns in the dataset for AutoML time series models: a time column, a time series identifier column, and a target column.\n- If the user wants to predict a specific value, that value should be the target column.\n- The time series identifier column is already chosen in the previous step, so the user needs to add at least one additional column to the BigQuery table to use as the time series column.\n- The user can add a column with the same value in each row to use as the time series column.\n- It is recommended to have at least 10 time series for every column used to train the model for best results.",
        "Answer_gpt_summary":"solut user need column dataset automl time seri model time column time seri identifi column target column user want predict specif valu valu target column time seri identifi column chosen previou step user need add addit column bigqueri tabl us time seri column user add column valu row us time seri column recommend time seri column train model best result"
    },
    {
        "Question_title":"Unable to connect to MLFLOW_TRACKING_URI when running MLflow run in Docker container",
        "Question_body":"<p>I have setup a mlflow server locally at http:\/\/localhost:5000<\/p>\n<p>I followed the instructions at <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker<\/a> and tried to run the example docker with<\/p>\n<pre><code>\/mlflow\/examples\/docker$ mlflow run . -P alpha=0.5\n<\/code><\/pre>\n<p>but I encountered the following error.<\/p>\n<pre><code>2021\/05\/09 17:11:20 INFO mlflow.projects.docker: === Building docker image docker-example:7530274 ===\n2021\/05\/09 17:11:20 INFO mlflow.projects.utils: === Created directory \/tmp\/tmp9wpxyzd_ for downloading remote URIs passed to arguments of type 'path' ===\n2021\/05\/09 17:11:20 INFO mlflow.projects.backend.local: === Running command 'docker run --rm -v \/home\/mlf\/mlf\/0\/ae69145133bf49efac22b1d390c354f1\/artifacts:\/home\/mlf\/mlf\/0\/ae69145133bf49efac22b1d390c354f1\/artifacts -e MLFLOW_RUN_ID=ae69145133bf49efac22b1d390c354f1 -e MLFLOW_TRACKING_URI=http:\/\/localhost:5000 -e MLFLOW_EXPERIMENT_ID=0 docker-example:7530274 python train.py --alpha 0.5 --l1-ratio 0.1' in run with ID 'ae69145133bf49efac22b1d390c354f1' === \n\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/__init__.py:55: DeprecationWarning: MLflow support for Python 2 is deprecated and will be dropped in a future release. At that point, existing Python 2 workflows that use MLflow will continue to work without modification, but Python 2 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3 - see https:\/\/docs.python.org\/3\/howto\/pyporting.html for a migration guide.\n  &quot;for a migration guide.&quot;, DeprecationWarning)\nTraceback (most recent call last):\n  File &quot;train.py&quot;, line 56, in &lt;module&gt;\n    with mlflow.start_run():\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/client.py&quot;, line 96, in get_run\n    return self._tracking_client.get_run(run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py&quot;, line 49, in get_run\n    return self.store.get_run(run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py&quot;, line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py&quot;, line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 133, in call_endpoint\n    host_creds=host_creds, endpoint=endpoint, method=method, params=json_body)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 70, in http_request\n    url=url, headers=headers, verify=verify, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 51, in request_with_ratelimit_retries\n    response = requests.request(**kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/api.py&quot;, line 58, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/sessions.py&quot;, line 508, in request\n    resp = self.send(prep, **send_kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/sessions.py&quot;, line 618, in send\n    r = adapter.send(request, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/adapters.py&quot;, line 508, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get?run_uuid=ae69145133bf49efac22b1d390c354f1&amp;run_id=ae69145133bf49efac22b1d390c354f1 (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f5cbd80d690&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))\n2021\/05\/09 17:11:22 ERROR mlflow.cli: === Run (ID 'ae69145133bf49efac22b1d390c354f1') failed ===\n<\/code><\/pre>\n<p>Any ideas how to fix this? I tried adding the following in MLproject file but it doesn't help<\/p>\n<pre><code>environment: [[&quot;network&quot;, &quot;host&quot;], [&quot;add-host&quot;, &quot;host.docker.internal:host-gateway&quot;]]\n<\/code><\/pre>\n<p>Thanks for your help! =)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620552530280,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1620554070856,
        "Question_score":0.0,
        "Question_view_count":1151.0,
        "Answer_body":"<p>Run MLflow server such was that it will use your machine IP instead of <code>localhost<\/code>.  Then point the <code>mlflow run<\/code> to that IP instead of <code>http:\/\/localhost:5000<\/code>.   The main reason is that <code>localhost<\/code> of Docker process is its own, not your machine.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67456172",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1620627546968,
        "Question_original_content":"unabl connect track uri run run docker contain setup server local http localhost follow instruct http github com tree master exampl docker tri run exampl docker exampl docker run alpha encount follow error info project docker build docker imag docker exampl info project util creat directori tmp tmpwpxyzd download remot uri pass argument type path info project backend local run command docker run home mlf mlf aebfefacbdcf artifact home mlf mlf aebfefacbdcf artifact run aebfefacbdcf track uri http localhost experi docker exampl python train alpha ratio run aebfefacbdcf opt conda lib python site packag init deprecationwarn support python deprec drop futur releas point exist python workflow us continu work modif python user longer access latest featur bugfix recommend upgrad python http doc python org howto pyport html migrat guid migrat guid deprecationwarn traceback recent file train line start run file opt conda lib python site packag track fluent line start run activ run obj client run exist run file opt conda lib python site packag track client line run return self track client run run file opt conda lib python site packag track track servic client line run return self store run run file opt conda lib python site packag store track rest store line run respons proto self endpoint getrun req bodi file opt conda lib python site packag store track rest store line endpoint return endpoint self host cred endpoint method json bodi respons proto file opt conda lib python site packag util rest util line endpoint host cred host cred endpoint endpoint method method param json bodi file opt conda lib python site packag util rest util line http request url url header header verifi verifi kwarg file opt conda lib python site packag util rest util line request ratelimit retri respons request request kwarg file opt conda lib python site packag request api line request return session request method method url url kwarg file opt conda lib python site packag request session line request resp self send prep send kwarg file opt conda lib python site packag request session line send adapt send request kwarg file opt conda lib python site packag request adapt line send rais connectionerror request request request except connectionerror httpconnectionpool host localhost port max retri exceed url api run run uuid aebfefacbdcf run aebfefacbdcf caus newconnectionerror fail establish new connect errno connect refus error cli run aebfefacbdcf fail idea fix tri ad follow mlproject file help environ network host add host host docker intern host gatewai thank help",
        "Question_preprocessed_content":"unabl connect run run docker contain setup server local follow instruct tri run exampl docker encount follow error idea fix tri ad follow mlproject file help thank help",
        "Question_gpt_summary_original":"The user encountered an error when trying to run an MLflow example docker with the MLflow server set up locally at http:\/\/localhost:5000. The error message suggests that the connection to the MLflow tracking URI failed, and the user is seeking help to fix the issue.",
        "Question_gpt_summary":"user encount error try run exampl docker server set local http localhost error messag suggest connect track uri fail user seek help fix issu",
        "Answer_original_content":"run server us machin instead localhost point run instead http localhost main reason localhost docker process machin",
        "Answer_preprocessed_content":"run server us machin instead point instead main reason docker process machin",
        "Answer_gpt_summary_original":"Solution: The user can run the MLflow server using their machine IP instead of localhost and then point the mlflow run to that IP instead of http:\/\/localhost:5000. This is because the localhost of the Docker process is its own, not the user's machine.",
        "Answer_gpt_summary":"solut user run server machin instead localhost point run instead http localhost localhost docker process user machin"
    },
    {
        "Question_title":"What is the cluster manager in SageMaker Spark Processing?",
        "Question_body":"SageMaker Processing can launch multi-instance jobs. What is the underlying cluster manager? Yarn? Mesos? Something custom?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1602770746000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":81.0,
        "Answer_body":"The Spark container uses YARN - for ref the bootstrap script on github: https:\/\/github.com\/aws\/sagemaker-spark-container\/blob\/master\/src\/smspark\/bootstrapper.py and the Dockerfile with hadoop-yarn dependencies",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUShPm0t4vR4S8XBKMiAcA6g\/what-is-the-cluster-manager-in-sage-maker-spark-processing",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-15T14:12:23.000Z",
                "Answer_score":1,
                "Answer_body":"The Spark container uses YARN - for ref the bootstrap script on github: https:\/\/github.com\/aws\/sagemaker-spark-container\/blob\/master\/src\/smspark\/bootstrapper.py and the Dockerfile with hadoop-yarn dependencies",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1602771143000,
        "Question_original_content":"cluster manag spark process process launch multi instanc job underli cluster manag yarn meso custom",
        "Question_preprocessed_content":"cluster manag spark process process launch job underli cluster manag yarn meso custom",
        "Question_gpt_summary_original":"The user is facing a challenge in determining the underlying cluster manager used by SageMaker Processing for launching multi-instance jobs. They are unsure if it is Yarn, Mesos, or a custom solution.",
        "Question_gpt_summary":"user face challeng determin underli cluster manag process launch multi instanc job unsur yarn meso custom solut",
        "Answer_original_content":"spark contain us yarn ref bootstrap script github http github com aw spark contain blob master src smspark bootstrapp dockerfil hadoop yarn depend",
        "Answer_preprocessed_content":"spark contain us yarn ref bootstrap script github dockerfil depend",
        "Answer_gpt_summary_original":"Solution: The user can determine that the underlying cluster manager used by SageMaker Processing for launching multi-instance jobs is YARN. This is based on the fact that the Spark container uses YARN, as evidenced by the bootstrap script on Github and the Dockerfile with Hadoop-YARN dependencies.",
        "Answer_gpt_summary":"solut user determin underli cluster manag process launch multi instanc job yarn base fact spark contain us yarn evidenc bootstrap script github dockerfil hadoop yarn depend"
    },
    {
        "Question_title":"Does the primary key of Web Service API in ML Studio expire?",
        "Question_body":"<p>I deployed a web-service from an experiment in ML studio. I tested the API, and everything was working fine. I tested it in Postman. After 2 hours, I got an authentication error when I sent a request using the same API. So to resolve this, I republished my Web Service and got new authentication code, so the API is working fine for now. I have two questions:<\/p>\n\n<p>1) Does the primary key automatically expire after a while or by signing out from ML studio? \n2) What is the application of the second key in ML Studio APIs? Where do we need the second key? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1535343803617,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":228.0,
        "Answer_body":"<blockquote>\n  <p>1) Does the primary key automatically expire after a while or by signing out from ML studio?<\/p>\n<\/blockquote>\n\n<p>I could not find any limit of the primary key in the office docs. Per my test, my primary key does not expire more than two hours or sign out from ML studio.<\/p>\n\n<blockquote>\n  <p>2) What is the application of the second key in ML Studio APIs? Where do we need the second key?<\/p>\n<\/blockquote>\n\n<p>The second key is the same usage of the primary key, like a backup of the primary key. Also, the primary key equals the API key in the ML studio.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52032535",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1535450343787,
        "Question_original_content":"primari kei web servic api studio expir deploi web servic experi studio test api work fine test postman hour got authent error sent request api resolv republish web servic got new authent code api work fine question primari kei automat expir sign studio applic second kei studio api need second kei",
        "Question_preprocessed_content":"primari kei web servic api studio expir deploi experi studio test api work fine test postman hour got authent error sent request api resolv republish web servic got new authent code api work fine question primari kei automat expir sign studio applic second kei studio api need second kei",
        "Question_gpt_summary_original":"The user encountered an authentication error when using a web-service API deployed from an experiment in ML Studio. They republished the service to obtain a new authentication code and resolved the issue. The user has questions about whether the primary key expires and the application of the second key in ML Studio APIs.",
        "Question_gpt_summary":"user encount authent error web servic api deploi experi studio republish servic obtain new authent code resolv issu user question primari kei expir applic second kei studio api",
        "Answer_original_content":"primari kei automat expir sign studio limit primari kei offic doc test primari kei expir hour sign studio applic second kei studio api need second kei second kei usag primari kei like backup primari kei primari kei equal api kei studio",
        "Answer_preprocessed_content":"primari kei automat expir sign studio limit primari kei offic doc test primari kei expir hour sign studio applic second kei studio api need second kei second kei usag primari kei like backup primari kei primari kei equal api kei studio",
        "Answer_gpt_summary_original":"Solution:\n- The primary key does not have a specific expiration time or affected by signing out from ML Studio.\n- The second key is a backup of the primary key and can be used in the same way as the primary key.",
        "Answer_gpt_summary":"solut primari kei specif expir time affect sign studio second kei backup primari kei wai primari kei"
    },
    {
        "Question_title":"R web scraping in Azure ML errors out",
        "Question_body":"<p>I have written a script in RStudio (running R 3.5.2) that scrapes data from a particular website. The script reaches out to a website, uses download.file to pull the underlying code, and uses tags to extract the desired data.<\/p>\n\n<p>The script runs without error in RStudio, but when I try to run the code in the \"Execute R Script\" node in Azure ML it throws a 0063 error saying that it \"cannot reach URL \". The code runs perfectly up until it tries to reach out to the URL. (see code below)<\/p>\n\n<p>I have tried switching the R version in Azure ML--neither of the 3 options work.<\/p>\n\n<pre class=\"lang-r prettyprint-override\"><code>for(a in 1:length(job_url)) {\n     download.file(url, destfile = filename, quiet=TRUE)\n      ...\n}\n<\/code><\/pre>\n\n<p>I expect the script to run the same in RStudio and Azure ML. Any ideas how to get this script to run in Azure ML the same way it runs in RStudio?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1556739448177,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":84.0,
        "Answer_body":"<blockquote>\n  <p>For security reasons, all networking from or to R code in Execute R Script modules is blocked by Azure.<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/execute-r-script#networking\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/execute-r-script#networking<\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55941720",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1556746082267,
        "Question_original_content":"web scrape error written script rstudio run scrape data particular websit script reach websit us download file pull underli code us tag extract desir data script run error rstudio try run code execut script node throw error sai reach url code run perfectli tri reach url code tri switch version option work length job url download file url destfil filenam quiet true expect script run rstudio idea script run wai run rstudio",
        "Question_preprocessed_content":"web scrape error written script rstudio scrape data particular websit script reach websit us pull underli code us tag extract desir data script run error rstudio try run code execut script node throw error sai reach url code run perfectli tri reach url tri switch version option work expect script run rstudio idea script run wai run rstudio",
        "Question_gpt_summary_original":"The user has encountered challenges while trying to run a web scraping script in Azure ML. The script runs without errors in RStudio but throws a 0063 error in Azure ML, indicating that it cannot reach the URL. The user has tried switching the R version in Azure ML, but none of the options work. The user is seeking help to get the script to run in Azure ML the same way it runs in RStudio.",
        "Question_gpt_summary":"user encount challeng try run web scrape script script run error rstudio throw error indic reach url user tri switch version option work user seek help script run wai run rstudio",
        "Answer_original_content":"secur reason network code execut script modul block azur http doc microsoft com azur machin learn studio modul refer execut script network",
        "Answer_preprocessed_content":"secur reason network code execut script modul block azur",
        "Answer_gpt_summary_original":"Solution: No solution is provided in the discussion. The user encountered a challenge due to the security restrictions in Azure ML that block networking from or to R code in Execute R Script modules.",
        "Answer_gpt_summary":"solut solut provid discuss user encount challeng secur restrict block network code execut script modul"
    },
    {
        "Question_title":"How can we get the pipeline to read columns with special characters?",
        "Question_body":"<p>I am using the \"usecols\" parameter to get some columns of a .xlsx file (I am using the xls_local.py file from the Kedro tutorial) but the program says that \"usecols do not match columns, columns expected but not found:\" and it only shows the columns that have special characters. How can I fix this, please? Thank you very much for your attention.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1573053746820,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":157.0,
        "Answer_body":"<p>As far as I can tell, this isn't a <code>kedro<\/code> issue, but a <code>pandas.read_excel<\/code> issue, which is what <code>kedro<\/code> uses under the hood. This seems to be broken in <code>pandas<\/code> itself, and a workaround is to reference the columns using letters instead, so something like <code>usecols='A:D'<\/code> and then you can rename the columns to what they should be by doing <code>df.columns = [\"colname with special characters\", \"b\", \"c\", \"d\"]<\/code> for example.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58733422",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1573057563320,
        "Question_original_content":"pipelin read column special charact usecol paramet column xlsx file xl local file tutori program sai usecol match column column expect show column special charact fix thank attent",
        "Question_preprocessed_content":"pipelin read column special charact usecol paramet column xlsx file program sai usecol match column column expect show column special charact fix thank attent",
        "Question_gpt_summary_original":"The user is facing a challenge with the \"usecols\" parameter while trying to read specific columns from a .xlsx file using the xls_local.py file from the Kedro tutorial. The program is not able to read columns with special characters and is displaying an error message stating that the expected columns are not found. The user is seeking help to fix this issue.",
        "Question_gpt_summary":"user face challeng usecol paramet try read specif column xlsx file xl local file tutori program abl read column special charact displai error messag state expect column user seek help fix issu",
        "Answer_original_content":"far tell isn issu panda read excel issu us hood broken panda workaround refer column letter instead like usecol renam column column colnam special charact exampl",
        "Answer_preprocessed_content":"far tell isn issu issu us hood broken workaround refer column letter instead like renam column exampl",
        "Answer_gpt_summary_original":"Solution: The user can reference the columns using letters instead of column names with special characters while using the \"usecols\" parameter to read specific columns from a .xlsx file. They can then rename the columns to their original names using the \"df.columns\" attribute.",
        "Answer_gpt_summary":"solut user refer column letter instead column name special charact usecol paramet read specif column xlsx file renam column origin name column attribut"
    },
    {
        "Question_title":"How can I export a model (Predictor) trained on Amazon Forecast?",
        "Question_body":"<p>On Amazon Forecast, how can I export a model (a Predictor in Forecast lingo) that I already trained? For example, export an ARIMA or Prophet model weights to a file t be downloaded or stored on S3.<\/p>\n<p>Running forecasts on new data is just too slow and I would like to use Forecast to train models and eventually deploy them somewhere else.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610981455530,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1611076562296,
        "Question_score":1.0,
        "Question_view_count":97.0,
        "Answer_body":"<p>Short answer: you can't export the models from Forecast<\/p>\n<p>Ref: <a href=\"https:\/\/github.com\/aws-samples\/amazon-forecast-samples\/issues\/104#issuecomment-763119541\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-forecast-samples\/issues\/104#issuecomment-763119541<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65776789",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1611089773083,
        "Question_original_content":"export model predictor train amazon forecast amazon forecast export model predictor forecast lingo train exampl export arima prophet model weight file download store run forecast new data slow like us forecast train model eventu deploi",
        "Question_preprocessed_content":"export model train amazon forecast amazon forecast export model train exampl export arima prophet model weight file download store run forecast new data slow like us forecast train model eventu deploi",
        "Question_gpt_summary_original":"The user is facing a challenge of exporting a model (Predictor) trained on Amazon Forecast, such as an ARIMA or Prophet model, to a file that can be downloaded or stored on S3. The user wants to use Forecast to train models and deploy them elsewhere, as running forecasts on new data is too slow.",
        "Question_gpt_summary":"user face challeng export model predictor train amazon forecast arima prophet model file download store user want us forecast train model deploi run forecast new data slow",
        "Answer_original_content":"short answer export model forecast ref http github com aw sampl amazon forecast sampl issu issuecom",
        "Answer_preprocessed_content":"short answer export model forecast ref",
        "Answer_gpt_summary_original":"Solution: No solution provided.",
        "Answer_gpt_summary":"solut solut provid"
    },
    {
        "Question_title":"Why can Random Cut Forest's `record_set()` method for data conversion\/upload not be used with the \"test\" channel?",
        "Question_body":"<h1>Original Question<\/h1>\n<p>I want to use RCF's &quot;test&quot; channel, to get performance-metrics of the model.<\/p>\n<p>I have previously used the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/algorithms\/randomcutforest.html?highlight=record_set#sagemaker.RandomCutForest.record_set\" rel=\"nofollow noreferrer\"><code>record_set()<\/code><\/a> method without specifying a channel and training worked fine.<\/p>\n<p>However if I upload my feature matrix and label vector using <code>record_set()<\/code> and set <code>channel='test'<\/code> like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker import RandomCutForest\n\nrcf = RandomCutForest(\n    role=execution_role,\n    instance_count=1,\n    instance_type='ml.m5.large',\n    data_location=f's3:\/\/{bucket}\/{prefix}\/',\n    output_path=f's3:\/\/{bucket}\/{prefix}\/output',\n    base_job_name=base_job_name,\n    eval_metrics=['accuracy', 'precision_recall_fscore']\n)\n\ntest_set = rcf.record_set(\n    features,\n    labels=labels,\n    channel='test'\n)\n\nrcf.fit(test_set)\n<\/code><\/pre>\n<p>... I get the following error during training:<\/p>\n<blockquote>\n<p>Failure reason<br \/>\nClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError) Caused by: 'ShardedByS3Key' is not one of ['FullyReplicated'] Failed validating 'enum' in schema['properties']['test']['properties']['S3DistributionType']: {'enum': ['FullyReplicated'], 'type': 'string'} On instance['test']['S3DistributionType']: 'ShardedByS3Key'<\/p>\n<\/blockquote>\n<hr \/>\n<h2>Appendix<\/h2>\n<p>The same is true for HPO-jobs that use data on the &quot;test&quot; channel:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tuner import HyperparameterTuner, IntegerParameter\n\nhpo = HyperparameterTuner(\n    estimator=rcf,\n    objective_metric_name='test:f1',\n    hyperparameter_ranges={\n        'num_samples_per_tree': IntegerParameter(32, 512),\n        'num_trees': IntegerParameter(50, 1000)\n    },\n    max_jobs=10,\n    max_parallel_jobs=2\n)\n\ntrain_set = rcf.record_set(\n    features,\n    channel='train'\n)\n\ntest_set = rcf.record_set(\n    features,\n    labels=labels,\n    channel='test'\n)\n\nhpo.fit([train_set, test_set])\n<\/code><\/pre>\n<p>Again the initial upload and &quot;train&quot; channel work fine, the &quot;test&quot; channel fails:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/p0xa0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/p0xa0.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<hr \/>\n<h3>EDIT 1<\/h3>\n<p>I tried to instantiate my own <code>RecordSet<\/code> objects like this (after the data was already uploaded to S3 in the correct format):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>train_data = RecordSet(\n    s3_data='s3:\/\/path-to-train-data\/.amazon.manifest',\n    feature_dim=features.shape[1],\n    num_records=features.shape[0],\n    s3_data_type='ManifestFile',\n    channel='train'\n)\n\ntest_data = RecordSet(\n    s3_data='s3:\/\/path-to-test-data\/.amazon.manifest',\n    feature_dim=features.shape[1],\n    num_records=features.shape[0],\n    s3_data_type='ManifestFile',\n    channel='test'\n)\n<\/code><\/pre>\n<p>But the same error occurs. The issue is, that the constructor of <code>RecordSet<\/code> does not permit to pass a different <code>distribution<\/code> argument. It is hardcoded to be <code>ShardedByS3Key<\/code>, as can be seen in the source code <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/2ebba8a454de03a2bc49267c91dbacddd6183585\/src\/sagemaker\/amazon\/amazon_estimator.py#L340\" rel=\"nofollow noreferrer\">HERE<\/a>.<\/p>\n<p>I've also opened an <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/2925\" rel=\"nofollow noreferrer\">issue on GitHub<\/a>, since this bug is still present in the latest version of SageMaker.<\/p>\n<h3>EDIT 2<\/h3>\n<p>There is another post on a similar issue on StackOverflow, where an older version of the Python API is used: <a href=\"https:\/\/stackoverflow.com\/questions\/68161725\/sagemaker-random-cut-forest-training-with-validation\">Sagemaker Random Cut Forest Training with Validation<\/a>.<\/p>\n<p>After some refactoring in newer versions, users don't have to create <code>TrainingInput<\/code> objects anymore. This is now supposed to be done via <code>record_set()<\/code> (which returns the class <code>RecordSet<\/code>). As a result, we lose control over the <code>distribution<\/code> property of the underlying <code>TrainingInput<\/code> object and can't set it to <code>FullyReplicated<\/code> anymore.<\/p>\n<p>This is essentially what breaks the &quot;test&quot; channel for RCF in current versions of SageMaker and also prevents hyperparameter optimization via <code>HyperparameterTuner<\/code>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644425382927,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1644496542936,
        "Question_score":0.0,
        "Question_view_count":95.0,
        "Answer_body":"<p>Thanks for opening the issue, I added a +1.\nIn the meantime, you can use alternative SDKs to train Random Cut Forest and set test channel distribution to FullyReplicated.<\/p>\n<p>For example, those SDKs should give you this control:<\/p>\n<ul>\n<li>AWS CLI <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-training-job.html\" rel=\"nofollow noreferrer\">create_training_job<\/a><\/li>\n<li><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_training_job\" rel=\"nofollow noreferrer\">boto3 create_training_job<\/a><\/li>\n<li>SageMaker Python SDK <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html#\" rel=\"nofollow noreferrer\">Estimator<\/a> to which you pass the RCF docker image in <code>image_uri<\/code> parameter<\/li>\n<\/ul>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71053554",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1645791604667,
        "Question_original_content":"random cut forest record set method data convers upload test channel origin question want us rcf test channel perform metric model previous record set method specifi channel train work fine upload featur matrix label vector record set set channel test like import randomcutforest rcf randomcutforest role execut role instanc count instanc type larg data locat bucket prefix output path bucket prefix output base job base job eval metric accuraci precis recal fscore test set rcf record set featur label label channel test rcf fit test set follow error train failur reason clienterror unabl initi algorithm fail valid input data configur caus validationerror caus shardedbyskei fullyrepl fail valid enum schema properti test properti sdistributiontyp enum fullyrepl type string instanc test sdistributiontyp shardedbyskei appendix true hpo job us data test channel tuner import hyperparametertun integerparamet hpo hyperparametertun estim rcf object metric test hyperparamet rang num sampl tree integerparamet num tree integerparamet max job max parallel job train set rcf record set featur channel train test set rcf record set featur label label channel test hpo fit train set test set initi upload train channel work fine test channel fail edit tri instanti recordset object like data upload correct format train data recordset data path train data amazon manifest featur dim featur shape num record featur shape data type manifestfil channel train test data recordset data path test data amazon manifest featur dim featur shape num record featur shape data type manifestfil channel test error occur issu constructor recordset permit pass differ distribut argument hardcod shardedbyskei seen sourc code open issu github bug present latest version edit post similar issu stackoverflow older version python api random cut forest train valid refactor newer version user creat traininginput object anymor suppos record set return class recordset result lose control distribut properti underli traininginput object set fullyrepl anymor essenti break test channel rcf current version prevent hyperparamet optim hyperparametertun",
        "Question_preprocessed_content":"random cut forest method data test channel origin question want us rcf test channel model previous method specifi channel train work fine upload featur matrix label vector set like follow error train failur reason clienterror unabl initi algorithm fail valid input data configur caus shardedbi kei fail valid enum schema instanc shardedbi kei appendix true us data test channel initi upload train channel work fine test channel fail edit tri instanti object like error occur issu constructor permit pass differ argument hardcod seen sourc code open issu github bug present latest version edit post similar issu stackoverflow older version python api random cut forest train valid refactor newer version user creat object anymor suppos result lose control properti underli object set anymor essenti break test channel rcf current version prevent hyperparamet optim",
        "Question_gpt_summary_original":"The user is encountering an error when attempting to use the Random Cut Forest's `record_set()` method with the \"test\" channel. The error message indicates that the algorithm is unable to validate the input data configuration due to an incorrect S3 distribution type. The user has attempted to create their own `RecordSet` objects, but the constructor does not allow for a different distribution argument. This issue prevents the user from using the \"test\" channel for performance metrics and hyperparameter optimization. The user has opened an issue on GitHub regarding this bug.",
        "Question_gpt_summary":"user encount error attempt us random cut forest record set method test channel error messag indic algorithm unabl valid input data configur incorrect distribut type user attempt creat recordset object constructor allow differ distribut argument issu prevent user test channel perform metric hyperparamet optim user open issu github bug",
        "Answer_original_content":"thank open issu ad meantim us altern sdk train random cut forest set test channel distribut fullyrepl exampl sdk control aw cli creat train job boto creat train job python sdk estim pass rcf docker imag imag uri paramet",
        "Answer_preprocessed_content":"thank open issu ad meantim us altern sdk train random cut forest set test channel distribut fullyrepl exampl sdk control aw cli boto python sdk estim pass rcf docker imag paramet",
        "Answer_gpt_summary_original":"Solution: The user can use alternative SDKs to train Random Cut Forest and set the test channel distribution to FullyReplicated until the bug is fixed. The AWS CLI, boto3 create_training_job, and SageMaker Python SDK Estimator can give the user control over the distribution.",
        "Answer_gpt_summary":"solut user us altern sdk train random cut forest set test channel distribut fullyrepl bug fix aw cli boto creat train job python sdk estim user control distribut"
    },
    {
        "Question_title":"How to convert jupyter notebook (ipython) to slideshow NOT using command line",
        "Question_body":"<p>I am new with <code>Jupyter<\/code>, and I use Amazon SageMaker so that everything is cloud based and not local.  I cannot use any resources locally, nor can I install <code>Jupyter<\/code> on this local computer that I want to do this on, so I cannot use the command line to put :<\/p>\n\n<pre><code>jupyter nbconvert Jupyter\\ Slides.ipynb --to slides --post serve\n<\/code><\/pre>\n\n<p>So, I am struggling to find a way to convert my notebook to a slideshow NOT using command line. Thanks in advance!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1532649972710,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1532675039560,
        "Question_score":1.0,
        "Question_view_count":4610.0,
        "Answer_body":"<p>You can follow below steps to convert your notebook to slides on AWS Sagemaker (tried on sagemaker notebook instance) without installing any extensions.<\/p>\n\n<p><strong>Step 1:<\/strong> Follow this <a href=\"https:\/\/medium.com\/@mjspeck\/presenting-code-using-jupyter-notebook-slides-a8a3c3b59d67\" rel=\"nofollow noreferrer\">article<\/a> to chose which cells in your notebook can be presented or skipped.\n  - Go to View \u2192 Cell Toolbar \u2192 Slideshow\n  - A light gray bar will appear above each cell with a scroll down window on the top right\n  - Select type of slide each cell should be - regular slide, sub-slide, skip, notes<\/p>\n\n<p><strong>Step 2:<\/strong> Go to Sagemaker notebook home page and open terminal<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/kDl3d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kDl3d.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 3:<\/strong> Change directory in the instance where your notebook exists<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/rA1lZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rA1lZ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 4:<\/strong> Clone <code>reveal.js<\/code> in the directory where notebook exists from <a href=\"https:\/\/github.com\/hakimel\/reveal.js\" rel=\"nofollow noreferrer\">github<\/a>. <code>reveal.js<\/code> is used for rendering HTML file as presentation.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/dillF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/dillF.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 5:<\/strong> Run the below command (same as in your question) to convert the notebook to slides without serving them (since there is no browser on the Sagemaker instance). This will just convert notebook to slides html.<\/p>\n\n<pre><code>jupyter nbconvert Image-classification-fulltraining.ipynb --to slides\n[NbConvertApp] Converting notebook Image-classification-fulltraining.ipynb to slides\n[NbConvertApp] Writing 346423 bytes to Image-classification-fulltraining.slides.html\n<\/code><\/pre>\n\n<p><strong>Step 6:<\/strong> Now open the html file from Sagemaker notebook file browser <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/fykyl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fykyl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Now you can see the notebook rendered as slides based on how setup each cell in your notebook in Step 1<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9PLUA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9PLUA.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Hope it helps.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51549048",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1532732192903,
        "Question_original_content":"convert jupyt notebook ipython slideshow command line new jupyt us cloud base local us resourc local instal jupyt local want us command line jupyt nbconvert jupyt slide ipynb slide post serv struggl wai convert notebook slideshow command line thank advanc",
        "Question_preprocessed_content":"convert jupyt notebook slideshow command line new us cloud base local us resourc local instal local want us command line struggl wai convert notebook slideshow command line thank advanc",
        "Question_gpt_summary_original":"The user is facing challenges in converting a Jupyter notebook to a slideshow without using the command line. They are unable to use any resources locally or install Jupyter on their local computer as they are using Amazon SageMaker, which is cloud-based.",
        "Question_gpt_summary":"user face challeng convert jupyt notebook slideshow command line unabl us resourc local instal jupyt local cloud base",
        "Answer_original_content":"follow step convert notebook slide tri notebook instanc instal extens step follow articl chose cell notebook present skip view cell toolbar slideshow light grai bar appear cell scroll window right select type slide cell regular slide sub slide skip note step notebook home page open termin step chang directori instanc notebook exist step clone reveal directori notebook exist github reveal render html file present step run command question convert notebook slide serv browser instanc convert notebook slide html jupyt nbconvert imag classif fulltrain ipynb slide nbconvertapp convert notebook imag classif fulltrain ipynb slide nbconvertapp write byte imag classif fulltrain slide html step open html file notebook file browser notebook render slide base setup cell notebook step hope help",
        "Answer_preprocessed_content":"follow step convert notebook slide instal extens step follow articl chose cell notebook present skip view cell toolbar slideshow light grai bar appear cell scroll window right select type slide cell regular slide skip note step notebook home page open termin step chang directori instanc notebook exist step clone directori notebook exist github render html file present step run command convert notebook slide serv convert notebook slide html step open html file notebook file browser notebook render slide base setup cell notebook step hope help",
        "Answer_gpt_summary_original":"Solution: The user can follow the steps mentioned below to convert their Jupyter notebook to a slideshow on AWS SageMaker without using the command line or installing any extensions. \n\nStep 1: Choose which cells in the notebook can be presented or skipped by going to View \u2192 Cell Toolbar \u2192 Slideshow and selecting the type of slide each cell should be - regular slide, sub-slide, skip, notes.\n\nStep 2: Open the terminal on the SageMaker notebook home page.\n\nStep 3: Change the directory to where the notebook exists.\n\nStep 4: Clone reveal.js in the directory where the notebook exists from GitHub. reveal.js is used for rendering the HTML file as a presentation.\n\nStep 5: Run the command \"jupyter nbconvert",
        "Answer_gpt_summary":"solut user follow step mention convert jupyt notebook slideshow command line instal extens step choos cell notebook present skip go view cell toolbar slideshow select type slide cell regular slide sub slide skip note step open termin notebook home page step chang directori notebook exist step clone reveal directori notebook exist github reveal render html file present step run command jupyt nbconvert"
    },
    {
        "Question_title":"mflow model register icon missing",
        "Question_body":"I'm using azure databricks where i couldn't find the model registry icon as shown below in the databricks page.\n\n\n\nAppreciate\u00a0your help.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593370699000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":17.0,
        "Answer_body":"Thanks for reporting the issue. Could you please provide a URL to the workspace you are using?\n\n\nRichard\n\n\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/zrqbH9zWo_8",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-29T13:30:09",
                "Answer_body":"Thanks for reporting the issue. Could you please provide a URL to the workspace you are using?\n\n\nRichard\n\n\n\ue5d3"
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"mflow model regist icon miss azur databrick couldn model registri icon shown databrick page appreciateyour help",
        "Question_preprocessed_content":"mflow model regist icon miss azur databrick couldn model registri icon shown databrick page appreciateyour help",
        "Question_gpt_summary_original":"The user is facing a challenge in finding the model registry icon in Azure Databricks.",
        "Question_gpt_summary":"user face challeng find model registri icon azur databrick",
        "Answer_original_content":"thank report issu provid url workspac richard",
        "Answer_preprocessed_content":"thank report issu provid url workspac richard",
        "Answer_gpt_summary_original":"No solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut mention discuss"
    },
    {
        "Question_title":"Dvc with git sparse-checkout",
        "Question_body":"<p>Hello<br>\nI have a large number of datasets, each in its own subdirectory. So far, I was always pulling all datasets with <code>git clone ... &amp; dvc pull<\/code>, which works perfectly.<\/p>\n<p>Now, I would like to load only a subset of my datasets. I tried to archive this by doing a sparse checkout of the git repository. However, if I run <code>git sparse-checkout init &amp; git sparse-checkout add .dvc dataset_name<\/code> followed by <code>dvc pull<\/code>, I get<\/p>\n<blockquote>\n<p>ERROR: unexpected error - (b\u2019worktreeConfig\u2019, b\u2019true\u2019)<\/p>\n<\/blockquote>\n<p>Is there a way to fix this, or maybe a better way to solve the problem in the first place?<br>\nI am aware that I could simply run <code>dvc pull --recursive dataset_name<\/code> - but given the large number of directories I am dealing with, I\u2019d prefer to clone only those that I actually need.<\/p>\n<p>Any hint is highly appreciated.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1675094654696,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":34.0,
        "Answer_body":"<p>Hi, can you provide a full traceback by running pull with the <code>-v<\/code> flag?<\/p>. <p>Here the traceback:<\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/dvc\/cli\/__init__.py\", line 183, in main\n    cmd = args.func(args)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/dvc\/cli\/command.py\", line 20, in __init__\n    self.repo: \"Repo\" = Repo(uninitialized=self.UNINITIALIZED)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/dvc\/repo\/__init__.py\", line 248, in __init__\n    self._ignore()\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/dvc\/repo\/__init__.py\", line 416, in _ignore\n    self.scm_context.ignore(file)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/funcy\/objects.py\", line 28, in __get__\n    res = instance.__dict__[self.fget.__name__] = self.fget(instance)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/dvc\/repo\/__init__.py\", line 313, in scm_context\n    return SCMContext(self.scm, self.config)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/funcy\/objects.py\", line 28, in __get__\n    res = instance.__dict__[self.fget.__name__] = self.fget(instance)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/dvc\/repo\/__init__.py\", line 301, in scm\n    return SCM(self.root_dir, no_scm=no_scm)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/dvc\/scm.py\", line 102, in SCM\n    return Git(\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/scmrepo\/git\/__init__.py\", line 98, in __init__\n    first_ = first(self.backends.values())\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/funcy\/seqs.py\", line 55, in first\n    return next(iter(seq), None)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/_collections_abc.py\", line 869, in __iter__\n    yield self._mapping[key]\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/scmrepo\/git\/__init__.py\", line 49, in __getitem__\n    initialized = backend(*self.args, **self.kwargs)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/scmrepo\/git\/backend\/dulwich\/__init__.py\", line 146, in __init__\n    self.repo = Repo.discover(start=root_dir)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/dulwich\/repo.py\", line 1240, in discover\n    return cls(path)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/dulwich\/repo.py\", line 1172, in __init__\n    raise UnsupportedExtension(extension)\ndulwich.repo.UnsupportedExtension: (b'worktreeConfig', b'true')\n<\/code><\/pre>. <p>Unfortunately our default git backend (dulwich) does not currently support the worktreeconfig option. The good news is that it has just been merged <a href=\"https:\/\/github.com\/jelmer\/dulwich\/commit\/a9bbc1604976b991408baba4e451459397467ee2\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Add support for worktreeconfig extension \u00b7 jelmer\/dulwich@a9bbc16 \u00b7 GitHub<\/a>  and will be included in the next dulwich release, meaning we\u2019ll be able to fix the issue as soon as the new dulwich release it out. For now, if it is an option, I\u2019d suggest to disable sparse checkout.<\/p>\n<p>I created <a href=\"https:\/\/github.com\/iterative\/scmrepo\/issues\/175\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">dulwich broken with sparse checkout \u00b7 Issue #175 \u00b7 iterative\/scmrepo \u00b7 GitHub<\/a> to track the issue.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-with-git-sparse-checkout\/1480",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-30T17:49:40.713Z",
                "Answer_body":"<p>Hi, can you provide a full traceback by running pull with the <code>-v<\/code> flag?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-30T21:18:01.224Z",
                "Answer_body":"<p>Here the traceback:<\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/dvc\/cli\/__init__.py\", line 183, in main\n    cmd = args.func(args)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/dvc\/cli\/command.py\", line 20, in __init__\n    self.repo: \"Repo\" = Repo(uninitialized=self.UNINITIALIZED)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/dvc\/repo\/__init__.py\", line 248, in __init__\n    self._ignore()\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/dvc\/repo\/__init__.py\", line 416, in _ignore\n    self.scm_context.ignore(file)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/funcy\/objects.py\", line 28, in __get__\n    res = instance.__dict__[self.fget.__name__] = self.fget(instance)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/dvc\/repo\/__init__.py\", line 313, in scm_context\n    return SCMContext(self.scm, self.config)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/funcy\/objects.py\", line 28, in __get__\n    res = instance.__dict__[self.fget.__name__] = self.fget(instance)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/dvc\/repo\/__init__.py\", line 301, in scm\n    return SCM(self.root_dir, no_scm=no_scm)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/dvc\/scm.py\", line 102, in SCM\n    return Git(\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/scmrepo\/git\/__init__.py\", line 98, in __init__\n    first_ = first(self.backends.values())\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/funcy\/seqs.py\", line 55, in first\n    return next(iter(seq), None)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/_collections_abc.py\", line 869, in __iter__\n    yield self._mapping[key]\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/scmrepo\/git\/__init__.py\", line 49, in __getitem__\n    initialized = backend(*self.args, **self.kwargs)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/scmrepo\/git\/backend\/dulwich\/__init__.py\", line 146, in __init__\n    self.repo = Repo.discover(start=root_dir)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/dulwich\/repo.py\", line 1240, in discover\n    return cls(path)\n  File \"\/Users\/marc\/miniconda3\/envs\/py39\/lib\/python3.9\/site-packages\/dulwich\/repo.py\", line 1172, in __init__\n    raise UnsupportedExtension(extension)\ndulwich.repo.UnsupportedExtension: (b'worktreeConfig', b'true')\n<\/code><\/pre>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-31T10:05:48.624Z",
                "Answer_body":"<p>Unfortunately our default git backend (dulwich) does not currently support the worktreeconfig option. The good news is that it has just been merged <a href=\"https:\/\/github.com\/jelmer\/dulwich\/commit\/a9bbc1604976b991408baba4e451459397467ee2\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Add support for worktreeconfig extension \u00b7 jelmer\/dulwich@a9bbc16 \u00b7 GitHub<\/a>  and will be included in the next dulwich release, meaning we\u2019ll be able to fix the issue as soon as the new dulwich release it out. For now, if it is an option, I\u2019d suggest to disable sparse checkout.<\/p>\n<p>I created <a href=\"https:\/\/github.com\/iterative\/scmrepo\/issues\/175\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">dulwich broken with sparse checkout \u00b7 Issue #175 \u00b7 iterative\/scmrepo \u00b7 GitHub<\/a> to track the issue.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"git spars checkout hello larg number dataset subdirectori far pull dataset git clone pull work perfectli like load subset dataset tri archiv spars checkout git repositori run git spars checkout init git spars checkout add dataset follow pull error unexpect error bworktreeconfig btrue wai fix mayb better wai solv problem place awar simpli run pull recurs dataset given larg number directori deal prefer clone actual need hint highli appreci",
        "Question_preprocessed_content":"git hello larg number dataset subdirectori far pull dataset work perfectli like load subset dataset tri archiv spars checkout git repositori run follow error unexpect error wai fix mayb better wai solv problem place awar simpli run given larg number directori deal prefer clone actual need hint highli appreci",
        "Question_gpt_summary_original":"The user has a large number of datasets in subdirectories and wants to load only a subset of them. They tried to achieve this by doing a sparse checkout of the git repository, but encountered an error when running \"git sparse-checkout init & git sparse-checkout add .dvc dataset_name followed by dvc pull\". The user is looking for a solution to this problem and prefers to clone only the directories that they need.",
        "Question_gpt_summary":"user larg number dataset subdirectori want load subset tri achiev spars checkout git repositori encount error run git spars checkout init git spars checkout add dataset follow pull user look solut problem prefer clone directori need",
        "Answer_original_content":"provid traceback run pull flag traceback traceback recent file user marc miniconda env lib python site packag cli init line main cmd arg func arg file user marc miniconda env lib python site packag cli command line init self repo repo repo uniniti self uniniti file user marc miniconda env lib python site packag repo init line init self ignor file user marc miniconda env lib python site packag repo init line ignor self scm context ignor file file user marc miniconda env lib python site packag funci object line re instanc dict self fget self fget instanc file user marc miniconda env lib python site packag repo init line scm context return scmcontext self scm self config file user marc miniconda env lib python site packag funci object line re instanc dict self fget self fget instanc file user marc miniconda env lib python site packag repo init line scm return scm self root dir scm scm file user marc miniconda env lib python site packag scm line scm return git file user marc miniconda env lib python site packag scmrepo git init line init self backend valu file user marc miniconda env lib python site packag funci seq line return iter seq file user marc miniconda env lib python collect abc line iter yield self map kei file user marc miniconda env lib python site packag scmrepo git init line getitem initi backend self arg self kwarg file user marc miniconda env lib python site packag scmrepo git backend dulwich init line init self repo repo discov start root dir file user marc miniconda env lib python site packag dulwich repo line discov return cl path file user marc miniconda env lib python site packag dulwich repo line init rais unsupportedextens extens dulwich repo unsupportedextens worktreeconfig true unfortun default git backend dulwich current support worktreeconfig option good new merg add support worktreeconfig extens jelmer dulwich abbc github includ dulwich releas mean abl fix issu soon new dulwich releas option suggest disabl spars checkout creat dulwich broken spars checkout issu iter scmrepo github track issu",
        "Answer_preprocessed_content":"provid traceback run pull flag traceback unfortun default git backend current support worktreeconfig option good new merg add support worktreeconfig extens github includ dulwich releas mean abl fix issu soon new dulwich releas option suggest disabl spars checkout creat dulwich broken spars checkout issu github track issu",
        "Answer_gpt_summary_original":"Solution:\n- The error encountered when running \"git sparse-checkout init & git sparse-checkout add .dvc dataset_name followed by dvc pull\" is due to the fact that the default git backend (dulwich) does not currently support the worktreeconfig option. The solution is to disable sparse checkout or wait for the next dulwich release that includes the fix. A Github issue has been created to track the problem.",
        "Answer_gpt_summary":"solut error encount run git spars checkout init git spars checkout add dataset follow pull fact default git backend dulwich current support worktreeconfig option solut disabl spars checkout wait dulwich releas includ fix github issu creat track problem"
    },
    {
        "Question_title":"can i take best parameters and best model of optuna function and apply this model directly in my notebook?",
        "Question_body":"<p>i esttablished a function of optuna to find out best model of gbm and xgboost for my data but i was wondering if i can take the best model and apply it directly into my notebook(extracting best model as an object to reuse it later)\nhere is my objective function:<\/p>\n<pre><code>import lightgbm as lgb \nimport optuna\nimport sklearn.metrics\nfrom xgboost import XGBRegressor\nfrom optuna.integration import XGBoostPruningCallback\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nbest_booster = None\ngbm = None\ndef objective(trial,random_state=22,n_jobs=1,early_stopping_rounds=50):\n    \n    regrosser_name = trial.suggest_categorical(&quot;regressor&quot;, [&quot;XGBoost&quot;, &quot;lightgbm&quot;])\n    train_x, valid_x, train_y, valid_y = train_test_split(X_train, y_train, test_size=0.25)\n    dtrain = lgb.Dataset(train_x, label=train_y)\n    # Step 2. Setup values for the hyperparameters:\n    if regrosser_name == 'XGBoost':\n        params = {\n        &quot;verbosity&quot;: 0,  # 0 (silent) - 3 (debug)\n        &quot;objective&quot;: &quot;reg:squarederror&quot;,\n        &quot;n_estimators&quot;: 10000,\n        &quot;max_depth&quot;: trial.suggest_int(&quot;max_depth&quot;, 4, 12),\n        &quot;learning_rate&quot;: trial.suggest_loguniform(&quot;learning_rate&quot;, 0.005, 0.05),\n        &quot;colsample_bytree&quot;: trial.suggest_loguniform(&quot;colsample_bytree&quot;, 0.2, 0.6),\n        &quot;subsample&quot;: trial.suggest_loguniform(&quot;subsample&quot;, 0.4, 0.8),\n        &quot;alpha&quot;: trial.suggest_loguniform(&quot;alpha&quot;, 0.01, 10.0),\n        &quot;lambda&quot;: trial.suggest_loguniform(&quot;lambda&quot;, 1e-8, 10.0),\n        &quot;gamma&quot;: trial.suggest_loguniform(&quot;lambda&quot;, 1e-8, 10.0),\n        &quot;min_child_weight&quot;: trial.suggest_loguniform(&quot;min_child_weight&quot;, 10, 1000),\n        &quot;seed&quot;: random_state,\n        &quot;n_jobs&quot;: n_jobs,\n        }\n        model = XGBRegressor(**params)\n        model.fit(train_x, train_y)\n        y_pred = model.predict(X_val)\n        accuracy_rf = sklearn.metrics.mean_absolute_error(valid_y, y_pred)\n        return accuracy_rf\n    \n        print(rf_max_depth)\n        print(rf_n_estimators)\n        \n    else:\n        param = {\n        &quot;objective&quot;: &quot;binary&quot;,\n        &quot;metric&quot;: &quot;binary_logloss&quot;,\n        &quot;verbosity&quot;: -1,\n        &quot;boosting_type&quot;: &quot;gbdt&quot;,\n        &quot;lambda_l1&quot;: trial.suggest_float(&quot;lambda_l1&quot;, 1e-8, 10.0, log=True),\n        &quot;lambda_l2&quot;: trial.suggest_float(&quot;lambda_l2&quot;, 1e-8, 10.0, log=True),\n        &quot;num_leaves&quot;: trial.suggest_int(&quot;num_leaves&quot;, 2, 256),\n        &quot;feature_fraction&quot;: trial.suggest_float(&quot;feature_fraction&quot;, 0.4, 1.0),\n        &quot;bagging_fraction&quot;: trial.suggest_float(&quot;bagging_fraction&quot;, 0.4, 1.0),\n        &quot;bagging_freq&quot;: trial.suggest_int(&quot;bagging_freq&quot;, 1, 7),\n        &quot;min_child_samples&quot;: trial.suggest_int(&quot;min_child_samples&quot;, 5, 100),\n        }\n        gbm = lgb.train(param, dtrain)\n        preds_gbm = gbm.predict(valid_x)\n        pred_labels_gbm = np.rint(preds_gbm)\n        accuracy_gbm = sklearn.metrics.mean_absolute_error(valid_y, pred_labels_gbm)\n        return accuracy_gbm\n<\/code><\/pre>\n<p>and here is how i tried to solve this issue:<\/p>\n<pre><code>def callback(study, trial):\n    global best_booster\n    if study.best_trial == trial:\n        best_booster = gbm\nif __name__ == &quot;__main__&quot;:\n    study = optuna.create_study(direction=&quot;maximize&quot;)\n    study.optimize(objective, n_trials=100, callbacks=[callback])\n<\/code><\/pre>\n<p>i think its about importing somthing, and if there is any tips on my optuna function please state it<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648541981733,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":585.0,
        "Answer_body":"<p>If I understood your question correctly, then yes, that's what models are for.<\/p>\n<p>Like bring your saved model to your notebook, feed it data that has the same structure  as what you used to train it, and it should serve its purpose.  Or use it in a pipeline.<\/p>\n<p>Even 1 line of the same structure as an np array can be used.  For example, my model predicts whether a loan should be approved or not.<\/p>\n<p>For example, a bank customer wants a loan and submits his information.  The bank officer inputs this info in the system.  The system transforms this information into a single np array with the same structure as the dataset used to train the model.<\/p>\n<p>The model is then used by the system to predict whether the loan should be approved or not.<\/p>\n<p>I save my optuna xgb models as json, e.g.<\/p>\n<p>my_model.get_booster().save_model(f'{savepath}my_model.json')<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71658687",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1649608540120,
        "Question_original_content":"best paramet best model function appli model directli notebook esttablish function best model gbm xgboost data wonder best model appli directli notebook extract best model object reus later object function import lightgbm lgb import import sklearn metric xgboost import xgbregressor integr import xgboostpruningcallback sklearn ensembl import histgradientboostingregressor sklearn model select import train test split sklearn import dataset sklearn ensembl import randomforestregressor sklearn metric import mean absolut error best booster gbm def object trial random state job earli stop round regross trial suggest categor regressor xgboost lightgbm train valid train valid train test split train train test size dtrain lgb dataset train label train step setup valu hyperparamet regross xgboost param verbos silent debug object reg squarederror estim max depth trial suggest int max depth learn rate trial suggest loguniform learn rate colsampl bytre trial suggest loguniform colsampl bytre subsampl trial suggest loguniform subsampl alpha trial suggest loguniform alpha lambda trial suggest loguniform lambda gamma trial suggest loguniform lambda min child weight trial suggest loguniform min child weight seed random state job job model xgbregressor param model fit train train pred model predict val accuraci sklearn metric mean absolut error valid pred return accuraci print max depth print estim param object binari metric binari logloss verbos boost type gbdt lambda trial suggest float lambda log true lambda trial suggest float lambda log true num leav trial suggest int num leav featur fraction trial suggest float featur fraction bag fraction trial suggest float bag fraction bag freq trial suggest int bag freq min child sampl trial suggest int min child sampl gbm lgb train param dtrain pred gbm gbm predict valid pred label gbm rint pred gbm accuraci gbm sklearn metric mean absolut error valid pred label gbm return accuraci gbm tri solv issu def callback studi trial global best booster studi best trial trial best booster gbm main studi creat studi direct maxim studi optim object trial callback callback think import somth tip function state",
        "Question_preprocessed_content":"best paramet best model function appli model directli notebook esttablish function best model gbm xgboost data wonder best model appli directli notebook object function tri solv issu think import somth tip function state",
        "Question_gpt_summary_original":"The user has established an Optuna function to find the best model of GBM and XGBoost for their data. They are wondering if they can take the best model and apply it directly into their notebook as an object to reuse it later. The user has tried to solve this issue by creating a callback function, but they are unsure if they need to import something and are open to any tips on their Optuna function.",
        "Question_gpt_summary":"user establish function best model gbm xgboost data wonder best model appli directli notebook object reus later user tri solv issu creat callback function unsur need import open tip function",
        "Answer_original_content":"understood question correctli ye model like bring save model notebook feed data structur train serv purpos us pipelin line structur arrai exampl model predict loan approv exampl bank custom want loan submit inform bank offic input info transform inform singl arrai structur dataset train model model predict loan approv save xgb model json model booster save model savepath model json",
        "Answer_preprocessed_content":"understood question correctli ye model like bring save model notebook feed data structur train serv purpos us pipelin line structur arrai exampl model predict loan approv exampl bank custom want loan submit inform bank offic input info transform inform singl arrai structur dataset train model model predict loan approv save xgb model json",
        "Answer_gpt_summary_original":"Solution: The user can save their Optuna XGBoost model as a JSON file using the `save_model()` function and then load it into their notebook as an object to reuse it later. They can feed the saved model with data that has the same structure as what they used to train it, and it should serve its purpose. Alternatively, they can use the saved model in a pipeline.",
        "Answer_gpt_summary":"solut user save xgboost model json file save model function load notebook object reus later feed save model data structur train serv purpos altern us save model pipelin"
    },
    {
        "Question_title":"Redshift ML \/ SageMaker - Deploy an existing model artifact to a Redshift Cluster",
        "Question_body":"Is it possible to deploy an existing model artifact from SageMaker to Redshift ML?\n\nFor example, with an Aurora ML you can reference a SageMaker endpoint and then use it as a UDF in a SELECT statement. Redshift ML works a bit differently - when you call CREATE MODEL - the model is trained with SageMaker Autopilot and then deployed to the Redshift Cluster.\n\nWhat if I already have a trained model, can i deploy it to a Redshift Cluster and then use a UDF for Inference?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1609954586000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":68.0,
        "Answer_body":"As of January 30 2021, you can't deploy an existing model artifact from SageMaker to Redshift ML directly with currently announced Redshift ML preview features. But you can reference sagemaker endpoint through a lambda function and use that lambda function as an user defined function in Redshift.\n\nBelow would be the steps:\n\nTrain and deploy your SageMaker model in a SageMaker Endpoint.\nUse Lambda function to reference sagemaker endpoint.\nCreate a Redshift Lambda UDF referring above lambda function to run predictions.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCMYCx28qRe-MOCIfj91Y2g\/redshift-ml-sage-maker-deploy-an-existing-model-artifact-to-a-redshift-cluster",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-06T17:52:12.000Z",
                "Answer_score":0,
                "Answer_body":"As of January 30 2021, you can't deploy an existing model artifact from SageMaker to Redshift ML directly with currently announced Redshift ML preview features. But you can reference sagemaker endpoint through a lambda function and use that lambda function as an user defined function in Redshift.\n\nBelow would be the steps:\n\nTrain and deploy your SageMaker model in a SageMaker Endpoint.\nUse Lambda function to reference sagemaker endpoint.\nCreate a Redshift Lambda UDF referring above lambda function to run predictions.",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1609955532000,
        "Question_original_content":"redshift deploi exist model artifact redshift cluster possibl deploi exist model artifact redshift exampl aurora refer endpoint us udf select statement redshift work bit differ creat model model train autopilot deploi redshift cluster train model deploi redshift cluster us udf infer",
        "Question_preprocessed_content":"redshift deploi exist model artifact redshift cluster possibl deploi exist model artifact redshift exampl aurora refer endpoint us udf select statement redshift work bit differ creat model model train autopilot deploi redshift cluster train model deploi redshift cluster us udf infer",
        "Question_gpt_summary_original":"The user is facing a challenge of deploying an existing model artifact from SageMaker to Redshift ML. They are unsure if it is possible to deploy a trained model to a Redshift Cluster and use a UDF for inference. The user notes that Redshift ML works differently from Aurora ML, where a SageMaker endpoint can be referenced and used as a UDF in a SELECT statement.",
        "Question_gpt_summary":"user face challeng deploi exist model artifact redshift unsur possibl deploi train model redshift cluster us udf infer user note redshift work differ aurora endpoint referenc udf select statement",
        "Answer_original_content":"januari deploi exist model artifact redshift directli current announc redshift preview featur refer endpoint lambda function us lambda function user defin function redshift step train deploi model endpoint us lambda function refer endpoint creat redshift lambda udf refer lambda function run predict",
        "Answer_preprocessed_content":"januari deploi exist model artifact redshift directli current announc redshift preview featur refer endpoint lambda function us lambda function user defin function redshift step train deploi model endpoint us lambda function refer endpoint creat redshift lambda udf refer lambda function run predict",
        "Answer_gpt_summary_original":"Solution: As of January 30, 2021, it is not possible to deploy an existing model artifact from SageMaker to Redshift ML directly. However, a possible solution is to reference the SageMaker endpoint through a lambda function and use that lambda function as a user-defined function in Redshift. The steps involved in this solution are to train and deploy the SageMaker model in a SageMaker endpoint, use a lambda function to reference the SageMaker endpoint, and create a Redshift Lambda UDF referring to the above lambda function to run predictions.",
        "Answer_gpt_summary":"solut januari possibl deploi exist model artifact redshift directli possibl solut refer endpoint lambda function us lambda function user defin function redshift step involv solut train deploi model endpoint us lambda function refer endpoint creat redshift lambda udf refer lambda function run predict"
    },
    {
        "Question_title":"How to mlflow-autolog a sklearn ConfusionMatrixDisplay?",
        "Question_body":"<p>I'm trying to log the plot of a <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_estimator\" rel=\"nofollow noreferrer\">confusion matrix generated with scikit-learn<\/a> for a <em>test<\/em> set using <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.sklearn.html\" rel=\"nofollow noreferrer\">mlflow's support for scikit-learn<\/a>.<\/p>\n<p>For this, I tried something that resemble the code below (I'm using mlflow hosted on Databricks, and <code>sklearn==1.0.1<\/code>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sklearn.datasets\nimport pandas as pd\nimport numpy as np\nimport mlflow\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nmlflow.set_tracking_uri(&quot;databricks&quot;)\nmlflow.set_experiment(&quot;\/Users\/name.surname\/plotcm&quot;)\n\ndata = sklearn.datasets.fetch_20newsgroups(categories=['alt.atheism', 'sci.space'])\n\ndf = pd.DataFrame(data = np.c_[data['data'], data['target']])\\\n       .rename({0:'text', 1:'class'}, axis = 'columns')\n\ntrain, test = train_test_split(df)\n\nmy_pipeline = Pipeline([\n    ('vectorizer', TfidfVectorizer()),\n    ('classifier', SGDClassifier(loss='modified_huber')),\n])\n\nmlflow.sklearn.autolog()\n\nfrom sklearn.metrics import ConfusionMatrixDisplay # should I import this after the call to `.autolog()`?\n\nmy_pipeline.fit(train['text'].values, train['class'].values)\n\ncm = ConfusionMatrixDisplay.from_predictions(\n      y_true=test[&quot;class&quot;], y_pred=my_pipeline.predict(test[&quot;text&quot;])\n  )\n<\/code><\/pre>\n<p>while the confusion matrix for the training set is saved in my mlflow run, no png file is created in the mlflow frontend for the <code>test<\/code> set.<\/p>\n<p>If I try to add<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>cm.figure_.savefig('test_confusion_matrix.png')\nmlflow.log_artifact('test_confusion_matrix.png')\n<\/code><\/pre>\n<p>that does the job, but requires explicitly logging the artifact.<\/p>\n<p>Is there an idiomatic\/proper way to autolog the confusion matrix computed using a test set after <code>my_pipeline.fit()<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657892681357,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1658083880967,
        "Question_score":0.0,
        "Question_view_count":157.0,
        "Answer_body":"<p>The proper way to do this is to use <code>mlflow.log_figure<\/code> as a fluent API announced in <code>MLflow 1.13.0<\/code>. You can read the documentation <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_figure\" rel=\"nofollow noreferrer\">here<\/a>. This code will do the job.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.log_figure(cm.figure_, 'test_confusion_matrix.png')\n<\/code><\/pre>\n<p>This function implicitly store the image, and then calls <code>log_artifact<\/code> against that path, something like you did.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72994988",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1658304934100,
        "Question_original_content":"autolog sklearn confusionmatrixdisplai try log plot confus matrix gener scikit learn test set support scikit learn tri resembl code host databrick sklearn import sklearn dataset import panda import numpi import sklearn pipelin import pipelin sklearn linear model import sgdclassifi sklearn featur extract text import tfidfvector sklearn model select import train test split set track uri databrick set experi user surnam plotcm data sklearn dataset fetch newsgroup categori alt atheism sci space datafram data data data data target renam text class axi column train test train test split pipelin pipelin vector tfidfvector classifi sgdclassifi loss modifi huber sklearn autolog sklearn metric import confusionmatrixdisplai import autolog pipelin fit train text valu train class valu confusionmatrixdisplai predict true test class pred pipelin predict test text confus matrix train set save run png file creat frontend test set try add figur savefig test confus matrix png log artifact test confus matrix png job requir explicitli log artifact idiomat proper wai autolog confus matrix comput test set pipelin fit",
        "Question_preprocessed_content":"autolog sklearn confusionmatrixdisplai try log plot confus matrix gener test set support tri resembl code confus matrix train set save run png file creat frontend set try add job requir explicitli log artifact wai autolog confus matrix comput test set",
        "Question_gpt_summary_original":"The user is trying to log the plot of a confusion matrix generated with scikit-learn for a test set using mlflow's support for scikit-learn. However, while the confusion matrix for the training set is saved in the mlflow run, no png file is created in the mlflow frontend for the test set. The user is looking for an idiomatic\/proper way to autolog the confusion matrix computed using a test set after my_pipeline.fit().",
        "Question_gpt_summary":"user try log plot confus matrix gener scikit learn test set support scikit learn confus matrix train set save run png file creat frontend test set user look idiomat proper wai autolog confus matrix comput test set pipelin fit",
        "Answer_original_content":"proper wai us log figur fluent api announc read document code job log figur figur test confus matrix png function implicitli store imag call log artifact path like",
        "Answer_preprocessed_content":"proper wai us fluent api announc read document code job function implicitli store imag call path like",
        "Answer_gpt_summary_original":"Solution: The proper way to autolog the confusion matrix computed using a test set after my_pipeline.fit() is to use mlflow.log_figure as a fluent API announced in MLflow 1.13.0. The function mlflow.log_figure(cm.figure_, 'test_confusion_matrix.png') implicitly stores the image and then calls log_artifact against that path.",
        "Answer_gpt_summary":"solut proper wai autolog confus matrix comput test set pipelin fit us log figur fluent api announc function log figur figur test confus matrix png implicitli store imag call log artifact path"
    },
    {
        "Question_title":"Assume Sagemaker Notebook instance role from Docker container with default network mode",
        "Question_body":"<p>I have an interesting use case and a problem.<\/p>\n<p>We are leveraging <strong>Sagemaker Notebooks<\/strong> as a development environment for our data science teams. These notebooks are essentially EC2 instances with a (relatively) nice IDE (not as good as Cloud9, though).<\/p>\n<p>In addition, we are running some docker containers on these instances. However, we are forced to use <code>--network=host<\/code> mode, otherwise, the role assigned to the Notebook Instance is not assumed inside the docker container.<\/p>\n<p>On the host (here <code>1234567890<\/code> is our account number, and <code>DataScientist<\/code> is the role attached to the Sagemaker Notebook instance):<\/p>\n<pre><code>$ aws sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>Running the same command inside a Docker container with <code>--network=host<\/code> produces the same result:<\/p>\n<pre><code>$ docker run --network host amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>However, it doesn't work with Docker <code>--network=bridge<\/code>:<\/p>\n<pre><code>$ docker run amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAIMGPPFPT5T6N7BYX6:i-0b2a9080d5ed1cb98&quot;,\n    &quot;Account&quot;: &quot;366152344081&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::366152344081:assumed-role\/BaseNotebookInstanceEc2InstanceRole\/i-0b2a9080d5ed1cb98&quot;\n}\n<\/code><\/pre>\n<p>As you can see, it's a completely different role being assumed. Notice the account number 366152344081 and the role ARN - it's sth internal to AWS.<\/p>\n<p>We would like to keep the default networking option for Docker (bridge) and at the same time be able to assume the correct role (the one attached to SageMaker Notebook instance e.g. <code>DataScientist<\/code> in our case) attached to the host system (Sagemaker Notebook). Are there any hacks (e.g. iptable rules, etc.) to achieve that?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1628255185970,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1639139946700,
        "Question_score":0.0,
        "Question_view_count":195.0,
        "Answer_body":"<p>If we look at the networks that were created on a clean Sagemaker Notebook instance, we can notice a user-defined bridge network named <code>sagemaker-local<\/code>:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nf1d5a59a8c9e        bridge              bridge              local\n6142e6764495        host                host                local\n194adfb00f0a        none                null                local\n99de6c086aa8        sagemaker-local     bridge              local\n<\/code><\/pre>\n<p>If we then attach to this custom bridge, we will be able to assume the correct role (the one attached to the Sagemaker Notebook instance itself):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker run --network sagemaker-local amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<hr \/>\n<p><strong>UPDATE<\/strong><\/p>\n<p>As of this writing (10 Dec 2021) you don't need to attach to <code>sagemaker-local<\/code> bridge network anymore, the default <code>bridge<\/code> will work as well (note <code>--network bridge<\/code> is implicit in this call):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker run amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>Make sure you restart your SageMaker Notebook instance.<\/p>\n<p>Also, <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/setup.sh\" rel=\"nofollow noreferrer\">here<\/a> I found some manual patching (iptables etc.), but with the update it's already patched.<\/p>\n<p>Thanks to AWS who fixed this :)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1639131288447,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68682085",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1638951877412,
        "Question_original_content":"assum notebook instanc role docker contain default network mode interest us case problem leverag notebook develop environ data scienc team notebook essenti instanc rel nice id good cloud addit run docker contain instanc forc us network host mode role assign notebook instanc assum insid docker contain host account number datascientist role attach notebook instanc aw st caller ident userid aroaupvgymmvxxj account arn arn aw st assum role datascientist run command insid docker contain network host produc result docker run network host amazon aw cli st caller ident userid aroaupvgymmvxxj account arn arn aw st assum role datascientist work docker network bridg docker run amazon aw cli st caller ident userid aroaimgppfpttnbyx badedcb account arn arn aw st assum role basenotebookinstanceecinstancerol badedcb complet differ role assum notic account number role arn sth intern aw like default network option docker bridg time abl assum correct role attach notebook instanc datascientist case attach host notebook hack iptabl rule achiev",
        "Question_preprocessed_content":"assum notebook instanc role docker contain default network mode interest us case problem leverag notebook develop environ data scienc team notebook essenti instanc nice id addit run docker contain instanc forc us mode role assign notebook instanc assum insid docker contain host run command insid docker contain produc result work docker complet differ role assum notic account number role arn sth intern aw like default network option docker time abl assum correct role attach host hack achiev",
        "Question_gpt_summary_original":"The user is facing a challenge in assuming the correct role attached to the Sagemaker Notebook instance inside a Docker container. They are forced to use the \"--network=host\" mode to assume the role, but they would like to use the default networking option for Docker (bridge) while still being able to assume the correct role. When running the same command inside a Docker container with \"--network=host\", the correct role is assumed, but it doesn't work with \"--network=bridge\". The user is seeking a solution to this problem.",
        "Question_gpt_summary":"user face challeng assum correct role attach notebook instanc insid docker contain forc us network host mode assum role like us default network option docker bridg abl assum correct role run command insid docker contain network host correct role assum work network bridg user seek solut problem",
        "Answer_original_content":"look network creat clean notebook instanc notic user defin bridg network name local docker network network driver scope fdaac bridg bridg local host host local adfbfa null local decaa local bridg local attach custom bridg abl assum correct role attach notebook instanc docker run network local amazon aw cli st caller ident userid aroaupvgymmvxxj account arn arn aw st assum role datascientist updat write dec need attach local bridg network anymor default bridg work note network bridg implicit docker run amazon aw cli st caller ident userid aroaupvgymmvxxj account arn arn aw st assum role datascientist sure restart notebook instanc manual patch iptabl updat patch thank aw fix",
        "Answer_preprocessed_content":"look network creat clean notebook instanc notic bridg network name attach custom bridg abl assum correct role updat write need attach bridg network anymor default work sure restart notebook instanc manual patch updat patch thank aw fix",
        "Answer_gpt_summary_original":"Solution:\n- The user can attach to the custom bridge network named \"sagemaker-local\" to assume the correct role attached to the Sagemaker Notebook instance inside a Docker container.\n- As of December 10, 2021, the default \"bridge\" network will work as well, and the user doesn't need to attach to \"sagemaker-local\" anymore.",
        "Answer_gpt_summary":"solut user attach custom bridg network name local assum correct role attach notebook instanc insid docker contain decemb default bridg network work user need attach local anymor"
    },
    {
        "Question_title":"Use previous version of model as initial weights",
        "Question_body":"<p>I have a pipeline the output of which is a model tracked with dvc and lets say it is tagged v0.1.0. I would like to use the weights of  this model as the initial weights in training a new model. I was wondering what the best way to achieve this would be?<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1602183567290,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":482.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/davesteps\">@davesteps<\/a>!<\/p>\n<p>So in this case what you could do is to leverage <code>dvc import<\/code> to achieve your desired results.<\/p>\n<ol>\n<li>You create model training stage eg.<br>\n<code>dvc run -n train_model -d -d train.py data -o model python train.py<\/code>\n<\/li>\n<li>commit and tag the project<\/li>\n<li>use <code>dvc import . model --rev {tag} -o initial_weights<\/code>\n<\/li>\n<li>\n<code>dvc run -n retrain_model -d data -d initial_weights -d train.py -o retrained_model python train.py<\/code> - I assume here that your <code>train.py<\/code> code is capable of reconginzing if <code>initial_weights<\/code> exists and using them if so - also in that case output should be under <code>retrained_model<\/code> instead of <code>model<\/code>\n<\/li>\n<li>commit repo state, and tag if necessary<\/li>\n<li>next repetitions are easier: if you want to reuse <code>retrained_model<\/code> - just run <code>dvc import . retrained_model --rev {new_tag} -o initial_weights<\/code> - since you use only one initial_weights you can override them<\/li>\n<li>Repeat commiting, tagging and 6. as long as necessary<\/li>\n<\/ol>\n<p>Some notes:<br>\nThe use case that you described could ideally be solved, if we would allow our stages to have <code>model<\/code> both as input and output. This case is currently not supported, due to potential problems with reproducibility of the project. I guess that with recent development of our <code>experiments<\/code> feature (<a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/4591\">https:\/\/github.com\/iterative\/dvc\/pull\/4591<\/a>) we could reconsider implementing them. I can\u2019t find any open issue for supporting them. <a class=\"mention\" href=\"\/u\/davesteps\">@davesteps<\/a> Could I ask you to create issue on github for circular dependencies support?<\/p>. <p>Thanks for the reply <a class=\"mention\" href=\"\/u\/paffciu\">@Paffciu<\/a>. Yes I will create feature request for circular dependency support. I think your solution of using <code>dvc import<\/code> will work. I simply modified my <code>train<\/code> stage to have a dependency on the <code>initial_weights<\/code> then a param to optional load them. Just need to remember to run the <code>dvc import<\/code> prior to <code>dvc repro<\/code> to make sure I am starting with the correct version. It would be really nice to have some way to just specify the version number of weights to load as a param, I guess this might be possible using the python api inside <code>train.py<\/code>?<\/p>. <p>The issue with using <code>import<\/code> is that it does not overwrite the existing, so I would need to manually delete every time:<\/p>\n<pre><code class=\"lang-auto\">ERROR: failed to import 'model' from '.'. - Paths for outs:           \n'initial_weights'('initial_weights.dvc')\n'initial_weights\/model'('initial_weights\/model.dvc')\noverlap. To avoid unpredictable behaviour, rerun command with non overlapping outs paths.\n<\/code><\/pre>\n<p>I have also experimented a solution using the <code>get<\/code> instead of <code>import<\/code>. The trouble with <code>get<\/code> is that it does not look in the local cache for the model and downloads from remote every time. Not sure if there is some reason <code>get<\/code> couldn\u2019t look in the cache?<\/p>. <p><a class=\"mention\" href=\"\/u\/davesteps\">@davesteps<\/a><br>\n<code>dvc import<\/code> (latest version) overwrites  existing paths - what is your <code>dvc version<\/code> result?<\/p>\n<p>What you seem to encounter is two <code>.dvc<\/code> files having overlapping paths - and DVC does not allow that. I am not sure, since I don\u2019t see your repo, but it seems to me like first you did <code>dvc import . initial_weights<\/code> and then tried to <code>dvc import . initial_weights\/model -o initial_weights\/model<\/code> - Am I right?<\/p>. <p>Hi <a class=\"mention\" href=\"\/u\/paffciu\">@Paffciu<\/a>. I have upgraded to version 1.8.2 but still get the issue. I think it might be something to do with the fact that the <code>model<\/code> object being imported is actually directory. I have tried importing a dvc tracked yaml file and that works fine, but then on another directory \u2018test_data\u2019 and get the same error message.<\/p>\n<p>Edit: in fact i can do <code>dvc import . model\/model-weights.h5 --rev v0.1.1  -o initial_weights.h5<\/code> so even though the whole dir is tracked i can import the contents separately <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<pre><code class=\"lang-auto\">DVC version: 1.8.2 (pip)\n---------------------------------\nPlatform: Python 3.6.9 on Linux-5.4.0-48-generic-x86_64-with-Ubuntu-18.04-bionic\nSupports: http, https, s3\nCache types: hardlink, symlink\nRepo: dvc, git\n<\/code><\/pre>. <p><a class=\"mention\" href=\"\/u\/davesteps\">@davesteps<\/a><br>\n<code>DVC<\/code> is supporting granular imports - it is possible to pick out singular elements of the directory.<br>\nSo, is your workflow behaving as intended now?<\/p>. <p>Yes <a class=\"mention\" href=\"\/u\/paffciu\">@Paffciu<\/a> thanks for your help, using  <code>dvc import . model\/model-weights.h5 --rev v0.1.1 -o initial_weights.h5<\/code> works as desired.<\/p>\n<p>I think I have three issues to create on github 1) circular dependencies support 2) The above error when re-importing a directory 3) Request that <code>get<\/code> tries the local cache first before getting from remote.<\/p>. <ol>\n<li>and 3. - sound like valid issues<\/li>\n<li>I am not sure if it\u2019s a valid bug - do you remember the steps you took which resulted in that error? I reckon there might have been some error in output generation \/ specifying import paths. But it\u2019s definitely worth investigating, so if you are willing to, you might create an issue for that as well. <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\">\n<\/li>\n<\/ol>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/use-previous-version-of-model-as-initial-weights\/525",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-09T11:07:49.235Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/davesteps\">@davesteps<\/a>!<\/p>\n<p>So in this case what you could do is to leverage <code>dvc import<\/code> to achieve your desired results.<\/p>\n<ol>\n<li>You create model training stage eg.<br>\n<code>dvc run -n train_model -d -d train.py data -o model python train.py<\/code>\n<\/li>\n<li>commit and tag the project<\/li>\n<li>use <code>dvc import . model --rev {tag} -o initial_weights<\/code>\n<\/li>\n<li>\n<code>dvc run -n retrain_model -d data -d initial_weights -d train.py -o retrained_model python train.py<\/code> - I assume here that your <code>train.py<\/code> code is capable of reconginzing if <code>initial_weights<\/code> exists and using them if so - also in that case output should be under <code>retrained_model<\/code> instead of <code>model<\/code>\n<\/li>\n<li>commit repo state, and tag if necessary<\/li>\n<li>next repetitions are easier: if you want to reuse <code>retrained_model<\/code> - just run <code>dvc import . retrained_model --rev {new_tag} -o initial_weights<\/code> - since you use only one initial_weights you can override them<\/li>\n<li>Repeat commiting, tagging and 6. as long as necessary<\/li>\n<\/ol>\n<p>Some notes:<br>\nThe use case that you described could ideally be solved, if we would allow our stages to have <code>model<\/code> both as input and output. This case is currently not supported, due to potential problems with reproducibility of the project. I guess that with recent development of our <code>experiments<\/code> feature (<a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/4591\">https:\/\/github.com\/iterative\/dvc\/pull\/4591<\/a>) we could reconsider implementing them. I can\u2019t find any open issue for supporting them. <a class=\"mention\" href=\"\/u\/davesteps\">@davesteps<\/a> Could I ask you to create issue on github for circular dependencies support?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-10-09T18:14:00.381Z",
                "Answer_body":"<p>Thanks for the reply <a class=\"mention\" href=\"\/u\/paffciu\">@Paffciu<\/a>. Yes I will create feature request for circular dependency support. I think your solution of using <code>dvc import<\/code> will work. I simply modified my <code>train<\/code> stage to have a dependency on the <code>initial_weights<\/code> then a param to optional load them. Just need to remember to run the <code>dvc import<\/code> prior to <code>dvc repro<\/code> to make sure I am starting with the correct version. It would be really nice to have some way to just specify the version number of weights to load as a param, I guess this might be possible using the python api inside <code>train.py<\/code>?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-10-10T09:39:47.924Z",
                "Answer_body":"<p>The issue with using <code>import<\/code> is that it does not overwrite the existing, so I would need to manually delete every time:<\/p>\n<pre><code class=\"lang-auto\">ERROR: failed to import 'model' from '.'. - Paths for outs:           \n'initial_weights'('initial_weights.dvc')\n'initial_weights\/model'('initial_weights\/model.dvc')\noverlap. To avoid unpredictable behaviour, rerun command with non overlapping outs paths.\n<\/code><\/pre>\n<p>I have also experimented a solution using the <code>get<\/code> instead of <code>import<\/code>. The trouble with <code>get<\/code> is that it does not look in the local cache for the model and downloads from remote every time. Not sure if there is some reason <code>get<\/code> couldn\u2019t look in the cache?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-10-10T11:03:36.007Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/davesteps\">@davesteps<\/a><br>\n<code>dvc import<\/code> (latest version) overwrites  existing paths - what is your <code>dvc version<\/code> result?<\/p>\n<p>What you seem to encounter is two <code>.dvc<\/code> files having overlapping paths - and DVC does not allow that. I am not sure, since I don\u2019t see your repo, but it seems to me like first you did <code>dvc import . initial_weights<\/code> and then tried to <code>dvc import . initial_weights\/model -o initial_weights\/model<\/code> - Am I right?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-10-10T16:09:42.470Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/paffciu\">@Paffciu<\/a>. I have upgraded to version 1.8.2 but still get the issue. I think it might be something to do with the fact that the <code>model<\/code> object being imported is actually directory. I have tried importing a dvc tracked yaml file and that works fine, but then on another directory \u2018test_data\u2019 and get the same error message.<\/p>\n<p>Edit: in fact i can do <code>dvc import . model\/model-weights.h5 --rev v0.1.1  -o initial_weights.h5<\/code> so even though the whole dir is tracked i can import the contents separately <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<pre><code class=\"lang-auto\">DVC version: 1.8.2 (pip)\n---------------------------------\nPlatform: Python 3.6.9 on Linux-5.4.0-48-generic-x86_64-with-Ubuntu-18.04-bionic\nSupports: http, https, s3\nCache types: hardlink, symlink\nRepo: dvc, git\n<\/code><\/pre>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-10-12T08:51:37.706Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/davesteps\">@davesteps<\/a><br>\n<code>DVC<\/code> is supporting granular imports - it is possible to pick out singular elements of the directory.<br>\nSo, is your workflow behaving as intended now?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-10-12T19:33:46.716Z",
                "Answer_body":"<p>Yes <a class=\"mention\" href=\"\/u\/paffciu\">@Paffciu<\/a> thanks for your help, using  <code>dvc import . model\/model-weights.h5 --rev v0.1.1 -o initial_weights.h5<\/code> works as desired.<\/p>\n<p>I think I have three issues to create on github 1) circular dependencies support 2) The above error when re-importing a directory 3) Request that <code>get<\/code> tries the local cache first before getting from remote.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-10-13T11:04:32.310Z",
                "Answer_body":"<ol>\n<li>and 3. - sound like valid issues<\/li>\n<li>I am not sure if it\u2019s a valid bug - do you remember the steps you took which resulted in that error? I reckon there might have been some error in output generation \/ specifying import paths. But it\u2019s definitely worth investigating, so if you are willing to, you might create an issue for that as well. <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\">\n<\/li>\n<\/ol>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"us previou version model initi weight pipelin output model track let tag like us weight model initi weight train new model wonder best wai achiev",
        "Question_preprocessed_content":"us previou version model initi weight pipelin output model track let tag like us weight model initi weight train new model wonder best wai achiev",
        "Question_gpt_summary_original":"The user wants to use the weights of a previously trained model as initial weights for a new model, but is unsure of the best way to do so.",
        "Question_gpt_summary":"user want us weight previous train model initi weight new model unsur best wai",
        "Answer_original_content":"davestep case leverag import achiev desir result creat model train stage run train model train data model python train commit tag project us import model rev tag initi weight run retrain model data initi weight train retrain model python train assum train code capabl reconginz initi weight exist case output retrain model instead model commit repo state tag necessari repetit easier want reus retrain model run import retrain model rev new tag initi weight us initi weight overrid repeat commit tag long necessari note us case describ ideal solv allow stage model input output case current support potenti problem reproduc project guess recent develop experi featur http github com iter pull reconsid implement open issu support davestep ask creat issu github circular depend support thank repli paffciu ye creat featur request circular depend support think solut import work simpli modifi train stage depend initi weight param option load need rememb run import prior repro sure start correct version nice wai specifi version number weight load param guess possibl python api insid train issu import overwrit exist need manual delet time error fail import model path out initi weight initi weight initi weight model initi weight model overlap avoid unpredict behaviour rerun command non overlap out path experi solut instead import troubl look local cach model download remot time sure reason look cach davestep import latest version overwrit exist path version result encount file have overlap path allow sure dont repo like import initi weight tri import initi weight model initi weight model right paffciu upgrad version issu think fact model object import actual directori tri import track yaml file work fine directori test data error messag edit fact import model model weight rev initi weight dir track import content separ version pip platform python linux gener ubuntu bionic support http http cach type hardlink symlink repo git davestep support granular import possibl pick singular element directori workflow behav intend ye paffciu thank help import model model weight rev initi weight work desir think issu creat github circular depend support error import directori request tri local cach get remot sound like valid issu sure valid bug rememb step took result error reckon error output gener specifi import path definit worth investig will creat issu",
        "Answer_preprocessed_content":"case leverag achiev desir result creat model train stage commit tag project us assum code capabl reconginz exist case output instead commit repo state tag necessari repetit easier want reus run us overrid repeat commit tag long necessari note us case describ ideal solv allow stage input output case current support potenti problem reproduc project guess recent develop featur reconsid implement open issu support ask creat issu github circular depend support thank repli ye creat featur request circular depend support think solut work simpli modifi stage depend param option load need rememb run prior sure start correct version nice wai specifi version number weight load param guess possibl python api insid issu overwrit exist need manual delet time experi solut instead troubl look local cach model download remot time sure reason look cach overwrit exist path result encount file have overlap path allow sure dont repo like tri right upgrad version issu think fact object import actual directori tri import track yaml file work fine directori error messag edit fact dir track import content separ support granular import possibl pick singular element directori workflow behav intend ye thank help work desir think issu creat github circular depend support error directori request tri local cach get sound like valid issu sure valid bug rememb step took result error reckon error output gener specifi import path definit worth investig will creat issu",
        "Answer_gpt_summary_original":"Solution:\n- Use `dvc import` to import the weights of the previously trained model as initial weights for the new model.\n- Create a model training stage and commit and tag the project.\n- Use `dvc import . model --rev {tag} -o initial_weights` to import the weights.\n- Modify the `train` stage to have a dependency on the `initial_weights` then a param to optionally load them.\n- Run `dvc import` prior to `dvc repro` to make sure to start with the correct version.\n- Use `dvc import . retrained_model --rev {new_tag} -o initial_weights` to reuse the `retrained_model`.\n- Repeat committing, tagging, and step 6",
        "Answer_gpt_summary":"solut us import import weight previous train model initi weight new model creat model train stage commit tag project us import model rev tag initi weight import weight modifi train stage depend initi weight param option load run import prior repro sure start correct version us import retrain model rev new tag initi weight reus retrain model repeat commit tag step"
    },
    {
        "Question_title":"azure cosmos db as a datastore in ml",
        "Question_body":"Hi, I'm wondering if I can register azure cosmos db as a datastore in azure machine learning?\nFrom your documentation, it seems not https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore%28class%29?view=azure-ml-py\n\nDo you have a plan to implement the feature in near future?\n\n\nAny recommended alternative solutions for now?\n\nThanks.",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1597331768270,
        "Question_favorite_count":5.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Hi, thanks for reaching out. Currently, Cosmos DB isn't a supported datasource when using Azure ML datastores. However, the product team are aware of this request and will provide updates accordingly. An alternative for now will be to use Azure ML Studio (Classic) which supports Cosmos DB as data source. You can also try a heuristic approach via Execute Python Script module in Designer to import data using python. Hope this helps.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/66297\/azure-cosmos-db-as-a-datastore-in-ml.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-08-14T22:43:18.18Z",
                "Answer_score":0,
                "Answer_body":"Hi, thanks for reaching out. Currently, Cosmos DB isn't a supported datasource when using Azure ML datastores. However, the product team are aware of this request and will provide updates accordingly. An alternative for now will be to use Azure ML Studio (Classic) which supports Cosmos DB as data source. You can also try a heuristic approach via Execute Python Script module in Designer to import data using python. Hope this helps.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-10-14T06:27:11.747Z",
                "Answer_score":2,
                "Answer_body":"Hi, Is there an update on this as a year has passed? For us, Cosmos DB is an important datasource (with no workable workarounds ), and not being able to use it as a datastore in Azure ML is forcing us to make a choice to continue with Azure ML or not.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-20T13:32:02.98Z",
                "Answer_score":0,
                "Answer_body":"I'm also interested in an update. We're in the process of migrating to azure and have lots of metadata associated with our training set stored in json files. It would be very nice to store it as a Table under our storage account and connect to it from AzureML via a datastore.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":1597444998180,
        "Question_original_content":"azur cosmo datastor wonder regist azur cosmo datastor document http doc microsoft com python api core core datastor class view azur plan implement featur near futur recommend altern solut thank",
        "Question_preprocessed_content":"azur cosmo datastor wonder regist azur cosmo datastor document plan implement featur near futur recommend altern solut thank",
        "Question_gpt_summary_original":"The user is facing a challenge of not being able to register Azure Cosmos DB as a datastore in Azure Machine Learning, as it is not supported according to the documentation. The user is inquiring about any plans to implement this feature in the future and is also seeking alternative solutions for the time being.",
        "Question_gpt_summary":"user face challeng abl regist azur cosmo datastor support accord document user inquir plan implement featur futur seek altern solut time",
        "Answer_original_content":"thank reach current cosmo isn support datasourc datastor product team awar request provid updat accordingli altern us studio classic support cosmo data sourc try heurist approach execut python script modul design import data python hope help",
        "Answer_preprocessed_content":"thank reach current cosmo isn support datasourc datastor product team awar request provid updat accordingli altern us studio support cosmo data sourc try heurist approach execut python script modul design import data python hope help",
        "Answer_gpt_summary_original":"Solution:\n- Currently, Azure Cosmos DB is not a supported datastore in Azure Machine Learning.\n- The product team is aware of this request and will provide updates accordingly.\n- An alternative solution for now is to use Azure ML Studio (Classic) which supports Cosmos DB as a data source.\n- Another option is to use the Execute Python Script module in Designer to import data using Python.",
        "Answer_gpt_summary":"solut current azur cosmo support datastor product team awar request provid updat accordingli altern solut us studio classic support cosmo data sourc option us execut python script modul design import data python"
    },
    {
        "Question_title":"Sagemaker Serverless Inference & custom container: Model archiver subprocess fails",
        "Question_body":"<p>I would like to host a model on Sagemaker using the new <a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/12\/amazon-sagemaker-serverless-inference\/?nc1=h_ls\" rel=\"nofollow noreferrer\">Serverless Inference<\/a>.<\/p>\n<p>I wrote my own container for inference and handler following several guides. These are the requirements:<\/p>\n<pre><code>mxnet\nmulti-model-server\nsagemaker-inference\nretrying\nnltk\ntransformers==4.12.4\ntorch==1.10.0\n<\/code><\/pre>\n<p>On non-serverless endpoints, this container works perfectly well. However, with the serverless version I get the following error message when loading the model:<\/p>\n<pre><code>ERROR - \/.sagemaker\/mms\/models\/model already exists.\n<\/code><\/pre>\n<p>The error is thrown by the following subprocess<\/p>\n<pre><code>['model-archiver', '--model-name', 'model', '--handler', '\/home\/model-server\/handler_service.py:handle', '--model-path', '\/opt\/ml\/model', '--export-path', '\/.sagemaker\/mms\/models', '--archive-format', 'no-archive']\n<\/code><\/pre>\n<p>So something that has to do with the <code>model-archiver<\/code> (which I guess is a process from the MMS package?).<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1639400254370,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":373.0,
        "Answer_body":"<p>So the issue really was related to hosting the model using the sagemaker inference toolkit and MMS which always uses the multi-model scenario which is not supported by serverless inference.<\/p>\n<p>I ended up writing my own Flask API which actually is nearly as easy and more customizable. Ping me for details if you're interested.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70335049",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1639671069487,
        "Question_original_content":"serverless infer custom contain model archiv subprocess fail like host model new serverless infer wrote contain infer handler follow guid requir mxnet multi model server infer retri nltk transform torch non serverless endpoint contain work perfectli serverless version follow error messag load model error mm model model exist error thrown follow subprocess model archiv model model handler home model server handler servic handl model path opt model export path mm model archiv format archiv model archiv guess process mm packag",
        "Question_preprocessed_content":"serverless infer custom contain model archiv subprocess fail like host model new serverless infer wrote contain infer handler follow guid requir endpoint contain work perfectli serverless version follow error messag load model error thrown follow subprocess",
        "Question_gpt_summary_original":"The user is encountering an error when trying to host a model on Sagemaker using the new Serverless Inference. They have written their own container for inference and handler, but when loading the model, they receive an error message stating that the model already exists. The error is thrown by the subprocess 'model-archiver', which is part of the MMS package.",
        "Question_gpt_summary":"user encount error try host model new serverless infer written contain infer handler load model receiv error messag state model exist error thrown subprocess model archiv mm packag",
        "Answer_original_content":"issu relat host model infer toolkit mm us multi model scenario support serverless infer end write flask api actual nearli easi customiz ping detail interest",
        "Answer_preprocessed_content":"issu relat host model infer toolkit mm us scenario support serverless infer end write flask api actual nearli easi customiz ping detail interest",
        "Answer_gpt_summary_original":"Solution: The user encountered an error when trying to host a model on Sagemaker using the new Serverless Inference. They have written their own container for inference and handler, but when loading the model, they receive an error message stating that the model already exists. One solution mentioned in the discussion is to write a custom Flask API, which is nearly as easy and more customizable than using the Sagemaker inference toolkit and MMS. However, no details were provided on how to write the custom Flask API.",
        "Answer_gpt_summary":"solut user encount error try host model new serverless infer written contain infer handler load model receiv error messag state model exist solut mention discuss write custom flask api nearli easi customiz infer toolkit mm detail provid write custom flask api"
    },
    {
        "Question_title":"Different run sets within a panel grid",
        "Question_body":"<p>Hello, I want to create a grid panel containing several linecharts but the selected runs are different.<br>\nTo elaborate on my need: I have several algorithms, evaluated across timesteps on several environments. I want one line chart per environment. To illustrate, my final requirement is to get something that looks like this:<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/f\/f2059dafd12562e424e2b66a3e2aabb2b4e1b204.png\" alt=\"Screenshot from 2023-01-19 13-12-26\" data-base62-sha1=\"yx1zXviDcru4wcBIHNfwKEhCyFK\" width=\"435\" height=\"213\"><\/p>\n<p>How would you do that?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1674130582264,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":126.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/qgallouedec\">@qgallouedec<\/a>, thanks for writing in! As you\u2019d like to have a figure with independent charts inside, one option would be to use <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/custom-charts\">custom charts<\/a>. I let you <a href=\"https:\/\/vega.github.io\/vega\/examples\/barley-trellis-plot\/\" rel=\"noopener nofollow ugc\">here<\/a> and <a href=\"https:\/\/vega.github.io\/vega\/examples\/brushing-scatter-plots\/\" rel=\"noopener nofollow ugc\">here<\/a> two Vega examples that may be useful in order to build your figure. Other way I can think of for this is rendering the chart through <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log\/plots#matplotlib-and-plotly-plots\">Plotly\/Matplotlib<\/a> and then log it. Please let me know if any of these would be useful!<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/different-run-sets-within-a-panel-grid\/3721",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-20T18:42:33.197Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/qgallouedec\">@qgallouedec<\/a>, thanks for writing in! As you\u2019d like to have a figure with independent charts inside, one option would be to use <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/custom-charts\">custom charts<\/a>. I let you <a href=\"https:\/\/vega.github.io\/vega\/examples\/barley-trellis-plot\/\" rel=\"noopener nofollow ugc\">here<\/a> and <a href=\"https:\/\/vega.github.io\/vega\/examples\/brushing-scatter-plots\/\" rel=\"noopener nofollow ugc\">here<\/a> two Vega examples that may be useful in order to build your figure. Other way I can think of for this is rendering the chart through <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log\/plots#matplotlib-and-plotly-plots\">Plotly\/Matplotlib<\/a> and then log it. Please let me know if any of these would be useful!<\/p>",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2023-01-26T11:04:05.664Z",
                "Answer_body":"<p>Hi Quentin,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-26T17:13:06.886Z",
                "Answer_body":"<p>Thanks very much for your answer <a class=\"mention\" href=\"\/u\/luis_bergua1\">@luis_bergua1<\/a>! Indeed, that\u2019s what I was looking for. With a reasonable effort, the custom chat can do what I want. However, I\u2019m not a big fan of the integration with Vega, the rendering is not the same as with Wandb charts, and we don\u2019t have access to the same options (or at least, not in the same way). I think it\u2019s a pity that we can\u2019t dissociate the runset within a panel grid , it would allow me to do exactly that, with lower effort and taking advantage of all the nice wandb options.<br>\nBest,<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-31T13:23:07.648Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/qgallouedec\">@qgallouedec<\/a>, thanks for your answer! Happy to hear that the custom chart would work for you. I\u2019ll also share your feedback about having this available without using Vega with our Team. Thanks!<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":1674240153196,
        "Question_original_content":"differ run set panel grid hello want creat grid panel contain linechart select run differ elabor need algorithm evalu timestep environ want line chart environ illustr final requir look like",
        "Question_preprocessed_content":"differ run set panel grid hello want creat grid panel contain linechart select run differ elabor need algorithm evalu timestep environ want line chart environ illustr final requir look like",
        "Question_gpt_summary_original":"The user wants to create a grid panel with several line charts, but the selected runs are different. The user has several algorithms evaluated across timesteps on several environments and wants one line chart per environment. The user needs help to achieve this requirement.",
        "Question_gpt_summary":"user want creat grid panel line chart select run differ user algorithm evalu timestep environ want line chart environ user need help achiev requir",
        "Answer_original_content":"qgallouedec thank write youd like figur independ chart insid option us custom chart let vega exampl us order build figur wai think render chart plotli matplotlib log let know us",
        "Answer_preprocessed_content":"thank write youd like figur independ chart insid option us custom chart let vega exampl us order build figur wai think render chart log let know us",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Using custom charts to create a figure with independent charts inside.\n2. Rendering the chart through Plotly\/Matplotlib and then logging it.\n\nBoth solutions are suggested to help the user achieve the requirement of having one line chart per environment.",
        "Answer_gpt_summary":"possibl solut mention discuss custom chart creat figur independ chart insid render chart plotli matplotlib log solut suggest help user achiev requir have line chart environ"
    },
    {
        "Question_title":"Vertex AI - how to monitor training progress?",
        "Question_body":"<h1>Question<\/h1>\n<p>Is there a way to monitor the console output of model training progress during the Vertex AI training?<\/p>\n<h2>Background<\/h2>\n<p>Suppose we have a Tensorflow\/Keras model training code:<\/p>\n<pre><code>model = keras.Sequential([\n    layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1)\n])\n\noptimizer = tf.keras.optimizers.RMSprop(0.001)\nmodel.compile(\n    loss='mse',\n    optimizer=optimizer,\n    metrics=['mae', 'mse']\n)\n\nEPOCHS = 1000\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n\nearly_history = model.fit(normed_train_data, train_labels, \n                    epochs=EPOCHS, validation_split = 0.2, \n                    callbacks=[early_stop])\n<\/code><\/pre>\n<p>When run the model training from the command line, we can see the progress in the console.<\/p>\n<pre><code>Epoch 1\/1000\nOMP: Info #211: KMP_AFFINITY: decoding x2APIC ids.\nOMP: Info #209: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\nOMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3\nOMP: Info #156: KMP_AFFINITY: 4 available OS procs\nOMP: Info #157: KMP_AFFINITY: Uniform topology\nOMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores\/pkg x 2 threads\/core (2 total cores)\nOMP: Info #213: KMP_AFFINITY: OS proc to physical thread map:\nOMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \nOMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 0 thread 1 \nOMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \nOMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 1 thread 1 \nOMP: Info #249: KMP_AFFINITY: pid 1 tid 17 thread 0 bound to OS proc set 0\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 17 thread 1 bound to OS proc set 1\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 28 thread 2 bound to OS proc set 2\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 29 thread 3 bound to OS proc set 3\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 30 thread 4 bound to OS proc set 0\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 18 thread 5 bound to OS proc set 1\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 31 thread 6 bound to OS proc set 2\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 32 thread 7 bound to OS proc set 3\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 33 thread 8 bound to OS proc set 0\n8\/8 [==============================] - 2s 31ms\/step - loss: 579.6393 - mae: 22.7661 - mse: 579.6393 - val_loss: 571.7239 - val_mae: 22.5494 - val_mse: 571.7239\nEpoch 2\/1000\n8\/8 [==============================] - 0s 7ms\/step - loss: 527.9056 - mae: 21.6268 - mse: 527.9056 - val_loss: 520.5531 - val_mae: 21.3917 - val_mse: 520.5531\n...\n<\/code><\/pre>\n<p>However, if we run the training in the Vertex AI training, there looks to be no menu\/option to see the console output. Not sure if it is logged in Log Explorer. Please help understand how to monitor the training progress realtime.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/tbaBD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tbaBD.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651193148017,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":339.0,
        "Answer_body":"<p>You may view training logs in the <strong>GCP Logs Explorer<\/strong> by using below query.<\/p>\n<pre><code>resource.type=&quot;ml_job&quot;\nresource.labels.job_id=&quot;your-training-custom-job-ID&quot;\n<\/code><\/pre>\n<p>The <em><strong>your-training-custom-job-ID<\/strong><\/em> can be found on the ongoing Vertex AI Training in GCP console as seen on the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Ks4Yf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ks4Yf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Below is the screenshot of the logs for the Vertex AI training in GCP logs explorer using the above query.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/waHu6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/waHu6.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>You may click on <strong>Jump to now<\/strong> to immediately view the latest logs. Also, you may use <strong>Stream Logs<\/strong> option to view <strong>REAL TIME<\/strong> log data which you can also adjust the buffer window in which has certain trade offs. You may refer to this <a href=\"https:\/\/cloud.google.com\/logging\/docs\/view\/streaming-live-tailing\" rel=\"nofollow noreferrer\">documentation<\/a> for more information on streaming logs in GCP logs explorer.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1651563481836,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72051655",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1651562908648,
        "Question_original_content":"monitor train progress question wai monitor consol output model train progress train background suppos tensorflow kera model train code model kera sequenti layer dens activ relu input shape len train dataset kei layer dens activ relu layer dens optim kera optim rmsprop model compil loss mse optim optim metric mae mse epoch earli stop kera callback earlystop monitor val loss patienc earli histori model fit norm train data train label epoch epoch valid split callback earli stop run model train command line progress consol epoch omp info kmp affin decod xapic id omp info kmp affin affin capabl global cpuid leaf info omp info kmp affin initi proc set respect omp info kmp affin avail proc omp info kmp affin uniform topolog omp info kmp affin packag core pkg thread core total core omp info kmp affin proc physic thread map omp info kmp affin proc map packag core thread omp info kmp affin proc map packag core thread omp info kmp affin proc map packag core thread omp info kmp affin proc map packag core thread omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set step loss mae mse val loss val mae val mse epoch step loss mae mse val loss val mae val mse run train train look menu option consol output sure log log explor help understand monitor train progress realtim",
        "Question_preprocessed_content":"monitor train progress question wai monitor consol output model train progress train background suppos model train code run model train command line progress consol run train train look consol output sure log log explor help understand monitor train progress realtim",
        "Question_gpt_summary_original":"The user is facing a challenge in monitoring the console output of model training progress during the Vertex AI training. They are unsure if the output is logged in Log Explorer and are seeking help to understand how to monitor the training progress in real-time.",
        "Question_gpt_summary":"user face challeng monitor consol output model train progress train unsur output log log explor seek help understand monitor train progress real time",
        "Answer_original_content":"view train log gcp log explor queri resourc type job resourc label job train custom job train custom job ongo train gcp consol seen screenshot screenshot log train gcp log explor queri click jump immedi view latest log us stream log option view real time log data adjust buffer window certain trade off refer document inform stream log gcp log explor",
        "Answer_preprocessed_content":"view train log gcp log explor queri ongo train gcp consol seen screenshot screenshot log train gcp log explor queri click jump immedi view latest log us stream log option view real time log data adjust buffer window certain trade off refer document inform stream log gcp log explor",
        "Answer_gpt_summary_original":"Solution: The user can monitor the console output of model training progress during the Vertex AI training by viewing the training logs in the GCP Logs Explorer using a specific query. The query is \"resource.type=ml_job resource.labels.job_id=your-training-custom-job-ID\" where \"your-training-custom-job-ID\" can be found on the ongoing Vertex AI Training in GCP console. The user can click on \"Jump to now\" to immediately view the latest logs or use the \"Stream Logs\" option to view real-time log data. The user can adjust the buffer window, but it has certain trade-offs. The documentation on streaming logs in GCP logs explorer can be referred to for more information.",
        "Answer_gpt_summary":"solut user monitor consol output model train progress train view train log gcp log explor specif queri queri resourc type job resourc label job train custom job train custom job ongo train gcp consol user click jump immedi view latest log us stream log option view real time log data user adjust buffer window certain trade off document stream log gcp log explor refer inform"
    },
    {
        "Question_title":"AWS Sagemaker - ClientError: Data download failed:Could not download",
        "Question_body":"<p>I encountered and error when I deploy my training job in my notebook instance.\nThis what it says:\n<code>&quot;UnexpectedStatusException: Error for Training job tensorflow-training-2021-01-26-09-55-05-768: Failed. Reason: ClientError: Data download failed:Could not download s3:\/\/forex-model-data\/data\/train2001_2020.npz: insufficient disk space&quot;<\/code><\/p>\n<p>I deploy training job to try running it to different instances in 3 epoch. I use ml.c5.4xlarge, ml.c5.18xlarge, ml.m5.24xlarge, also I have two sets of training data, train2001_2020.npz and train2016_2020.npz.<\/p>\n<p>First, I run train2001_2020 to ml.c5.18xlarge and ml.c5.18xlarge and the training job completed, then I switch to train2016_2020 and run it to ml.c5.4xlarge and ml.c5.18xlarge and it goes well. Then when I tried to run it using ml.m5.24xlarge I got an error (quoted above), but my dataset is train2016_2020 not train2001_2020 then when I rerun it again with all other instances it has the same error. What happen?<\/p>\n<p>I stopped the instances and refresh everything, but I encountered same issue.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611668079743,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":774.0,
        "Answer_body":"<p>It's not really clear to all the test are you doing, but that error usually means that there is not enough disk space on the instance you are using for the training job. You can try to increase the additional storage for the instance (you can do in the estimator parameters if you are using the sagemaker SDK in a notebook).<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65902366",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1611831268823,
        "Question_original_content":"clienterror data download fail download encount error deploi train job notebook instanc sai unexpectedstatusexcept error train job tensorflow train fail reason clienterror data download fail download forex model data data train npz insuffici disk space deploi train job try run differ instanc epoch us xlarg xlarg xlarg set train data train npz train npz run train xlarg xlarg train job complet switch train run xlarg xlarg goe tri run xlarg got error quot dataset train train rerun instanc error happen stop instanc refresh encount issu",
        "Question_preprocessed_content":"clienterror data download fail download encount error deploi train job notebook instanc sai deploi train job try run differ instanc epoch us set train data run train job complet switch run goe tri run got error dataset rerun instanc error happen stop instanc refresh encount issu",
        "Question_gpt_summary_original":"The user encountered an error when deploying a training job in their notebook instance on AWS Sagemaker. The error message indicated that there was insufficient disk space to download the training data from an S3 bucket. The user attempted to run the training job on different instances with different sets of training data, and while some instances completed the job successfully, others encountered the same error. The user tried refreshing everything and restarting the instances, but the issue persisted.",
        "Question_gpt_summary":"user encount error deploi train job notebook instanc error messag indic insuffici disk space download train data bucket user attempt run train job differ instanc differ set train data instanc complet job successfulli encount error user tri refresh restart instanc issu persist",
        "Answer_original_content":"clear test error usual mean disk space instanc train job try increas addit storag instanc estim paramet sdk notebook",
        "Answer_preprocessed_content":"clear test error usual mean disk space instanc train job try increas addit storag instanc",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to increase the additional storage for the instance being used for the training job. This can be done in the estimator parameters if the user is using the Sagemaker SDK in a notebook.",
        "Answer_gpt_summary":"solut possibl solut mention discuss increas addit storag instanc train job estim paramet user sdk notebook"
    },
    {
        "Question_title":"Get Experiment that Created Model in MLflow",
        "Question_body":"<p>I want to get the name of the experiment that contains the run that created a registered MLflow model. How can I do this using MLflow, if I just have the name of the model and the version?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1658855429667,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":44.0,
        "Answer_body":"<p>As @Andre has said, I had to write my own function to achieve this,<\/p>\n<pre><code>def get_model_experiment(model_name, model_version):\n    # get run_id of the model version\n    run_id = mlflow_client.get_model_version(model_name, model_version).run_id\n\n    # get the experiment_id from the run_id\n    experiment_id = mlflow_client.get_run(run_id).info.experiment_id\n\n    # get the experiment name from the experiment_id\n    return mlflow_client.get_experiment(experiment_id).name\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73127303",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1659073607990,
        "Question_original_content":"experi creat model want experi contain run creat regist model model version",
        "Question_preprocessed_content":"experi creat model want experi contain run creat regist model model version",
        "Question_gpt_summary_original":"The user is facing a challenge in identifying the name of the experiment that contains the run responsible for creating a registered MLflow model. The user has only the name and version of the model and is seeking guidance on how to retrieve this information using MLflow.",
        "Question_gpt_summary":"user face challeng identifi experi contain run respons creat regist model user version model seek guidanc retriev inform",
        "Answer_original_content":"andr said write function achiev def model experi model model version run model version run client model version model model version run experi run experi client run run info experi experi experi return client experi experi",
        "Answer_preprocessed_content":"said write function achiev",
        "Answer_gpt_summary_original":"Solution: One solution mentioned in the discussion is to write a function that retrieves the experiment name containing the run responsible for creating a registered MLflow model. The function uses the MLflow client to get the run ID of the model version, then gets the experiment ID from the run ID, and finally retrieves the experiment name from the experiment ID.",
        "Answer_gpt_summary":"solut solut mention discuss write function retriev experi contain run respons creat regist model function us client run model version get experi run final retriev experi experi"
    },
    {
        "Question_title":"Is there any equivalent of hyperopts lognormal in Optuna?",
        "Question_body":"<p>I am trying to use Optuna for hyperparameter tuning of my model.<\/p>\n<p>I am stuck in a place where I want to define a search space having lognormal\/normal distribution. It is possible in <code>hyperopt<\/code> using <code>hp.lognormal<\/code>. Is it possible to define such a space using a combination of the existing <code>suggest_<\/code> api of <code>Optuna<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1610971789293,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1610981620636,
        "Question_score":0.0,
        "Question_view_count":140.0,
        "Answer_body":"<p>You could perhaps make use of inverse transforms from <code>suggest_float(..., 0, 1)<\/code> (i.e. U(0, 1)) since Optuna currently doesn't provide <code>suggest_<\/code> variants for those two distributions directly. This example might be a starting point <a href=\"https:\/\/gist.github.com\/hvy\/4ef02ee2945fe50718c71953e1d6381d\" rel=\"nofollow noreferrer\">https:\/\/gist.github.com\/hvy\/4ef02ee2945fe50718c71953e1d6381d<\/a>\nPlease find the code below<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import erfcinv\n\nimport optuna\n\n\ndef objective(trial):\n    # Suggest from U(0, 1) with Optuna.\n    x = trial.suggest_float(&quot;x&quot;, 0, 1)\n\n    # Inverse transform into normal.\n    y0 = norm.ppf(x, loc=0, scale=1)\n\n    # Inverse transform into lognormal.\n    y1 = np.exp(-np.sqrt(2) * erfcinv(2 * x))\n\n    return y0, y1\n\n\nif __name__ == &quot;__main__&quot;:\n    n_objectives = 2  # Normal and lognormal.\n\n    study = optuna.create_study(\n        sampler=optuna.samplers.RandomSampler(),\n        # Could be &quot;maximize&quot;. Does not matter for this demonstration.\n        directions=[&quot;minimize&quot;] * n_objectives,\n    )\n    study.optimize(objective, n_trials=10000)\n\n    fig, axs = plt.subplots(n_objectives)\n    for i in range(n_objectives):\n        axs[i].hist(list(t.values[i] for t in study.trials), bins=100)\n    plt.show()\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1611565726960,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65774253",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1611382397500,
        "Question_original_content":"equival hyperopt lognorm try us hyperparamet tune model stuck place want defin search space have lognorm normal distribut possibl hyperopt lognorm possibl defin space combin exist suggest api",
        "Question_preprocessed_content":"equival hyperopt lognorm try us hyperparamet tune model stuck place want defin search space have distribut possibl possibl defin space combin exist api",
        "Question_gpt_summary_original":"The user is facing a challenge in defining a search space with lognormal\/normal distribution while using Optuna for hyperparameter tuning of their model. They are seeking to know if it is possible to define such a space using a combination of the existing suggest_ api of Optuna.",
        "Question_gpt_summary":"user face challeng defin search space lognorm normal distribut hyperparamet tune model seek know possibl defin space combin exist suggest api",
        "Answer_original_content":"us invers transform suggest float current provid suggest variant distribut directli exampl start point http gist github com hvy efeefecedd code import matplotlib pyplot plt import numpi scipi stat import norm scipi special import erfcinv import def object trial suggest trial suggest float invers transform normal norm ppf loc scale invers transform lognorm exp sqrt erfcinv return main object normal lognorm studi creat studi sampler sampler randomsampl maxim matter demonstr direct minim object studi optim object trial fig ax plt subplot object rang object ax hist list valu studi trial bin plt",
        "Answer_preprocessed_content":"us invers transform current provid variant distribut directli exampl start point code",
        "Answer_gpt_summary_original":"Solution:\nThe user can make use of inverse transforms from `suggest_float(..., 0, 1)` to define a search space with lognormal\/normal distribution since Optuna currently doesn't provide `suggest_` variants for those two distributions directly. The code example provided in the discussion can be used as a starting point.",
        "Answer_gpt_summary":"solut user us invers transform suggest float defin search space lognorm normal distribut current provid suggest variant distribut directli code exampl provid discuss start point"
    },
    {
        "Question_title":"Failure of experiment when using base_dockerfile instead of base_image",
        "Question_body":"<p>I am attempting to submit an experiment to the Azure Machine Learning Service using a custom docker image.  Everything works ok when I provide the docker image, but fails if I choose to provide a dockerfile.<\/p>\n\n<p>The use of a base_dockerfile in the DockerSection object is documented <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment.dockersection?view=azure-ml-py\" rel=\"nofollow noreferrer\">here<\/a> and was added in v1.0.53 of the sdk (as noted <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/service\/azure-machine-learning-release-notes.md\" rel=\"nofollow noreferrer\">here<\/a>)<\/p>\n\n<p>Example code:<\/p>\n\n<pre><code>ds = DockerSection()\nds.enabled = True\nds.base_dockerfile = \"FROM ubuntu:latest RUN echo 'Hello world!'\"\nds.base_image = None\n<\/code><\/pre>\n\n<p>The rest of the code is the same as when running with a predefined image from the registry (e.g. setting base_image in the above code).<\/p>\n\n<p>Example error from ML service is:<\/p>\n\n<blockquote>\n  <p>raise ActivityFailedException(error_details=json.dumps(error,\n  indent=4))\n  azureml.exceptions._azureml_exception.ActivityFailedException:\n  ActivityFailedException:\n          Message: Activity Failed: {\n      \"error\": {\n          \"code\": \"ServiceError\",\n          \"message\": \"InternalServerError\",\n          \"details\": []\n      },\n      \"correlation\": {\n          \"operation\": null,\n          \"request\": \"K\/C4FSnEz74=\"\n      },\n      \"environment\": \"southcentralus\",\n      \"location\": \"southcentralus\",\n      \"time\": \"2019-08-20T16:33:17.130928Z\" }\n          InnerException None\n          ErrorResponse {\"error\": {\"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"ServiceError\\\",\\n<br>\n  \\\"message\\\": \\\"InternalServerError\\\",\\n        \\\"details\\\": []\\n<br>\n  },\\n    \\\"correlation\\\": {\\n        \\\"operation\\\": null,\\n<br>\n  \\\"request\\\": \\\"K\/C4FSnEz74=\\\"\\n  },\\n    \\\"environment\\\":\n  \\\"southcentralus\\\",\\n    \\\"location\\\": \\\"southcentralus\\\",\\n<br>\n  \\\"time\\\": \\\"2019-08-20T16:33:17.130928Z\\\"\\n}\"}}<\/p>\n<\/blockquote>\n\n<p>I've used an example dockerfile in the code above (taken from the SDK documentation) but get the same error if I use the dockerfile that created the base image that works ok from the registry.<\/p>\n\n<p>Any ideas - or pointers to samples where this actually works - appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1566320034643,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":307.0,
        "Answer_body":"<p>Thanks for reporting this issue! This appears to be a bug that our team is investigating.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57578332",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1566323058620,
        "Question_original_content":"failur experi base dockerfil instead base imag attempt submit experi servic custom docker imag work provid docker imag fail choos provid dockerfil us base dockerfil dockersect object document ad sdk note exampl code dockersect enabl true base dockerfil ubuntu latest run echo hello world base imag rest code run predefin imag registri set base imag code exampl error servic rais activityfailedexcept error detail json dump error indent except except activityfailedexcept activityfailedexcept messag activ fail error code serviceerror messag internalservererror detail correl oper null request cfsnez environ southcentralu locat southcentralu time innerexcept errorrespons error messag activ fail error code serviceerror messag internalservererror detail correl oper null request cfsnez environ southcentralu locat southcentralu time exampl dockerfil code taken sdk document error us dockerfil creat base imag work registri idea pointer sampl actual work appreci",
        "Question_preprocessed_content":"failur experi instead attempt submit experi servic custom docker imag work provid docker imag fail choos provid dockerfil us dockersect object document ad sdk exampl code rest code run predefin imag registri exampl error servic rais indent activityfailedexcept messag activ fail correl environ southcentralu locat southcentralu time innerexcept errorrespons exampl dockerfil code error us dockerfil creat base imag work registri idea pointer sampl actual work appreci",
        "Question_gpt_summary_original":"The user is encountering challenges when attempting to submit an experiment to the Azure Machine Learning Service using a custom docker image. The experiment fails when using a base_dockerfile instead of a base_image. The error message received is an ActivityFailedException with an InternalServerError. The user has tried using an example dockerfile from the SDK documentation but still encounters the same error. The user is seeking help or pointers to samples where this issue has been resolved.",
        "Question_gpt_summary":"user encount challeng attempt submit experi servic custom docker imag experi fail base dockerfil instead base imag error messag receiv activityfailedexcept internalservererror user tri exampl dockerfil sdk document encount error user seek help pointer sampl issu resolv",
        "Answer_original_content":"thank report issu appear bug team investig",
        "Answer_preprocessed_content":"thank report issu appear bug team investig",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"How to create Parallel Coordinates plot without sweeps",
        "Question_body":"<p>It would be great to plot hparams without doing sweeps, most of the time I\u2019m doing experiments and I would love the plot to be across runs and not as a sweep. It might be complex to make this feature automated, but I\u2019m fine if it\u2019s within one run, would be great to have something like <code>wandb.plots.ParallelCoordinates<\/code><\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_creation_time":1671394517195,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":146.0,
        "Answer_body":"<p>Hi Faris!<\/p>\n<p>You absolutely can use Parallel coordinates plots without sweeps. The web UI has an option to add additional plots on the top right of the graph section which contains the Parallel Coordinates Plot.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-create-parallel-coordinates-plot-without-sweeps\/3566",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-12-19T04:49:34.290Z",
                "Answer_body":"<p>Hi Faris!<\/p>\n<p>You absolutely can use Parallel coordinates plots without sweeps. The web UI has an option to add additional plots on the top right of the graph section which contains the Parallel Coordinates Plot.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-12-21T18:17:04.195Z",
                "Answer_body":"<p>Hi Faris,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-23T17:55:49.906Z",
                "Answer_body":"<p>Hi Faris , since we have not heard back from you we are going to close this request. If you would like to re-open this conversation, please let us know!<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-13T13:36:47.336Z",
                "Answer_body":"<p>I ran on exactly the same issue, and for me it was the browser. When I switched from Safari to Chrome the \u201cpattern\u201d disappared.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-14T09:56:42.381Z",
                "Answer_body":"<p>hello and sorry for not responding earlier.<\/p>\n<p>Yes I found it, this is indeed what I\u2019m looking for but I\u2019d like to be able to do it programmatically as well, I took some time to play around with it but it seems that I\u2019ll have to add each HParam manually (one at a time). It would be great to be able to add them all because there could be 10s or 100s of hparams, also would be great to have the magic wand that will reduce to only the important hparams<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-14T13:00:13.108Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"farishijazi\" data-post=\"6\" data-topic=\"3566\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/f\/da6949\/40.png\" class=\"avatar\"> farishijazi:<\/div>\n<blockquote>\n<p>also would be great to have the magic wand that will reduce to only the important hparams<\/p>\n<\/blockquote>\n<\/aside>\n<p>You can use the parameter importance plot to find the most important parameters and then only use those Hparams when creating the parallel coordinates plot. That said, I understand it would be nice to wave a magic wand and have the parallel coordinate plot do that magically.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":1671425374288,
        "Question_original_content":"creat parallel coordin plot sweep great plot hparam sweep time experi love plot run sweep complex featur autom fine run great like plot parallelcoordin",
        "Question_preprocessed_content":"creat parallel coordin plot sweep great plot hparam sweep time experi love plot run sweep complex featur autom fine run great like",
        "Question_gpt_summary_original":"The user is facing a challenge in creating a Parallel Coordinates plot without doing sweeps. They would like to plot hparams across runs and not as a sweep, but find it complex to make this feature automated. They suggest having something like \"wandb.plots.ParallelCoordinates\" within one run.",
        "Question_gpt_summary":"user face challeng creat parallel coordin plot sweep like plot hparam run sweep complex featur autom suggest have like plot parallelcoordin run",
        "Answer_original_content":"fari absolut us parallel coordin plot sweep web option add addit plot right graph section contain parallel coordin plot thank ramit",
        "Answer_preprocessed_content":"fari absolut us parallel coordin plot sweep web option add addit plot right graph section contain parallel coordin plot thank ramit",
        "Answer_gpt_summary_original":"Solution: The solution provided in the discussion is to use the web UI option to add additional plots on the top right of the graph section which contains the Parallel Coordinates Plot. This allows the user to plot hparams across runs without doing sweeps.",
        "Answer_gpt_summary":"solut solut provid discuss us web option add addit plot right graph section contain parallel coordin plot allow user plot hparam run sweep"
    },
    {
        "Question_title":"Parameterized SQL query in Azure ML",
        "Question_body":"<p>Background: There seems to be a way to parameterize <code>DataPath<\/code> with <code>PipelineParameter<\/code>\n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb<\/a><\/p>\n<p>But I'd like to parameterize my SQL query with PipelineParameter, for example, with this query<\/p>\n<pre><code>sql_query = &quot;&quot;&quot;\nSELECT id, foo, bar FROM baz\nWHERE baz.id BETWEEN 10 AND 20\n&quot;&quot;&quot;\ndataset = Dataset.Tabular.from_sql_query((sql_datastore, sql_query))\n<\/code><\/pre>\n<p>I'd like to use PipelineParameter to parameterize <code>10<\/code> and <code>20<\/code> as <code>param_1<\/code> and <code>param_2<\/code>. Is this possible?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603497171040,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":188.0,
        "Answer_body":"<p>Found a way to do this:<\/p>\n<p>Pass your params to PythonScriptStep<\/p>\n<pre><code>param_1 = PipelineParameter(name='min_id', default_value=5)\nparam_2 = PipelineParameter(name='max_id', default_value=10)\nsql_datastore = &quot;sql_datastore&quot;\nstep = PythonScriptStep(script_name='script.py', arguments=[param_1, param_2, \nsql_datastore])\n<\/code><\/pre>\n<p>In script.py<\/p>\n<pre><code>min_id_param = sys.argv[1]\nmax_id_param = sys.argv[2]\nsql_datastore_name = sys.argv[3]\nquery = &quot;&quot;&quot;\nSELECT id, foo, bar FROM baz\nWHERE baz.id BETWEEN {} AND {}\n&quot;&quot;&quot;.format(min_id_param, max_id_param)\nrun = Run.get_context()\nsql_datastore = Datastore.get(run.experiment.workspace, sql_datastore_name)\ndataset = Dataset.Tabular.from_sql_query((sql_datastore, query))\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1603755560267,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64508625",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1603727871847,
        "Question_original_content":"parameter sql queri background wai parameter datapath pipelineparamet http github com azur machinelearningnotebook blob master us machin learn pipelin intro pipelin aml pipelin showcas datapath pipelineparamet ipynb like parameter sql queri pipelineparamet exampl queri sql queri select foo bar baz baz dataset dataset tabular sql queri sql datastor sql queri like us pipelineparamet parameter param param possibl",
        "Question_preprocessed_content":"parameter sql queri background wai parameter like parameter sql queri pipelineparamet exampl queri like us pipelineparamet parameter possibl",
        "Question_gpt_summary_original":"The user is trying to parameterize a SQL query in Azure ML using PipelineParameter, specifically to parameterize the values of \"10\" and \"20\" in the query. The user is seeking clarification on whether this is possible.",
        "Question_gpt_summary":"user try parameter sql queri pipelineparamet specif parameter valu queri user seek clarif possibl",
        "Answer_original_content":"wai pass param pythonscriptstep param pipelineparamet min default valu param pipelineparamet max default valu sql datastor sql datastor step pythonscriptstep script script argument param param sql datastor script min param sy argv max param sy argv sql datastor sy argv queri select foo bar baz baz format min param max param run run context sql datastor datastor run experi workspac sql datastor dataset dataset tabular sql queri sql datastor queri",
        "Answer_preprocessed_content":"wai pass param pythonscriptstep",
        "Answer_gpt_summary_original":"Solution: The user can pass the parameters to PythonScriptStep and then use them in the script to parameterize the SQL query. The solution involves defining PipelineParameter for the parameters, passing them to PythonScriptStep, and then using them in the script to parameterize the SQL query.",
        "Answer_gpt_summary":"solut user pass paramet pythonscriptstep us script parameter sql queri solut involv defin pipelineparamet paramet pass pythonscriptstep script parameter sql queri"
    },
    {
        "Question_title":"enhanced speech feature",
        "Question_body":"Hi I have a queryIn Dailogflow if we enable enhanced speech feature, specifically, credit card info (i.e. number), if that is spoken by user, is that stored by Google. Please help",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629336840000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":414.0,
        "Answer_body":"Hello,\n\nI understand you are looking to use the enhanced model in Dialogflow and you are looking to understand the Data Security. Please let me know if my understanding is wrong.\n\nIf that is what you are looking for, then I think you should read this section of this article[0] which addresses this concern. As explained in the doc[0], Google uses the data sent to Dialogflow on the project with data logging enabled. Google uses this data solely to train and improve Google products and services. So, while you'll maintain full ownership of all data that you upload to a project with data logging enabled, there are some terms[1] which I think you should be aware of.\n\n[0]https:\/\/cloud.google.com\/dialogflow\/es\/docs\/speech-enhanced-models#data-security\n[1]https:\/\/cloud.google.com\/dialogflow\/docs\/data-logging-terms\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/enhanced-speech-feature\/td-p\/167747\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Hello,\n\nI understand you are looking to use the enhanced model in Dialogflow and you are looking to understand the Data Security. Please let me know if my understanding is wrong.\n\nIf that is what you are looking for, then I think you should read this section of this article[0] which addresses this concern. As explained in the doc[0], Google uses the data sent to Dialogflow on the project with data logging enabled. Google uses this data solely to train and improve Google products and services. So, while you'll maintain full ownership of all data that you upload to a project with data logging enabled, there are some terms[1] which I think you should be aware of.\n\n[0]https:\/\/cloud.google.com\/dialogflow\/es\/docs\/speech-enhanced-models#data-security\n[1]https:\/\/cloud.google.com\/dialogflow\/docs\/data-logging-terms\n\nView solution in original post"
            },
            {
                "Answer_creation_time":"2021-08-20T06:59:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Hello,\n\nI understand you are looking to use the enhanced model in Dialogflow and you are looking to understand the Data Security. Please let me know if my understanding is wrong.\n\nIf that is what you are looking for, then I think you should read this section of this article[0] which addresses this concern. As explained in the doc[0], Google uses the data sent to Dialogflow on the project with data logging enabled. Google uses this data solely to train and improve Google products and services. So, while you'll maintain full ownership of all data that you upload to a project with data logging enabled, there are some terms[1] which I think you should be aware of.\n\n[0]https:\/\/cloud.google.com\/dialogflow\/es\/docs\/speech-enhanced-models#data-security\n[1]https:\/\/cloud.google.com\/dialogflow\/docs\/data-logging-terms"
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"enhanc speech featur queryin dailogflow enabl enhanc speech featur specif credit card info number spoken user store googl help",
        "Question_preprocessed_content":"enhanc speech featur queryin dailogflow enabl enhanc speech featur specif credit card info spoken user store googl help",
        "Question_gpt_summary_original":"The user is seeking clarification on whether enabling the enhanced speech feature in Dialogflow will result in Google storing any credit card information spoken by the user.",
        "Question_gpt_summary":"user seek clarif enabl enhanc speech featur dialogflow result googl store credit card inform spoken user",
        "Answer_original_content":"hello understand look us enhanc model dialogflow look understand data secur let know understand wrong look think read section articl address concern explain doc googl us data sent dialogflow project data log enabl googl us data sole train improv googl product servic maintain ownership data upload project data log enabl term think awar http cloud googl com dialogflow doc speech enhanc model data secur http cloud googl com dialogflow doc data log term view solut origin post",
        "Answer_preprocessed_content":"hello understand look us enhanc model dialogflow look understand data secur let know understand wrong look think read section articl address concern explain doc googl us data sent dialogflow project data log enabl googl us data sole train improv googl product servic maintain ownership data upload project data log enabl term think awar view solut origin post",
        "Answer_gpt_summary_original":"Solution: The discussion provides a link to an article that addresses the user's concern about data security when using the enhanced speech feature in Dialogflow. The article explains that Google uses the data sent to Dialogflow on the project with data logging enabled solely to train and improve Google products and services. The user will maintain full ownership of all data uploaded to a project with data logging enabled, but there are some terms that the user should be aware of. No credit card information storage solution is mentioned in the discussion.",
        "Answer_gpt_summary":"solut discuss provid link articl address user concern data secur enhanc speech featur dialogflow articl explain googl us data sent dialogflow project data log enabl sole train improv googl product servic user maintain ownership data upload project data log enabl term user awar credit card inform storag solut mention discuss"
    },
    {
        "Question_title":"Which IAM roles and policies should I delete to not being charged by AWS?",
        "Question_body":"<p>Are these roles to be deleted?<\/p>\n<ol>\n<li>AmazonSageMakerServiceCatalogProductsLaunchRole<\/li>\n<li>AmazonSageMakerServiceCatalogProductsUseRole<\/li>\n<li>AWSServiceRoleForAmazonSageMakerNotebooks<\/li>\n<\/ol>\n<p>Are these roles to be deleted?<\/p>\n<ol>\n<li>AmazonSageMakerServiceCatalogProductsUseRole<\/li>\n<li>Plus some execution policies<\/li>\n<\/ol>\n<p>Is Jupyter server within sagemaker studio also be stopped for not being charged?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633708664963,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1634116759467,
        "Question_score":0.0,
        "Question_view_count":105.0,
        "Answer_body":"<p><strong>AWS IAM is a free service<\/strong> - you do not get charged for roles, policies or any other aspect of IAM.<\/p>\n<p>From <a href=\"https:\/\/aws.amazon.com\/iam\/#:%7E:text=IAM%20is%20a%20feature%20of,AWS%20services%20by%20your%20users.\" rel=\"nofollow noreferrer\">the documentation<\/a>:<\/p>\n<blockquote>\n<p>IAM is a feature of your AWS account offered at no additional charge. You will be charged only for use of other AWS services by your users.<\/p>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69498670",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1633709785048,
        "Question_original_content":"iam role polici delet charg aw role delet amazonservicecatalogproductslaunchrol amazonservicecatalogproductsuserol awsserviceroleforamazonnotebook role delet amazonservicecatalogproductsuserol plu execut polici jupyt server studio stop charg",
        "Question_preprocessed_content":"iam role polici delet charg aw role delet amazonservicecatalogproductslaunchrol amazonservicecatalogproductsuserol awsserviceroleforamazonnotebook role delet amazonservicecatalogproductsuserol plu execut polici jupyt server studio stop charg",
        "Question_gpt_summary_original":"The user is seeking guidance on which IAM roles and policies to delete in order to avoid being charged by AWS. They are specifically asking if certain roles, such as AmazonSageMakerServiceCatalogProductsLaunchRole and AmazonSageMakerServiceCatalogProductsUseRole, should be deleted, as well as some execution policies. The user also inquires if stopping the Jupyter server within SageMaker Studio will prevent charges.",
        "Question_gpt_summary":"user seek guidanc iam role polici delet order avoid charg aw specif ask certain role amazonservicecatalogproductslaunchrol amazonservicecatalogproductsuserol delet execut polici user inquir stop jupyt server studio prevent charg",
        "Answer_original_content":"aw iam free servic charg role polici aspect iam document iam featur aw account offer addit charg charg us aw servic user",
        "Answer_preprocessed_content":"aw iam free servic charg role polici aspect iam document iam featur aw account offer addit charg charg us aw servic user",
        "Answer_gpt_summary_original":"Solution: There are no solutions provided as the user's concern about being charged for IAM roles and policies is unfounded. IAM is a free service and users will only be charged for the use of other AWS services.",
        "Answer_gpt_summary":"solut solut provid user concern charg iam role polici unfound iam free servic user charg us aw servic"
    },
    {
        "Question_title":"Azure ML ExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH",
        "Question_body":"<p>I am using LightGBM in Azure ML Jupyter notebooks, it works fine and I also installed graphviz.<\/p>\n<p>However this line:<\/p>\n<pre><code>lgb.plot_tree(clf, tree_index = 1, figsize=(20,12))\n<\/code><\/pre>\n<p>throws this error:<\/p>\n<pre><code>ExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660921902893,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":94.0,
        "Answer_body":"<p>Common problem (very common).  There are two systems named Graphviz, and you need both!\nsee <a href=\"https:\/\/stackoverflow.com\/questions\/73040021\/im-getting-this-issue-when-trying-to-run-the-code-i-found-on-github-pydot-and\/73041302#73041302\">I&#39;m getting this issue when trying to run the code I found on GitHub. Pydot and graphivz are installed but still getting this error<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73418843",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1660922530248,
        "Question_original_content":"executablenotfound fail execut posixpath dot sure graphviz execut system path lightgbm jupyt notebook work fine instal graphviz line lgb plot tree clf tree index figsiz throw error executablenotfound fail execut posixpath dot sure graphviz execut system path",
        "Question_preprocessed_content":"executablenotfound fail execut posixpath sure graphviz execut system path lightgbm jupyt notebook work fine instal graphviz line throw error",
        "Question_gpt_summary_original":"The user encountered an error while using LightGBM in Azure ML Jupyter notebooks. The error message \"ExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\" appeared when trying to plot a tree using the lgb.plot_tree() function. The user had already installed graphviz but was still unable to execute the function.",
        "Question_gpt_summary":"user encount error lightgbm jupyt notebook error messag executablenotfound fail execut posixpath dot sure graphviz execut system path appear try plot tree lgb plot tree function user instal graphviz unabl execut function",
        "Answer_original_content":"common problem common system name graphviz need get issu try run code github pydot graphivz instal get error",
        "Answer_preprocessed_content":"common problem system name graphviz need get issu try run code github pydot graphivz instal get error",
        "Answer_gpt_summary_original":"Solutions provided: The discussion suggests that the error is a common problem and that the user needs to install both systems named Graphviz to resolve the issue.",
        "Answer_gpt_summary":"solut provid discuss suggest error common problem user need instal system name graphviz resolv issu"
    },
    {
        "Question_title":"How to use fitted_model.forecast() for AutoML forecasting model?",
        "Question_body":"<p>Say I have a dataset with a monthly granularity with the following columns:<\/p>\n\n<ul>\n<li>Timestamp<\/li>\n<li>Issues (i.e. number of GitHub issues)<\/li>\n<\/ul>\n\n<p>There is data for each month for all of 2016-2019, so I divide the data accordingly.<\/p>\n\n<ul>\n<li><code>training_data<\/code>: 2016-2017<\/li>\n<li><code>validation_data<\/code>: 2018<\/li>\n<li><code>holdout_data<\/code>: 2019<\/li>\n<\/ul>\n\n<p>If I have a <code>fitted_model<\/code> that is a <code>ForecastingPipelineWrapper<\/code> which is the best run from AutoML where I passed gave it <code>training_data<\/code> and <code>validation_data<\/code>.<\/p>\n\n<p>Looking at the <a href=\"https:\/\/gist.github.com\/swanderz\/c68514d955bebf20f48ffd91aabdc55d\" rel=\"nofollow noreferrer\">ForecastingPipelineWrapper class docstring<\/a> documentation only serves to confuse me more. What is <code>X_past<\/code>, <code>X_future<\/code>, and <code>Y_future<\/code>?<\/p>\n\n<p>How do I use the above dataframes with <code>fitted_model.forecast()<\/code> to manually validate model fit on the <code>holdout_data<\/code> dataframe?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586997104997,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1587136317380,
        "Question_score":2.0,
        "Question_view_count":284.0,
        "Answer_body":"<p>The following notebook illustrates how to leverage y_past, x_past, y_future, x_future, and fitted_model.forecast in the bottom half, 'Forecasting away from training data'. <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-high-frequency\/auto-ml-forecasting-function.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-high-frequency\/auto-ml-forecasting-function.ipynb<\/a><\/p>\n\n<p>The notebook will be a much better guide to grasping these concepts than perhaps a docstring doc. Should you have any more questions or need clarity, let us know!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61240809",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1587062122060,
        "Question_original_content":"us fit model forecast automl forecast model dataset monthli granular follow column timestamp issu number github issu data month divid data accordingli train data valid data holdout data fit model forecastingpipelinewrapp best run automl pass gave train data valid data look forecastingpipelinewrapp class docstr document serv confus past futur futur us datafram fit model forecast manual valid model fit holdout data datafram",
        "Question_preprocessed_content":"us automl forecast model dataset monthli granular follow column timestamp issu data month divid data accordingli best run automl pass gave look forecastingpipelinewrapp class docstr document serv confus us datafram manual valid model fit datafram",
        "Question_gpt_summary_original":"The user is facing challenges in understanding how to use the fitted_model.forecast() function for an AutoML forecasting model. They have a dataset with monthly granularity and are dividing the data into training_data, validation_data, and holdout_data. The confusion arises from the ForecastingPipelineWrapper class docstring documentation, specifically regarding the X_past, X_future, and Y_future dataframes, and how to use them with the fitted_model.forecast() function to validate the model fit on the holdout_data dataframe.",
        "Question_gpt_summary":"user face challeng understand us fit model forecast function automl forecast model dataset monthli granular divid data train data valid data holdout data confus aris forecastingpipelinewrapp class docstr document specif past futur futur datafram us fit model forecast function valid model fit holdout data datafram",
        "Answer_original_content":"follow notebook illustr leverag past past futur futur fit model forecast half forecast awai train data http github com azur machinelearningnotebook blob master us autom machin learn forecast high frequenc auto forecast function ipynb notebook better guid grasp concept docstr doc question need clariti let know",
        "Answer_preprocessed_content":"follow notebook illustr leverag half forecast awai train data notebook better guid grasp concept docstr doc question need clariti let know",
        "Answer_gpt_summary_original":"Solution: The discussion suggests referring to a notebook that illustrates how to use y_past, x_past, y_future, x_future, and fitted_model.forecast in the 'Forecasting away from training data' section. The notebook provides a better guide to understanding these concepts than the docstring documentation.",
        "Answer_gpt_summary":"solut discuss suggest refer notebook illustr us past past futur futur fit model forecast forecast awai train data section notebook provid better guid understand concept docstr document"
    },
    {
        "Question_title":"AWS SageMaker Minimum Configuration",
        "Question_body":"<p>Why do I need Container for AWS SageMaker? If I want to run Scikit Learn on SageMaker's Jupyter notebook for self learning purposes, do I still need to configure Container for it?<\/p>\n\n<p>What is the minimum configuration on SageMaker I will need if I just want to learn Scikit Learn? For example, I want to run Scikit Learn's Decision Tree algorithm with a set of training data and a set of test data. What do I need to do on SageMaker to perform the tasks? Thanks.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1526098834910,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1531211592336,
        "Question_score":9.0,
        "Question_view_count":849.0,
        "Answer_body":"<p>You don't need much. Just an AWS Account with the correlated permissions on your role.\nInside the AWS SageMaker Console you can just run an AWS Notebook Instance with one click. There is Sklearn preinstalled and you can use it out of the box. No special container needed.<\/p>\n\n<p>As minimum you just need your AWS Account with the correlated permissions to create EC2 Instances and read \/ write from your S3. Thats all, just try it. :)<\/p>\n\n<p>Use this as a starting point: <a href=\"https:\/\/aws.amazon.com\/blogs\/aws\/sagemaker\/\" rel=\"noreferrer\">Amazon SageMaker \u2013 Accelerating Machine Learning<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/98gRb.png\" rel=\"noreferrer\">You can also access it via the Jupyter Terminal<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50302810",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1526309522536,
        "Question_original_content":"minimum configur need contain want run scikit learn jupyt notebook self learn purpos need configur contain minimum configur need want learn scikit learn exampl want run scikit learn decis tree algorithm set train data set test data need perform task thank",
        "Question_preprocessed_content":"minimum configur need contain want run scikit learn jupyt notebook self learn purpos need configur contain minimum configur need want learn scikit learn exampl want run scikit learn decis tree algorithm set train data set test data need perform task thank",
        "Question_gpt_summary_original":"The user is seeking information on the minimum configuration required for AWS SageMaker to run Scikit Learn's Decision Tree algorithm with a set of training and test data. They are unsure if they need to configure a container for this purpose.",
        "Question_gpt_summary":"user seek inform minimum configur requir run scikit learn decis tree algorithm set train test data unsur need configur contain purpos",
        "Answer_original_content":"need aw account correl permiss role insid consol run aw notebook instanc click sklearn preinstal us box special contain need minimum need aw account correl permiss creat instanc read write that try us start point acceler machin learn access jupyt termin",
        "Answer_preprocessed_content":"need aw account correl permiss role insid consol run aw notebook instanc click sklearn preinstal us box special contain need minimum need aw account correl permiss creat instanc read write that try us start point acceler machin learn access jupyt termin",
        "Answer_gpt_summary_original":"Solution: The user does not need to configure a container for running Scikit Learn's Decision Tree algorithm on AWS SageMaker. They can simply use an AWS Notebook Instance with Sklearn preinstalled, which can be launched from the AWS SageMaker Console with just one click. The minimum requirement is an AWS Account with the necessary permissions to create EC2 Instances and read\/write from S3. The user can refer to the Amazon SageMaker documentation for more information and can also access it via the Jupyter Terminal.",
        "Answer_gpt_summary":"solut user need configur contain run scikit learn decis tree algorithm simpli us aw notebook instanc sklearn preinstal launch consol click minimum requir aw account necessari permiss creat instanc read write user refer document inform access jupyt termin"
    },
    {
        "Question_title":"Create a Multi Model Endpoint using AWS Sagemaker Boto",
        "Question_body":"<p>I have created 2 models which are not too complex and renamed them and placed them into a same location in S3 bucket.<\/p>\n\n<p>I need to create a multi model endpoint such that the 2 models have a same end point. \nThe model i am using is AWS in built Linear-learner model type regressor. <\/p>\n\n<p>I am stuck as to how they should be deployed. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1574954456480,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":364.0,
        "Answer_body":"<p>SageMaker's Linear Learner algorithm container does not currently implement the requirements for <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">multi-model endpoints<\/a>. You could request support in the <a href=\"https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285&amp;start=0\" rel=\"nofollow noreferrer\">AWS Forums<\/a>.<\/p>\n\n<p>You could also build your own version of the Linear Learner algorithm. To deploy the models to a multi-model endpoint you would need to build your own container that meets the requirements for multi-model endpoints and implement your own version of the Linear Learner algorithm. This sample notebook gives an example of how you would create your multi-model compatible container that serves MxNet models, but you could adapt it to implement a Linear Learner algorithm:<\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1582591338968,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59091944",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1582590253856,
        "Question_original_content":"creat multi model endpoint boto creat model complex renam place locat bucket need creat multi model endpoint model end point model aw built linear learner model type regressor stuck deploi",
        "Question_preprocessed_content":"creat multi model endpoint boto creat model complex renam place locat bucket need creat multi model endpoint model end point model aw built model type regressor stuck deploi",
        "Question_gpt_summary_original":"The user has created two models and stored them in the same location in an S3 bucket. They need to create a multi-model endpoint for the two models using the AWS in-built Linear-learner model type regressor, but they are unsure how to deploy them.",
        "Question_gpt_summary":"user creat model store locat bucket need creat multi model endpoint model aw built linear learner model type regressor unsur deploi",
        "Answer_original_content":"linear learner algorithm contain current implement requir multi model endpoint request support aw forum build version linear learner algorithm deploi model multi model endpoint need build contain meet requir multi model endpoint implement version linear learner algorithm sampl notebook give exampl creat multi model compat contain serv mxnet model adapt implement linear learner algorithm http github com awslab amazon exampl blob master advanc function multi model bring multi model endpoint bring ipynb",
        "Answer_preprocessed_content":"linear learner algorithm contain current implement requir endpoint request support aw forum build version linear learner algorithm deploi model endpoint need build contain meet requir endpoint implement version linear learner algorithm sampl notebook give exampl creat compat contain serv mxnet model adapt implement linear learner algorithm",
        "Answer_gpt_summary_original":"Solutions provided: \n- Request support in the AWS forums to implement multi-model endpoints for the Linear Learner algorithm container.\n- Build a custom container that meets the requirements for multi-model endpoints and implement a version of the Linear Learner algorithm. A sample notebook is provided as an example. \n\nNo personal opinions or biases were included in the summary.",
        "Answer_gpt_summary":"solut provid request support aw forum implement multi model endpoint linear learner algorithm contain build custom contain meet requir multi model endpoint implement version linear learner algorithm sampl notebook provid exampl person opinion bias includ summari"
    },
    {
        "Question_title":"Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden",
        "Question_body":"<p>this might sounds similar to what others have asked by I couldn\u2019t really find a solution that fix my problem.<\/p>\n<p>my <em>~\/.aws\/credentials<\/em> looks like<\/p>\n<pre><code class=\"lang-auto\">[default]\naws_access_key_id = XYZ\naws_secret_access_key = ABC\n\n[testing]\nsource_profile = default\nrole_arn = arn:aws:iam::54:role\/ad\n<\/code><\/pre>\n<p>I add my remote like<\/p>\n<p><code>dvc remote add --local -v myremote s3:\/\/bib-ds-models-testing\/data\/dvc-test<\/code><br>\nand use<\/p>\n<p>I have made my<code> .dvc\/config.local<\/code> to look like<\/p>\n<p>[\u2018remote \u201cmyremote\u201d\u2019]<br>\nurl = s3:\/\/bib-ds-models-testing\/data\/dvc-test<br>\naccess_key_id = XYZ<br>\nsecret_access_key = ABC\/h2hOsRcCIFqwYWV7eZaUq3gNmS<br>\nprofile=\u2018testing\u2019<br>\ncredentialpath = \/Users\/nyt21\/.aws\/credentials<\/p>\n<p>but still after running <code>dvc push -r myremote<\/code> I get<\/p>\n<p><code>ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden<\/code><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1627166936199,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":9715.0,
        "Answer_body":"<p>For the record: replied in <a href=\"https:\/\/stackoverflow.com\/questions\/68517516\/forbidden-an-error-occurred-403-when-calling-the-headobject-operation\" class=\"inline-onebox\">dvc - Forbidden: An error occurred (403) when calling the HeadObject operation: - Stack Overflow<\/a><\/p>. <p>I\u2019m having a similar issue though I\u2019m perfectly available to use the cli:<\/p>\n<p><code>dvc push<\/code> consistently fails while <code>aws s3 cp<\/code> or <code>aws s3 sync<\/code> do work.<\/p>\n<p>I don\u2019t know what to try yet<\/p>\n<p>See:<\/p>\n<pre><code class=\"lang-auto\">2022-04-05 13:02:29,278 DEBUG: Preparing to transfer data from '\/Users\/clementwalter\/Documents\/perso\/co-bots-monorepo\/packages\/co-bots-image-processing\/.dvc\/cache' to 'co-bots\/.dvc'\n2022-04-05 13:02:29,278 DEBUG: Preparing to collect status from 'co-bots\/.dvc'\n2022-04-05 13:02:29,279 DEBUG: Collecting status from 'co-bots\/.dvc'\n2022-04-05 13:02:29,279 DEBUG: Querying 2 hashes via object_exists\n2022-04-05 13:02:30,608 ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/s3fs\/core.py\", line 261, in _call_s3\n    out = await method(**additional_kwargs)\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/aiobotocore\/client.py\", line 187, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/main.py\", line 55, in main\n    ret = cmd.do_run()\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/command\/base.py\", line 45, in do_run\n    return self.run()\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/command\/data_sync.py\", line 57, in run\n    processed_files_count = self.repo.push(\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py\", line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/repo\/push.py\", line 56, in push\n    pushed += self.cloud.push(\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py\", line 85, in push\n    return transfer(\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/objects\/transfer.py\", line 153, in transfer\n    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py\", line 158, in compare_status\n    dest_exists, dest_missing = status(\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py\", line 122, in status\n    exists = hashes.intersection(\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py\", line 48, in _indexed_dir_hashes\n    dir_exists.update(odb.list_hashes_exists(dir_hashes - dir_exists))\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py\", line 421, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 619, in result_iterator\n    yield fs.pop().result()\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 444, in result\n    return self.__get_result()\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 389, in __get_result\n    raise self._exception\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/lib\/python3.8\/concurrent\/futures\/thread.py\", line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py\", line 412, in exists_with_progress\n    ret = self.fs.exists(fs_path)\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/fs\/fsspec_wrapper.py\", line 91, in exists\n    return self.fs.exists(path)\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/fsspec\/asyn.py\", line 91, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/fsspec\/asyn.py\", line 71, in sync\n    raise return_result\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/fsspec\/asyn.py\", line 25, in _runner\n    result[0] = await coro\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/s3fs\/core.py\", line 835, in _exists\n    await self._info(path, bucket, key, version_id=version_id)\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/s3fs\/core.py\", line 1029, in _info\n    out = await self._call_s3(\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/s3fs\/core.py\", line 281, in _call_s3\n    raise err\nPermissionError: Forbidden\n------------------------------------------------------------\n2022-04-05 13:02:30,671 DEBUG: Adding '\/Users\/clementwalter\/Documents\/perso\/co-bots-monorepo\/packages\/co-bots-image-processing\/.dvc\/config.local' to gitignore file.\n2022-04-05 13:02:30,674 DEBUG: Adding '\/Users\/clementwalter\/Documents\/perso\/co-bots-monorepo\/packages\/co-bots-image-processing\/.dvc\/tmp' to gitignore file.\n2022-04-05 13:02:30,674 DEBUG: Adding '\/Users\/clementwalter\/Documents\/perso\/co-bots-monorepo\/packages\/co-bots-image-processing\/.dvc\/cache' to gitignore file.\n2022-04-05 13:02:30,676 DEBUG: Removing '\/Users\/clementwalter\/Documents\/perso\/co-bots-monorepo\/packages\/.ALtVEwomf9fsFX2UmKbut9.tmp'\n2022-04-05 13:02:30,677 DEBUG: Removing '\/Users\/clementwalter\/Documents\/perso\/co-bots-monorepo\/packages\/.ALtVEwomf9fsFX2UmKbut9.tmp'\n2022-04-05 13:02:30,678 DEBUG: Removing '\/Users\/clementwalter\/Documents\/perso\/co-bots-monorepo\/packages\/.ALtVEwomf9fsFX2UmKbut9.tmp'\n2022-04-05 13:02:30,678 DEBUG: Removing '\/Users\/clementwalter\/Documents\/perso\/co-bots-monorepo\/packages\/co-bots-image-processing\/.dvc\/cache\/.TZa9m7D429SR85Soc3euje.tmp'\n2022-04-05 13:02:30,681 DEBUG: Version info for developers:\nDVC version: 2.9.3 (pip)\n---------------------------------\nPlatform: Python 3.8.11 on macOS-11.6.2-x86_64-i386-64bit\nSupports:\n\twebhdfs (fsspec = 2022.1.0),\n\thttp (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),\n\thttps (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),\n\ts3 (s3fs = 2022.1.0, boto3 = 1.20.24)\nCache types: reflink, hardlink, symlink\nCache directory: apfs on \/dev\/disk1s1s1\nCaches: local\nRemotes: s3\nWorkspace directory: apfs on \/dev\/disk1s1s1\nRepo: dvc (subdir), git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code class=\"lang-auto\"> (co-bots) packages\/co-bots-image-processing \ue0b0 \ue0a0 main \u00b1 \ue0b0 aws s3 sync .dvc\/cache s3:\/\/co-bots\nupload: .dvc\/cache\/00\/f6b1d1f06b8754d64c452f4ef58f16 to s3:\/\/co-bots\/00\/f6b1d1f06b8754d64c452f4ef58f16\nupload: .dvc\/cache\/00\/a7bfb27455742c9479d25779e4214a to s3:\/\/co-bots\/00\/a7bfb27455742c9479d25779e4214a\nupload: .dvc\/cache\/00\/377bfc7e3cf1a189380a3e9f52d7c6 to s3:\/\/co-bots\/00\/377bfc7e3cf1a189380a3e9f52d7c6\nupload: .dvc\/cache\/04\/e800606cdafbea256f9ff537bc289a to s3:\/\/co-bots\/04\/e800606cdafbea256f9ff537bc289a\nupload: .dvc\/cache\/04\/84a80227ef30c3ce9f506d8cce8150 to s3:\/\/co-bots\/04\/84a80227ef30c3ce9f506d8cce8150\nupload: .dvc\/cache\/02\/22acd5f40b2cc2073a3e2e9a42828a to s3:\/\/co-bots\/02\/22acd5f40b2cc2073a3e2e9a42828a\nupload: .dvc\/cache\/00\/2bbbc2d424b03e445ad9188d5f771c to s3:\/\/co-bots\/00\/2bbbc2d424b03e445ad9188d5f771c\nupload: .dvc\/cache\/01\/112074beaadff8b4a6ff1fd27a5dd9 to s3:\/\/co-bots\/01\/112074beaadff8b4a6ff1fd27a5dd9\nupload: .dvc\/cache\/04\/2ad2b365565fb1c70773cd51a7f931.dir to s3:\/\/co-bots\/04\/2ad2b365565fb1c70773cd51a7f931.dir\nupload: .dvc\/cache\/06\/bdb87ab7ecadfdae2e9d52fa7f8a28 to s3:\/\/co-bots\/06\/bdb87ab7ecadfdae2e9d52fa7f8a28\nupload: .dvc\/cache\/06\/5350251c0f1545eb0b48ca698e89f2 to s3:\/\/co-bots\/06\/5350251c0f1545eb0b48ca698e89f2\nupload: .dvc\/cache\/06\/f402f0fe899292715a8dc7d393914a to s3:\/\/co-bots\/06\/f402f0fe899292715a8dc7d393914a\n...\n<\/code><\/pre>\n<p>My dvc config is as follows<\/p>\n<pre><code class=\"lang-auto\">[core]\n    remote = s3\n['remote \"s3\"']\n    url = s3:\/\/co-bots\/\n<\/code><\/pre>. <p>Hi <a class=\"mention\" href=\"\/u\/clementwalter\">@clementwalter<\/a> , could you fill a bug report in DVC repo (<a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/new?assignees=&amp;labels=&amp;template=bug_report.md\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Sign in to GitHub \u00b7 GitHub<\/a>)<\/p>. <p>Hey, sorry I solved myself already, I had to specify the bucket region (which is not the usual default one, I am using eu-west-3 in my case) in the dvc config file. It seams like there is a different behavior when it is not provided between dvc and the aws cli.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/forbidden-an-error-occurred-403-when-calling-the-headobject-operation-forbidden\/828",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-07-25T18:11:28.496Z",
                "Answer_body":"<p>For the record: replied in <a href=\"https:\/\/stackoverflow.com\/questions\/68517516\/forbidden-an-error-occurred-403-when-calling-the-headobject-operation\" class=\"inline-onebox\">dvc - Forbidden: An error occurred (403) when calling the HeadObject operation: - Stack Overflow<\/a><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-05T11:01:33.089Z",
                "Answer_body":"<p>I\u2019m having a similar issue though I\u2019m perfectly available to use the cli:<\/p>\n<p><code>dvc push<\/code> consistently fails while <code>aws s3 cp<\/code> or <code>aws s3 sync<\/code> do work.<\/p>\n<p>I don\u2019t know what to try yet<\/p>\n<p>See:<\/p>\n<pre><code class=\"lang-auto\">2022-04-05 13:02:29,278 DEBUG: Preparing to transfer data from '\/Users\/clementwalter\/Documents\/perso\/co-bots-monorepo\/packages\/co-bots-image-processing\/.dvc\/cache' to 'co-bots\/.dvc'\n2022-04-05 13:02:29,278 DEBUG: Preparing to collect status from 'co-bots\/.dvc'\n2022-04-05 13:02:29,279 DEBUG: Collecting status from 'co-bots\/.dvc'\n2022-04-05 13:02:29,279 DEBUG: Querying 2 hashes via object_exists\n2022-04-05 13:02:30,608 ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/s3fs\/core.py\", line 261, in _call_s3\n    out = await method(**additional_kwargs)\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/aiobotocore\/client.py\", line 187, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/main.py\", line 55, in main\n    ret = cmd.do_run()\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/command\/base.py\", line 45, in do_run\n    return self.run()\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/command\/data_sync.py\", line 57, in run\n    processed_files_count = self.repo.push(\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py\", line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/repo\/push.py\", line 56, in push\n    pushed += self.cloud.push(\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py\", line 85, in push\n    return transfer(\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/objects\/transfer.py\", line 153, in transfer\n    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py\", line 158, in compare_status\n    dest_exists, dest_missing = status(\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py\", line 122, in status\n    exists = hashes.intersection(\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py\", line 48, in _indexed_dir_hashes\n    dir_exists.update(odb.list_hashes_exists(dir_hashes - dir_exists))\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py\", line 421, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 619, in result_iterator\n    yield fs.pop().result()\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 444, in result\n    return self.__get_result()\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 389, in __get_result\n    raise self._exception\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/lib\/python3.8\/concurrent\/futures\/thread.py\", line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py\", line 412, in exists_with_progress\n    ret = self.fs.exists(fs_path)\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/dvc\/fs\/fsspec_wrapper.py\", line 91, in exists\n    return self.fs.exists(path)\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/fsspec\/asyn.py\", line 91, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/fsspec\/asyn.py\", line 71, in sync\n    raise return_result\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/fsspec\/asyn.py\", line 25, in _runner\n    result[0] = await coro\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/s3fs\/core.py\", line 835, in _exists\n    await self._info(path, bucket, key, version_id=version_id)\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/s3fs\/core.py\", line 1029, in _info\n    out = await self._call_s3(\n  File \"\/Users\/clementwalter\/.pyenv\/versions\/3.8.11\/envs\/co-bots\/lib\/python3.8\/site-packages\/s3fs\/core.py\", line 281, in _call_s3\n    raise err\nPermissionError: Forbidden\n------------------------------------------------------------\n2022-04-05 13:02:30,671 DEBUG: Adding '\/Users\/clementwalter\/Documents\/perso\/co-bots-monorepo\/packages\/co-bots-image-processing\/.dvc\/config.local' to gitignore file.\n2022-04-05 13:02:30,674 DEBUG: Adding '\/Users\/clementwalter\/Documents\/perso\/co-bots-monorepo\/packages\/co-bots-image-processing\/.dvc\/tmp' to gitignore file.\n2022-04-05 13:02:30,674 DEBUG: Adding '\/Users\/clementwalter\/Documents\/perso\/co-bots-monorepo\/packages\/co-bots-image-processing\/.dvc\/cache' to gitignore file.\n2022-04-05 13:02:30,676 DEBUG: Removing '\/Users\/clementwalter\/Documents\/perso\/co-bots-monorepo\/packages\/.ALtVEwomf9fsFX2UmKbut9.tmp'\n2022-04-05 13:02:30,677 DEBUG: Removing '\/Users\/clementwalter\/Documents\/perso\/co-bots-monorepo\/packages\/.ALtVEwomf9fsFX2UmKbut9.tmp'\n2022-04-05 13:02:30,678 DEBUG: Removing '\/Users\/clementwalter\/Documents\/perso\/co-bots-monorepo\/packages\/.ALtVEwomf9fsFX2UmKbut9.tmp'\n2022-04-05 13:02:30,678 DEBUG: Removing '\/Users\/clementwalter\/Documents\/perso\/co-bots-monorepo\/packages\/co-bots-image-processing\/.dvc\/cache\/.TZa9m7D429SR85Soc3euje.tmp'\n2022-04-05 13:02:30,681 DEBUG: Version info for developers:\nDVC version: 2.9.3 (pip)\n---------------------------------\nPlatform: Python 3.8.11 on macOS-11.6.2-x86_64-i386-64bit\nSupports:\n\twebhdfs (fsspec = 2022.1.0),\n\thttp (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),\n\thttps (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),\n\ts3 (s3fs = 2022.1.0, boto3 = 1.20.24)\nCache types: reflink, hardlink, symlink\nCache directory: apfs on \/dev\/disk1s1s1\nCaches: local\nRemotes: s3\nWorkspace directory: apfs on \/dev\/disk1s1s1\nRepo: dvc (subdir), git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code class=\"lang-auto\"> (co-bots) packages\/co-bots-image-processing \ue0b0 \ue0a0 main \u00b1 \ue0b0 aws s3 sync .dvc\/cache s3:\/\/co-bots\nupload: .dvc\/cache\/00\/f6b1d1f06b8754d64c452f4ef58f16 to s3:\/\/co-bots\/00\/f6b1d1f06b8754d64c452f4ef58f16\nupload: .dvc\/cache\/00\/a7bfb27455742c9479d25779e4214a to s3:\/\/co-bots\/00\/a7bfb27455742c9479d25779e4214a\nupload: .dvc\/cache\/00\/377bfc7e3cf1a189380a3e9f52d7c6 to s3:\/\/co-bots\/00\/377bfc7e3cf1a189380a3e9f52d7c6\nupload: .dvc\/cache\/04\/e800606cdafbea256f9ff537bc289a to s3:\/\/co-bots\/04\/e800606cdafbea256f9ff537bc289a\nupload: .dvc\/cache\/04\/84a80227ef30c3ce9f506d8cce8150 to s3:\/\/co-bots\/04\/84a80227ef30c3ce9f506d8cce8150\nupload: .dvc\/cache\/02\/22acd5f40b2cc2073a3e2e9a42828a to s3:\/\/co-bots\/02\/22acd5f40b2cc2073a3e2e9a42828a\nupload: .dvc\/cache\/00\/2bbbc2d424b03e445ad9188d5f771c to s3:\/\/co-bots\/00\/2bbbc2d424b03e445ad9188d5f771c\nupload: .dvc\/cache\/01\/112074beaadff8b4a6ff1fd27a5dd9 to s3:\/\/co-bots\/01\/112074beaadff8b4a6ff1fd27a5dd9\nupload: .dvc\/cache\/04\/2ad2b365565fb1c70773cd51a7f931.dir to s3:\/\/co-bots\/04\/2ad2b365565fb1c70773cd51a7f931.dir\nupload: .dvc\/cache\/06\/bdb87ab7ecadfdae2e9d52fa7f8a28 to s3:\/\/co-bots\/06\/bdb87ab7ecadfdae2e9d52fa7f8a28\nupload: .dvc\/cache\/06\/5350251c0f1545eb0b48ca698e89f2 to s3:\/\/co-bots\/06\/5350251c0f1545eb0b48ca698e89f2\nupload: .dvc\/cache\/06\/f402f0fe899292715a8dc7d393914a to s3:\/\/co-bots\/06\/f402f0fe899292715a8dc7d393914a\n...\n<\/code><\/pre>\n<p>My dvc config is as follows<\/p>\n<pre><code class=\"lang-auto\">[core]\n    remote = s3\n['remote \"s3\"']\n    url = s3:\/\/co-bots\/\n<\/code><\/pre>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-08T10:41:22.439Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/clementwalter\">@clementwalter<\/a> , could you fill a bug report in DVC repo (<a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/new?assignees=&amp;labels=&amp;template=bug_report.md\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Sign in to GitHub \u00b7 GitHub<\/a>)<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-08T12:23:59.957Z",
                "Answer_body":"<p>Hey, sorry I solved myself already, I had to specify the bucket region (which is not the usual default one, I am using eu-west-3 in my case) in the dvc config file. It seams like there is a different behavior when it is not provided between dvc and the aws cli.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"forbidden error occur call headobject oper forbidden sound similar ask solut fix problem aw credenti look like default aw access kei xyz aw secret access kei abc test sourc profil default role arn arn aw iam role add remot like remot add local myremot bib model test data test us config local look like remot myremot url bib model test data test access kei xyz secret access kei abc hhosrccifqwywvezauqgnm profil test credentialpath user nyt aw credenti run push myremot error unexpect error forbidden error occur call headobject oper forbidden",
        "Question_preprocessed_content":"forbidden error occur call headobject oper forbidden sound similar ask solut fix problem look like add remot like us look like remot myremot url xyz profil test credentialpath run",
        "Question_gpt_summary_original":"The user is encountering an error (403) when calling the HeadObject operation while trying to push data to an S3 bucket using DVC. The user has tried to add their remote and configure their credentials, but the error persists. The user is seeking a solution to fix the problem.",
        "Question_gpt_summary":"user encount error call headobject oper try push data bucket user tri add remot configur credenti error persist user seek solut fix problem",
        "Answer_original_content":"record repli forbidden error occur call headobject oper stack overflow have similar issu perfectli avail us cli push consist fail aw aw sync work dont know try debug prepar transfer data user clementwalt document perso bot monorepo packag bot imag process cach bot debug prepar collect statu bot debug collect statu bot debug queri hash object exist error unexpect error forbidden error occur call headobject oper forbidden traceback recent file user clementwalt pyenv version env bot lib python site packag sf core line await method addit kwarg file user clementwalt pyenv version env bot lib python site packag aiobotocor client line api rais error class pars respons oper botocor except clienterror error occur call headobject oper forbidden except direct caus follow except traceback recent file user clementwalt pyenv version env bot lib python site packag main line main ret cmd run file user clementwalt pyenv version env bot lib python site packag command base line run return self run file user clementwalt pyenv version env bot lib python site packag command data sync line run process file count self repo push file user clementwalt pyenv version env bot lib python site packag repo init line wrapper return repo arg kwarg file user clementwalt pyenv version env bot lib python site packag repo push line push push self cloud push file user clementwalt pyenv version env bot lib python site packag data cloud line push return transfer file user clementwalt pyenv version env bot lib python site packag object transfer line transfer statu compar statu src dest obj id check delet fals kwarg file user clementwalt pyenv version env bot lib python site packag object statu line compar statu dest exist dest miss statu file user clementwalt pyenv version env bot lib python site packag object statu line statu exist hash intersect file user clementwalt pyenv version env bot lib python site packag object statu line index dir hash dir exist updat odb list hash exist dir hash dir exist file user clementwalt pyenv version env bot lib python site packag object base line list hash exist ret list itertool compress hash remot file user clementwalt pyenv version lib python concurr futur base line result iter yield pop result file user clementwalt pyenv version lib python concurr futur base line result return self result file user clementwalt pyenv version lib python concurr futur base line result rais self except file user clementwalt pyenv version lib python concurr futur thread line run result self self arg self kwarg file user clementwalt pyenv version env bot lib python site packag object base line exist progress ret self exist path file user clementwalt pyenv version env bot lib python site packag fsspec wrapper line exist return self exist path file user clementwalt pyenv version env bot lib python site packag fsspec asyn line wrapper return sync self loop func arg kwarg file user clementwalt pyenv version env bot lib python site packag fsspec asyn line sync rais return result file user clementwalt pyenv version env bot lib python site packag fsspec asyn line runner result await coro file user clementwalt pyenv version env bot lib python site packag sf core line exist await self info path bucket kei version version file user clementwalt pyenv version env bot lib python site packag sf core line info await self file user clementwalt pyenv version env bot lib python site packag sf core line rais err permissionerror forbidden debug ad user clementwalt document perso bot monorepo packag bot imag process config local gitignor file debug ad user clementwalt document perso bot monorepo packag bot imag process tmp gitignor file debug ad user clementwalt document perso bot monorepo packag bot imag process cach gitignor file debug remov user clementwalt document perso bot monorepo packag altvewomffsfxumkbut tmp debug remov user clementwalt document perso bot monorepo packag altvewomffsfxumkbut tmp debug remov user clementwalt document perso bot monorepo packag altvewomffsfxumkbut tmp debug remov user clementwalt document perso bot monorepo packag bot imag process cach tzamdsrsoceuj tmp debug version info develop version pip platform python maco bit support webhdf fsspec http aiohttp aiohttp retri http aiohttp aiohttp retri sf boto cach type reflink hardlink symlink cach directori apf dev diskss cach local remot workspac directori apf dev diskss repo subdir git have troubl hit http org support happi help bot packag bot imag process main aw sync cach bot upload cach fbdfbdcfeff bot fbdfbdcfeff upload cach abfbcdea bot abfbcdea upload cach bfcecfaaefdc bot bfcecfaaefdc upload cach ecdafbeafffbca bot ecdafbeafffbca upload cach aefccefdcc bot aefccefdcc upload cach acdfbccaeeaa bot acdfbccaeeaa upload cach cdbeaddfc bot cdbeaddfc upload cach beaadffbafffdadd bot beaadffbafffdadd upload cach adbfbccdaf dir bot adbfbccdaf dir upload cach bdbabecadfdaeedfafa bot bdbabecadfdaeedfafa upload cach cfebbcaef bot cfebbcaef upload cach fffeadcda bot fffeadcda config follow core remot remot url bot clementwalt bug report repo sign github github hei sorri solv specifi bucket region usual default west case config file seam like differ behavior provid aw cli",
        "Answer_preprocessed_content":"record repli forbidden error occur call headobject oper stack overflow have similar issu perfectli avail us cli consist fail work dont know try config follow bug report repo hei sorri solv specifi bucket region config file seam like differ behavior provid aw cli",
        "Answer_gpt_summary_original":"Solution:\nThe user found a solution to their problem. They had to specify the bucket region in the DVC config file, which was not the usual default one.",
        "Answer_gpt_summary":"solut user solut problem specifi bucket region config file usual default"
    },
    {
        "Question_title":"Does AWS Sagemaker charges you per API request?",
        "Question_body":"<p>AWS pricing page describes how much it costs per hour to run AWS Sagemaker for online realtime inference.\n<a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/<\/a><\/p>\n<p>But AWS usually also charges for API requests.\nDo they charge extra per every API inference request to the Sagemaker model?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1604509294833,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":187.0,
        "Answer_body":"<p>I am on the AWS SageMaker team.  For &quot;Real-Time Inference&quot; you are only charged for:<\/p>\n<ol>\n<li>usage of the instance types you choose (instance hours)<\/li>\n<li>storage attached to those instance (GB storage hours)<\/li>\n<li>data in and out of your Endpoint (Bytes in\/out)<\/li>\n<\/ol>\n<p>See &quot;Pricing Example #6: Real-Time Inference&quot; as well.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64684503",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1608159311612,
        "Question_original_content":"charg api request aw price page describ cost hour run onlin realtim infer http aw amazon com price aw usual charg api request charg extra api infer request model",
        "Question_preprocessed_content":"charg api request aw price page describ cost hour run onlin realtim infer aw usual charg api request charg extra api infer request model",
        "Question_gpt_summary_original":"The user is seeking clarification on whether AWS charges extra for every API inference request made to the Sagemaker model, in addition to the hourly cost of running the service.",
        "Question_gpt_summary":"user seek clarif aw charg extra api infer request model addit hourli cost run servic",
        "Answer_original_content":"team real time infer charg usag instanc type choos instanc hour storag attach instanc storag hour data endpoint byte price exampl real time infer",
        "Answer_preprocessed_content":"team infer charg usag instanc type choos storag attach instanc data endpoint price exampl infer",
        "Answer_gpt_summary_original":"Solution: The AWS SageMaker team member clarified that for \"Real-Time Inference,\" users are only charged for the usage of the instance types chosen, storage attached to those instances, and data in and out of the Endpoint. There are no additional charges for every API inference request made to the Sagemaker model.",
        "Answer_gpt_summary":"solut team member clarifi real time infer user charg usag instanc type chosen storag attach instanc data endpoint addit charg api infer request model"
    },
    {
        "Question_title":"How can i access azure ml pipeline parameters from a python script running in designer?",
        "Question_body":"I would like to perform some data transformations using the Python script module in Designer for which i would need to access some pipeline parameters. How can i get those values?\n\nWhat would be the equivalent for an R script?",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1620522230407,
        "Question_favorite_count":7.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@javier-8889 Thanks, Currently passing a pipeline parameter to the script of Execute Python\/R Module is not supported. We have a new feature custom module which is in private preview. you can write your own module and use in Designer. If it's a common case, it might be better to use custom module.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/387875\/how-can-i-access-azure-ml-pipeline-parameters-from.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-05-18T04:06:56.287Z",
                "Answer_score":0,
                "Answer_body":"@javier-8889 Thanks, Currently passing a pipeline parameter to the script of Execute Python\/R Module is not supported. We have a new feature custom module which is in private preview. you can write your own module and use in Designer. If it's a common case, it might be better to use custom module.",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1621310816287,
        "Question_original_content":"access pipelin paramet python script run design like perform data transform python script modul design need access pipelin paramet valu equival script",
        "Question_preprocessed_content":"access pipelin paramet python script run design like perform data transform python script modul design need access pipelin paramet valu equival script",
        "Question_gpt_summary_original":"The user is facing challenges in accessing pipeline parameters from a Python script module in Azure ML Designer for performing data transformations. They are seeking guidance on how to obtain these values and are also curious about the equivalent process for an R script.",
        "Question_gpt_summary":"user face challeng access pipelin paramet python script modul design perform data transform seek guidanc obtain valu curiou equival process script",
        "Answer_original_content":"javier thank current pass pipelin paramet script execut python modul support new featur custom modul privat preview write modul us design common case better us custom modul",
        "Answer_preprocessed_content":"thank current pass pipelin paramet script execut modul support new featur custom modul privat preview write modul us design common case better us custom modul",
        "Answer_gpt_summary_original":"Solution: The user can write their own custom module in Azure ML Designer to access pipeline parameters from a Python script module. This feature is currently in private preview. However, passing a pipeline parameter to the script of Execute Python\/R Module is not supported. No solution was provided for accessing pipeline parameters from an R script.",
        "Answer_gpt_summary":"solut user write custom modul design access pipelin paramet python script modul featur current privat preview pass pipelin paramet script execut python modul support solut provid access pipelin paramet script"
    },
    {
        "Question_title":"Azure machine learning predict order for customer",
        "Question_body":"<p>I have created a new experiment in Azure Machine Learning and added two datasets by manually uploading csv's.<\/p>\n\n<ul>\n<li>One is from a customer of which I'd like to predict which products he will order next. <\/li>\n<li>The second dataset has the same type of data, only then from all other customers as reference for learning.<\/li>\n<\/ul>\n\n<p>I have <code>productid<\/code>, <code>amount<\/code>, and <code>orderdate<\/code> and <code>orderid<\/code> for grouping and putting it on a timeframe.\nThe customer (dataset one) is always several months behind with ordering the latest products. therefor I added the dataset two with all other customers as reference.<\/p>\n\n<p>Also because the reference can tell which products are more popular (ordered more and by several customers) so perhaps I should add a customerid column to the dataset.<\/p>\n\n<p>I know how to start and get the data in, and I do know that it is common to split the data for training, feed it to the train model with a <code>Ilearnerdotnet<\/code> type and give the output to the score model and evaluate the model.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/qH7lb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qH7lb.png\" alt=\"current experiment\"><\/a>\nI do not know how to choose a classification type and how this can give an output for the next three months of order. I have read some tutorials, but I just need someone who can give me some pointers.<\/p>\n\n<p><strong>edit<\/strong> I have added the customerid to the dataset so that I have just one set now which I should split to focus on a specific customer.\n<strong>edit2<\/strong> found these templates. will look into it <a href=\"https:\/\/stackoverflow.com\/a\/36552849\/169714\">https:\/\/stackoverflow.com\/a\/36552849\/169714<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1463047086927,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1495541203567,
        "Question_score":2.0,
        "Question_view_count":845.0,
        "Answer_body":"<p>Go over this <a href=\"http:\/\/download.microsoft.com\/download\/0\/5\/A\/05AE6B94-E688-403E-90A5-6035DBE9EEC5\/machine-learning-basics-infographic-with-algorithm-examples.pdf\" rel=\"nofollow\">http:\/\/download.microsoft.com\/download\/0\/5\/A\/05AE6B94-E688-403E-90A5-6035DBE9EEC5\/machine-learning-basics-infographic-with-algorithm-examples.pdf<\/a><\/p>\n\n<p>If above infographic doesn't help, then you can try all of the learners by going over this experiment and use the one with best results - <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Algo-Evaluater-Compare-Performance-of-Multiple-Algos-against-Your-Data-1\" rel=\"nofollow\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Algo-Evaluater-Compare-Performance-of-Multiple-Algos-against-Your-Data-1<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37183494",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1463090186116,
        "Question_original_content":"predict order custom creat new experi ad dataset manual upload csv custom like predict product order second dataset type data custom refer learn productid orderd orderid group put timefram custom dataset month order latest product therefor ad dataset custom refer refer tell product popular order custom add customerid column dataset know start data know common split data train feed train model ilearnerdotnet type output score model evalu model know choos classif type output month order read tutori need pointer edit ad customerid dataset set split focu specif custom edit templat look http stackoverflow com",
        "Question_preprocessed_content":"predict order custom creat new experi ad dataset manual upload csv custom like predict product order second dataset type data custom refer learn group put timefram custom month order latest product therefor ad dataset custom refer refer tell product popular add customerid column dataset know start data know common split data train feed train model type output score model evalu model know choos classif type output month order read tutori need pointer edit ad customerid dataset set split focu specif custom edit templat look",
        "Question_gpt_summary_original":"The user is facing challenges in predicting which products a customer will order next using Azure Machine Learning. The customer dataset is several months behind in ordering the latest products, so the user added a second dataset with all other customers as reference. The user is unsure of how to choose a classification type and how to get an output for the next three months of orders. The user has added a customerid column to the dataset and is considering using templates found on Stack Overflow.",
        "Question_gpt_summary":"user face challeng predict product custom order custom dataset month order latest product user ad second dataset custom refer user unsur choos classif type output month order user ad customerid column dataset consid templat stack overflow",
        "Answer_original_content":"http download microsoft com download aeb dbeeec machin learn basic infograph algorithm exampl pdf infograph help try learner go experi us best result http galleri cortanaintellig com experi algo evaluat compar perform multipl algo data",
        "Answer_preprocessed_content":"infograph help try learner go experi us best result",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion include using the machine learning algorithm examples provided in the Microsoft infographic or trying all of the learners in an experiment and using the one with the best results. The user is also considering using templates found on Stack Overflow.",
        "Answer_gpt_summary":"possibl solut mention discuss includ machin learn algorithm exampl provid microsoft infograph try learner experi best result user consid templat stack overflow"
    },
    {
        "Question_title":"Returning nearest neighbors from SKLearn model deployed in AWS SageMaker",
        "Question_body":"<p>I built an unsupervised NearestNeighbors model in AWS Sagemaker, and deployed this to an endpoint. Now, I am trying to use the model endpoint to generate the k-nearest neighbors for a given input vector. <\/p>\n\n<p>However, I am getting the following error:<\/p>\n\n<pre><code>AttributeError                            Traceback (most recent call last)\n&lt;ipython-input-31-f595a603f928&gt; in &lt;module&gt;()\n     12 # print(predictor.predict(sample_vector))\n     13 \n---&gt; 14 distance, indice = pred.kneighbors(sample_vector, n_neighbors=11)\n\nAttributeError: 'SKLearnPredictor' object has no attribute 'kneighbors'\n<\/code><\/pre>\n\n<p>The SKLearn NearestNeighbors learner does not have a predict method. Trying to use the 'predict' method instead of '.kneighbors' therefore also yields an error:<\/p>\n\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"&lt;!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\"&gt;\n&lt;title&gt;500 Internal Server Error&lt;\/title&gt;\n&lt;h1&gt;Internal Server Error&lt;\/h1&gt;\n&lt;p&gt;The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.&lt;\/p&gt;\n\". See https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-scikit-learn-2019-06-29-13-11-50-512 in account 820407560908 for more information.\n<\/code><\/pre>\n\n<p>Is there a way to call this endpoint within Sagemaker, or does the Sagemaker SKLearn SDK only allow for models with a 'predict' method?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1561818096170,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":694.0,
        "Answer_body":"<p>At inference, 3 functions are used one after the other: <code>input_fn<\/code>, <code>predict_fn<\/code>, <code>output_fn<\/code>. They take default values, but you can override them to do desired custom actions. In your case, you can for example override the <code>predict_fn<\/code> to run the desired command. See more details here <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html#deploying-scikit-learn-models\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html#deploying-scikit-learn-models<\/a> <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56818280",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1561838537720,
        "Question_original_content":"return nearest neighbor sklearn model deploi built unsupervis nearestneighbor model deploi endpoint try us model endpoint gener nearest neighbor given input vector get follow error attributeerror traceback recent print predictor predict sampl vector distanc indic pred kneighbor sampl vector neighbor attributeerror sklearnpredictor object attribut kneighbor sklearn nearestneighbor learner predict method try us predict method instead kneighbor yield error modelerror error occur modelerror call invokeendpoint oper receiv server error model messag intern server error intern server error server encount intern error unabl complet request server overload error applic http east consol aw amazon com cloudwatch home region east logeventview group aw endpoint scikit learn account inform wai endpoint sklearn sdk allow model predict method",
        "Question_preprocessed_content":"return nearest neighbor sklearn model deploi built unsupervis nearestneighbor model deploi endpoint try us model endpoint gener neighbor given input vector get follow error sklearn nearestneighbor learner predict method try us predict method instead yield error wai endpoint sklearn sdk allow model predict method",
        "Question_gpt_summary_original":"The user built an unsupervised NearestNeighbors model in AWS Sagemaker and deployed it to an endpoint. However, when trying to generate the k-nearest neighbors for a given input vector, the user encountered an error indicating that the SKLearn NearestNeighbors learner does not have a predict method. Attempting to use the predict method instead of '.kneighbors' also resulted in an error. The user is unsure if there is a way to call this endpoint within Sagemaker or if the Sagemaker SKLearn SDK only allows for models with a 'predict' method.",
        "Question_gpt_summary":"user built unsupervis nearestneighbor model deploi endpoint try gener nearest neighbor given input vector user encount error indic sklearn nearestneighbor learner predict method attempt us predict method instead kneighbor result error user unsur wai endpoint sklearn sdk allow model predict method",
        "Answer_original_content":"infer function input predict output default valu overrid desir custom action case exampl overrid predict run desir command detail http readthedoc stabl sklearn html deploi scikit learn model",
        "Answer_preprocessed_content":"infer function default valu overrid desir custom action case exampl overrid run desir command detail",
        "Answer_gpt_summary_original":"Solution: The user can override the predict_fn function to run the desired command. More details can be found in the Sagemaker documentation.",
        "Answer_gpt_summary":"solut user overrid predict function run desir command detail document"
    },
    {
        "Question_title":"Customer Error: imread read blank (None) image for file- Sagemaker AWS",
        "Question_body":"<p>I am following this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_mscoco_multi_label\/Image-classification-multilabel-lst.ipynb\" rel=\"nofollow noreferrer\">tutorial<\/a> with my custom data and my custom S3 buckets where train and validation data are. I am getting the following error:<\/p>\n<pre><code>Customer Error: imread read blank (None) image for file: \/opt\/ml\/input\/data\/train\/s3:\/\/image-classification\/image_classification_model_data\/train\/img-001.png\n<\/code><\/pre>\n<p>I have all my training data are in one folder named '<code>train<\/code>' I have set up my <code>lst<\/code> file like this suggested by <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">doc<\/a>,<\/p>\n<pre><code>22  1   s3:\/\/image-classification\/image_classification_model_data\/train\/img-001.png\n86  0   s3:\/\/image-classification\/image_classification_model_data\/train\/img-002.png\n...\n<\/code><\/pre>\n<p>My other configurations:<\/p>\n<pre><code>s3_bucket = 'image-classification'\nprefix =  'image_classification_model_data'\n\n\ns3train = 's3:\/\/{}\/{}\/train\/'.format(s3_bucket, prefix)\ns3validation = 's3:\/\/{}\/{}\/validation\/'.format(s3_bucket, prefix)\n\ns3train_lst = 's3:\/\/{}\/{}\/train_lst\/'.format(s3_bucket, prefix)\ns3validation_lst = 's3:\/\/{}\/{}\/validation_lst\/'.format(s3_bucket, prefix)\n\n\n\ntrain_data = sagemaker.inputs.TrainingInput(s3train, distribution='FullyReplicated', \n                        content_type='application\/x-image', s3_data_type='S3Prefix')\n\nvalidation_data = sagemaker.inputs.TrainingInput(s3validation, distribution='FullyReplicated', \n                             content_type='application\/x-image', s3_data_type='S3Prefix')\n\ntrain_data_lst = sagemaker.inputs.TrainingInput(s3train_lst, distribution='FullyReplicated', \n                        content_type='application\/x-image', s3_data_type='S3Prefix')\n\nvalidation_data_lst = sagemaker.inputs.TrainingInput(s3validation_lst, distribution='FullyReplicated', \n                             content_type='application\/x-image', s3_data_type='S3Prefix')\n\n\ndata_channels = {'train': train_data, 'validation': validation_data, 'train_lst': train_data_lst, \n                 'validation_lst': validation_data_lst}\n<\/code><\/pre>\n<p>I checked the images downloaded and checked physically, I see the image. Now sure what this error gets thrown out as <code>blank<\/code>. Any suggestion would be great.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616651745117,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1616652513867,
        "Question_score":0.0,
        "Question_view_count":164.0,
        "Answer_body":"<p>Sagemaker copies the input data you specify in <code>s3train<\/code> into the instance in <code>\/opt\/ml\/input\/data\/train\/<\/code> and that's why you have an error, because as you can see from the error message is trying to concatenate the filename in the <code>lst<\/code> file with the path where it expect the image to be. So just put only the filenames in your <code>lst<\/code>and should be fine (remove the s3 path).<\/p>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66793845",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1616684272012,
        "Question_original_content":"custom error imread read blank imag file aw follow tutori custom data custom bucket train valid data get follow error custom error imread read blank imag file opt input data train imag classif imag classif model data train img png train data folder name train set lst file like suggest doc imag classif imag classif model data train img png imag classif imag classif model data train img png configur bucket imag classif prefix imag classif model data strain train format bucket prefix svalid valid format bucket prefix strain lst train lst format bucket prefix svalid lst valid lst format bucket prefix train data input traininginput strain distribut fullyrepl content type applic imag data type sprefix valid data input traininginput svalid distribut fullyrepl content type applic imag data type sprefix train data lst input traininginput strain lst distribut fullyrepl content type applic imag data type sprefix valid data lst input traininginput svalid lst distribut fullyrepl content type applic imag data type sprefix data channel train train data valid valid data train lst train data lst valid lst valid data lst check imag download check physic imag sure error get thrown blank suggest great",
        "Question_preprocessed_content":"custom error imread read blank imag file aw follow tutori custom data custom bucket train valid data get follow error train data folder name set file like suggest doc configur check imag download check physic imag sure error get thrown suggest great",
        "Question_gpt_summary_original":"The user is encountering an error while following an image classification tutorial using custom data and S3 buckets. The error message indicates that the image file is blank, even though the user has physically checked and confirmed that the image is present. The user has provided their lst file and other configurations, but is unsure why the error is occurring and is seeking suggestions to resolve the issue.",
        "Question_gpt_summary":"user encount error follow imag classif tutori custom data bucket error messag indic imag file blank user physic check confirm imag present user provid lst file configur unsur error occur seek suggest resolv issu",
        "Answer_original_content":"copi input data specifi strain instanc opt input data train error error messag try concaten filenam lst file path expect imag filenam lstand fine remov path",
        "Answer_preprocessed_content":"copi input data specifi instanc error error messag try concaten filenam file path expect imag filenam fine",
        "Answer_gpt_summary_original":"Solution: The user can resolve the error by removing the S3 path from the filenames in their lst file and only including the filenames. This will allow Sagemaker to copy the input data into the instance and concatenate the filename in the lst file with the path where it expects the image to be.",
        "Answer_gpt_summary":"solut user resolv error remov path filenam lst file includ filenam allow copi input data instanc concaten filenam lst file path expect imag"
    },
    {
        "Question_title":"How do I make this IAM role error in aws sagemaker go away?",
        "Question_body":"<p>I suspect this has to more to do with IAM roles than Sagemaker.<\/p>\n\n<p>I'm following the example <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"noreferrer\">here<\/a><\/p>\n\n<p>Specifically, when it makes this call<\/p>\n\n<pre><code>tf_estimator.fit('s3:\/\/bucket\/path\/to\/training\/data')\n<\/code><\/pre>\n\n<p>I get this error<\/p>\n\n<pre><code>ClientError: An error occurred (AccessDenied) when calling the GetRole operation: User: arn:aws:sts::013772784144:assumed-role\/AmazonSageMaker-ExecutionRole-20181022T195630\/SageMaker is not authorized to perform: iam:GetRole on resource: role SageMakerRole\n<\/code><\/pre>\n\n<p>My notebook instance has an IAM role attached to it.\nThat role has the <code>AmazonSageMakerFullAccess<\/code> policy. It also has a custom policy that looks like this<\/p>\n\n<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"s3:GetObject\",\n            \"s3:PutObject\",\n            \"s3:DeleteObject\",\n            \"s3:ListBucket\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::*\"\n        ]\n    }\n]\n<\/code><\/pre>\n\n<p>}<\/p>\n\n<p>My input files and .py script is in an s3 bucket with the phrase <code>sagemaker<\/code> in it.<\/p>\n\n<p>What else am I missing?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1542853625410,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":6.0,
        "Question_view_count":8160.0,
        "Answer_body":"<p>If you're running the example code on a SageMaker notebook instance, you can use the execution_role which has the <code>AmazonSageMakerFullAccess<\/code> attached.<\/p>\n<pre><code>from sagemaker import get_execution_role\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n<\/code><\/pre>\n<p>And you can pass this role when initializing <code>tf_estimator<\/code>.\nYou can check out the example <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-role.html\" rel=\"nofollow noreferrer\">here<\/a> for using <code>execution_role<\/code> with S3 on notebook instance.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1620293748347,
        "Answer_score":8.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53423061",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1543010904776,
        "Question_original_content":"iam role error awai suspect iam role follow exampl specif make estim fit bucket path train data error clienterror error occur accessdeni call getrol oper user arn aw st assum role amazon executionrol author perform iam getrol resourc role role notebook instanc iam role attach role amazonfullaccess polici custom polici look like version statement effect allow action getobject putobject deleteobject listbucket resourc arn aw input file script bucket phrase miss",
        "Question_preprocessed_content":"iam role error awai suspect iam role follow exampl specif make error notebook instanc iam role attach role polici custom polici look like input file script bucket phrase miss",
        "Question_gpt_summary_original":"The user is encountering an IAM role error when trying to run a call in AWS Sagemaker. The error message indicates that the user's role is not authorized to perform the GetRole operation on the SageMakerRole resource. The user's notebook instance has an IAM role attached to it with the AmazonSageMakerFullAccess policy and a custom policy that allows access to S3 resources. The user's input files and .py script are in an S3 bucket with the phrase \"sagemaker\" in it. The user is unsure of what else they may be missing.",
        "Question_gpt_summary":"user encount iam role error try run error messag indic user role author perform getrol oper role resourc user notebook instanc iam role attach amazonfullaccess polici custom polici allow access resourc user input file script bucket phrase user unsur miss",
        "Answer_original_content":"run exampl code notebook instanc us execut role amazonfullaccess attach import execut role session session role execut role pass role initi estim check exampl execut role notebook instanc",
        "Answer_preprocessed_content":"run exampl code notebook instanc us attach pass role initi check exampl notebook instanc",
        "Answer_gpt_summary_original":"Solution:\n- Use the `execution_role` with `AmazonSageMakerFullAccess` attached when running the example code on a SageMaker notebook instance.\n- Pass this role when initializing `tf_estimator`.\n- Check out the example provided in the link for using `execution_role` with S3 on a notebook instance.",
        "Answer_gpt_summary":"solut us execut role amazonfullaccess attach run exampl code notebook instanc pass role initi estim check exampl provid link execut role notebook instanc"
    },
    {
        "Question_title":"Re-introduce a per-operation automatic build process",
        "Question_body":"This discussion was started on slack and was moved to github for future reference.\n\nOriginal content:\n\nAfter some thinking about the impact and the size of this feature request https:\/\/polyaxon.slack.com\/archives\/C6QBND2SV\/p1618316341162200\nI think we will probably deploy an experimental version next release (v1.9) or the one after (v1.10) to Polyaxon Cloud. Once we think the feature is stable and does not require any major migrations (both on the spec and db level) we will move it to CE & EE.\n\nThis feature will not replace the ad-hoc build operations, users can still create independent polyaxonfiles with a kaniko\/dockerize hub ref.\nThe no-build requirement that the platform provides at the moment will stay the same, so users who have stable pipelines that do not require frequent changes to their images can safely ignore this feature.\nA new section build will be introduced, where users can signal to the platform that a build is required prior to starting the main operation, the build section will provide the necessary fields to provide stuff like, queue, preset, resources, node selectors, ... specific to the build.\nCache (and invalidation) requires some improvements on the commercial offering, but they are planned anyway. I am not sure yet how it would work for CE, but we will work on it when we get to that point and will add docs around edge cases.\nBy providing a build section, Polyaxon will take care of generating the image based on the project and the uuid, project:build-uuid, and will set that image automatically on the container of the main operation. The registry to use will be still configured via a connection.\nA new status building will be added to show that a build is progressing before compiling the main manifest (without the build). And a build icon in the UI to redirect to the build run for viewing info, logs, \u2026 about the build operation.\nWhen the build and matrix sections are used together, a single build operation will be scheduled and will be used for all runs.\nTo make the process predictable and easy, when the build section is used with -u\/--upload flag of the run command, the uploaded artifacts will automatically be injected in the build operation and not the main operation, which means users have to copy any necessary artifacts to their generated image.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619176829000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"https:\/\/polyaxon.com\/docs\/automation\/builds\/",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1286",
        "Tool":"Polyaxon",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-05-27T18:21:27Z",
                "Answer_score":1,
                "Answer_body":"https:\/\/polyaxon.com\/docs\/automation\/builds\/"
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"introduc oper automat build process discuss start slack move github futur refer origin content think impact size featur request http slack com archiv cqbndsv think probabl deploi experiment version releas cloud think featur stabl requir major migrat spec level featur replac hoc build oper user creat independ file kaniko docker hub ref build requir platform provid moment stai user stabl pipelin requir frequent chang imag safe ignor featur new section build introduc user signal platform build requir prior start main oper build section provid necessari field provid stuff like queue preset resourc node selector specif build cach invalid requir improv commerci offer plan sure work work point add doc edg case provid build section care gener imag base project uuid project build uuid set imag automat contain main oper registri us configur connect new statu build ad build progress compil main manifest build build icon redirect build run view info log build oper build matrix section singl build oper schedul run process predict easi build section upload flag run command upload artifact automat inject build oper main oper mean user copi necessari artifact gener imag",
        "Question_preprocessed_content":"automat build process discuss start slack move github futur refer origin content think impact size featur request think probabl deploi experiment version releas cloud think featur stabl requir major migrat featur replac build oper user creat independ file hub ref requir platform provid moment stai user stabl pipelin requir frequent chang imag safe ignor featur new section build introduc user signal platform build requir prior start main oper build section provid necessari field provid stuff like queue preset resourc node selector specif build cach requir improv commerci offer plan sure work work point add doc edg case provid build section care gener imag base project uuid set imag automat contain main oper registri us configur connect new statu build ad build progress compil main manifest build icon redirect build run view info log build oper build matrix section singl build oper schedul run process predict easi build section flag run command upload artifact automat inject build oper main oper mean user copi necessari artifact gener imag",
        "Question_gpt_summary_original":"The user is proposing a feature request for an automatic build process in Polyaxon. The platform will introduce a new section called \"build\" where users can signal to the platform that a build is required prior to starting the main operation. The feature will not replace the ad-hoc build operations, and users can still create independent polyaxonfiles with a kaniko\/dockerize hub ref. The cache and invalidation require some improvements on the commercial offering, but they are planned anyway. When the build and matrix sections are used together, a single build operation will be scheduled and will be used for all runs.",
        "Question_gpt_summary":"user propos featur request automat build process platform introduc new section call build user signal platform build requir prior start main oper featur replac hoc build oper user creat independ file kaniko docker hub ref cach invalid requir improv commerci offer plan build matrix section singl build oper schedul run",
        "Answer_original_content":"http com doc autom build",
        "Answer_preprocessed_content":"",
        "Answer_gpt_summary_original":"Solution: The proposed feature request for an automatic build process in Polyaxon will introduce a new section called \"build\" where users can signal to the platform that a build is required prior to starting the main operation. The feature will not replace the ad-hoc build operations, and users can still create independent polyaxonfiles with a kaniko\/dockerize hub ref. When the build and matrix sections are used together, a single build operation will be scheduled and will be used for all runs. However, the cache and invalidation require some improvements on the commercial offering, but they are planned anyway.",
        "Answer_gpt_summary":"solut propos featur request automat build process introduc new section call build user signal platform build requir prior start main oper featur replac hoc build oper user creat independ file kaniko docker hub ref build matrix section singl build oper schedul run cach invalid requir improv commerci offer plan"
    },
    {
        "Question_title":"databricks to power bi",
        "Question_body":"Hello community , i'm trying to save stream data in delta lake and i want to push this data to power bi for real time insights , however when i try to connect databricks to power bi i get an empty table , someone can help me please ??\u00a0\nif there's another alternative it would be grateful\u00a0\nthank you",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1621248613000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":9.0,
        "Answer_body":"Hello Nadine,\n\n\nYou might want to post this on the delta-lake user group or Delta Lake slack channel. You can join both here:\u00a0https:\/\/delta.io\/\n\u00a0\n\n\n\nThanks,\nJules\n\n\n\n\n\n\n\u2013\u2013\n\nThe Best Ideas are Simple\n\nJules S. Damji\n\nSr. Developer Advocate\n\nDatabricks, Inc.\n\nju...@databricks.com\n\n(510) 304-7686\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/dc8ee8ff-3f04-423c-a446-96b7f250a3e5n%40googlegroups.com.. Ok , thank you\u00a0\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/GvkoTw0sL4U",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-05-17T11:47:47",
                "Answer_body":"Hello Nadine,\n\n\nYou might want to post this on the delta-lake user group or Delta Lake slack channel. You can join both here:\u00a0https:\/\/delta.io\/\n\u00a0\n\n\n\nThanks,\nJules\n\n\n\n\n\n\n\u2013\u2013\n\nThe Best Ideas are Simple\n\nJules S. Damji\n\nSr. Developer Advocate\n\nDatabricks, Inc.\n\nju...@databricks.com\n\n(510) 304-7686\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/dc8ee8ff-3f04-423c-a446-96b7f250a3e5n%40googlegroups.com."
            },
            {
                "Answer_creation_time":"2021-05-17T11:50:14",
                "Answer_body":"Ok , thank you\u00a0\n\ue5d3"
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"databrick power hello commun try save stream data delta lake want push data power real time insight try connect databrick power tabl help altern grate thank",
        "Question_preprocessed_content":"databrick power hello commun try save stream data delta lake want push data power real time insight try connect databrick power tabl help altern grate thank",
        "Question_gpt_summary_original":"The user is facing challenges in connecting Databricks to Power BI to save stream data in Delta Lake and obtain real-time insights. Despite attempting to connect the two platforms, the user is encountering an empty table and is seeking assistance or alternative solutions.",
        "Question_gpt_summary":"user face challeng connect databrick power save stream data delta lake obtain real time insight despit attempt connect platform user encount tabl seek assist altern solut",
        "Answer_original_content":"hello nadin want post delta lake user group delta lake slack channel join http delta thank jule best idea simpl jule damji develop advoc databrick databrick com receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user dceeff bfaen googlegroup com thank",
        "Answer_preprocessed_content":"hello nadin want post user group delta lake slack channel join thank jule best idea simpl jule damji develop advoc databrick receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit thank",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"MLFlow UI issue when running in docker",
        "Question_body":"I installed mlflow and started the ui with no issues on my windows 10 machine in an anaconda python 3.7 environment and am able to access the UI via http:\/\/localhost:5000\nHowever, when doing the exact same thing within the anacanda3 docker container the UI doesn't appear to be rendering\/responding.\nMy docker run command includes the proper port and I am able to exec into the container, install mlflow and start the ui (>mlflow ui) without any errors.\n\n\ndocker run --name conda3 -d -t -v \/\/c\/\/develop:\/develop -p 5000:5000 continuumio\/anaconda3\\\n\nWhen I try to access\u00a0http:\/\/localhost:5000\u00a0(or\u00a0http:\/\/127.0.0.1:5000\/), the response in the browser is \"ERR_EMPTY_RESPONSE\".\nI tried to access the UI within the contain via lynx, to confirm that it is running, however, the response is just a warning that this site is javascript which can't be rendered in lynx.\n\nI also tried the above docker approach using python 3.6 and also utilizing the docker container I normally develop with but in both cases I still get \"ERR_EMPTY_RESPONSE\".\n\nI am pretty much stuck at this point so any suggestions will be appreciated. Thanks.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1575115725000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":557.0,
        "Answer_body":"Solution was to start the server with the 'server' command instead of the 'ui' command like this: mlflow server -h 0.0.0.0\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/7cezJGzfnic",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-12-12T23:21:54",
                "Answer_body":"Solution was to start the server with the 'server' command instead of the 'ui' command like this: mlflow server -h 0.0.0.0\n\ue5d3"
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"issu run docker instal start issu window machin anaconda python environ abl access http localhost exact thing anacanda docker contain appear render respond docker run command includ proper port abl exec contain instal start error docker run conda develop develop continuumio anaconda try accesshttp localhost orhttp respons browser err respons tri access contain lynx confirm run respons warn site javascript render lynx tri docker approach python util docker contain normal develop case err respons pretti stuck point suggest appreci thank",
        "Question_preprocessed_content":"issu run docker instal start issu window machin anaconda python environ abl access exact thing anacanda docker contain appear docker run command includ proper port abl exec contain instal start error docker run conda try respons browser tri access contain lynx confirm run respons warn site javascript render lynx tri docker approach python util docker contain normal develop case pretti stuck point suggest appreci thank",
        "Question_gpt_summary_original":"The user is facing challenges with MLFlow UI when running it in a docker container. The UI is not rendering\/responding when accessed through the browser, and the response is \"ERR_EMPTY_RESPONSE\". The user has tried accessing the UI within the container via lynx, but it only shows a warning that the site is javascript which cannot be rendered in lynx. The user has tried different approaches, including using python 3.6 and the docker container they normally develop with, but the issue persists.",
        "Question_gpt_summary":"user face challeng run docker contain render respond access browser respons err respons user tri access contain lynx show warn site javascript render lynx user tri differ approach includ python docker contain normal develop issu persist",
        "Answer_original_content":"solut start server server command instead command like server",
        "Answer_preprocessed_content":"solut start server server command instead command like server",
        "Answer_gpt_summary_original":"Solution: The solution mentioned in the discussion was to start the server with the 'server' command instead of the 'ui' command. The command to start the server is \"mlflow server -h 0.0.0.0\".",
        "Answer_gpt_summary":"solut solut mention discuss start server server command instead command command start server server"
    },
    {
        "Question_title":"Sagemaker to use processed pickled ndarray instead of csv files from S3",
        "Question_body":"<p>I understand that you can pass a CSV file from S3 into a Sagemaker XGBoost container using the following code<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>train_channel = sagemaker.session.s3_input(train_data, content_type='text\/csv')\nvalid_channel = sagemaker.session.s3_input(validation_data, content_type='text\/csv')\n\ndata_channels = {'train': train_channel, 'validation': valid_channel}\nxgb_model.fit(inputs=data_channels,  logs=True)\n<\/code><\/pre>\n\n<p>But I have an ndArray stored in S3 bucket. These are processed, label encoded, feature engineered arrays. I would want to pass this into the container instead of the csv. I do understand I can always convert my ndarray into csv files before saving it in S3. Just checking if there is an array option.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1566996073187,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":92.0,
        "Answer_body":"<p>There are multiple options for algorithms in SageMaker:<\/p>\n\n<ol>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">Built-in algorithms<\/a>, like the SageMaker XGBoost you mention<\/li>\n<li>Custom, user-created algorithm code, which can be:\n\n<ul>\n<li>Written for a pre-built docker image, available for Sklearn, TensorFlow, Pytorch, MXNet<\/li>\n<li>Written in your own container<\/li>\n<\/ul><\/li>\n<\/ol>\n\n<p>When you use built-ins (option 1), your choice of data format options is limited to what the built-ins support, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#InputOutput-XGBoost\" rel=\"nofollow noreferrer\">which is only csv and libsvm in the case of the built-in XGBoost<\/a>. If you want to use custom data formats and pre-processing logic before XGBoost, it is absolutely possible if you use your own script leveraging the open-source XGBoost. You can get inspiration from the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">Random Forest demo<\/a> to see how to create custom models in pre-built containers<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57692681",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1568417822248,
        "Question_original_content":"us process pickl ndarrai instead csv file understand pass csv file xgboost contain follow code train channel session input train data content type text csv valid channel session input valid data content type text csv data channel train train channel valid valid channel xgb model fit input data channel log true ndarrai store bucket process label encod featur engin arrai want pass contain instead csv understand convert ndarrai csv file save check arrai option",
        "Question_preprocessed_content":"us process pickl ndarrai instead csv file understand pass csv file xgboost contain follow code ndarrai store bucket process label encod featur engin arrai want pass contain instead csv understand convert ndarrai csv file save check arrai option",
        "Question_gpt_summary_original":"The user wants to pass a processed pickled ndarray from an S3 bucket into a Sagemaker XGBoost container instead of using CSV files. They are looking for an array option and are aware that they can convert their ndarray into CSV files before saving it in S3.",
        "Question_gpt_summary":"user want pass process pickl ndarrai bucket xgboost contain instead csv file look arrai option awar convert ndarrai csv file save",
        "Answer_original_content":"multipl option algorithm built algorithm like xgboost mention custom user creat algorithm code written pre built docker imag avail sklearn tensorflow pytorch mxnet written contain us built in option choic data format option limit built in support csv libsvm case built xgboost want us custom data format pre process logic xgboost absolut possibl us script leverag open sourc xgboost inspir random forest demo creat custom model pre built contain",
        "Answer_preprocessed_content":"multipl option algorithm algorithm like xgboost mention custom algorithm code written docker imag avail sklearn tensorflow pytorch mxnet written contain us choic data format option limit support csv libsvm case xgboost want us custom data format logic xgboost absolut possibl us script leverag xgboost inspir random forest demo creat custom model contain",
        "Answer_gpt_summary_original":"There are two options for algorithms in SageMaker: built-in algorithms and custom, user-created algorithm code. When using built-in algorithms, the choice of data format options is limited to what the built-ins support, which is only csv and libsvm in the case of the built-in XGBoost. However, it is possible to use custom data formats and pre-processing logic before XGBoost by using a custom script leveraging the open-source XGBoost. The user can get inspiration from the Random Forest demo to see how to create custom models in pre-built containers. No solution is provided for passing a processed pickled ndarray from an S3 bucket into a Sagemaker XGBoost container instead of using CSV files.",
        "Answer_gpt_summary":"option algorithm built algorithm custom user creat algorithm code built algorithm choic data format option limit built in support csv libsvm case built xgboost possibl us custom data format pre process logic xgboost custom script leverag open sourc xgboost user inspir random forest demo creat custom model pre built contain solut provid pass process pickl ndarrai bucket xgboost contain instead csv file"
    },
    {
        "Question_title":"How to setup AWS sagemaker - Resource limit Error",
        "Question_body":"<p>I'm trying to set up my first SageMaker Studio so my team and myself can run some post processing scripts in a shared environment but I'm having issues.<\/p>\n<p>I've followed the steps in this video(<a href=\"https:\/\/www.youtube.com\/watch?v=wiDHCWVrjCU&amp;ab_channel=AmazonWebServices\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=wiDHCWVrjCU&amp;ab_channel=AmazonWebServices<\/a>) which are:<\/p>\n<ol>\n<li>Select Standard setup<\/li>\n<li>Select AWS Identity and Access Management (IAM)<\/li>\n<li>Under permissions - Create and select new execution role<\/li>\n<li>Under Network and storage - Select VPC, Subnet and Security group<\/li>\n<li>Hit the submit button at the bottom of the page.<\/li>\n<\/ol>\n<p>In the video, he clicks submit and is taken to the control panel where he starts the next phase of adding users, however I'm greeted with this error.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/g4k2g.png\" rel=\"nofollow noreferrer\"> Resource limit Error<\/a><\/p>\n<p>I've checked my Registered domains under route 53 and it says No domains to display, I've also checked my S2 and I have no instances so I have no idea where the 2 domains being utilized are.<\/p>\n<p>My dashboard, image and Notebooks are all empty so as far as I know there's nothing setup on this Sage Maker account.<\/p>\n<p>Could anyone tell me how to resolve this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615398273417,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":237.0,
        "Answer_body":"<p>You can have maximum 1 studio domain per region, by the default limits. Though, it seems like you have two domains already provisioned. Try to delete all the domains through the AWS cli and recreate with the AWS Management Console.<\/p>\n<p>Unfortunately, AWS Management Console cannot visualize more than one Studio domain.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66570138",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1626975605176,
        "Question_original_content":"setup resourc limit error try set studio team run post process script share environ have issu follow step video http youtub com watch widhcwvrjcu channel amazonwebservic select standard setup select aw ident access manag iam permiss creat select new execut role network storag select vpc subnet secur group hit submit button page video click submit taken control panel start phase ad user greet error resourc limit error check regist domain rout sai domain displai check instanc idea domain util dashboard imag notebook far know setup sage maker account tell resolv error",
        "Question_preprocessed_content":"setup resourc limit error try set studio team run post process script share environ have issu follow step select standard setup select aw ident access manag permiss creat select new execut role network storag select vpc subnet secur group hit submit button page video click submit taken control panel start phase ad user greet error resourc limit error check regist domain rout sai domain displai check instanc idea domain util dashboard imag notebook far know setup sage maker account tell resolv error",
        "Question_gpt_summary_original":"The user is encountering a \"Resource limit Error\" while setting up their first SageMaker Studio to run post-processing scripts in a shared environment. They followed the steps in a video tutorial but are unable to proceed due to the error. The user has checked their Registered domains under route 53 and S2 but found no instances. They are seeking help to resolve the error.",
        "Question_gpt_summary":"user encount resourc limit error set studio run post process script share environ follow step video tutori unabl proce error user check regist domain rout instanc seek help resolv error",
        "Answer_original_content":"maximum studio domain region default limit like domain provis try delet domain aw cli recreat aw manag consol unfortun aw manag consol visual studio domain",
        "Answer_preprocessed_content":"maximum studio domain region default limit like domain provis try delet domain aw cli recreat aw manag consol unfortun aw manag consol visual studio domain",
        "Answer_gpt_summary_original":"Solution: The user can try to delete all the domains through the AWS cli and recreate with the AWS Management Console. However, it should be noted that the AWS Management Console cannot visualize more than one Studio domain.",
        "Answer_gpt_summary":"solut user try delet domain aw cli recreat aw manag consol note aw manag consol visual studio domain"
    },
    {
        "Question_title":"Random forests on Azure GPU VM using the SDK",
        "Question_body":"Can you please share any code examples for training random forests with GPU on Azure using libraries.\nI want to run on the multiple nodes.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1595050125193,
        "Question_favorite_count":34.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"@vautoml-0887 Thanks for the question. You can run LightGBM with boosting=random_forest, Please follow the below documentation:\nhttps:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst#boosting\n\n\n\n\nHere is a general tutorial on how to run LightGBM on GPU, You can run it on any Azure GPU VM:\nhttps:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/GPU-Tutorial.rst\n\n\n\n\nIf you need to run it on multiple nodes, there is also a distributed spark implementation available at https:\/\/github.com\/Azure\/mmlspark.\n\n\n\n\nRandom Forests for the GPU using PyCUDA: https:\/\/pypi.org\/project\/cudatree\/",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/49008\/random-forests-on-azure-gpu-vm-using-the-sdk.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-20T07:47:48.033Z",
                "Answer_score":0,
                "Answer_body":"@vautoml-0887 Thanks for the question. You can run LightGBM with boosting=random_forest, Please follow the below documentation:\nhttps:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst#boosting\n\n\n\n\nHere is a general tutorial on how to run LightGBM on GPU, You can run it on any Azure GPU VM:\nhttps:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/GPU-Tutorial.rst\n\n\n\n\nIf you need to run it on multiple nodes, there is also a distributed spark implementation available at https:\/\/github.com\/Azure\/mmlspark.\n\n\n\n\nRandom Forests for the GPU using PyCUDA: https:\/\/pypi.org\/project\/cudatree\/",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1595231268032,
        "Question_original_content":"random forest azur gpu sdk share code exampl train random forest gpu azur librari want run multipl node",
        "Question_preprocessed_content":"random forest azur gpu sdk share code exampl train random forest gpu azur librari want run multipl node",
        "Question_gpt_summary_original":"The user is seeking code examples for training random forests with GPU on Azure using libraries and wants to run it on multiple nodes.",
        "Question_gpt_summary":"user seek code exampl train random forest gpu azur librari want run multipl node",
        "Answer_original_content":"vautoml thank question run lightgbm boost random forest follow document http github com microsoft lightgbm blob master doc paramet rst boost gener tutori run lightgbm gpu run azur gpu http github com microsoft lightgbm blob master doc gpu tutori rst need run multipl node distribut spark implement avail http github com azur mmlspark random forest gpu pycuda http pypi org project cudatre",
        "Answer_preprocessed_content":"thank question run lightgbm follow document gener tutori run lightgbm gpu run azur gpu need run multipl node distribut spark implement avail random forest gpu pycuda",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- LightGBM with boosting=random_forest can be used to train random forests with GPU on Azure. Documentation for this is available at https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst#boosting.\n- A general tutorial on how to run LightGBM on GPU is available at https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/GPU-Tutorial.rst.\n- If the user needs to run it on multiple nodes, a distributed spark implementation is available at https:\/\/github.com\/Azure\/mmlspark.\n- Another option is to use PyCUDA to train random forests for the GPU. The package cudatree can be used for this purpose, and",
        "Answer_gpt_summary":"possibl solut mention discuss lightgbm boost random forest train random forest gpu azur document avail http github com microsoft lightgbm blob master doc paramet rst boost gener tutori run lightgbm gpu avail http github com microsoft lightgbm blob master doc gpu tutori rst user need run multipl node distribut spark implement avail http github com azur mmlspark option us pycuda train random forest gpu packag cudatre purpos"
    },
    {
        "Question_title":"How do I retrieve a model in Vertex AI?",
        "Question_body":"<p>I defined a training job:<\/p>\n<pre><code>job = aiplatform.AutoMLTextTrainingJob(...\n<\/code><\/pre>\n<p>then I created a model by running the job:<\/p>\n<pre><code>model = job.run(...\n<\/code><\/pre>\n<p>It worked fine but it is now the next day and the variable <code>model<\/code> was in a Jupyter notebook and no longer exists. I have tried to get it back with:<\/p>\n<pre><code>from google.cloud import aiplatform_v1beta1\n\ndef sample_get_model():\n    client = aiplatform_v1beta1.ModelServiceClient()\n\n    model_id=id_of_training_pipeline\n    name= f'projects\/{PROJECT}\/locations\/{REGION}\/models\/{model_id}'\n    \n    request = aiplatform_v1beta1.GetModelRequest(name=name)\n    response = client.get_model(request=request)\n    print(response)\n\nsample_get_model()\n<\/code><\/pre>\n<p>I have also tried the id of v1 of the model created in place of <code>id_of_training_pipeline<\/code> and I have tried <code>\/pipelines\/pipeline_id<\/code><\/p>\n<p>but I get:\n<code>E0805 15:12:36.784008212   28406 hpack_parser.cc:1234]       Error parsing metadata: error=invalid value key=content-type value=text\/html; charset=UTF-8<\/code><\/p>\n<p>(<code>PROJECT<\/code> and <code>REGION<\/code> are set correctly).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659709186783,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":45.0,
        "Answer_body":"<p>Found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/samples\/aiplatform-get-model-sample#aiplatform_get_model_sample-python\" rel=\"nofollow noreferrer\">this<\/a> Google code which works.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73251212",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1659712728223,
        "Question_original_content":"retriev model defin train job job aiplatform automltexttrainingjob creat model run job model job run work fine dai variabl model jupyt notebook longer exist tri googl cloud import aiplatform vbeta def sampl model client aiplatform vbeta modelservicecli model train pipelin project project locat region model model request aiplatform vbeta getmodelrequest respons client model request request print respons sampl model tri model creat place train pipelin tri pipelin pipelin hpack parser error pars metadata error invalid valu kei content type valu text html charset utf project region set correctli",
        "Question_preprocessed_content":"retriev model defin train job creat model run job work fine dai variabl jupyt notebook longer exist tri tri model creat place tri set correctli",
        "Question_gpt_summary_original":"The user is facing a challenge in retrieving a model in Vertex AI that was created by running a training job in a Jupyter notebook. The user has tried to retrieve the model using the id of the training pipeline and the v1 id of the model, but is encountering an error related to parsing metadata.",
        "Question_gpt_summary":"user face challeng retriev model creat run train job jupyt notebook user tri retriev model train pipelin model encount error relat pars metadata",
        "Answer_original_content":"googl code work",
        "Answer_preprocessed_content":"googl code work",
        "Answer_gpt_summary_original":"Solution: A Google code was suggested which can be used to retrieve the model in Vertex AI that was created by running a training job in a Jupyter notebook.",
        "Answer_gpt_summary":"solut googl code suggest retriev model creat run train job jupyt notebook"
    },
    {
        "Question_title":"SageMaker create PyTorchModel without deploying",
        "Question_body":"<p>If I instantiate a SageMaker <code>PyTorchModel<\/code> object like this:<\/p>\n\n<pre><code>from sagemaker.pytorch import PyTorchModel\n\nmodel = PyTorchModel(name=name_from_base('model-name'),\n                     model_data=model_data,\n                     role=role,\n                     framework_version='1.0.0',\n                     entry_point='serve.py',\n                     source_dir='src',\n                     sagemaker_session=sagemaker_session,\n                     predictor_cls=ImagePredictor)\n\n#model.create_without_deploying??\n<\/code><\/pre>\n\n<p>Is there a way that I can create this model using the sagemaker python SDK so that the model shows up in the SageMaker console, but <em>without<\/em> actually deploying it to an endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559938343560,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1559938910380,
        "Question_score":2.0,
        "Question_view_count":434.0,
        "Answer_body":"<p>I don't think it is possible to do so using the high-level SageMaker Pyhton SDK. However, you should be able to do it by calling the CreateModel API using the low-level boto3 <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model<\/a>. For your reference, below is an example snippet code on how to do it.<\/p>\n\n<pre><code>%%time\nimport boto3\nimport time\n\nsage = boto3.Session().client(service_name='sagemaker')\n\nimage_uri = '520713654638.dkr.ecr.us-east-1.amazonaws.com\/sagemaker-pytorch:1.0.0-cpu-py3'\nmodel_data ='s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\/output\/model.tar.gz'\nsource = 's3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\/sourcedir.tar.gz'\nrole = 'arn:aws:iam::xxxxxxxx:role\/service-role\/AmazonSageMaker-ExecutionRole-xxxxxx'\n\ntimestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\nmodel_name = 'my-pytorch-model' + timestamp\n\nresponse = sage.create_model(\n    ModelName=model_name,\n    PrimaryContainer={\n        'Image': image_uri,\n        'ModelDataUrl': model_data,\n        'Environment': { 'SAGEMAKER_CONTAINER_LOG_LEVEL':'20', 'SAGEMAKER_ENABLE_CLOUDWATCH_METRICS': 'False', \n                   'SAGEMAKER_PROGRAM': 'generate.py','SAGEMAKER_REGION': 'us-east-1','SAGEMAKER_SUBMIT_DIRECTORY': source}\n         },\n         ExecutionRoleArn=role\n}\nprint(response)\n<\/code><\/pre>\n\n<p>If you get no error message, then the model will shows up in the SageMaker console<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56500704",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1561626837347,
        "Question_original_content":"creat pytorchmodel deploi instanti pytorchmodel object like pytorch import pytorchmodel model pytorchmodel base model model data model data role role framework version entri point serv sourc dir src session session predictor cl imagepredictor model creat deploi wai creat model python sdk model show consol actual deploi endpoint",
        "Question_preprocessed_content":"creat pytorchmodel deploi instanti object like wai creat model python sdk model show consol actual deploi endpoint",
        "Question_gpt_summary_original":"The user is looking for a way to create a SageMaker PyTorchModel object using the sagemaker python SDK so that the model appears in the SageMaker console without deploying it to an endpoint.",
        "Question_gpt_summary":"user look wai creat pytorchmodel object python sdk model appear consol deploi endpoint",
        "Answer_original_content":"think possibl high level pyhton sdk abl call createmodel api low level boto http boto amazonaw com document api latest refer servic html client creat model refer exampl snippet code time import boto import time sage boto session client servic imag uri dkr ecr east amazonaw com pytorch cpu model data output model tar sourc sourcedir tar role arn aw iam role servic role amazon executionrol timestamp time strftime time gmtime model pytorch model timestamp respons sage creat model modelnam model primarycontain imag imag uri modeldataurl model data environ contain log level enabl cloudwatch metric fals program gener region east submit directori sourc executionrolearn role print respons error messag model show consol",
        "Answer_preprocessed_content":"think possibl pyhton sdk abl call createmodel api boto refer exampl snippet code error messag model show consol",
        "Answer_gpt_summary_original":"Solution: It is not possible to create a SageMaker PyTorchModel object using the high-level SageMaker Python SDK without deploying it to an endpoint. However, it can be done by calling the CreateModel API using the low-level boto3. The discussion provides an example snippet code on how to do it using boto3.",
        "Answer_gpt_summary":"solut possibl creat pytorchmodel object high level python sdk deploi endpoint call createmodel api low level boto discuss provid exampl snippet code boto"
    },
    {
        "Question_title":"Sagemaker batch transform",
        "Question_body":"Hi, it seems that Sagemaker Batch Transform is limited to 100MB payloads I'd like to run preds against a 5GB csv file, what the recommended way to do so?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1532619204000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":254.0,
        "Answer_body":"SageMaker Batch Transform will automatically split your input file into whatever payload size is specified if you use \"SplitType\": \"Line\" and \"BatchStrategy\": \"MultiRecord\". There's no need to split files yourself or to use large payload sizes unless you have very large single records.\n\nHope that helps!",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlefH1ni4QOaulUT4870D5g\/sagemaker-batch-transform",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-07-26T18:02:46.000Z",
                "Answer_score":0,
                "Answer_body":"SageMaker Batch Transform will automatically split your input file into whatever payload size is specified if you use \"SplitType\": \"Line\" and \"BatchStrategy\": \"MultiRecord\". There's no need to split files yourself or to use large payload sizes unless you have very large single records.\n\nHope that helps!",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1532628166000,
        "Question_original_content":"batch transform batch transform limit payload like run pred csv file recommend wai",
        "Question_preprocessed_content":"batch transform batch transform limit payload like run pred csv file recommend wai",
        "Question_gpt_summary_original":"The user is facing a challenge with Sagemaker Batch Transform as it has a limit of 100MB payloads. The user wants to run predictions against a 5GB csv file and is seeking recommendations on how to do so.",
        "Question_gpt_summary":"user face challeng batch transform limit payload user want run predict csv file seek recommend",
        "Answer_original_content":"batch transform automat split input file payload size specifi us splittyp line batchstrategi multirecord need split file us larg payload size larg singl record hope help",
        "Answer_preprocessed_content":"batch transform automat split input file payload size specifi us splittyp line batchstrategi multirecord need split file us larg payload size larg singl record hope help",
        "Answer_gpt_summary_original":"Solution: The solution provided in the discussion is to use \"SplitType\": \"Line\" and \"BatchStrategy\": \"MultiRecord\" in SageMaker Batch Transform. This will automatically split the input file into smaller payload sizes, eliminating the need to split files manually or use large payload sizes.",
        "Answer_gpt_summary":"solut solut provid discuss us splittyp line batchstrategi multirecord batch transform automat split input file smaller payload size elimin need split file manual us larg payload size"
    },
    {
        "Question_title":"how to login wandb with another acount using colab?",
        "Question_body":"<p>For example, I have A, B acounts.<\/p>\n<p>First, I log in Google Colab with A account.\nand I want to log in wandb with B acounts. ( using !wandb login )\nis it possible??<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1644556957917,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":316.0,
        "Answer_body":"<p>You can you the following commands to force a relogin:<\/p>\n<ul>\n<li>from terminal<\/li>\n<\/ul>\n<pre><code>wandb login --relogin\n<\/code><\/pre>\n<ul>\n<li>Using the API:<\/li>\n<\/ul>\n<pre><code>import wandb\nwandb.login(relogin=True)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71075704",
        "Tool":"Weights & Biases",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1644559750243,
        "Question_original_content":"login acount colab exampl acount log googl colab account want log acount login possibl",
        "Question_preprocessed_content":"login acount colab exampl acount log googl colab account want log acount possibl",
        "Question_gpt_summary_original":"The user is trying to log in to Wandb with a different account while using Google Colab. They are currently logged in with account A and want to log in with account B using the command \"!wandb login\". The user is seeking clarification on whether this is possible.",
        "Question_gpt_summary":"user try log differ account googl colab current log account want log account command login user seek clarif possibl",
        "Answer_original_content":"follow command forc relogin termin login relogin api import login relogin true",
        "Answer_preprocessed_content":"follow command forc relogin termin api",
        "Answer_gpt_summary_original":"Solution: The user can force a relogin to Wandb with a different account using the command \"!wandb login --relogin\" from the terminal or \"wandb.login(relogin=True)\" using the API.",
        "Answer_gpt_summary":"solut user forc relogin differ account command login relogin termin login relogin true api"
    },
    {
        "Question_title":"Facing Problems in mlflow deployment on windows server",
        "Question_body":"I am new to mlflow and finding its windows deployment extremely challenging. Has anyone been able to successfully deploy models with mlflow on windows machine ??",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1568860989000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":7.0,
        "Answer_body":"Maybe you can use docker container .\n\n\nOn Thu, 19 Sep 2019, 12:13 babar ali, <bac...@gmail.com> wrote:\n\nI am new to mlflow and finding its windows deployment extremely challenging. Has anyone been able to successfully deploy models with mlflow on windows machine ??\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/5e8a0cee-7833-48d3-9fed-a61c3db8c349%40googlegroups.com.. Hi Babar.\u00a0\n\n\nUnfortunately mlflow windows support is limited to experiment tracking for now (we would appreciate contributions).\n\ue5d3. Just to add to that, you may want to consider Docker on Windows as a way to run the packaged models. MLflow provides a command to package models as a Docker container already. The simple built-in model server in mlflow serve is not designed to run on Windows right now.\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\n\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/2c536152-711c-4be5-b79b-45530411ac14%40googlegroups.com.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/lovl7Ns6x0Y",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-09-20T06:19:56",
                "Answer_body":"Maybe you can use docker container .\n\n\nOn Thu, 19 Sep 2019, 12:13 babar ali, <bac...@gmail.com> wrote:\n\nI am new to mlflow and finding its windows deployment extremely challenging. Has anyone been able to successfully deploy models with mlflow on windows machine ??\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/5e8a0cee-7833-48d3-9fed-a61c3db8c349%40googlegroups.com."
            },
            {
                "Answer_creation_time":"2019-09-20T12:45:11",
                "Answer_body":"Hi Babar.\u00a0\n\n\nUnfortunately mlflow windows support is limited to experiment tracking for now (we would appreciate contributions).\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-09-22T19:27:11",
                "Answer_body":"Just to add to that, you may want to consider Docker on Windows as a way to run the packaged models. MLflow provides a command to package models as a Docker container already. The simple built-in model server in mlflow serve is not designed to run on Windows right now.\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\n\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/2c536152-711c-4be5-b79b-45530411ac14%40googlegroups.com."
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"face problem deploy window server new find window deploy extrem challeng abl successfulli deploi model window machin",
        "Question_preprocessed_content":"face problem deploy window server new find window deploy extrem challeng abl successfulli deploi model window machin",
        "Question_gpt_summary_original":"The user is facing challenges in deploying mlflow on a Windows server and is seeking advice from others who have successfully deployed models with mlflow on a Windows machine.",
        "Question_gpt_summary":"user face challeng deploi window server seek advic successfulli deploi model window machin",
        "Answer_original_content":"mayb us docker contain thu sep babar ali wrote new find window deploy extrem challeng abl successfulli deploi model window machin receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user eace fed acdbc googlegroup com babar unfortun window support limit experi track appreci contribut add want consid docker window wai run packag model provid command packag model docker contain simpl built model server serv design run window right receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user googlegroup com",
        "Answer_preprocessed_content":"mayb us docker contain thu sep babar ali wrote new find window deploy extrem challeng abl successfulli deploi model window machin receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit babar unfortun window support limit experi track add want consid docker window wai run packag model provid command packag model docker contain simpl model server serv design run window right receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to use Docker on Windows as a way to run the packaged models. MLflow provides a command to package models as a Docker container already. The simple built-in model server in mlflow serve is not designed to run on Windows right now.",
        "Answer_gpt_summary":"solut possibl solut mention discuss us docker window wai run packag model provid command packag model docker contain simpl built model server serv design run window right"
    },
    {
        "Question_title":"Best way to run 1000s of training jobs on sagemaker",
        "Question_body":"<p>I have thousands of training jobs that I want to run on sagemaker. Basically I have a list of hyperparameters and I want to train the model for <em>all<\/em> of those hyperparmeters in parallel (not a standard hyperparameter tuning where we just want to optimize the hyperparameter, here we want to train for all of the hyperparameters). I have searched the docs quite extensively but it surprises me that I couldn't find any info about this, even though it seems like a pretty basic functionality.<\/p>\n<p>For example, let's say I have 10,000 training jobs, and my quota is 20 instances, what is the best way to run these jobs utilizing all my available instances? In particular,<\/p>\n<ul>\n<li>Is there a &quot;queue manager&quot; functionality that takes the list of hyperparameters and runs the training jobs in batches of 20 until they are all done (even better if it could keep track of failed\/completed jobs).<\/li>\n<li>Is it best practice to run a single training job per instance? If that's the case do I need to ask for a much higher quota on the number of instance?<\/li>\n<li>If this functionality does not exist in sagemaker, is it worth using EC2 instead since it's a bit cheaper?<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1650506611153,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1650508898300,
        "Question_score":0.0,
        "Question_view_count":269.0,
        "Answer_body":"<p>Your question is very broad and the best way forward would depend on other details of your use-case, so we will have to make some assumptions.<\/p>\n<p>[Queue manager]\nSageMaker does <em>not<\/em> have a queue manager. If at the end you decide you need a queue manager, I would suggest looking towards AWS Batch.<\/p>\n<p>[Single vs multiple training jobs]\nSince you need to run 10s of thousands job I assume you are training fairly lightweight models, so to save on time, you would be better off reusing instances for multiple training jobs. (Otherwise, with 20 instances limit, you need 500 rounds of training, with a 3 min start time - depending on instance type - you need 25 hours just for the wait time. Depending on the complexity of each individual model, this 25hours might be significant or totally acceptable).<\/p>\n<p>[Instance limit increase]\nYou can always ask for a limit increase, but going from a limit of 20 to 10k at once is likely that will not be accepted by the AWS support team, unless you are part of an organisation with a track record of usage on AWS, in which case this might be fine.<\/p>\n<p>[One possible option] (Assuming multiple lightweight models)\nYou could create a single training job, with instance count, the number of instances available to you.\nInside the training job, your code can run a for loop and perform all the individual training jobs you need.<\/p>\n<p>In this case, you will need to know which which instance is which so you can make the split of the HPOs. SageMaker writes this information on the file: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-dist-training\" rel=\"nofollow noreferrer\">\/opt\/ml\/input\/config\/resourceconfig.json<\/a> so using that you can easily have each instance run a subset of the trainings required.<\/p>\n<p>Another thing to think of, is if you need to save the generated models (which you probably need). You can either save everything in the output model directory - standard SM approach- but this would zip all models in a model.tar.gz file.\nIf you don't want this, and prefer to have each model individually saved, I'd suggest using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-checkpoints.html\" rel=\"nofollow noreferrer\">checkpoints<\/a> directory that will sync anything written there to your s3 location.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71948090",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1650614502720,
        "Question_original_content":"best wai run train job thousand train job want run basic list hyperparamet want train model hyperparmet parallel standard hyperparamet tune want optim hyperparamet want train hyperparamet search doc extens surpris couldn info like pretti basic function exampl let train job quota instanc best wai run job util avail instanc particular queue manag function take list hyperparamet run train job batch better track fail complet job best practic run singl train job instanc case need ask higher quota number instanc function exist worth instead bit cheaper",
        "Question_preprocessed_content":"best wai run train job thousand train job want run basic list hyperparamet want train model hyperparmet parallel search doc extens surpris couldn info like pretti basic function exampl let train job quota instanc best wai run job util avail instanc particular queue manag function take list hyperparamet run train job batch best practic run singl train job instanc case need ask higher quota number instanc function exist worth instead bit cheaper",
        "Question_gpt_summary_original":"The user is facing challenges in running thousands of training jobs on sagemaker. They are looking for a way to train the model for all hyperparameters in parallel and utilize all available instances. The user is seeking information on whether there is a queue manager functionality that can run training jobs in batches, whether it is best practice to run a single training job per instance, and whether it is worth using EC2 instead of sagemaker.",
        "Question_gpt_summary":"user face challeng run thousand train job look wai train model hyperparamet parallel util avail instanc user seek inform queue manag function run train job batch best practic run singl train job instanc worth instead",
        "Answer_original_content":"question broad best wai forward depend detail us case assumpt queue manag queue manag end decid need queue manag suggest look aw batch singl multipl train job need run thousand job assum train fairli lightweight model save time better reus instanc multipl train job instanc limit need round train min start time depend instanc type need hour wait time depend complex individu model hour signific total accept instanc limit increas ask limit increas go limit like accept aw support team organis track record usag aw case fine possibl option assum multipl lightweight model creat singl train job instanc count number instanc avail insid train job code run loop perform individu train job need case need know instanc split hpo write inform file opt input config resourceconfig json easili instanc run subset train requir thing think need save gener model probabl need save output model directori standard approach zip model model tar file want prefer model individu save suggest checkpoint directori sync written locat",
        "Answer_preprocessed_content":"question broad best wai forward depend detail assumpt queue manag queue manag end decid need queue manag suggest look aw batch singl multipl train job need run thousand job assum train fairli lightweight model save time better reus instanc multipl train job instanc limit increas ask limit increas go limit like accept aw support team organis track record usag aw case fine possibl option creat singl train job instanc count number instanc avail insid train job code run loop perform individu train job need case need know instanc split hpo write inform file easili instanc run subset train requir thing think need save gener model save output model directori standard approach zip model file want prefer model individu save suggest checkpoint directori sync written locat",
        "Answer_gpt_summary_original":"Solutions provided in the discussion are:\n\n- SageMaker does not have a queue manager. AWS Batch can be used if a queue manager is needed.\n- Reusing instances for multiple training jobs is better to save time.\n- Instance limit increase can be requested, but it may not be accepted.\n- A single training job can be created with the instance count equal to the number of instances available. Inside the training job, a for loop can be used to perform all the individual training jobs needed.\n- The information on which instance is which can be found in the file \/opt\/ml\/input\/config\/resourceconfig.json.\n- Checkpoints directory can be used to save each model individually instead of saving all models in a model.tar.gz file.",
        "Answer_gpt_summary":"solut provid discuss queue manag aw batch queue manag need reus instanc multipl train job better save time instanc limit increas request accept singl train job creat instanc count equal number instanc avail insid train job loop perform individu train job need inform instanc file opt input config resourceconfig json checkpoint directori save model individu instead save model model tar file"
    },
    {
        "Question_title":"reading hdf5 file from s3 to sagemaker, is the whole file transferred?",
        "Question_body":"<p>I'm reading a file from my S3 bucket in a notebook in sagemaker studio (same account) using the following code:<\/p>\n<pre><code>dataset_path_in_h5=&quot;\/Mode1\/SingleFault\/SimulationCompleted\/IDV2\/Mode1_IDVInfo_2_100\/Run1\/processdata&quot;\ns3 = s3fs.S3FileSystem()\nh5_file = h5py.File(s3.open(s3url,'rb'), 'r')\ndata = h5_file.get(dataset_path_in_h5)\n<\/code><\/pre>\n<p>But I don't know what actually append behind the scene, does the whole h5 file is being transferred  ? that's seems unlikely as the code is executed quite fast while the whole file is 20GB. Or is just the dataset in dataset_path_in_h5 is transferred ?\nI suppose that if the whole file is transferred at each call it could cost me a lot.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662025046283,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":18.0,
        "Answer_body":"<p>When you open the file, a file object is created. It has a tiny memory footprint. The dataset values aren't read into memory until you access them.<\/p>\n<p>You are returning <code>data<\/code> as a NumPy array. That loads the entire dataset into memory. (NOTE: the <code>.get()<\/code> method you are using is deprecated. Current syntax is provided in the example.)<\/p>\n<p>As an alternative to returning an array, you can create a dataset object (which also has a small memory foorprint). When you do, the data is read into memory as you need it. Dataset objects behave like NumPy arrays. (Use of a dataset object vs NumPy array depends on downstream usage. Frequently you don't need an array, but sometimes they are required.) Also, if chunked I\/O was enabled when the dataset was created, datasets are read in chunks.<\/p>\n<p>Differences shown below. Note, I used Python's file context manager to open the file. It avoids problems if the file isn't closed properly (you forget or the program exits prematurely).<\/p>\n<pre><code>dataset_path_in_h5=&quot;\/Mode1\/SingleFault\/SimulationCompleted\/IDV2\/Mode1_IDVInfo_2_100\/Run1\/processdata&quot;\ns3 = s3fs.S3FileSystem()\nwith h5py.File(s3.open(s3url,'rb'), 'r') as h5_file:\n     # your way to get a numpy array -- .get() is depreciated:\n     data = h5_file.get(dataset_path_in_h5)\n     # this is the preferred syntax to return an array:\n     data_arr = h5_file[dataset_path_in_h5][()]\n     # this returns a h5py dataset object:\n     data_ds = h5_file[dataset_path_in_h5]  # deleted [()] \n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73567221",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1662041978820,
        "Question_original_content":"read hdf file file transfer read file bucket notebook studio account follow code dataset path mode singlefault simulationcomplet idv mode idvinfo run processdata sf sfilesystem file hpy file open surl data file dataset path know actual append scene file transfer unlik code execut fast file dataset dataset path transfer suppos file transfer cost lot",
        "Question_preprocessed_content":"read hdf file file transfer read file bucket notebook studio follow code know actual append scene file transfer unlik code execut fast file dataset transfer suppos file transfer cost lot",
        "Question_gpt_summary_original":"The user is facing a challenge in determining whether the entire 20GB HDF5 file is being transferred when reading a specific dataset from their S3 bucket in Sagemaker Studio using Python code. They are concerned about the potential cost implications if the entire file is being transferred each time the code is executed.",
        "Question_gpt_summary":"user face challeng determin entir hdf file transfer read specif dataset bucket studio python code concern potenti cost implic entir file transfer time code execut",
        "Answer_original_content":"open file file object creat tini memori footprint dataset valu aren read memori access return data numpi arrai load entir dataset memori note method deprec current syntax provid exampl altern return arrai creat dataset object small memori foorprint data read memori need dataset object behav like numpi arrai us dataset object numpi arrai depend downstream usag frequent need arrai requir chunk enabl dataset creat dataset read chunk differ shown note python file context manag open file avoid problem file isn close properli forget program exit prematur dataset path mode singlefault simulationcomplet idv mode idvinfo run processdata sf sfilesystem hpy file open surl file wai numpi arrai depreci data file dataset path prefer syntax return arrai data arr file dataset path return hpy dataset object data file dataset path delet",
        "Answer_preprocessed_content":"open file file object creat tini memori footprint dataset valu aren read memori access return numpi arrai load entir dataset memori altern return arrai creat dataset object data read memori need dataset object behav like numpi arrai chunk enabl dataset creat dataset read chunk differ shown note python file context manag open file avoid problem file isn close properli",
        "Answer_gpt_summary_original":"Solution:\n- Instead of returning a NumPy array, create a dataset object which has a small memory footprint and reads data into memory as needed.\n- Use of a dataset object vs NumPy array depends on downstream usage. Frequently you don't need an array, but sometimes they are required.\n- If chunked I\/O was enabled when the dataset was created, datasets are read in chunks.",
        "Answer_gpt_summary":"solut instead return numpi arrai creat dataset object small memori footprint read data memori need us dataset object numpi arrai depend downstream usag frequent need arrai requir chunk enabl dataset creat dataset read chunk"
    },
    {
        "Question_title":"Should I run forecast predictive model with AWS lambda or sagemaker?",
        "Question_body":"<p>I've been reading some articles regarding this topic and have preliminary thoughts as what I should do with it, but still want to see if anyone can share comments if you have more experience with running machine learning on AWS. I was doing a project for a professor at school, and we decided to use AWS. I need to find a cost-effective and efficient way to deploy a forecasting model on it. <\/p>\n\n<p>What we want to achieve is:<\/p>\n\n<ul>\n<li>read the data from S3 bucket monthly (there will be new data coming in every month), <\/li>\n<li>run a few python files (.py) for custom-built packages and install dependencies (including the files, no more than 30kb), <\/li>\n<li>produce predicted results into a file back in S3 (JSON or CSV works), or push to other endpoints (most likely to be some BI tools - tableau etc.) - but really this step can be flexible (not web for sure) <\/li>\n<\/ul>\n\n<p><strong>First thought I have is AWS sagemaker<\/strong>. However, we'll be using \"fb prophet\" model to predict the results, and we built a customized package to use in the model, therefore, I don't think the notebook instance is gonna help us. (Please correct me if I'm wrong) My understanding is that sagemaker is a environment to build and train the model, but we already built and trained the model. Plus, we won't be using AWS pre-built models anyways.<\/p>\n\n<p>Another thing is if we want to use custom-built package, we will need to create container image, and I've never done that before, not sure about the efforts to do that.<\/p>\n\n<p><strong>2nd option is to create multiple lambda functions<\/strong><\/p>\n\n<ul>\n<li><p>one that triggers to run the python scripts from S3 bucket (2-3 .py files) every time a new file is imported into S3 bucket, which will happen monthly.<\/p><\/li>\n<li><p>one that trigger after the python scripts are done running and produce results and save into S3 bucket.<\/p><\/li>\n<\/ul>\n\n<p>3rd option will combine both options:\n - Use lambda function to trigger the implementation on the python scripts in S3 bucket when the new file comes in.\n - Push the result using sagemaker endpoint, which means we host the model on sagemaker and deploy from there.<\/p>\n\n<p>I am still not entirely sure how to put pre-built model and python scripts onto sagemaker instance and host from there.<\/p>\n\n<p>I'm hoping whoever has more experience with AWS service can help give me some guidance, in terms of more cost-effective and efficient way to run model.<\/p>\n\n<p>Thank you!! <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1586306942937,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":2675.0,
        "Answer_body":"<p>I would say it all depends on how heavy your model is \/ how much data you're running through it. You're right to identify that Lambda will likely be less work. It's quite easy to get a lambda up and running to do the things that you need, and <a href=\"https:\/\/aws.amazon.com\/lambda\/pricing\/\" rel=\"nofollow noreferrer\">Lambda has a very generous free tier<\/a>. The problem is:<\/p>\n\n<ol>\n<li><p>Lambda functions are fundamentally limited in their processing capacity (they timeout after <em>max<\/em> 15 minutes).<\/p><\/li>\n<li><p>Your model might be expensive to load.<\/p><\/li>\n<\/ol>\n\n<p>If you have a lot of data to run through your model, you will need multiple lambdas. Multiple lambdas means you have to load your model multiple times, and that's wasted work. If you're working with \"big data\" this will get expensive once you get through the free tier.<\/p>\n\n<p>If you don't have much data, Lambda will work just fine. I would eyeball it as follows: assuming your data processing step is dominated by your model step, and if all your model interactions (loading the model + evaluating all your data) take less than 15min, you're definitely fine. If they take more, you'll need to do a back-of-the-envelope calculation to figure out whether you'd leave the Lambda free tier.<\/p>\n\n<p>Regarding Lambda: You can literally copy-paste code in to setup a prototype. If your execution takes more than 15min for all your data, you'll need a method of splitting your data up between multiple Lambdas. Consider <a href=\"https:\/\/aws.amazon.com\/step-functions\/\" rel=\"nofollow noreferrer\">Step Functions<\/a> for this.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61091659",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1586317398727,
        "Question_original_content":"run forecast predict model aw lambda read articl topic preliminari thought want share comment experi run machin learn aw project professor school decid us aw need cost effect effici wai deploi forecast model want achiev read data bucket monthli new data come month run python file custom built packag instal depend includ file produc predict result file json csv work push endpoint like tool tableau step flexibl web sure thought prophet model predict result built custom packag us model think notebook instanc gonna help correct wrong understand environ build train model built train model plu won aw pre built model anywai thing want us custom built packag need creat contain imag sure effort option creat multipl lambda function trigger run python script bucket file time new file import bucket happen monthli trigger python script run produc result save bucket option combin option us lambda function trigger implement python script bucket new file come push result endpoint mean host model deploi entir sure pre built model python script instanc host hope experi aw servic help guidanc term cost effect effici wai run model thank",
        "Question_preprocessed_content":"run forecast predict model aw lambda read articl topic preliminari thought want share comment experi run machin learn aw project professor school decid us aw need effici wai deploi forecast model want achiev read data bucket monthli run python file packag instal depend produc predict result file push endpoint step flexibl thought prophet model predict result built custom packag us model think notebook instanc gonna help understand environ build train model built train model plu won aw model anywai thing want us packag need creat contain imag sure effort option creat multipl lambda function trigger run python script bucket time new file import bucket happen monthli trigger python script run produc result save bucket option combin option us lambda function trigger implement python script bucket new file come push result endpoint mean host model deploi entir sure model python script instanc host hope experi aw servic help guidanc term effici wai run model thank",
        "Question_gpt_summary_original":"The user is trying to find a cost-effective and efficient way to deploy a forecasting model on AWS. They are considering using AWS Sagemaker, but they have a customized package and won't be using AWS pre-built models. They are also unsure about creating a container image. Another option they are considering is creating multiple lambda functions to trigger the implementation of python scripts in S3 bucket and push the result using Sagemaker endpoint. The user is seeking guidance on the most cost-effective and efficient way to run the model.",
        "Question_gpt_summary":"user try cost effect effici wai deploi forecast model aw consid custom packag won aw pre built model unsur creat contain imag option consid creat multipl lambda function trigger implement python script bucket push result endpoint user seek guidanc cost effect effici wai run model",
        "Answer_original_content":"depend heavi model data run right identifi lambda like work easi lambda run thing need lambda gener free tier problem lambda function fundament limit process capac timeout max minut model expens load lot data run model need multipl lambda multipl lambda mean load model multipl time wast work work big data expens free tier data lambda work fine eyebal follow assum data process step domin model step model interact load model evalu data min definit fine need envelop calcul figur leav lambda free tier lambda liter copi past code setup prototyp execut take min data need method split data multipl lambda consid step function",
        "Answer_preprocessed_content":"depend heavi model data run right identifi lambda like work easi lambda run thing need lambda gener free tier problem lambda function fundament limit process capac model expens load lot data run model need multipl lambda multipl lambda mean load model multipl time wast work work big data expens free tier data lambda work fine eyebal follow assum data process step domin model step model interact min definit fine need calcul figur leav lambda free tier lambda liter code setup prototyp execut take min data need method split data multipl lambda consid step function",
        "Answer_gpt_summary_original":"The discussion suggests that using AWS Lambda could be a cost-effective solution for deploying the forecasting model, but it has limitations in processing capacity and may require multiple lambdas if there is a lot of data to process. If the model interactions take less than 15 minutes, Lambda will work fine. However, if it takes more than 15 minutes, the user will need to split the data between multiple Lambdas using Step Functions. No solution is provided for creating a container image.",
        "Answer_gpt_summary":"discuss suggest aw lambda cost effect solut deploi forecast model limit process capac requir multipl lambda lot data process model interact minut lambda work fine take minut user need split data multipl lambda step function solut provid creat contain imag"
    },
    {
        "Question_title":"mlflow.exceptions.MlflowException: Changing param values is not allowed. Param with key='input_rows' was already logged with value='32205'",
        "Question_body":"<p>I am using Mlflow as a work orchestration tool. I have a Machine Learning pipeline. In this pipeline, I have real-time data. I'm listening this data with Apache Kafka. Also, I'm doing this: Whenever 250 message comes to this topic, I'm gathering them, and I'm appending this message my previous data. After that, my training function is triggered. Thus, I am able to making new training in every 250 new data. With Mlflow, I can show the results, metrics and any other parameters of trained models. But After training occurred one time, the second one doesn't occurs, and It throws me this error which I have shown in title. Here it is my consumer:<\/p>\n<pre><code>topic_name = 'twitterdata'\ntrain_every = 250\n\n\ndef consume_tweets():\n    consumer = KafkaConsumer(\n        topic_name,\n        bootstrap_servers=['localhost:9093'],\n        auto_offset_reset='latest',\n        enable_auto_commit=True,\n        auto_commit_interval_ms=5000,\n        fetch_max_bytes=128,\n        max_poll_records=100,\n        value_deserializer=lambda x: json.loads(x.decode('utf-8')))\n\n    tweet_counter = 0\n    for message in consumer:\n        tweets = json.loads(json.dumps(message.value))\n        # print(tweets['text'])\n        tweet_sentiment = make_prediction(tweets['text'])\n\n        if tweet_counter == train_every:\n            update_df()\n            data_path = 'data\/updated_tweets.csv'\n            train(data_path)\n            print(&quot;\\nTraining with new data is completed!\\n&quot;)\n            tweet_counter = 0\n\n        else:\n            tweet_counter += 1\n\n        publish_prediction(tweet_sentiment, tweets['text'])\n\n<\/code><\/pre>\n<p>And here it is my train.py:<\/p>\n<pre><code>train_tweets = pd.read_csv(DATA_PATH)\n    # train_tweets = train_tweets[:20000]\n\n    tweets = train_tweets.tweet.values\n    labels = train_tweets.label.values\n\n    # Log data params\n    mlflow.log_param('input_rows', train_tweets.shape[0])\n\n    # Do preprocessing and return vectorizer with it\n    vectorizer, processed_features = embedding(tweets)\n\n    # Saving vectorizer\n    save_vectorizer(vectorizer)\n\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)\n\n    # Handle imbalanced data by using 'Smote' and log to Mlflow\n    smote = SMOTE('minority')\n    mlflow.log_param(&quot;over-sampling&quot;, smote)\n\n    X_train, y_train = smote.fit_sample(X_train, y_train)\n\n    # text_classifier = MultinomialNB()\n    text_classifier = LogisticRegression(max_iter=10000)\n    text_classifier.fit(X_train, y_train)\n    predictions = text_classifier.predict(X_test)\n\n    # Model metrics\n    (rmse, mae, r2) = eval_metrics(y_test, predictions)\n\n    mlflow.log_param('os-row-Xtrain', X_train.shape[0])\n    mlflow.log_param('os-row-ytrain', y_train.shape[0])\n    mlflow.log_param(&quot;model_name&quot;, text_classifier)\n    mlflow.log_metric(&quot;rmse&quot;, rmse)\n    mlflow.log_metric(&quot;r2&quot;, r2)\n    mlflow.log_metric(&quot;mae&quot;, mae)\n    mlflow.log_metric('acc_score', accuracy_score(y_test, predictions))\n\n    mlflow.sklearn.log_model(text_classifier, &quot;model&quot;)\n<\/code><\/pre>\n<p>I couldn't solve the problem. MLflow is one of the newest tool, so issues and examples of Mlflow are very few.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1614014592930,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1614015098323,
        "Question_score":1.0,
        "Question_view_count":2716.0,
        "Answer_body":"<p>I think you need an MLflow &quot;run&quot; for every new batch of data, so that your parameters are logged independently for each new training.<\/p>\n<p>So, try the following in your consumer:<\/p>\n<pre><code>if tweet_counter == train_every:\n            update_df()\n            data_path = 'data\/updated_tweets.csv'\n            with mlflow.start_run() as mlrun:\n               train(data_path)\n            print(&quot;\\nTraining with new data is completed!\\n&quot;)\n            tweet_counter = 0\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66320435",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1614350710747,
        "Question_original_content":"except except chang param valu allow param kei input row log valu work orchestr tool machin learn pipelin pipelin real time data listen data apach kafka messag come topic gather append messag previou data train function trigger abl make new train new data result metric paramet train model train occur time second occur throw error shown titl consum topic twitterdata train def consum tweet consum kafkaconsum topic bootstrap server localhost auto offset reset latest enabl auto commit true auto commit interv fetch max byte max poll record valu deseri lambda json load decod utf tweet counter messag consum tweet json load json dump messag valu print tweet text tweet sentiment predict tweet text tweet counter train updat data path data updat tweet csv train data path print ntrain new data complet tweet counter tweet counter publish predict tweet sentiment tweet text train train tweet read csv data path train tweet train tweet tweet train tweet tweet valu label train tweet label valu log data param log param input row train tweet shape preprocess return vector vector process featur embed tweet save vector save vector vector split data train test train test train test split process featur label test size random state handl imbalanc data smote log smote smote minor log param sampl smote train train smote fit sampl train train text classifi multinomialnb text classifi logisticregress max iter text classifi fit train train predict text classifi predict test model metric rmse mae eval metric test predict log param row xtrain train shape log param row ytrain train shape log param model text classifi log metric rmse rmse log metric log metric mae mae log metric acc score accuraci score test predict sklearn log model text classifi model couldn solv problem newest tool issu exampl",
        "Question_preprocessed_content":"chang param valu allow param log valu work orchestr tool machin learn pipelin pipelin data listen data apach kafka messag come topic gather append messag previou data train function trigger abl make new train new data result metric paramet train model train occur time second occur throw error shown titl consum couldn solv problem newest tool issu exampl",
        "Question_gpt_summary_original":"The user is encountering an error while using Mlflow as a work orchestration tool for a machine learning pipeline. The pipeline involves real-time data gathered from Apache Kafka, and the training function is triggered every time 250 new data is received. However, after the first training, the second one does not occur, and the error \"mlflow.exceptions.MlflowException: Changing param values is not allowed. Param with key='input_rows' was already logged with value='32205'\" is thrown. The user has provided the code for the consumer and train.py but has been unable to solve the problem.",
        "Question_gpt_summary":"user encount error work orchestr tool machin learn pipelin pipelin involv real time data gather apach kafka train function trigger time new data receiv train second occur error except except chang param valu allow param kei input row log valu thrown user provid code consum train unabl solv problem",
        "Answer_original_content":"think need run new batch data paramet log independ new train try follow consum tweet counter train updat data path data updat tweet csv start run mlrun train data path print ntrain new data complet tweet counter",
        "Answer_preprocessed_content":"think need run new batch data paramet log independ new train try follow consum",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to create a new MLflow \"run\" for every new batch of data, so that the parameters are logged independently for each new training. The suggested code involves starting a new MLflow run for every 250 new data received and training the model with the updated data.",
        "Answer_gpt_summary":"solut possibl solut mention discuss creat new run new batch data paramet log independ new train suggest code involv start new run new data receiv train model updat data"
    },
    {
        "Question_title":"Limit on the rate of inferences one can make for a SageMaker endpoint",
        "Question_body":"<p>Is there a limit on the rate of inferences one can make for a SageMaker endpoint?<\/p>\n\n<p>Is it determined somehow by the instance type behind the endpoint or the number of instances?<\/p>\n\n<p>I tried looking for this info as <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws_service_limits.html#limits_sagemaker\" rel=\"nofollow noreferrer\">AWS Service Quotas for SageMaker<\/a> but couldn't find it.<\/p>\n\n<p>I am invoking the endpoint from a Spark job abd wondered if the number of concurrent tasks is a factor I should be taking care of when running inference (assuming each task runs one inference at a time) <\/p>\n\n<p>Here's the throttling error I got:<\/p>\n\n<pre><code>com.amazonaws.services.sagemakerruntime.model.AmazonSageMakerRuntimeException: null (Service: AmazonSageMakerRuntime; Status Code: 400; Error Code: ThrottlingException; Request ID: b515121b-f3d5-4057-a8a4-6716f0708980)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.doInvoke(AmazonSageMakerRuntimeClient.java:236)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.invoke(AmazonSageMakerRuntimeClient.java:212)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.executeInvokeEndpoint(AmazonSageMakerRuntimeClient.java:176)\n    at com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClient.invokeEndpoint(AmazonSageMakerRuntimeClient.java:151)\n    at lineefd06a2d143b4016906a6138a6ffec15194.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$a5cddfc4633c5dd8aa603ddc4f9aad5$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Predictor.predict(command-2334973:41)\n    at lineefd06a2d143b4016906a6138a6ffec15200.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$50a9225beeac265557e61f69d69d7d$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.apply(command-2307906:11)\n    at lineefd06a2d143b4016906a6138a6ffec15200.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$$$50a9225beeac265557e61f69d69d7d$$$$w$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.apply(command-2307906:11)\n    at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n    at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:2000)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1220)\n    at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1220)\n    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n    at org.apache.spark.scheduler.Task.run(Task.scala:113)\n    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1579709239420,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1579710725728,
        "Question_score":3.0,
        "Question_view_count":1716.0,
        "Answer_body":"<p>Amazon SageMaker is offering model hosting service (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-hosting.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-hosting.html<\/a>), which gives you a lot of flexibility based on your inference requirements. <\/p>\n\n<p>As you noted, first you can choose the instance type to use for your model hosting. The large set of options is important to tune to your models. You can host the model on a GPU based machines (P2\/P3\/P4) or CPU ones. You can have instances with faster CPU (C4, for example), or more RAM (R4, for example). You can also choose instances with more cores (16xl, for example) or less (medium, for example). Here is a list of the full range of instances that you can choose: <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/<\/a> . It is important to balance your performance and costs. The selection of the instance type and the type and size of your model will determine the invocations-per-second that you can expect from your model in this single-node configuration. It is important to measure this number to avoid hitting the throttle errors that you saw. <\/p>\n\n<p>The second important feature of the SageMaker hosting that you use is the ability to auto-scale your model to multiple instances. You can configure the endpoint of your model hosting to automatically add and remove instances based on the load on the endpoint. AWS is adding a load balancer in front of the multiple instances that are hosting your models and distributing the requests among them. Using the autoscaling functionality allows you to keep a smaller instance for low traffic hours, and to be able to scale up during peak traffic hours, and still keep your costs low and your throttle errors to the minimum. See here for documentation on the SageMaker autoscaling options: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling.html<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59863842",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1579951518923,
        "Question_original_content":"limit rate infer endpoint limit rate infer endpoint determin instanc type endpoint number instanc tri look info aw servic quota couldn invok endpoint spark job abd wonder number concurr task factor take care run infer assum task run infer time throttl error got com amazonaw servic runtim model amazonruntimeexcept null servic amazonruntim statu code error code throttlingexcept request com amazonaw http amazonhttpcli requestexecutor handleerrorrespons amazonhttpcli java com amazonaw http amazonhttpcli requestexecutor executeonerequest amazonhttpcli java com amazonaw http amazonhttpcli requestexecutor executehelp amazonhttpcli java com amazonaw http amazonhttpcli requestexecutor doexecut amazonhttpcli java com amazonaw http amazonhttpcli requestexecutor executewithtim amazonhttpcli java com amazonaw http amazonhttpcli requestexecutor execut amazonhttpcli java com amazonaw http amazonhttpcli requestexecutor access amazonhttpcli java com amazonaw http amazonhttpcli requestexecutionbuilderimpl execut amazonhttpcli java com amazonaw http amazonhttpcli execut amazonhttpcli java com amazonaw http amazonhttpcli execut amazonhttpcli java com amazonaw servic runtim amazonruntimecli doinvok amazonruntimecli java com amazonaw servic runtim amazonruntimecli invok amazonruntimecli java com amazonaw servic runtim amazonruntimecli executeinvokeendpoint amazonruntimecli java com amazonaw servic runtim amazonruntimecli invokeendpoint amazonruntimecli java lineefdadbaaffec read acddfccddaaddcfaad predictor predict command lineefdadbaaffec read abeeacefddd anonfun appli command lineefdadbaaffec read abeeacefddd anonfun appli command scala collect iter anon iter scala org apach spark util util getiterators util scala org apach spark rdd rdd anonfun count appli rdd scala org apach spark rdd rdd anonfun count appli rdd scala org apach spark sparkcontext anonfun runjob appli sparkcontext scala org apach spark sparkcontext anonfun runjob appli sparkcontext scala org apach spark schedul resulttask runtask resulttask scala org apach spark schedul task doruntask task scala org apach spark schedul task run task scala org apach spark executor executor taskrunn anonfun appli executor scala org apach spark util util trywithsafefin util scala org apach spark executor executor taskrunn run executor scala java util concurr threadpoolexecutor runwork threadpoolexecutor java java util concurr threadpoolexecutor worker run threadpoolexecutor java java lang thread run thread java",
        "Question_preprocessed_content":"limit rate infer endpoint limit rate infer endpoint determin instanc type endpoint number instanc tri look info aw servic quota couldn invok endpoint spark job abd wonder number concurr task factor take care run infer throttl error got",
        "Question_gpt_summary_original":"The user is encountering a throttling error while invoking an endpoint from a Spark job in SageMaker. The user is unsure if the limit on the rate of inferences is determined by the instance type or the number of instances, and could not find this information in AWS Service Quotas for SageMaker. The user is also unsure if the number of concurrent tasks is a factor that needs to be taken care of while running inference.",
        "Question_gpt_summary":"user encount throttl error invok endpoint spark job user unsur limit rate infer determin instanc type number instanc inform aw servic quota user unsur number concurr task factor need taken care run infer",
        "Answer_original_content":"offer model host servic http doc aw amazon com latest work host html give lot flexibl base infer requir note choos instanc type us model host larg set option import tune model host model gpu base machin cpu on instanc faster cpu exampl ram exampl choos instanc core exampl medium exampl list rang instanc choos http aw amazon com price instanc type import balanc perform cost select instanc type type size model determin invoc second expect model singl node configur import measur number avoid hit throttl error saw second import featur host us abil auto scale model multipl instanc configur endpoint model host automat add remov instanc base load endpoint aw ad load balanc multipl instanc host model distribut request autosc function allow smaller instanc low traffic hour abl scale peak traffic hour cost low throttl error minimum document autosc option http doc aw amazon com latest endpoint auto scale html",
        "Answer_preprocessed_content":"offer model host servic give lot flexibl base infer requir note choos instanc type us model host larg set option import tune model host model gpu base machin cpu on instanc faster cpu ram choos instanc core list rang instanc choos import balanc perform cost select instanc type type size model determin expect model configur import measur number avoid hit throttl error saw second import featur host us abil model multipl instanc configur endpoint model host automat add remov instanc base load endpoint aw ad load balanc multipl instanc host model distribut request autosc function allow smaller instanc low traffic hour abl scale peak traffic hour cost low throttl error minimum document autosc option",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Choose the appropriate instance type based on the model and performance requirements to avoid hitting throttle errors.\n2. Measure the invocations-per-second to determine the appropriate instance type and avoid throttle errors.\n3. Use the autoscaling functionality to add or remove instances based on the load on the endpoint to keep costs low and minimize throttle errors.\n\nNo solution was mentioned for the user's concern about the number of concurrent tasks.",
        "Answer_gpt_summary":"possibl solut mention discuss choos appropri instanc type base model perform requir avoid hit throttl error measur invoc second determin appropri instanc type avoid throttl error us autosc function add remov instanc base load endpoint cost low minim throttl error solut mention user concern number concurr task"
    },
    {
        "Question_title":"How to tell programmatically that an AWS Step Function execution has been completed?",
        "Question_body":"<p>I am triggering a Step Function execution via a Python cell in a SageMaker Notebook, like this:<\/p>\n<pre><code>state_machine_arn = 'arn:aws:states:us-west-1:1234567891:stateMachine:alexanderMyPackageStateMachineE3411O13-A1vQWERTP9q9'\nsfn = boto3.client('stepfunctions')\n..\nsfn.start_execution(**kwargs)  # Non Blocking Call\nrun_arn = response['executionArn']\nprint(f&quot;Started run {run_name}. ARN is {run_arn}.&quot;)\n<\/code><\/pre>\n<p>and then in order to check that the execution (which might take hours to complete depending on the input) has been completed, before I start doing some custom post-analysis on the results, I manually execute a cell with:<\/p>\n<pre><code>response = sfn.list_executions(\n    stateMachineArn=state_machine_arn,\n    maxResults=1\n)\nprint(response)\n<\/code><\/pre>\n<p>where I can see from the output the status of the execution, e.g. <code>'status': 'RUNNING'<\/code>.<\/p>\n<p>How can I automate this, i.e. trigger the Step Function and continue the execution on my post-analysis custom logic only after the execution has finished? Is there for example a blocking call to start the execution, or a callback method I could use?<\/p>\n<p>I can think of putting a sleep method, so that the Python Notebook cell would periodically call <code>list_executions()<\/code> and check the status, and only when the execution is completed, continue to rest of the code. I can statistically determine the sleep period, but I was wondering if there is a simpler\/more accurate way.<\/p>\n<hr \/>\n<p>PS: Related: <a href=\"https:\/\/stackoverflow.com\/questions\/46878423\/how-to-avoid-simultaneous-execution-in-aws-step-function\">How to avoid simultaneous execution in aws step function<\/a>, however I would like to avoid creating any new AWS resource, just for this, I would like to do everything from within the Notebook.<\/p>\n<p>PPS: I cannot make any change to <code>MyPackage<\/code> and the Step Function definition.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1618566783027,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1618585069787,
        "Question_score":4.0,
        "Question_view_count":1216.0,
        "Answer_body":"<p>Based on the comments.<\/p>\n<p>If no new resources are to be created (no CloudWatch Event rules, lambda functions) nor any changes to existing Step Function are allowed, then <strong>pooling iteratively<\/strong> <code>list_executions<\/code> would be the best solution.<\/p>\n<p>AWS CLI and boto3 have implemented similar solutions (not for Step Functions), but for some other services. They are called <code>waiters<\/code> (e.g. <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/ec2.html#waiters\" rel=\"nofollow noreferrer\">ec2 waiters<\/a>). So basically you would have to create your own <strong>waiter for Step Function<\/strong>, as AWS does not provide one for that. AWS uses <strong>15 seconds<\/strong> sleep time from what I recall for its waiters.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67123040",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1618568305232,
        "Question_original_content":"tell programmat aw step function execut complet trigger step function execut python cell notebook like state machin arn arn aw state west statemachin alexandermypackagestatemachineeo avqwertpq sfn boto client stepfunct sfn start execut kwarg non block run arn respons executionarn print start run run arn run arn order check execut hour complet depend input complet start custom post analysi result manual execut cell respons sfn list execut statemachinearn state machin arn maxresult print respons output statu execut statu run autom trigger step function continu execut post analysi custom logic execut finish exampl block start execut callback method us think put sleep method python notebook cell period list execut check statu execut complet continu rest code statist determin sleep period wonder simpler accur wai relat avoid simultan execut aw step function like avoid creat new aw resourc like notebook pp chang mypackag step function definit",
        "Question_preprocessed_content":"tell programmat aw step function execut complet trigger step function execut python cell notebook like order check execut complet start custom result manual execut cell output statu execut autom trigger step function continu execut custom logic execut finish exampl block start execut callback method us think put sleep method python notebook cell period check statu execut complet continu rest code statist determin sleep period wonder accur wai relat avoid simultan execut aw step function like avoid creat new aw resourc like notebook pp chang step function definit",
        "Question_gpt_summary_original":"The user is triggering a Step Function execution via a Python cell in a SageMaker Notebook and wants to automate the process of checking if the execution has been completed before starting post-analysis custom logic. The user is currently manually executing a cell to check the status of the execution and is considering using a sleep method to periodically call list_executions() to check the status. The user wants to know if there is a simpler\/more accurate way to automate this process without creating any new AWS resource.",
        "Question_gpt_summary":"user trigger step function execut python cell notebook want autom process check execut complet start post analysi custom logic user current manual execut cell check statu execut consid sleep method period list execut check statu user want know simpler accur wai autom process creat new aw resourc",
        "Answer_original_content":"base comment new resourc creat cloudwatch event rule lambda function chang exist step function allow pool iter list execut best solut aw cli boto implement similar solut step function servic call waiter waiter basic creat waiter step function aw provid aw us second sleep time recal waiter",
        "Answer_preprocessed_content":"base comment new resourc creat chang exist step function allow pool iter best solut aw cli boto implement similar solut servic call basic creat waiter step function aw provid aw us second sleep time recal waiter",
        "Answer_gpt_summary_original":"The suggested solution in the discussion is to use pooling iteratively with the list_executions() method to check the status of the Step Function execution. Another solution mentioned is to create a custom waiter for Step Function, similar to the waiters implemented in AWS CLI and boto3 for other services. However, this would require creating a new resource. No simpler or more accurate solution is provided without creating any new AWS resource.",
        "Answer_gpt_summary":"suggest solut discuss us pool iter list execut method check statu step function execut solut mention creat custom waiter step function similar waiter implement aw cli boto servic requir creat new resourc simpler accur solut provid creat new aw resourc"
    },
    {
        "Question_title":"Delete a run in the experiment of mlflow from the UI so the run does not exist in backend store",
        "Question_body":"<p>I found deleting a <code>run<\/code> only change the state from <code>active<\/code> to <code>deleted<\/code>, because the run is still visible in the UI if searching by <code>deleted<\/code>. <\/p>\n\n<p>Is it possible to remove a <code>run<\/code> from the UI to save the space? \nWhen removing a run, does the artifact correspond to the run is also removed?<\/p>\n\n<p>If not, can the run be removed through rest call?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1568794247973,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1568796468592,
        "Question_score":6.0,
        "Question_view_count":3582.0,
        "Answer_body":"<p>You can't do it via the web UI but you can from a python terminal<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmlflow.delete_experiment(69)\n<\/code><\/pre>\n\n<p>Where 69 is the experiment ID<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57987999",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1569793174476,
        "Question_original_content":"delet run experi run exist backend store delet run chang state activ delet run visibl search delet possibl remov run save space remov run artifact correspond run remov run remov rest",
        "Question_preprocessed_content":"delet run experi run exist backend store delet chang state run visibl search possibl remov save space remov run artifact correspond run remov run remov rest",
        "Question_gpt_summary_original":"The user is facing challenges in deleting a run in the mlflow experiment from the UI. They have found that deleting a run only changes its state from active to deleted, but it is still visible in the UI if searched by deleted. The user is wondering if it is possible to remove a run from the UI to save space and if the artifact corresponding to the run is also removed. They are also asking if the run can be removed through a rest call.",
        "Question_gpt_summary":"user face challeng delet run experi delet run chang state activ delet visibl search delet user wonder possibl remov run save space artifact correspond run remov ask run remov rest",
        "Answer_original_content":"web python termin import delet experi experi",
        "Answer_preprocessed_content":"web python termin experi",
        "Answer_gpt_summary_original":"Solution: The user cannot delete a run from the UI, but they can delete it using a Python terminal by importing mlflow and using the mlflow.delete_experiment() function with the experiment ID as the argument. This will remove the run from the UI and delete the corresponding artifact as well.",
        "Answer_gpt_summary":"solut user delet run delet python termin import delet experi function experi argument remov run delet correspond artifact"
    },
    {
        "Question_title":"How do handle overlapping segmentation masks?",
        "Question_body":"<p>In maskrcnn I\u2019m pretty sure it\u2019s valid to output overlapping segmentation masks, but I\u2019m not sure how to handle this in wandb Masks and I haven\u2019t been able to find an example. The documentation says each \u201cMask\u201d is a 2D numpy array filled with class ids, which implies one class per pixel, right?<\/p>\n<p>Is there a way to visualize the masks if they\u2019re overlapping?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1675707126695,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":33.0,
        "Answer_body":"<p>I was told by wandb support this was not possible, posting here for transparency<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-do-handle-overlapping-segmentation-masks\/3836",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-06T18:56:27.252Z",
                "Answer_body":"<p>I was told by wandb support this was not possible, posting here for transparency<\/p>",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1675709787252,
        "Question_original_content":"handl overlap segment mask maskrcnn pretti sure valid output overlap segment mask sure handl mask havent abl exampl document sai mask numpi arrai fill class id impli class pixel right wai visual mask theyr overlap",
        "Question_preprocessed_content":"handl overlap segment mask maskrcnn pretti sure valid output overlap segment mask sure handl mask havent abl exampl document sai mask numpi arrai fill class id impli class pixel right wai visual mask theyr overlap",
        "Question_gpt_summary_original":"The user is facing challenges in handling overlapping segmentation masks in maskrcnn and is unsure how to handle this in wandb Masks. The user is also seeking guidance on whether it is possible to visualize the masks if they are overlapping.",
        "Question_gpt_summary":"user face challeng handl overlap segment mask maskrcnn unsur handl mask user seek guidanc possibl visual mask overlap",
        "Answer_original_content":"told support possibl post transpar",
        "Answer_preprocessed_content":"told support possibl post transpar",
        "Answer_gpt_summary_original":"Solution: No solution was mentioned in the discussion.",
        "Answer_gpt_summary":"solut solut mention discuss"
    },
    {
        "Question_title":"git-ignore dvc.lock in repositories where only the DVC pipelines are used",
        "Question_body":"<p>I want to use the pipeline functionality of dvc in a git repository. The data is managed otherwise and should not be versioned by dvc. The only functionality which is needed is that dvc reproduces the needed steps of the pipeline when <code>dvc repro<\/code> is called. Checking out the repository on a new system should lead to an 'empty' repository, where none of the pipeline steps are stored.<\/p>\n<p>Thus, - if I understand correctly - there is no need to track the dvc.lock file in the repository. However, adding dvc.lock to the .gitginore file leads to an error message:<\/p>\n<pre><code>ERROR: 'dvc.lock' is git-ignored.\n<\/code><\/pre>\n<p>Is there any way to disable the dvc.lock in .gitignore check for this usecase?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624362030233,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":493.0,
        "Answer_body":"<p>This is definitely possible, as DVC features are loosely coupled to one another. You can do pipelining by writing your dvc.yaml file(s), but avoid data management\/versioning by using <code>cache: false<\/code> in the stage outputs (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#output-subfields\" rel=\"nofollow noreferrer\"><code>outs<\/code> field<\/a>). See also helper <code>dvc stage add -O<\/code> (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/stage\/add#options\" rel=\"nofollow noreferrer\">big O<\/a>, alias of <code>--outs-no-cache<\/code>).<\/p>\n<p>And the same for initial data dependencies, you can <code>dvc add --no-commit<\/code> them (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#options\" rel=\"nofollow noreferrer\">ref<\/a>).<\/p>\n<p>You do want to track <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#dvclock-file\" rel=\"nofollow noreferrer\">dvc.lock<\/a> in Git though, so that DVC can determine the latest stage of the pipeline associated with the Git commit in every repo copy or branch.<\/p>\n<p>You'll be responsible for placing the right data files\/dirs (matching .dvc files and dvc.lock) in the workspace for <code>dvc repro<\/code> or <code>dvc exp run<\/code> to behave as expected. <code>dvc checkout<\/code> won't be able to help you.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68082912",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1624393487732,
        "Question_original_content":"git ignor lock repositori pipelin want us pipelin function git repositori data manag version function need reproduc need step pipelin repro call check repositori new lead repositori pipelin step store understand correctli need track lock file repositori ad lock gitginor file lead error messag error lock git ignor wai disabl lock gitignor check usecas",
        "Question_preprocessed_content":"lock repositori pipelin want us pipelin function git repositori data manag version function need reproduc need step pipelin call check repositori new lead repositori pipelin step store understand correctli need track lock file repositori ad lock gitginor file lead error messag wai disabl lock gitignor check usecas",
        "Question_gpt_summary_original":"The user wants to use the pipeline functionality of dvc in a git repository without versioning the data by dvc. However, adding dvc.lock to the .gitignore file leads to an error message, and the user is looking for a way to disable the dvc.lock in .gitignore check for this use case.",
        "Question_gpt_summary":"user want us pipelin function git repositori version data ad lock gitignor file lead error messag user look wai disabl lock gitignor check us case",
        "Answer_original_content":"definit possibl featur loos coupl pipelin write yaml file avoid data manag version cach fals stage output out field helper stage add big alia out cach initi data depend add commit ref want track lock git determin latest stage pipelin associ git commit repo copi branch respons place right data file dir match file lock workspac repro exp run behav expect checkout won abl help",
        "Answer_preprocessed_content":"definit possibl featur loos coupl pipelin write yaml file avoid data stage output helper initi data depend want track lock git determin latest stage pipelin associ git commit repo copi branch respons place right data workspac behav expect won abl help",
        "Answer_gpt_summary_original":"Solution: The user can use the pipeline functionality of dvc in a git repository without versioning the data by dvc. They can write their dvc.yaml file(s) and avoid data management\/versioning by using <code>cache: false<\/code> in the stage outputs (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#output-subfields\" rel=\"nofollow noreferrer\"><code>outs<\/code> field<\/a>). They can also use <code>dvc add --no-commit<\/code> for initial data dependencies. However, they should track <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#dvclock-file\" rel=\"nofollow noreferrer\">dvc",
        "Answer_gpt_summary":"solut user us pipelin function git repositori version data write yaml file avoid data manag version cach fals stage output out field us add commit initi data depend track"
    },
    {
        "Question_title":"Is it possible to version files independently?",
        "Question_body":"<p>Hello DVC community!<\/p>\n<p>I have a tricky use case and I\u2019m not sure this is feasible with DVC.<br>\nMy project features several datasets that I would like to version independently. I would like to keep track of:<\/p>\n<ul>\n<li>dataset 1 with its own set of tags<\/li>\n<li>datset 2 with its own set of tags<\/li>\n<li>dataset 3 with its own set of tags<br>\nso that I can execute a script with any combination of the versions of the three datasets (ex: dataset 1 v1, dataset 2 v2, dataset 3 v3 or dataset 1 v3, dataset 2 v2, dataset 3 v3, etc.).<\/li>\n<\/ul>\n<p>Is this use case feasible with DVC?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1675461764980,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":25.0,
        "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/hugoehlinger\">@hugoehlinger<\/a> !<br>\nDVC can\u2019t easily handle independently-versioned datasets within the same project. I guess the best way would be to have a data preparation stage that has the dataset versions as params and pulls the right combination of data from elsewhere (possibly auxiliary DVC repos containing only one dataset each).<\/p>. <p>Hello <a class=\"mention\" href=\"\/u\/ronan\">@ronan<\/a> ! Thanks for answering me and giving me this tip<br>\nBest,<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/is-it-possible-to-version-files-independently\/1498",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-07T12:31:21.767Z",
                "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/hugoehlinger\">@hugoehlinger<\/a> !<br>\nDVC can\u2019t easily handle independently-versioned datasets within the same project. I guess the best way would be to have a data preparation stage that has the dataset versions as params and pulls the right combination of data from elsewhere (possibly auxiliary DVC repos containing only one dataset each).<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-15T21:18:58.881Z",
                "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/ronan\">@ronan<\/a> ! Thanks for answering me and giving me this tip<br>\nBest,<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"possibl version file independ hello commun tricki us case sure feasibl project featur dataset like version independ like track dataset set tag datset set tag dataset set tag execut script combin version dataset dataset dataset dataset dataset dataset dataset us case feasibl",
        "Question_preprocessed_content":"possibl version file independ hello commun tricki us case sure feasibl project featur dataset like version independ like track dataset set tag datset set tag dataset set tag execut script combin version dataset us case feasibl",
        "Question_gpt_summary_original":"The user is facing a challenge of versioning multiple datasets independently in their project using DVC. They want to keep track of each dataset with its own set of tags so that they can execute a script with any combination of the versions of the three datasets. The user is unsure if this use case is feasible with DVC.",
        "Question_gpt_summary":"user face challeng version multipl dataset independ project want track dataset set tag execut script combin version dataset user unsur us case feasibl",
        "Answer_original_content":"hello hugoehling easili handl independ version dataset project guess best wai data prepar stage dataset version param pull right combin data possibl auxiliari repo contain dataset hello ronan thank answer give tip best",
        "Answer_preprocessed_content":"hello easili handl dataset project guess best wai data prepar stage dataset version param pull right combin data hello thank answer give tip best",
        "Answer_gpt_summary_original":"Solution: One solution suggested in the discussion is to have a data preparation stage that has the dataset versions as parameters and pulls the right combination of data from elsewhere, possibly auxiliary DVC repositories containing only one dataset each. This way, the user can keep track of each dataset with its own set of tags and execute a script with any combination of the versions of the three datasets.",
        "Answer_gpt_summary":"solut solut suggest discuss data prepar stage dataset version paramet pull right combin data possibl auxiliari repositori contain dataset wai user track dataset set tag execut script combin version dataset"
    },
    {
        "Question_title":"AzureML multiple models stored in dictionary",
        "Question_body":"<p><strong>Overview:<\/strong><\/p>\n\n<p>I have a unique Python model where we hold n trained random forest models in a dictionary. I tried to avoid this setup, but for the time being it's necessary. On my local, I can make predictions by passing a dataframe to a predict function and looping through the rows, calling the appropriate model for each row, like rf_models[model].predict().<\/p>\n\n<p>In AzureML I created a toy model that allows me to go:\nWeb Input -> Python Script -> Score Model -> Web output. <\/p>\n\n<p><strong>Challenge:<\/strong><\/p>\n\n<p>I need to be able to call the score_model, or specifically the predict method, from inside the \"Python Script\" function on AzureML so I can deal with the loops and n models stored in the dict. The results, either a JSON or dataframe, would be sent to AzureML's Web Output.<\/p>\n\n<p>I found a link online (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-execute-python-scripts\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-execute-python-scripts<\/a>) that got me close, but the example shows a model being trained and used to predict at the same time inside the same Python script, thus calling the predict method as a local variable and not calling a previously trained model. I found only limited documentation online to solve this problem and I could not get the rest of the way there. I'm unsure if this type of customization is not yet available or if I'm completely overlooking some key functionality.<\/p>\n\n<p>Thank you for your assistance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1502218686207,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":200.0,
        "Answer_body":"<p>Here are two links that might help:<\/p>\n\n<ol>\n<li><a href=\"https:\/\/github.com\/Azure\/Machine-Learning-Operationalization\" rel=\"nofollow noreferrer\">AzureML Operationalization<\/a> <\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/Machine-Learning-Operationalization\/blob\/master\/samples\/python\/tutorials\/realtime\/digit_classification.ipynb\" rel=\"nofollow noreferrer\">Example notebook<\/a> that shows how to publish Python model as a web service. You would do a similar thing, only you would pickle the dictionary of your models instead. <\/li>\n<\/ol>\n\n<p>Note that this functionality is currently in preview mode.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45576092",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1503860922243,
        "Question_original_content":"multipl model store dictionari overview uniqu python model hold train random forest model dictionari tri avoid setup time necessari local predict pass datafram predict function loop row call appropri model row like model model predict creat toi model allow web input python script score model web output challeng need abl score model specif predict method insid python script function deal loop model store dict result json datafram sent web output link onlin http doc microsoft com azur machin learn machin learn execut python script got close exampl show model train predict time insid python script call predict method local variabl call previous train model limit document onlin solv problem rest wai unsur type custom avail complet overlook kei function thank assist",
        "Question_preprocessed_content":"multipl model store dictionari overview uniqu python model hold train random forest model dictionari tri avoid setup time necessari local predict pass datafram predict function loop row call appropri model row like creat toi model allow web input python script score model web output challeng need abl specif predict method insid python script function deal loop model store dict result json datafram sent web output link onlin got close exampl show model train predict time insid python script call predict method local variabl call previous train model limit document onlin solv problem rest wai unsur type custom avail complet overlook kei function thank assist",
        "Question_gpt_summary_original":"The user has a unique Python model where they hold n trained random forest models in a dictionary. They need to be able to call the predict method from inside the \"Python Script\" function on AzureML so they can deal with the loops and n models stored in the dict. The user has found limited documentation online to solve this problem and is unsure if this type of customization is not yet available or if they are completely overlooking some key functionality.",
        "Question_gpt_summary":"user uniqu python model hold train random forest model dictionari need abl predict method insid python script function deal loop model store dict user limit document onlin solv problem unsur type custom avail complet overlook kei function",
        "Answer_original_content":"link help operation exampl notebook show publish python model web servic similar thing pickl dictionari model instead note function current preview mode",
        "Answer_preprocessed_content":"link help operation exampl notebook show publish python model web servic similar thing pickl dictionari model instead note function current preview mode",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Check out AzureML Operationalization and the example notebook provided to publish the Python model as a web service. The user can pickle the dictionary of their models instead of a single model.\n- Note that this functionality is currently in preview mode.",
        "Answer_gpt_summary":"possibl solut mention discuss check operation exampl notebook provid publish python model web servic user pickl dictionari model instead singl model note function current preview mode"
    },
    {
        "Question_title":"Can I use AWS CLI to add tags to all processing jobs matching a certain regex",
        "Question_body":"<p>I have close to 100 processing jobs to which I want to add certain tags. I've found commands that you can use to tag one resource with a list of tags. Is there any way I can do this for multiple jobs? Through CLI or through python+boto?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1657515736330,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1657607287156,
        "Question_score":0.0,
        "Question_view_count":38.0,
        "Answer_body":"<p>You can <code>ResourceGroupsTaggingAPI<\/code>'s method <code>tag_resources()<\/code>.<br \/>\nThis is used to apply one or more tags to the specified list of resources.<\/p>\n<p>References:<\/p>\n<ol>\n<li><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/resourcegroupstaggingapi.html#ResourceGroupsTaggingAPI.Client.tag_resources\" rel=\"nofollow noreferrer\">Tag Resources using boto3<\/a><\/li>\n<li><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/resourcegroupstaggingapi.html#ResourceGroupsTaggingAPI.Client.untag_resources\" rel=\"nofollow noreferrer\">UnTag Resources using boto3<\/a><\/li>\n<\/ol>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72933908",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1657516175783,
        "Question_original_content":"us aw cli add tag process job match certain regex close process job want add certain tag command us tag resourc list tag wai multipl job cli python boto",
        "Question_preprocessed_content":"us aw cli add tag process job match certain regex close process job want add certain tag command us tag resourc list tag wai multipl job cli python boto",
        "Question_gpt_summary_original":"The user is facing a challenge of adding tags to close to 100 processing jobs. They are looking for a way to add tags to multiple jobs using AWS CLI or python+boto instead of tagging one resource at a time.",
        "Question_gpt_summary":"user face challeng ad tag close process job look wai add tag multipl job aw cli python boto instead tag resourc time",
        "Answer_original_content":"resourcegroupstaggingapi method tag resourc appli tag specifi list resourc refer tag resourc boto untag resourc boto",
        "Answer_preprocessed_content":"method appli tag specifi list resourc refer tag resourc boto untag resourc boto",
        "Answer_gpt_summary_original":"Solution: The user can use the `tag_resources()` method of `ResourceGroupsTaggingAPI` using boto3 to apply one or more tags to the specified list of resources. References to the documentation of `tag_resources()` and `untag_resources()` methods are provided.",
        "Answer_gpt_summary":"solut user us tag resourc method resourcegroupstaggingapi boto appli tag specifi list resourc refer document tag resourc untag resourc method provid"
    },
    {
        "Question_title":"SageMaker - Getting file URL",
        "Question_body":"<p>Hi there,<br>\nI am trying to get the url to my dvc remote data from SageMaker Studio by running the below code. However, I get the below error and have not been able to solve it. Any advice would be greatly appreciated.<\/p>\n<p>!pip install dvc<br>\nfrom dvc.api import get_url<\/p>\n<p>url = get_url(<br>\nrepo=\u201c<a href=\"https:\/\/github.com\/...\/machine_learning\" rel=\"noopener nofollow ugc\">https:\/\/github.com\/...\/machine_learning<\/a>\u201d,<br>\npath=\u201cexperiments\/\u2026\/data.pkl\u201d<br>\n)<br>\nprint(url)<\/p>\n<p>CloneError: Failed to clone repo \u2018<a href=\"https:\/\/github.com\/...\/machine_learning\" rel=\"noopener nofollow ugc\">https:\/\/github.com\/...\/machine_learning<\/a>\u2019 to \u2018\/tmp\/tmpo6kgus_3dvc-clone\u2019<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617987668667,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":228.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mrebollo\">@MRebollo<\/a>, could you please try cloning that git repo locally? DVC uses git underneath, so if that works, DVC should be able to clone the repo as well.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/sagemaker-getting-file-url\/717",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-12T08:11:18.779Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mrebollo\">@MRebollo<\/a>, could you please try cloning that git repo locally? DVC uses git underneath, so if that works, DVC should be able to clone the repo as well.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"get file url try url remot data studio run code error abl solv advic greatli appreci pip instal api import url url url repo http github com machin learn path experi data pkl print url cloneerror fail clone repo http github com machin learn tmp tmpokgu clone",
        "Question_preprocessed_content":"get file url try url remot data studio run code error abl solv advic greatli appreci pip instal api import url print cloneerror fail clone repo",
        "Question_gpt_summary_original":"The user is facing a challenge in getting the URL to their dvc remote data from SageMaker Studio. They have tried running a code to get the URL but encountered an error message \"CloneError: Failed to clone repo\" and have not been able to solve it. They are seeking advice to resolve the issue.",
        "Question_gpt_summary":"user face challeng get url remot data studio tri run code url encount error messag cloneerror fail clone repo abl solv seek advic resolv issu",
        "Answer_original_content":"mrebollo try clone git repo local us git underneath work abl clone repo",
        "Answer_preprocessed_content":"try clone git repo local us git underneath work abl clone repo",
        "Answer_gpt_summary_original":"Solution: The discussion does not provide any specific solution to the challenge.",
        "Answer_gpt_summary":"solut discuss provid specif solut challeng"
    },
    {
        "Question_title":"Does a call to \"Deploy web service(via API key) \" re run trained Azure ML model again",
        "Question_body":"<p>I wanted to know how exactly the following works in backend<\/p>\n\n<p><strong>Scenario :<\/strong> <\/p>\n\n<blockquote>\n  <p>-> We get data from Edgex foundry in UTC format and we it store it in Azure Document DB in (CST\/CDT timezone) format<\/p>\n  \n  <p>-> We trained ML model on data(with Date in CST\/CDT timezone) and Deploy web service.<\/p>\n<\/blockquote>\n\n<p><strong>So I have few basic doubts below<\/strong><\/p>\n\n<blockquote>\n  <ol>\n  <li><p>When web job hits our predictive webservice , will the trained ML model be run again?<\/p><\/li>\n  <li><p>Do we need to convert the UTC timezone for new incoming test data( which we want to predict) into CST\/CDT timezone, as TimeStamp does\n  matter for our prediction?<\/p><\/li>\n  <li><p>What happens in backend when predictive webservice API is called?<\/p><\/li>\n  <\/ol>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1520339633757,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":92.0,
        "Answer_body":"<p>This is only based on my experience with Azure ML, but I think I can help with your questions.<\/p>\n\n<blockquote>\n  <p>When web job hits our predictive webservice, will the trained ML model be run again?<\/p>\n<\/blockquote>\n\n<p>Yes, in the sense that it will call the <code>predict<\/code> (or similar) method on the model on the new data. For instance, in <code>scikit-learn<\/code> you would train your model using the <code>fit<\/code> method. Once the model is in production, only the <code>predict<\/code> method would be called.<\/p>\n\n<p>It will also run the whole workflow you have set up to be deployed as the web service. As an example below is a workflow I've played around with before. Each time the web service is run with new data, this whole thing will be run. This is like creating a Pipeline in <code>scikit-learn<\/code>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YMFZb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YMFZb.png\" alt=\"Azure ML Workflow\"><\/a><\/p>\n\n<blockquote>\n  <p>Do we need to convert the UTC timezone for new incoming test data( which we want to predict) into CST\/CDT timezone, as TimeStamp does matter for our prediction?<\/p>\n<\/blockquote>\n\n<p>I would say yes, you would need to convert to the timezone that was used when training in the model. This can be done by adding a step in your workflow then when you call the web service it will do the necessary converting for you before making a prediction.<\/p>\n\n<blockquote>\n  <p>What happens in backend when predictive webservice API is called?<\/p>\n<\/blockquote>\n\n<p>I'm not sure if anyone knows for sure other than the folks at Microsoft, but for sure it will run the workflow you have set up.<\/p>\n\n<hr>\n\n<p>I know it's not much, but I hope this helps or at least gets you on the right track for what you need.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49130977",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1521026355920,
        "Question_original_content":"deploi web servic api kei run train model want know exactli follow work backend scenario data edgex foundri utc format store azur document cst cdt timezon format train model data date cst cdt timezon deploi web servic basic doubt web job hit predict webservic train model run need convert utc timezon new incom test data want predict cst cdt timezon timestamp matter predict happen backend predict webservic api call",
        "Question_preprocessed_content":"deploi web servic run train model want know exactli follow work backend scenario data edgex foundri utc format store azur document format train model data deploi web servic basic doubt web job hit predict webservic train model run need convert utc timezon new incom test data timezon timestamp matter predict happen backend predict webservic api call",
        "Question_gpt_summary_original":"The user is seeking clarification on how a trained ML model works in the backend when a web service is deployed via an API key. They have specific doubts about whether the model is run again when the web job hits the predictive web service, whether incoming test data needs to be converted to the correct timezone, and what happens in the backend when the predictive web service API is called.",
        "Question_gpt_summary":"user seek clarif train model work backend web servic deploi api kei specif doubt model run web job hit predict web servic incom test data need convert correct timezon happen backend predict web servic api call",
        "Answer_original_content":"base experi think help question web job hit predict webservic train model run ye sens predict similar method model new data instanc scikit learn train model fit method model product predict method call run workflow set deploi web servic exampl workflow plai time web servic run new data thing run like creat pipelin scikit learn need convert utc timezon new incom test data want predict cst cdt timezon timestamp matter predict ye need convert timezon train model ad step workflow web servic necessari convert make predict happen backend predict webservic api call sure know sure folk microsoft sure run workflow set know hope help get right track need",
        "Answer_preprocessed_content":"base experi think help question web job hit predict webservic train model run ye sens method model new data instanc train model method model product method call run workflow set deploi web servic exampl workflow plai time web servic run new data thing run like creat pipelin need convert utc timezon new incom test data timezon timestamp matter predict ye need convert timezon train model ad step workflow web servic necessari convert make predict happen backend predict webservic api call sure know sure folk microsoft sure run workflow set know hope help get right track need",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- When the web job hits the predictive web service, the trained ML model will be run again by calling the predict method on the model with the new data.\n- The whole workflow that has been set up to be deployed as the web service will also be run each time the web service is called with new data.\n- It is recommended to convert incoming test data to the timezone that was used when training the model. This can be done by adding a step in the workflow.\n- It is unclear what exactly happens in the backend when the predictive web service API is called, but it is certain that the workflow set up for the web service will be executed.",
        "Answer_gpt_summary":"possibl solut mention discuss web job hit predict web servic train model run call predict method model new data workflow set deploi web servic run time web servic call new data recommend convert incom test data timezon train model ad step workflow unclear exactli happen backend predict web servic api call certain workflow set web servic execut"
    },
    {
        "Question_title":"Azure ML Enpoint deployment failed EAST US region",
        "Question_body":"I have an Azure ML Real-time inference endpoint deployed ran for a month till yesterday. Today it is in the state of \"Failed\".\n\nI did create a new compute and did a new deployment in the same region EAST US and it failed again.\n\nWhat's going? Is this just a problem for me or a general issue?\n\nThanks\n-Dali",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1594945530060,
        "Question_favorite_count":44.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, thanks for reaching out. I successfully deployed in the east us region. Please review the following troubleshooting guidelines. Also check for any service\/resource health issues that could be impacting your service. Let me know if you're still experiencing issues afterwards and please share the logs so we can investigate further. Thanks.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/48609\/azure-ml-enpoint-deployment-failed-east-us-region.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-17T02:30:29.753Z",
                "Answer_score":0,
                "Answer_body":"Hi, thanks for reaching out. I successfully deployed in the east us region. Please review the following troubleshooting guidelines. Also check for any service\/resource health issues that could be impacting your service. Let me know if you're still experiencing issues afterwards and please share the logs so we can investigate further. Thanks.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1594953029752,
        "Question_original_content":"enpoint deploy fail east region real time infer endpoint deploi ran month till yesterdai todai state fail creat new comput new deploy region east fail go problem gener issu thank dali",
        "Question_preprocessed_content":"enpoint deploy fail east region infer endpoint deploi ran month till yesterdai todai state fail creat new comput new deploy region east fail go problem gener issu thank dali",
        "Question_gpt_summary_original":"The user's Azure ML Real-time inference endpoint deployment in the EAST US region has failed after running successfully for a month. The user attempted to create a new compute and deploy again in the same region, but it failed again. The user is unsure if this is a general issue or specific to their account.",
        "Question_gpt_summary":"user real time infer endpoint deploy east region fail run successfulli month user attempt creat new comput deploi region fail user unsur gener issu specif account",
        "Answer_original_content":"thank reach successfulli deploi east region review follow troubleshoot guidelin check servic resourc health issu impact servic let know experienc issu share log investig thank",
        "Answer_preprocessed_content":"thank reach successfulli deploi east region review follow troubleshoot guidelin check health issu impact servic let know experienc issu share log investig thank",
        "Answer_gpt_summary_original":"Solutions provided:\n- Review troubleshooting guidelines\n- Check for any service\/resource health issues that could be impacting the service\n- Share logs for further investigation\n\nNo personal opinions or biases were included in the response.",
        "Answer_gpt_summary":"solut provid review troubleshoot guidelin check servic resourc health issu impact servic share log investig person opinion bias includ respons"
    },
    {
        "Question_title":"AzureML Notebooks: how to access data from an experiment",
        "Question_body":"Hi, I am new to Azure ML, and I have been trying to replicate the same structure presented in the MNIST tutorial, but I don't understand how to adapt it to my case.\n\nI am running a python file from the experiment, but I don't understand how I can access data that is currently in a folder in the cloud file system from the script running in the experiment.\nI have found many examples about accessing one single .csv file, but my data is made of many images.\n\nFrom my understanding I should first load the folder to a datastore, then use Dataset.File.upload_directory to create a dataset containing my folder, and here is how I tried to do it:\n\n # Create dataset from data directory\n datastore = Datastore.get(ws, 'workspaceblobstore')\n dataset = Dataset.File.upload_directory(path_data, target, pattern=None, overwrite=False, show_progress=True)\n    \n file_dataset = dataset.register(workspace=ws, name='reduced_classification_dataset',\n                                                  description='reduced_classification_dataset',\n                                                  create_new_version=True)\n\n\n\nBut then I don't understand if and how I can access this data like a normal file system from my python script, or I need further steps to be able to do that.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1614762558803,
        "Question_favorite_count":7.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"@Matzof Thanks for the question. Please follow the below code for writing.\n\n    datastore = ## get your defined in Workspace as Datastore \n datastore.upload(src_dir='.\/files\/to\/copy\/...',\n                  target_path='target\/directory',\n                  overwrite=True)\n\n\n\nDatastore.upload only support blob and fileshare. For adlsgen2 upload, you can try our new dataset upload API:\n\n\n\n from azureml.core import Dataset, Datastore\n datastore = Datastore.get(workspace, 'mayadlsgen2')\n Dataset.File.upload_directory(src_dir='.\/data', target=(datastore,'data'))\n\n\n\n\nPandas is integrated with fsspec which provides Pythonic implementation for filesystems including s3, gcs, and Azure. You can check the source for Azure here: dask\/adlfs: fsspec-compatible Azure Datake and Azure Blob Storage access (github.com). With this you can use normal filesystem operations like ls, glob, info, etc.\n\nYou can find an example (for reading data) here: azureml-examples\/1.intro-to-dask.ipynb at main \u00b7 Azure\/azureml-examples (github.com)\n\nWriting is essentially the same as reading, you need to switch the protocol to abfs (or az), slightly modify how you're accessing the data, and provide credentials unless your blob has public write access.\n\nYou can use the Azure ML Datastore to retrieve credentials like this (taken from example):",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/296661\/azureml-notebooks-how-to-access-data-from-an-exper.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-04T04:48:33.09Z",
                "Answer_score":0,
                "Answer_body":"@Matzof Thanks for the question. Please follow the below code for writing.\n\n    datastore = ## get your defined in Workspace as Datastore \n datastore.upload(src_dir='.\/files\/to\/copy\/...',\n                  target_path='target\/directory',\n                  overwrite=True)\n\n\n\nDatastore.upload only support blob and fileshare. For adlsgen2 upload, you can try our new dataset upload API:\n\n\n\n from azureml.core import Dataset, Datastore\n datastore = Datastore.get(workspace, 'mayadlsgen2')\n Dataset.File.upload_directory(src_dir='.\/data', target=(datastore,'data'))\n\n\n\n\nPandas is integrated with fsspec which provides Pythonic implementation for filesystems including s3, gcs, and Azure. You can check the source for Azure here: dask\/adlfs: fsspec-compatible Azure Datake and Azure Blob Storage access (github.com). With this you can use normal filesystem operations like ls, glob, info, etc.\n\nYou can find an example (for reading data) here: azureml-examples\/1.intro-to-dask.ipynb at main \u00b7 Azure\/azureml-examples (github.com)\n\nWriting is essentially the same as reading, you need to switch the protocol to abfs (or az), slightly modify how you're accessing the data, and provide credentials unless your blob has public write access.\n\nYou can use the Azure ML Datastore to retrieve credentials like this (taken from example):",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1614833313088,
        "Question_original_content":"notebook access data experi new try replic structur present mnist tutori understand adapt case run python file experi understand access data current folder cloud file script run experi exampl access singl csv file data imag understand load folder datastor us dataset file upload directori creat dataset contain folder tri creat dataset data directori datastor datastor workspaceblobstor dataset dataset file upload directori path data target pattern overwrit fals progress true file dataset dataset regist workspac reduc classif dataset descript reduc classif dataset creat new version true understand access data like normal file python script need step abl",
        "Question_preprocessed_content":"notebook access data experi new try replic structur present mnist tutori understand adapt case run python file experi understand access data current folder cloud file script run experi exampl access singl csv file data imag understand load folder datastor us creat dataset contain folder tri creat dataset data directori datastor workspaceblobstor dataset target pattern overwrit fals understand access data like normal file python script need step abl",
        "Question_gpt_summary_original":"The user is facing challenges in accessing data from a folder in the cloud file system in Azure ML. They are trying to replicate the structure presented in the MNIST tutorial but are unsure how to adapt it to their case, which involves many images. They have attempted to create a dataset from the data directory but are unsure how to access it like a normal file system from their python script.",
        "Question_gpt_summary":"user face challeng access data folder cloud file try replic structur present mnist tutori unsur adapt case involv imag attempt creat dataset data directori unsur access like normal file python script",
        "Answer_original_content":"matzof thank question follow code write datastor defin workspac datastor datastor upload src dir file copi target path target directori overwrit true datastor upload support blob fileshar adlsgen upload try new dataset upload api core import dataset datastor datastor datastor workspac mayadlsgen dataset file upload directori src dir data target datastor data panda integr fsspec provid python implement filesystem includ gc azur check sourc azur dask adlf fsspec compat azur datak azur blob storag access github com us normal filesystem oper like glob info exampl read data exampl intro dask ipynb main azur exampl github com write essenti read need switch protocol abf slightli modifi access data provid credenti blob public write access us datastor retriev credenti like taken exampl",
        "Answer_preprocessed_content":"thank question follow code write datastor defin workspac datastor overwrit true support blob fileshar adlsgen upload try new dataset upload api core import dataset datastor datastor mayadlsgen target datastor data panda integr fsspec provid python implement filesystem includ gc azur check sourc azur azur datak azur blob storag access us normal filesystem oper like glob info exampl main write essenti read need switch protocol abf slightli modifi access data provid credenti blob public write access us datastor retriev credenti like",
        "Answer_gpt_summary_original":"Solutions provided:\n- Use the Azure ML Datastore to upload the data directory and access it like a normal file system from the python script.\n- Use the new dataset upload API for adlsgen2 upload.\n- Use Pandas integrated with fsspec to use normal filesystem operations like ls, glob, info, etc.\n- Use the Azure ML Datastore to retrieve credentials.",
        "Answer_gpt_summary":"solut provid us datastor upload data directori access like normal file python script us new dataset upload api adlsgen upload us panda integr fsspec us normal filesystem oper like glob info us datastor retriev credenti"
    },
    {
        "Question_title":"How to correctly write a sagemaker tensorflow input_handler() that returns a numpy array?",
        "Question_body":"<p>I am trying to implement a input_handler() in inference.py for a sagemaker inference container.<\/p>\n<p>The images\/arrays are very big (3D). So I want to pass in a S3 URI, then the input_handler() function should load the image\/array from s3 and return the actual numpy array for the model (which expects a tensor):<\/p>\n<pre><code>def input_handler(data, context):\n\n    d = data.read().decode('utf-8')\n\n    body = json.loads(d)\n    s3path = body['s3_path']\n\n    s3 = S3FileSystem()\n    df = np.load(s3.open(s3path))\n\n    return df\n<\/code><\/pre>\n<p>Returning a numpy array worked with the Sagemaker python api version &lt; 1.0 and input_fn(), but does not work with the new container used by sagemaker python api &gt; 2.0 that expects input_handler().<\/p>\n<p>The actual container image is &quot;763104351884.dkr.ecr.eu-central-1.amazonaws.com\/tensorflow-inference:1.15-gpu&quot;.<\/p>\n<p>During inference, I get the following error in CloudWatch thrown by the container:<\/p>\n<pre><code>ERROR:python_service:exception handling request: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(\n\nTraceback (most recent call last):\n  File &quot;\/sagemaker\/python_service.py&quot;, line 289, in _handle_invocation_post\n    res.body, res.content_type = self._handlers(data, context)\n  File &quot;\/sagemaker\/python_service.py&quot;, line 322, in handler\n    response = requests.post(context.rest_uri, data=processed_input)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/api.py&quot;, line 116, in post\n    return request('post', url, data=data, json=json, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/api.py&quot;, line 60, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/requests\/sessions.py&quot;, line 512, in request\n    data=data or \n{}\n,\n<\/code><\/pre>\n<p>What is the correct return type? All examples I found were for json &amp; text...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1599496754993,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1599548555527,
        "Question_score":0.0,
        "Question_view_count":636.0,
        "Answer_body":"<p>This seems to work:<\/p>\n<p><code>return json.dumps({&quot;inputs&quot;: df.tolist() }).<\/code><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63781356",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1600259722303,
        "Question_original_content":"correctli write tensorflow input handler return numpi arrai try implement input handler infer infer contain imag arrai big want pass uri input handler function load imag arrai return actual numpi arrai model expect tensor def input handler data context data read decod utf bodi json load spath bodi path sfilesystem load open spath return return numpi arrai work python api version expect input handler actual contain imag dkr ecr central amazonaw com tensorflow infer gpu infer follow error cloudwatch thrown contain error python servic except handl request truth valu arrai element ambigu us traceback recent file python servic line handl invoc post re bodi re content type self handler data context file python servic line handler respons request post context rest uri data process input file usr local lib python dist packag request api line post return request post url data data json json kwarg file usr local lib python dist packag request api line request return session request method method url url kwarg file usr local lib python dist packag request session line request data data correct return type exampl json text",
        "Question_preprocessed_content":"correctli write tensorflow return numpi arrai try implement infer contain big want pass uri function load return actual numpi arrai model return numpi arrai work python api version expect actual contain imag infer follow error cloudwatch thrown contain correct return type exampl json",
        "Question_gpt_summary_original":"The user is facing challenges in implementing an input_handler() function for a Sagemaker inference container that can load large 3D images\/arrays from S3 and return a numpy array for the model. The function worked with the Sagemaker python API version <1.0 and input_fn() but not with the new container used by Sagemaker python API >2.0 that expects input_handler(). The user is getting an error during inference, which suggests an issue with the return type. The user is seeking guidance on the correct return type for the input_handler() function.",
        "Question_gpt_summary":"user face challeng implement input handler function infer contain load larg imag arrai return numpi arrai model function work python api version expect input handler user get error infer suggest issu return type user seek guidanc correct return type input handler function",
        "Answer_original_content":"work return json dump input tolist",
        "Answer_preprocessed_content":"work",
        "Answer_gpt_summary_original":"Solution: The following solution was suggested in the discussion: \n\n- Return the numpy array as a JSON object using the following code: `return json.dumps({\"inputs\": df.tolist()})`.",
        "Answer_gpt_summary":"solut follow solut suggest discuss return numpi arrai json object follow code return json dump input tolist"
    },
    {
        "Question_title":"Use Azure ML methods like an API",
        "Question_body":"<p>Is that possible to use machine learning methods from Microsoft Azure Machine Learning  as an API from my own code (without ML Studio) with possibility to calculate everything on their side?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1450294016510,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":180.0,
        "Answer_body":"<p>You can <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-publish-a-machine-learning-web-service\/\" rel=\"nofollow\">publish<\/a> an experiment (machine learning functions you hooked together in Azure ML Studio) as an API. When you call that API in your custom code you give it your data and all the computation runs in the cloud in Azure ML. <\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34320449",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1450317613532,
        "Question_original_content":"us method like api possibl us machin learn method api code studio possibl calcul",
        "Question_preprocessed_content":"us method like api possibl us machin learn method api code possibl calcul",
        "Question_gpt_summary_original":"The user is facing a challenge of whether it is possible to use Azure ML methods as an API from their own code without using ML Studio and with the ability to perform calculations on the Azure side.",
        "Question_gpt_summary":"user face challeng possibl us method api code studio abil perform calcul azur",
        "Answer_original_content":"publish experi machin learn function hook studio api api custom code data comput run cloud",
        "Answer_preprocessed_content":"publish experi api api custom code data comput run cloud",
        "Answer_gpt_summary_original":"Solution: The user can publish an experiment as an API and call it in their custom code, which will allow them to perform calculations on the Azure side without using ML Studio.",
        "Answer_gpt_summary":"solut user publish experi api custom code allow perform calcul azur studio"
    },
    {
        "Question_title":"What is the best way to deploy my machine learning model using GPUs, specifically as a web based API?",
        "Question_body":"I am trying to find the best way to run my machine learning models on GPUs for inference as an http request. Do Azure functions support GPUs? if not, what are other options I can look into?\n\nnote: I also want to use packaged models, not necessarily ones of my own creation (such as easyOCR for python)",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630916125883,
        "Question_favorite_count":14.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi,\n\nIf you need GPU support on ML inference the only supported option is the Azure Kubernetes Service as stated in this documentation\n\nFor guidance on deploying an ML model to AKS, please refer to this documenation on deploying to AKS",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/541074\/what-is-a-the-best-way-to-deploy-my-machine-learni.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-06T09:42:39.623Z",
                "Answer_score":1,
                "Answer_body":"Hi,\n\nIf you need GPU support on ML inference the only supported option is the Azure Kubernetes Service as stated in this documentation\n\nFor guidance on deploying an ML model to AKS, please refer to this documenation on deploying to AKS",
                "Answer_comment_count":4,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1630921359623,
        "Question_original_content":"best wai deploi machin learn model gpu specif web base api try best wai run machin learn model gpu infer http request azur function support gpu option look note want us packag model necessarili on creation easyocr python",
        "Question_preprocessed_content":"best wai deploi machin learn model gpu specif web base api try best wai run machin learn model gpu infer http request azur function support gpu option look note want us packag model necessarili on creation",
        "Question_gpt_summary_original":"The user is facing challenges in finding the best way to deploy their machine learning models on GPUs for inference as an http request. They are specifically looking for options to use packaged models and are unsure if Azure functions support GPUs.",
        "Question_gpt_summary":"user face challeng find best wai deploi machin learn model gpu infer http request specif look option us packag model unsur azur function support gpu",
        "Answer_original_content":"need gpu support infer support option azur kubernet servic state document guidanc deploi model ak refer documen deploi ak",
        "Answer_preprocessed_content":"need gpu support infer support option azur kubernet servic state document guidanc deploi model ak refer documen deploi ak",
        "Answer_gpt_summary_original":"Solution: The only supported option for GPU support on ML inference is the Azure Kubernetes Service (AKS). The user can refer to the documentation on deploying an ML model to AKS for guidance. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut support option gpu support infer azur kubernet servic ak user refer document deploi model ak guidanc solut mention discuss"
    },
    {
        "Question_title":"ModelUploadOp step failing with custom prediction container",
        "Question_body":"<p>I am currenlty trying to deploy a Vertex pipeline to achieve the following:<\/p>\n<ol>\n<li><p>Train a custom model (from a custom training python package) and dump model artifacts (trained model and data preprocessor that will be sed at prediction time). This is step is working fine as I can see new resources being created in the storage bucket.<\/p>\n<\/li>\n<li><p>Create a model resource via <code>ModelUploadOp<\/code>. This step fails for some reason when specifying <code>serving_container_environment_variables<\/code> and <code>serving_container_ports<\/code> with the error message in the <strong>errors<\/strong> section below. This is somewhat surprising as they are both needed by the prediction container and environment variables are passed as a dict as specified in the documentation.<br \/>\nThis step works just fine using <code>gcloud<\/code> commands:<\/p>\n<\/li>\n<\/ol>\n<pre class=\"lang-sh prettyprint-override\"><code>gcloud ai models upload \\\n    --region us-west1 \\\n    --display-name session_model_latest \\\n    --container-image-uri gcr.io\/and-reporting\/pred:latest \\\n    --container-env-vars=&quot;MODEL_BUCKET=ml_session_model&quot; \\\n    --container-health-route=\/\/health \\\n    --container-predict-route=\/\/predict \\\n    --container-ports=5000\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Create an endpoint.<\/li>\n<li>Deploy the model to the endpoint.<\/li>\n<\/ol>\n<p>There is clearly something that I am getting wrong with Vertex, the components <a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.2.2\/index.html\" rel=\"nofollow noreferrer\">documentation<\/a> doesn't help much in this case.<\/p>\n<h2>Pipeline<\/h2>\n<pre class=\"lang-py prettyprint-override\"><code>from datetime import datetime\n\nimport kfp\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom kfp.v2 import compiler\n\nPIPELINE_ROOT = &quot;gs:\/\/ml_model_bucket\/pipeline_root&quot;\n\n\n@kfp.dsl.pipeline(name=&quot;session-train-deploy&quot;, pipeline_root=PIPELINE_ROOT)\ndef pipeline():\n    training_op = gcc_aip.CustomPythonPackageTrainingJobRunOp(\n        project=&quot;my-project&quot;,\n        location=&quot;us-west1&quot;,\n        display_name=&quot;train_session_model&quot;,\n        model_display_name=&quot;session_model&quot;,\n        service_account=&quot;name@my-project.iam.gserviceaccount.com&quot;,\n        environment_variables={&quot;MODEL_BUCKET&quot;: &quot;ml_session_model&quot;},\n        python_module_name=&quot;trainer.train&quot;,\n        staging_bucket=&quot;gs:\/\/ml_model_bucket\/&quot;,\n        base_output_dir=&quot;gs:\/\/ml_model_bucket\/&quot;,\n        args=[\n            &quot;--gcs-data-path&quot;,\n            &quot;gs:\/\/ml_model_data\/2019-Oct_short.csv&quot;,\n            &quot;--gcs-model-path&quot;,\n            &quot;gs:\/\/ml_model_bucket\/model\/model.joblib&quot;,\n            &quot;--gcs-preproc-path&quot;,\n            &quot;gs:\/\/ml_model_bucket\/model\/preproc.pkl&quot;,\n        ],\n        container_uri=&quot;us-docker.pkg.dev\/vertex-ai\/training\/scikit-learn-cpu.0-23:latest&quot;,\n        python_package_gcs_uri=&quot;gs:\/\/ml_model_bucket\/trainer-0.0.1.tar.gz&quot;,\n        model_serving_container_image_uri=&quot;gcr.io\/my-project\/pred&quot;,\n        model_serving_container_predict_route=&quot;\/predict&quot;,\n        model_serving_container_health_route=&quot;\/health&quot;,\n        model_serving_container_ports=[5000],\n        model_serving_container_environment_variables={\n            &quot;MODEL_BUCKET&quot;: &quot;ml_model_bucket\/model&quot;\n        },\n    )\n\n    model_upload_op = gcc_aip.ModelUploadOp(\n        project=&quot;and-reporting&quot;,\n        location=&quot;us-west1&quot;,\n        display_name=&quot;session_model&quot;,\n        serving_container_image_uri=&quot;gcr.io\/my-project\/pred:latest&quot;,\n        # When passing the following 2 arguments this step fails...\n        serving_container_environment_variables={&quot;MODEL_BUCKET&quot;: &quot;ml_model_bucket\/model&quot;},\n        serving_container_ports=[5000],\n        serving_container_predict_route=&quot;\/predict&quot;,\n        serving_container_health_route=&quot;\/health&quot;,\n    )\n    model_upload_op.after(training_op)\n\n    endpoint_create_op = gcc_aip.EndpointCreateOp(\n        project=&quot;my-project&quot;,\n        location=&quot;us-west1&quot;,\n        display_name=&quot;pipeline_endpoint&quot;,\n    )\n\n    model_deploy_op = gcc_aip.ModelDeployOp(\n        model=model_upload_op.outputs[&quot;model&quot;],\n        endpoint=endpoint_create_op.outputs[&quot;endpoint&quot;],\n        deployed_model_display_name=&quot;session_model&quot;,\n        traffic_split={&quot;0&quot;: 100},\n        service_account=&quot;name@my-project.iam.gserviceaccount.com&quot;,\n    )\n    model_deploy_op.after(endpoint_create_op)\n\n\nif __name__ == &quot;__main__&quot;:\n    ts = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n    compiler.Compiler().compile(pipeline, &quot;custom_train_pipeline.json&quot;)\n    pipeline_job = aiplatform.PipelineJob(\n        display_name=&quot;session_train_and_deploy&quot;,\n        template_path=&quot;custom_train_pipeline.json&quot;,\n        job_id=f&quot;session-custom-pipeline-{ts}&quot;,\n        enable_caching=True,\n    )\n    pipeline_job.submit()\n\n<\/code><\/pre>\n<h3>Errors and notes<\/h3>\n<ol>\n<li>When specifying <code>serving_container_environment_variables<\/code> and <code>serving_container_ports<\/code> the step fails with the following error:<\/li>\n<\/ol>\n<pre><code>{'code': 400, 'message': 'Invalid JSON payload received. Unknown name &quot;MODEL_BUCKET&quot; at \\'model.container_spec.env[0]\\': Cannot find field.\\nInvalid value at \\'model.container_spec.ports[0]\\' (type.googleapis.com\/google.cloud.aiplatform.v1.Port), 5000', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com\/google.rpc.BadRequest', 'fieldViolations': [{'field': 'model.container_spec.env[0]', 'description': 'Invalid JSON payload received. Unknown name &quot;MODEL_BUCKET&quot; at \\'model.container_spec.env[0]\\': Cannot find field.'}, {'field': 'model.container_spec.ports[0]', 'description': &quot;Invalid value at 'model.container_spec.ports[0]' (type.googleapis.com\/google.cloud.aiplatform.v1.Port), 5000&quot;}]}]}\n<\/code><\/pre>\n<p>When commenting out <code>serving_container_environment_variables<\/code> and <code>serving_container_ports<\/code>  the model resource gets created but deploying it manually to the endpoint results into a failed deployment with no output logs.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1643879294320,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1644239389000,
        "Question_score":0.0,
        "Question_view_count":273.0,
        "Answer_body":"<p>After some time researching the problem I've stumbled upon <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/6848\" rel=\"nofollow noreferrer\">this<\/a> Github issue. The problem was originated by a mismatch between <a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.2.2\/index.html\" rel=\"nofollow noreferrer\"><code>google_cloud_pipeline_components<\/code><\/a> and <a href=\"https:\/\/kubernetes.io\/docs\/reference\/generated\/kubernetes-api\/v1.19\/#envvar-v1-core\" rel=\"nofollow noreferrer\"><code>kubernetes_api<\/code><\/a> docs. In this case, <code>serving_container_environment_variables<\/code> is typed as an <code>Optional[dict[str, str]]<\/code> whereas it should have been typed as a <code>Optional[list[dict[str, str]]]<\/code>. A similar mismatch can be found for <code>serving_container_ports<\/code> argument as well. Passing arguments following kubernetes documentation did the trick:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_upload_op = gcc_aip.ModelUploadOp(\n    project=&quot;my-project&quot;,\n    location=&quot;us-west1&quot;,\n    display_name=&quot;session_model&quot;,\n    serving_container_image_uri=&quot;gcr.io\/my-project\/pred:latest&quot;,\n    serving_container_environment_variables=[\n        {&quot;name&quot;: &quot;MODEL_BUCKET&quot;, &quot;value&quot;: &quot;ml_session_model&quot;}\n    ],\n    serving_container_ports=[{&quot;containerPort&quot;: 5000}],\n    serving_container_predict_route=&quot;\/predict&quot;,\n    serving_container_health_route=&quot;\/health&quot;,\n)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70968460",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1643965835692,
        "Question_original_content":"modeluploadop step fail custom predict contain currenlti try deploi vertex pipelin achiev follow train custom model custom train python packag dump model artifact train model data preprocessor sed predict time step work fine new resourc creat storag bucket creat model resourc modeluploadop step fail reason specifi serv contain environ variabl serv contain port error messag error section somewhat surpris need predict contain environ variabl pass dict specifi document step work fine gcloud command gcloud model upload region west displai session model latest contain imag uri gcr report pred latest contain env var model bucket session model contain health rout health contain predict rout predict contain port creat endpoint deploi model endpoint clearli get wrong vertex compon document help case pipelin datetim import datetim import kfp googl cloud import aiplatform googl cloud pipelin compon import aiplatform gcc aip kfp import compil pipelin root model bucket pipelin root kfp dsl pipelin session train deploi pipelin root pipelin root def pipelin train gcc aip custompythonpackagetrainingjobrunop project project locat west displai train session model model displai session model servic account project iam gserviceaccount com environ variabl model bucket session model python modul trainer train stage bucket model bucket base output dir model bucket arg gc data path model data oct short csv gc model path model bucket model model joblib gc preproc path model bucket model preproc pkl contain uri docker pkg dev vertex train scikit learn cpu latest python packag gc uri model bucket trainer tar model serv contain imag uri gcr project pred model serv contain predict rout predict model serv contain health rout health model serv contain port model serv contain environ variabl model bucket model bucket model model upload gcc aip modeluploadop project report locat west displai session model serv contain imag uri gcr project pred latest pass follow argument step fail serv contain environ variabl model bucket model bucket model serv contain port serv contain predict rout predict serv contain health rout health model upload train endpoint creat gcc aip endpointcreateop project project locat west displai pipelin endpoint model deploi gcc aip modeldeployop model model upload output model endpoint endpoint creat output endpoint deploi model displai session model traffic split servic account project iam gserviceaccount com model deploi endpoint creat main datetim strftime compil compil compil pipelin custom train pipelin json pipelin job aiplatform pipelinejob displai session train deploi templat path custom train pipelin json job session custom pipelin enabl cach true pipelin job submit error note specifi serv contain environ variabl serv contain port step fail follow error code messag invalid json payload receiv unknown model bucket model contain spec env field ninvalid valu model contain spec port type googleapi com googl cloud aiplatform port statu invalid argument detail type type googleapi com googl rpc badrequest fieldviol field model contain spec env descript invalid json payload receiv unknown model bucket model contain spec env field field model contain spec port descript invalid valu model contain spec port type googleapi com googl cloud aiplatform port comment serv contain environ variabl serv contain port model resourc get creat deploi manual endpoint result fail deploy output log",
        "Question_preprocessed_content":"modeluploadop step fail custom predict contain currenlti try deploi vertex pipelin achiev follow train custom model dump model artifact step work fine new resourc creat storag bucket creat model resourc step fail reason specifi error messag error section somewhat surpris need predict contain environ variabl pass dict specifi document step work fine command creat endpoint deploi model endpoint clearli get wrong vertex compon document help case pipelin error note specifi step fail follow error comment model resourc get creat deploi manual endpoint result fail deploy output log",
        "Question_gpt_summary_original":"The user is encountering challenges while deploying a Vertex pipeline. The ModelUploadOp step is failing when specifying serving_container_environment_variables and serving_container_ports. The error message indicates that the JSON payload received is invalid and the field \"MODEL_BUCKET\" cannot be found. The user has tried to create a model resource via gcloud commands, which works fine. However, when using Vertex, the documentation doesn't help much.",
        "Question_gpt_summary":"user encount challeng deploi vertex pipelin modeluploadop step fail specifi serv contain environ variabl serv contain port error messag indic json payload receiv invalid field model bucket user tri creat model resourc gcloud command work fine vertex document help",
        "Answer_original_content":"time research problem stumbl github issu problem origin mismatch googl cloud pipelin compon kubernet api doc case serv contain environ variabl type option dict str str type option list dict str str similar mismatch serv contain port argument pass argument follow kubernet document trick model upload gcc aip modeluploadop project project locat west displai session model serv contain imag uri gcr project pred latest serv contain environ variabl model bucket valu session model serv contain port containerport serv contain predict rout predict serv contain health rout health",
        "Answer_preprocessed_content":"time research problem stumbl github issu problem origin mismatch doc case type type similar mismatch argument pass argument follow kubernet document trick",
        "Answer_gpt_summary_original":"Solution:\nThe problem was caused by a mismatch between `google_cloud_pipeline_components` and `kubernetes_api` docs. The `serving_container_environment_variables` and `serving_container_ports` arguments should have been typed as `Optional[list[dict[str, str]]]` instead of `Optional[dict[str, str]]`. Passing arguments following kubernetes documentation resolved the issue.",
        "Answer_gpt_summary":"solut problem caus mismatch googl cloud pipelin compon kubernet api doc serv contain environ variabl serv contain port argument type option list dict str str instead option dict str str pass argument follow kubernet document resolv issu"
    },
    {
        "Question_title":"All records are lost in a project without any action",
        "Question_body":"<p>Dear Sir or Madam,<\/p>\n<p>Sorry for bothering you, I think there is an error in one of my wandb projects and the records of all runs were lost. The account is nbower0707, email 1155156871@link.cuhk.edu.hk, and the project name is ocp22.<\/p>\n<p>Everything worked fine before today, and I did a lot of experiments on this project. I\u2019m uploading records of my metric around every 5000 steps, and the result validation metric plot should be something like  figure 1 shows(continuous lines of records, with multiple data points) I\u2019m uploading the corresponding metrics every 2500 steps, and wandb displayed all results fine yesterday (either undergoing or finished runs)<\/p>\n<p>However, when I check the plot today, the record of metric in all runs were (completely or partly) lost, except for some small isolated data points left (as figure 2 and 3 shows).<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d.jpeg\" data-download-href=\"\/uploads\/short-url\/n0TMrYL9SyvpaH1YKsBmceDhhRb.jpeg?dl=1\" title=\"Picture 1\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_414x500.jpeg\" alt=\"Picture 1\" data-base62-sha1=\"n0TMrYL9SyvpaH1YKsBmceDhhRb\" width=\"414\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_414x500.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_621x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_828x1000.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Picture 1<\/span><span class=\"informations\">2337\u00d72818 348 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>I tried to use <strong>wandb sync<\/strong> from the local file, and upload the runs to a new project, the result is still the same.<\/p>\n<p>I didn\u2019t do any specific operations regarding wandb logging process or on the website. The project consist of runs uploaded from different machines, therefore it wouldn\u2019t be mistakenly deletion\/ false operation offline. And the phenomenon of lost of data also occurs on old runs that finished weeks ago.<\/p>\n<p>Please let me know if you have any suggestions on this error, and if the records could be recovered.<\/p>\n<p>Your time and patience are sincerely appreciated.<\/p>\n<p>Bowen Wang<\/p>",
        "Question_answer_count":13,
        "Question_comment_count":0,
        "Question_creation_time":1661403318517,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":162.0,
        "Answer_body":"<p>Hey all,<\/p>\n<p>Our engineering team looked into this and rolled back some changes, everything should be working fine now.<\/p>\n<p>Please let us know if this issue persists.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/all-records-are-lost-in-a-project-without-any-action\/2993",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-25T05:10:38.981Z",
                "Answer_body":"<p>I have the same problem\u2026<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T06:26:28.134Z",
                "Answer_body":"<p>Me too. From night to morning the runs graphs miss a lot of data points in the validation section and I also noted that the resize of the panels in that section doesn\u2019t work properly<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T06:27:35.035Z",
                "Answer_body":"<p>Same problem here, the plots are weird and loses a lot of data points<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T06:44:26.346Z",
                "Answer_body":"<p>Me too\u2026 It seem to be  same problem<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T07:43:03.396Z",
                "Answer_body":"<p>Hi Everyone,<\/p>\n<p>Apologies for the inconvenience here. We are looking into the issue - any links to workspaces where you see this currently would be greatly appreciated.<\/p>\n<p>Thanks,<br>\nRamit<br>\nW&amp;B Support<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T08:19:07.481Z",
                "Answer_body":"<p>Mine is <a href=\"https:\/\/wandb.ai\/niansong1996\/cot-codegen?workspace=user-niansong1996\" class=\"inline-onebox\">Weights &amp; Biases<\/a><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T08:20:00.625Z",
                "Answer_body":"<p>Same problem here, plots look similar to the ones shown<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T08:45:43.885Z",
                "Answer_body":"<p>Here\u2019s <a href=\"https:\/\/wandb.ai\/johnminelli\/TwoWaySinth\">mine<\/a><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T09:41:56.499Z",
                "Answer_body":"<p>The same issue\u2026<br>\nYesterday  they were fine\u2026<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T09:46:40.765Z",
                "Answer_body":"<p>Same issue  <a href=\"https:\/\/wandb.ai\/ecotoxformer\/fish-EC50-MOR?workspace=user-styrbjornkall\">here<\/a>. Though the charts look fine when opened in their respective run, just not in the combined workspace\u2026<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T16:49:12.578Z",
                "Answer_body":"<p>Now it\u2019s fine for me, thank you for the support<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T21:07:26.293Z",
                "Answer_body":"<p>Hey all,<\/p>\n<p>Our engineering team looked into this and rolled back some changes, everything should be working fine now.<\/p>\n<p>Please let us know if this issue persists.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-10-24T21:08:14.661Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":1661461646292,
        "Question_original_content":"record lost project action dear sir madam sorri bother think error project record run lost account nbower email link cuhk edu project ocp work fine todai lot experi project upload record metric step result valid metric plot like figur show continu line record multipl data point upload correspond metric step displai result fine yesterdai undergo finish run check plot todai record metric run complet partli lost small isol data point left figur show pictur tri us sync local file upload run new project result didnt specif oper log process websit project consist run upload differ machin wouldnt mistakenli delet fals oper offlin phenomenon lost data occur old run finish week ago let know suggest error record recov time patienc sincer appreci bowen wang",
        "Question_preprocessed_content":"record lost project action dear sir madam sorri bother think error project record run lost account nbower email project ocp work fine todai lot experi project upload record metric step result valid metric plot like figur show upload correspond metric step displai result fine yesterdai check plot todai record metric run lost small isol data point left pictur tri us sync local file upload run new project result didnt specif oper log process websit project consist run upload differ machin wouldnt mistakenli delet fals oper offlin phenomenon lost data occur old run finish week ago let know suggest error record recov time patienc sincer appreci bowen wang",
        "Question_gpt_summary_original":"The user has encountered a challenge where all records of their runs in a wandb project named \"ocp22\" have been lost without any action on their part. The user had been uploading records of their metric every 5000 steps and the validation metric plot was displaying fine until today. The records of the metric in all runs were lost, except for some small isolated data points. The user tried to use \"wandb sync\" from the local file and upload the runs to a new project, but the result was still the same. The user did not perform any specific operations regarding the wandb logging process or on the website, and the phenomenon of lost data also occurred on old runs that finished weeks ago. The user is seeking suggestions on how to recover the lost records.",
        "Question_gpt_summary":"user encount challeng record run project name ocp lost action user upload record metric step valid metric plot displai fine todai record metric run lost small isol data point user tri us sync local file upload run new project result user perform specif oper log process websit phenomenon lost data occur old run finish week ago user seek suggest recov lost record",
        "Answer_original_content":"hei engin team look roll chang work fine let know issu persist thank ramit",
        "Answer_preprocessed_content":"hei engin team look roll chang work fine let know issu persist thank ramit",
        "Answer_gpt_summary_original":"Solution: The engineering team has looked into the issue and rolled back some changes, which should have resolved the problem. The user is advised to check if the issue persists and report it if it does. No other solutions were mentioned.",
        "Answer_gpt_summary":"solut engin team look issu roll chang resolv problem user advis check issu persist report solut mention"
    },
    {
        "Question_title":"Why package is not updated even the lifecycle script has been executed successfully in SageMaker?",
        "Question_body":"<p>I wanted to update pandas version in 'conda-python3' in SageMaker, I've followed the steps in this <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">page<\/a>, and linked the new configuration to my instance, CloudWatch log shows me the script has been executed successfully, but when I restart my instance and print out the panda version, it's still showing the old version 0.24.2, I don't understand why?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fR82t.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fR82t.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>This is the script in the lifecycle configuration:<\/p>\n<pre><code>#!\/bin\/bash\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\npip install pandas\n\nconda update pandas\n\nsource deactivate\n\nEOF\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593901036277,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":106.0,
        "Answer_body":"<p>You are not activating any conda environment such as <a href=\"https:\/\/stackoverflow.com\/questions\/60036916\/sagemaker-lifecycle-configuration-for-installing-pandas-not-working\">python3<\/a>.<\/p>\n<pre><code>#!\/bin\/bash\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\n# This will affect only the Jupyter kernel called &quot;conda_python3&quot;.\nsource activate python3\n\npip install pandas\n\nconda update pandas\n\nsource deactivate\n\nEOF\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1605563749572,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62734994",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1594359260763,
        "Question_original_content":"packag updat lifecycl script execut successfulli want updat panda version conda python follow step page link new configur instanc cloudwatch log show script execut successfulli restart instanc print panda version show old version understand script lifecycl configur bin bash sudo user eof pip instal panda conda updat panda sourc deactiv eof",
        "Question_preprocessed_content":"packag updat lifecycl script execut successfulli want updat panda version follow step page link new configur instanc cloudwatch log show script execut successfulli restart instanc print panda version show old version understand script lifecycl configur",
        "Question_gpt_summary_original":"The user encountered a challenge in updating the pandas version in 'conda-python3' in SageMaker. The user followed the steps in the provided page and linked the new configuration to their instance. The CloudWatch log showed that the script was executed successfully, but upon restarting the instance and checking the pandas version, it still showed the old version. The user is unsure why this is happening.",
        "Question_gpt_summary":"user encount challeng updat panda version conda python user follow step provid page link new configur instanc cloudwatch log show script execut successfulli restart instanc check panda version show old version user unsur happen",
        "Answer_original_content":"activ conda environ python bin bash sudo user eof affect jupyt kernel call conda python sourc activ python pip instal panda conda updat panda sourc deactiv eof",
        "Answer_preprocessed_content":"activ conda environ python",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to activate the conda environment such as 'python3' before installing and updating pandas. The provided script should be modified to include the command 'source activate python3' before installing and updating pandas, and 'source deactivate' after completing the installation and update.",
        "Answer_gpt_summary":"solut possibl solut mention discuss activ conda environ python instal updat panda provid script modifi includ command sourc activ python instal updat panda sourc deactiv complet instal updat"
    },
    {
        "Question_title":"How to configure comet (comet.ml) to track Keras?",
        "Question_body":"<p>im trying to setup <a href=\"https:\/\/www.comet.ml\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml<\/a> to log my experiment details <\/p>\n\n<p>getting strange error:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"train.py\", line 7, in &lt;module&gt;\n    from comet_ml import Experiment\nImportError: No module named comet_ml\n<\/code><\/pre>\n\n<p>trying in python 2 and python3<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1506066265147,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1506066568087,
        "Question_score":3.0,
        "Question_view_count":1208.0,
        "Answer_body":"<p>it seems like comet isn't installed on your machine.<\/p>\n\n<p>you can use :<\/p>\n\n<pre><code>pip3 install comet_ml\npip install comet_ml\n<\/code><\/pre>\n\n<p>take a look at the example projects at: <\/p>\n\n<p><a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\" rel=\"nofollow noreferrer\">https:\/\/github.com\/comet-ml\/comet-quickstart-guide<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1513514919392,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46359436",
        "Tool":"Comet",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1506066553020,
        "Question_original_content":"configur track kera try setup http log experi detail get strang error traceback recent file train line import experi importerror modul name try python python",
        "Question_preprocessed_content":"configur track kera try setup log experi detail get strang error try python python",
        "Question_gpt_summary_original":"The user is encountering an error while trying to set up comet.ml to log their experiment details. They are receiving an ImportError message stating that there is no module named comet_ml, despite trying in both Python 2 and Python 3.",
        "Question_gpt_summary":"user encount error try set log experi detail receiv importerror messag state modul name despit try python python",
        "Answer_original_content":"like isn instal machin us pip instal pip instal look exampl project http github com quickstart guid",
        "Answer_preprocessed_content":"like isn instal machin us look exampl project",
        "Answer_gpt_summary_original":"Solution: The user can install comet_ml by running either \"pip3 install comet_ml\" or \"pip install comet_ml\" in their terminal. They can also refer to the example projects provided in the quickstart guide on GitHub.",
        "Answer_gpt_summary":"solut user instal run pip instal pip instal termin refer exampl project provid quickstart guid github"
    },
    {
        "Question_title":"Dvc garbage collector permissions for remote SSH",
        "Question_body":"<p>Is there a way to prevent people to run dvc gc with the \u201c\u2013cloud\u201d option ?<\/p>\n<p>To give a bit more context, we would want to put dvc in place in our company with a SSH remote storage but we realized there was no user management in DVC for now. So every people would be able to push to the remote, but also to garbage collect. For data safety  purposes, this is not something we want. To the best of our knowledge, there is also no possibility to restrict directly on the SSH storage side beause there is no way to authorize write access but no deletion access.<\/p>\n<p>What we would want is to have only an \u201cadmin\u201d user that could run this garbage collection on the remote.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1615400634250,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":345.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/alombard\">@alombard<\/a>, DVC doesn\u2019t provide any file permission layer on remotes, as each one already has it\u2019s own controls in place. In the case of SSH that would be Linux permissions (rwx) for example.<\/p>\n<p>In this case unfortunately if you give users write permission that implies they can delete their own files too. One idea is to setup some batch process (e.g. a cron) to change the file owner of pushed data, which should prevent others from deleting it if the perms are something like 755 (default).<\/p>. <p>Another option could be to provide a limited dvc-shell to be invoked via ~\/.ssh\/authorized_keys like gitolite does.  That would be handy for providing a public, read-only ssh backend.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-garbage-collector-permissions-for-remote-ssh\/700",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-10T20:34:01.889Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/alombard\">@alombard<\/a>, DVC doesn\u2019t provide any file permission layer on remotes, as each one already has it\u2019s own controls in place. In the case of SSH that would be Linux permissions (rwx) for example.<\/p>\n<p>In this case unfortunately if you give users write permission that implies they can delete their own files too. One idea is to setup some batch process (e.g. a cron) to change the file owner of pushed data, which should prevent others from deleting it if the perms are something like 755 (default).<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-16T14:10:23.898Z",
                "Answer_body":"<p>Another option could be to provide a limited dvc-shell to be invoked via ~\/.ssh\/authorized_keys like gitolite does.  That would be handy for providing a public, read-only ssh backend.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"garbag collector permiss remot ssh wai prevent peopl run cloud option bit context want place compani ssh remot storag realiz user manag peopl abl push remot garbag collect data safeti purpos want best knowledg possibl restrict directli ssh storag beaus wai author write access delet access want admin user run garbag collect remot",
        "Question_preprocessed_content":"garbag collector permiss remot ssh wai prevent peopl run cloud option bit context want place compani ssh remot storag realiz user manag peopl abl push remot garbag collect data safeti purpos want best knowledg possibl restrict directli ssh storag beaus wai author write access delet access want admin user run garbag collect remot",
        "Question_gpt_summary_original":"The user is facing a challenge in preventing people from running dvc gc with the \"--cloud\" option. They want to implement dvc in their company with a SSH remote storage, but there is no user management in DVC, which means anyone can push to the remote and garbage collect. The user wants to restrict garbage collection to only an \"admin\" user for data safety purposes, but there is no way to authorize write access without deletion access on the SSH storage side.",
        "Question_gpt_summary":"user face challeng prevent peopl run cloud option want implement compani ssh remot storag user manag mean push remot garbag collect user want restrict garbag collect admin user data safeti purpos wai author write access delet access ssh storag",
        "Answer_original_content":"alombard doesnt provid file permiss layer remot control place case ssh linux permiss rwx exampl case unfortun user write permiss impli delet file idea setup batch process cron chang file owner push data prevent delet perm like default option provid limit shell invok ssh author kei like gitolit handi provid public read ssh backend",
        "Answer_preprocessed_content":"doesnt provid file permiss layer remot control place case ssh linux permiss exampl case unfortun user write permiss impli delet file idea setup batch process chang file owner push data prevent delet perm like option provid limit shell invok like gitolit handi provid public ssh backend",
        "Answer_gpt_summary_original":"Solutions provided in the discussion:\n- Set up a batch process to change the file owner of pushed data to prevent others from deleting it if the permissions are set to default.\n- Provide a limited dvc-shell to be invoked via ~\/.ssh\/authorized_keys like gitolite does to provide a public, read-only SSH backend.\n\nNo personal opinions or biases were included in the response.",
        "Answer_gpt_summary":"solut provid discuss set batch process chang file owner push data prevent delet permiss set default provid limit shell invok ssh author kei like gitolit provid public read ssh backend person opinion bias includ respons"
    },
    {
        "Question_title":"Importing from a git repo, then pulling",
        "Question_body":"<p>Hi all!<\/p>\n<p>I\u2019ve got a dvc repo which <code>dvc import<\/code>s a directory from another normal git repo. This works fine.<\/p>\n<p>However if I clone the dvc repo (or do a <code>git clean -xdf<\/code> to simulate) then run <code>dvc pull<\/code>, I can\u2019t pull the imports properly. I notice that they\u2019re not in my remote: presumably that\u2019s because dvc is getting them from the original git repo url instead which is fine.<\/p>\n<p>The error I get is:<\/p>\n<pre><code>$ dvc pull imported_steps\/step_A.dvc \nERROR: unexpected error - 'ExternalGitRepo' object has no attribute 'cache'\n<\/code><\/pre>\n<p>The associated .dvc file is<\/p>\n<pre><code>md5: 89ee29a257a80e53398e0927103f9c40\nlocked: true\ndeps:\n- path: src\n  repo:\n    url: \/path\/to\/original\/repo\/step_A\n    rev_lock: 4244a25681d4e9dcbd9795e9406c9d0734dac3f9\nouts:\n- md5: d3a51ab352355f4f39f5adf02bcd698a.dir\n  path: step_A\n  cache: true\n  metric: false\n  persist: false<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1581796222088,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":607.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/cbaynham\">@cbaynham<\/a> and thanks for the question! Sorry for the delay\u2026<\/p>\n<p>This sounds like a bug in something that is a recent feature (importing Git-tracked files).<\/p>\n<p>Do you mind opening this report as a bug in <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\">https:\/\/github.com\/iterative\/dvc\/issues<\/a> (including DVC version, etc.)?<\/p>. <p>Cheers Jorge, have done in <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/3377\" rel=\"nofollow noopener\">https:\/\/github.com\/iterative\/dvc\/issues\/3377<\/a><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/importing-from-a-git-repo-then-pulling\/320",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-02-20T22:49:23.903Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/cbaynham\">@cbaynham<\/a> and thanks for the question! Sorry for the delay\u2026<\/p>\n<p>This sounds like a bug in something that is a recent feature (importing Git-tracked files).<\/p>\n<p>Do you mind opening this report as a bug in <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\">https:\/\/github.com\/iterative\/dvc\/issues<\/a> (including DVC version, etc.)?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-02-21T15:09:13.212Z",
                "Answer_body":"<p>Cheers Jorge, have done in <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/3377\" rel=\"nofollow noopener\">https:\/\/github.com\/iterative\/dvc\/issues\/3377<\/a><\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"import git repo pull iv got repo import directori normal git repo work fine clone repo git clean xdf simul run pull pull import properli notic theyr remot presum that get origin git repo url instead fine error pull import step step error unexpect error externalgitrepo object attribut cach associ file eeaaeefc lock true dep path src repo url path origin repo step rev lock adedcbdecddacf out daabfffadfbcda dir path step cach true metric fals persist fals",
        "Question_preprocessed_content":"import git repo pull iv got repo directori normal git repo work fine clone repo run pull import properli notic theyr remot presum that get origin git repo url instead fine error associ file",
        "Question_gpt_summary_original":"The user is facing challenges while importing from a git repo and then pulling it. The user is unable to pull the imports properly and is getting an error message stating that the 'ExternalGitRepo' object has no attribute 'cache'. The associated .dvc file is also provided.",
        "Question_gpt_summary":"user face challeng import git repo pull user unabl pull import properli get error messag state externalgitrepo object attribut cach associ file provid",
        "Answer_original_content":"cbaynham thank question sorri delai sound like bug recent featur import git track file mind open report bug http github com iter issu includ version cheer jorg http github com iter issu",
        "Answer_preprocessed_content":"thank question sorri delai sound like bug recent featur mind open report bug cheer jorg",
        "Answer_gpt_summary_original":"Solution: No solution is provided in the discussion. The user is advised to report the issue as a bug in the DVC repository.",
        "Answer_gpt_summary":"solut solut provid discuss user advis report issu bug repositori"
    },
    {
        "Question_title":"Confirming endpoints were deleted in SageMaker notebook",
        "Question_body":"<p>I'm just testing out AWS Sagemaker notebook and created an endpoint using a partial script below:<\/p>\n\n<pre><code>endpoint_name = 'engine' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nendpoint_config_name = 'engine_config' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nmodel_name = 'engine_model' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n\nwhile status=='Creating':\n    time.sleep(60)\n    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n    status = resp['EndpointStatus']\n    print(\"Status: \" + status)\n\n<\/code><\/pre>\n\n<p>I'm trying to remove that endpoint by using:\nsm_client.delete_endpoint(EndpointName=endpoint_name)<\/p>\n\n<p>However, it didn't work because I naively used timestamps for the endpoint_name and I didn't remember them. The original variable values were overriden when I re-run the code. As a result, I can't delete the existing endpoint.\nI went to the Sagemaker management dashboard --> inference --> endpoints, but it's empty. I don't even know if I'm currently having any active endpoints or not. Please advise how to delete my endpoint in this case. Thank you in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1574199115877,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1574275275427,
        "Question_score":1.0,
        "Question_view_count":661.0,
        "Answer_body":"<p><strong>If there are no endpoints active under the \"Endpoints\" tab in the SageMaker service console, then you will not be incurring any charges for inference or endpoint infrastructure.<\/strong><\/p>\n\n<p>If this is the case, your Endpoints tab should look like the following:\n<a href=\"https:\/\/i.stack.imgur.com\/5QvEn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5QvEn.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Endpoint Configurations<\/strong>, on the other hand, involve the metadata necessary for an endpoint deployment. This is just the metadata, and are stored (without cost) in your account, visible in the console under the \"Endpoint Configurations\" tab. You do not need to remove these configurations when tearing down an endpoint.<\/p>\n\n<p><strong>Important note:<\/strong> Double check that you are checking in the console for the <em>region you would have deployed to<\/em>. For example, if you ran the notebook and deployed an endpoint in <code>us-east-1<\/code>, but check the SageMaker console for <code>us-west-2<\/code>, it would not be displaying endpoints from the other region.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58943117",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1574200291200,
        "Question_original_content":"confirm endpoint delet notebook test notebook creat endpoint partial script endpoint engin strftime gmtime endpoint config engin config strftime gmtime model engin model strftime gmtime statu creat time sleep resp client endpoint endpointnam endpoint statu resp endpointstatu print statu statu try remov endpoint client delet endpoint endpointnam endpoint work naiv timestamp endpoint rememb origin variabl valu overriden run code result delet exist endpoint went manag dashboard infer endpoint know current have activ endpoint advis delet endpoint case thank advanc",
        "Question_preprocessed_content":"confirm endpoint delet notebook test notebook creat endpoint partial script try remov endpoint work naiv timestamp rememb origin variabl valu overriden code result delet exist endpoint went manag dashboard infer endpoint know current have activ endpoint advis delet endpoint case thank advanc",
        "Question_gpt_summary_original":"The user created an endpoint in AWS SageMaker notebook using a script with timestamp-based variable names. When attempting to delete the endpoint using the endpoint name, the user realized they did not remember the original variable values and could not delete the existing endpoint. The user checked the SageMaker management dashboard but found it empty and is unsure if they have any active endpoints. The user is seeking advice on how to delete the endpoint in this case.",
        "Question_gpt_summary":"user creat endpoint notebook script timestamp base variabl name attempt delet endpoint endpoint user realiz rememb origin variabl valu delet exist endpoint user check manag dashboard unsur activ endpoint user seek advic delet endpoint case",
        "Answer_original_content":"endpoint activ endpoint tab servic consol incur charg infer endpoint infrastructur case endpoint tab look like follow endpoint configur hand involv metadata necessari endpoint deploy metadata store cost account visibl consol endpoint configur tab need remov configur tear endpoint import note doubl check check consol region deploi exampl ran notebook deploi endpoint east check consol west displai endpoint region",
        "Answer_preprocessed_content":"endpoint activ endpoint tab servic consol incur charg infer endpoint infrastructur case endpoint tab look like follow endpoint configur hand involv metadata necessari endpoint deploy metadata store account visibl consol endpoint configur tab need remov configur tear endpoint import note doubl check check consol region deploi exampl ran notebook deploi endpoint check consol displai endpoint region",
        "Answer_gpt_summary_original":"Solution:\n- If there are no endpoints active under the \"Endpoints\" tab in the SageMaker service console, then the user will not be incurring any charges for inference or endpoint infrastructure.\n- Endpoint Configurations involve the metadata necessary for an endpoint deployment and are stored (without cost) in the user's account, visible in the console under the \"Endpoint Configurations\" tab. The user does not need to remove these configurations when tearing down an endpoint.\n- Double check that the user is checking in the console for the region they would have deployed to.",
        "Answer_gpt_summary":"solut endpoint activ endpoint tab servic consol user incur charg infer endpoint infrastructur endpoint configur involv metadata necessari endpoint deploy store cost user account visibl consol endpoint configur tab user need remov configur tear endpoint doubl check user check consol region deploi"
    },
    {
        "Question_title":"Azure Machine Learning Request Response latency",
        "Question_body":"<p>I have made an Azure Machine Learning Experiment which takes a small dataset (12x3 array) and some parameters and does some calculations using a few Python modules (a linear regression calculation and some more). This all works fine.<\/p>\n\n<p>I have deployed the experiment and now want to throw data at it from the front-end of my application. The API-call goes in and comes back with correct results, but it takes up to 30 seconds to calculate a simple linear regression. Sometimes it is 20 seconds, sometimes only 1 second. I even got it down to 100 ms one time (which is what I'd like), but 90% of the time the request takes more than 20 seconds to complete, which is unacceptable.<\/p>\n\n<p>I guess it has something to do with it still being an experiment, or it is still in a development slot, but I can't find the settings to get it to run on a faster machine.<\/p>\n\n<p>Is there a way to speed up my execution?<\/p>\n\n<p>Edit: To clarify: The varying timings are obtained with the same test data, simply by sending the same request multiple times. This made me conclude it must have something to do with my request being put in a queue, there is some start-up latency or I'm throttled in some other way.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1453718439993,
        "Question_favorite_count":5.0,
        "Question_last_edit_time":1453911336527,
        "Question_score":8.0,
        "Question_view_count":1128.0,
        "Answer_body":"<p>First, I am assuming you are doing your timing test on the published AML endpoint.<\/p>\n\n<p>When a call is made to the AML the first call must warm up the container. By default a web service has 20 containers. Each container is cold, and a cold container can cause a large(30 sec) delay. In the string returned by the AML endpoint, only count requests that have the <code>isWarm<\/code> flag set to true. By smashing the service with MANY requests(relative to how many containers you have running) can get all your containers warmed.<\/p>\n\n<p>If you are sending out dozens of requests a instance, the endpoint might be getting throttled. You can adjust the number of calls your endpoint can accept by going to manage.windowsazure.com\/<\/p>\n\n<ol>\n<li>manage.windowsazure.com\/<\/li>\n<li>Azure ML Section from left bar<\/li>\n<li>select your workspace<\/li>\n<li>go to web services tab<\/li>\n<li>Select your web service from list<\/li>\n<li>adjust the number of calls with slider<\/li>\n<\/ol>\n\n<p>By enabling debugging onto your endpoint you can get logs about the execution time for each of your modules to complete. You can use this to determine if a module is not running as you intended which may add to the time.<\/p>\n\n<p>Overall, there is an overhead when using the Execute python module, but I'd expect this request to complete in under 3 secs. <\/p>",
        "Answer_comment_count":11.0,
        "Answer_last_edit_time":1453911048927,
        "Answer_score":8.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34990561",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1453832406127,
        "Question_original_content":"request respons latenc experi take small dataset arrai paramet calcul python modul linear regress calcul work fine deploi experi want throw data end applic api goe come correct result take second calcul simpl linear regress second second got time like time request take second complet unaccept guess experi develop slot set run faster machin wai speed execut edit clarifi vari time obtain test data simpli send request multipl time conclud request queue start latenc throttl wai",
        "Question_preprocessed_content":"request respons latenc experi take small dataset paramet calcul python modul work fine deploi experi want throw data applic goe come correct result take second calcul simpl linear regress second second got time time request take second complet unaccept guess experi develop slot set run faster machin wai speed execut edit clarifi vari time obtain test data simpli send request multipl time conclud request queue latenc throttl wai",
        "Question_gpt_summary_original":"The user has encountered challenges with the response latency of their Azure Machine Learning Experiment API. Although the API-call returns correct results, the time taken to calculate a simple linear regression varies from 1 second to 30 seconds, which is unacceptable. The user suspects that the issue may be due to the experiment still being in a development slot or being run on a slower machine. The user is seeking a way to speed up the execution.",
        "Question_gpt_summary":"user encount challeng respons latenc experi api api return correct result time taken calcul simpl linear regress vari second second unaccept user suspect issu experi develop slot run slower machin user seek wai speed execut",
        "Answer_original_content":"assum time test publish aml endpoint aml warm contain default web servic contain contain cold cold contain caus larg sec delai string return aml endpoint count request iswarm flag set true smash servic request rel contain run contain warm send dozen request instanc endpoint get throttl adjust number call endpoint accept go manag windowsazur com manag windowsazur com section left bar select workspac web servic tab select web servic list adjust number call slider enabl debug endpoint log execut time modul complet us determin modul run intend add time overal overhead execut python modul expect request complet sec",
        "Answer_preprocessed_content":"assum time test publish aml endpoint aml warm contain default web servic contain contain cold cold contain caus larg delai string return aml endpoint count request flag set true smash servic request contain warm send dozen request instanc endpoint get throttl adjust number call endpoint accept go section left bar select workspac web servic tab select web servic list adjust number call slider enabl debug endpoint log execut time modul complet us determin modul run intend add time overal overhead execut python modul expect request complet sec",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Warm up the container by sending many requests with the `isWarm` flag set to true to get all the containers warmed.\n- Adjust the number of calls the endpoint can accept by going to manage.windowsazure.com.\n- Enable debugging on the endpoint to get logs about the execution time for each module to determine if a module is not running as intended, which may add to the time.",
        "Answer_gpt_summary":"possibl solut mention discuss warm contain send request iswarm flag set true contain warm adjust number call endpoint accept go manag windowsazur com enabl debug endpoint log execut time modul determin modul run intend add time"
    },
    {
        "Question_title":"Is there a way to pass arguments to our own docker container in sagemaker?",
        "Question_body":"<p>I am trying to train my model using Bring your own container technique in sagemaker. My model training runs correctly without any issues locally. But my docker image takes env-file as an input that could change at different runs. But in sagemaker when passing the ECR image, I don't know how to pass this env-file. So instead, inside the <code>train<\/code> script, which is called by the sagemaker, I added <code>export KEY=value<\/code> statements to create my variables. Even that did not expose my variables. Another way I tried it was by executing <code>RUN source file.env<\/code> while building my image. Even this approach did not work out as I got an error <code>\/bin\/sh: 1: source: not found<\/code>.<\/p>\n<p>I could try <code>ENV<\/code> while building my image and that would probably work but this approach won't be flexible as my variables could change at different runs. Is there any way to pass docker run arguments from a sagemaker estimator or notebook? I checked out the documentation but I couldn't find anything.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1634032422110,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":292.0,
        "Answer_body":"<p>I've been passing environment variables along with the Docker image URL when creating the Training job using the SageMaker Python SDK. Documentation of the <code>train<\/code> method states that:<\/p>\n<pre><code>environment (dict[str, str]) : Environment variables to be set for\n            use during training job (default: ``None``): \n<\/code><\/pre>\n<p>For reference, the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/5bc3ccf\/src\/sagemaker\/session.py#L569\" rel=\"nofollow noreferrer\">SDK source<\/a>.<\/p>\n<p>Because the SDK is a wrapper on top of <a href=\"https:\/\/pypi.org\/project\/boto3\/\" rel=\"nofollow noreferrer\">Boto3<\/a>, I'm pretty sure that the same can be implemented with Boto3 alone, and that there is an equivalent for every other <a href=\"https:\/\/aws.amazon.com\/developer\/tools\/#SDKs\" rel=\"nofollow noreferrer\">Amazon Services SDK<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69538469",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1662661497360,
        "Question_original_content":"wai pass argument docker contain try train model bring contain techniqu model train run correctli issu local docker imag take env file input chang differ run pass ecr imag know pass env file instead insid train script call ad export kei valu statement creat variabl expos variabl wai tri execut run sourc file env build imag approach work got error bin sourc try env build imag probabl work approach won flexibl variabl chang differ run wai pass docker run argument estim notebook check document couldn",
        "Question_preprocessed_content":"wai pass argument docker contain try train model bring contain techniqu model train run correctli issu local docker imag take input chang differ run pass ecr imag know pass instead insid script call ad statement creat variabl expos variabl wai tri execut build imag approach work got error try build imag probabl work approach won flexibl variabl chang differ run wai pass docker run argument estim notebook check document couldn",
        "Question_gpt_summary_original":"The user is facing challenges in passing arguments to their own docker container in Sagemaker while using the Bring your own container technique. The docker image takes an env-file as an input that could change at different runs, but the user does not know how to pass this env-file while passing the ECR image. The user tried adding export KEY=value statements to create variables, but it did not expose the variables. The user also tried executing RUN source file.env while building the image, but it resulted in an error. The user is looking for a way to pass docker run arguments from a Sagemaker estimator or notebook, but could not find any documentation on it.",
        "Question_gpt_summary":"user face challeng pass argument docker contain bring contain techniqu docker imag take env file input chang differ run user know pass env file pass ecr imag user tri ad export kei valu statement creat variabl expos variabl user tri execut run sourc file env build imag result error user look wai pass docker run argument estim notebook document",
        "Answer_original_content":"pass environ variabl docker imag url creat train job python sdk document train method state environ dict str str environ variabl set us train job default refer sdk sourc sdk wrapper boto pretti sure implement boto equival amazon servic sdk",
        "Answer_preprocessed_content":"pass environ variabl docker imag url creat train job python sdk document method state refer sdk sourc sdk wrapper boto pretti sure implement boto equival amazon servic sdk",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to pass environment variables along with the Docker image URL when creating the training job using the SageMaker Python SDK. The SDK provides an option to set environment variables for use during the training job. It is also suggested that the same can be implemented with Boto3 alone, and that there is an equivalent for every other Amazon Services SDK.",
        "Answer_gpt_summary":"solut possibl solut mention discuss pass environ variabl docker imag url creat train job python sdk sdk provid option set environ variabl us train job suggest implement boto equival amazon servic sdk"
    },
    {
        "Question_title":"Customvision run trained tensorflow model in Python: Placeholder:0 refers to a non existing tensor in image classification",
        "Question_body":"Hi all,\n\nI have trained an image classifier with the customvision service, which worked as charm. Now I would like to run the model inference locally with a python script on my PC. Therefore I have been following the tutorial on https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/custom-vision-service\/export-model-python\n\nI am having troubles with sess.graph.get_tensor_by_name('Placeholder:0').shape.as_list()\n\nCould you please provide some information on the system requirements and the python package versions? An openCV 4.5.1 C++ code snippet on how to consume the downloaded model would be also great if possible.\n\nI am using Python 3.8.5\n\nThank you",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1621939072900,
        "Question_favorite_count":9.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Thank you, will do.\n\nI have solved the issue with using C++ openCV instead and WinML also helps with rapid prototyping.\n\nThis was a particularly good example I have found:\n\nhttps:\/\/github.com\/Azure-Samples\/cognitive-services-onnx-customvision-sample\n\nWould be great to have more of those.\n\nBest.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/408585\/customvision-run-trained-tensorflow-model-in-pytho.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-05-26T07:34:21.963Z",
                "Answer_score":0,
                "Answer_body":"Thank you, will do.\n\nI have solved the issue with using C++ openCV instead and WinML also helps with rapid prototyping.\n\nThis was a particularly good example I have found:\n\nhttps:\/\/github.com\/Azure-Samples\/cognitive-services-onnx-customvision-sample\n\nWould be great to have more of those.\n\nBest.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1622014461963,
        "Question_original_content":"customvis run train tensorflow model python placehold refer non exist tensor imag classif train imag classifi customvis servic work charm like run model infer local python script follow tutori http doc microsoft com azur cognit servic custom vision servic export model python have troubl sess graph tensor placehold shape list provid inform requir python packag version opencv code snippet consum download model great possibl python thank",
        "Question_preprocessed_content":"customvis run train tensorflow model python placehold refer non exist tensor imag classif train imag classifi customvis servic work charm like run model infer local python script follow tutori have troubl provid inform requir python packag version opencv code snippet consum download model great possibl python thank",
        "Question_gpt_summary_original":"The user has encountered challenges while trying to run a locally trained image classifier model using a Python script on their PC. They are specifically having trouble with the 'Placeholder:0' tensor and are seeking information on system requirements, python package versions, and an OpenCV 4.5.1 C++ code snippet to consume the downloaded model. The user is using Python 3.8.5.",
        "Question_gpt_summary":"user encount challeng try run local train imag classifi model python script specif have troubl placehold tensor seek inform requir python packag version opencv code snippet consum download model user python",
        "Answer_original_content":"thank solv issu opencv instead winml help rapid prototyp particularli good exampl http github com azur sampl cognit servic onnx customvis sampl great best",
        "Answer_preprocessed_content":"thank solv issu opencv instead winml help rapid prototyp particularli good exampl great best",
        "Answer_gpt_summary_original":"Solutions provided:\n- Using C++ OpenCV instead of Python script to run the locally trained image classifier model.\n- Using WinML for rapid prototyping.\n- Providing a GitHub link to a good example of using cognitive services ONNX custom vision.",
        "Answer_gpt_summary":"solut provid opencv instead python script run local train imag classifi model winml rapid prototyp provid github link good exampl cognit servic onnx custom vision"
    },
    {
        "Question_title":"Moving Azure Machine Learning Studio jobs to a new region",
        "Question_body":"I need to move my Machine Learning Studio workspace to a new region. I am aware that the move function doesn't allow automatically moving to a new region, so I'll have to create a new workspace. That's not a big problem, but I still want to keep my job\/experiment history (in my new workspace). How can I do that?",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1662467585447,
        "Question_favorite_count":11.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @David-3633\n\nSorry, I just got confimation from product team, this is currently impossible. I am sorry for the inconvenience.\n\nA near future workaround which could let users at least share some experiment outputs\/inputs like environments, models, datasets cross region, but not the jobs\/metrics\/logs themselves. This feature is in private preview now and will be in public preview soon.\n\nI hope this information helps.\n\n\n\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/995833\/moving-azure-machine-learning-studio-jobs-to-a-new.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-27T15:12:08.087Z",
                "Answer_score":0,
                "Answer_body":"Hello @David-3633\n\nSorry, I just got confimation from product team, this is currently impossible. I am sorry for the inconvenience.\n\nA near future workaround which could let users at least share some experiment outputs\/inputs like environments, models, datasets cross region, but not the jobs\/metrics\/logs themselves. This feature is in private preview now and will be in public preview soon.\n\nI hope this information helps.\n\n\n\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1664291528087,
        "Question_original_content":"move studio job new region need machin learn studio workspac new region awar function allow automat move new region creat new workspac big problem want job experi histori new workspac",
        "Question_preprocessed_content":"move studio job new region need machin learn studio workspac new region awar function allow automat move new region creat new workspac big problem want histori",
        "Question_gpt_summary_original":"The user needs to move their Azure Machine Learning Studio workspace to a new region, but the move function doesn't allow for automatic moving. They will have to create a new workspace, but they want to keep their job\/experiment history in the new workspace. The user is seeking advice on how to accomplish this.",
        "Question_gpt_summary":"user need studio workspac new region function allow automat move creat new workspac want job experi histori new workspac user seek advic accomplish",
        "Answer_original_content":"hello david sorri got confim product team current imposs sorri inconveni near futur workaround let user share experi output input like environ model dataset cross region job metric log featur privat preview public preview soon hope inform help regard yutong kindli accept answer feel help support commun thank lot",
        "Answer_preprocessed_content":"hello sorri got confim product team current imposs sorri inconveni near futur workaround let user share experi like environ model dataset cross region featur privat preview public preview soon hope inform help regard yutong kindli accept answer feel help support commun thank lot",
        "Answer_gpt_summary_original":"Solution: Unfortunately, there is currently no automatic way to move an Azure Machine Learning Studio workspace to a new region while keeping job\/experiment history. However, there is a future workaround in private preview that will allow users to share experiment outputs\/inputs like environments, models, and datasets cross-region, but not the jobs\/metrics\/logs themselves. This feature will be in public preview soon.",
        "Answer_gpt_summary":"solut unfortun current automat wai studio workspac new region keep job experi histori futur workaround privat preview allow user share experi output input like environ model dataset cross region job metric log featur public preview soon"
    },
    {
        "Question_title":"AWS Sagemaker inference endpoint not utilizing all vCPUs",
        "Question_body":"<p>I have deployed a custom model on sagemaker inference endpoint (single instance) and while I was load testing, I have observed that CPU utilization metric is maxing out at 100% but according to <a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/sagemaker-cpu-gpu-utilization-100\/\" rel=\"nofollow noreferrer\">this post<\/a> it should max out at #vCPU*100 %. I have confirmed that the inference endpoint is not using all cores in clowdwatch logs.<\/p>\n<p>So if one prediction call requires one second to be processed to give response, the deployed model is only able to handle one API call per second which could have been increased to 8 calls per second if all vCPUs would have been used.<\/p>\n<p>Are there any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency?<\/p>\n<p>Or could we use multiprocessing python package inside <code>inference.py<\/code> file while deploying such that each call comes to the default core and from there all calculations\/prediction is done in any other core whichever is empty at that instance?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1624366085347,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1628443120180,
        "Question_score":3.0,
        "Question_view_count":661.0,
        "Answer_body":"<p>UPDATE<\/p>\n<ul>\n<li><p>Set three environment variables<\/p>\n<ol>\n<li>ENABLE_MULTI_MODEL as &quot;true&quot; (make sure it is string and not bool) and set <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L74\" rel=\"nofollow noreferrer\">SAGEMAKER_HANDLER<\/a> as custom model handler python module path if custom service else dont define it. Also make sure model name <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L94\" rel=\"nofollow noreferrer\">model.mar<\/a>, before compressing it as tar ball and storing in s3<\/li>\n<li>TS_DEFAULT_WORKERS_PER_MODEL as number of vcpus<\/li>\n<li>First environment variable makes sure torch serve env_vars are enabled and second one uses first setting and loads requested number of workers<\/li>\n<li>Setting can be done by passing env dictionary argument to <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-an-estimator\" rel=\"nofollow noreferrer\">PyTorch function<\/a>. Below is explanation as to why it works<\/li>\n<\/ol>\n<\/li>\n<li><p>From the looks of it, sagemaker deployment for pytorch model as given in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-an-estimator\" rel=\"nofollow noreferrer\">Sagemaker SDK guide<\/a>, uses <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu\" rel=\"nofollow noreferrer\">this dockerfile<\/a>. In this docker, entrypoint is <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">torchserve-entrypoint.py<\/a> as in <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu#L124\" rel=\"nofollow noreferrer\">Dockerfile line#124<\/a>.<\/p>\n<\/li>\n<li><p>This <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">torchserve-entrypoint.py<\/a> calls <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">serving.main()<\/a> from <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/serving.py\" rel=\"nofollow noreferrer\">serving.py<\/a>. Which ends up calling <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/serving.py#L34\" rel=\"nofollow noreferrer\">torchserve.start_torchserve(handler_service=HANDLER_SERVICE)<\/a> from <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py\" rel=\"nofollow noreferrer\">torchserve.py<\/a>.<\/p>\n<\/li>\n<li><p><a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L34\" rel=\"nofollow noreferrer\">At line 34 in torchserve.py<\/a> it defines &quot;\/etc\/default-ts.properties&quot; as DEFAULT_TS_CONFIG_FILE. This file is located <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/etc\/default-ts.properties\" rel=\"nofollow noreferrer\">here<\/a>. In this file <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/etc\/default-ts.properties#L2\" rel=\"nofollow noreferrer\">enable_envvars_config=true<\/a> is set. It will use this file setting IFF Environment variable &quot;ENABLE_MULTI_MODEL&quot; is set to &quot;false&quot; as refered <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L167\" rel=\"nofollow noreferrer\">here<\/a>. If it is set to &quot;true&quot; then it will use \/etc\/mme-ts.properties<\/p>\n<\/li>\n<\/ul>\n<hr \/>\n<p>As for the question <code>Are there any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency?<\/code>\nThere are various settings you can use\nFor models you can set <code>default_workers_per_model<\/code> in config.properties <code>TS_DEFAULT_WORKERS_PER_MODEL=$(nproc --all)<\/code> in environment variables. Environment variables take top priority.<\/p>\n<p>Other than that for each model, you can set the number of workers by using management API, but sadly it is not possible to curl to management API in sagemaker. SO TS_DEFAULT_WORKERS_PER_MODEL is the best bet.\nSetting this should make sure all cores are used.<\/p>\n<p>But if you are using docker file then in entrypoint you can setup scripts which wait for model loading and curl to it to set number of workers<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code># load the model\ncurl -X POST localhost:8081\/models?url=model_1.mar&amp;batch_size=8&amp;max_batch_delay=50\n# after loading the model it is possible to set min_worker, etc\ncurl -v -X PUT http:\/\/localhost:8081\/models\/model_1?min_worker=1\n<\/code><\/pre>\n<p>About the other issue that logs confirm that not all cores are used, I face the same issue and believe that is a problem in the logging system. Please look at this issue <a href=\"https:\/\/github.com\/pytorch\/serve\/issues\/782\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pytorch\/serve\/issues\/782<\/a>. The community itself agrees that if threads are not set, then by default then it prints 0, even if by default it uses 2*num_cores.<\/p>\n<p><strong>For an exhaustive set of all configs possible<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Reference: https:\/\/github.com\/pytorch\/serve\/blob\/master\/docs\/configuration.md\n# Variables that can be configured through config.properties and Environment Variables\n# NOTE: Variables which can be configured through environment variables **SHOULD** have a\n# &quot;TS_&quot; prefix\n# debug\ninference_address=http:\/\/0.0.0.0:8080\nmanagement_address=http:\/\/0.0.0.0:8081\nmetrics_address=http:\/\/0.0.0.0:8082\nmodel_store=\/opt\/ml\/model\nload_models=model_1.mar\n# blacklist_env_vars\n# default_workers_per_model\n# default_response_timeout\n# unregister_model_timeout\n# number_of_netty_threads\n# netty_client_threads\n# job_queue_size\n# number_of_gpu\n# async_logging\n# cors_allowed_origin\n# cors_allowed_methods\n# cors_allowed_headers\n# decode_input_request\n# keystore\n# keystore_pass\n# keystore_type\n# certificate_file\n# private_key_file\n# max_request_size\n# max_response_size\n# default_service_handler\n# service_envelope\n# model_server_home\n# snapshot_store\n# prefer_direct_buffer\n# allowed_urls\n# install_py_dep_per_model\n# metrics_format\n# enable_metrics_api\n# initial_worker_port\n\n# Configuration which are not documented or enabled through environment variables\n\n# When below variable is set true, then the variables set in environment have higher precedence.\n# For example, the value of an environment variable overrides both command line arguments and a property in the configuration file. The value of a command line argument overrides a value in the configuration file.\n# When set to false, environment variables are not used at all\n# use_native_io=\n# io_ratio=\n# metric_time_interval=\nenable_envvars_config=true\n# model_snapshot=\n# version=\n<\/code><\/pre>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":1624470584452,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68083831",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1624416005592,
        "Question_original_content":"infer endpoint util vcpu deploi custom model infer endpoint singl instanc load test observ cpu util metric max accord post max vcpu confirm infer endpoint core clowdwatch log predict requir second process respons deploi model abl handl api second increas call second vcpu set deploy us vcpu increas concurr us multiprocess python packag insid infer file deploi come default core calcul predict core whichev instanc",
        "Question_preprocessed_content":"infer endpoint util vcpu deploi custom model infer endpoint load test observ cpu util metric max accord post max vcpu confirm infer endpoint core clowdwatch log predict requir second process respons deploi model abl handl api second increas call second vcpu set deploy us vcpu increas concurr us multiprocess python packag insid file deploi come default core core whichev instanc",
        "Question_gpt_summary_original":"The user has encountered a challenge with their AWS Sagemaker inference endpoint, as it is not utilizing all vCPUs. Despite load testing, the CPU utilization metric is maxing out at 100%, which is not in line with AWS's guidelines. As a result, the deployed model is only able to handle one API call per second, instead of eight. The user is seeking advice on whether there are any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency or if they could use the multiprocessing python package inside the inference.py file to distribute the workload across multiple cores.",
        "Question_gpt_summary":"user encount challeng infer endpoint util vcpu despit load test cpu util metric max line aw guidelin result deploi model abl handl api second instead user seek advic set deploy us vcpu increas concurr us multiprocess python packag insid infer file distribut workload multipl core",
        "Answer_original_content":"updat set environ variabl enabl multi model true sure string bool set handler custom model handler python modul path custom servic dont defin sure model model mar compress tar ball store default worker model number vcpu environ variabl make sure torch serv env var enabl second us set load request number worker set pass env dictionari argument pytorch function explan work look deploy pytorch model given sdk guid us dockerfil docker entrypoint torchserv entrypoint dockerfil line torchserv entrypoint call serv main serv end call torchserv start torchserv handler servic handler servic torchserv line torchserv defin default properti default config file file locat file enabl envvar config true set us file set iff environ variabl enabl multi model set fals refer set true us mme properti question set deploy us vcpu increas concurr set us model set default worker model config properti default worker model nproc environ variabl environ variabl prioriti model set number worker manag api sadli possibl curl manag api default worker model best bet set sure core docker file entrypoint setup script wait model load curl set number worker load model curl post localhost model url model mar batch size max batch delai load model possibl set min worker curl http localhost model model min worker issu log confirm core face issu believ problem log look issu http github com pytorch serv issu commun agre thread set default print default us num core exhaust set config possibl refer http github com pytorch serv blob master doc configur variabl configur config properti environ variabl note variabl configur environ variabl prefix debug infer address http manag address http metric address http model store opt model load model model mar blacklist env var default worker model default respons timeout unregist model timeout number netti thread netti client thread job queue size number gpu async log cor allow origin cor allow method cor allow header decod input request keystor keystor pass keystor type certif file privat kei file max request size max respons size default servic handler servic envelop model server home snapshot store prefer direct buffer allow url instal dep model metric format enabl metric api initi worker port configur document enabl environ variabl variabl set true variabl set environ higher preced exampl valu environ variabl overrid command line argument properti configur file valu command line argument overrid valu configur file set fals environ variabl us nativ ratio metric time interv enabl envvar config true model snapshot version",
        "Answer_preprocessed_content":"updat set environ variabl true set custom model handler python modul path custom servic dont defin sure model compress tar ball store number vcpu environ variabl make sure torch serv enabl second us set load request number worker set pass env dictionari argument pytorch function explan work look deploy pytorch model given sdk guid us dockerfil docker entrypoint dockerfil line call end call line defin file locat file set us file set iff environ variabl set fals refer set true us question set us model set environ variabl environ variabl prioriti model set number worker manag api sadli possibl curl manag api best bet set sure core docker file entrypoint setup script wait model load curl set number worker issu log confirm core face issu believ problem log look issu commun agre thread set default print default us exhaust set config possibl",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Set three environment variables: ENABLE_MULTI_MODEL as \"true\", TS_DEFAULT_WORKERS_PER_MODEL as the number of vCPUs, and SAGEMAKER_HANDLER as a custom model handler python module path if custom service else don't define it. This setting can be done by passing env dictionary argument to PyTorch function. \n\n2. For models, set default_workers_per_model in config.properties TS_DEFAULT_WORKERS_PER_MODEL=$(nproc --all) in environment variables. Other than that, for each model, set the number of workers by using management API, but it is not possible to curl to management API in Sagemaker. \n\n3. If using a docker file, in entrypoint",
        "Answer_gpt_summary":"possibl solut mention discuss set environ variabl enabl multi model true default worker model number vcpu handler custom model handler python modul path custom servic defin set pass env dictionari argument pytorch function model set default worker model config properti default worker model nproc environ variabl model set number worker manag api possibl curl manag api docker file entrypoint"
    },
    {
        "Question_title":"Automating Azure Machine Learning",
        "Question_body":"<p>Is there a way of automating the calls to the Azure Machine Learning Service (AML)? <\/p>\n\n<p>I\u2019ve created the web service from AML. Now I have to do the calls the automated way. I\u2019m trying to build a system, that connects to a Raspberry Pi for sensor data and gets a prediction from the ML service to be saved with the data itself. <\/p>\n\n<p>Is there something in Azure to automate this or should I do it within the application?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1459095760807,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1459199614030,
        "Question_score":0.0,
        "Question_view_count":341.0,
        "Answer_body":"<p>I'm assuming you've created the webservice from the experiment and asking about the consumption of the webservice. You can consume the webservice from anything that can do an API call to the endpoint. I don't know the exact architecture of your solution but take a look at this as it might suit your scenario. <\/p>\n\n<p>Stream analytics on Azure has a new feature called Functions(just a heads-up, its still in preview) that can automate the usage of deployed ML services from your account.Since you are trying to gather info from IoT devices, you might use <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/event-hubs-csharp-ephcs-getstarted\/\" rel=\"nofollow\">Event Hubs<\/a> or <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/iot-hub-csharp-csharp-getstarted\/\" rel=\"nofollow\">IoT Hubs<\/a> to get the data and process it using Stream Analytics and during the process you can use the Webservice as Function in SA to achieve on-the-go ML results.<\/p>\n\n<p>Usage is relatively simple if you are familiar with Stream Analytics or SQL queries in general.This <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/stream-analytics-machine-learning-integration-tutorial\/\" rel=\"nofollow\">link<\/a> shows the step by step implementation and the usage is below;<\/p>\n\n<pre><code>    WITH subquery AS (  \n    SELECT text, \"webservicealias\"(text) as result from input  \n    )  \n\n    Select text, result.[Score]  \n    Into output  \n    From subquery  \n<\/code><\/pre>\n\n<p>Hope this helps!<\/p>\n\n<p>Mert<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36249716",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1459099456052,
        "Question_original_content":"autom wai autom call servic aml iv creat web servic aml call autom wai try build connect raspberri sensor data get predict servic save data azur autom applic",
        "Question_preprocessed_content":"autom wai autom call servic iv creat web servic aml call autom wai try build connect raspberri sensor data get predict servic save data azur autom applic",
        "Question_gpt_summary_original":"The user is facing a challenge of automating calls to the Azure Machine Learning Service (AML) for a system that connects to a Raspberry Pi for sensor data and gets a prediction from the ML service to be saved with the data itself. The user is unsure whether to automate this within the application or if there is something in Azure to automate it.",
        "Question_gpt_summary":"user face challeng autom call servic aml connect raspberri sensor data get predict servic save data user unsur autom applic azur autom",
        "Answer_original_content":"assum creat webservic experi ask consumpt webservic consum webservic api endpoint know exact architectur solut look suit scenario stream analyt azur new featur call function head preview autom usag deploi servic account try gather info iot devic us event hub iot hub data process stream analyt process us webservic function achiev result usag rel simpl familiar stream analyt sql queri gener link show step step implement usag subqueri select text webservicealia text result input select text result score output subqueri hope help mert",
        "Answer_preprocessed_content":"assum creat webservic experi ask consumpt webservic consum webservic api endpoint know exact architectur solut look suit scenario stream analyt azur new featur call function autom usag deploi servic try gather info iot devic us event hub iot hub data process stream analyt process us webservic function achiev result usag rel simpl familiar stream analyt sql queri link show step step implement usag hope help mert",
        "Answer_gpt_summary_original":"Solution: The discussion suggests using Stream Analytics on Azure with a new feature called Functions to automate the usage of deployed ML services from the user's account. The user can use Event Hubs or IoT Hubs to get the data and process it using Stream Analytics. During the process, the user can use the Webservice as Function in SA to achieve on-the-go ML results. The usage is relatively simple if the user is familiar with Stream Analytics or SQL queries in general.",
        "Answer_gpt_summary":"solut discuss suggest stream analyt azur new featur call function autom usag deploi servic user account user us event hub iot hub data process stream analyt process user us webservic function achiev result usag rel simpl user familiar stream analyt sql queri gener"
    },
    {
        "Question_title":"Sweep from existing runs not showing up in parallel coordinates, is this intended or a bug?",
        "Question_body":"<p>Hi, I created a sweep from existing runs, but the panel Parallel Coordinates are empty, is this an intended behaviour or a bug?<\/p>\n<p>Here is what I did:<\/p>\n<ul>\n<li>populate projects with many runs (using ray\u2019s wandb_mixin)<\/li>\n<li>create a sweep following <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project#seed-a-new-sweep-with-existing-runs\">https:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project#seed-a-new-sweep-with-existing-runs<\/a>\n<\/li>\n<li>the panel at \u201cSweeps &gt; [2]\u201d contains only 1 run, should contains all 42 runs.<\/li>\n<\/ul>\n<p>The sweep is at <a href=\"https:\/\/wandb.ai\/inc\/try_ray_tune\/sweeps\/smh3d0wg\" class=\"inline-onebox\">Weights &amp; Biases<\/a>, if any one is interested.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1640245639181,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":201.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/inc\">@inc<\/a>,<\/p>\n<p>You should be able to see all 42 runs on your parallel coordinates plot by ungrouping the runs. Grouping runs groups them for charts on your workspace as well.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweep-from-existing-runs-not-showing-up-in-parallel-coordinates-is-this-intended-or-a-bug\/1601",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-23T21:43:17.019Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/inc\">@inc<\/a>,<\/p>\n<p>Checking the link for your run, I see all 42 runs, grouped together as one single grouped run. Are you still facing this issue?<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-24T01:21:21.696Z",
                "Answer_body":"<p>I can see all 42 runs no problem, but I expect to see them on the sweeps\u2019 parallel coordinates panel:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/7417aff14e55470287a1e936710606abe4488cbf.png\" data-download-href=\"\/uploads\/short-url\/gz08s2oOYUdETsRiUUbSDUf6ZKD.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/7417aff14e55470287a1e936710606abe4488cbf_2_690x242.png\" alt=\"image\" data-base62-sha1=\"gz08s2oOYUdETsRiUUbSDUf6ZKD\" width=\"690\" height=\"242\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/7417aff14e55470287a1e936710606abe4488cbf_2_690x242.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/7417aff14e55470287a1e936710606abe4488cbf_2_1035x363.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/7417aff14e55470287a1e936710606abe4488cbf_2_1380x484.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/7417aff14e55470287a1e936710606abe4488cbf_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1793\u00d7631 35.9 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Thanks for the reply.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-24T01:39:21.154Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/inc\">@inc<\/a>,<\/p>\n<p>You should be able to see all 42 runs on your parallel coordinates plot by ungrouping the runs. Grouping runs groups them for charts on your workspace as well.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-12-24T02:54:06.812Z",
                "Answer_body":"<p>Thanks, that solved it.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-22T02:54:52.566Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":1640309961152,
        "Question_original_content":"sweep exist run show parallel coordin intend bug creat sweep exist run panel parallel coordin intend behaviour bug popul project run rai mixin creat sweep follow http doc guid sweep exist project seed new sweep exist run panel sweep contain run contain run sweep interest",
        "Question_preprocessed_content":"sweep exist run show parallel coordin intend bug creat sweep exist run panel parallel coordin intend behaviour bug popul project run creat sweep follow panel sweep contain run contain run sweep interest",
        "Question_gpt_summary_original":"The user created a sweep from existing runs using ray's wandb_mixin, but the panel Parallel Coordinates is empty. The user wonders if this is intended or a bug. The panel at \"Sweeps > [2]\" contains only 1 run instead of all 42 runs.",
        "Question_gpt_summary":"user creat sweep exist run rai mixin panel parallel coordin user wonder intend bug panel sweep contain run instead run",
        "Answer_original_content":"abl run parallel coordin plot ungroup run group run group chart workspac thank ramit",
        "Answer_preprocessed_content":"abl run parallel coordin plot ungroup run group run group chart workspac thank ramit",
        "Answer_gpt_summary_original":"Solution: The user can ungroup the runs to see all 42 runs on the parallel coordinates plot. Grouping runs groups them for charts on the workspace as well.",
        "Answer_gpt_summary":"solut user ungroup run run parallel coordin plot group run group chart workspac"
    },
    {
        "Question_title":"How can I match my local azure automl python sdk version to the remote version?",
        "Question_body":"<p>I'm using the azure automl python sdk to download and save a model then reload it. I get the following error:<\/p>\n<pre><code>anaconda3\\envs\\automl_21\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator Pipeline from version 0.22.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n  UserWarning)\n<\/code><\/pre>\n<p>How can I ensure that the versions match?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617943623520,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":62.0,
        "Answer_body":"<p>My Microsoft contact says -<\/p>\n<p>&quot;For this, their best  bet is probably to see what the training env was pinned to and install those same pins. They can get that env by running child_run.get_environment() and then pip install all the pkgs listed in there with the pins listed there.&quot;<\/p>\n<p>A useful code snippet.<\/p>\n<pre><code>for run in experiment.get_runs():\n    tags_dictionary = run.get_tags()\n    best_run = AutoMLRun(experiment, tags_dictionary['automl_best_child_run_id'])\n    env = best_run.get_environment()\n    print(env.python.conda_dependencies.serialize_to_string())\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1618178182616,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67015185",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1618176409272,
        "Question_original_content":"match local azur automl python sdk version remot version azur automl python sdk download save model reload follow error anaconda env automl lib site packag sklearn base userwarn try unpickl estim pipelin version version post lead break code invalid result us risk userwarn ensur version match",
        "Question_preprocessed_content":"match local azur automl python sdk version remot version azur automl python sdk download save model reload follow error ensur version match",
        "Question_gpt_summary_original":"The user is encountering an error while using the Azure AutoML Python SDK to download and save a model, which warns about trying to unpickle an estimator Pipeline from version 0.22.1 when using version 0.22.2.post1. The user is seeking guidance on how to match the local and remote versions to avoid breaking code or invalid results.",
        "Question_gpt_summary":"user encount error azur automl python sdk download save model warn try unpickl estim pipelin version version post user seek guidanc match local remot version avoid break code invalid result",
        "Answer_original_content":"microsoft contact sai best bet probabl train env pin instal pin env run child run environ pip instal pkg list pin list us code snippet run experi run tag dictionari run tag best run automlrun experi tag dictionari automl best child run env best run environ print env python conda depend serial string",
        "Answer_preprocessed_content":"microsoft contact sai best bet probabl train env pin instal pin env run pip instal pkg list pin list us code snippet",
        "Answer_gpt_summary_original":"Solution: The Microsoft contact suggests that the user should check the training environment and install the same pinned packages. The user can get the environment by running child_run.get_environment() and then pip install all the packages listed in there with the pins listed there. A code snippet is also provided to get the environment.",
        "Answer_gpt_summary":"solut microsoft contact suggest user check train environ instal pin packag user environ run child run environ pip instal packag list pin list code snippet provid environ"
    },
    {
        "Question_title":"Using Sagemaker predictor in a Spark UDF function",
        "Question_body":"<p>I am trying to run inference on a Tensorflow model deployed on SageMaker from a Python Spark job.\nI am running a (Databricks) notebook which has the following cell:<\/p>\n\n<pre><code>def call_predict():\n        batch_size = 1\n        data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n        tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n        prediction = predictor.predict(tensor_proto)\n        print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n<\/code><\/pre>\n\n<p>If I just call call_predict() it works fine:<\/p>\n\n<pre><code>call_predict()\n<\/code><\/pre>\n\n<p>and I get the output:<\/p>\n\n<pre><code>Process time: 65.261396\nOut[61]: {'model_spec': {'name': u'generic_model',\n  'signature_name': u'serving_default',\n  'version': {'value': 1578909324L}},\n 'outputs': {u'ages': {'dtype': 1,\n   'float_val': [5.680944442749023],\n   'tensor_shape': {'dim': [{'size': 1L}]}}}}\n<\/code><\/pre>\n\n<p>but when I try to call from a Spark context (in a UDF) I get a serialization error.\nThe code I'm trying to run is:<\/p>\n\n<pre><code>dataRange = range(1, 10001)\nrangeRDD = sc.parallelize(dataRange, 8)\nnew_data = rangeRDD.map(lambda x : call_predict())\nnew_data.count()\n<\/code><\/pre>\n\n<p>and the error I get is:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nPicklingError                             Traceback (most recent call last)\n&lt;command-2282434&gt; in &lt;module&gt;()\n      2 rangeRDD = sc.parallelize(dataRange, 8)\n      3 new_data = rangeRDD.map(lambda x : call_predict())\n----&gt; 4 new_data.count()\n      5 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in count(self)\n   1094         3\n   1095         \"\"\"\n-&gt; 1096         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n   1097 \n   1098     def stats(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in sum(self)\n   1085         6.0\n   1086         \"\"\"\n-&gt; 1087         return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n   1088 \n   1089     def count(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in fold(self, zeroValue, op)\n    956         # zeroValue provided to each partition is unique from the one provided\n    957         # to the final reduce call\n--&gt; 958         vals = self.mapPartitions(func).collect()\n    959         return reduce(op, vals, zeroValue)\n    960 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in collect(self)\n    829         # Default path used in OSS Spark \/ for non-credential passthrough clusters:\n    830         with SCCallSiteSync(self.context) as css:\n--&gt; 831             sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n    832         return list(_load_from_socket(sock_info, self._jrdd_deserializer))\n    833 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _jrdd(self)\n   2573 \n   2574         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n-&gt; 2575                                       self._jrdd_deserializer, profiler)\n   2576         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n   2577                                              self.preservesPartitioning, self.is_barrier)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _wrap_function(sc, func, deserializer, serializer, profiler)\n   2475     assert serializer, \"serializer should not be empty\"\n   2476     command = (func, profiler, deserializer, serializer)\n-&gt; 2477     pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n   2478     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n   2479                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _prepare_for_python_RDD(sc, command)\n   2461     # the serialized command will be compressed by broadcast\n   2462     ser = CloudPickleSerializer()\n-&gt; 2463     pickled_command = ser.dumps(command)\n   2464     if len(pickled_command) &gt; sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  # Default 1M\n   2465         # The broadcast will have same life cycle as created PythonRDD\n\n\/databricks\/spark\/python\/pyspark\/serializers.pyc in dumps(self, obj)\n    709                 msg = \"Could not serialize object: %s: %s\" % (e.__class__.__name__, emsg)\n    710             cloudpickle.print_exec(sys.stderr)\n--&gt; 711             raise pickle.PicklingError(msg)\n    712 \n    713 \n\nPicklingError: Could not serialize object: TypeError: can't pickle _ssl._SSLSocket objects\n<\/code><\/pre>\n\n<p>Not sure what is this serialization error - does is complain about failing to deserialize the Predictor<\/p>\n\n<p>My notebook has a cell which was called prior to the above cells with the following imports:<\/p>\n\n<pre><code>import sagemaker\nimport boto3\nfrom sagemaker.tensorflow.model import TensorFlowPredictor\nimport tensorflow as tf\nimport numpy as np\nimport time\n<\/code><\/pre>\n\n<p>The Predictor was created with the following code:<\/p>\n\n<pre><code>sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY,\n                                aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\nsagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY,\n                                        aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\nboto_session = boto3.Session(region_name='us-east-1')\nsagemaker_session = sagemaker.Session(boto_session, sagemaker_client=sagemaker_client, sagemaker_runtime_client=sagemaker_runtime_client)\n\npredictor = TensorFlowPredictor('endpoint-poc', sagemaker_session)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579190415880,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":322.0,
        "Answer_body":"<p>The udf function will be executed by multiple spark tasks in parallel. Those tasks run in completely isolated python processes and they are scheduled to physically different machines. Hence each data, those functions reference, must be on the same node. This is the case for everything created within the udf.<\/p>\n\n<p>Whenever you reference any object outside of the udf from the function, this data structure needs to be serialised (pickled) to each executor. Some object state, like open connections to a socket, cannot be pickled.<\/p>\n\n<p>You need to make sure, that connections are lazily opened each executor. It must happen only on the first function call on that executor. The <a href=\"https:\/\/spark.apache.org\/docs\/latest\/streaming-programming-guide.html#design-patterns-for-using-foreachrdd\" rel=\"nofollow noreferrer\">connection pooling topic<\/a> is covered in the docs, however only in the spark streaming guide (though it also applies for normal batch jobs).<\/p>\n\n<p>Normally one can use the Singleton Pattern for this. But in python people use the Borgh pattern.<\/p>\n\n<pre><code>class Env:\n    _shared_state = {\n        \"sagemaker_client\": None\n        \"sagemaker_runtime_client\": None\n        \"boto_session\": None\n        \"sagemaker_session\": None\n        \"predictor\": None\n    }\n    def __init__(self):\n        self.__dict__ = self._shared_state\n        if not self.predictor:\n            self.sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n            self.sagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\n            self.boto_session = boto3.Session(region_name='us-east-1')\n            self.sagemaker_session = sagemaker.Session(self.boto_session, sagemaker_client=self.sagemaker_client, sagemaker_runtime_client=self.sagemaker_runtime_client)\n\n            self.predictor = TensorFlowPredictor('endpoint-poc', self.sagemaker_session)\n\n\n#....\ndef call_predict():\n   env = Env()\n   batch_size = 1\n   data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n   tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n   prediction = env.predictor.predict(tensor_proto)\n\n   print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n\nnew_data = rangeRDD.map(lambda x : call_predict())\n<\/code><\/pre>\n\n<p>The Env class is defined on the master node. Its <code>_shared_state<\/code> has empty entries. When then Env object is instantiated first time, it shares the state with all further instances of Env on any subsequent call to the udf. On each separate parallel running process this will happen exactly one time. This way the sessions are shared and do not need to pickled. <\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1579506845700,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59773503",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1579206604950,
        "Question_original_content":"predictor spark udf function try run infer tensorflow model deploi python spark job run databrick notebook follow cell def predict batch size data tensor proto tensor proto valu asarrai data shape batch size len data dtype float predict predictor predict tensor proto print process time format time clock start return predict predict work fine predict output process time model spec gener model signatur serv default version valu output ag dtype float val tensor shape dim size try spark context udf serial error code try run datarang rang rangerdd parallel datarang new data rangerdd map lambda predict new data count error picklingerror traceback recent rangerdd parallel datarang new data rangerdd map lambda predict new data count databrick spark python pyspark rdd pyc count self return self mappartit lambda sum sum def stat self databrick spark python pyspark rdd pyc sum self return self mappartit lambda sum fold oper add def count self databrick spark python pyspark rdd pyc fold self zerovalu zerovalu provid partit uniqu provid final reduc val self mappartit func collect return reduc val zerovalu databrick spark python pyspark rdd pyc collect self default path oss spark non credenti passthrough cluster sccallsitesync self context css sock info self ctx jvm pythonrdd collectandserv self jrdd rdd return list load socket sock info self jrdd deseri databrick spark python pyspark rdd pyc jrdd self wrap func wrap function self ctx self func self prev jrdd deseri self jrdd deseri profil python rdd self ctx jvm pythonrdd self prev jrdd rdd wrap func self preservespartit self barrier databrick spark python pyspark rdd pyc wrap function func deseri serial profil assert serial serial command func profil deseri serial pickl command broadcast var env includ prepar python rdd command return jvm pythonfunct bytearrai pickl command env includ pythonexec pythonv broadcast var javaaccumul databrick spark python pyspark rdd pyc prepar python rdd command serial command compress broadcast ser cloudpickleseri pickl command ser dump command len pickl command jvm pythonutil getbroadcastthreshold jsc default broadcast life cycl creat pythonrdd databrick spark python pyspark serial pyc dump self obj msg serial object class emsg cloudpickl print exec sy stderr rais pickl picklingerror msg picklingerror serial object typeerror pickl ssl sslsocket object sure serial error complain fail deseri predictor notebook cell call prior cell follow import import import boto tensorflow model import tensorflowpredictor import tensorflow import numpi import time predictor creat follow code client boto client aw access kei access kei aw secret access kei secret kei region east runtim client boto client runtim aw access kei access kei aw secret access kei secret kei region east boto session boto session region east session session boto session client client runtim client runtim client predictor tensorflowpredictor endpoint poc session",
        "Question_preprocessed_content":"predictor spark udf function try run infer tensorflow model deploi python spark job run notebook follow cell work fine output try spark context serial error code try run error sure serial error complain fail deseri predictor notebook cell call prior cell follow import predictor creat follow code",
        "Question_gpt_summary_original":"The user is encountering a serialization error when trying to call a Sagemaker predictor from a Spark context in a UDF. The error message suggests that the issue may be related to failing to deserialize the predictor. The user has imported the necessary libraries and created the predictor successfully in a previous cell.",
        "Question_gpt_summary":"user encount serial error try predictor spark context udf error messag suggest issu relat fail deseri predictor user import necessari librari creat predictor successfulli previou cell",
        "Answer_original_content":"udf function execut multipl spark task parallel task run complet isol python process schedul physic differ machin data function refer node case creat udf refer object outsid udf function data structur need serialis pickl executor object state like open connect socket pickl need sure connect lazili open executor happen function executor connect pool topic cover doc spark stream guid appli normal batch job normal us singleton pattern python peopl us borgh pattern class env share state client runtim client boto session session predictor def init self self dict self share state self predictor self client boto client aw access kei access kei aw secret access kei secret kei region east self runtim client boto client runtim aw access kei access kei aw secret access kei secret kei region east self boto session boto session region east self session session self boto session client self client runtim client self runtim client self predictor tensorflowpredictor endpoint poc self session def predict env env batch size data tensor proto tensor proto valu asarrai data shape batch size len data dtype float predict env predictor predict tensor proto print process time format time clock start return predict new data rangerdd map lambda predict env class defin master node share state entri env object instanti time share state instanc env subsequ udf separ parallel run process happen exactli time wai session share need pickl",
        "Answer_preprocessed_content":"udf function execut multipl spark task parallel task run complet isol python process schedul physic differ machin data function refer node case creat udf refer object outsid udf function data structur need serialis executor object state like open connect socket pickl need sure connect lazili open executor happen function executor connect pool topic cover doc spark stream guid normal us singleton pattern python peopl us borgh pattern env class defin master node entri env object instanti time share state instanc env subsequ udf separ parallel run process happen exactli time wai session share need pickl",
        "Answer_gpt_summary_original":"Solution:\n- The issue may be related to failing to deserialize the predictor when calling a Sagemaker predictor from a Spark context in a UDF.\n- The udf function will be executed by multiple spark tasks in parallel, and those tasks run in completely isolated python processes and are scheduled to physically different machines. Hence each data, those functions reference, must be on the same node.\n- Whenever you reference any object outside of the udf from the function, this data structure needs to be serialized (pickled) to each executor. Some object state, like open connections to a socket, cannot be pickled.\n- You need to make sure that connections are lazily opened each executor. It must happen only on the first function call on that executor",
        "Answer_gpt_summary":"solut issu relat fail deseri predictor call predictor spark context udf udf function execut multipl spark task parallel task run complet isol python process schedul physic differ machin data function refer node refer object outsid udf function data structur need serial pickl executor object state like open connect socket pickl need sure connect lazili open executor happen function executor"
    },
    {
        "Question_title":"How to deploy sagemaker.workflow.pipeline.Pipeline?",
        "Question_body":"<p>I have a <code>sagemaker.workflow.pipeline.Pipeline<\/code> which contains multiple <code>sagemaker.workflow.steps.ProcessingStep<\/code> and each <code>ProcessingStep<\/code> contains <code>sagemaker.processing.ScriptProcessor<\/code>.<\/p>\n<p>The current pipeline graph look like the below shown image. It will take data from multiple sources from S3, process it and create a final dataset using the data from previous steps.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6XImq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6XImq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As the <code>Pipeline<\/code> object doesn't support <code>.deploy<\/code> method, how to deploy this pipeline?<\/p>\n<p>While inference\/scoring, When we receive a raw data(single row for each source), how to trigger the pipeline?<\/p>\n<p>or Sagemaker Pipeline is designed for only data processing and model training on huge\/batch data? Not for the inference with the single data point?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639040387083,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":313.0,
        "Answer_body":"<blockquote>\n<p>As the Pipeline object doesn't support .deploy method, how to deploy this pipeline?<\/p>\n<\/blockquote>\n<p>Pipeline does not have a <code>.deploy()<\/code> method, no<\/p>\n<p>Use <code>pipeline.upsert(role_arn='...')<\/code> to create\/update the pipeline definition to SageMaker, then call <code>pipeline.start()<\/code> . Docs <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html#pipeline\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<blockquote>\n<p>While inference\/scoring, When we receive a raw data(single row for each source), how to trigger the pipeline?<\/p>\n<\/blockquote>\n<p>There are actually two types of pipelines in SageMaker. Model Building Pipelines (which you have in your question), and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">Serial Inference Pipelines<\/a>, which are used for Inference. AWS definitely should have called the former &quot;workflows&quot;<\/p>\n<p>You can use a model building pipeline to setup a serial inference pipeline<\/p>\n<p>To do pre-processing in a serial inference pipeline, you want to train an encoder\/estimator (such as SKLearn) and save its model. Then train a learning algorithm, and save its model, then create a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/pipeline.html\" rel=\"nofollow noreferrer\">PipelineModel<\/a> using both models<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70287087",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1639073178300,
        "Question_original_content":"deploi workflow pipelin pipelin workflow pipelin pipelin contain multipl workflow step processingstep processingstep contain process scriptprocessor current pipelin graph look like shown imag data multipl sourc process creat final dataset data previou step pipelin object support deploi method deploi pipelin infer score receiv raw data singl row sourc trigger pipelin pipelin design data process model train huge batch data infer singl data point",
        "Question_preprocessed_content":"deploi contain multipl contain current pipelin graph look like shown imag data multipl sourc process creat final dataset data previou step object support method deploi pipelin receiv raw data trigger pipelin pipelin design data process model train data infer singl data point",
        "Question_gpt_summary_original":"The user is facing challenges in deploying a sagemaker.workflow.pipeline.Pipeline that contains multiple sagemaker.workflow.steps.ProcessingStep, each with sagemaker.processing.ScriptProcessor. The user is unsure how to trigger the pipeline during inference\/scoring with a single data point and is questioning whether Sagemaker Pipeline is only designed for data processing and model training on batch data.",
        "Question_gpt_summary":"user face challeng deploi workflow pipelin pipelin contain multipl workflow step processingstep process scriptprocessor user unsur trigger pipelin infer score singl data point question pipelin design data process model train batch data",
        "Answer_original_content":"pipelin object support deploi method deploi pipelin pipelin deploi method us pipelin upsert role arn creat updat pipelin definit pipelin start doc infer score receiv raw data singl row sourc trigger pipelin actual type pipelin model build pipelin question serial infer pipelin infer aw definit call workflow us model build pipelin setup serial infer pipelin pre process serial infer pipelin want train encod estim sklearn save model train learn algorithm save model creat pipelinemodel model",
        "Answer_preprocessed_content":"pipelin object support deploi method deploi pipelin pipelin method us pipelin definit doc receiv raw data trigger pipelin actual type pipelin model build pipelin serial infer pipelin infer aw definit call workflow us model build pipelin setup serial infer pipelin serial infer pipelin want train save model train learn algorithm save model creat pipelinemodel model",
        "Answer_gpt_summary_original":"Solution:\n- To deploy the pipeline, use `pipeline.upsert(role_arn='...')` to create\/update the pipeline definition to SageMaker, then call `pipeline.start()`.\n- There are two types of pipelines in SageMaker: Model Building Pipelines and Serial Inference Pipelines. To do pre-processing in a serial inference pipeline, train an encoder\/estimator and save its model, then train a learning algorithm and save its model. Finally, create a `PipelineModel` using both models.",
        "Answer_gpt_summary":"solut deploi pipelin us pipelin upsert role arn creat updat pipelin definit pipelin start type pipelin model build pipelin serial infer pipelin pre process serial infer pipelin train encod estim save model train learn algorithm save model final creat pipelinemodel model"
    },
    {
        "Question_title":"MLflow 0.6.0 released!",
        "Question_body":"MLflow 0.6.0 has been released: https:\/\/github.com\/mlflow\/mlflow\/releases\/tag\/v0.6.0\n\n\nMLflow 0.6.0 introduces several major features:\n\n\n- A Java client API (to be published on Maven within the next day or two)\n- Support for saving and serving SparkML models as MLeap for low-latency serving\n- Support for tagging runs with metadata, during and after the run completion\n- Support for deleting (and restoring deleted) experiments\n\n\nIn addition to these features, there are a host of improvements and bugfixes to the REST API, Python API, tracking UI, and documentation.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1536613157000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":29.0,
        "Answer_body":"Congrats to all contributors!\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CAGnzRok5teZdZwaYyAVho-MFRe0OUSE8TLaKKP3aODwe9eCJcA%40mail.gmail.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.. Congratulations to all of the contributors!\n\n\nOne website problem: when I visit announcement page at https:\/\/mlflow.org\/news\/2018\/08\/24\/0.5.2-release\/ I get\n\n\n\n\u00a0\u00a0 \"This XML file does not appear to have any style information associated with it. The document tree is shown below.\"\n\n\nand the following:\n\n\n<Error>\n\u00a0 <Code>AccessDenied<\/Code>\n\u00a0 <Message>Access Denied<\/Message>\n\u00a0 <RequestId>43FC41ABEE5DDD92<\/RequestId>\n\u00a0 <HostId>+pqcuwYpiZTWNZ2Qhvj\/SJfj6UPnqy76B0laQtqcsxF0Pr0DS4iPEEjrIWssCZPMll9ZtAvGV0Y=<\/HostId>\n<\/Error>\n\n\n\nSimilar for other announcement web pages.\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/kJz7T4052RM",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-09-10T23:21:07",
                "Answer_body":"Congrats to all contributors!\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CAGnzRok5teZdZwaYyAVho-MFRe0OUSE8TLaKKP3aODwe9eCJcA%40mail.gmail.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2018-09-11T04:04:57",
                "Answer_body":"Congratulations to all of the contributors!\n\n\nOne website problem: when I visit announcement page at https:\/\/mlflow.org\/news\/2018\/08\/24\/0.5.2-release\/ I get\n\n\n\n\u00a0\u00a0 \"This XML file does not appear to have any style information associated with it. The document tree is shown below.\"\n\n\nand the following:\n\n\n<Error>\n\u00a0 <Code>AccessDenied<\/Code>\n\u00a0 <Message>Access Denied<\/Message>\n\u00a0 <RequestId>43FC41ABEE5DDD92<\/RequestId>\n\u00a0 <HostId>+pqcuwYpiZTWNZ2Qhvj\/SJfj6UPnqy76B0laQtqcsxF0Pr0DS4iPEEjrIWssCZPMll9ZtAvGV0Y=<\/HostId>\n<\/Error>\n\n\n\nSimilar for other announcement web pages.\n\ue5d3"
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"releas releas http github com releas tag introduc major featur java client api publish maven dai support save serv sparkml model mleap low latenc serv support tag run metadata run complet support delet restor delet experi addit featur host improv bugfix rest api python api track document",
        "Question_preprocessed_content":"releas releas introduc major featur java client api support save serv sparkml model mleap serv support tag run metadata run complet support delet experi addit featur host improv bugfix rest api python api track document",
        "Question_gpt_summary_original":"The user may face challenges in using MLflow 0.6.0 due to the introduction of new features such as a Java client API, support for saving and serving SparkML models as MLeap, tagging runs with metadata, and deleting experiments. However, there are also improvements and bugfixes to the REST API, Python API, tracking UI, and documentation.",
        "Question_gpt_summary":"user face challeng introduct new featur java client api support save serv sparkml model mleap tag run metadata delet experi improv bugfix rest api python api track document",
        "Answer_original_content":"congrat contributor receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user cagnzroktezdzwayyavho mfreousetlakkpaodweecjca mail gmail com option visit http group googl com optout congratul contributor websit problem visit announc page http org new releas xml file appear style inform associ document tree shown follow accessdeni access deni fcabe pqcuwypiztwnzqhvj sjfjupnqyblaqtqcsxfprdsipeejriwssczpmllztavgvi similar announc web page",
        "Answer_preprocessed_content":"congrat contributor receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit congratul contributor websit problem visit announc page xml file appear style inform associ document tree shown follow access deni abe similar announc web page",
        "Answer_gpt_summary_original":"No solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut mention discuss"
    },
    {
        "Question_title":"Online Learning - fit",
        "Question_body":"Hey,\n\n\nThanks for this project, MLflow really help us to leverage our data-science team.\n\n\nI'm trying to understand what is the best option to have an online learning algo, which does \"model.fit\" every 30 minutes with new data and re-deployed to production.\nOur case is recommendations systems that works with REST-API and needs to be refreshed often.\n\n\nWhat is the best way to do it in mlflow?\n\n\nThanks,",
        "Question_answer_count":7,
        "Question_comment_count":0,
        "Question_creation_time":1545296754000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":40.0,
        "Answer_body":"What do you want MLflow to do for this algorithm? You can use the API to track the performance of each training run, but there\u2019s nothing specific in it to online learning.\n\nMatei\n\n\ue5d3\n> --\n> You received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n> To unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\n> To post to this group, send email to mlflow...@googlegroups.com.\n> To view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/8ae508a3-f9aa-4829-a1c2-2e7422188a34%40googlegroups.com.\n> For more options, visit https:\/\/groups.google.com\/d\/optout.. I want to be able to basically retrain my model and re-save it (while the first model is live)\n\n\nExample flow:\u00a0\n1. train a model, work on it, save it, deploy it\n2. retrain every hour the model and save it\u00a0\n3. the API will be seemless to the save, and will start serving the new model every hour\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/EE6A679A-AC65-49F2-A571-40BD8685C6E0%40databricks.com.\n\nFor more options, visit https:\/\/groups.google.com\/d\/optout.\n\n\n\n\n\n--\n\n\n\nOr Koren\nHead of Data, Digital Solutions\n\n\nemail\u00a0o...@ironsrc.com\nmobile\u00a0+972-528286444\nfax\u00a0+972-77-5448273\nironSource HQ - 121 Derech Menachem Begin st. Tel Aviv\n\nironsrc.com\n\n\nThis email (including any attachments) is for the sole use of the intended recipient and may contain confidential information which may be protected by legal privilege. If you are not the intended recipient, or the employee or agent responsible for delivering it to the intended recipient, you are hereby notified that any use, dissemination, distribution or copying of this communication and\/or its content is strictly prohibited. If you are not the intended recipient, please immediately notify us by reply email or by telephone, delete this email and destroy any copies. Thank you.. WDYT?\n\n\nOr Koren\n\u00a0\nFrom: Or Koren <o...@ironsrc.com>\nSent: Wednesday, December 26, 2018 11:01 AM\nTo: Matei Zaharia\nCc: mlflow-users\nSubject: Re: Online Learning - fit\n\u00a0\n\ue5d3. Got it, that makes sense. The Models package for deploying should work well for this purpose. You can train an updated model and deploy it as a Docker container. You\u2019ll have to write your own script to do the periodic training and deployment though \u2014 MLflow doesn\u2019t include a job scheduler, but you can use some exiting one to call the training step, some step to query the metrics, and some step to deploy it if it\u2019s good.\n\nMatei\n\n> On Jan 8, 2019, at 9:48 AM, Or Koren <o...@ironsrc.com> wrote:\n>\n> WDYT?\n>\n> Or Koren\n\n\ue5d3\n> email o...@ironsrc.com\n> mobile +972-528286444\n> fax +972-77-5448273\n> ironSource HQ - 121 Derech Menachem Begin st. Tel Aviv\n> ironsrc.com\n>\n\n\ue5d3. but i will have to re-save my new model (the updated one) and re-deploy it and re-route my ELB to the new container?\n\ue5d3\n\ue5d3\nemail\u00a0o...@ironsrc.com\nmobile\u00a0+972-528286444\nfax\u00a0+972-77-5448273\nironSource HQ - 121 Derech Menachem Begin st. Tel Aviv\n\nironsrc.com\n\ue5d3. in my opinion, we have to separate 2 things. online learning model wich is\u00a0 updated inside deployed app and I think there is nothing to do.the second option is for the \"frequently\" updated model outside deployed app and the most interesting thinks to have is that for each \"deployment\" define a hook or notification then do something, so for each new model we can imagine that mlflow server is notifyed automatically just like a file watcher or webhook ( calling Rest url...)...then we can do automatic hot reload. one of issues we wull face is to have a compatibility process (something analog to Kafka Topics and Schema registry.\u00a0 think new Schema as a \"New model\").\ncheers\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CAKVTWMOK%3D8OBa%2BuA2QOLLb5UZ7PgkXdqR5%3Dg0tH3JfdJck_QrQ%40mail.gmail.com.\n\ue5d3. ok, i'll try it 10x\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CAL5yX1ZB1eF%3DvqGat1v21rx9VAaRnboreB2YYs4qbGYy8c9C1Q%40mail.gmail.com.\n\nFor more options, visit https:\/\/groups.google.com\/d\/optout.\n\n\n\n\n\n--\n\n\n\nOr Koren\nHead of Data, Digital Solutions\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/bpllkE4Rdio",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-12-25T21:31:41",
                "Answer_body":"What do you want MLflow to do for this algorithm? You can use the API to track the performance of each training run, but there\u2019s nothing specific in it to online learning.\n\nMatei\n\n\ue5d3\n> --\n> You received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n> To unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\n> To post to this group, send email to mlflow...@googlegroups.com.\n> To view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/8ae508a3-f9aa-4829-a1c2-2e7422188a34%40googlegroups.com.\n> For more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2018-12-26T04:01:56",
                "Answer_body":"I want to be able to basically retrain my model and re-save it (while the first model is live)\n\n\nExample flow:\u00a0\n1. train a model, work on it, save it, deploy it\n2. retrain every hour the model and save it\u00a0\n3. the API will be seemless to the save, and will start serving the new model every hour\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/EE6A679A-AC65-49F2-A571-40BD8685C6E0%40databricks.com.\n\nFor more options, visit https:\/\/groups.google.com\/d\/optout.\n\n\n\n\n\n--\n\n\n\nOr Koren\nHead of Data, Digital Solutions\n\n\nemail\u00a0o...@ironsrc.com\nmobile\u00a0+972-528286444\nfax\u00a0+972-77-5448273\nironSource HQ - 121 Derech Menachem Begin st. Tel Aviv\n\nironsrc.com\n\n\nThis email (including any attachments) is for the sole use of the intended recipient and may contain confidential information which may be protected by legal privilege. If you are not the intended recipient, or the employee or agent responsible for delivering it to the intended recipient, you are hereby notified that any use, dissemination, distribution or copying of this communication and\/or its content is strictly prohibited. If you are not the intended recipient, please immediately notify us by reply email or by telephone, delete this email and destroy any copies. Thank you."
            },
            {
                "Answer_creation_time":"2019-01-08T12:48:34",
                "Answer_body":"WDYT?\n\n\nOr Koren\n\u00a0\nFrom: Or Koren <o...@ironsrc.com>\nSent: Wednesday, December 26, 2018 11:01 AM\nTo: Matei Zaharia\nCc: mlflow-users\nSubject: Re: Online Learning - fit\n\u00a0\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-01-11T02:15:54",
                "Answer_body":"Got it, that makes sense. The Models package for deploying should work well for this purpose. You can train an updated model and deploy it as a Docker container. You\u2019ll have to write your own script to do the periodic training and deployment though \u2014 MLflow doesn\u2019t include a job scheduler, but you can use some exiting one to call the training step, some step to query the metrics, and some step to deploy it if it\u2019s good.\n\nMatei\n\n> On Jan 8, 2019, at 9:48 AM, Or Koren <o...@ironsrc.com> wrote:\n>\n> WDYT?\n>\n> Or Koren\n\n\ue5d3\n> email o...@ironsrc.com\n> mobile +972-528286444\n> fax +972-77-5448273\n> ironSource HQ - 121 Derech Menachem Begin st. Tel Aviv\n> ironsrc.com\n>\n\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-01-13T07:33:09",
                "Answer_body":"but i will have to re-save my new model (the updated one) and re-deploy it and re-route my ELB to the new container?\n\ue5d3\n\ue5d3\nemail\u00a0o...@ironsrc.com\nmobile\u00a0+972-528286444\nfax\u00a0+972-77-5448273\nironSource HQ - 121 Derech Menachem Begin st. Tel Aviv\n\nironsrc.com\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-01-13T08:50:37",
                "Answer_body":"in my opinion, we have to separate 2 things. online learning model wich is\u00a0 updated inside deployed app and I think there is nothing to do.the second option is for the \"frequently\" updated model outside deployed app and the most interesting thinks to have is that for each \"deployment\" define a hook or notification then do something, so for each new model we can imagine that mlflow server is notifyed automatically just like a file watcher or webhook ( calling Rest url...)...then we can do automatic hot reload. one of issues we wull face is to have a compatibility process (something analog to Kafka Topics and Schema registry.\u00a0 think new Schema as a \"New model\").\ncheers\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CAKVTWMOK%3D8OBa%2BuA2QOLLb5UZ7PgkXdqR5%3Dg0tH3JfdJck_QrQ%40mail.gmail.com.\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-01-13T09:06:56",
                "Answer_body":"ok, i'll try it 10x\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CAL5yX1ZB1eF%3DvqGat1v21rx9VAaRnboreB2YYs4qbGYy8c9C1Q%40mail.gmail.com.\n\nFor more options, visit https:\/\/groups.google.com\/d\/optout.\n\n\n\n\n\n--\n\n\n\nOr Koren\nHead of Data, Digital Solutions\n\ue5d3"
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"onlin learn fit hei thank project help leverag data scienc team try understand best option onlin learn algo model fit minut new data deploi product case recommend system work rest api need refresh best wai thank",
        "Question_preprocessed_content":"onlin learn fit hei thank project help leverag team try understand best option onlin learn algo minut new data product case recommend system work need refresh best wai thank",
        "Question_gpt_summary_original":"The user is facing a challenge in determining the best option for an online learning algorithm that can refresh every 30 minutes with new data and be redeployed to production. They are specifically looking for the best way to do this using MLflow for their recommendation system that works with a REST-API.",
        "Question_gpt_summary":"user face challeng determin best option onlin learn algorithm refresh minut new data redeploi product specif look best wai recommend work rest api",
        "Answer_original_content":"want algorithm us api track perform train run there specif onlin learn matei receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user aea faa googlegroup com option visit http group googl com optout want abl basic retrain model save model live exampl flow train model work save deploi retrain hour model save api seemless save start serv new model hour view discuss web visit http group googl com msgid user eeaa bdce databrick com option visit http group googl com optout koren head data digit solut emailo ironsrc com mobil fax ironsourc derech menachem begin tel aviv ironsrc com email includ attach sole us intend recipi contain confidenti inform protect legal privileg intend recipi employe agent respons deliv intend recipi notifi us dissemin distribut copi commun content strictli prohibit intend recipi immedi notifi repli email telephon delet email destroi copi thank wdyt koren koren sent wednesdai decemb matei zaharia user subject onlin learn fit got make sens model packag deploi work purpos train updat model deploi docker contain youll write script period train deploy doesnt includ job schedul us exit train step step queri metric step deploi good matei jan koren wrote wdyt koren email ironsrc com mobil fax ironsourc derech menachem begin tel aviv ironsrc com save new model updat deploi rout elb new contain emailo ironsrc com mobil fax ironsourc derech menachem begin tel aviv ironsrc com opinion separ thing onlin learn model wich updat insid deploi app think second option frequent updat model outsid deploi app interest think deploy defin hook notif new model imagin server notifi automat like file watcher webhook call rest url automat hot reload issu wull face compat process analog kafka topic schema registri think new schema new model cheer view discuss web visit http group googl com msgid user cakvtwmok doba buaqollbuzpgkxdqr dgthjfdjck qrq mail gmail com try view discuss web visit http group googl com msgid user calyxzbef dvqgatvrxvaarnborebyysqbgyyccq mail gmail com option visit http group googl com optout koren head data digit solut",
        "Answer_preprocessed_content":"want algorithm us api track perform train run there specif onlin learn matei receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit want abl basic retrain model exampl flow train model work save deploi retrain hour model save api seemless save start serv new model hour view discuss web visit option visit koren head data digit solut ironsourc derech menachem begin tel aviv email sole us intend recipi contain confidenti inform protect legal privileg intend recipi employe agent respons deliv intend recipi notifi us dissemin distribut copi commun content strictli prohibit intend recipi immedi notifi repli email telephon delet email destroi copi thank wdyt koren koren sent wednesdai decemb matei zaharia user subject onlin learn fit got make sens model packag deploi work purpos train updat model deploi docker contain youll write script period train deploy doesnt includ job schedul us exit train step step queri metric step deploi good matei jan koren wrote wdyt koren email mobil fax ironsourc derech menachem begin tel aviv new model elb new contain ironsourc derech menachem begin tel aviv opinion separ thing onlin learn model wich updat insid deploi app think second option frequent updat model outsid deploi app interest think deploy defin hook notif new model imagin server notifi automat like file watcher webhook automat hot reload issu wull face compat process cheer view discuss web visit try view discuss web visit option visit koren head data digit solut",
        "Answer_gpt_summary_original":"Solution:\n- The user can use the Models package for deploying the model as a Docker container and write their own script to do the periodic training and deployment.\n- They can use an existing job scheduler to call the training step, query the metrics, and deploy the model if it's good.\n- They can define a hook or notification for each deployment to notify the MLflow server automatically and do automatic hot reload.",
        "Answer_gpt_summary":"solut user us model packag deploi model docker contain write script period train deploy us exist job schedul train step queri metric deploi model good defin hook notif deploy notifi server automat automat hot reload"
    },
    {
        "Question_title":"Can't find scoring.py when using PythonScriptStep() in Databricks",
        "Question_body":"<p>We are defining in Databricks a PythonScriptStep(). When using PythonScriptStep() within our pipeline script we can't find the scoring.py file.<\/p>\n<pre><code>scoring_step = PythonScriptStep(\n    name=&quot;Scoring_Step&quot;,\n    source_directory=os.getenv(&quot;DATABRICKS_NOTEBOOK_PATH&quot;, &quot;\/Users\/USER_NAME\/source_directory&quot;),\n    script_name=&quot;.\/scoring.py&quot;,\n    arguments=[&quot;--input_dataset&quot;, ds_consumption],\n    compute_target=pipeline_cluster,\n    runconfig=pipeline_run_config,\n    allow_reuse=False)\n<\/code><\/pre>\n<p>We getting the following error message:<\/p>\n<pre><code>Step [Scoring_Step]: script not found at: \/databricks\/driver\/scoring.py. Make sure to specify an appropriate source_directory on the Step or default_source_directory on the Pipeline.\n<\/code><\/pre>\n<p>For some reason Databricks is searching for the file in '\/databricks\/driver\/' instead of the folder we entered.<\/p>\n<p>There is also the way to use DatabricksStep() instead of PythonScriptStep(), but because of specific reasons we need to use the PythonSriptStep() class.<\/p>\n<p>Could anybody help us with this specific problem?<\/p>\n<p>Thank you very much for any help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655997421433,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1656039942703,
        "Question_score":0.0,
        "Question_view_count":68.0,
        "Answer_body":"<pre><code>scoring_step = PythonScriptStep(\n    name=&quot;Scoring_Step&quot;,\n    source_directory=os.getenv(&quot;DATABRICKS_NOTEBOOK_PATH&quot;, &quot;\/Users\/USER_NAME\/source_directory&quot;),\n    script_name=&quot;.\/scoring.py&quot;,\n    arguments=[&quot;--input_dataset&quot;, ds_consumption],\n    compute_target=pipeline_cluster,\n    runconfig=pipeline_run_config,\n    allow_reuse=False)\n<\/code><\/pre>\n<p>Change the above code block with below code block. It will resolve the error<\/p>\n<pre><code>data_ref = OutputFileDatasetConfig(\n    name='data_ref',\n    destination=(ds, '\/data')\n).as_upload()\n\n\ndata_prep_step = PythonScriptStep(\n    name='data_prep',\n    script_name='pipeline_steps\/data_prep.py',\n    source_directory='\/.',\n    arguments=[\n        '--main_path', main_ref,\n        '--data_ref_folder', data_ref\n                ],\n    inputs=[main_ref, data_ref],\n    outputs=[data_ref],\n    runconfig=arbitrary_run_config,\n    allow_reuse=False\n)\n<\/code><\/pre>\n<p>Reference link for the <a href=\"https:\/\/scoring_step%20=%20PythonScriptStep(%20%20%20%20%20name=%22Scoring_Step%22,%20%20%20%20%20source_directory=os.getenv(%22DATABRICKS_NOTEBOOK_PATH%22,%20%22\/Users\/USER_NAME\/source_directory%22),%20%20%20%20%20script_name=%22.\/scoring.py%22,%20%20%20%20%20arguments=%5B%22--input_dataset%22,%20ds_consumption%5D,%20%20%20%20%20compute_target=pipeline_cluster,%20%20%20%20%20runconfig=pipeline_run_config,%20%20%20%20%20allow_reuse=False)\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72732616",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1656324774632,
        "Question_original_content":"score pythonscriptstep databrick defin databrick pythonscriptstep pythonscriptstep pipelin script score file score step pythonscriptstep score step sourc directori getenv databrick notebook path user user sourc directori script score argument input dataset consumpt comput target pipelin cluster runconfig pipelin run config allow reus fals get follow error messag step score step script databrick driver score sure specifi appropri sourc directori step default sourc directori pipelin reason databrick search file databrick driver instead folder enter wai us databricksstep instead pythonscriptstep specif reason need us pythonsriptstep class anybodi help specif problem thank help",
        "Question_preprocessed_content":"pythonscriptstep databrick defin databrick pythonscriptstep pythonscriptstep pipelin script file get follow error messag reason databrick search file instead folder enter wai us databricksstep instead pythonscriptstep specif reason need us pythonsriptstep class anybodi help specif problem thank help",
        "Question_gpt_summary_original":"The user is encountering a challenge when using PythonScriptStep() in Databricks to find the scoring.py file. The error message indicates that Databricks is searching for the file in '\/databricks\/driver\/' instead of the specified folder. The user needs to use PythonScriptStep() instead of DatabricksStep() due to specific reasons.",
        "Question_gpt_summary":"user encount challeng pythonscriptstep databrick score file error messag indic databrick search file databrick driver instead specifi folder user need us pythonscriptstep instead databricksstep specif reason",
        "Answer_original_content":"score step pythonscriptstep score step sourc directori getenv databrick notebook path user user sourc directori script score argument input dataset consumpt comput target pipelin cluster runconfig pipelin run config allow reus fals chang code block code block resolv error data ref outputfiledatasetconfig data ref destin data upload data prep step pythonscriptstep data prep script pipelin step data prep sourc directori argument main path main ref data ref folder data ref input main ref data ref output data ref runconfig arbitrari run config allow reus fals refer link document",
        "Answer_preprocessed_content":"chang code block code block resolv error refer link document",
        "Answer_gpt_summary_original":"Solution: The solution provided in the discussion is to change the code block with the new code block provided. The new code block includes the use of OutputFileDatasetConfig() and PythonScriptStep() to resolve the error. No other solutions are mentioned in the discussion.",
        "Answer_gpt_summary":"solut solut provid discuss chang code block new code block provid new code block includ us outputfiledatasetconfig pythonscriptstep resolv error solut mention discuss"
    },
    {
        "Question_title":"Why is my GCP Vertex pipeline api_endpoint not right?",
        "Question_body":"<p>My <code>API_ENDPOINT<\/code> is set to <code>europe-west1-aiplatform.googleapis.com<\/code>.<\/p>\n<p>I define a pipeline:<\/p>\n<pre><code>def pipeline(project: str = PROJECT_ID, region: str = REGION, api_endpoint: str = API_ENDPOINT):\n<\/code><\/pre>\n<p>when I run it:<\/p>\n<pre><code>job = aip.PipelineJob(\ndisplay_name=DISPLAY_NAME,\ntemplate_path=&quot;image classification_pipeline.json&quot;.replace(&quot; &quot;, &quot;_&quot;),)\njob.run()\n<\/code><\/pre>\n<p>it is always created in USandA:<\/p>\n<pre><code>INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. \nResource name: projects\/my_proj_id\/locations\/us-central1\/pipelineJobs\/automl-image-training-v2-anumber\n<\/code><\/pre>\n<p>How do I get it into Europe?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1641293452093,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":92.0,
        "Answer_body":"<p>The <code>location<\/code> parameter in the <code>aip.PipelineJob()<\/code> class can be used to specify in which region the pipeline will be deployed. Refer to this <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/aiplatform#class-googlecloudaiplatformpipelinejobdisplayname-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-templatepath-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-jobid-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-pipelineroot-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-parametervalues-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-any--none-enablecaching-optionalboolhttpspythonreadthedocsioenlatestlibraryfunctionshtmlbool--none-encryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-labels-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-credentials-optionalgoogleauthcredentialscredentialshttpsgoogleapisdevpythongoogle-authlatestreferencegoogleauthcredentialshtmlgoogleauthcredentialscredentials--none-project-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-location-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none\" rel=\"nofollow noreferrer\">documentation<\/a> for more information about the <code>PipelineJob()<\/code> method.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>REGION = &quot;europe-west1&quot;\n\njob = aip.PipelineJob(\n          display_name=DISPLAY_NAME,\n          template_path=&quot;image classification_pipeline.json&quot;.replace(&quot; &quot;, &quot;_&quot;),\n          location=REGION)\n\njob.run()\n<\/code><\/pre>\n<p>The above code will deploy a pipeline in the <code>europe-west1<\/code> region. The code returns the following output. The job is now deployed in the specified region.<\/p>\n<pre><code>INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\nINFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects\/&lt;project-id&gt;\/locations\/europe-west1\/pipelineJobs\/hello-world-pipeline\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1647760091087,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70577610",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1641305260580,
        "Question_original_content":"gcp vertex pipelin api endpoint right api endpoint set europ west aiplatform googleapi com defin pipelin def pipelin project str project region str region api endpoint str api endpoint run job aip pipelinejob displai displai templat path imag classif pipelin json replac job run creat usanda info googl cloud aiplatform pipelin job pipelinejob creat resourc project proj locat central pipelinejob automl imag train anumb europ",
        "Question_preprocessed_content":"gcp vertex pipelin right set defin pipelin run creat usanda europ",
        "Question_gpt_summary_original":"The user is facing a challenge with their GCP Vertex pipeline API endpoint, which is set to \"europe-west1-aiplatform.googleapis.com\". Despite this, when they run the pipeline, it is always created in the US. The user is seeking a solution to get the pipeline created in Europe.",
        "Question_gpt_summary":"user face challeng gcp vertex pipelin api endpoint set europ west aiplatform googleapi com despit run pipelin creat user seek solut pipelin creat europ",
        "Answer_original_content":"locat paramet aip pipelinejob class specifi region pipelin deploi refer document inform pipelinejob method region europ west job aip pipelinejob displai displai templat path imag classif pipelin json replac locat region job run code deploi pipelin europ west region code return follow output job deploi specifi region info googl cloud aiplatform pipelin job creat pipelinejob info googl cloud aiplatform pipelin job pipelinejob creat resourc project locat europ west pipelinejob hello world pipelin",
        "Answer_preprocessed_content":"paramet class specifi region pipelin deploi refer document inform method code deploi pipelin region code return follow output job deploi specifi region",
        "Answer_gpt_summary_original":"Solution: The user can use the `location` parameter in the `aip.PipelineJob()` class to specify the region where the pipeline will be deployed. By setting the `location` parameter to \"europe-west1\", the pipeline will be created in the Europe region. The documentation provides more information about the `PipelineJob()` method.",
        "Answer_gpt_summary":"solut user us locat paramet aip pipelinejob class specifi region pipelin deploi set locat paramet europ west pipelin creat europ region document provid inform pipelinejob method"
    },
    {
        "Question_title":"Can I use Azure interactive mode for azure-cli-ml extension?",
        "Question_body":"<p>I'm using Azure CLI interactive mode <code>az interactive<\/code> to run below command. <br \/>\n<code>az ml folder attach -w yhd-mlws -g yhd-mlws-rg<\/code><br \/><br \/>\nIt prompts me with below error message.<br \/>\n<code>az: error: unrecognized arguments: -w yhd-mlws -g yhd-mlws-rg<\/code><br \/><br \/>\nBTW, both my Machine Learning workspace <code>yhd-mlws<\/code> and resource group <code>yhd-mlws-rg<\/code> had been created in my Azure subscription. Azure CLI extension for machine learning service had also been installed via <code>az extension add -n azure-cli-ml<\/code>.<br \/><br \/>\nThen I run command <code>az ml folder attach<\/code> without any argument. I get bellow error message.<br \/><\/p>\n\n<pre><code>Message: Error, default workspace not set and workspace name parameter not provided.\nPlease set a default workspace using \"az ml folder attach -w myworkspace -g myresourcegroup\" or provide a value for the workspace name parameter.\n<\/code><\/pre>\n\n<p>The command window exit the interactive mode after above error message. Then I try the command <code>az ml folder attach -w yhd-mlws -g yhd-mlws-rg<\/code> again, bingo! It works. <br \/>\nHere comes my question, does azure-cli-ml extension support Azure CLI interactive mode? You know, Azure CLI interactive mode is amazing and I want to use it whenever possible. Thanks!<br \/><br \/>\nBTW, I'm running windows command window in Windows Server 2016 Datcenter. Azure-cli version is 2.0.79.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1582642024300,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1582678582156,
        "Question_score":1.0,
        "Question_view_count":408.0,
        "Answer_body":"<p>I can reproduce your issue, the interactive mode should support the <code>azure-cli-ml<\/code> extension, because when I run <code>az ml workspace list<\/code>, it works, once I pass the <code>-g<\/code> parameter, it gives the same error, maybe it is a bug, but I am not sure, the <code>interactive<\/code> is in preview currently.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/g4FvM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/g4FvM.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>If you want to run <code>az ml folder attach -w yhd-mlws -g yhd-mlws-rg<\/code> in the interactive mode, my workaround is to pass the <code>#<\/code>, i.e. <code># az ml folder attach -w yhd-mlws -g yhd-mlws-rg<\/code>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/pWMyH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/pWMyH.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60397252",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1582686710412,
        "Question_original_content":"us azur interact mode azur cli extens azur cli interact mode interact run command folder attach yhd mlw yhd mlw prompt error messag error unrecogn argument yhd mlw yhd mlw btw machin learn workspac yhd mlw resourc group yhd mlw creat azur subscript azur cli extens machin learn servic instal extens add azur cli run command folder attach argument bellow error messag messag error default workspac set workspac paramet provid set default workspac folder attach myworkspac myresourcegroup provid valu workspac paramet command window exit interact mode error messag try command folder attach yhd mlw yhd mlw bingo work come question azur cli extens support azur cli interact mode know azur cli interact mode amaz want us possibl thank btw run window command window window server datcent azur cli version",
        "Question_preprocessed_content":"us azur interact mode extens azur cli interact mode run command prompt error messag btw machin learn workspac resourc group creat azur subscript azur cli extens machin learn servic instal run command argument bellow error messag command window exit interact mode error messag try command bingo work come question extens support azur cli interact mode know azur cli interact mode amaz want us possibl thank btw run window command window window server datcent version",
        "Question_gpt_summary_original":"The user encountered challenges while using Azure CLI interactive mode to run the command 'az ml folder attach'. The prompt displayed an error message stating that the arguments were unrecognized. The user had installed the Azure CLI extension for machine learning service and created a machine learning workspace and resource group in their Azure subscription. When the user ran the command without any argument, it prompted an error message stating that the default workspace was not set. However, when the user ran the command with the workspace and resource group arguments, it worked. The user's question was whether the azure-cli-ml extension supports Azure CLI interactive mode.",
        "Question_gpt_summary":"user encount challeng azur cli interact mode run command folder attach prompt displai error messag state argument unrecogn user instal azur cli extens machin learn servic creat machin learn workspac resourc group azur subscript user ran command argument prompt error messag state default workspac set user ran command workspac resourc group argument work user question azur cli extens support azur cli interact mode",
        "Answer_original_content":"reproduc issu interact mode support azur cli extens run workspac list work pass paramet give error mayb bug sure interact preview current want run folder attach yhd mlw yhd mlw interact mode workaround pass folder attach yhd mlw yhd mlw",
        "Answer_preprocessed_content":"reproduc issu interact mode support extens run work pass paramet give error mayb bug sure preview current want run interact mode workaround pass",
        "Answer_gpt_summary_original":"Solution:\n- A workaround suggested by a user is to pass the '#' before the command in the interactive mode.",
        "Answer_gpt_summary":"solut workaround suggest user pass command interact mode"
    },
    {
        "Question_title":"mlflow scaling issue",
        "Question_body":"Hi all,\nwhen running below script, we notice a visible performance degradation as number of experiments increases.\nimport mlflow\n  \nfor i in range(5000):\n\n    mlflow.set_experiment('exp%d' % i)\n\n    for j in range(5):\n\n        with mlflow.start_run() as run:\n\n            mlflow.log_param('n', '%d-%d' % (i, j))\n            mlflow.log_metric('fscore', 0.9242)\n            mlflow.set_tag('some tag', '%d-%d' % (i, j))\n            with open('test.json', 'w') as f:\n                f.write('{ \"i\": %d, \"j\": %d }' % (i, j))\n            mlflow.log_artifact('test.json')\n            print(mlflow.active_run())\n\nabove script creates 5000 experiments, with 5 dummy runs in each experiment.  After about 18 hours, it has only created 2500 experiments.  At this point, it is taking about a minute to create a new experiment, and 5 seconds to create a run.\n\nwhat is the bottleneck. \nThanks in advance ..",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1549379818000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":32.0,
        "Answer_body":"Are you running this on a local machine? Note that each experiment under \"mlruns\" directory has an experiment_id as the name of the directory, and\neach run_id for that experiment gets a run_id as a directory. Furthermore, each artifact, metrics, and\u00a0params receive its own directory. So what you seeing is an explosion of file descriptors across the UNIX filesystem, which\nexplains its degradation over time.\n\n\nThis issue is being addressed by using a DB to track experiments and run_ids, mitigating (or eliminating) file descriptor degradation over volumes of experiments\nwith its respective runs at scale.\u00a0\n\n\n\n\n\n--\u00a0\n\n\nThe Best Ideas are Simple\n\nJules S. Damji\n\nApache Spark Developer & Community Advocate\n\nDatabricks, Inc.\n\nju...@databricks.com\n\n(510) 304-7686\n\ndatabricks.com\n\n\n\n\n\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/bbb34a21-66d7-474e-9c3e-20355eebec07%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/MfgmfilrJsI",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-02-05T16:14:40",
                "Answer_body":"Are you running this on a local machine? Note that each experiment under \"mlruns\" directory has an experiment_id as the name of the directory, and\neach run_id for that experiment gets a run_id as a directory. Furthermore, each artifact, metrics, and\u00a0params receive its own directory. So what you seeing is an explosion of file descriptors across the UNIX filesystem, which\nexplains its degradation over time.\n\n\nThis issue is being addressed by using a DB to track experiments and run_ids, mitigating (or eliminating) file descriptor degradation over volumes of experiments\nwith its respective runs at scale.\u00a0\n\n\n\n\n\n--\u00a0\n\n\nThe Best Ideas are Simple\n\nJules S. Damji\n\nApache Spark Developer & Community Advocate\n\nDatabricks, Inc.\n\nju...@databricks.com\n\n(510) 304-7686\n\ndatabricks.com\n\n\n\n\n\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/bbb34a21-66d7-474e-9c3e-20355eebec07%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"scale issu run script notic visibl perform degrad number experi increas import rang set experi exp rang start run run log param log metric fscore set tag tag open test json write log artifact test json print activ run script creat experi dummi run experi hour creat experi point take minut creat new experi second creat run bottleneck thank advanc",
        "Question_preprocessed_content":"scale issu run script notic visibl perform degrad number experi increas import rang rang run tag script creat experi dummi run experi hour creat experi point take minut creat new experi second creat run bottleneck thank advanc",
        "Question_gpt_summary_original":"The user is facing a performance degradation issue while running a script that creates 5000 experiments with 5 dummy runs in each experiment using mlflow. After 18 hours, only 2500 experiments have been created, and it takes about a minute to create a new experiment and 5 seconds to create a run. The user is seeking help to identify the bottleneck causing this issue.",
        "Question_gpt_summary":"user face perform degrad issu run script creat experi dummi run experi hour experi creat take minut creat new experi second creat run user seek help identifi bottleneck caus issu",
        "Answer_original_content":"run local machin note experi mlrun directori experi directori run experi get run directori furthermor artifact metric andparam receiv directori see explos file descriptor unix filesystem explain degrad time issu address track experi run id mitig elimin file descriptor degrad volum experi respect run scale best idea simpl jule damji apach spark develop commun advoc databrick databrick com databrick com receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user eebec googlegroup com option visit http group googl com optout",
        "Answer_preprocessed_content":"run local machin note experi mlrun directori directori experi get directori furthermor artifact metric andparam receiv directori see explos file descriptor unix filesystem explain degrad time issu address track experi mitig file descriptor degrad volum experi respect run scale best idea simpl jule damji apach spark develop commun advoc databrick receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit",
        "Answer_gpt_summary_original":"Solution: The issue of performance degradation while running a script that creates 5000 experiments with 5 dummy runs in each experiment using mlflow can be addressed by using a DB to track experiments and run_ids, mitigating (or eliminating) file descriptor degradation over volumes of experiments with its respective runs at scale.",
        "Answer_gpt_summary":"solut issu perform degrad run script creat experi dummi run experi address track experi run id mitig elimin file descriptor degrad volum experi respect run scale"
    },
    {
        "Question_title":"Does compute cluster & Endpoints costs if we dont delete after use ?",
        "Question_body":"Hi,\n\nI'm aware that running compute Instance costs us for the number of hours we used. So, we stop it when ever we don't need it.\n\nSimilarly, does compute cluster & Endpoints also costs us if we do not delete them after use?\n\n\n\n\nThanks\nBhaskar",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1617894953567,
        "Question_favorite_count":7.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Hi @Bhaskar11-9991\n\nWhen a compute cluster is idle, it autoscales to 0 nodes, so you don't pay when it's not in use. A compute instance is always on and doesn't autoscale. You should stop the compute instance when you aren't using it to avoid extra cost.\nrefer - https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-target\n\n\n\n\nIf the Answer is helpful, please click Accept Answer and up-vote, this can be beneficial to other community members.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/349617\/does-compute-cluster-amp-endpoints-costs-if-we-don.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-08T15:33:31.617Z",
                "Answer_score":5,
                "Answer_body":"Hi @Bhaskar11-9991\n\nWhen a compute cluster is idle, it autoscales to 0 nodes, so you don't pay when it's not in use. A compute instance is always on and doesn't autoscale. You should stop the compute instance when you aren't using it to avoid extra cost.\nrefer - https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-target\n\n\n\n\nIf the Answer is helpful, please click Accept Answer and up-vote, this can be beneficial to other community members.",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1617896011616,
        "Question_original_content":"comput cluster endpoint cost dont delet us awar run comput instanc cost number hour stop need similarli comput cluster endpoint cost delet us thank bhaskar",
        "Question_preprocessed_content":"comput cluster endpoint cost dont delet us awar run comput instanc cost number hour stop need similarli comput cluster endpoint cost delet us thank bhaskar",
        "Question_gpt_summary_original":"The user is inquiring whether compute clusters and endpoints will continue to incur costs if they are not deleted after use, similar to how compute instances cost for the number of hours used.",
        "Question_gpt_summary":"user inquir comput cluster endpoint continu incur cost delet us similar comput instanc cost number hour",
        "Answer_original_content":"bhaskar comput cluster idl autoscal node pai us comput instanc autoscal stop comput instanc aren avoid extra cost refer http doc microsoft com azur machin learn concept comput target answer help click accept answer vote benefici commun member",
        "Answer_preprocessed_content":"comput cluster idl autoscal node pai us comput instanc autoscal stop comput instanc aren avoid extra cost refer answer help click accept answer benefici commun member",
        "Answer_gpt_summary_original":"Solution: Compute clusters do not incur costs when they are idle and autoscale to 0 nodes. However, compute instances are always on and do not autoscale, so it is recommended to stop them when not in use to avoid extra costs. The solution is provided in the reference link provided in the discussion.",
        "Answer_gpt_summary":"solut comput cluster incur cost idl autoscal node comput instanc autoscal recommend stop us avoid extra cost solut provid refer link provid discuss"
    },
    {
        "Question_title":"Use tensorboard with object detection API in sagemaker",
        "Question_body":"<p>with <a href=\"https:\/\/github.com\/svpino\/tensorflow-object-detection-sagemaker\" rel=\"nofollow noreferrer\">this<\/a> I successfully created a training job on sagemaker using the Tensorflow Object Detection API in a docker container. Now I'd like to monitor the training job using sagemaker, but cannot find anything explaining how to do it. I don't use a sagemaker notebook.\nI think I can do it by saving the logs into a S3 bucket and point there a local tensorboard instance .. but don't know how to tell the tensorflow object detection API where to save the logs (is there any command line argument for this ?).\nSomething like <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/keras_script_mode_pipe_mode_horovod\/tensorflow_keras_CIFAR10.ipynb\" rel=\"nofollow noreferrer\">this<\/a>, but the script <code>generate_tensorboard_command.py<\/code> fails because my training job don't have the <code>sagemaker_submit_directory<\/code> parameter..<\/p>\n<p>The fact is when I start the training job nothing is created on my s3 until the job finish and upload everything. There should be a way tell tensorflow where to save the logs (s3) during the training, hopefully without modifying the API source code..<\/p>\n<p><strong>Edit<\/strong><\/p>\n<p>I can finally make it works with the accepted solution (tensorflow natively supports read\/write to s3), there are however additional steps to do:<\/p>\n<ol>\n<li>Disable network isolation in the training job configuration<\/li>\n<li>Provide credentials to the docker image to write to S3 bucket<\/li>\n<\/ol>\n<p>The only thing is that Tensorflow continuously polls filesystem (i.e. looking for an updated model in serving mode) and this cause useless requests to S3, that you will have to pay (together with a buch of errors in the console). I opened a new question <a href=\"https:\/\/stackoverflow.com\/q\/64969198\/4267439\">here<\/a> for this. At least it works.<\/p>\n<p><strong>Edit 2<\/strong><\/p>\n<p>I was wrong, TF just write logs, is not polling so it's an expected behavior and the extra costs are minimal.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1590408759343,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1606323198012,
        "Question_score":2.0,
        "Question_view_count":311.0,
        "Answer_body":"<p>Looking through the example you posted, it looks as though the <code>model_dir<\/code> passed to the TensorFlow Object Detection package is configured to <code>\/opt\/ml\/model<\/code>:<\/p>\n<pre><code># These are the paths to where SageMaker mounts interesting things in your container.\nprefix = '\/opt\/ml\/'\ninput_path = os.path.join(prefix, 'input\/data')\noutput_path = os.path.join(prefix, 'output')\nmodel_path = os.path.join(prefix, 'model')\nparam_path = os.path.join(prefix, 'input\/config\/hyperparameters.json')\n<\/code><\/pre>\n<p>During the training process, tensorboard logs will be written to <code>\/opt\/ml\/model<\/code>, and then uploaded to s3 as a final model artifact AFTER training: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html<\/a>.<\/p>\n<p>You <em>might<\/em> be able to side-step the SageMaker artifact upload step and point the <code>model_dir<\/code> of TensorFlow Object Detection API directly at an s3 location during training:<\/p>\n<pre><code>model_path = &quot;s3:\/\/your-bucket\/path\/here\n<\/code><\/pre>\n<p>This means that the TensorFlow library within the SageMaker job is directly writing to S3 instead of the filesystem inside of it's container. Assuming the underlying TensorFlow Object Detection code can write directly to S3 (something you'll have to verify), you should be able to see the tensorboard logs and checkpoints there in realtime.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62002183",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1594137982056,
        "Question_original_content":"us tensorboard object detect api successfulli creat train job tensorflow object detect api docker contain like monitor train job explain us notebook think save log bucket point local tensorboard instanc know tell tensorflow object detect api save log command line argument like script gener tensorboard command fail train job submit directori paramet fact start train job creat job finish upload wai tell tensorflow save log train hopefulli modifi api sourc code edit final work accept solut tensorflow nativ support read write addit step disabl network isol train job configur provid credenti docker imag write bucket thing tensorflow continu poll filesystem look updat model serv mode caus useless request pai buch error consol open new question work edit wrong write log poll expect behavior extra cost minim",
        "Question_preprocessed_content":"us tensorboard object detect api successfulli creat train job tensorflow object detect api docker contain like monitor train job explain us notebook think save log bucket point local tensorboard instanc know tell tensorflow object detect api save log like script fail train job fact start train job creat job finish upload wai tell tensorflow save log train hopefulli modifi api sourc edit final work accept solut addit step disabl network isol train job configur provid credenti docker imag write bucket thing tensorflow continu poll filesystem caus useless request pai open new question work edit wrong write log poll expect behavior extra cost minim",
        "Question_gpt_summary_original":"The user successfully created a training job on sagemaker using the Tensorflow Object Detection API in a docker container, but is facing challenges in monitoring the training job using sagemaker. The user is unable to find any information on how to do it and is considering saving the logs into an S3 bucket and pointing to a local tensorboard instance. However, the user does not know how to tell the Tensorflow Object Detection API where to save the logs. The user has tried using a script but it failed because the training job does not have the required parameter. The user was able to make it work with the accepted solution, but had to disable network isolation in the training job configuration and provide credentials to the docker image to write to the S3 bucket. The user also discovered that Tensorflow continuously writes logs and does not poll the filesystem, which results in minimal extra costs.",
        "Question_gpt_summary":"user successfulli creat train job tensorflow object detect api docker contain face challeng monitor train job user unabl inform consid save log bucket point local tensorboard instanc user know tell tensorflow object detect api save log user tri script fail train job requir paramet user abl work accept solut disabl network isol train job configur provid credenti docker imag write bucket user discov tensorflow continu write log poll filesystem result minim extra cost",
        "Answer_original_content":"look exampl post look model dir pass tensorflow object detect packag configur opt model path mount interest thing contain prefix opt input path path join prefix input data output path path join prefix output model path path join prefix model param path path join prefix input config hyperparamet json train process tensorboard log written opt model upload final model artifact train http doc aw amazon com latest algorithm train algo envvari html abl step artifact upload step point model dir tensorflow object detect api directli locat train model path bucket path mean tensorflow librari job directli write instead filesystem insid contain assum underli tensorflow object detect code write directli verifi abl tensorboard log checkpoint realtim",
        "Answer_preprocessed_content":"look exampl post look pass tensorflow object detect packag configur train process tensorboard log written upload final model artifact train abl artifact upload step point tensorflow object detect api directli locat train mean tensorflow librari job directli write instead filesystem insid contain assum underli tensorflow object detect code write directli abl tensorboard log checkpoint realtim",
        "Answer_gpt_summary_original":"Solution: The user can configure the `model_dir` passed to the TensorFlow Object Detection package to `\/opt\/ml\/model`, where tensorboard logs will be written during the training process and then uploaded to S3 as a final model artifact after training. Alternatively, the user can point the `model_dir` of TensorFlow Object Detection API directly at an S3 location during training by setting `model_path = \"s3:\/\/your-bucket\/path\/here\"`. This means that the TensorFlow library within the SageMaker job is directly writing to S3 instead of the filesystem inside of its container. Assuming the underlying TensorFlow Object Detection code can write directly to S3, the user should be able to see the tensorboard logs and checkpoints there in real-time.",
        "Answer_gpt_summary":"solut user configur model dir pass tensorflow object detect packag opt model tensorboard log written train process upload final model artifact train altern user point model dir tensorflow object detect api directli locat train set model path bucket path mean tensorflow librari job directli write instead filesystem insid contain assum underli tensorflow object detect code write directli user abl tensorboard log checkpoint real time"
    },
    {
        "Question_title":"Where does the Azure Machine ACI Webservice deploy?",
        "Question_body":"<p>When we deploy a model as an ACIWebService in Azure Machine Learning Service, we do not need to specify any <code>deployment_target<\/code>.<\/p>\n<p>According to the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#deploy-workspace--name--models--inference-config-none--deployment-config-none--deployment-target-none--overwrite-false-\" rel=\"nofollow noreferrer\">AzureML documentation<\/a> for <code>azureml.core.model.model<\/code> class,<\/p>\n<pre><code>deployment_target\nComputeTarget\ndefault value: None\nA ComputeTarget to deploy the Webservice to. As Azure Container Instances has no associated ComputeTarget, leave this parameter as None to deploy to Azure Container Instances.\n<\/code><\/pre>\n<p>What does Microsoft mean by<\/p>\n<blockquote>\n<p>As Azure Container Instances has no associated ComputeTarget<\/p>\n<\/blockquote>\n<p>In which &quot;Compute Target&quot; is an ACIWebService deployed?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610952043250,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":317.0,
        "Answer_body":"<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/container-instances\/container-instances-overview\" rel=\"nofollow noreferrer\">Azure Container Instances<\/a> itself is the compute platform. It spins up a container in a serverless-fashion.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65769868",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1610958655960,
        "Question_original_content":"azur machin aci webservic deploi deploi model aciwebservic servic need specifi deploy target accord document core model model class deploy target computetarget default valu computetarget deploi webservic azur contain instanc associ computetarget leav paramet deploi azur contain instanc microsoft mean azur contain instanc associ computetarget comput target aciwebservic deploi",
        "Question_preprocessed_content":"azur machin aci webservic deploi deploi model aciwebservic servic need specifi accord document class microsoft mean azur contain instanc associ computetarget comput target aciwebservic deploi",
        "Question_gpt_summary_original":"The user is facing a challenge in understanding where an Azure Machine ACI Webservice is deployed as they do not need to specify a deployment target. The AzureML documentation states that the deployment target for an ACI Webservice is None, as Azure Container Instances has no associated ComputeTarget. The user is seeking clarification on where the Compute Target for an ACI Webservice is deployed.",
        "Question_gpt_summary":"user face challeng understand azur machin aci webservic deploi need specifi deploy target document state deploy target aci webservic azur contain instanc associ computetarget user seek clarif comput target aci webservic deploi",
        "Answer_original_content":"azur contain instanc comput platform spin contain serverless fashion",
        "Answer_preprocessed_content":"azur contain instanc comput platform spin contain",
        "Answer_gpt_summary_original":"Solution: The Compute Target for an Azure Machine ACI Webservice is deployed on Azure Container Instances, which is a serverless compute platform that spins up a container.",
        "Answer_gpt_summary":"solut comput target azur machin aci webservic deploi azur contain instanc serverless comput platform spin contain"
    },
    {
        "Question_title":"Relevancy of SageMaker Model Monitor for NLP?",
        "Question_body":"Hi,\n\nCan SageMaker Model Monitor be applied in NLP models? Is it necessary to do some preprocessing of the data? How can we use SageMaker Model Monitor? sentence length, unseen words, language etc. Any thoughts or experience on that?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1605005518000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":61.0,
        "Answer_body":"Yes, You can use model monitor for data capture and scheduling in your own custom container with the relevant monitoring for NLP use case.\nFor example, there's a blog post for model monitor for computer vision classification prediction with defined *alert * of predict more than expected.\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/automated-monitoring-of-your-machine-learning-models-with-amazon-sagemaker-model-monitor-and-sending-predictions-to-human-review-workflows-using-amazon-a2i\/?nc1=b_rp",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxCKLg-eiQ1mwZvzFyczBEg\/relevancy-of-sage-maker-model-monitor-for-nlp",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-10T11:24:02.000Z",
                "Answer_score":0,
                "Answer_body":"Yes, You can use model monitor for data capture and scheduling in your own custom container with the relevant monitoring for NLP use case.\nFor example, there's a blog post for model monitor for computer vision classification prediction with defined *alert * of predict more than expected.\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/automated-monitoring-of-your-machine-learning-models-with-amazon-sagemaker-model-monitor-and-sending-predictions-to-human-review-workflows-using-amazon-a2i\/?nc1=b_rp",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1605007442000,
        "Question_original_content":"relev model monitor nlp model monitor appli nlp model necessari preprocess data us model monitor sentenc length unseen word languag thought experi",
        "Question_preprocessed_content":"relev model monitor nlp model monitor appli nlp model necessari preprocess data us model monitor sentenc length unseen word languag thought experi",
        "Question_gpt_summary_original":"The user is seeking information on whether SageMaker Model Monitor can be used for NLP models and if any preprocessing of data is necessary. They are also asking for advice on how to use the tool for factors such as sentence length, unseen words, and language.",
        "Question_gpt_summary":"user seek inform model monitor nlp model preprocess data necessari ask advic us tool factor sentenc length unseen word languag",
        "Answer_original_content":"ye us model monitor data captur schedul custom contain relev monitor nlp us case exampl blog post model monitor vision classif predict defin alert predict expect http aw amazon com blog machin learn autom monitor machin learn model amazon model monitor send predict human review workflow amazon",
        "Answer_preprocessed_content":"ye us model monitor data captur schedul custom contain relev monitor nlp us case exampl blog post model monitor vision classif predict defin alert predict expect",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that SageMaker Model Monitor can be used for NLP models with data capture and scheduling in a custom container. The blog post shared in the discussion provides an example of using model monitor for computer vision classification prediction with defined alerts. However, no specific advice is provided for factors such as sentence length, unseen words, and language.",
        "Answer_gpt_summary":"solut discuss suggest model monitor nlp model data captur schedul custom contain blog post share discuss provid exampl model monitor vision classif predict defin alert specif advic provid factor sentenc length unseen word languag"
    },
    {
        "Question_title":"Can you share success stories of AWS customers performing ML CI\/CD?",
        "Question_body":"I want to create simple templates for scientists so that they can fit their models easily into a continuous integration\/continuous delivery (CI\/CD) pipeline. I want to know about success stories of AWS customers performing CI\/CD on machine learning pipelines.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592577511000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":32.0,
        "Answer_body":"Amazon has released the [Amazon SageMaker Pipelines][1] that are the first purpose-built CI\/CD service for machine learning: [1]: https:\/\/aws.amazon.com\/sagemaker\/pipelines\/\n\nFor more information, see [New \u2013 Amazon SageMaker Pipelines brings DevOps capabilities to your machine learning projects] [2] [2]: https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-pipelines-brings-devops-to-machine-learning-projects\/\n\nAdditionally, we have a case-study where a customer created one on their own for model development using Airflow. For more information, see [NerdWallet uses machine learning on AWS to power recommendations platform][3] and [Using Amazon SageMaker to build a machine learning platform with just three engineers][4]. [3]: https:\/\/aws.amazon.com\/solutions\/case-studies\/nerdwallet-case-study\/ [4]: https:\/\/www.nerdwallet.com\/blog\/engineering\/machine-learning-platform-amazon-sagemaker\/",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwLq6HNRZSOK7ODKKc_lC3Q\/can-you-share-success-stories-of-aws-customers-performing-ml-ci-cd",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-19T15:15:42.000Z",
                "Answer_score":1,
                "Answer_body":"Amazon has released the [Amazon SageMaker Pipelines][1] that are the first purpose-built CI\/CD service for machine learning: [1]: https:\/\/aws.amazon.com\/sagemaker\/pipelines\/\n\nFor more information, see [New \u2013 Amazon SageMaker Pipelines brings DevOps capabilities to your machine learning projects] [2] [2]: https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-pipelines-brings-devops-to-machine-learning-projects\/\n\nAdditionally, we have a case-study where a customer created one on their own for model development using Airflow. For more information, see [NerdWallet uses machine learning on AWS to power recommendations platform][3] and [Using Amazon SageMaker to build a machine learning platform with just three engineers][4]. [3]: https:\/\/aws.amazon.com\/solutions\/case-studies\/nerdwallet-case-study\/ [4]: https:\/\/www.nerdwallet.com\/blog\/engineering\/machine-learning-platform-amazon-sagemaker\/",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1592579742000,
        "Question_original_content":"share success stori aw custom perform want creat simpl templat scientist fit model easili continu integr continu deliveri pipelin want know success stori aw custom perform machin learn pipelin",
        "Question_preprocessed_content":"share success stori aw custom perform want creat simpl templat scientist fit model easili continu deliveri pipelin want know success stori aw custom perform machin learn pipelin",
        "Question_gpt_summary_original":"The user is seeking success stories of AWS customers who have implemented continuous integration\/continuous delivery (CI\/CD) on machine learning pipelines. They aim to create simple templates for scientists to easily fit their models into a CI\/CD pipeline.",
        "Question_gpt_summary":"user seek success stori aw custom implement continu integr continu deliveri machin learn pipelin aim creat simpl templat scientist easili fit model pipelin",
        "Answer_original_content":"amazon releas pipelin purpos built servic machin learn http aw amazon com pipelin inform new pipelin bring devop capabl machin learn project http aw amazon com blog aw amazon pipelin bring devop machin learn project addition case studi custom creat model develop airflow inform nerdwallet us machin learn aw power recommend platform build machin learn platform engin http aw amazon com solut case studi nerdwallet case studi http nerdwallet com blog engin machin learn platform amazon",
        "Answer_preprocessed_content":"amazon releas servic machin learn inform addition custom creat model develop airflow inform",
        "Answer_gpt_summary_original":"Solution: Amazon has released the Amazon SageMaker Pipelines, which is the first purpose-built CI\/CD service for machine learning. There is also a case-study where a customer created their own CI\/CD pipeline for model development using Airflow.",
        "Answer_gpt_summary":"solut amazon releas pipelin purpos built servic machin learn case studi custom creat pipelin model develop airflow"
    },
    {
        "Question_title":"Image push with Kaniko stuck on ContainersNotReady containers with unready status: [polyaxon-main polyaxon-sidecar]",
        "Question_body":"This question was resolved and discussed on Polyaxon Slack. Posting for visibility if someone stumbles upon the same issue.\n\nGiven setup:\nDocker-registry provider: Amazon Elastic Container Registry (ECR)\nPolyaxon version: 1.7.3 CE\nDeployed with Kubernetes on AWS\nAnd credentials setup from Kaniko github: Pushing to Amazon ECR\nAnd Kaniko integration in polyaxon-config.yml:\nconnections:\n  - name: docker-registry\n    kind: registry\n    description: \"aws docker repository\"\n    schema:\n      url: https:\/\/ID.dkr.ecr.SOME-REGION.amazonaws.com\n    secret:\n      name: docker-conf\n      mountPath: \/kaniko\/.docker\nAnd polyaxonfile.yml from polyaxon example:\nversion: 1.1\nkind: operation\nname: build\nparams:\n  destination:\n    connection: docker-registry\n    value: polyaxon-examples:ml\nrunPatch:\n  init:\n  - dockerfile:\n      image: \"tensorflow\/tensorflow:2.0.1-py3\"\n      run:\n      - 'pip3 install --no-cache-dir -U polyaxon[\"polyboard\",\"polytune\"]'\n      langEnv: 'en_US.UTF-8'\nhubRef: kaniko\nThen warning was raised:\n\nand job would be stuck like this until manually stopped.\n\nTYPE     STATUS    REASON              MESSAGE                                                           LAST_UPDATE_TIME    LAST_TRANSITION_TIME\n-------  --------  ------------------  ----------------------------------------------------------------  ------------------  ----------------------\nwarning  True      ContainersNotReady  containers with unready status: [polyaxon-main polyaxon-sidecar]  a few seconds ago   a few seconds ago",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619182392000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":null,
        "Answer_body":"Resolution:\n\nThe issue was with aws-secret type.\n\nAt first we had the type of secret that expires every 12 hours.\nChanging it to one that does not expire allowed successful connection and push of built image.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1296",
        "Tool":"Polyaxon",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-23T12:57:53Z",
                "Answer_score":1,
                "Answer_body":"Resolution:\n\nThe issue was with aws-secret type.\n\nAt first we had the type of secret that expires every 12 hours.\nChanging it to one that does not expire allowed successful connection and push of built image."
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"imag push kaniko stuck containersnotreadi contain unreadi statu main sidecar question resolv discuss slack post visibl stumbl issu given setup docker registri provid amazon elast contain registri ecr version deploi kubernet aw credenti setup kaniko github push amazon ecr kaniko integr config yml connect docker registri kind registri descript aw docker repositori schema url http dkr ecr region amazonaw com secret docker conf mountpath kaniko docker file yml exampl version kind oper build param destin connect docker registri valu exampl runpatch init dockerfil imag tensorflow tensorflow run pip instal cach dir polyboard polytun langenv utf hubref kaniko warn rais job stuck like manual stop type statu reason messag updat time transit time warn true containersnotreadi contain unreadi statu main sidecar second ago second ago",
        "Question_preprocessed_content":"imag push kaniko stuck containersnotreadi contain unreadi statu question resolv discuss slack post visibl stumbl issu given setup provid amazon elast contain registri version deploi kubernet aw credenti setup kaniko github push amazon ecr kaniko integr connect kind registri descript aw docker repositori schema url secret mountpath exampl version kind oper build param destin connect valu exampl runpatch init dockerfil imag run pip instal langenv hubref kaniko warn rais job stuck like manual stop type statu reason messag warn true containersnotreadi contain unreadi statu second ago second ago",
        "Question_gpt_summary_original":"The user encountered a challenge with Kaniko integration in Polyaxon, where the image push was stuck on ContainersNotReady containers with unready status. The setup involved using Amazon Elastic Container Registry (ECR) as the Docker-registry provider, Polyaxon version 1.7.3 CE, and Kubernetes on AWS. The warning message indicated that the containers were not ready, and the job would be stuck until manually stopped.",
        "Question_gpt_summary":"user encount challeng kaniko integr imag push stuck containersnotreadi contain unreadi statu setup involv amazon elast contain registri ecr docker registri provid version kubernet aw warn messag indic contain readi job stuck manual stop",
        "Answer_original_content":"resolut issu aw secret type type secret expir hour chang expir allow success connect push built imag",
        "Answer_preprocessed_content":"resolut issu type type secret expir hour chang expir allow success connect push built imag",
        "Answer_gpt_summary_original":"Solution: The issue with Kaniko integration in Polyaxon was resolved by changing the type of aws-secret from one that expires every 12 hours to one that does not expire. This allowed for successful connection and push of the built image.",
        "Answer_gpt_summary":"solut issu kaniko integr resolv chang type aw secret expir hour expir allow success connect push built imag"
    },
    {
        "Question_title":"How to assign users in SageMaker Studio?",
        "Question_body":"<p>I have successfully created SageMaker Studio- Status in service. However, it asks me to asks to assign users to it and I don't have any listed. Are these users from my IAM (I have many) or my organization (I have a couple).<\/p>\n\n<p>Where am I supposed to find these users?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1576591296847,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1576641179672,
        "Question_score":1.0,
        "Question_view_count":1029.0,
        "Answer_body":"<hr \/>\n<p>Did you setup SageMaker Studio to use AWS SSO or IAM for the authentication method?<\/p>\n<p>From what I have gathered, the SageMaker Studio users, when setup using IAM for the authentication method are not actually users. They just provide partitions within Studio for different work environments. You can then control access to these partitions using IAM policies for your IAM users \/ roles for federated users.<\/p>\n<p>Each Studio user has it's own URL to access that environment.<\/p>\n<p>Here is the SageMaker developer guide: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf#%5B%7B%22num%22%3A14340%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C72%2C533.986%2Cnull%5D\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf#%5B%7B%22num%22%3A14340%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C72%2C533.986%2Cnull%5D<\/a><\/p>\n<p>Page 36 discusses on-boarding with IAM.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59375896",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1603758115467,
        "Question_original_content":"assign user studio successfulli creat studio statu servic ask ask assign user list user iam organ coupl suppos user",
        "Question_preprocessed_content":"assign user studio successfulli creat studio statu servic ask ask assign user list user iam organ suppos user",
        "Question_gpt_summary_original":"The user has encountered a challenge in assigning users to their SageMaker Studio as they are not listed and they are unsure where to find them. They are unsure if the users are from their IAM or organization.",
        "Question_gpt_summary":"user encount challeng assign user studio list unsur unsur user iam organ",
        "Answer_original_content":"setup studio us aw sso iam authent method gather studio user setup iam authent method actual user provid partit studio differ work environ control access partit iam polici iam user role feder user studio user url access environ develop guid http doc aw amazon com latest pdf num gen xyz cnull page discuss board iam",
        "Answer_preprocessed_content":"setup studio us aw sso iam authent method gather studio user setup iam authent method actual user provid partit studio differ work environ control access partit iam polici iam user role feder user studio user url access environ develop guid page discuss iam",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that if SageMaker Studio is set up using IAM for authentication, the users are not actually users but provide partitions within Studio for different work environments. Access to these partitions can be controlled using IAM policies for IAM users or roles for federated users. Each Studio user has its own URL to access that environment. The SageMaker developer guide provides more information on on-boarding with IAM on page 36.",
        "Answer_gpt_summary":"solut discuss suggest studio set iam authent user actual user provid partit studio differ work environ access partit control iam polici iam user role feder user studio user url access environ develop guid provid inform board iam page"
    },
    {
        "Question_title":"How to format parameter of data type json in a aws cloudformation yaml template?",
        "Question_body":"<p>The yaml template <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-properties-sagemaker-model-containerdefinition.html\" rel=\"nofollow noreferrer\">documentation<\/a> for the AWS Cloudformation AWS::SageMaker::Model ContainerDefinition specifies that &quot;Environment&quot; is of type Json. I can't work out how to submit json in my yaml template that does not cause a &quot;CREATE_FAILED    Internal Failure&quot; after running a deploy with the below command.<\/p>\n<pre><code>aws cloudformation deploy --stack-name test1 --template-file test-template-export.yml\n<\/code><\/pre>\n<p>test-template-export.yml<\/p>\n<pre><code>Description: Example yaml\n\nResources:\n  Model:\n    Type: AWS::SageMaker::Model\n    Properties:\n      Containers:\n      - ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n      - Environment: '{&quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;}'\n      ExecutionRoleArn: arn:aws:iam::123456789123:role\/service-role\/AmazonSageMakerServiceCatalogProductsUseRole\n<\/code><\/pre>\n<p>I have also tried the below formats as well and still no luck.<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n  Environment: '{&quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;}'\n<\/code><\/pre>\n<p>--<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n- Environment: | \n         {\n            &quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;\n          }\n<\/code><\/pre>\n<p>--<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n- Environment:\n  - SAGEMAKER_CONTAINER_LOG_LEVEL: &quot;20&quot;\n<\/code><\/pre>\n<p>Running without Environment deploys fine.<\/p>\n<p>I have tried everything in <a href=\"https:\/\/stackoverflow.com\/questions\/39041209\/how-to-specify-json-formatted-string-in-cloudformation\">this answer.<\/a>\nHow do I format this Environment argument?<\/p>\n<p>My version of aws cli is &quot;aws-cli\/2.4.10 Python\/3.8.8&quot;<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643283742143,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":391.0,
        "Answer_body":"<p>Hi when you see json format think more dict.\nSo write it like this:<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n  Environment:\n     SAGEMAKER_CONTAINER_LOG_LEVEL: 20\n<\/code><\/pre>\n<p>For IAM Policies the PolicyDocument is json type and this is how AWS do it in their exempel:<\/p>\n<pre><code>Type: 'AWS::IAM::Policy'\nProperties:\n  PolicyName: CFNUsers\n  PolicyDocument:\n    Version: &quot;2012-10-17&quot;\n    Statement:\n      - Effect: Allow\n        Action:\n          - 'cloudformation:Describe*'\n          - 'cloudformation:List*'\n          - 'cloudformation:Get*'\n        Resource: '*'\n  Groups:\n    - !Ref CFNUserGroup\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1643297093190,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70877982",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1643284402368,
        "Question_original_content":"format paramet data type json aw cloudform yaml templat yaml templat document aw cloudform aw model containerdefinit specifi environ type json work submit json yaml templat caus creat fail intern failur run deploi command aw cloudform deploi stack test templat file test templat export yml test templat export yml descript exampl yaml resourc model type aw model properti contain modelpackagenam arn aw east model packag environ contain log level executionrolearn arn aw iam role servic role amazonservicecatalogproductsuserol tri format luck contain modelpackagenam arn aw east model packag environ contain log level contain modelpackagenam arn aw east model packag environ contain log level contain modelpackagenam arn aw east model packag environ contain log level run environ deploi fine tri answer format environ argument version aw cli aw cli python",
        "Question_preprocessed_content":"format paramet data type json aw cloudform yaml templat yaml templat document aw cloudform aw model containerdefinit specifi environ type json work submit json yaml templat caus intern failur run deploi command tri format luck run environ deploi fine tri answer format environ argument version aw cli",
        "Question_gpt_summary_original":"The user is facing challenges in formatting the parameter of data type json in an AWS CloudFormation YAML template. Specifically, they are unable to submit JSON in their YAML template without causing a \"CREATE_FAILED Internal Failure\" error. The user has tried various formats for the JSON parameter, but none have worked. The issue seems to be with the \"Environment\" parameter of the AWS::SageMaker::Model ContainerDefinition.",
        "Question_gpt_summary":"user face challeng format paramet data type json aw cloudform yaml templat specif unabl submit json yaml templat caus creat fail intern failur error user tri format json paramet work issu environ paramet aw model containerdefinit",
        "Answer_original_content":"json format think dict write like contain modelpackagenam arn aw east model packag environ contain log level iam polici policydocu json type aw exempel type aw iam polici properti policynam cfnuser policydocu version statement effect allow action cloudform cloudform list cloudform resourc group ref cfnusergroup",
        "Answer_preprocessed_content":"json format think dict write like iam polici policydocu json type aw exempel",
        "Answer_gpt_summary_original":"Solution:\nThe solution provided in the discussion is to format the JSON parameter as a dictionary in the YAML template. The user can write the JSON parameter in the YAML template as shown in the example provided in the discussion. For IAM Policies, the PolicyDocument is in JSON format, and the user can follow the AWS example to format it correctly in the YAML template.",
        "Answer_gpt_summary":"solut solut provid discuss format json paramet dictionari yaml templat user write json paramet yaml templat shown exampl provid discuss iam polici policydocu json format user follow aw exampl format correctli yaml templat"
    },
    {
        "Question_title":"azure ml designer: how to call a pipeline from another pipeline",
        "Question_body":"I'm using ML Designer and i have created a sub-pipeline that i want to use it in other pipelines. how do i call that sub-pipeline from the designer?\n\nThe purpose of the subpipeline is to transform data, so the output is a dataset.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619288778083,
        "Question_favorite_count":7.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@javier-8889Thanks for the question. Can you please add more details about the pipeline steps. You can implement an AML pipeline with Python code, but also with the new AML designer which under the covers is creating an AML Pipeline defining that \u201cvisual workflow\u201d. Basically you can register a trained model in Designer bring it out with SDK\/CLI to deploy it. Currently only Data Drift Monitor (Data Drift->EventGrid->LogicApp->Trigger Pipeline) allows to trigger a pipeline when dataset drift has been detected.\n\nWhen designing a pipeline in Azure ML Designer, each step or module creates intermediate datasets that can be seen using the UI using Visualize option. Those datasets are persisted in the blob storage.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/370433\/azure-ml-designer-how-to-call-a-pipeline-from-anot.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-26T14:59:45.637Z",
                "Answer_score":0,
                "Answer_body":"@javier-8889Thanks for the question. Can you please add more details about the pipeline steps. You can implement an AML pipeline with Python code, but also with the new AML designer which under the covers is creating an AML Pipeline defining that \u201cvisual workflow\u201d. Basically you can register a trained model in Designer bring it out with SDK\/CLI to deploy it. Currently only Data Drift Monitor (Data Drift->EventGrid->LogicApp->Trigger Pipeline) allows to trigger a pipeline when dataset drift has been detected.\n\nWhen designing a pipeline in Azure ML Designer, each step or module creates intermediate datasets that can be seen using the UI using Visualize option. Those datasets are persisted in the blob storage.",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1619449185636,
        "Question_original_content":"design pipelin pipelin design creat sub pipelin want us pipelin sub pipelin design purpos subpipelin transform data output dataset",
        "Question_preprocessed_content":"design pipelin pipelin design creat want us pipelin design purpos subpipelin transform data output dataset",
        "Question_gpt_summary_original":"The user is facing a challenge in calling a sub-pipeline created in Azure ML Designer from other pipelines. The sub-pipeline is designed to transform data and produce a dataset as output.",
        "Question_gpt_summary":"user face challeng call sub pipelin creat design pipelin sub pipelin design transform data produc dataset output",
        "Answer_original_content":"javier thank question add detail pipelin step implement aml pipelin python code new aml design cover creat aml pipelin defin visual workflow basic regist train model design bring sdk cli deploi current data drift monitor data drift eventgrid logicapp trigger pipelin allow trigger pipelin dataset drift detect design pipelin design step modul creat intermedi dataset seen visual option dataset persist blob storag",
        "Answer_preprocessed_content":"question add detail pipelin step implement aml pipelin python code new aml design cover creat aml pipelin defin visual workflow basic regist train model design bring deploi current data drift monitor allow trigger pipelin dataset drift detect design pipelin design step modul creat intermedi dataset seen visual option dataset persist blob storag",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Non-interactive login to registered dataset",
        "Question_body":"I'm trying to tune hyperparameters similar to the following guide: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters\n\nMy PyTorch Dataset in train.py contains:\n\n ws = Workspace.from_config()\n ds = Dataset.get_by_name(ws, 'train')\n df = ds.to_pandas_dataframe()\n\nThis code works fine when run from the command-line, but when I submit a hyperparam tuning job to each node of a training cluster, I get the following error:\n\nWe could not find config.json in: \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/adamml\/azureml\/hd_ba15bb39-f0fe-47a7-afbc-d2f9968e9687_3\/mounts\/workspaceblobstore\/azureml\/HD_ba15bb39-f0fe-47a7-afbc-d2f9968e9687_3 or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.\n\nIf I manually pass my subscription id, resource group, and workspace id, I don't get this error, but now every single hyperparam tuning experiment requires me to log in through the web portal. Is there a way to do a non-interactive login?",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1595346764937,
        "Question_favorite_count":36.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"If I read the post correctly, you were trying to get an registered dataset within a submitted run. There, Workspace.from_config() won't work since there is no config.json file as the error suggested.\n\nAnd when you created an auth object which is InteractiveLoginAuth, it is expected to perform interactive login.\n\nWithin a run the recommended way to connect to current workspace it via:\n\nfrom azureml.core import Run\nrun = Run.get_context().experiment.workspace\n\n\n\nMeanwhile, there is way to pass in an dataset object to a run without involving register and workspace signin. If that fit your scenario better, please refer to the example in this document https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-with-datasets#access-and-explore-input-datasets\n\nfrom azureml.core import Dataset, Run\nrun = Run.get_context()\n# get the input dataset by name\ndataset = run.input_datasets['titanic']\n# load the TabularDataset to pandas DataFrame\ndf = dataset.to_pandas_dataframe()",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/50386\/non-interactive-login-to-registered-dataset.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-23T20:05:42.24Z",
                "Answer_score":2,
                "Answer_body":"If I read the post correctly, you were trying to get an registered dataset within a submitted run. There, Workspace.from_config() won't work since there is no config.json file as the error suggested.\n\nAnd when you created an auth object which is InteractiveLoginAuth, it is expected to perform interactive login.\n\nWithin a run the recommended way to connect to current workspace it via:\n\nfrom azureml.core import Run\nrun = Run.get_context().experiment.workspace\n\n\n\nMeanwhile, there is way to pass in an dataset object to a run without involving register and workspace signin. If that fit your scenario better, please refer to the example in this document https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-with-datasets#access-and-explore-input-datasets\n\nfrom azureml.core import Dataset, Run\nrun = Run.get_context()\n# get the input dataset by name\ndataset = run.input_datasets['titanic']\n# load the TabularDataset to pandas DataFrame\ndf = dataset.to_pandas_dataframe()",
                "Answer_comment_count":3,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1595534742240,
        "Question_original_content":"non interact login regist dataset try tune hyperparamet similar follow guid http doc microsoft com azur machin learn tune hyperparamet pytorch dataset train contain workspac config dataset train panda datafram code work fine run command line submit hyperparam tune job node train cluster follow error config json mnt batch task share root job adamml babb ffe afbc dfe mount workspaceblobstor babb ffe afbc dfe parent directori provid path config file ensur config json exist parent directori manual pass subscript resourc group workspac error singl hyperparam tune experi requir log web portal wai non interact login",
        "Question_preprocessed_content":"login regist dataset try tune hyperparamet similar follow guid pytorch dataset contain train code work fine run submit hyperparam tune job node train cluster follow error parent directori provid path config file ensur exist parent directori manual pass subscript resourc group workspac error singl hyperparam tune experi requir log web portal wai login",
        "Question_gpt_summary_original":"The user is encountering an error when submitting a hyperparameter tuning job to a training cluster due to the inability to find config.json in the specified directory. Manually passing subscription id, resource group, and workspace id resolves the issue but requires logging in through the web portal for every hyperparameter tuning experiment. The user is seeking a solution for non-interactive login.",
        "Question_gpt_summary":"user encount error submit hyperparamet tune job train cluster inabl config json specifi directori manual pass subscript resourc group workspac resolv issu requir log web portal hyperparamet tune experi user seek solut non interact login",
        "Answer_original_content":"read post correctli try regist dataset submit run workspac config won work config json file error suggest creat auth object interactiveloginauth expect perform interact login run recommend wai connect current workspac core import run run run context experi workspac wai pass dataset object run involv regist workspac signin fit scenario better refer exampl document http doc microsoft com azur machin learn train dataset access explor input dataset core import dataset run run run context input dataset dataset run input dataset titan load tabulardataset panda datafram dataset panda datafram",
        "Answer_preprocessed_content":"read post correctli try regist dataset submit run won work file error suggest creat auth object interactiveloginauth expect perform interact login run recommend wai connect current workspac core import run run wai pass dataset object run involv regist workspac signin fit scenario better refer exampl document core import dataset run run input dataset dataset load tabulardataset panda datafram",
        "Answer_gpt_summary_original":"Solution: The recommended way to connect to the current workspace within a run is via \"from azureml.core import Run; run = Run.get_context().experiment.workspace\". Another solution is to pass in a dataset object to a run without involving register and workspace signin. This can be done by using \"from azureml.core import Dataset, Run; run = Run.get_context(); dataset = run.input_datasets['titanic']; df = dataset.to_pandas_dataframe()\".",
        "Answer_gpt_summary":"solut recommend wai connect current workspac run core import run run run context experi workspac solut pass dataset object run involv regist workspac signin core import dataset run run run context dataset run input dataset titan dataset panda datafram"
    },
    {
        "Question_title":"Authenticate DVC python api with GitHub app credentials",
        "Question_body":"<p>Hi Folks,<\/p>\n<p>I am trying to use dvc python api which refers to private GitHub repo.<\/p>\n<pre><code class=\"lang-auto\">dvc.api.get_url(\n    path='data\/data.json',\n    repo='https:\/\/github.com\/owner\/private-repo.git'\n)\n<\/code><\/pre>\n<p>Without any credentials I get <code>dulwich.client.HTTPUnauthorized: No valid credentials provided<\/code> error.<\/p>\n<p>I have registered Github app and want to use credentials of same to do some operations using python api. Can someone help me with how do we provide credentials to dvc python api?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1666090734944,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":128.0,
        "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/harshad\">@Harshad<\/a>!<br>\nDo you have an ssh access to the repo?<br>\nReplacing <code>https<\/code> address with <code>git<\/code> address might help. In your case it could be:<br>\n<code>repo='https:\/\/github.com\/owner\/private-repo.git'<\/code> \u2192 <code>repo=git@github.com:owner\/private-repo.git<\/code><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/authenticate-dvc-python-api-with-github-app-credentials\/1364",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-27T11:59:24.394Z",
                "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/harshad\">@Harshad<\/a>!<br>\nDo you have an ssh access to the repo?<br>\nReplacing <code>https<\/code> address with <code>git<\/code> address might help. In your case it could be:<br>\n<code>repo='https:\/\/github.com\/owner\/private-repo.git'<\/code> \u2192 <code>repo=git@github.com:owner\/private-repo.git<\/code><\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"authent python api github app credenti folk try us python api refer privat github repo api url path data data json repo http github com owner privat repo git credenti dulwich client httpunauthor valid credenti provid error regist github app want us credenti oper python api help provid credenti python api",
        "Question_preprocessed_content":"authent python api github app credenti folk try us python api refer privat github repo credenti error regist github app want us credenti oper python api help provid credenti python api",
        "Question_gpt_summary_original":"The user is facing challenges in using the DVC Python API to access a private GitHub repository. They are receiving an error message stating that no valid credentials have been provided. The user has registered a GitHub app and is seeking assistance on how to provide the app's credentials to the DVC Python API.",
        "Question_gpt_summary":"user face challeng python api access privat github repositori receiv error messag state valid credenti provid user regist github app seek assist provid app credenti python api",
        "Answer_original_content":"hello harshad ssh access repo replac http address git address help case repo http github com owner privat repo git repo git github com owner privat repo git",
        "Answer_preprocessed_content":"hello ssh access repo replac address address help case",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to replace the \"https\" address with the \"git\" address and use the latter to access the private GitHub repository. The user is advised to check if they have ssh access to the repository and modify the repository address accordingly.",
        "Answer_gpt_summary":"solut possibl solut mention discuss replac http address git address us access privat github repositori user advis check ssh access repositori modifi repositori address accordingli"
    },
    {
        "Question_title":"Can I make Amazon SageMaker deliver a recommendation based on historic data instead of a probability score?",
        "Question_body":"<p>We have a huge set of data in CSV format, containing a few numeric elements, like this:<\/p>\n\n<pre><code>Year,BinaryDigit,NumberToPredict,JustANumber, ...other stuff\n1954,1,762,16, ...other stuff\n1965,0,142,16, ...other stuff\n1977,1,172,16, ...other stuff\n<\/code><\/pre>\n\n<p>The thing here is that there is a strong correlation between the third column and the columns before that. So I have pre-processed the data and it's now available in a format I think is perfect:<\/p>\n\n<pre><code>1954,1,762\n1965,0,142\n1977,1,172\n<\/code><\/pre>\n\n<p>What I want is a predicition on the value in the third column, using the first two as input. So in the case above, I want the input 1965,0 to return 142. In real life this file is thousands of rows, but since there's a pattern, I'd like to retrieve the most possible value.<\/p>\n\n<p>So far I've setup a train job on the CSV file using the L<em>inear Learner<\/em> algorithm, with the following settings:<\/p>\n\n<pre><code>label_size = 1\nfeature_dim = 2\npredictor_type = regression\n<\/code><\/pre>\n\n<p>I've also created a model from it, and setup an endpoint. When I invoke it, I get a score in return.<\/p>\n\n<pre><code>    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                   ContentType='text\/csv',\n                                   Body=payload)\n<\/code><\/pre>\n\n<p>My goal here is to get the third column prediction instead. How can I achieve that? I have read a lot of the documentation regarding this, but since I'm not very familiar with AWS, I might as well have used the wrong algorithms for what I am trying to do.<\/p>\n\n<p>(Please feel free to edit this question to better suit AWS terminology)<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1552553455710,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1552895653107,
        "Question_score":1.0,
        "Question_view_count":271.0,
        "Answer_body":"<p>For csv input, the label should be in the first column, as mentioned <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/cdf-training.html\" rel=\"nofollow noreferrer\">here<\/a>:  So you should preprocess your data to put the label (the column you want to predict) on the left.<\/p>\n\n<p>Next, you need to decide whether this is a regression problem or a classification problem. <\/p>\n\n<p>If you want to predict a number that's as close as possible to the true number, that's regression. For example, the truth might be 4, and the model might predict 4.15. If you need an integer prediction, you could round the model's output.<\/p>\n\n<p>If you want the prediction to be one of a few categories, then you have a classification problem. For example, we might encode 'North America' = 0, 'Europe' = 1, 'Africa' = 2, and so on. In this case, a fractional prediction wouldn't make sense. <\/p>\n\n<p>For regression, use <code>'predictor_type' = 'regressor'<\/code> and for classification with more than 2 classes, use <code>'predictor_type' = 'multiclass_classifier'<\/code> as documented <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>The output of regression will contain only a <code>'score'<\/code> field, which is the model's prediction. The output of multiclass classification will contain a <code>'predicted_label'<\/code> field, which is the model's prediction, as well as a <code>'score'<\/code> field, which is a vector of probabilities representing the model's confidence. The index with the highest probability will be the one that's predicted as the <code>'predicted_label'<\/code>. The output formats are documented <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/LL-in-formats.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55158307",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1553880598532,
        "Question_original_content":"deliv recommend base histor data instead probabl score huge set data csv format contain numer element like year binarydigit numbertopredict justanumb stuff stuff stuff stuff thing strong correl column column pre process data avail format think perfect want predicit valu column input case want input return real life file thousand row pattern like retriev possibl valu far setup train job csv file linear learner algorithm follow set label size featur dim predictor type regress creat model setup endpoint invok score return respons runtim invok endpoint endpointnam endpoint contenttyp text csv bodi payload goal column predict instead achiev read lot document familiar aw wrong algorithm try feel free edit question better suit aw terminolog",
        "Question_preprocessed_content":"deliv recommend base histor data instead probabl score huge set data csv format contain numer element like thing strong correl column column data avail format think perfect want predicit valu column input case want input return real life file thousand row pattern like retriev possibl valu far setup train job csv file linear learner algorithm follow set creat model setup endpoint invok score return goal column predict instead achiev read lot document familiar aw wrong algorithm try feel free edit question better suit aw terminolog",
        "Question_gpt_summary_original":"The user has a large dataset in CSV format with a strong correlation between the third column and the columns before it. They have pre-processed the data and want to predict the value in the third column using the first two columns as input. The user has set up a train job using the Linear Learner algorithm with regression settings and created a model with an endpoint. However, when invoking the endpoint, they receive a score instead of the desired prediction for the third column. The user is seeking guidance on how to achieve their goal.",
        "Question_gpt_summary":"user larg dataset csv format strong correl column column pre process data want predict valu column column input user set train job linear learner algorithm regress set creat model endpoint invok endpoint receiv score instead desir predict column user seek guidanc achiev goal",
        "Answer_original_content":"csv input label column mention preprocess data label column want predict left need decid regress problem classif problem want predict number close possibl true number regress exampl truth model predict need integ predict round model output want predict categori classif problem exampl encod north america europ africa case fraction predict wouldn sens regress us predictor type regressor classif class us predictor type multiclass classifi document output regress contain score field model predict output multiclass classif contain predict label field model predict score field vector probabl repres model confid index highest probabl predict predict label output format document",
        "Answer_preprocessed_content":"csv input label column mention preprocess data label left need decid regress problem classif problem want predict number close possibl true number regress exampl truth model predict need integ predict round model output want predict categori classif problem exampl encod north america europ africa case fraction predict wouldn sens regress us classif class us document output regress contain field model predict output multiclass classif contain field model predict field vector probabl repres model confid index highest probabl predict output format document",
        "Answer_gpt_summary_original":"The user should preprocess their data to put the label (the column they want to predict) on the left. They need to decide whether this is a regression problem or a classification problem. For regression, use 'predictor_type' = 'regressor' and for classification with more than 2 classes, use 'predictor_type' = 'multiclass_classifier'. The output of regression will contain only a 'score' field, which is the model's prediction. The output of multiclass classification will contain a 'predicted_label' field, which is the model's prediction, as well as a 'score' field, which is a vector of probabilities representing the model's confidence.",
        "Answer_gpt_summary":"user preprocess data label column want predict left need decid regress problem classif problem regress us predictor type regressor classif class us predictor type multiclass classifi output regress contain score field model predict output multiclass classif contain predict label field model predict score field vector probabl repres model confid"
    },
    {
        "Question_title":"Trying to use the 'log_metric() function.",
        "Question_body":"I used\u00a0 'with mlflow.start_run()\u00a0 to log and show the params in the web ui and it works,\u00a0 when i try to use log_metric() i cant get it to show in the ui.\u00a0\n\n\nI used a variable called 'history' to store the values output from model.compile, then a function to extract the float value, and then pass that variable to log_metric()\u00a0\n\n\nmy 'history' variable is not getting anything stored in it though.",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1541184539000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":19.0,
        "Answer_body":"Can you try replacing the 'history' variable with something simpler like a literal number, say 10 and see if that value shows up in the UI?\n\n\nThen if that works, work backwards and confirm the variable you are passing is a simple numberic value.\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/67c34783-3f24-4fb8-a33b-981e596bee02%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.. I dont follow you.\u00a0 replace 'history' with what ?\n\ue5d3. Hi Paul,\n\n\nAndy's suggestion was to replace history['loss'], history['metrics'] with a static constant value and verify that the constant gets reported on UI. This will verify that the setup was correct and mlflow APIs are storing values appropriately and UI can read those.\u00a0\n\n\nSo your example would look like\n\u00a0 mlflow.log_metric(\"metric\", 10.12)\n\n\nIf you haven't tried it already,\u00a0quickstart example would also be a good one to try.\u00a0\n\n\nHowever, the error message in the picture suggests that \"history\" might be getting set to None at some point before reaching line 116.\n\n\nRegards,\n\n\n\nMani Parkhe\n\nma...@databricks.com\n\n\n\n\n\n\n\n\n\n\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/7dd6e1c6-9067-4df8-9beb-fb8191055889%40googlegroups.com.\n\ue5d3. \ue5d3. I was able to log an int an a float. what is the syntax to capture the real data and pass i to log_metric()\u00a0 ?\u00a0\n\ue5d3. Next step is to dig into the Keras API docs and figure out what functions Keras provides that you can use to access the metrics of the model that you're looking to log.\n\n\nAndy\n\n\n\ue5d3\n\ue5d3\n--\n\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\n\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/658d5364-fc30-4d27-bf90-3dae926ce169%40googlegroups.com.\n\ue5d3. A potentially handy reference per Andy's call out is\u00a0How to Use MLflow to Experiment a Keras Network Model: Binary Classification for Movie Reviews.\u00a0 HTH!\n\n\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CALEZFQzkuxgN7SDpWFaC8T_oLfmaR2eekandF6EToO%3DucJnRKQ%40mail.gmail.com.\n\ue5d3. thanks. but I have already seen that example and i tried to implement it into my specific needs and i still doesnt work.\u00a0 'history = model.compile()'\u00a0 my history variable keeps getting nothing stored into it.\u00a0\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/qZONNmbAASk",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-11-02T19:53:41",
                "Answer_body":"Can you try replacing the 'history' variable with something simpler like a literal number, say 10 and see if that value shows up in the UI?\n\n\nThen if that works, work backwards and confirm the variable you are passing is a simple numberic value.\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/67c34783-3f24-4fb8-a33b-981e596bee02%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2018-11-02T20:24:44",
                "Answer_body":"I dont follow you.\u00a0 replace 'history' with what ?\n\ue5d3"
            },
            {
                "Answer_creation_time":"2018-11-02T21:17:16",
                "Answer_body":"Hi Paul,\n\n\nAndy's suggestion was to replace history['loss'], history['metrics'] with a static constant value and verify that the constant gets reported on UI. This will verify that the setup was correct and mlflow APIs are storing values appropriately and UI can read those.\u00a0\n\n\nSo your example would look like\n\u00a0 mlflow.log_metric(\"metric\", 10.12)\n\n\nIf you haven't tried it already,\u00a0quickstart example would also be a good one to try.\u00a0\n\n\nHowever, the error message in the picture suggests that \"history\" might be getting set to None at some point before reaching line 116.\n\n\nRegards,\n\n\n\nMani Parkhe\n\nma...@databricks.com\n\n\n\n\n\n\n\n\n\n\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/7dd6e1c6-9067-4df8-9beb-fb8191055889%40googlegroups.com.\n\ue5d3"
            },
            {
                "Answer_creation_time":"2018-11-02T22:33:04",
                "Answer_body":"\ue5d3"
            },
            {
                "Answer_creation_time":"2018-11-02T22:34:26",
                "Answer_body":"I was able to log an int an a float. what is the syntax to capture the real data and pass i to log_metric()\u00a0 ?\u00a0\n\ue5d3"
            },
            {
                "Answer_creation_time":"2018-11-02T23:48:20",
                "Answer_body":"Next step is to dig into the Keras API docs and figure out what functions Keras provides that you can use to access the metrics of the model that you're looking to log.\n\n\nAndy\n\n\n\ue5d3\n\ue5d3\n--\n\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\n\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/658d5364-fc30-4d27-bf90-3dae926ce169%40googlegroups.com.\n\ue5d3"
            },
            {
                "Answer_creation_time":"2018-11-03T01:31:29",
                "Answer_body":"A potentially handy reference per Andy's call out is\u00a0How to Use MLflow to Experiment a Keras Network Model: Binary Classification for Movie Reviews.\u00a0 HTH!\n\n\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CALEZFQzkuxgN7SDpWFaC8T_oLfmaR2eekandF6EToO%3DucJnRKQ%40mail.gmail.com.\n\ue5d3"
            },
            {
                "Answer_creation_time":"2018-11-03T17:26:01",
                "Answer_body":"thanks. but I have already seen that example and i tried to implement it into my specific needs and i still doesnt work.\u00a0 'history = model.compile()'\u00a0 my history variable keeps getting nothing stored into it.\u00a0\n\ue5d3"
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"try us log metric function start run log param web work try us log metric variabl call histori store valu output model compil function extract float valu pass variabl log metric histori variabl get store",
        "Question_preprocessed_content":"try us function log param web work try us variabl call histori store valu output function extract float valu pass variabl histori variabl get store",
        "Question_gpt_summary_original":"The user is facing challenges while trying to use the 'log_metric() function. They were able to use 'with mlflow.start_run() to log and show the params in the web UI, but they are unable to get log_metric() to show in the UI. The user is using a variable called 'history' to store the values output from model.compile, but it is not getting anything stored in it.",
        "Question_gpt_summary":"user face challeng try us log metric function abl us start run log param web unabl log metric user variabl call histori store valu output model compil get store",
        "Answer_original_content":"try replac histori variabl simpler like liter number valu show work work backward confirm variabl pass simpl number valu receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user ebe googlegroup com option visit http group googl com optout dont follow replac histori paul andi suggest replac histori loss histori metric static constant valu verifi constant get report verifi setup correct api store valu appropri read exampl look like log metric metric haven tri quickstart exampl good try error messag pictur suggest histori get set point reach line regard mani parkh databrick com view discuss web visit http group googl com msgid user ddec beb googlegroup com abl log int float syntax captur real data pass log metric step dig kera api doc figur function kera provid us access metric model look log andi receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user daec googlegroup com potenti handi refer andi ishow us experi kera network model binari classif movi review hth view discuss web visit http group googl com msgid user calezfqzkuxgnsdpwfact olfmareekandfetoo ducjnrkq mail gmail com thank seen exampl tri implement specif need doesnt work histori model compil histori variabl keep get store",
        "Answer_preprocessed_content":"try replac histori variabl simpler like liter number valu show work work backward confirm variabl pass simpl number valu receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit dont follow replac histori paul andi suggest replac histori histori static constant valu verifi constant get report verifi setup correct api store valu appropri read exampl look like haven tri quickstart exampl good try error messag pictur suggest histori get set point reach line regard mani parkh view discuss web visit abl log int float syntax captur real data pass step dig kera api doc figur function kera provid us access metric model look log andi receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit potenti handi refer andi ishow us experi kera network model binari classif movi review hth view discuss web visit thank seen exampl tri implement specif need doesnt work histori histori variabl keep get store",
        "Answer_gpt_summary_original":"Solutions provided:\n- Replace the 'history' variable with a simple numeric value to verify that the setup was correct and mlflow APIs are storing values appropriately and UI can read those.\n- Dig into the Keras API docs and figure out what functions Keras provides that you can use to access the metrics of the model that you're looking to log.\n- Refer to the article \"How to Use MLflow to Experiment a Keras Network Model: Binary Classification for Movie Reviews\" for a potentially handy reference. \n\nNo solution was provided for the issue of the 'history' variable not getting anything stored in it.",
        "Answer_gpt_summary":"solut provid replac histori variabl simpl numer valu verifi setup correct api store valu appropri read dig kera api doc figur function kera provid us access metric model look log refer articl us experi kera network model binari classif movi review potenti handi refer solut provid issu histori variabl get store"
    },
    {
        "Question_title":"MLflow proxied artifact access: Unable to locate credentials",
        "Question_body":"<p>I am using MLflow to track my experiments. I am using an S3 bucket as an artifact store. For acessing it, I want to use <em>proxied artifact access<\/em>, as described in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>, however this does not work for me, since it locally looks for credentials (but the server should handle this).<\/p>\n<h2>Expected Behaviour<\/h2>\n<p>As described in the docs, I would expect that locally, I do not need to specify my AWS credentials, since the server handles this for me. From <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n<blockquote>\n<p>This eliminates the need to allow end users to have direct path access to a remote object store (e.g., s3, adls, gcs, hdfs) for artifact handling and eliminates the need for an end-user to provide access credentials to interact with an underlying object store.<\/p>\n<\/blockquote>\n<h2>Actual Behaviour \/ Error<\/h2>\n<p>Whenever I run an experiment on my machine, I am running into the following error:<\/p>\n<p><code>botocore.exceptions.NoCredentialsError: Unable to locate credentials<\/code><\/p>\n<p>So the error is local. However, this should not happen since the server should handle the auth instead of me needing to store my credentials locally. Also, I would expect that I would not even need library <code>boto3<\/code> locally.<\/p>\n<h2>Solutions Tried<\/h2>\n<p>I am aware that I need to create a new experiment, because existing experiments might still use a different artifact location which is proposed in <a href=\"https:\/\/stackoverflow.com\/a\/71417933\/10465165\">this SO answer<\/a> as well as in the note in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>. Creating a new experiment did not solve the error for me. Whenever I run the experiment, I get an explicit log in the console validating this:<\/p>\n<p><code>INFO mlflow.tracking.fluent: Experiment with name 'test' does not exist. Creating a new experiment.<\/code><\/p>\n<p>Related Questions (<a href=\"https:\/\/stackoverflow.com\/questions\/72206086\/cant-log-mlflow-artifacts-to-s3-with-docker-based-tracking-server\">#1<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/72236258\/mlflow-unable-to-store-artifacts-to-s3\/72261826#comment128726676_72261826\">#2<\/a>) refer to a different scenario, which is also <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-4-mlflow-with-remote-tracking-server-backend-and-artifact-stores\" rel=\"nofollow noreferrer\">described in the docs<\/a><\/p>\n<h2>Server Config<\/h2>\n<p>The server runs on a kubernetes pod with the following config:<\/p>\n<pre><code>mlflow server \\\n    --host 0.0.0.0 \\\n    --port 5000 \\\n    --backend-store-uri postgresql:\/\/user:pw@endpoint \\\n    --artifacts-destination s3:\/\/my_bucket\/artifacts \\\n    --serve-artifacts \\\n    --default-artifact-root s3:\/\/my_bucket\/artifacts \\\n<\/code><\/pre>\n<p>I would expect my config to be correct, looking at doc <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">page 1<\/a> and <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#using-the-tracking-server-for-proxied-artifact-access\" rel=\"nofollow noreferrer\">page 2<\/a><\/p>\n<p>I am able to see the mlflow UI if I forward the port to my local machine. I also see the experiment runs as failed, because of the error I sent above.<\/p>\n<h2>My Code<\/h2>\n<p>The relevant part of my code which fails is the logging of the model:<\/p>\n<pre><code>mlflow.set_tracking_uri(&quot;http:\/\/localhost:5000&quot;)\nmlflow.set_experiment(&quot;test2)\n\n...\n\n# this works\nmlflow.log_params(hyperparameters)\n                        \nmodel = self._train(model_name, hyperparameters, X_train, y_train)\ny_pred = model.predict(X_test)\nself._evaluate(y_test, y_pred)\n\n# this fails with the error from above\nmlflow.sklearn.log_model(model, &quot;artifacts&quot;)\n\n<\/code><\/pre>\n<h2>Question<\/h2>\n<p>I am probably overlooking something. Is there a need to locally indicate that I want to use proxied artified access? If yes, how do I do this? Is there something I have missed?<\/p>\n<h2>Full Traceback<\/h2>\n<pre><code>  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/models\/model.py&quot;, line 295, in log\n    mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 726, in log_artifacts\n    MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py&quot;, line 1001, in log_artifacts\n    self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py&quot;, line 346, in log_artifacts\n    self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/store\/artifact\/s3_artifact_repo.py&quot;, line 141, in log_artifacts\n    self._upload_file(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/store\/artifact\/s3_artifact_repo.py&quot;, line 117, in _upload_file\n    s3_client.upload_file(Filename=local_file, Bucket=bucket, Key=key, ExtraArgs=extra_args)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/boto3\/s3\/inject.py&quot;, line 143, in upload_file\n    return transfer.upload_file(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/boto3\/s3\/transfer.py&quot;, line 288, in upload_file\n    future.result()\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/futures.py&quot;, line 103, in result\n    return self._coordinator.result()\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/futures.py&quot;, line 266, in result\n    raise self._exception\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/tasks.py&quot;, line 139, in __call__\n    return self._execute_main(kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/tasks.py&quot;, line 162, in _execute_main\n    return_value = self._main(**kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/upload.py&quot;, line 758, in _main\n    client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 508, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 898, in _make_api_call\n    http, parsed_response = self._make_request(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 921, in _make_request\n    return self._endpoint.make_request(operation_model, request_dict)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 119, in make_request\n    return self._send_request(request_dict, operation_model)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 198, in _send_request\n    request = self.create_request(request_dict, operation_model)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 134, in create_request\n    self._event_emitter.emit(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 412, in emit\n    return self._emitter.emit(aliased_event_name, **kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 256, in emit\n    return self._emit(event_name, kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 239, in _emit\n    response = handler(**kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/signers.py&quot;, line 103, in handler\n    return self.sign(operation_name, request)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/signers.py&quot;, line 187, in sign\n    auth.add_auth(request)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/auth.py&quot;, line 407, in add_auth\n    raise NoCredentialsError()\nbotocore.exceptions.NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1657122030593,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1657122771947,
        "Question_score":0.0,
        "Question_view_count":237.0,
        "Answer_body":"<p>The problem is that the server is running on wrong run parameters, the <code>--default-artifact-root<\/code> needs to either be removed or set to <code>mlflow-artifacts:\/<\/code>.<\/p>\n<p>From <code>mlflow server --help<\/code>:<\/p>\n<pre><code>  --default-artifact-root URI  Directory in which to store artifacts for any\n                               new experiments created. For tracking server\n                               backends that rely on SQL, this option is\n                               required in order to store artifacts. Note that\n                               this flag does not impact already-created\n                               experiments with any previous configuration of\n                               an MLflow server instance. By default, data\n                               will be logged to the mlflow-artifacts:\/ uri\n                               proxy if the --serve-artifacts option is\n                               enabled. Otherwise, the default location will\n                               be .\/mlruns.\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72886409",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1657186814368,
        "Question_original_content":"proxi artifact access unabl locat credenti track experi bucket artifact store acess want us proxi artifact access describ doc work local look credenti server handl expect behaviour describ doc expect local need specifi aw credenti server handl doc elimin need allow end user direct path access remot object store adl gc hdf artifact handl elimin need end user provid access credenti interact underli object store actual behaviour error run experi machin run follow error botocor except nocredentialserror unabl locat credenti error local happen server handl auth instead need store credenti local expect need librari boto local solut tri awar need creat new experi exist experi us differ artifact locat propos answer note doc creat new experi solv error run experi explicit log consol valid info track fluent experi test exist creat new experi relat question refer differ scenario describ doc server config server run kubernet pod follow config server host port backend store uri postgresql user endpoint artifact destin bucket artifact serv artifact default artifact root bucket artifact expect config correct look doc page page abl forward port local machin experi run fail error sent code relev code fail log model set track uri http localhost set experi test work log param hyperparamet model self train model hyperparamet train train pred model predict test self evalu test pred fail error sklearn log model model artifact question probabl overlook need local indic want us proxi artifi access ye miss traceback file dir venv lib python site packag model model line log track fluent log artifact local path artifact path file dir venv lib python site packag track fluent line log artifact client log artifact run local dir artifact path file dir venv lib python site packag track client line log artifact self track client log artifact run local dir artifact path file dir venv lib python site packag track track servic client line log artifact self artifact repo run log artifact local dir artifact path file dir venv lib python site packag store artifact artifact repo line log artifact self upload file file dir venv lib python site packag store artifact artifact repo line upload file client upload file filenam local file bucket bucket kei kei extraarg extra arg file dir venv lib python site packag boto inject line upload file return transfer upload file file dir venv lib python site packag boto transfer line upload file futur result file dir venv lib python site packag stransfer futur line result return self coordin result file dir venv lib python site packag stransfer futur line result rais self except file dir venv lib python site packag stransfer task line return self execut main kwarg file dir venv lib python site packag stransfer task line execut main return valu self main kwarg file dir venv lib python site packag stransfer upload line main client object bucket bucket kei kei bodi bodi extra arg file dir venv lib python site packag botocor client line api return self api oper kwarg file dir venv lib python site packag botocor client line api http pars respons self request file dir venv lib python site packag botocor client line request return self endpoint request oper model request dict file dir venv lib python site packag botocor endpoint line request return self send request request dict oper model file dir venv lib python site packag botocor endpoint line send request request self creat request request dict oper model file dir venv lib python site packag botocor endpoint line creat request self event emitt emit file dir venv lib python site packag botocor hook line emit return self emitt emit alias event kwarg file dir venv lib python site packag botocor hook line emit return self emit event kwarg file dir venv lib python site packag botocor hook line emit respons handler kwarg file dir venv lib python site packag botocor signer line handler return self sign oper request file dir venv lib python site packag botocor signer line sign auth add auth request file dir venv lib python site packag botocor auth line add auth rais nocredentialserror botocor except nocredentialserror unabl locat credenti",
        "Question_preprocessed_content":"proxi artifact access unabl locat credenti track experi bucket artifact store acess want us proxi artifact access describ doc work local look credenti expect behaviour describ doc expect local need specifi aw credenti server handl doc elimin need allow end user direct path access remot object store artifact handl elimin need provid access credenti interact underli object store actual behaviour error run experi machin run follow error error local happen server handl auth instead need store credenti local expect need librari local solut tri awar need creat new experi exist experi us differ artifact locat propos answer note doc creat new experi solv error run experi explicit log consol valid relat question refer differ scenario describ doc server config server run kubernet pod follow config expect config correct look doc page page abl forward port local machin experi run fail error sent code relev code fail log model question probabl overlook need local indic want us proxi artifi access ye miss traceback",
        "Question_gpt_summary_original":"The user is encountering an error while using MLflow to track experiments with an S3 bucket as an artifact store. They are attempting to use proxied artifact access, but are receiving a \"NoCredentialsError\" locally, despite the server being configured to handle authentication. The user has tried creating a new experiment and ensuring their server configuration is correct, but the error persists. They are seeking assistance in resolving the issue.",
        "Question_gpt_summary":"user encount error track experi bucket artifact store attempt us proxi artifact access receiv nocredentialserror local despit server configur handl authent user tri creat new experi ensur server configur correct error persist seek assist resolv issu",
        "Answer_original_content":"problem server run wrong run paramet default artifact root need remov set artifact server help default artifact root uri directori store artifact new experi creat track server backend reli sql option requir order store artifact note flag impact creat experi previou configur server instanc default data log artifact uri proxi serv artifact option enabl default locat mlrun",
        "Answer_preprocessed_content":"problem server run wrong run paramet need remov set",
        "Answer_gpt_summary_original":"Solution: The solution to the problem is to ensure that the server is running on the correct run parameters. The user needs to either remove the <code>--default-artifact-root<\/code> or set it to <code>mlflow-artifacts:\/<\/code>. This can be done by referring to the <code>mlflow server --help<\/code> command.",
        "Answer_gpt_summary":"solut solut problem ensur server run correct run paramet user need remov default artifact root set artifact refer server help command"
    },
    {
        "Question_title":"Brewing up custom ML models on AWS SageMaker",
        "Question_body":"<p>Iam new with SageMaker and I try to use my own sickit-learn algorithm . For this I use Docker.\nI try to do the same task as described here in this github account : <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a><\/p>\n\n<p>My question is should I create manually the repository <strong><code>\/opt\/ml<\/code><\/strong>  (I work with windows OS) ?<\/p>\n\n<p>Can you explain me please?<\/p>\n\n<p>thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1533919042793,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":106.0,
        "Answer_body":"<p>You don't need to create <code>\/opt\/ml<\/code>, SageMaker will do it for you when it launches your training job.<\/p>\n\n<p>The contents of the <code>\/opt\/ml<\/code> directory are determined by the parameters you pass to the CreateTrainingJob API call. The scikit example notebook you linked to describes this (look at the <strong>Running your container<\/strong> sections). You can find more info about this in the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">Create a Training Job<\/a> section of the main SageMaker documentation.<\/p>\n\n<hr>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51790720",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1533928824743,
        "Question_original_content":"brew custom model iam new try us sickit learn algorithm us docker try task describ github account http github com awslab amazon exampl blob master advanc function scikit bring scikit bring ipynb question creat manual repositori opt work window explain thank",
        "Question_preprocessed_content":"brew custom model iam new try us algorithm us docker try task describ github account question creat manual repositori explain thank",
        "Question_gpt_summary_original":"The user is facing a challenge in using their own sickit-learn algorithm on AWS SageMaker through Docker. They are unsure whether they need to manually create the repository \"\/opt\/ml\" and are seeking clarification on this issue.",
        "Question_gpt_summary":"user face challeng sickit learn algorithm docker unsur need manual creat repositori opt seek clarif issu",
        "Answer_original_content":"need creat opt launch train job content opt directori determin paramet pass createtrainingjob api scikit exampl notebook link describ look run contain section info creat train job section main document",
        "Answer_preprocessed_content":"need creat launch train job content directori determin paramet pass createtrainingjob api scikit exampl notebook link describ info creat train job section main document",
        "Answer_gpt_summary_original":"Solution: The user does not need to manually create the repository \"\/opt\/ml\" as AWS SageMaker will do it for them when launching the training job. The contents of the directory are determined by the parameters passed to the CreateTrainingJob API call. The scikit example notebook and the Create a Training Job section of the main SageMaker documentation provide more information on this.",
        "Answer_gpt_summary":"solut user need manual creat repositori opt launch train job content directori determin paramet pass createtrainingjob api scikit exampl notebook creat train job section main document provid inform"
    },
    {
        "Question_title":"ML Pickle file size Azure Machine Learning Service",
        "Question_body":"Is there any restriction on registering an ML pickle model into Azure Machine Learning Service in terms of the size of the pickle file?\n\nDoes it cause latency in realtime data processing and getting the prediction results from the pickle file if we have a model that let's say it 5MB and the other one is 500MB (The bigger file has better performance in terms of accuracy)?\nThanks,\n\nJohn",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1599612419390,
        "Question_favorite_count":4.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"@JA-4570 Thanks, For ACI we recommend not using a model over 1GB in size.\nFor AKS you are limited by the memory resources that you request for your service, minus about 500mb for the running python process in the pod.\n\nThere will be no difference in prediction speed once the model is successfully deployed.\nRegistering will take longer as we have to upload the model, and deploying will take longer as the service must download the model.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/89630\/ml-pickle-file-size-azure-machine-learning-service.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-09-11T04:54:35.417Z",
                "Answer_score":0,
                "Answer_body":"@JA-4570 Thanks, For ACI we recommend not using a model over 1GB in size.\nFor AKS you are limited by the memory resources that you request for your service, minus about 500mb for the running python process in the pod.\n\nThere will be no difference in prediction speed once the model is successfully deployed.\nRegistering will take longer as we have to upload the model, and deploying will take longer as the service must download the model.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1599800075416,
        "Question_original_content":"pickl file size servic restrict regist pickl model servic term size pickl file caus latenc realtim data process get predict result pickl file model let bigger file better perform term accuraci thank john",
        "Question_preprocessed_content":"pickl file size servic restrict regist pickl model servic term size pickl file caus latenc realtim data process get predict result pickl file model let thank john",
        "Question_gpt_summary_original":"The user is inquiring about any restrictions on registering an ML pickle model into Azure Machine Learning Service in terms of the size of the pickle file. They are also concerned about the potential latency in real-time data processing and getting prediction results from a larger pickle file, even if it has better performance in terms of accuracy.",
        "Question_gpt_summary":"user inquir restrict regist pickl model servic term size pickl file concern potenti latenc real time data process get predict result larger pickl file better perform term accuraci",
        "Answer_original_content":"thank aci recommend model size ak limit memori resourc request servic minu run python process pod differ predict speed model successfulli deploi regist longer upload model deploi longer servic download model",
        "Answer_preprocessed_content":"thank aci recommend model size ak limit memori resourc request servic minu run python process pod differ predict speed model successfulli deploi regist longer upload model deploi longer servic download model",
        "Answer_gpt_summary_original":"Solutions provided in the discussion are:\n\n- For ACI, it is recommended not to use a model over 1GB in size.\n- For AKS, the user is limited by the memory resources that they request for their service, minus about 500mb for the running python process in the pod.\n- There will be no difference in prediction speed once the model is successfully deployed.\n- Registering will take longer as the model needs to be uploaded, and deploying will take longer as the service must download the model.",
        "Answer_gpt_summary":"solut provid discuss aci recommend us model size ak user limit memori resourc request servic minu run python process pod differ predict speed model successfulli deploi regist longer model need upload deploi longer servic download model"
    },
    {
        "Question_title":"Connect 2 separate experiments via webservice - Azure MLS Classic",
        "Question_body":"I have 2 experiments A and B in Azure MLS classic. I need the web service output of experiment A as one of the web service inputs for experiment B. Please let me know if it is possible and if yes, how I can do it.",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1592407262483,
        "Question_favorite_count":4.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"I used export module in experiment A and import module in experiment B to transfer the output of A as input of B.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/37128\/connect-2-separate-experiments-via-webservice-azur.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-17T22:36:57.623Z",
                "Answer_score":0,
                "Answer_body":"I used export module in experiment A and import module in experiment B to transfer the output of A as input of B.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1592433417623,
        "Question_original_content":"connect separ experi webservic classic experi classic need web servic output experi web servic input experi let know possibl ye",
        "Question_preprocessed_content":"connect separ experi webservic classic experi classic need web servic output experi web servic input experi let know possibl ye",
        "Question_gpt_summary_original":"The user is facing a challenge of connecting two separate experiments, A and B, in Azure MLS classic via a webservice. They need the output of experiment A as one of the inputs for experiment B and are seeking guidance on how to achieve this.",
        "Question_gpt_summary":"user face challeng connect separ experi classic webservic need output experi input experi seek guidanc achiev",
        "Answer_original_content":"export modul experi import modul experi transfer output input",
        "Answer_preprocessed_content":"export modul experi import modul experi transfer output input",
        "Answer_gpt_summary_original":"Solution: The user suggested using the export module in experiment A and the import module in experiment B to transfer the output of A as input of B.",
        "Answer_gpt_summary":"solut user suggest export modul experi import modul experi transfer output input"
    },
    {
        "Question_title":"ERROR conda.core.link:_execute(502): An error occurred while installing package 'conda-forge::astor-0.7.1-py_0'",
        "Question_body":"<p>I am trying to follow a Python tutorial and I have been able to execute almost everything, until the point of Deploying an endpoint to Azure with python.<\/p>\n\n<p>In order to give some context I have uploaded the scripts to my git account:\n<a href=\"https:\/\/github.com\/levalencia\/MLTutorial\" rel=\"nofollow noreferrer\">https:\/\/github.com\/levalencia\/MLTutorial<\/a><\/p>\n\n<p>File 1 and 2 Work perfectly fine<\/p>\n\n<p>However the following section in File 3 fails:<\/p>\n\n<pre><code>%%time\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\n\ninference_config = InferenceConfig(runtime= \"python\", \n                                   entry_script=\"score.py\",\n                                   conda_file=\"myenv.yml\")\n\nservice = Model.deploy(workspace=ws, \n                       name='keras-mnist-svc2', \n                       models=[amlModel], \n                       inference_config=inference_config, \n                       deployment_config=aciconfig)\n\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n\n<p>with below error:<\/p>\n\n<pre><code>ERROR - Service deployment polling reached non-successful terminal state, current service state: Transitioning\nOperation ID: 8353cad2-4218-450a-a03b-df418725acb1\nMore information can be found here: https:\/\/machinelearnin1143382465.blob.core.windows.net\/azureml\/ImageLogs\/8353cad2-4218-450a-a03b-df418725acb1\/build.log?sv=2018-03-28&amp;sr=b&amp;sig=UKzefxIrm3l7OsXxj%2FT4RsvUfAuhuaBwaz2P4mJu7vY%3D&amp;st=2020-03-11T12%3A23%3A33Z&amp;se=2020-03-11T20%3A28%3A33Z&amp;sp=r\nError:\n{\n  \"code\": \"EnvironmentBuildFailed\",\n  \"statusCode\": 400,\n  \"message\": \"Failed Building the Environment.\"\n}\n\nERROR - Service deployment polling reached non-successful terminal state, current service state: Transitioning\nOperation ID: 8353cad2-4218-450a-a03b-df418725acb1\nMore information can be found here: https:\/\/machinelearnin1143382465.blob.core.windows.net\/azureml\/ImageLogs\/8353cad2-4218-450a-a03b-df418725acb1\/build.log?sv=2018-03-28&amp;sr=b&amp;sig=UKzefxIrm3l7OsXxj%2FT4RsvUfAuhuaBwaz2P4mJu7vY%3D&amp;st=2020-03-11T12%3A23%3A33Z&amp;se=2020-03-11T20%3A28%3A33Z&amp;sp=r\nError:\n{\n  \"code\": \"EnvironmentBuildFailed\",\n  \"statusCode\": 400,\n  \"message\": \"Failed Building the Environment.\"\n}\n<\/code><\/pre>\n\n<p>When I download the logs, I got this:<\/p>\n\n<pre><code>wheel-0.34.2         | 24 KB     |            |   0% [0m[91m\nwheel-0.34.2         | 24 KB     | ########## | 100% [0m\nDownloading and Extracting Packages\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... failed\n[91m\nERROR conda.core.link:_execute(502): An error occurred while installing package 'conda-forge::astor-0.7.1-py_0'.\nFileNotFoundError(2, \"No such file or directory: '\/azureml-envs\/azureml_6abde325a12ccdba9b5ba76900b99b56\/bin\/python3.6'\")\nAttempting to roll back.\n\n[0mRolling back transaction: ...working... done\n[91m\nFileNotFoundError(2, \"No such file or directory: '\/azureml-envs\/azureml_6abde325a12ccdba9b5ba76900b99b56\/bin\/python3.6'\")\n\n\n[0mThe command '\/bin\/sh -c ldconfig \/usr\/local\/cuda\/lib64\/stubs &amp;&amp; conda env create -p \/azureml-envs\/azureml_6abde325a12ccdba9b5ba76900b99b56 -f azureml-environment-setup\/mutated_conda_dependencies.yml &amp;&amp; rm -rf \"$HOME\/.cache\/pip\" &amp;&amp; conda clean -aqy &amp;&amp; CONDA_ROOT_DIR=$(conda info --root) &amp;&amp; rm -rf \"$CONDA_ROOT_DIR\/pkgs\" &amp;&amp; find \"$CONDA_ROOT_DIR\" -type d -name __pycache__ -exec rm -rf {} + &amp;&amp; ldconfig' returned a non-zero code: 1\n2020\/03\/11 12:28:11 Container failed during run: acb_step_0. No retries remaining.\nfailed to run step ID: acb_step_0: exit status 1\n\nRun ID: cb3 failed after 2m21s. Error: failed during run, err: exit status 1\n<\/code><\/pre>\n\n<p>Update 1:<\/p>\n\n<p>I tried to run:\nconda list    --name base  conda<\/p>\n\n<p>inside the notebook and I got this:<\/p>\n\n<pre><code> # packages in environment at \/anaconda:\n    #\n    # Name                    Version                   Build  Channel\n    _anaconda_depends         2019.03                  py37_0  \n    anaconda                  custom                   py37_1  \n    anaconda-client           1.7.2                    py37_0  \n    anaconda-navigator        1.9.6                    py37_0  \n    anaconda-project          0.8.4                      py_0  \n    conda                     4.8.2                    py37_0  \n    conda-build               3.17.6                   py37_0  \n    conda-env                 2.6.0                         1  \n    conda-package-handling    1.6.0            py37h7b6447c_0  \n    conda-verify              3.1.1                    py37_0  \n\n    Note: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n\n<p>However in the deployment log I got this:<\/p>\n\n<pre><code>Solving environment: ...working... \ndone\n[91m\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 4.5.11\n  latest version: 4.8.2\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1583931341127,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1584001089836,
        "Question_score":2.0,
        "Question_view_count":5656.0,
        "Answer_body":"<p>Unfortunately there seems to an issue with this version of Conda (4.5.11). To complete this task in the tutorial, you can just update the dependency for Tensorflow and Keras to be from <code>pip<\/code> and not <code>conda<\/code>. There are reasons why this is less than ideal for a production environment. The Azure ML <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py\" rel=\"nofollow noreferrer\">documentation states<\/a>:<\/p>\n\n<blockquote>\n  <p>\"If your dependency is available through both Conda and pip (from\n  PyPi), use the Conda version, as Conda packages typically come with\n  pre-built binaries that make installation more reliable.\"<\/p>\n<\/blockquote>\n\n<p>In this case though, if you update the following code block:<\/p>\n\n<pre><code>from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies()\nmyenv.add_conda_package(\"tensorflow\")\nmyenv.add_conda_package(\"keras\")\n\nwith open(\"myenv.yml\",\"w\") as f:\n    f.write(myenv.serialize_to_string())\n\n# Review environment file\nwith open(\"myenv.yml\",\"r\") as f:\n    print(f.read())\n<\/code><\/pre>\n\n<p>To be the following:<\/p>\n\n<pre><code>from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies()\nmyenv.add_pip_package(\"tensorflow==2.0.0\")\nmyenv.add_pip_package(\"azureml-defaults\")\nmyenv.add_pip_package(\"keras\")\n\nwith open(\"myenv.yml\", \"w\") as f:\n    f.write(myenv.serialize_to_string())\n\nwith open(\"myenv.yml\", \"r\") as f:\n    print(f.read())\n<\/code><\/pre>\n\n<p>The tutorial should be able to be completed.  Let me know if any of this does not work for you once this update has been made.<\/p>\n\n<p>I have also reported this issue to Microsoft (in regards to the Conda version).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60636558",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1584015256823,
        "Question_original_content":"error conda core link execut error occur instal packag conda forg astor try follow python tutori abl execut point deploi endpoint azur python order context upload script git account http github com levalencia mltutori file work perfectli fine follow section file fail time core webservic import webservic core model import inferenceconfig infer config inferenceconfig runtim python entri script score conda file myenv yml servic model deploi workspac kera mnist svc model amlmodel infer config infer config deploy config aciconfig servic wait deploy output true error error servic deploy poll reach non success termin state current servic state transit oper cad dfacb inform http machinelearnin blob core window net imagelog cad dfacb build log sig ukzefxirmlosxxj ftrsvufauhuabwazpmjuvi error code environmentbuildfail statuscod messag fail build environ error servic deploy poll reach non success termin state current servic state transit oper cad dfacb inform http machinelearnin blob core window net imagelog cad dfacb build log sig ukzefxirmlosxxj ftrsvufauhuabwazpmjuvi error code environmentbuildfail statuscod messag fail build environ download log got wheel wheel download extract packag prepar transact work verifi transact work execut transact work fail error conda core link execut error occur instal packag conda forg astor filenotfounderror file directori env abdeaccdbabbabb bin python attempt roll mroll transact work filenotfounderror file directori env abdeaccdbabbabb bin python mthe command bin ldconfig usr local cuda lib stub conda env creat env abdeaccdbabbabb environ setup mutat conda depend yml home cach pip conda clean aqi conda root dir conda info root conda root dir pkg conda root dir type pycach exec ldconfig return non zero code contain fail run acb step retri remain fail run step acb step exit statu run fail error fail run err exit statu updat tri run conda list base conda insid notebook got packag environ anaconda version build channel anaconda depend anaconda custom anaconda client anaconda navig anaconda project conda conda build conda env conda packag handl pyhbc conda verifi note need restart kernel us updat packag deploy log got solv environ work warn newer version conda exist current version latest version updat conda run conda updat base default conda",
        "Question_preprocessed_content":"error error occur instal packag try follow python tutori abl execut point deploi endpoint azur python order context upload script git account file work perfectli fine follow section file fail error download log got updat tri run conda list base conda insid notebook got deploy log got",
        "Question_gpt_summary_original":"The user encountered an error while trying to deploy an endpoint to Azure with Python. The error occurred during the installation of the package 'conda-forge::astor-0.7.1-py_0'. The deployment log showed that a newer version of conda exists and needs to be updated. The user also tried to run 'conda list --name base conda' inside the notebook, which showed a different version of conda than what was shown in the deployment log.",
        "Question_gpt_summary":"user encount error try deploi endpoint azur python error occur instal packag conda forg astor deploy log show newer version conda exist need updat user tri run conda list base conda insid notebook show differ version conda shown deploy log",
        "Answer_original_content":"unfortun issu version conda complet task tutori updat depend tensorflow kera pip conda reason ideal product environ document state depend avail conda pip pypi us conda version conda packag typic come pre built binari instal reliabl case updat follow code block core conda depend import condadepend myenv condadepend myenv add conda packag tensorflow myenv add conda packag kera open myenv yml write myenv serial string review environ file open myenv yml print read follow core conda depend import condadepend myenv condadepend myenv add pip packag tensorflow myenv add pip packag default myenv add pip packag kera open myenv yml write myenv serial string open myenv yml print read tutori abl complet let know work updat report issu microsoft regard conda version",
        "Answer_preprocessed_content":"unfortun issu version conda complet task tutori updat depend tensorflow kera reason ideal product environ document state depend avail conda pip us conda version conda packag typic come binari instal case updat follow code block follow tutori abl complet let know work updat report issu microsoft",
        "Answer_gpt_summary_original":"Solution: The user can update the dependency for Tensorflow and Keras to be from pip and not conda. The user needs to update the code block to add pip packages instead of conda packages. The updated code block is provided in the discussion. The tutorial should be able to be completed with this update. The issue with the Conda version has been reported to Microsoft.",
        "Answer_gpt_summary":"solut user updat depend tensorflow kera pip conda user need updat code block add pip packag instead conda packag updat code block provid discuss tutori abl complet updat issu conda version report microsoft"
    },
    {
        "Question_title":"Deploying custom model on Azure ML Studio",
        "Question_body":"<p>In Azure ML Studio, we have the option of choosing a number of inbuilt ML models like Classification, Regression, etc. , which we can drag and drop to our workflow.<\/p>\n\n<p>My question is, can I upload a custom ML model that I have built locally on my system in Python, and add it to the workflow?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1565761282107,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1565767277470,
        "Question_score":1.0,
        "Question_view_count":1187.0,
        "Answer_body":"<ol>\n<li>Take the model.pkl file, zip it, and upload it into Azure Machine Learning Studio. Click the \u201cNew\u201d icon in the bottom left:\n<a href=\"https:\/\/i.stack.imgur.com\/Iwvhi.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Iwvhi.jpg\" alt=\"\"><\/a><\/li>\n<li>In the pane that comes up, click on dataset, and then \u201cFrom Local File\u201d:\n<a href=\"https:\/\/i.stack.imgur.com\/DvyjO.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DvyjO.jpg\" alt=\"\"><\/a><\/li>\n<li>Select the zip file where you stored your serialized model and click the tick. You expirement should look like this:\n<a href=\"https:\/\/i.stack.imgur.com\/0efka.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0efka.jpg\" alt=\"\"><\/a><\/li>\n<li>Put the following code to run your classification experiment:<\/li>\n<\/ol>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport sys\nimport pickle\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    sys.path.insert(0,\".\\Script Bundle\")\n    model = pickle.load(open(\".\\Script Bundle\\model.pkl\", 'rb'))\n    pred = model.predict(dataframe1)\n    return pd.DataFrame([pred[0]])\n<\/code><\/pre>\n\n<p><strong>Update<\/strong> \nIf you want to declare this experiment as an API you need to add web input and output to the Python script module.\n<a href=\"https:\/\/i.stack.imgur.com\/eqV8W.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eqV8W.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":7.0,
        "Answer_last_edit_time":1566565696980,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57488706",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1566202471556,
        "Question_original_content":"deploi custom model studio studio option choos number inbuilt model like classif regress drag drop workflow question upload custom model built local python add workflow",
        "Question_preprocessed_content":"deploi custom model studio studio option choos number inbuilt model like classif regress drag drop workflow question upload custom model built local python add workflow",
        "Question_gpt_summary_original":"The user is facing a challenge of deploying a custom machine learning model on Azure ML Studio and wants to know if it is possible to upload a locally built model in Python to the workflow.",
        "Question_gpt_summary":"user face challeng deploi custom machin learn model studio want know possibl upload local built model python workflow",
        "Answer_original_content":"model pkl file zip upload studio click new icon left pane come click dataset local file select zip file store serial model click tick expir look like follow code run classif experi import panda import sy import pickl def main datafram datafram sy path insert script bundl model pickl load open script bundl model pkl pred model predict datafram return datafram pred updat want declar experi api need add web input output python script modul",
        "Answer_preprocessed_content":"file zip upload studio click new icon left pane come click dataset local file select zip file store serial model click tick expir look like follow code run classif experi updat want declar experi api need add web input output python script modul",
        "Answer_gpt_summary_original":"Solution: The user can upload the locally built model in Python to Azure ML Studio by zipping the model.pkl file and uploading it as a dataset in the workflow. Then, the user can run the classification experiment by adding the provided code to the Python script module. If the user wants to declare this experiment as an API, they need to add web input and output to the Python script module.",
        "Answer_gpt_summary":"solut user upload local built model python studio zip model pkl file upload dataset workflow user run classif experi ad provid code python script modul user want declar experi api need add web input output python script modul"
    },
    {
        "Question_title":"Azure Recommendations API's Parameter",
        "Question_body":"<p>I would like to make a recommendation model using Recommendations API on Azure MS Cognitive Services. I can't understand three API's parameters below for \"Create\/Trigger a build.\" What do these parameters mean?<\/p>\n\n<p><a href=\"https:\/\/westus.dev.cognitive.microsoft.com\/docs\/services\/Recommendations.V4.0\/operations\/56f30d77eda5650db055a3d0\" rel=\"nofollow\">https:\/\/westus.dev.cognitive.microsoft.com\/docs\/services\/Recommendations.V4.0\/operations\/56f30d77eda5650db055a3d0<\/a><\/p>\n\n<blockquote>\n  <p>EnableModelingInsights<br> Allows you to compute metrics on the\n  recommendation model. <br> Valid Values: True\/False<\/p>\n  \n  <p>AllowColdItemPlacement<br> Indicates if the recommendation should also\n  push cold items via feature similarity. <br> Valid Values: True\/False<\/p>\n  \n  <p>ReasoningFeatureList<br> Comma-separated list of feature names to be\n  used for reasoning sentences (e.g. recommendation explanations).<br>\n  Valid Values: Feature names, up to 512 chars<\/p>\n<\/blockquote>\n\n<p>Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1459942893017,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":279.0,
        "Answer_body":"<p>That page is missing references to content mentioned at other locations.  See this page for a more complete guide...<\/p>\n\n<p><a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-recommendation-api-documentation\/\" rel=\"nofollow\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-recommendation-api-documentation\/<\/a><\/p>\n\n<p>It describes Cold Items in the Rank Build section in the document as...<\/p>\n\n<p>Features can enhance the recommendation model, but to do so requires the use of meaningful features. For this purpose a new build was introduced - a rank build. This build will rank the usefulness of features. A meaningful feature is a feature with a rank score of 2 and up. After understanding which of the features are meaningful, trigger a recommendation build with the list (or sublist) of meaningful features. It is possible to use these feature for the enhancement of both warm items and cold items. In order to use them for warm items, the UseFeatureInModel build parameter should be set up. In order to use features for cold items, the AllowColdItemPlacement build parameter should be enabled. Note: It is not possible to enable AllowColdItemPlacement without enabling UseFeatureInModel.<\/p>\n\n<p>It also describes the ReasoningFeatureList in the Recommendation Reasoning section as...<\/p>\n\n<p>Recommendation reasoning is another aspect of feature usage. Indeed, the Azure Machine Learning Recommendations engine can use features to provide recommendation explanations (a.k.a. reasoning), leading to more confidence in the recommended item from the recommendation consumer. To enable reasoning, the AllowFeatureCorrelation and ReasoningFeatureList parameters should be setup prior to requesting a recommendation build.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36450108",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1460495281968,
        "Question_original_content":"azur recommend api paramet like recommend model recommend api azur cognit servic understand api paramet creat trigger build paramet mean http westu dev cognit microsoft com doc servic recommend oper fdedadbad enablemodelinginsight allow comput metric recommend model valid valu true fals allowcolditemplac indic recommend push cold item featur similar valid valu true fals reasoningfeaturelist comma separ list featur name reason sentenc recommend explan valid valu featur name char thank",
        "Question_preprocessed_content":"azur recommend api paramet like recommend model recommend api azur cognit servic understand api paramet paramet mean enablemodelinginsight allow comput metric recommend model valid valu allowcolditemplac indic recommend push cold item featur similar valid valu reasoningfeaturelist list featur name reason sentenc valid valu featur name char thank",
        "Question_gpt_summary_original":"The user is facing challenges in understanding three parameters of the Azure Recommendations API, specifically \"EnableModelingInsights,\" \"AllowColdItemPlacement,\" and \"ReasoningFeatureList.\" The user is seeking clarification on the meaning and valid values of these parameters.",
        "Question_gpt_summary":"user face challeng understand paramet azur recommend api specif enablemodelinginsight allowcolditemplac reasoningfeaturelist user seek clarif mean valid valu paramet",
        "Answer_original_content":"page miss refer content mention locat page complet guid http azur microsoft com document articl machin learn recommend api document describ cold item rank build section document featur enhanc recommend model requir us meaning featur purpos new build introduc rank build build rank us featur meaning featur featur rank score understand featur meaning trigger recommend build list sublist meaning featur possibl us featur enhanc warm item cold item order us warm item usefeatureinmodel build paramet set order us featur cold item allowcolditemplac build paramet enabl note possibl enabl allowcolditemplac enabl usefeatureinmodel describ reasoningfeaturelist recommend reason section recommend reason aspect featur usag recommend engin us featur provid recommend explan reason lead confid recommend item recommend consum enabl reason allowfeaturecorrel reasoningfeaturelist paramet setup prior request recommend build",
        "Answer_preprocessed_content":"page miss refer content mention locat page complet describ cold item rank build section document featur enhanc recommend model requir us meaning featur purpos new build introduc rank build build rank us featur meaning featur featur rank score understand featur meaning trigger recommend build list meaning featur possibl us featur enhanc warm item cold item order us warm item usefeatureinmodel build paramet set order us featur cold item allowcolditemplac build paramet enabl note possibl enabl allowcolditemplac enabl usefeatureinmodel describ reasoningfeaturelist recommend reason section recommend reason aspect featur usag recommend engin us featur provid recommend explan lead confid recommend item recommend consum enabl reason allowfeaturecorrel reasoningfeaturelist paramet setup prior request recommend build",
        "Answer_gpt_summary_original":"Solution:\n- The \"AllowColdItemPlacement\" parameter is used to enable the use of features for cold items in the recommendation model. It can only be enabled if the \"UseFeatureInModel\" parameter is also enabled.\n- The \"EnableModelingInsights\" parameter is used to enable the collection of data about the recommendation model's performance and usage.\n- The \"ReasoningFeatureList\" parameter is used to enable the recommendation engine to provide reasoning or explanation for the recommended item. It should be set up along with the \"AllowFeatureCorrelation\" parameter.",
        "Answer_gpt_summary":"solut allowcolditemplac paramet enabl us featur cold item recommend model enabl usefeatureinmodel paramet enabl enablemodelinginsight paramet enabl collect data recommend model perform usag reasoningfeaturelist paramet enabl recommend engin provid reason explan recommend item set allowfeaturecorrel paramet"
    },
    {
        "Question_title":"SageMaker gives CannotStartContainerError although I specified an entrypoint",
        "Question_body":"<p>I want to train a custom ML model with SageMaker. The model is written in Python and should be shipped to SageMaker in a Docker image. Here is a simplified version of my Dockerfile (the model sits in the train.py file):<\/p>\n\n<pre><code>FROM amazonlinux:latest\n\n# Install Python 3\nRUN yum -y update &amp;&amp; yum install -y python3-pip python3-devel gcc &amp;&amp; yum clean all\n\n# Install sagemaker-containers (the official SageMaker utils package)\nRUN pip3 install --target=\/usr\/local\/lib\/python3.7\/site-packages sagemaker-containers &amp;&amp; rm -rf \/root\/.cache\n\n# Bring the script with the model to the image \nCOPY train.py \/opt\/ml\/code\/train.py\n\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n\n<p>Now, if I initialize this image as a SageMaker estimator and then run the <code>fit<\/code> method on this estimator I get the following error:<\/p>\n\n<p>\"AlgorithmError: CannotStartContainerError. Please make sure the container can be run with 'docker run  train'.\"<\/p>\n\n<p>In other words: SageMaker is not able to get into the container and run the train.py file. But why? The way I am specifying the entrypoint with <code>ENV SAGEMAKER_PROGRAM train.py<\/code> is recommended in the <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\/blob\/master\/README.rst\" rel=\"nofollow noreferrer\">docs of the sagemaker-containers package<\/a> (see 'How a script is executed inside the container').<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1585665709967,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1133.0,
        "Answer_body":"<p>I found a hint in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">the AWS docs<\/a> and came up with this solution:<\/p>\n\n<pre><code>ENTRYPOINT [\"python3.7\", \"\/opt\/ml\/code\/train.py\"]\n<\/code><\/pre>\n\n<p>With this the container <a href=\"https:\/\/docs.docker.com\/engine\/reference\/builder\/#entrypoint\" rel=\"nofollow noreferrer\">will run as an executable<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60953289",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1585671760392,
        "Question_original_content":"give cannotstartcontainererror specifi entrypoint want train custom model model written python ship docker imag simplifi version dockerfil model sit train file amazonlinux latest instal python run yum updat yum instal python pip python devel gcc yum clean instal contain offici util packag run pip instal target usr local lib python site packag contain root cach bring script model imag copi train opt code train env program train initi imag estim run fit method estim follow error algorithmerror cannotstartcontainererror sure contain run docker run train word abl contain run train file wai specifi entrypoint env program train recommend doc contain packag script execut insid contain",
        "Question_preprocessed_content":"give cannotstartcontainererror specifi entrypoint want train custom model model written python ship docker imag simplifi version dockerfil initi imag estim run method estim follow error algorithmerror cannotstartcontainererror sure contain run docker run word abl contain run file wai specifi entrypoint recommend doc contain packag",
        "Question_gpt_summary_original":"The user is encountering a challenge while training a custom ML model with SageMaker. The error message \"CannotStartContainerError\" is displayed when the user runs the \"fit\" method on the estimator. The user has specified the entrypoint using \"ENV SAGEMAKER_PROGRAM train.py\" which is recommended in the sagemaker-containers package documentation.",
        "Question_gpt_summary":"user encount challeng train custom model error messag cannotstartcontainererror displai user run fit method estim user specifi entrypoint env program train recommend contain packag document",
        "Answer_original_content":"hint aw doc came solut entrypoint python opt code train contain run execut",
        "Answer_preprocessed_content":"hint aw doc came solut contain run execut",
        "Answer_gpt_summary_original":"Solution: The user found a solution in the AWS documentation and specified the entrypoint using \"ENTRYPOINT [\"python3.7\", \"\/opt\/ml\/code\/train.py\"]\" which will allow the container to run as an executable.",
        "Answer_gpt_summary":"solut user solut aw document specifi entrypoint entrypoint python opt code train allow contain run execut"
    },
    {
        "Question_title":"Workaround for timeout error in Dataset.download()",
        "Question_body":"<p>azureml-sdk version: <code>1.0.85<\/code><\/p>\n\n<p>Calling below (as given in the Dataset UI), I get this<\/p>\n\n<pre><code>ds_split = Dataset.get_by_name(workspace, name='ret- holdout-split')\nds_split.download(target_path=dir_outputs, overwrite=True)\n<\/code><\/pre>\n\n<pre><code>UnexpectedError:\n{'errorCode': 'Microsoft.DataPrep.ErrorCodes.Unknown', 'message':\n    'The client could not finish the operation within specified timeout.',\n    'errorData': {}}\n<\/code><\/pre>\n\n<p>The <code>FileDataset<\/code> 1GB pickled file stored in blob.\n<a href=\"https:\/\/gist.github.com\/swanderz\/c608ced5f2c6a2802b7553bc9ead0762\" rel=\"nofollow noreferrer\">Here's a gist with the full traceback<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1581384771000,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1581436827487,
        "Question_score":0.0,
        "Question_view_count":217.0,
        "Answer_body":"<p>Tried again this AM and it worked. let's file this under \"transient error\"<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60160773",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1581436892352,
        "Question_original_content":"workaround timeout error dataset download sdk version call given dataset split dataset workspac ret holdout split split download target path dir output overwrit true unexpectederror errorcod microsoft dataprep errorcod unknown messag client finish oper specifi timeout errordata filedataset pickl file store blob gist traceback",
        "Question_preprocessed_content":"workaround timeout error sdk version call pickl file store blob gist traceback",
        "Question_gpt_summary_original":"The user is encountering a timeout error while trying to download a 1GB pickled file stored in a blob using the Dataset.get_by_name() function in azureml-sdk version 1.0.85. The error message received is \"The client could not finish the operation within specified timeout.\"",
        "Question_gpt_summary":"user encount timeout error try download pickl file store blob dataset function sdk version error messag receiv client finish oper specifi timeout",
        "Answer_original_content":"tri work let file transient error",
        "Answer_preprocessed_content":"tri work let file transient error",
        "Answer_gpt_summary_original":"No solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut mention discuss"
    },
    {
        "Question_title":"Control tracked version of external dependency",
        "Question_body":"<p>I am trying to set up a DVC repository for machine learning data with different tagged versions of the dataset. I do this with something like:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ cd \/raid\/ml_data  # folder on a data drive\n$ git init\n$ dvc init\n$ [add data]\n$ [commit to dvc, git]\n$ git tag -a 1.0.0\n$ [add or change data]\n$ [commit to dvc, git]\n$ git tag -a 1.1.0\n<\/code><\/pre>\n<p>I have multiple projects that each need to reference some version of this dataset. The problem is I can't figure out how to set up those projects to reference a specific version. I'm able to track the <code>HEAD<\/code> of the repo with something like:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ cd ~\/my_proj  # different drive than the remote\n$ mkdir data\n$ git init\n$ dvc init\n$ dvc remote add -d local \/raid\/ml_data  # add the remote on my data drive\n$ dvc cache dir \/raid\/ml_data\/.dvc\/cache  # tell DVC to use the remote cache\n$ dvc checkout\n$ dvc run --external -d \/raid\/ml_data -o data\/ cp -r \/raid\/ml_data data\n<\/code><\/pre>\n<p>This gets me the latest version of the dataset, symlinked into my <code>data<\/code> folder, but what if I want some projects to use the <code>1.0.0<\/code> version and some to use the <code>1.1.0<\/code> version, or another version? Or for that matter, if I update the dataset to <code>2.0.0<\/code> but don't want my existing projects to necessarily track <code>HEAD<\/code> and instead keep the version with which they were set up?<\/p>\n<p>It's important to me to not create a ton of local copies of my dataset as the <code>\/home<\/code> drive is much smaller than the <code>\/raid<\/code> drive and some of these datasets are huge.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1604349754297,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1604361023336,
        "Question_score":2.0,
        "Question_view_count":139.0,
        "Answer_body":"<p>I think you are looking for the <a href=\"https:\/\/dvc.org\/doc\/start\/data-access\" rel=\"nofollow noreferrer\">data access<\/a> set of commands.<\/p>\n<p>In your particular case, <code>dvc import<\/code> makes sense:<\/p>\n<pre><code>$ dvc import \/raid\/ml_data data\n<\/code><\/pre>\n<p>if you want to get the most recent version (HEAD). Then you will be able to update it with the <code>dvc update<\/code> command (if 2.0.0 is released, for example).<\/p>\n<pre><code>$ dvc import \/raid\/ml_data data --rev 1.0.0\n<\/code><\/pre>\n<p>if you'd like to &quot;fix&quot; it to the specific version.<\/p>\n<h3>Avoiding copies<\/h3>\n<p>Make sure also, that <code>symlinks<\/code> are set for the second project, as described in the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"nofollow noreferrer\">Large Dataset Optimization<\/a>:<\/p>\n<pre><code>$ dvc config cache.type reflink,hardlink,symlink,copy\n<\/code><\/pre>\n<p>(there are config modifiers <code>--global<\/code>, <code>--local<\/code>, <code>--system<\/code> to set this setting for everyone at once, or just for one project, etc)<\/p>\n<p>Check the details instruction <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization#configuring-dvc-cache-file-link-type\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<hr \/>\n<p>Overall, it's a great setup, and looks like you got pretty much everything right. Please, don't hesitate to follow up and\/or create other questions here- we'll help you with this.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1604362563343,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64653042",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1604351561432,
        "Question_original_content":"control track version extern depend try set repositori machin learn data differ tag version dataset like raid data folder data drive git init init add data commit git git tag add chang data commit git git tag multipl project need refer version dataset problem figur set project refer specif version abl track head repo like proj differ drive remot mkdir data git init init remot add local raid data add remot data drive cach dir raid data cach tell us remot cach checkout run extern raid data data raid data data get latest version dataset symlink data folder want project us version us version version matter updat dataset want exist project necessarili track head instead version set import creat ton local copi dataset home drive smaller raid drive dataset huge",
        "Question_preprocessed_content":"control track version extern depend try set repositori machin learn data differ tag version dataset like multipl project need refer version dataset problem figur set project refer specif version abl track repo like get latest version dataset symlink folder want project us version us version version matter updat dataset want exist project necessarili track instead version set import creat ton local copi dataset drive smaller drive dataset huge",
        "Question_gpt_summary_original":"The user is facing challenges in setting up a DVC repository for machine learning data with different tagged versions of the dataset. They are unable to figure out how to set up multiple projects to reference a specific version of the dataset and avoid creating local copies of the dataset due to limited storage space.",
        "Question_gpt_summary":"user face challeng set repositori machin learn data differ tag version dataset unabl figur set multipl project refer specif version dataset avoid creat local copi dataset limit storag space",
        "Answer_original_content":"think look data access set command particular case import make sens import raid data data want recent version head abl updat updat command releas exampl import raid data data rev like fix specif version avoid copi sure symlink set second project describ larg dataset optim config cach type reflink hardlink symlink copi config modifi global local set set project check detail instruct overal great setup look like got pretti right hesit follow creat question help",
        "Answer_preprocessed_content":"think look data access set command particular case make sens want recent version abl updat command like fix specif version avoid copi sure set second project describ larg dataset optim config modifi set set project check detail instruct overal great setup look like got pretti right hesit follow creat question help",
        "Answer_gpt_summary_original":"Solution:\n- Use the \"data access\" set of commands, specifically the \"dvc import\" command to reference a specific version of the dataset.\n- Use the \"dvc update\" command to update the dataset to a newer version.\n- Set symlinks for the second project to avoid creating local copies of the dataset.\n- Use the \"dvc config cache.type\" command to set the cache type for the project. \n\nNo personal opinions or biases were included in the summary.",
        "Answer_gpt_summary":"solut us data access set command specif import command refer specif version dataset us updat command updat dataset newer version set symlink second project avoid creat local copi dataset us config cach type command set cach type project person opinion bias includ summari"
    },
    {
        "Question_title":"a request to change the personal account (MSA) associated with my Certification profile",
        "Question_body":"I want to connect my MSA with my certification profile.\n\nMy MSA is julia.loef@hotmail.com and my work account is julia.loef-bleiksch@nl.abnamro.com\n\nCan you please help me with this?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661434194677,
        "Question_favorite_count":11.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi @JuliaLoefBleiksch-4662\n\nPlease post this on Microsoft Certifications forum found at https:\/\/trainingsupport.microsoft.com\/en-us\/mcp and someone will gladly assist.\n\nUnfortunately MS Certifications is not supported on this forum.\n\nIf this was helpful please accept answer.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/981835\/a-request-to-change-the-personal-account-msa-assoc.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-25T13:50:43.443Z",
                "Answer_score":0,
                "Answer_body":"Hi @JuliaLoefBleiksch-4662\n\nPlease post this on Microsoft Certifications forum found at https:\/\/trainingsupport.microsoft.com\/en-us\/mcp and someone will gladly assist.\n\nUnfortunately MS Certifications is not supported on this forum.\n\nIf this was helpful please accept answer.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1661435443443,
        "Question_original_content":"request chang person account msa associ certif profil want connect msa certif profil msa julia loef hotmail com work account julia loef bleiksch abnamro com help",
        "Question_preprocessed_content":"request chang person account associ certif profil want connect msa certif profil msa work account help",
        "Question_gpt_summary_original":"The user is requesting assistance in changing the personal account associated with their certification profile to their Microsoft account (MSA) from their work account.",
        "Question_gpt_summary":"user request assist chang person account associ certif profil microsoft account msa work account",
        "Answer_original_content":"julialoefbleiksch post microsoft certif forum http trainingsupport microsoft com mcp gladli assist unfortun certif support forum help accept answer",
        "Answer_preprocessed_content":"post microsoft certif forum gladli assist unfortun certif support forum help accept answer",
        "Answer_gpt_summary_original":"Solution: The user was advised to post their request on the Microsoft Certifications forum where someone will assist them with changing their personal account associated with their certification profile to their Microsoft account (MSA) from their work account.",
        "Answer_gpt_summary":"solut user advis post request microsoft certif forum assist chang person account associ certif profil microsoft account msa work account"
    },
    {
        "Question_title":"How to make DVC wait before checking if output file exists?",
        "Question_body":"<p>I\u2019m running a command remotely through ssh, and it takes the remote cluster a while to finish the script. The problem is that DVC fails on this step because it immediately checks if the output file was created. Apart from just removing this file as an output, is there a way to have DVC wait and\/or ignore whether the file was created?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1650539400631,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":129.0,
        "Answer_body":"<p>Hello, <a class=\"mention\" href=\"\/u\/jedphillips\">@JedPhillips<\/a> . I don\u2019t know what script you are using, but I guess that you need some synchronization before finishing it. For example, setting a while loop until the outputs had been generated.<\/p>. <p>Thanks for the reply. That\u2019s what I\u2019ve ended up doing, just adding a while loop in the bash script that runs the cluster commands. It still creates some issues since it can take a while for the files to sync across mounted drives, but better than nothing. If you think of any canonical way to do it using dvc, definitely let me know.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-make-dvc-wait-before-checking-if-output-file-exists\/1170",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-21T12:05:09.812Z",
                "Answer_body":"<p>Hello, <a class=\"mention\" href=\"\/u\/jedphillips\">@JedPhillips<\/a> . I don\u2019t know what script you are using, but I guess that you need some synchronization before finishing it. For example, setting a while loop until the outputs had been generated.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-21T15:34:05.272Z",
                "Answer_body":"<p>Thanks for the reply. That\u2019s what I\u2019ve ended up doing, just adding a while loop in the bash script that runs the cluster commands. It still creates some issues since it can take a while for the files to sync across mounted drives, but better than nothing. If you think of any canonical way to do it using dvc, definitely let me know.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"wait check output file exist run command remot ssh take remot cluster finish script problem fail step immedi check output file creat apart remov file output wai wait ignor file creat",
        "Question_preprocessed_content":"wait check output file exist run command remot ssh take remot cluster finish script problem fail step immedi check output file creat apart remov file output wai wait ignor file creat",
        "Question_gpt_summary_original":"The user is facing a challenge with DVC, as it immediately checks if the output file was created, causing it to fail when running a command remotely through ssh. The user is seeking a solution to make DVC wait or ignore whether the file was created without removing it as an output.",
        "Question_gpt_summary":"user face challeng immedi check output file creat caus fail run command remot ssh user seek solut wait ignor file creat remov output",
        "Answer_original_content":"hello jedphillip dont know script guess need synchron finish exampl set loop output gener thank repli that iv end ad loop bash script run cluster command creat issu file sync mount drive better think canon wai definit let know",
        "Answer_preprocessed_content":"hello dont know script guess need synchron finish exampl set loop output thank repli that iv end ad loop bash script run cluster command creat issu file sync mount drive better think canon wai definit let know",
        "Answer_gpt_summary_original":"Solution: No canonical solution using DVC was mentioned in the discussion. The user added a while loop in the bash script that runs the cluster commands to synchronize the outputs before finishing the script.",
        "Answer_gpt_summary":"solut canon solut mention discuss user ad loop bash script run cluster command synchron output finish script"
    },
    {
        "Question_title":"Organizations using MLflow - Emerton Data",
        "Question_body":"Hi !\n\n\nAt Emerton Data, we are big fans of MLflow and are using it in our project to industrialize AI models and data projects.\n\n\nHappy to be one of the mlflow supporter and would be glad to appear on your website as an organization using MLFlow.\u00a0\n\n\nCheers,\n\n\n\nYannick LEO\nDirector Data Science\n\n16 avenue Hoche\u00a0\u00a0\n75008 Paris\u00a0\nM + 33 6 38 21 33 99\nT + 33 1 53 75 38 75\nyanni...@emerton-data.com\u00a0|\u00a0http:\/\/www.emerton-data.com",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593271817000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":30.0,
        "Answer_body":"Hello Yannick,\n\n\nWe will be happy to include you as supporters and users of mlflow.\u00a0\n\n\nCan you send me your preferred logo? We will include in the list.\n\n\nThanks for being a fan and supporter of MLflow.\n\n\nCheers\nJules\u00a0\n\n\nSent from my iPhone\nPardon the dumb thumb typos :)\n\n\nOn Jun 27, 2020, at 12:32 PM, Yannick Leo <yanni...@emerton-data.com> wrote:\n\n\n\ufeffHi !\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/a52afb80-ce39-4985-8e13-ef601a2d4edfn%40googlegroups.com.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/9WHVX1YBK_A",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-27T17:33:39",
                "Answer_body":"Hello Yannick,\n\n\nWe will be happy to include you as supporters and users of mlflow.\u00a0\n\n\nCan you send me your preferred logo? We will include in the list.\n\n\nThanks for being a fan and supporter of MLflow.\n\n\nCheers\nJules\u00a0\n\n\nSent from my iPhone\nPardon the dumb thumb typos :)\n\n\nOn Jun 27, 2020, at 12:32 PM, Yannick Leo <yanni...@emerton-data.com> wrote:\n\n\n\ufeffHi !\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/a52afb80-ce39-4985-8e13-ef601a2d4edfn%40googlegroups.com."
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"organ emerton data emerton data big fan project industri model data project happi support glad appear websit organ cheer yannick leo director data scienc avenu hoch pari yanni emerton data com http emerton data com",
        "Question_preprocessed_content":"organ emerton data emerton data big fan project industri model data project happi support glad appear websit organ cheer yannick leo director data scienc avenu hoch pari",
        "Question_gpt_summary_original":"The given text does not mention any challenges faced by the user. It is a message from Yannick LEO, Director Data Science at Emerton Data, expressing their support for MLflow and their interest in being listed as an organization using MLflow.",
        "Question_gpt_summary":"given text mention challeng face user messag yannick leo director data scienc emerton data express support list organ",
        "Answer_original_content":"hello yannick happi includ support user send prefer logo includ list thank fan support cheer jule sent iphon pardon dumb thumb typo jun yannick leo wrote receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user aafb efadedfn googlegroup com",
        "Answer_preprocessed_content":"hello yannick happi includ support user send prefer logo includ list thank fan support cheer jule sent iphon pardon dumb thumb typo jun yannick leo wrote receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit",
        "Answer_gpt_summary_original":"There are no solutions mentioned in the given text as it is a message expressing support for MLflow and interest in being listed as an organization using MLflow.",
        "Answer_gpt_summary":"solut mention given text messag express support list organ"
    },
    {
        "Question_title":"interpret statistical model metrics",
        "Question_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/7MiTV.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7MiTV.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Do you know how to intepret RAE and RSE values? I know a COD closer to 1 is a good sign. Does this indicate that boosted decision tree regression is best?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1437855669030,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":209.0,
        "Answer_body":"<p>RAE and RSE closer to 0 is a good sign...you want error to be as low as possible.  See <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-evaluate-model-performance\/\" rel=\"nofollow\">this article<\/a> for more information on evaluating your model.  From that page:<\/p>\n\n<blockquote>\n  <p>The term \"error\" here represents the difference between the predicted value and the true value. The absolute value or the square of this difference are usually computed to capture the total magnitude of error across all instances, as the difference between the predicted and true value could be negative in some cases. The error metrics measure the predictive performance of a regression model in terms of the mean deviation of its predictions from the true values. Lower error values mean the model is more accurate in making predictions. An overall error metric of 0 means that the model fits the data perfectly.<\/p>\n<\/blockquote>\n\n<p>Yes, with your current results, the boosted decision tree performs best.  I don't know the details of your work well enough to determine if that is good enough.  It honestly may be.  But if you determine it's not, you can also tweak the input parameters in your \"Boosted Decision Tree Regression\" module to try to get even better results.  The \"<a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/038d91b6-c2f2-42a1-9215-1f2c20ed1b40\/\" rel=\"nofollow\">ParameterSweep<\/a>\" module can help with that by trying many different input parameters for you and you specify the parameter that you want to optimize for (such as your RAE, RSE, or COD referenced in your question).  See <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-algorithm-parameters-optimize\/\" rel=\"nofollow\">this article<\/a> for a brief description.  Hope this helps.\n<br\/><br\/>\nP.S. I'm glad that you're looking into the black carbon levels in Westeros...I'm sure Cersei doesn't even care.  <\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/31630745",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1438012963580,
        "Question_original_content":"interpret statist model metric know intepret rae rse valu know cod closer good sign indic boost decis tree regress best",
        "Question_preprocessed_content":"interpret statist model metric know intepret rae rse valu know cod closer good sign indic boost decis tree regress best",
        "Question_gpt_summary_original":"The user is seeking help in interpreting RAE and RSE values and is unsure if a COD closer to 1 indicates that boosted decision tree regression is the best option.",
        "Question_gpt_summary":"user seek help interpret rae rse valu unsur cod closer indic boost decis tree regress best option",
        "Answer_original_content":"rae rse closer good sign want error low possibl articl inform evalu model page term error repres differ predict valu true valu absolut valu squar differ usual comput captur total magnitud error instanc differ predict true valu neg case error metric measur predict perform regress model term mean deviat predict true valu lower error valu mean model accur make predict overal error metric mean model fit data perfectli ye current result boost decis tree perform best know detail work determin good honestli determin tweak input paramet boost decis tree regress modul try better result parametersweep modul help try differ input paramet specifi paramet want optim rae rse cod referenc question articl brief descript hope help glad look black carbon level westero sure cersei care",
        "Answer_preprocessed_content":"rae rse closer good want error low possibl articl inform evalu model page term error repres differ predict valu true valu absolut valu squar differ usual comput captur total magnitud error instanc differ predict true valu neg case error metric measur predict perform regress model term mean deviat predict true valu lower error valu mean model accur make predict overal error metric mean model fit data perfectli ye current result boost decis tree perform best know detail work determin good honestli determin tweak input paramet boost decis tree regress modul try better result parametersweep modul help try differ input paramet specifi paramet want optim articl brief descript hope help glad look black carbon level sure cersei care",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- RAE and RSE values closer to 0 indicate lower error and better predictive performance of a regression model.\n- Boosted decision tree regression performs best with the current results.\n- Input parameters in the \"Boosted Decision Tree Regression\" module can be tweaked to try to get even better results.\n- The \"ParameterSweep\" module can help with trying many different input parameters and optimizing for RAE, RSE, or COD.\n- The provided article can offer more information on evaluating the model's performance.\n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss rae rse valu closer indic lower error better predict perform regress model boost decis tree regress perform best current result input paramet boost decis tree regress modul tweak try better result parametersweep modul help try differ input paramet optim rae rse cod provid articl offer inform evalu model perform person opinion bias includ summari"
    },
    {
        "Question_title":"Continue stopped run in MLflow",
        "Question_body":"<p>We run our experiment on AWS spot instances. Sometimes the experiments are stopped, and we would prefer to continue logging to the same run. How can you set the run-id of the active run?<\/p>\n<p>Something like this pseudocode (not working):<\/p>\n<pre><code>if new:\n    mlflow.start_run(experiment_id=1, run_name=x)\nelse:\n    mlflow.set_run(run_id)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618244956103,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":131.0,
        "Answer_body":"<p>You can pass the run_id directly to <code>start_run<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.start_run(experiment_id=1,\n                 run_name=x,\n                 run_id=&lt;run_id_of_interrupted_run&gt; # pass None to start a new run\n                 ) \n<\/code><\/pre>\n<p>Of course, you have to store the run_id for this. You can get it with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.entities.html#mlflow.entities.RunInfo.run_id\" rel=\"nofollow noreferrer\"><code>run.info.run_id<\/code><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67062145",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1631884865500,
        "Question_original_content":"continu stop run run experi aw spot instanc experi stop prefer continu log run set run activ run like pseudocod work new start run experi run set run run",
        "Question_preprocessed_content":"continu stop run run experi aw spot instanc experi stop prefer continu log run set activ run like pseudocod",
        "Question_gpt_summary_original":"The user is facing a challenge of continuing a stopped run in MLflow while logging to the same run. They are seeking a way to set the run-id of the active run to continue logging.",
        "Question_gpt_summary":"user face challeng continu stop run log run seek wai set run activ run continu log",
        "Answer_original_content":"pass run directli start run start run experi run run pass start new run cours store run run info run",
        "Answer_preprocessed_content":"pass directli cours store",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to pass the run_id directly to `start_run` function and store the run_id for this. The run_id can be obtained using `run.info.run_id` function.",
        "Answer_gpt_summary":"solut solut suggest discuss pass run directli start run function store run run obtain run info run function"
    },
    {
        "Question_title":"Is there a way for Optuna `suggest_categorical`to return multiple choices from list?",
        "Question_body":"<p>I am using Optuna for hyperparametrization of my model. And i have a field where I want to test multiple combinations from a list. For example: I have <code>[&quot;lisa&quot;,&quot;adam&quot;,&quot;test&quot;]<\/code> and i want <code>suggest_categorical<\/code> to return not just one, but a random combination: maybe <code>[&quot;lisa&quot;, &quot;adam&quot;]<\/code>, maybe <code>[&quot;adam&quot;]<\/code>, maybe <code>[&quot;lisa&quot;, &quot;adam&quot;, &quot;test&quot;]<\/code>. Is there a way to get this with built in Optuna function?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660737627797,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":56.0,
        "Answer_body":"<p>You could use <code>itertools.combinations<\/code> to generate all possible combinations of list items and then pass them to optuna's <code>suggest_categorical<\/code> as choices:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import optuna\nimport itertools\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# generate the combinations\niterable = ['lisa', 'adam', 'test']\ncombinations = []\nfor r in range(1, len(iterable) + 1):\n    combinations.extend([list(x) for x in itertools.combinations(iterable=iterable, r=r)])\nprint(combinations)\n# [['lisa'], ['adam'], ['test'], ['lisa', 'adam'], ['lisa', 'test'], ['adam', 'test'], ['lisa', 'adam', 'test']]\n\n# sample the combinations\ndef objective(trial):\n    combination = trial.suggest_categorical(name='combination', choices=combinations)\n    return round(random.random(), 2)\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=3)\n# [I 2022-08-18 08:03:51,658] A new study created in memory with name: no-name-3874ce95-2394-4526-bb19-0d9822d7e45c\n# [I 2022-08-18 08:03:51,659] Trial 0 finished with value: 0.94 and parameters: {'combination': ['adam']}. Best is trial 0 with value: 0.94.\n# [I 2022-08-18 08:03:51,660] Trial 1 finished with value: 0.87 and parameters: {'combination': ['lisa', 'test']}. Best is trial 1 with value: 0.87.\n# [I 2022-08-18 08:03:51,660] Trial 2 finished with value: 0.29 and parameters: {'combination': ['lisa', 'adam']}. Best is trial 2 with value: 0.29.\n<\/code><\/pre>\n<p>Using lists as choices in optuna's <code>suggest_categorical<\/code> throws a warning message, but apparently this is mostly inconsequential (see <a href=\"https:\/\/github.com\/optuna\/optuna\/issues\/2341\" rel=\"nofollow noreferrer\">this issue<\/a> in optuna's GitHub repository).<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1660803027380,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73388133",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1660802705056,
        "Question_original_content":"wai suggest categor return multipl choic list hyperparametr model field want test multipl combin list exampl lisa adam test want suggest categor return random combin mayb lisa adam mayb adam mayb lisa adam test wai built function",
        "Question_preprocessed_content":"wai return multipl choic list hyperparametr model field want test multipl combin list exampl want return random combin mayb mayb mayb wai built function",
        "Question_gpt_summary_original":"The user is facing a challenge with using Optuna for hyperparameter optimization of their model. They want to test multiple combinations from a list using the `suggest_categorical` function, but the function only returns one choice at a time. The user is looking for a way to get a random combination of choices from the list using the built-in Optuna function.",
        "Question_gpt_summary":"user face challeng hyperparamet optim model want test multipl combin list suggest categor function function return choic time user look wai random combin choic list built function",
        "Answer_original_content":"us itertool combin gener possibl combin list item pass suggest categor choic import import itertool import random import warn warn filterwarn ignor gener combin iter lisa adam test combin rang len iter combin extend list itertool combin iter iter print combin lisa adam test lisa adam lisa test adam test lisa adam test sampl combin def object trial combin trial suggest categor combin choic combin return round random random studi creat studi studi optim object trial new studi creat memori ddec trial finish valu paramet combin adam best trial valu trial finish valu paramet combin lisa test best trial valu trial finish valu paramet combin lisa adam best trial valu list choic suggest categor throw warn messag appar inconsequenti issu github repositori",
        "Answer_preprocessed_content":"us gener possibl combin list item pass choic list choic throw warn messag appar inconsequenti",
        "Answer_gpt_summary_original":"Solution: One possible solution is to use `itertools.combinations` to generate all possible combinations of list items and then pass them to Optuna's `suggest_categorical` as choices. The combinations can be sampled using the `objective` function and the `create_study` function from Optuna. However, using lists as choices in Optuna's `suggest_categorical` throws a warning message, but this is mostly inconsequential.",
        "Answer_gpt_summary":"solut possibl solut us itertool combin gener possibl combin list item pass suggest categor choic combin sampl object function creat studi function list choic suggest categor throw warn messag inconsequenti"
    },
    {
        "Question_title":"Azure Machine Learning Prediction - Input and Outputs",
        "Question_body":"<p>I am attempting to follow this <a href=\"http:\/\/www.toptal.com\/machine-learning\/predicting-gas-prices-using-azure-machine-learning-studio\" rel=\"nofollow\">tutorial<\/a> however I was attempting to predict MPG for a set of cars rather than oil prices and have the following set up:<\/p>\n\n<ol>\n<li>MPG Sample dataset<\/li>\n<li>Remove missing values, project everything (weight, displacement, cylinders, etc) except model name<\/li>\n<li>Split 75 to train model, 25 to score model<\/li>\n<li>Train model on MPG column with neural network<\/li>\n<li>Score model which is fed by Train Model and Split<\/li>\n<li>Score model is fed to Evaluate model<\/li>\n<\/ol>\n\n<p>This all seems to run fine and without issue, so I create a scoring experiment and then publish it as a web service, however when I attempt to input values it is asking for an MPG input. My understanding is that this would be the predicted value, so it seems somewhat opposite to have to enter this as a value, or am I just understanding a basic tenet of machine learning? <\/p>\n\n<p>In short: Ideally I would like to be able to enter everything but the MPG and get a prediction on what the MPG is for a given set of value.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1432296670560,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1527.0,
        "Answer_body":"<p>You could also add project columns to exclude label as part of scoring experiment and connect web service output port to the output of project columns<\/p>",
        "Answer_comment_count":8.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/30396392",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1432778316968,
        "Question_original_content":"predict input output attempt follow tutori attempt predict mpg set car oil price follow set mpg sampl dataset remov miss valu project weight displac cylind model split train model score model train model mpg column neural network score model fed train model split score model fed evalu model run fine issu creat score experi publish web servic attempt input valu ask mpg input understand predict valu somewhat opposit enter valu understand basic tenet machin learn short ideal like abl enter mpg predict mpg given set valu",
        "Question_preprocessed_content":"predict input output attempt follow tutori attempt predict mpg set car oil price follow set mpg sampl dataset remov miss valu project model split train model score model train model mpg column neural network score model fed train model split score model fed evalu model run fine issu creat score experi publish web servic attempt input valu ask mpg input understand predict valu somewhat opposit enter valu understand basic tenet machin learn short ideal like abl enter mpg predict mpg given set valu",
        "Question_gpt_summary_original":"The user is encountering a challenge in Azure Machine Learning Prediction where they are attempting to predict MPG for a set of cars, but the system is asking for an MPG input. The user is unsure if they are misunderstanding a basic tenet of machine learning and would like to be able to enter everything but the MPG and get a prediction on what the MPG is for a given set of values.",
        "Question_gpt_summary":"user encount challeng predict attempt predict mpg set car ask mpg input user unsur misunderstand basic tenet machin learn like abl enter mpg predict mpg given set valu",
        "Answer_original_content":"add project column exclud label score experi connect web servic output port output project column",
        "Answer_preprocessed_content":"add project column exclud label score experi connect web servic output port output project column",
        "Answer_gpt_summary_original":"Solution: The discussion suggests adding project columns to exclude the label as part of the scoring experiment and connecting the web service output port to the output of project columns. This solution can help the user enter everything but the MPG and get a prediction on what the MPG is for a given set of values.",
        "Answer_gpt_summary":"solut discuss suggest ad project column exclud label score experi connect web servic output port output project column solut help user enter mpg predict mpg given set valu"
    },
    {
        "Question_title":"Does SageMaker Multi-Model Endpoint support SageMaker Model Monitor?",
        "Question_body":"Does SageMaker Multi-Model Endpoint support SageMaker Model Monitor?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1590501108000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":130.0,
        "Answer_body":"Amazon SageMaker Model Monitor currently supports only endpoints that host a single model and does not support monitoring multi-model endpoints. For information on using multi-model endpoints, see Host Multiple Models with Multi-Model Endpoints . https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor.html",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq2z-BEt7TnmZ8vFYs-Hu7g\/does-sage-maker-multi-model-endpoint-support-sage-maker-model-monitor",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-05-26T13:58:28.000Z",
                "Answer_score":0,
                "Answer_body":"Amazon SageMaker Model Monitor currently supports only endpoints that host a single model and does not support monitoring multi-model endpoints. For information on using multi-model endpoints, see Host Multiple Models with Multi-Model Endpoints . https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor.html",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1590501508000,
        "Question_original_content":"multi model endpoint support model monitor multi model endpoint support model monitor",
        "Question_preprocessed_content":"endpoint support model monitor endpoint support model monitor",
        "Question_gpt_summary_original":"The user is seeking information on whether SageMaker Multi-Model Endpoint supports SageMaker Model Monitor. No specific challenges are mentioned in the given text.",
        "Question_gpt_summary":"user seek inform multi model endpoint support model monitor specif challeng mention given text",
        "Answer_original_content":"model monitor current support endpoint host singl model support monitor multi model endpoint inform multi model endpoint host multipl model multi model endpoint http doc aw amazon com latest model monitor html",
        "Answer_preprocessed_content":"model monitor current support endpoint host singl model support monitor endpoint inform endpoint host multipl model endpoint",
        "Answer_gpt_summary_original":"Solution: The discussion mentions that currently, SageMaker Model Monitor does not support monitoring multi-model endpoints. However, the user can refer to the documentation on Host Multiple Models with Multi-Model Endpoints for information on using multi-model endpoints. No other solutions are provided in the given text.",
        "Answer_gpt_summary":"solut discuss mention current model monitor support monitor multi model endpoint user refer document host multipl model multi model endpoint inform multi model endpoint solut provid given text"
    },
    {
        "Question_title":"Failed to parse column picker rules - azure ML designer",
        "Question_body":"<p>I am trying to use the azure ML designer (preview).<\/p>\n\n<p>referencing this - <a href=\"https:\/\/docs.microsoft.com\/en-in\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-in\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score<\/a><\/p>\n\n<p>using my own input sheet which has four columns and some decimal values. nothing fancy and identical to the sample datasets provided. <\/p>\n\n<p>I do this step (from the linked document above)<\/p>\n\n<p><em>Select the Train Model module.\nIn the module details pane to the right of the canvas, select Edit column selector.\nIn the Label column dialog box, expand the drop-down menu and select Column names.\nIn the text box, enter price to specify the value that your model is going to predict.<\/em><\/p>\n\n<p>and I get this (but there are no errors in the actual designer window.<\/p>\n\n<p>\"Failed to parse column picker rules\"<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1582125953377,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1582126556240,
        "Question_score":1.0,
        "Question_view_count":156.0,
        "Answer_body":"<p>Okay, I found an answer myself. Hope that is okay.<\/p>\n\n<p>In my input sheet, the title was something like this \"Interest Rate %\". Looks like azure was trying to say that it does not like special characters it the column names.<\/p>\n\n<p>I edited my original csv file in excel, and removed the % in all the titles. <\/p>\n\n<p>Then, created a new data store. problem solved. <\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60303714",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1582126496627,
        "Question_original_content":"fail pars column picker rule design try us design preview referenc http doc microsoft com azur machin learn tutori design automobil price train score input sheet column decim valu fanci ident sampl dataset provid step link document select train model modul modul detail pane right canva select edit column selector label column dialog box expand drop menu select column name text box enter price specifi valu model go predict error actual design window fail pars column picker rule",
        "Question_preprocessed_content":"fail pars column picker rule design try us design referenc input sheet column decim valu fanci ident sampl dataset provid step select train model modul modul detail pane right canva select edit column selector label column dialog box expand menu select column name text box enter price specifi valu model go predict error actual design window fail pars column picker rule",
        "Question_gpt_summary_original":"The user encountered a challenge while using the Azure ML designer preview. Specifically, when attempting to select the Train Model module and edit the column selector, they received an error message stating \"Failed to parse column picker rules.\" This occurred despite the fact that there were no errors visible in the designer window.",
        "Question_gpt_summary":"user encount challeng design preview specif attempt select train model modul edit column selector receiv error messag state fail pars column picker rule occur despit fact error visibl design window",
        "Answer_original_content":"okai answer hope okai input sheet titl like rate look like azur try like special charact column name edit origin csv file excel remov titl creat new data store problem solv",
        "Answer_preprocessed_content":"okai answer hope okai input sheet titl like rate look like azur try like special charact column name edit origin csv file excel remov titl creat new data store problem solv",
        "Answer_gpt_summary_original":"Solution: The user found a solution to the challenge they encountered while using the Azure ML designer preview. They discovered that the error message they received when attempting to select the Train Model module and edit the column selector was due to special characters in the column names. Specifically, the % symbol in the column title \"Interest Rate %\" caused the error. To resolve the issue, the user edited the original CSV file in Excel and removed the % symbol from all column titles. They then created a new data store, and the problem was solved.",
        "Answer_gpt_summary":"solut user solut challeng encount design preview discov error messag receiv attempt select train model modul edit column selector special charact column name specif symbol column titl rate caus error resolv issu user edit origin csv file excel remov symbol column titl creat new data store problem solv"
    },
    {
        "Question_title":"Define pipelines in multiple files",
        "Question_body":"<p>I have a repo with a bunch of related pipelines (sharing stages, in some cases).  The <code>dvc.yaml<\/code> file is getting very long.  Is there any way to split the pipelines into multiple <code>yaml<\/code> files?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1602014302887,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":520.0,
        "Answer_body":"<p>Yes <a class=\"mention\" href=\"\/u\/nimrand\">@nimrand<\/a>, you can have a dvc.yaml file per project subdirectory. Use <code>dvc repro<\/code> with the <code>--cwd<\/code>, <code>-R<\/code>, and <code>-P<\/code> options to reproduce several pipelines. See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/repro\">https:\/\/dvc.org\/doc\/command-reference\/repro<\/a>.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/define-pipelines-in-multiple-files\/524",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-06T20:44:31.142Z",
                "Answer_body":"<p>Yes <a class=\"mention\" href=\"\/u\/nimrand\">@nimrand<\/a>, you can have a dvc.yaml file per project subdirectory. Use <code>dvc repro<\/code> with the <code>--cwd<\/code>, <code>-R<\/code>, and <code>-P<\/code> options to reproduce several pipelines. See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/repro\">https:\/\/dvc.org\/doc\/command-reference\/repro<\/a>.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"defin pipelin multipl file repo bunch relat pipelin share stage case yaml file get long wai split pipelin multipl yaml file",
        "Question_preprocessed_content":"defin pipelin multipl file repo bunch relat pipelin file get long wai split pipelin multipl file",
        "Question_gpt_summary_original":"The user is facing challenges with managing a repository containing multiple related pipelines, as the dvc.yaml file is becoming too long. They are seeking a solution to split the pipelines into multiple yaml files.",
        "Question_gpt_summary":"user face challeng manag repositori contain multipl relat pipelin yaml file long seek solut split pipelin multipl yaml file",
        "Answer_original_content":"ye nimrand yaml file project subdirectori us repro cwd option reproduc pipelin http org doc command refer repro",
        "Answer_preprocessed_content":"ye yaml file project subdirectori us option reproduc pipelin",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that the user can split the pipelines into multiple yaml files by having a dvc.yaml file per project subdirectory. They can use the \"dvc repro\" command with the \"--cwd\", \"-R\", and \"-P\" options to reproduce several pipelines.",
        "Answer_gpt_summary":"solut discuss suggest user split pipelin multipl yaml file have yaml file project subdirectori us repro command cwd option reproduc pipelin"
    },
    {
        "Question_title":"Outgoing mail for sagemaker labeling job",
        "Question_body":"When having made a labeling job on Ground Truth, an outgoing mail should be sent to team member, but in my case, mail not be sent with no error message.\n\nin case no private team created (the first job creation) : mail can be sent. (set up a team during job creation)\nin case a private team already set up: mail cannot be sent. (select a existing team during job creation)\n\nI think policies of the job role might not be enough, for example, cognito policy. How can I make sure the cause of the error?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662016903588,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":36.0,
        "Answer_body":"To Successfully create a SageMaker Labeling Job you will need the following Permission Policies applied within your account:\n\nThe IAM entity you have used to create the job will need permissions outlined in the \"Permissions Required to Use the Amazon SageMaker Ground Truth Console\" [1]\nYour Labelling Job Role will need SageMakerFullAccess [2]\n\nWith these permissions in place your job should create successfully.\n\nLinks to documentation provided by AWS:\n\n[1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security_iam_id-based-policy-examples.html#console-permissions\n\n[2] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security-iam-awsmanpol.html#security-iam-awsmanpol-AmazonSageMakerFullAccess",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqbxtiU_kSe-GoPcj6g0pzg\/outgoing-mail-for-sagemaker-labeling-job",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-02T14:53:23.761Z",
                "Answer_score":0,
                "Answer_body":"To Successfully create a SageMaker Labeling Job you will need the following Permission Policies applied within your account:\n\nThe IAM entity you have used to create the job will need permissions outlined in the \"Permissions Required to Use the Amazon SageMaker Ground Truth Console\" [1]\nYour Labelling Job Role will need SageMakerFullAccess [2]\n\nWith these permissions in place your job should create successfully.\n\nLinks to documentation provided by AWS:\n\n[1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security_iam_id-based-policy-examples.html#console-permissions\n\n[2] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/security-iam-awsmanpol.html#security-iam-awsmanpol-AmazonSageMakerFullAccess",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1662130403760,
        "Question_original_content":"outgo mail label job have label job ground truth outgo mail sent team member case mail sent error messag case privat team creat job creation mail sent set team job creation case privat team set mail sent select exist team job creation think polici job role exampl cognito polici sure caus error",
        "Question_preprocessed_content":"outgo mail label job have label job ground truth outgo mail sent team member case mail sent error messag case privat team creat mail sent case privat team set mail sent think polici job role exampl cognito polici sure caus error",
        "Question_gpt_summary_original":"The user is facing challenges with outgoing mail for a labeling job on Ground Truth. The user is unable to send an outgoing mail to team members when a private team is already set up, but can send the mail when no private team is created. The user suspects that the job role policies, such as the cognito policy, may not be sufficient and is unsure how to determine the cause of the error.",
        "Question_gpt_summary":"user face challeng outgo mail label job ground truth user unabl send outgo mail team member privat team set send mail privat team creat user suspect job role polici cognito polici suffici unsur determin caus error",
        "Answer_original_content":"successfulli creat label job need follow permiss polici appli account iam entiti creat job need permiss outlin permiss requir us ground truth consol label job role need fullaccess permiss place job creat successfulli link document provid aw http doc aw amazon com latest secur iam base polici exampl html consol permiss http doc aw amazon com latest secur iam awsmanpol html secur iam awsmanpol amazonfullaccess",
        "Answer_preprocessed_content":"successfulli creat label job need follow permiss polici appli account iam entiti creat job need permiss outlin permiss requir us ground truth consol label job role need fullaccess permiss place job creat successfulli link document provid aw",
        "Answer_gpt_summary_original":"Solution: The user needs to ensure that the IAM entity used to create the job has the necessary permissions outlined in the \"Permissions Required to Use the Amazon SageMaker Ground Truth Console\" and that the labeling job role has SageMakerFullAccess. AWS documentation links are provided to help the user understand the required permissions. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut user need ensur iam entiti creat job necessari permiss outlin permiss requir us ground truth consol label job role fullaccess aw document link provid help user understand requir permiss solut mention discuss"
    },
    {
        "Question_title":"Could google cloud platform vertextAI running code on backend without output?",
        "Question_body":"<p>The problem is my local internet connection is unstable, I could run the code through Jupyter to the interactive google cloud platform vertexAI, while it seems that there're always outputs returns back to the Jupyter interface. So when my local internet connection is interrupted, the code running is also interrupted.<\/p>\n<p>Is there any methods that I could let the codes just run on the vertexAI backends? Then outputs could be written in the log file at last.<\/p>\n<p>This could be a very basic question. Thanks.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qQJbo.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qQJbo.jpg\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_time":1643658605453,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1645284643350,
        "Question_score":1.0,
        "Question_view_count":514.0,
        "Answer_body":"<p>To be able to run your notebook on the background, I did the following steps:<\/p>\n<ol>\n<li>Open Jupyter notebook in GCP &gt; Vertex AI &gt; Workbench &gt; Open Jupyterlab<\/li>\n<li>Open a terminal<\/li>\n<li>Use the command below.\n<pre><code>nohup jupyter nbconvert --to notebook --execute test.ipynb &amp;\n<\/code><\/pre>\n<ul>\n<li><code>nohup<\/code> and <code>&amp;<\/code> is added so that the command will run on the background<\/li>\n<li>Output logs for the actual command will be appened to file <strong>nohup.out<\/strong><\/li>\n<li>Use <code>jupyter nbconvert --to notebook --execute test.ipynb<\/code> to execute the notebook specified after <code>--execute<\/code>. <code>--to notebook<\/code> will create a new notebook that contains the executed notebook with its logs.<\/li>\n<li>There other formats other than notebook to convert it. You can read thru more in <a href=\"https:\/\/nbconvert.readthedocs.io\/en\/latest\/usage.html\" rel=\"nofollow noreferrer\">nbconvert documentation<\/a>.<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<p>For testing I made notebook <strong>(test.ipynb)<\/strong> that has a loop that runs for 1.5 hours, that should emulate a long process.<\/p>\n<pre><code>import time\n\nfor x in range(1,1080):\n    print(x)\n    time.sleep(5)\n<\/code><\/pre>\n<p>I ran the command provided above and closed my notebook and anything related to GCP. After 1.5 hours I opened the notebook and terminal says its done.<\/p>\n<p><strong>Terminal upon checking back after 1.5 hours:<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/aVMcb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aVMcb.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Content of <strong>nohup.out<\/strong>:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kcbKi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kcbKi.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It created a new notebook named <strong>&quot;test.nbconvert.ipynb&quot;<\/strong> that contains the code from test.ipynb and its output.<\/p>\n<p>Snippet of test.nbconvert.ipynb as seen below. It completed the loop up to 1080 iterations that took 1.5 hours:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fnPh4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fnPh4.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70932012",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1643782095707,
        "Question_original_content":"googl cloud platform vertextai run code backend output problem local internet connect unstabl run code jupyt interact googl cloud platform vertexai output return jupyt interfac local internet connect interrupt code run interrupt method let code run vertexai backend output written log file basic question thank",
        "Question_preprocessed_content":"googl cloud platform vertextai run code backend output problem local internet connect unstabl run code jupyt interact googl cloud platform vertexai output return jupyt interfac local internet connect interrupt code run interrupt method let code run vertexai backend output written log file basic question thank",
        "Question_gpt_summary_original":"The user is facing challenges with running code on the Google Cloud Platform VertexAI due to an unstable local internet connection. The user is seeking a solution to allow the code to run on the backend without interruptions and have the outputs written in a log file.",
        "Question_gpt_summary":"user face challeng run code googl cloud platform vertexai unstabl local internet connect user seek solut allow code run backend interrupt output written log file",
        "Answer_original_content":"abl run notebook background follow step open jupyt notebook gcp workbench open jupyterlab open termin us command nohup jupyt nbconvert notebook execut test ipynb nohup ad command run background output log actual command appen file nohup us jupyt nbconvert notebook execut test ipynb execut notebook specifi execut notebook creat new notebook contain execut notebook log format notebook convert read nbconvert document test notebook test ipynb loop run hour emul long process import time rang print time sleep ran command provid close notebook relat gcp hour open notebook termin sai termin check hour content nohup creat new notebook name test nbconvert ipynb contain code test ipynb output snippet test nbconvert ipynb seen complet loop iter took hour",
        "Answer_preprocessed_content":"abl run notebook background follow step open jupyt notebook gcp workbench open jupyterlab open termin us command ad command run background output log actual command appen file us execut notebook specifi creat new notebook contain execut notebook log format notebook convert read nbconvert document test notebook loop run hour emul long process ran command provid close notebook relat gcp hour open notebook termin sai termin check hour content creat new notebook name contain code output snippet seen complet loop iter took hour",
        "Answer_gpt_summary_original":"Solution:\n- Open Jupyter notebook in GCP Vertex AI Workbench and open a terminal.\n- Use the command \"nohup jupyter nbconvert --to notebook --execute test.ipynb &\" to run the notebook on the background.\n- The output logs for the command will be appended to the file \"nohup.out\".\n- The executed notebook with its logs will be created in a new notebook named \"test.nbconvert.ipynb\".\n- The user can check the output logs and the new notebook after the process is completed.",
        "Answer_gpt_summary":"solut open jupyt notebook gcp workbench open termin us command nohup jupyt nbconvert notebook execut test ipynb run notebook background output log command append file nohup execut notebook log creat new notebook name test nbconvert ipynb user check output log new notebook process complet"
    },
    {
        "Question_title":"Removal of partial pull's",
        "Question_body":"<p>Dear DVC community,<\/p>\n<p>I am rather new to DVC. I am interested to use DVC in a use case that is maybe related to what is called \u201cData Registrty\u201d, but it is not entirely the same.<\/p>\n<p>I do have a repository that stores in a structured way folders with outputs from expensive computational runs with many files and high storage volume. I added the individual folders as one object, i.e. have one .dvc file per folder.<\/p>\n<p>Example:<\/p>\n<pre><code class=\"lang-auto\">repo\n|- folder_a.dvc\n|- folder_b.dvc\n|- folder_c.dvc\n|- folder_d.dvc\n<\/code><\/pre>\n<p>A user of the repository would first clone it and, thanks to your partial pull feature, only pull those directories from the remote that are necessary for the next steps in the data analysis pipeline. Thereby the huge repository, for which a full pull would not fit on standard storage, can still be properly used.<\/p>\n<p>Example (cont.):<br>\n(result after \u201cdvc pull folder_a.dvc folder_c.dvc\u201d)<\/p>\n<pre><code class=\"lang-auto\">repo\n|- folder_a.dvc\n|- folder_a\n|- folder_b.dvc\n|- folder_c.dvc\n|- folder_c\n|- folder_d.dvc\n<\/code><\/pre>\n<p>Let\u2019s assume now that the user stopped to use one of those folders (say \u201cfolder_a\u201d) after having pushed its updates. Is there an obvious way to remove that partial pull (here \u201cfolder_a\u201d) from the local working copy, without affecting the remote? (This would be done for keeping the local storage requirements on a moderate level.)<\/p>\n<p>Example (cont.):<br>\n(result after applying the searched for operation)<\/p>\n<pre><code class=\"lang-auto\">repo\n|- folder_a.dvc\n|- folder_b.dvc\n|- folder_c.dvc\n|- folder_c\n|- folder_d.dvc\n<\/code><\/pre>\n<p>I do not ask for removing data from the remote but rather a proper way to remove the folder (not it\u2019s .dvc file) and all remaining data in the local cache. I assume here that a simple removal  (\u201crm -fR folder_a\u201d) of the folder would not be enough\u2026<\/p>\n<p>Then, the simplest solution would of course be to delete the local working directory and just clone a new one, where one would start from scratch with a partial pull. However, that might become a bit unhandy over time.<\/p>\n<p>So is there any simple way to do this by a kind of \u201cunpull\u201d operation that I overlooked?<\/p>\n<p>If you need further details, please let me know.<\/p>\n<p>Thanks a lot in advance!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1659111282606,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":128.0,
        "Answer_body":"<p>Hello again,<\/p>\n<p>After way more reading, the above searched operation for the given example seems to be<\/p>\n<pre><code class=\"lang-auto\">rm -fR folder_a\ndvc gc -w\n<\/code><\/pre>\n<p>Would that be correct?<\/p>\n<p>Thanks!<\/p>. <p>Hi <a class=\"mention\" href=\"\/u\/zaspel\">@zaspel<\/a>. Welcome to the community.<\/p>\n<p>There\u2019s no straight way to do this. I can suggest a workaround:<\/p>\n<pre data-code-wrap=\"console\"><code class=\"lang-nohighlight\">dvc remove folder_a.dvc --outs\ndvc gc -w\ngit checkout HEAD -- folder_a.dvc\n<\/code><\/pre>\n<p>This will delete the <code>folder_a.dvc<\/code> and <code>folder_a<\/code> directory temporarily, and then run <code>gc<\/code> to delete the contents in the cache.<br>\nAfter that, we checkout the <code>folder_a.dvc<\/code> back again. <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>This is a simple usecase that we should support, feel free to create a feature request. Thanks.<\/p>. <p>Dear <a class=\"mention\" href=\"\/u\/skshetry\">@skshetry<\/a>,<\/p>\n<p>thank you so much for this!! This will be of great help in our project!<\/p>\n<p>I will immediately proceed to propose this as a feature.<\/p>\n<p>Thanks again!<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/removal-of-partial-pulls\/1277",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-29T21:36:37.355Z",
                "Answer_body":"<p>Hello again,<\/p>\n<p>After way more reading, the above searched operation for the given example seems to be<\/p>\n<pre><code class=\"lang-auto\">rm -fR folder_a\ndvc gc -w\n<\/code><\/pre>\n<p>Would that be correct?<\/p>\n<p>Thanks!<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-30T15:19:42.852Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/zaspel\">@zaspel<\/a>. Welcome to the community.<\/p>\n<p>There\u2019s no straight way to do this. I can suggest a workaround:<\/p>\n<pre data-code-wrap=\"console\"><code class=\"lang-nohighlight\">dvc remove folder_a.dvc --outs\ndvc gc -w\ngit checkout HEAD -- folder_a.dvc\n<\/code><\/pre>\n<p>This will delete the <code>folder_a.dvc<\/code> and <code>folder_a<\/code> directory temporarily, and then run <code>gc<\/code> to delete the contents in the cache.<br>\nAfter that, we checkout the <code>folder_a.dvc<\/code> back again. <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>This is a simple usecase that we should support, feel free to create a feature request. Thanks.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-30T21:18:41.995Z",
                "Answer_body":"<p>Dear <a class=\"mention\" href=\"\/u\/skshetry\">@skshetry<\/a>,<\/p>\n<p>thank you so much for this!! This will be of great help in our project!<\/p>\n<p>I will immediately proceed to propose this as a feature.<\/p>\n<p>Thanks again!<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"remov partial pull dear commun new interest us us case mayb relat call data registrti entir repositori store structur wai folder output expens comput run file high storag volum ad individu folder object file folder exampl repo folder folder folder folder user repositori clone thank partial pull featur pull directori remot necessari step data analysi pipelin huge repositori pull fit standard storag properli exampl cont result pull folder folder repo folder folder folder folder folder folder let assum user stop us folder folder have push updat obviou wai remov partial pull folder local work copi affect remot keep local storag requir moder level exampl cont result appli search oper repo folder folder folder folder folder ask remov data remot proper wai remov folder file remain data local cach assum simpl remov folder folder simplest solut cours delet local work directori clone new start scratch partial pull bit unhandi time simpl wai kind unpul oper overlook need detail let know thank lot advanc",
        "Question_preprocessed_content":"remov partial pull dear commun new interest us us case mayb relat call data registrti entir repositori store structur wai folder output expens comput run file high storag volum ad individu folder object file folder exampl user repositori clone thank partial pull featur pull directori remot necessari step data analysi pipelin huge repositori pull fit standard storag properli exampl result pull let assum user stop us folder have push updat obviou wai remov partial pull local work copi affect remot exampl result appli search oper ask remov data remot proper wai remov folder remain data local cach assum simpl remov folder simplest solut cours delet local work directori clone new start scratch partial pull bit unhandi time simpl wai kind unpul oper overlook need detail let know thank lot advanc",
        "Question_gpt_summary_original":"The user is facing a challenge in removing a partial pull from their local working copy without affecting the remote. They have a repository with folders containing outputs from expensive computational runs, and they added individual folders as one object with one .dvc file per folder. The user wants to remove a folder and all remaining data in the local cache to keep local storage requirements on a moderate level. They are looking for a proper way to remove the folder and all remaining data in the local cache without affecting the remote. The simplest solution would be to delete the local working directory and clone a new one, but that might become unhandy over time.",
        "Question_gpt_summary":"user face challeng remov partial pull local work copi affect remot repositori folder contain output expens comput run ad individu folder object file folder user want remov folder remain data local cach local storag requir moder level look proper wai remov folder remain data local cach affect remot simplest solut delet local work directori clone new unhandi time",
        "Answer_original_content":"hello wai read search oper given exampl folder correct thank zaspel welcom commun there straight wai suggest workaround remov folder out git checkout head folder delet folder folder directori temporarili run delet content cach checkout folder simpl usecas support feel free creat featur request thank dear skshetri thank great help project immedi proce propos featur thank",
        "Answer_preprocessed_content":"hello wai read search oper given exampl correct thank welcom commun there straight wai suggest workaround delet directori temporarili run delet content cach checkout simpl usecas support feel free creat featur request dear thank great help project immedi proce propos featur thank",
        "Answer_gpt_summary_original":"Solution:\n- One solution suggested by a community member is to use the command \"rm -fR folder_a\" to remove the folder and all remaining data in the local cache, followed by \"dvc gc -w\" to delete the contents in the cache.\n- Another solution suggested by another community member is to use the commands \"dvc remove folder_a.dvc --outs\", \"dvc gc -w\", and \"git checkout HEAD -- folder_a.dvc\" to temporarily delete the folder and its corresponding .dvc file, run gc to delete the contents in the cache, and then checkout the .dvc file back again. \n- The user expressed gratitude for the solutions and proposed to create a feature request.",
        "Answer_gpt_summary":"solut solut suggest commun member us command folder remov folder remain data local cach follow delet content cach solut suggest commun member us command remov folder out git checkout head folder temporarili delet folder correspond file run delet content cach checkout file user express gratitud solut propos creat featur request"
    },
    {
        "Question_title":"What is the name of the driver to connect to Azure SQL Database from pyodbc in Azure ML?",
        "Question_body":"<p>I'm trying to create a '<strong>Reader<\/strong>' alternative to read data from Azure SQL Database using the 'Execute python script' module in <strong>Azure ML<\/strong>.\nwhile doing so, I'm trying to connect to Azure Sql using pyodbc library.\nhere's my code:<\/p>\n\n<pre><code>def azureml_main(dataframe1 = None, dataframe2 = None):\n    import pyodbc   \n    import pandas as pd\n\n    conn = pyodbc.connect('DRIVER={SQL Server}; SERVER=server.database.windows.net; DATABASE=db_name; UID=user; PWD=Password')\n    SQLCommand = ('''select * from table1 ''')\n    data_frame = pd.read_sql(SQLCommand, conn)\n    return data_frame,\n<\/code><\/pre>\n\n<p>also tried to use a different driver name: {SQL Server Native Client 11.0}<\/p>\n\n<p>Here is the error i'm getting:<\/p>\n\n<pre><code>Error: ('IM002', '[IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified (0) (SQLDriverConnect)')\n<\/code><\/pre>\n\n<p>Does anybody know which driver should I use?<\/p>\n\n<p>just to make sure, I tried  \"{SQL Server}\", \"{SQL Server Native Client 11.0}\" and \"{SQL Server Native Client 10.0}\" and got the same error<\/p>\n\n<p>I also tried a different format: <\/p>\n\n<pre><code>conn = pyodbc.connect('DRIVER={SQL Server}; SERVER=server.database.windows.net; DATABASE=db_name; user=user@server; password=Password')\n<\/code><\/pre>\n\n<p>and <\/p>\n\n<pre><code>conn = pyodbc.connect('DRIVER={SQL Server Native Client 11.0}; SERVER=server.database.windows.net; DATABASE=db_name; user=user@server; password=Password')\n<\/code><\/pre>",
        "Question_answer_count":5,
        "Question_comment_count":1,
        "Question_creation_time":1456664661510,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1456812850912,
        "Question_score":6.0,
        "Question_view_count":2168.0,
        "Answer_body":"<p>I got an answer from azure support:<\/p>\n\n<blockquote>\n  <p>Currently it is not possible to access sql azure dbs from within  an\n  \u201cexecute python script\u201d module. As you suspected this is due to\n  missing odbc drivers in the execution environment.   Suggested\n  workarounds are to  a) use reader module   or   b) export to blobs\n  and use the Azure Python SDK for accessing those blobs\n  <a href=\"http:\/\/blogs.msdn.com\/b\/bigdatasupport\/archive\/2015\/10\/02\/using-azure-sdk-for-python.aspx\" rel=\"nofollow\">http:\/\/blogs.msdn.com\/b\/bigdatasupport\/archive\/2015\/10\/02\/using-azure-sdk-for-python.aspx<\/a><\/p>\n<\/blockquote>\n\n<p>So currently it it <strong>impossible<\/strong> to connect to SQL server from \u201cexecute python script\u201d module in Azure-ML. If you like to change it, please vote <a href=\"https:\/\/feedback.azure.com\/forums\/257792-machine-learning\/suggestions\/12589266-enable-odbc-connection-from-excute-python-script\" rel=\"nofollow\">here<\/a> <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1457002912110,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35682879",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1456990625532,
        "Question_original_content":"driver connect azur sql databas pyodbc try creat reader altern read data azur sql databas execut python script modul try connect azur sql pyodbc librari code def main datafram datafram import pyodbc import panda conn pyodbc connect driver sql server server server databas window net databas uid user pwd password sqlcommand select tabl data frame read sql sqlcommand conn return data frame tri us differ driver sql server nativ client error get error microsoft odbc driver manag data sourc default driver specifi sqldriverconnect anybodi know driver us sure tri sql server sql server nativ client sql server nativ client got error tri differ format conn pyodbc connect driver sql server server server databas window net databas user user server password password conn pyodbc connect driver sql server nativ client server server databas window net databas user user server password password",
        "Question_preprocessed_content":"driver connect azur sql databas pyodbc try creat reader altern read data azur sql databas execut python script modul try connect azur sql pyodbc librari code tri us differ driver error get anybodi know driver us sure tri sql server sql server nativ client sql server nativ client got error tri differ format",
        "Question_gpt_summary_original":"The user is trying to connect to Azure SQL Database using pyodbc library in Azure ML to create a 'Reader' alternative. However, the user is encountering an error message \"Data source name not found and no default driver specified\" and is unsure which driver to use. The user has tried different driver names and formats but still getting the same error.",
        "Question_gpt_summary":"user try connect azur sql databas pyodbc librari creat reader altern user encount error messag data sourc default driver specifi unsur driver us user tri differ driver name format get error",
        "Answer_original_content":"got answer azur support current possibl access sql azur db execut python script modul suspect miss odbc driver execut environ suggest workaround us reader modul export blob us azur python sdk access blob http blog msdn com bigdatasupport archiv azur sdk python aspx current imposs connect sql server execut python script modul azur like chang vote",
        "Answer_preprocessed_content":"got answer azur support current possibl access sql azur db execut python script modul suspect miss odbc driver execut environ suggest workaround us reader modul export blob us azur python sdk access blob current imposs connect sql server execut python script modul like chang vote",
        "Answer_gpt_summary_original":"Solutions provided: \n- Use the 'Reader' module instead of pyodbc library to access SQL Azure databases.\n- Export data to blobs and use the Azure Python SDK to access those blobs. \n\nNo solution provided for the specific error message encountered by the user.",
        "Answer_gpt_summary":"solut provid us reader modul instead pyodbc librari access sql azur databas export data blob us azur python sdk access blob solut provid specif error messag encount user"
    },
    {
        "Question_title":"How to catalog datasets & models by S3 URI, but keep a local copy?",
        "Question_body":"<p>I'm trying to figure out how to store intermediate Kedro pipeline objects both locally AND on S3. In particular, say I have a dataset on S3:<\/p>\n<pre><code>my_big_dataset.hdf5:\n  type: kedro.extras.datasets.pandas.HDFDataSet\n  filepath: &quot;s3:\/\/my_bucket\/data\/04_feature\/my_big_dataset.hdf5&quot;\n<\/code><\/pre>\n<p>I want to refer to these objects in the catalog by their S3 URI so that my team can use them. HOWEVER, I want to avoid re-downloading the datasets, model weights, etc. every time I run a pipeline by keeping a local copy in addition to the S3 copy. How do I mirror files with Kedro?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597008515007,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1597058318400,
        "Question_score":2.0,
        "Question_view_count":595.0,
        "Answer_body":"<p>This is a good question, Kedro has <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.CachedDataSet.html\" rel=\"nofollow noreferrer\"><code>CachedDataSet<\/code><\/a> for caching datasets within the same run, which handles caching the dataset in memory when it's used\/loaded multiple times in the same run. There isn't really the same thing that persists across runs, in general Kedro doesn't do much persistent stuff.<\/p>\n<p>That said, off the top of my head, I can think of two options that (mostly) replicates or gives this functionality:<\/p>\n<ol>\n<li>Use the same <code>catalog<\/code> in the same config environment but with the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_kedro_project_setup\/02_configuration.html?#templating-configuration\" rel=\"nofollow noreferrer\"><code>TemplatedConfigLoader<\/code><\/a> where your catalog datasets have their filepaths looking something like:<\/li>\n<\/ol>\n<pre><code>my_dataset:\n  filepath: ${base_data}\/01_raw\/blah.csv\n<\/code><\/pre>\n<p>and you set <code>base_data<\/code> to <code>s3:\/\/bucket\/blah<\/code> when running in &quot;production&quot; mode and with <code>local_filepath\/data<\/code> locally. You can decide how exactly you do this in your overriden <code>context<\/code> method (whether it's using <code>local\/globals.yml<\/code> (see the linked documentation above) or environment variables or what not.<\/p>\n<ol start=\"2\">\n<li>Use separate environments, likely <code>local<\/code> (it's kind of what it was made for!) where you keep a separate copy of your catalog where the filepaths are replaced with local ones.<\/li>\n<\/ol>\n<p>Otherwise, your next best bet is to write a <code>PersistentCachedDataSet<\/code> similar to <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.CachedDataSet.html\" rel=\"nofollow noreferrer\"><code>CachedDataSet<\/code><\/a> which intercepts the loading\/saving for the wrapped dataset and makes a local copy when loading for the first time in a deterministic location that you look up on subsequent loads.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63331505",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1597010598252,
        "Question_original_content":"catalog dataset model uri local copi try figur store intermedi pipelin object local particular dataset big dataset hdf type extra dataset panda hdfdataset filepath bucket data featur big dataset hdf want refer object catalog uri team us want avoid download dataset model weight time run pipelin keep local copi addit copi mirror file",
        "Question_preprocessed_content":"catalog dataset model uri local copi try figur store intermedi pipelin object local particular dataset want refer object catalog uri team us want avoid dataset model weight time run pipelin keep local copi addit copi mirror file",
        "Question_gpt_summary_original":"The user is facing a challenge in storing intermediate Kedro pipeline objects both locally and on S3. They want to refer to the objects in the catalog by their S3 URI to enable their team to use them, but also want to avoid re-downloading the datasets and model weights every time they run a pipeline by keeping a local copy in addition to the S3 copy. The user is seeking guidance on how to mirror files with Kedro.",
        "Question_gpt_summary":"user face challeng store intermedi pipelin object local want refer object catalog uri enabl team us want avoid download dataset model weight time run pipelin keep local copi addit copi user seek guidanc mirror file",
        "Answer_original_content":"good question cacheddataset cach dataset run handl cach dataset memori load multipl time run isn thing persist run gener persist stuff said head think option replic give function us catalog config environ templatedconfigload catalog dataset filepath look like dataset filepath base data raw blah csv set base data bucket blah run product mode local filepath data local decid exactli overriden context method local global yml link document environ variabl us separ environ like local kind separ copi catalog filepath replac local on best bet write persistentcacheddataset similar cacheddataset intercept load save wrap dataset make local copi load time determinist locat look subsequ load",
        "Answer_preprocessed_content":"good question cach dataset run handl cach dataset memori multipl time run isn thing persist run gener persist stuff said head think option replic give function us config environ catalog dataset filepath look like set run product mode local decid exactli overriden method environ variabl us separ environ like separ copi catalog filepath replac local on best bet write similar intercept wrap dataset make local copi load time determinist locat look subsequ load",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Use the same catalog in the same config environment but with the TemplatedConfigLoader where the catalog datasets have their filepaths looking something like: `my_dataset: filepath: ${base_data}\/01_raw\/blah.csv` and set `base_data` to `s3:\/\/bucket\/blah` when running in \"production\" mode and with `local_filepath\/data` locally.\n2. Use separate environments, likely `local`, where you keep a separate copy of your catalog where the filepaths are replaced with local ones.\n3. Write a `PersistentCachedDataSet` similar to `CachedDataSet` which intercepts the loading\/saving for the wrapped dataset and makes a local copy when loading for the first",
        "Answer_gpt_summary":"possibl solut mention discuss us catalog config environ templatedconfigload catalog dataset filepath look like dataset filepath base data raw blah csv set base data bucket blah run product mode local filepath data local us separ environ like local separ copi catalog filepath replac local on write persistentcacheddataset similar cacheddataset intercept load save wrap dataset make local copi load"
    },
    {
        "Question_title":"Sagemaker Endpoint returning strange error",
        "Question_body":"<p>Hey guys so recently i started working with sagemaker and I was testing autopilot and it got a fairly good accuracy and I wanted to test it on some more data so I chose the one with best ACC and created an endpoint. The problem now is that I don't know how to use the endpoit properly. I tried using AWS CLI but I keep getting the following errors:<\/p>\n<p>The command:<\/p>\n<pre><code>aws sagemaker-runtime invoke-endpoint --endpoint-name autopilottest --body 'SW0gaGFwcHk=' f\n<\/code><\/pre>\n<p>The error message:<\/p>\n<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from container-1 with message &quot;'application\/json' is an unsupported content type.&quot;. See https:\/\/eu-west-2.console.aws.amazon.com\/cloudwatch\/home?region=eu-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/autopilottest in account 288240193481 for more information.\n<\/code><\/pre>\n<p>The command:<\/p>\n<pre><code>aws sagemaker-runtime invoke-endpoint --endpoint-name autopilottest --body 'Im happy!' f\n<\/code><\/pre>\n<p>The error message:<\/p>\n<pre><code>Invalid base64: &quot;Im happy!&quot;\n<\/code><\/pre>\n<p>Endpoit configuration:\n<a href=\"https:\/\/i.stack.imgur.com\/11qAt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/11qAt.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633715769763,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":521.0,
        "Answer_body":"<p>Ended up fixing the issue by adding <code>--content-type text\/csv<\/code> and using base64 and it worked like a charm.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69499960",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1633716252927,
        "Question_original_content":"endpoint return strang error hei gui recent start work test autopilot got fairli good accuraci want test data chose best acc creat endpoint problem know us endpoit properli tri aw cli get follow error command runtim invok endpoint endpoint autopilottest bodi swgagfwchk error messag error occur modelerror call invokeendpoint oper receiv client error contain messag applic json unsupport content type http west consol aw amazon com cloudwatch home region west logeventview group aw endpoint autopilottest account inform command runtim invok endpoint endpoint autopilottest bodi happi error messag invalid base happi endpoit configur",
        "Question_preprocessed_content":"endpoint return strang error hei gui recent start work test autopilot got fairli good accuraci want test data chose best acc creat endpoint problem know us endpoit properli tri aw cli get follow error command error messag command error messag endpoit configur",
        "Question_gpt_summary_original":"The user is encountering challenges while using an AWS Sagemaker endpoint. They are trying to use AWS CLI to invoke the endpoint but are receiving errors related to unsupported content type and invalid base64. The user has provided the endpoint configuration for reference.",
        "Question_gpt_summary":"user encount challeng endpoint try us aw cli invok endpoint receiv error relat unsupport content type invalid base user provid endpoint configur refer",
        "Answer_original_content":"end fix issu ad content type text csv base work like charm",
        "Answer_preprocessed_content":"end fix issu ad base work like charm",
        "Answer_gpt_summary_original":"Solution: The user was able to fix the issue by adding the parameter \"--content-type text\/csv\" and using base64 encoding. This solution resolved the errors related to unsupported content type and invalid base64 while using AWS CLI to invoke the Sagemaker endpoint.",
        "Answer_gpt_summary":"solut user abl fix issu ad paramet content type text csv base encod solut resolv error relat unsupport content type invalid base aw cli invok endpoint"
    },
    {
        "Question_title":"SageMaker Batch Transform fails with ID Column",
        "Question_body":"<p>I am using SageMaker pipeline to do inference on test data. The Pipeline uses a SKLearn perprocessor and a XGBoost model. The pipeline works fine on data without an ID column. However, when I try to include an ID column to track the predictions, it fails. I have given the code snippets below.<\/p>\n<pre><code>import sagemaker\nfrom sagemaker.predictor import json_serializer, csv_serializer, json_deserializer\n\ninput_data_path = 's3:\/\/batch-transform\/input-data\/validation_data.csv'\noutput_data_path = 's3:\/\/batch-transform\/predictions\/'\n\ntransform_job = sagemaker.transformer.Transformer(\n    model_name = model_name,\n    instance_count = 1,\n    instance_type = 'ml.m4.xlarge',\n    strategy = 'MultiRecord',\n    assemble_with = 'Line',\n    output_path = output_data_path,\n    base_transform_job_name='pipeline_with_id',\n    sagemaker_session=sagemaker.Session(),\n    accept = 'text\/csv')\n\ntransform_job.transform(data = input_data_path,\n                        content_type = 'text\/csv', \n                        split_type = 'Line',\n                        input_filter='$[1:]', \n                        join_source='Input')\n                        output_filter='$[0,-1]')\n<\/code><\/pre>\n<p>This results in the following error:<\/p>\n<pre><code>Fail to join data: mismatched line count between the input and the output\n<\/code><\/pre>\n<p>I am following the example given in this page:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/<\/a><\/p>\n<p>Can someone provide pointers to what is causing the error? Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1605472910920,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1080.0,
        "Answer_body":"<p>Came across the same issue.<\/p>\n<p>Check the number of rows returned after prediction in your serving code. In my case, my prediction output didn't have a column header.<\/p>\n<p>e.g. As a text\/csv response, using batch transform with join will post join the input &amp; output.<\/p>\n<p>A single input record would be [[&quot;feature_1&quot;, &quot;feature_2&quot;],[0, 1]], while my model predicted output returned [1].<\/p>\n<p>add column name to predicted output like this [&quot;result&quot;, 1] then returning the csv result will yield [[&quot;result&quot;],[1]] matching input.<\/p>\n<p>P.S. you may need to find a scalable way of doing this for multi-row  batch. Not sure.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64849557",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1610703448710,
        "Question_original_content":"batch transform fail column pipelin infer test data pipelin us sklearn perprocessor xgboost model pipelin work fine data column try includ column track predict fail given code snippet import predictor import json serial csv serial json deseri input data path batch transform input data valid data csv output data path batch transform predict transform job transform transform model model instanc count instanc type xlarg strategi multirecord assembl line output path output data path base transform job pipelin session session accept text csv transform job transform data input data path content type text csv split type line input filter join sourc input output filter result follow error fail join data mismatch line count input output follow exampl given page http aw amazon com blog machin learn associ predict result input data amazon batch transform provid pointer caus error thank",
        "Question_preprocessed_content":"batch transform fail column pipelin infer test data pipelin us sklearn perprocessor xgboost model pipelin work fine data column try includ column track predict fail given code snippet result follow error follow exampl given page provid pointer caus error thank",
        "Question_gpt_summary_original":"The user is encountering an error when using SageMaker Batch Transform to do inference on test data that includes an ID column to track predictions. The pipeline works fine on data without an ID column, but when an ID column is included, the error \"Fail to join data: mismatched line count between the input and the output\" occurs. The user is seeking help to identify the cause of the error.",
        "Question_gpt_summary":"user encount error batch transform infer test data includ column track predict pipelin work fine data column column includ error fail join data mismatch line count input output occur user seek help identifi caus error",
        "Answer_original_content":"came issu check number row return predict serv code case predict output column header text csv respons batch transform join post join input output singl input record featur featur model predict output return add column predict output like result return csv result yield result match input need scalabl wai multi row batch sure",
        "Answer_preprocessed_content":"came issu check number row return predict serv code case predict output column header respons batch transform join post join input output singl input record model predict output return add column predict output like return csv result yield match input need scalabl wai batch sure",
        "Answer_gpt_summary_original":"Solution: The cause of the error is the mismatched line count between the input and the output. One possible solution is to check the number of rows returned after prediction in the serving code and ensure that the predicted output has a column header. Adding a column name to the predicted output can help in returning the CSV result that matches the input. However, it may be necessary to find a scalable way of doing this for multi-row batch.",
        "Answer_gpt_summary":"solut caus error mismatch line count input output possibl solut check number row return predict serv code ensur predict output column header ad column predict output help return csv result match input necessari scalabl wai multi row batch"
    },
    {
        "Question_title":"how to set path of bucket in amazonsagemaker jupyter notebook?",
        "Question_body":"<p>I'm new to the aws how to set path of my bucket and access file of that bucket?<\/p>\n\n<p>Is there anything i need to change with prefix ?<\/p>\n\n<pre><code>import os\nimport boto3\nimport re\nimport copy\nimport time\nfrom time import gmtime, strftime\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\n\nregion = boto3.Session().region_name\n\nbucket='ltfs1' # Replace with your s3 bucket name\nprefix = 'sagemaker\/ltfs1' # Used as part of the path in the bucket where you store data\n# bucket_path = 'https:\/\/s3-{}.amazonaws.com\/{}'.format(region,bucket) # The URL to access the bucket\n<\/code><\/pre>\n\n<p>I'm using the above code but it's showing file not found error<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1562134154127,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":852.0,
        "Answer_body":"<p>If the file you are accessing is in the root directory of your s3 bucket, you can access the file like this:<\/p>\n\n<pre><code>import pandas as pd\n\nbucket='ltfs1'\ndata_key = 'data.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\ntraining_data = pd.read_csv(data_location)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56863907",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1562166882852,
        "Question_original_content":"set path bucket amazon jupyt notebook new aw set path bucket access file bucket need chang prefix import import boto import import copi import time time import gmtime strftime import execut role role execut role region boto session region bucket ltf replac bucket prefix ltf path bucket store data bucket path http amazonaw com format region bucket url access bucket code show file error",
        "Question_preprocessed_content":"set path bucket amazon jupyt notebook new aw set path bucket access file bucket need chang prefix code show file error",
        "Question_gpt_summary_original":"The user is facing challenges in setting the path of their bucket in Amazon SageMaker Jupyter Notebook and accessing files from that bucket. They have provided a code snippet that they are using, but it is resulting in a \"file not found\" error. The user is seeking guidance on whether they need to change anything with the prefix.",
        "Question_gpt_summary":"user face challeng set path bucket jupyt notebook access file bucket provid code snippet result file error user seek guidanc need chang prefix",
        "Answer_original_content":"file access root directori bucket access file like import panda bucket ltf data kei data csv data locat format bucket data kei train data read csv data locat",
        "Answer_preprocessed_content":"file access root directori bucket access file like",
        "Answer_gpt_summary_original":"Solution: The user can access the file in the root directory of their s3 bucket by using the code snippet provided in the discussion. They need to replace the 'ltfs1' with their own bucket name and 'data.csv' with the name of the file they want to access.",
        "Answer_gpt_summary":"solut user access file root directori bucket code snippet provid discuss need replac ltf bucket data csv file want access"
    },
    {
        "Question_title":"How to connect AMLS to ADLS Gen 2?",
        "Question_body":"<p>I would like to register a dataset from ADLS Gen2 in my Azure Machine Learning workspace (<code>azureml-core==1.12.0<\/code>). Given that service principal information is not required in the Python SDK <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py#register-azure-data-lake-gen2-workspace--datastore-name--filesystem--account-name--tenant-id-none--client-id-none--client-secret-none--resource-url-none--authority-url-none--protocol-none--endpoint-none--overwrite-false-\" rel=\"noreferrer\">documentation<\/a> for <code>.register_azure_data_lake_gen2()<\/code>, I successfully used the following code to register ADLS gen2 as a datastore:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Datastore\n\nadlsgen2_datastore_name = os.environ['adlsgen2_datastore_name']\naccount_name=os.environ['account_name'] # ADLS Gen2 account name\nfile_system=os.environ['filesystem']\n\nadlsgen2_datastore = Datastore.register_azure_data_lake_gen2(\n    workspace=ws,\n    datastore_name=adlsgen2_datastore_name,\n    account_name=account_name, \n    filesystem=file_system\n)\n<\/code><\/pre>\n<p>However, when I try to register a dataset, using<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Dataset\nadls_ds = Datastore.get(ws, datastore_name=adlsgen2_datastore_name)\ndata = Dataset.Tabular.from_delimited_files((adls_ds, 'folder\/data.csv'))\n<\/code><\/pre>\n<p>I get an error<\/p>\n<blockquote>\n<p>Cannot load any data from the specified path. Make sure the path is accessible and contains data.\n<code>ScriptExecutionException<\/code> was caused by <code>StreamAccessException<\/code>.\nStreamAccessException was caused by AuthenticationException.\n<code>'AdlsGen2-ReadHeaders'<\/code> for '[REDACTED]' on storage failed with status code 'Forbidden' (This request is not authorized to perform this operation using this permission.), client request ID &lt;CLIENT_REQUEST_ID&gt;, request ID &lt;REQUEST_ID&gt;. Error message: [REDACTED]\n| session_id=&lt;SESSION_ID&gt;<\/p>\n<\/blockquote>\n<p>Do I need the to enable the service principal to get this to work? Using the ML Studio UI, it appears that the service principal is required even to register the datastore.<\/p>\n<p>Another issue I noticed is that AMLS is trying to access the dataset here:\n<code>https:\/\/adls_gen2_account_name.**dfs**.core.windows.net\/container\/folder\/data.csv<\/code> whereas the actual URI in ADLS Gen2 is: <code>https:\/\/adls_gen2_account_name.**blob**.core.windows.net\/container\/folder\/data.csv<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1600115991930,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1600160631356,
        "Question_score":7.0,
        "Question_view_count":3331.0,
        "Answer_body":"<p>According to this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#azure-data-lake-storage-generation-2\" rel=\"noreferrer\">documentation<\/a>,you need to enable the service principal.<\/p>\n<p>1.you need to register your application and grant the service principal with <strong>Storage Blob Data Reader access<\/strong>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/FZl8O.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FZl8O.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>2.try this code:<\/p>\n<pre><code>adlsgen2_datastore = Datastore.register_azure_data_lake_gen2(workspace=ws,\n                                                             datastore_name=adlsgen2_datastore_name,\n                                                             account_name=account_name,\n                                                             filesystem=file_system,\n                                                             tenant_id=tenant_id,\n                                                             client_id=client_id,\n                                                             client_secret=client_secret\n                                                             )\n\nadls_ds = Datastore.get(ws, datastore_name=adlsgen2_datastore_name)\ndataset = Dataset.Tabular.from_delimited_files((adls_ds,'sample.csv'))\nprint(dataset.to_pandas_dataframe())\n<\/code><\/pre>\n<p><strong>Result:<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/50mit.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/50mit.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1600166834147,
        "Answer_score":9.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63891547",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1600155716360,
        "Question_original_content":"connect aml adl gen like regist dataset adl gen workspac core given servic princip inform requir python sdk document regist azur data lake gen successfulli follow code regist adl gen datastor core import datastor adlsgen datastor environ adlsgen datastor account environ account adl gen account file environ filesystem adlsgen datastor datastor regist azur data lake gen workspac datastor adlsgen datastor account account filesystem file try regist dataset core import dataset adl datastor datastor adlsgen datastor data dataset tabular delimit file adl folder data csv error load data specifi path sure path access contain data scriptexecutionexcept caus streamaccessexcept streamaccessexcept caus authenticationexcept adlsgen readhead redact storag fail statu code forbidden request author perform oper permiss client request request error messag redact session need enabl servic princip work studio appear servic princip requir regist datastor issu notic aml try access dataset http adl gen account df core window net contain folder data csv actual uri adl gen http adl gen account blob core window net contain folder data csv",
        "Question_preprocessed_content":"connect aml adl gen like regist dataset adl gen workspac given servic princip inform requir python sdk document successfulli follow code regist adl gen datastor try regist dataset error load data specifi path sure path access contain data caus streamaccessexcept caus authenticationexcept storag fail statu code forbidden client request request error messag need enabl servic princip work studio appear servic princip requir regist datastor issu notic aml try access dataset actual uri adl gen",
        "Question_gpt_summary_original":"The user is facing challenges while trying to register a dataset from ADLS Gen2 in their Azure Machine Learning workspace. Although they were able to register ADLS Gen2 as a datastore without using service principal information, they encountered an error while trying to register a dataset. The error message suggests that the path is inaccessible and contains no data. The user is unsure if they need to enable the service principal to make it work. Additionally, they noticed that AMLS is trying to access the dataset using the wrong URI.",
        "Question_gpt_summary":"user face challeng try regist dataset adl gen workspac abl regist adl gen datastor servic princip inform encount error try regist dataset error messag suggest path inaccess contain data user unsur need enabl servic princip work addition notic aml try access dataset wrong uri",
        "Answer_original_content":"accord document need enabl servic princip need regist applic grant servic princip storag blob data reader access try code adlsgen datastor datastor regist azur data lake gen workspac datastor adlsgen datastor account account filesystem file tenant tenant client client client secret client secret adl datastor datastor adlsgen datastor dataset dataset tabular delimit file adl sampl csv print dataset panda datafram result",
        "Answer_preprocessed_content":"accord document need enabl servic princip need regist applic grant servic princip storag blob data reader access try code result",
        "Answer_gpt_summary_original":"Solution:\n- Enable the service principal and grant it with Storage Blob Data Reader access.\n- Use the provided code to register the ADLS Gen2 datastore and access the dataset.",
        "Answer_gpt_summary":"solut enabl servic princip grant storag blob data reader access us provid code regist adl gen datastor access dataset"
    },
    {
        "Question_title":"Using DVC in github codespaces",
        "Question_body":"<p>I\u2019m trying to use dvc in a github codespace and gdrive remote storage. However once I try to \u201cdvc push\u201d<br>\nI get this error<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/0ba0f9f145ef2814e08ffd3f4e84be037bd79a34.png\" data-download-href=\"\/uploads\/short-url\/1ES8Pl3nOEFzmQGXs0qEaUQLYRC.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/0ba0f9f145ef2814e08ffd3f4e84be037bd79a34.png\" alt=\"image\" data-base62-sha1=\"1ES8Pl3nOEFzmQGXs0qEaUQLYRC\" width=\"690\" height=\"169\" data-dominant-color=\"282828\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1015\u00d7249 15.3 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1673510308004,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":48.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mouadaqsone\">@mouadAqsone<\/a> ! I haven\u2019t looked in depth, but looks like the authorization method we use for GDrive remotes (<a href=\"https:\/\/github.com\/iterative\/PyDrive2\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - iterative\/PyDrive2: Google Drive API Python wrapper library. Maintained fork of PyDrive.<\/a>) doesn\u2019t work inside a GitHub codespace.<\/p>\n<p>As a workaround, I would suggest that you run a first-time <code>dvc push<\/code> on your local machine. This would  create a credentials files in that local machine (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/setup-google-drive-remote#authorization\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">How to Setup a Google Drive DVC Remote<\/a>) and you could use Codespaces secrets (<a href=\"https:\/\/docs.github.com\/en\/codespaces\/managing-your-codespaces\/managing-encrypted-secrets-for-your-codespaces#about-encrypted-secrets-for-github-codespaces\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Managing encrypted secrets for your codespaces - GitHub Docs<\/a>) to expose the contents of that file as an environment variable (<code>GDRIVE_CREDENTIALS_DATA<\/code>) .<\/p>\n<p>I think that would make <code>dvc push<\/code> work inside the codespace<\/p>. <p>Hello <a class=\"mention\" href=\"\/u\/daavoo\">@daavoo<\/a>, I\u2019ve finally came back to this as it\u2019s important for our company to work on codespaces. I have made the codespaces secrets with the full content of the credentials files (default.json).<\/p>\n<p>But a dvc pull still just creates a new port and tries to open a new window from which I have to agree to access for the gdrive ( and when I do, i get nothing )<\/p>\n<p>this is my error message<\/p>\n<p>\/usr\/local\/python\/3.10.4\/lib\/python3.10\/site-packages\/oauth2client\/_helpers.py:255: UserWarning: Cannot access \/home\/codespace\/.cache\/pydrive2fs\/710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com\/default.json: No such file or directory<br>\nwarnings.warn(_MISSING_FILE_MESSAGE.format(filename))<br>\nYour browser has been opened to visit:<br>\n\u2026<\/p>\n<p>I\u2019m not very familiar with these types of things, but the logical thing that came to my mind is just copy the default.json file into this folder. Is there a better way than that with codespace Secrets ?<\/p>. <p>I know it\u2019s super confusing with GDrive, but there are two different credentials files:<\/p>\n<ol>\n<li>\n<p>The one you downloaded from the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/setup-google-drive-remote#using-a-custom-google-cloud-project-recommended\">Google Cloud console<\/a> (this step is optional though, and I\u2019m not sure if you did it or not).<\/p>\n<\/li>\n<li>\n<p>The one that is generated during the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/setup-google-drive-remote#authorization\">Authorization<\/a> process.<\/p>\n<\/li>\n<\/ol>\n<p>You need to get the second one and put it either as an env variable (should be possible to set an env variable in codespaces secrets?) or copy in some location and set up the <code>gdrive_user_credentials_file<\/code> path. Could you please make sure that you are indeed using the second file? What is the structure of the file (field names, etc) that you are copying?<\/p>\n<hr>\n<p>Btw, I definitely recommend taking a look into Dev Containers, and specifically DVC feature <a href=\"https:\/\/github.com\/iterative\/features\" class=\"inline-onebox\">GitHub - iterative\/features: A collection of development container 'features'<\/a> . It simplifies the Codespaces setup with DVC. You can find a bit more sophisticated example here- <a href=\"https:\/\/github.com\/shcheklein\/hackathon\/blob\/main\/.devcontainer.json\" class=\"inline-onebox\">hackathon\/.devcontainer.json at main \u00b7 shcheklein\/hackathon \u00b7 GitHub<\/a><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/using-dvc-in-github-codespaces\/1461",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-12T16:22:28.776Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mouadaqsone\">@mouadAqsone<\/a> ! I haven\u2019t looked in depth, but looks like the authorization method we use for GDrive remotes (<a href=\"https:\/\/github.com\/iterative\/PyDrive2\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - iterative\/PyDrive2: Google Drive API Python wrapper library. Maintained fork of PyDrive.<\/a>) doesn\u2019t work inside a GitHub codespace.<\/p>\n<p>As a workaround, I would suggest that you run a first-time <code>dvc push<\/code> on your local machine. This would  create a credentials files in that local machine (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/setup-google-drive-remote#authorization\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">How to Setup a Google Drive DVC Remote<\/a>) and you could use Codespaces secrets (<a href=\"https:\/\/docs.github.com\/en\/codespaces\/managing-your-codespaces\/managing-encrypted-secrets-for-your-codespaces#about-encrypted-secrets-for-github-codespaces\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Managing encrypted secrets for your codespaces - GitHub Docs<\/a>) to expose the contents of that file as an environment variable (<code>GDRIVE_CREDENTIALS_DATA<\/code>) .<\/p>\n<p>I think that would make <code>dvc push<\/code> work inside the codespace<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-10T11:08:25.145Z",
                "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/daavoo\">@daavoo<\/a>, I\u2019ve finally came back to this as it\u2019s important for our company to work on codespaces. I have made the codespaces secrets with the full content of the credentials files (default.json).<\/p>\n<p>But a dvc pull still just creates a new port and tries to open a new window from which I have to agree to access for the gdrive ( and when I do, i get nothing )<\/p>\n<p>this is my error message<\/p>\n<p>\/usr\/local\/python\/3.10.4\/lib\/python3.10\/site-packages\/oauth2client\/_helpers.py:255: UserWarning: Cannot access \/home\/codespace\/.cache\/pydrive2fs\/710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com\/default.json: No such file or directory<br>\nwarnings.warn(_MISSING_FILE_MESSAGE.format(filename))<br>\nYour browser has been opened to visit:<br>\n\u2026<\/p>\n<p>I\u2019m not very familiar with these types of things, but the logical thing that came to my mind is just copy the default.json file into this folder. Is there a better way than that with codespace Secrets ?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-11T02:08:41.738Z",
                "Answer_body":"<p>I know it\u2019s super confusing with GDrive, but there are two different credentials files:<\/p>\n<ol>\n<li>\n<p>The one you downloaded from the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/setup-google-drive-remote#using-a-custom-google-cloud-project-recommended\">Google Cloud console<\/a> (this step is optional though, and I\u2019m not sure if you did it or not).<\/p>\n<\/li>\n<li>\n<p>The one that is generated during the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/setup-google-drive-remote#authorization\">Authorization<\/a> process.<\/p>\n<\/li>\n<\/ol>\n<p>You need to get the second one and put it either as an env variable (should be possible to set an env variable in codespaces secrets?) or copy in some location and set up the <code>gdrive_user_credentials_file<\/code> path. Could you please make sure that you are indeed using the second file? What is the structure of the file (field names, etc) that you are copying?<\/p>\n<hr>\n<p>Btw, I definitely recommend taking a look into Dev Containers, and specifically DVC feature <a href=\"https:\/\/github.com\/iterative\/features\" class=\"inline-onebox\">GitHub - iterative\/features: A collection of development container 'features'<\/a> . It simplifies the Codespaces setup with DVC. You can find a bit more sophisticated example here- <a href=\"https:\/\/github.com\/shcheklein\/hackathon\/blob\/main\/.devcontainer.json\" class=\"inline-onebox\">hackathon\/.devcontainer.json at main \u00b7 shcheklein\/hackathon \u00b7 GitHub<\/a><\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"github codespac try us github codespac gdrive remot storag try push error imag",
        "Question_preprocessed_content":"github codespac try us github codespac gdrive remot storag try push error imag",
        "Question_gpt_summary_original":"The user is facing challenges while trying to use DVC in a Github codespace and Gdrive remote storage. They encounter an error when attempting to \"dvc push\".",
        "Question_gpt_summary":"user face challeng try us github codespac gdrive remot storag encount error attempt push",
        "Answer_original_content":"mouadaqson havent look depth look like author method us gdrive remot github iter pydriv googl drive api python wrapper librari maintain fork pydriv doesnt work insid github codespac workaround suggest run time push local machin creat credenti file local machin setup googl drive remot us codespac secret manag encrypt secret codespac github doc expos content file environ variabl gdrive credenti data think push work insid codespac hello daavoo iv final came import compani work codespac codespac secret content credenti file default json pull creat new port tri open new window agre access gdrive error messag usr local python lib python site packag oauthclient helper userwarn access home codespac cach pydrivef iivsgbgsbuvfapdhvueioc app googleusercont com default json file directori warn warn miss file messag format filenam browser open visit familiar type thing logic thing came mind copi default json file folder better wai codespac secret know super confus gdrive differ credenti file download googl cloud consol step option sure gener author process need second env variabl possibl set env variabl codespac secret copi locat set gdrive user credenti file path sure second file structur file field name copi btw definit recommend take look dev contain specif featur github iter featur collect develop contain featur simplifi codespac setup bit sophist exampl hackathon devcontain json main shcheklein hackathon github",
        "Answer_preprocessed_content":"havent look depth look like author method us gdrive remot doesnt work insid github codespac workaround suggest run local machin creat credenti file local machin us codespac secret expos content file environ variabl think work insid codespac hello iv final came import compani work codespac codespac secret content credenti file pull creat new port tri open new window agre access gdrive error messag userwarn access file directori browser open visit familiar type thing logic thing came mind copi file folder better wai codespac secret know super confus gdrive differ credenti file download googl cloud consol gener author process need second env variabl copi locat set path sure second file structur file copi btw definit recommend take look dev contain specif featur github collect develop contain featur simplifi codespac setup bit sophist exampl main github",
        "Answer_gpt_summary_original":"Solution:\n- Run a first-time \"dvc push\" on the local machine to create a credentials file and use Codespaces secrets to expose the contents of that file as an environment variable.\n- Get the second credentials file generated during the Authorization process and put it either as an env variable or copy it in some location and set up the \"gdrive_user_credentials_file\" path.",
        "Answer_gpt_summary":"solut run time push local machin creat credenti file us codespac secret expos content file environ variabl second credenti file gener author process env variabl copi locat set gdrive user credenti file path"
    },
    {
        "Question_title":"\"dvc.api.get_url()\" is not working for --external outputs",
        "Question_body":"<p>I have added dvc remote external output to track (external cache and external data storage),<\/p>\n<ul>\n<li>\n<p>Following is the .dvc generated,<br>\n<strong>path in github:<\/strong> remoteTrack\/wine-quality.csv.dvc<br>\nouts:<\/p>\n<ul>\n<li>etag: 5d6f24258e3c50bb01a61194b5401f5d<br>\nsize: 264426<br>\npath: remote:\/\/s3remote\/wine-quality.csv<\/li>\n<\/ul>\n<\/li>\n<li>\n<p>But same works if not mentioned as external data, following is .csv for non external<br>\n<strong>path in github:<\/strong> wine-quality.csv.dvc<br>\nouts:<\/p>\n<ul>\n<li>md5: 5d6f24258e3c50bb01a61194b5401f5d<br>\nsize: 264426<br>\npath: wine-quality.csv<br>\n.<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/a743fa995cf8a15725ea820f6753ee1cc0d30c95.png\" data-download-href=\"\/uploads\/short-url\/nRHnFdVZK97Uul8v5E9CBTwtFPv.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/a743fa995cf8a15725ea820f6753ee1cc0d30c95.png\" alt=\"image\" data-base62-sha1=\"nRHnFdVZK97Uul8v5E9CBTwtFPv\" width=\"685\" height=\"500\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/a743fa995cf8a15725ea820f6753ee1cc0d30c95_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">900\u00d7656 37.2 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nEven after mentioning path as \u201cremote:\/\/s3remote\/wine-quality.csv\u201d, it is not working.<br>\n<strong>Error<\/strong>: PathMissingError: The path \u2018remoteTrack\/wine-quality.csv\u2019 does not exist in the target repository  neither as a DVC output nor as a Git-tracked file.<\/p>\n<p>What should be the path for remote external data?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1618813629880,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":651.0,
        "Answer_body":"<aside class=\"quote no-group\" data-username=\"veeresh\" data-post=\"1\" data-topic=\"730\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/v\/df705f\/40.png\" class=\"avatar\"> veeresh:<\/div>\n<blockquote>\n<p>What should be the path for remote external data?<\/p>\n<\/blockquote>\n<\/aside>\n<p>Support for external outputs in <code>dvc.api.*<\/code> is highly experimental, and it seems like it doesn\u2019t work for your use case. If this is a single use case, I\u2019d advise just expanding <code>remote:\/\/s3remote\/wine-quality.csv<\/code> to <code>s3:\/\/bucket\/wine-quality.csv<\/code> and use boto etc to read it.<\/p>. <p>okay.<br>\nI will try with \u201cS3:\/\/bucket\u201d instead of remote,<br>\nBut does that solve the issue?<br>\nThe problem am facing is when reading the URL using dvc api for different versions of data.<br>\nThe data is being stored correctly and .dvc files are generated.<br>\nI can manually enter s3 URL and read file from that location (or download using boto3 if I know the URL).<\/p>\n<p>Example URLs for different versions of data from cache,<br>\ns3:\/\/datasource-bucket\/cache\/559\/27fce671701990608798ea11403459<br>\ns3:\/\/datasource-bucket\/cache\/5d\/6f24258e3c50bb01a61194b5401f5d<\/p>. <p>By the way, what is your use case that requires using external outputs? It might be just simpler to use regular outputs and push\/pull. It is an advanced feature with some parts are still in an experimental mode.<\/p>. <p>The use case am exploring is trying to track data in remote location without storing\/downloading it locally(I will not push or pull data locally).<br>\nThere can be other systems, which can change data in remote, which again I want to track from git dvc but without downloading it.<br>\nAnd am trying to load different versions of remote data in jupyter notebook using dvc api.(the notebook is in cloud).<br>\nExcept --external option, other options\/features download the data locally right either in cache or in local system?<\/p>. <aside class=\"quote no-group\" data-username=\"veeresh\" data-post=\"5\" data-topic=\"730\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/v\/df705f\/40.png\" class=\"avatar\"> veeresh:<\/div>\n<blockquote>\n<p>Except --external option, other options\/features download the data locally right either in cache or in local system?<\/p>\n<\/blockquote>\n<\/aside>\n<p>Actually, you can use <code>--to-remote<\/code>, which should sync your data from the remote source to the remote storage and then you can use <code>dvc.api.read()<\/code> etc.  See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-remote-storage\">https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-remote-storage<\/a><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-api-get-url-is-not-working-for-external-outputs\/730",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-19T08:38:28.405Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"veeresh\" data-post=\"1\" data-topic=\"730\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/v\/df705f\/40.png\" class=\"avatar\"> veeresh:<\/div>\n<blockquote>\n<p>What should be the path for remote external data?<\/p>\n<\/blockquote>\n<\/aside>\n<p>Support for external outputs in <code>dvc.api.*<\/code> is highly experimental, and it seems like it doesn\u2019t work for your use case. If this is a single use case, I\u2019d advise just expanding <code>remote:\/\/s3remote\/wine-quality.csv<\/code> to <code>s3:\/\/bucket\/wine-quality.csv<\/code> and use boto etc to read it.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-19T08:59:41.110Z",
                "Answer_body":"<p>okay.<br>\nI will try with \u201cS3:\/\/bucket\u201d instead of remote,<br>\nBut does that solve the issue?<br>\nThe problem am facing is when reading the URL using dvc api for different versions of data.<br>\nThe data is being stored correctly and .dvc files are generated.<br>\nI can manually enter s3 URL and read file from that location (or download using boto3 if I know the URL).<\/p>\n<p>Example URLs for different versions of data from cache,<br>\ns3:\/\/datasource-bucket\/cache\/559\/27fce671701990608798ea11403459<br>\ns3:\/\/datasource-bucket\/cache\/5d\/6f24258e3c50bb01a61194b5401f5d<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-20T07:26:37.499Z",
                "Answer_body":"<p>By the way, what is your use case that requires using external outputs? It might be just simpler to use regular outputs and push\/pull. It is an advanced feature with some parts are still in an experimental mode.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-20T09:05:53.400Z",
                "Answer_body":"<p>The use case am exploring is trying to track data in remote location without storing\/downloading it locally(I will not push or pull data locally).<br>\nThere can be other systems, which can change data in remote, which again I want to track from git dvc but without downloading it.<br>\nAnd am trying to load different versions of remote data in jupyter notebook using dvc api.(the notebook is in cloud).<br>\nExcept --external option, other options\/features download the data locally right either in cache or in local system?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-20T09:19:28.697Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"veeresh\" data-post=\"5\" data-topic=\"730\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/v\/df705f\/40.png\" class=\"avatar\"> veeresh:<\/div>\n<blockquote>\n<p>Except --external option, other options\/features download the data locally right either in cache or in local system?<\/p>\n<\/blockquote>\n<\/aside>\n<p>Actually, you can use <code>--to-remote<\/code>, which should sync your data from the remote source to the remote storage and then you can use <code>dvc.api.read()<\/code> etc.  See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-remote-storage\">https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-remote-storage<\/a><\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"api url work extern output ad remot extern output track extern cach extern data storag follow gener path github remotetrack wine qualiti csv out etag dfecbbabfd size path remot sremot wine qualiti csv work mention extern data follow csv non extern path github wine qualiti csv out dfecbbabfd size path wine qualiti csv imag mention path remot sremot wine qualiti csv work error pathmissingerror path remotetrack wine qualiti csv exist target repositori output git track file path remot extern data",
        "Question_preprocessed_content":"work output ad remot extern output track follow gener path github out etag size path work mention extern data follow csv non extern path github out size path imag mention path work error pathmissingerror path exist target repositori output file path remot extern data",
        "Question_gpt_summary_original":"The user is facing challenges with the dvc.api.get_url() function while trying to track external cache and data storage. The user has mentioned the path as \"remote:\/\/s3remote\/wine-quality.csv\" but is still encountering an error stating that the path does not exist in the target repository. The user is seeking guidance on the correct path for remote external data.",
        "Question_gpt_summary":"user face challeng api url function try track extern cach data storag user mention path remot sremot wine qualiti csv encount error state path exist target repositori user seek guidanc correct path remot extern data",
        "Answer_original_content":"veeresh path remot extern data support extern output api highli experiment like doesnt work us case singl us case advis expand remot sremot wine qualiti csv bucket wine qualiti csv us boto read okai try bucket instead remot solv issu problem face read url api differ version data data store correctli file gener manual enter url read file locat download boto know url exampl url differ version data cach datasourc bucket cach fceea datasourc bucket cach fecbbabfd wai us case requir extern output simpler us regular output push pull advanc featur part experiment mode us case explor try track data remot locat store download local push pull data local system chang data remot want track git download try load differ version remot data jupyt notebook api notebook cloud extern option option featur download data local right cach local veeresh extern option option featur download data local right cach local actual us remot sync data remot sourc remot storag us api read http org doc command refer add exampl transfer remot storag",
        "Answer_preprocessed_content":"veeresh path remot extern data support extern output highli experiment like doesnt work us case singl us case advis expand us boto read okai try instead remot solv issu problem face read url api differ version data data store correctli file gener manual enter url read file locat exampl url differ version data cach wai us case requir extern output simpler us regular output advanc featur part experiment us case explor try track data remot locat local system chang data remot want track git download try load differ version remot data jupyt notebook notebook cloud option download data local right cach local veeresh option download data local right cach local actual us sync data remot sourc remot storag us",
        "Answer_gpt_summary_original":"Solution:\n- The user can try expanding the path to \"s3:\/\/bucket\/wine-quality.csv\" and use boto to read it.\n- The use of external outputs in dvc.api is experimental and may not work for this use case.\n- The user can use \"--to-remote\" option to sync data from the remote source to the remote storage and then use dvc.api.read().",
        "Answer_gpt_summary":"solut user try expand path bucket wine qualiti csv us boto read us extern output api experiment work us case user us remot option sync data remot sourc remot storag us api read"
    },
    {
        "Question_title":"MLflow Error while deploying the Model to local REST server",
        "Question_body":"<blockquote>\n  <p><strong>System Details:<\/strong><\/p>\n  \n  <p>Operating System: Ubuntu 19.04<\/p>\n  \n  <p>Anaconda version: 2019.03<\/p>\n  \n  <p>Python version: 3.7.3<\/p>\n  \n  <p>mlflow version: 1.0.0<\/p>\n<\/blockquote>\n\n<p><strong>Steps to Reproduce:<\/strong> <a href=\"https:\/\/mlflow.org\/docs\/latest\/tutorial.html\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tutorial.html<\/a><\/p>\n\n<p><strong>Error at line\/command:<\/strong> <code>mlflow models serve -m [path_to_model] -p 1234<\/code><\/p>\n\n<p><strong>Error:<\/strong>\nCommand 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1>&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1<\/p>\n\n<p><strong>Terminal Log:<\/strong><\/p>\n\n<pre><code>(mlflow) root@user:\/home\/user\/mlflow\/mlflow\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/e3dd02d5d84545ffab858db13ede7366\/artifacts\/model# mlflow models serve -m $(pwd) -p 1234\n2019\/06\/18 16:15:16 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n2019\/06\/18 16:15:17 INFO mlflow.pyfunc.backend: === Running command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1&gt;&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app'\nbash: activate: No such file or directory\nTraceback (most recent call last):\n  File \"\/root\/anaconda3\/envs\/mlflow\/bin\/mlflow\", line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/models\/cli.py\", line 43, in serve\n    host=host)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/backend.py\", line 76, in serve\n    command_env=command_env)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/backend.py\", line 147, in _execute_in_conda_env\n    command, rc\nException: Command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1&gt;&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1\n(mlflow) root@user:\/home\/user\/mlflow\/mlflow\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/e3dd02d5d84545ffab858db13ede7366\/artifacts\/model# \n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1560855399150,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1840.0,
        "Answer_body":"<p>Following the steps mentioned in the GitHub Issue <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1507\" rel=\"nofollow noreferrer\">1507<\/a> (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1507\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/1507<\/a>) I was able to resolve this issue.<\/p>\n\n<p>In reference to this post, the \"<strong>anaconda\/bin\/<\/strong>\" directory is never added to the list of environment variables i.e. PATH variable. In order to resolve this issue, add the \"<strong>else<\/strong>\" part of conda initialize code block from ~\/.bashrc file to your PATH variable.<\/p>\n\n<pre><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$('\/home\/atulk\/anaconda3\/bin\/conda' 'shell.bash' 'hook' 2&gt; \/dev\/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"\/home\/atulk\/anaconda3\/etc\/profile.d\/conda.sh\" ]; then\n        . \"\/home\/atulk\/anaconda3\/etc\/profile.d\/conda.sh\"\n    else\n        export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n<\/code><\/pre>\n\n<p>In this case, I added <strong>export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"<\/strong> to the PATH variable. However, this is just a temporary fix until the issue is fixed in the project.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56647549",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1561730574528,
        "Question_original_content":"error deploi model local rest server detail oper ubuntu anaconda version python version version step reproduc http org doc latest tutori html error line command model serv path model error command sourc activ cceeebbfbdcebbdb gunicorn timeout pyfunc score server wsgi app return non zero return code return code termin log root user home user exampl sklearn elasticnet wine mlrun eddddffabdbed artifact model model serv pwd info model cli select backend flavor python function info pyfunc backend run command sourc activ cceeebbfbdcebbdb gunicorn timeout pyfunc score server wsgi app bash activ file directori traceback recent file root anaconda env bin line sy exit cli file root anaconda env lib python site packag click core line return self main arg kwarg file root anaconda env lib python site packag click core line main self invok ctx file root anaconda env lib python site packag click core line invok return process result sub ctx command invok sub ctx file root anaconda env lib python site packag click core line invok return process result sub ctx command invok sub ctx file root anaconda env lib python site packag click core line invok return ctx invok self callback ctx param file root anaconda env lib python site packag click core line invok return callback arg kwarg file root anaconda env lib python site packag model cli line serv host host file root anaconda env lib python site packag pyfunc backend line serv command env command env file root anaconda env lib python site packag pyfunc backend line execut conda env command except command sourc activ cceeebbfbdcebbdb gunicorn timeout pyfunc score server wsgi app return non zero return code return code root user home user exampl sklearn elasticnet wine mlrun eddddffabdbed artifact model",
        "Question_preprocessed_content":"error deploi model local rest server detail oper ubuntu anaconda version python version version step reproduc error error command sourc activ bfb dce gunicorn return non zero return code return code termin log",
        "Question_gpt_summary_original":"The user encountered an error while deploying a model to a local REST server using MLflow. The error occurred at the command \"mlflow models serve -m [path_to_model] -p 1234\" and returned a non-zero return code. The terminal log shows that the error was caused by a missing file or directory for the command \"source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1>&2 && gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app\".",
        "Question_gpt_summary":"user encount error deploi model local rest server error occur command model serv path model return non zero return code termin log show error caus miss file directori command sourc activ cceeebbfbdcebbdb gunicorn timeout pyfunc score server wsgi app",
        "Answer_original_content":"follow step mention github issu http github com issu abl resolv issu refer post anaconda bin directori ad list environ variabl path variabl order resolv issu add conda initi code block bashrc file path variabl conda initi content block manag conda init conda setup home atulk anaconda bin conda shell bash hook dev null eval conda setup home atulk anaconda profil conda home atulk anaconda profil conda export path home atulk anaconda bin path unset conda setup conda initi case ad export path home atulk anaconda bin path path variabl temporari fix issu fix project",
        "Answer_preprocessed_content":"follow step mention github issu abl resolv issu refer post directori ad list environ variabl path variabl order resolv issu add conda initi code block file path variabl case ad export path variabl temporari fix issu fix project",
        "Answer_gpt_summary_original":"Solution: The user was able to resolve the issue by adding the \"else\" part of conda initialize code block from ~\/.bashrc file to the PATH variable. The solution involved adding \"export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"\" to the PATH variable. However, this is just a temporary fix until the issue is fixed in the project.",
        "Answer_gpt_summary":"solut user abl resolv issu ad conda initi code block bashrc file path variabl solut involv ad export path home atulk anaconda bin path path variabl temporari fix issu fix project"
    },
    {
        "Question_title":"Can I use SageMaker Autopilot for my time-series modeling?",
        "Question_body":"I'm working on time-series modeling. I'm comparing battle-of-algorithms against the autopilot machine learning approach to identify the model that best fits my use case. I understand that Amazon SageMaker Autopilot doesn't work with time series. Is there an alternative library or algorithm in the AWS ecosystem that implements battle-of-algorithms for time series?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1601671292000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":166.0,
        "Answer_body":"Amazon SageMaker Autopilot currently supports regression, binary classification, and multi-class classification. SageMaker supports only tabular data formatted in files with comma-separated values. For more information, see Automate model development with Amazon SageMaker Autopilot.\n\nFor your use case, you can use Amazon Forecast. Amazon Forecast includes the AutoML feature that trains different models with your target time series, related time series, and item metadata. Amazon Forecast then uses the model with the best accuracy metrics.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxaXaoPpqRyGdkh6aKK9uew\/can-i-use-sage-maker-autopilot-for-my-time-series-modeling",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-05T07:31:28.000Z",
                "Answer_score":0,
                "Answer_body":"Amazon SageMaker Autopilot currently supports regression, binary classification, and multi-class classification. SageMaker supports only tabular data formatted in files with comma-separated values. For more information, see Automate model development with Amazon SageMaker Autopilot.\n\nFor your use case, you can use Amazon Forecast. Amazon Forecast includes the AutoML feature that trains different models with your target time series, related time series, and item metadata. Amazon Forecast then uses the model with the best accuracy metrics.",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1601883088000,
        "Question_original_content":"us autopilot time seri model work time seri model compar battl algorithm autopilot machin learn approach identifi model best fit us case understand autopilot work time seri altern librari algorithm aw ecosystem implement battl algorithm time seri",
        "Question_preprocessed_content":"us autopilot model work model compar autopilot machin learn approach identifi model best fit us case understand autopilot work time seri altern librari algorithm aw ecosystem implement time seri",
        "Question_gpt_summary_original":"The user is facing a challenge in finding an alternative library or algorithm in the AWS ecosystem that implements battle-of-algorithms for time series, as Amazon SageMaker Autopilot does not work with time series.",
        "Question_gpt_summary":"user face challeng find altern librari algorithm aw ecosystem implement battl algorithm time seri autopilot work time seri",
        "Answer_original_content":"autopilot current support regress binari classif multi class classif support tabular data format file comma separ valu inform autom model develop autopilot us case us amazon forecast amazon forecast includ automl featur train differ model target time seri relat time seri item metadata amazon forecast us model best accuraci metric",
        "Answer_preprocessed_content":"autopilot current support regress binari classif classif support tabular data format file valu inform autom model develop autopilot us case us amazon forecast amazon forecast includ automl featur train differ model target time seri relat time seri item metadata amazon forecast us model best accuraci metric",
        "Answer_gpt_summary_original":"Solution: The discussion suggests using Amazon Forecast as an alternative library or algorithm in the AWS ecosystem that implements battle-of-algorithms for time series. Amazon Forecast includes the AutoML feature that trains different models with the target time series, related time series, and item metadata, and then uses the model with the best accuracy metrics.",
        "Answer_gpt_summary":"solut discuss suggest amazon forecast altern librari algorithm aw ecosystem implement battl algorithm time seri amazon forecast includ automl featur train differ model target time seri relat time seri item metadata us model best accuraci metric"
    },
    {
        "Question_title":"'Enter Data' as list instead of list of lists in Azure ML Web Service",
        "Question_body":"<p>In Azure ML, I want to enter data to a model through a published Web Service. \nThe way to tell this to the Web Service, as far as I can tell, it to have an 'Enter Data' box coming into the same input as the Web service. <\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/m1x5x.png\" alt=\"enter image description here\"><\/p>\n\n<p>You can then set you data format in the 'Enter Data' properties:<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/VQJ9V.png\" alt=\"enter image description here\"><\/p>\n\n<p>I want that list to be an arbitrary-length array of samples. This works if your input is:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"samples\"\n      ],\n      \"Values\": [\n        [\n          1\n        ],\n        [\n          2\n        ],\n        [\n          3\n        ],\n        [\n          4\n        ],\n        [\n          5\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>This is ok, but ideally it would be easier, and (more importantly) more network-efficient, if I could send them as:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"samples\"\n      ],\n      \"Values\": [\n        [\n          1,2,3,4,5\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>Is there a correct syntax to implement this? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1437737046243,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":174.0,
        "Answer_body":"<p>I have worked internally to request a confirmation of your concern - <\/p>\n\n<blockquote>\n  <p>'Enter Data' as list instead of list of lists in Azure ML Web Service<\/p>\n<\/blockquote>\n\n<p>but you expected feature is not available today in Azure ML Studio (The reason behind is Azure ML has to be able to read the input data as a tabular format, rows and columns). Such being the case, I would like to suggest you to submit a new feature request via below option:<\/p>\n\n<p>On Azure ML Studio -> the upper right corner, there is a smiley face, please click that and send the feedback.<\/p>\n\n<p>Should you have any further concerns, please feel free to let me know.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/31609319",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1438871325836,
        "Question_original_content":"enter data list instead list list web servic want enter data model publish web servic wai tell web servic far tell enter data box come input web servic set data format enter data properti want list arbitrari length arrai sampl work input input input columnnam sampl valu globalparamet ideal easier importantli network effici send input input columnnam sampl valu globalparamet correct syntax implement",
        "Question_preprocessed_content":"enter data list instead list list web servic want enter data model publish web servic wai tell web servic far tell enter data box come input web servic set data format enter data properti want list arrai sampl work input ideal easier send correct syntax implement",
        "Question_gpt_summary_original":"The user is facing a challenge in entering data to a model through a published Web Service in Azure ML. They want to send an arbitrary-length array of samples as a list instead of a list of lists, which would be more network-efficient. The user is seeking guidance on the correct syntax to implement this.",
        "Question_gpt_summary":"user face challeng enter data model publish web servic want send arbitrari length arrai sampl list instead list list network effici user seek guidanc correct syntax implement",
        "Answer_original_content":"work intern request confirm concern enter data list instead list list web servic expect featur avail todai studio reason abl read input data tabular format row column case like suggest submit new featur request option studio upper right corner smilei face click send feedback concern feel free let know",
        "Answer_preprocessed_content":"work intern request confirm concern enter data list instead list list web servic expect featur avail todai studio case like suggest submit new featur request option studio upper right corner smilei face click send feedback concern feel free let know",
        "Answer_gpt_summary_original":"Solution: No solution is provided in the discussion. The user is advised to submit a new feature request via Azure ML Studio's feedback option.",
        "Answer_gpt_summary":"solut solut provid discuss user advis submit new featur request studio feedback option"
    },
    {
        "Question_title":"Failed to call Java API: MlflowClient.listArtifacts in docker",
        "Question_body":"HI,\n\n\nmy Dockerfile as follows:\n\n\nFROM python:3.6\n\nENV MLFLOW_VERSION 0.8.2\n\nWORKDIR \/\nRUN apt-get update \\\n   && apt-get install -y ca-certificates wget openjdk-8-jdk\n\nRUN pip install mlflow==${MLFLOW_VERSION}\n\nRUN wget -O dd-java-agent.jar \\\n         'https:\/\/search.maven.org\/classic\/remote_content?g=com.datadoghq&a=dd-java-agent&v=LATEST'\n\n\nCOPY target\/xxxxxx.jar app.jar\nCOPY pipeline\/ci\/entrypoint.sh .\nCOPY pipeline\/ci\/sysctl.conf \/etc\/sysctl.d\/00-alpine.conf\n\nRUN chmod +x entrypoint.sh\n\nEXPOSE 8080\nENTRYPOINT [\"\/entrypoint.sh\"]\n\n\nmlflow is deployed in localhost outer docker\nartifact storage is s3 implemented by minio in localhost\n\n\nAfter started docker container, MlflowClient.listArtifacts is called by my own rest API, but it failed\n\n\n\n[http-nio-8080-exec-9] - [ERROR] - [com.xxx.dsd.aop.ApiExceptionHandler.internalErrorHandler(line:26)] - org.mlflow.tracking.MlflowClientException: Failed to exec 'python -m mlflow.cli', needed to access artifacts within the non-Java-native artifact store at 's3:\/\/mlflow\/artifacts\/1\/a34faddc96a544ce8e41f6049189a351\/artifacts'. Please make sure mlflow is available on your local system path (e.g., from 'pip install mlflow')\n\nat org.mlflow.artifacts.CliBasedArtifactRepository.checkMlflowAccessible(CliBasedArtifactRepository.java:181)\n\nat org.mlflow.artifacts.CliBasedArtifactRepository.listArtifacts(CliBasedArtifactRepository.java:127)\n\nat org.mlflow.artifacts.CliBasedArtifactRepository.listArtifacts(CliBasedArtifactRepository.java:137)\n\nat org.mlflow.tracking.MlflowClient.listArtifacts(MlflowClient.java:438)\n\nat com.rakuten.dsd.service.ModelService.getArtifacts(ModelService.java:97)\n\nat com.rakuten.dsd.controller.ModelController.getArtifacts(ModelController.java:117)\n\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\nat java.lang.reflect.Method.invoke(Method.java:498)\n\nat org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:209)\n\nat org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:136)\n\nat org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:102)\n\nat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:870)\n\nat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:776)\n\nat org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)\n\nat org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:991)\n\nat org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:925)\n\nat org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:978)\n\nat org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:870)\n\nat javax.servlet.http.HttpServlet.service(HttpServlet.java:635)\n\nat org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:855)\n\nat javax.servlet.http.HttpServlet.service(HttpServlet.java:742)\n\nat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231)\n\nat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)\n\nat org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)\n\nat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)\n\nat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)\n\nat org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.filterAndRecordMetrics(WebMvcMetricsFilter.java:158)\n\nat org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.filterAndRecordMetrics(WebMvcMetricsFilter.java:126)\n\nat org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:111)\n\nat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)\n\nat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)\n\nat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)\n\nat org.springframework.boot.actuate.web.trace.servlet.HttpTraceFilter.doFilterInternal(HttpTraceFilter.java:84)\n\nat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)\n\nat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)\n\nat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)\n\nat org.springframework.web.filter.CorsFilter.doFilterInternal(CorsFilter.java:96)\n\nat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)\n\nat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)\n\nat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)\n\nat org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:200)\n\nat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)\n\nat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)\n\nat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)\n\nat org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:199)\n\nat org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96)\n\nat org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:496)\n\nat org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:140)\n\nat org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:81)\n\nat org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:87)\n\nat org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)\n\nat org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:803)\n\nat org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66)\n\nat org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:790)\n\nat org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1459)\n\nat org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49)\n\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\nat org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)\n\nat java.lang.Thread.run(Thread.java:748)\n\nCaused by: org.mlflow.tracking.MlflowClientException: Failed to get mlflow version. Error: Traceback (most recent call last):\n\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/runpy.py\", line 193, in _run_module_as_main\n\n\u00a0 \u00a0 \"__main__\", mod_spec)\n\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/runpy.py\", line 85, in _run_code\n\n\u00a0 \u00a0 exec(code, run_globals)\n\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/site-packages\/mlflow\/cli.py\", line 223, in <module>\n\n\u00a0 \u00a0 cli()\n\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/site-packages\/click\/core.py\", line 764, in __call__\n\n\u00a0 \u00a0 return self.main(*args, **kwargs)\n\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/site-packages\/click\/core.py\", line 696, in main\n\n\u00a0 \u00a0 _verify_python3_env()\n\n\u00a0 File \"\/usr\/local\/lib\/python3.6\/site-packages\/click\/_unicodefun.py\", line 124, in _verify_python3_env\n\n\u00a0 \u00a0 ' mitigation steps.' + extra\n\nRuntimeError: Click will abort further execution because Python 3 was configured to use ASCII as encoding for the environment. Consult https:\/\/click.palletsprojects.com\/en\/7.x\/python3\/ for mitigation steps.\n\n\n\n\nThis system supports the C.UTF-8 locale which is recommended.\n\nYou might be able to resolve your issue by exporting the\n\nfollowing environment variables:\n\n\n\n\n\u00a0 \u00a0 export LC_ALL=C.UTF-8\n\n\u00a0 \u00a0 export LANG=C.UTF-8\n\n\n\n\nat org.mlflow.artifacts.CliBasedArtifactRepository.forkMlflowProcess(CliBasedArtifactRepository.java:209)\n\nat org.mlflow.artifacts.CliBasedArtifactRepository.checkMlflowAccessible(CliBasedArtifactRepository.java:173)\n\n... 61 more\n\n\n\nBut, if I use 'exec' command to enter into the container, and use python API of mlflow, everything looks OK.\n\n\nSo, I'm not suer\u00a0 if there's anything wrong in my docker image or any other things wrong ?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1560123243000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":11.0,
        "Answer_body":"Have you tried exporting the suggested variables? Looks like the python mlflow library doesn't want to run because of that.\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/eb7d29ff-f103-467d-bee6-40370812e7b7%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.. Thanks for replying, I've tried that, but didn't work.\n\n\nI 've tried to enter into the running container\ndocker exec -it mlflow-container-name sh\n\n\nthen use python API, like\u00a0\nmlflow.tracking.MlflowClient.list_experiment\n\nand this works.\n\n\nand I also tried \"python -m mlflow.cli --help\" just like the error message showed, and it works too.\n\n\nso I'm confused.\n\ue5d3\n\ue5d3\n\ue5d3\n\ue5d3\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow...@googlegroups.com.\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/SrdkbCtKmgs",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-06-10T00:39:16",
                "Answer_body":"Have you tried exporting the suggested variables? Looks like the python mlflow library doesn't want to run because of that.\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/eb7d29ff-f103-467d-bee6-40370812e7b7%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2019-06-10T01:43:46",
                "Answer_body":"Thanks for replying, I've tried that, but didn't work.\n\n\nI 've tried to enter into the running container\ndocker exec -it mlflow-container-name sh\n\n\nthen use python API, like\u00a0\nmlflow.tracking.MlflowClient.list_experiment\n\nand this works.\n\n\nand I also tried \"python -m mlflow.cli --help\" just like the error message showed, and it works too.\n\n\nso I'm confused.\n\ue5d3\n\ue5d3\n\ue5d3\n\ue5d3\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow...@googlegroups.com.\n\ue5d3"
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"fail java api client listartifact docker dockerfil follow python env version workdir run apt updat apt instal certif wget openjdk jdk run pip instal version run wget java agent jar http search maven org classic remot content com datadoghq java agent latest copi target jar app jar copi pipelin entrypoint copi pipelin sysctl conf sysctl alpin conf run chmod entrypoint expos entrypoint entrypoint deploi localhost outer docker artifact storag implement minio localhost start docker contain client listartifact call rest api fail http nio exec error com dsd aop apiexceptionhandl internalerrorhandl line org track clientexcept fail exec python cli need access artifact non java nativ artifact store artifact afaddcaceefa artifact sure avail local path pip instal org artifact clibasedartifactrepositori checkaccess clibasedartifactrepositori java org artifact clibasedartifactrepositori listartifact clibasedartifactrepositori java org artifact clibasedartifactrepositori listartifact clibasedartifactrepositori java org track client listartifact client java com rakuten dsd servic modelservic getartifact modelservic java com rakuten dsd control modelcontrol getartifact modelcontrol java sun reflect nativemethodaccessorimpl invok nativ method sun reflect nativemethodaccessorimpl invok nativemethodaccessorimpl java sun reflect delegatingmethodaccessorimpl invok delegatingmethodaccessorimpl java java lang reflect method invok method java org springframework web method support invocablehandlermethod doinvok invocablehandlermethod java org springframework web method support invocablehandlermethod invokeforrequest invocablehandlermethod java org springframework web servlet mvc method annot servletinvocablehandlermethod invokeandhandl servletinvocablehandlermethod java org springframework web servlet mvc method annot requestmappinghandleradapt invokehandlermethod requestmappinghandleradapt java org springframework web servlet mvc method annot requestmappinghandleradapt handleintern requestmappinghandleradapt java org springframework web servlet mvc method abstracthandlermethodadapt handl abstracthandlermethodadapt java org springframework web servlet dispatcherservlet dodispatch dispatcherservlet java org springframework web servlet dispatcherservlet doservic dispatcherservlet java org springframework web servlet frameworkservlet processrequest frameworkservlet java org springframework web servlet frameworkservlet doget frameworkservlet java javax servlet http httpservlet servic httpservlet java org springframework web servlet frameworkservlet servic frameworkservlet java javax servlet http httpservlet servic httpservlet java org apach catalina core applicationfilterchain internaldofilt applicationfilterchain java org apach catalina core applicationfilterchain dofilt applicationfilterchain java org apach tomcat websocket server wsfilter dofilt wsfilter java org apach catalina core applicationfilterchain internaldofilt applicationfilterchain java org apach catalina core applicationfilterchain dofilt applicationfilterchain java org springframework boot actuat metric web servlet webmvcmetricsfilt filterandrecordmetr webmvcmetricsfilt java org springframework boot actuat metric web servlet webmvcmetricsfilt filterandrecordmetr webmvcmetricsfilt java org springframework boot actuat metric web servlet webmvcmetricsfilt dofilterintern webmvcmetricsfilt java org springframework web filter onceperrequestfilt dofilt onceperrequestfilt java org apach catalina core applicationfilterchain internaldofilt applicationfilterchain java org apach catalina core applicationfilterchain dofilt applicationfilterchain java org springframework boot actuat web trace servlet httptracefilt dofilterintern httptracefilt java org springframework web filter onceperrequestfilt dofilt onceperrequestfilt java org apach catalina core applicationfilterchain internaldofilt applicationfilterchain java org apach catalina core applicationfilterchain dofilt applicationfilterchain java org springframework web filter corsfilt dofilterintern corsfilt java org springframework web filter onceperrequestfilt dofilt onceperrequestfilt java org apach catalina core applicationfilterchain internaldofilt applicationfilterchain java org apach catalina core applicationfilterchain dofilt applicationfilterchain java org springframework web filter characterencodingfilt dofilterintern characterencodingfilt java org springframework web filter onceperrequestfilt dofilt onceperrequestfilt java org apach catalina core applicationfilterchain internaldofilt applicationfilterchain java org apach catalina core applicationfilterchain dofilt applicationfilterchain java org apach catalina core standardwrappervalv invok standardwrappervalv java org apach catalina core standardcontextvalv invok standardcontextvalv java org apach catalina authent authenticatorbas invok authenticatorbas java org apach catalina core standardhostvalv invok standardhostvalv java org apach catalina valv errorreportvalv invok errorreportvalv java org apach catalina core standardenginevalv invok standardenginevalv java org apach catalina connector coyoteadapt servic coyoteadapt java org apach coyot http httpprocessor servic httpprocessor java org apach coyot abstractprocessorlight process abstractprocessorlight java org apach coyot abstractprotocol connectionhandl process abstractprotocol java org apach tomcat util net nioendpoint socketprocessor dorun nioendpoint java org apach tomcat util net socketprocessorbas run socketprocessorbas java java util concurr threadpoolexecutor runwork threadpoolexecutor java java util concurr threadpoolexecutor worker run threadpoolexecutor java org apach tomcat util thread taskthread wrappingrunn run taskthread java java lang thread run thread java caus org track clientexcept fail version error traceback recent file usr local lib python runpi line run modul main main mod spec file usr local lib python runpi line run code exec code run global file usr local lib python site packag cli line cli file usr local lib python site packag click core line return self main arg kwarg file usr local lib python site packag click core line main verifi python env file usr local lib python site packag click unicodefun line verifi python env mitig step extra runtimeerror click abort execut python configur us ascii encod environ consult http click palletsproject com python mitig step support utf local recommend abl resolv issu export follow environ variabl export utf export lang utf org artifact clibasedartifactrepositori forkprocess clibasedartifactrepositori java org artifact clibasedartifactrepositori checkaccess clibasedartifactrepositori java us exec command enter contain us python api look suer wrong docker imag thing wrong",
        "Question_preprocessed_content":"fail java api docker dockerfil follow env workdir run updat instal wget run pip instal run wget copi copi copi run chmod expos entrypoint deploi localhost outer docker artifact storag implement minio localhost start docker contain call rest api fail fail exec python cli need access artifact artifact store sure avail local path method caus fail version error traceback file line file line exec file line cli file line return kwarg file line main file line mitig extra runtimeerror click abort execut python configur us ascii encod environ consult mitig step support local recommend abl resolv issu export follow environ variabl export export us exec command enter contain us python api look suer wrong docker imag thing wrong",
        "Question_gpt_summary_original":"The user encountered a challenge when calling the Java API MlflowClient.listArtifacts in Docker. The error message suggests that the mlflow is not available on the local system path and that there may be an issue with the encoding for the environment. However, when the user enters the container and uses the Python API of mlflow, everything works fine. The user is unsure if there is anything wrong with the Docker image or if there are other issues.",
        "Question_gpt_summary":"user encount challeng call java api client listartifact docker error messag suggest avail local path issu encod environ user enter contain us python api work fine user unsur wrong docker imag issu",
        "Answer_original_content":"tri export suggest variabl look like python librari want run receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user ebdff bee googlegroup com option visit http group googl com optout thank repli tri work tri enter run contain docker exec contain us python api like track client list experi work tri python cli help like error messag show work confus unsubscrib group stop receiv email send email googlegroup com",
        "Answer_preprocessed_content":"tri export suggest variabl look like python librari want run receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit thank repli tri work tri enter run contain docker exec us python api like work tri python cli like error messag show work confus unsubscrib group stop receiv email send email",
        "Answer_gpt_summary_original":"Solutions provided: \n- Export the suggested variables to resolve the issue with the Python mlflow library not running.\n- Enter the running container and use the Python API of mlflow, which works fine.\n- Use \"python -m mlflow.cli --help\" just like the error message showed, which also works fine.",
        "Answer_gpt_summary":"solut provid export suggest variabl resolv issu python librari run enter run contain us python api work fine us python cli help like error messag show work fine"
    },
    {
        "Question_title":"Sagemaker XGBoost Hyperparameter Tuning Error",
        "Question_body":"<p>I am new to Sagemaker and trying to set up a hyperparameter tuning job for xgboost algorithm in Sagemaker. I have very imbalanced data (98% majority class, 2% minority\u00a0class) and would like to use the\u00a0&quot;scale_pos_weight&quot; parameter but the below error happens.<\/p>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: The hyperparameter tuning job that you requested has the following untunable hyperparameters: [scale_pos_weight]. For the algorithm, ---------------.us-east-1.amazonaws.com\/xgboost:1, you can tune only [colsample_bytree, lambda, eta, max_depth, alpha, num_round, colsample_bylevel, subsample, min_child_weight, max_delta_step, gamma]. Delete untunable hyperparameters.\u00a0\u00a0\n<\/code><\/pre>\n<p>I have upgraded the sagemaker package, restarted my kernel (I am using juptyer notebook), and instance but the problem still exists.<\/p>\n<p>Does anyone have any ideas why this error happens and how I can fix it? I appreciate the help.<\/p>\n<p>\u00a0\nHere is my code that I followed from an example in AWS.\u00a0<\/p>\n<pre><code>sess = sagemaker.Session()\ncontainer = get_image_uri(region, 'xgboost', '1.0-1')\n\nxgb = sagemaker.estimator.Estimator(container,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 role, \n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 train_instance_count=1, \n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 train_instance_type='ml.m4.4xlarge',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output_path='s3:\/\/{}\/{}\/output'.format(bucket, prefix),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sagemaker_session=sess)\n\nxgb.set_hyperparameters(eval_metric='auc',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 objective='binary:logistic',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 num_round=100,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rate_drop=0.3,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 tweedie_variance_power=1.4)\n\nhyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'min_child_weight': ContinuousParameter(1, 10),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'scale_pos_weight' : ContinuousParameter(700, 800),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'alpha': ContinuousParameter(0, 2),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'max_depth': IntegerParameter(1, 10),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'colsample_bytree' : ContinuousParameter(0.1, 0.9)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\nobjective_metric_name = 'validation:auc'\n\ntuner = HyperparameterTuner(xgb,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 objective_metric_name,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 hyperparameter_ranges,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 max_jobs=10,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 max_parallel_jobs=2)\n\ns3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train'.format(bucket, prefix), content_type='csv')\ns3_input_validation = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/validation\/'.format(bucket, prefix), content_type='csv')\n\ntuner.fit({'train': s3_input_train, 'validation': s3_input_validation}, include_cls_metadata=False)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603939946643,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":827.0,
        "Answer_body":"<p>Based on the Sagemaker developer documentation, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html<\/a>, the hyperparameter <code>scale_pos_weight<\/code> is NOT tunable. The only parameters that you can tune are given in the link.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64584295",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1603941358963,
        "Question_original_content":"xgboost hyperparamet tune error new try set hyperparamet tune job xgboost algorithm imbalanc data major class minorityclass like us scale po weight paramet error happen clienterror error occur validationexcept call createhyperparametertuningjob oper hyperparamet tune job request follow untun hyperparamet scale po weight algorithm east amazonaw com xgboost tune colsampl bytre lambda eta max depth alpha num round colsampl bylevel subsampl min child weight max delta step gamma delet untun hyperparamet upgrad packag restart kernel juptyer notebook instanc problem exist idea error happen fix appreci help code follow exampl aw sess session contain imag uri region xgboost xgb estim estim contain role train instanc count train instanc type xlarg output path output format bucket prefix session sess xgb set hyperparamet eval metric auc object binari logist num round rate drop tweedi varianc power hyperparamet rang eta continuousparamet min child weight continuousparamet scale po weight continuousparamet alpha continuousparamet max depth integerparamet colsampl bytre continuousparamet object metric valid auc tuner hyperparametertun xgb object metric hyperparamet rang max job max parallel job input train input data train format bucket prefix content type csv input valid input data valid format bucket prefix content type csv tuner fit train input train valid input valid includ cl metadata fals",
        "Question_preprocessed_content":"xgboost hyperparamet tune error new try set hyperparamet tune job xgboost algorithm imbalanc data like us paramet error happen upgrad packag restart kernel instanc problem exist idea error happen fix appreci help code follow exampl aw",
        "Question_gpt_summary_original":"The user is encountering an error while setting up a hyperparameter tuning job for the xgboost algorithm in Sagemaker. The error is related to the \"scale_pos_weight\" parameter, which cannot be tuned for the algorithm. The user has tried upgrading the Sagemaker package and restarting the kernel and instance, but the problem persists. The user is seeking help to understand why the error is happening and how to fix it.",
        "Question_gpt_summary":"user encount error set hyperparamet tune job xgboost algorithm error relat scale po weight paramet tune algorithm user tri upgrad packag restart kernel instanc problem persist user seek help understand error happen fix",
        "Answer_original_content":"base develop document http doc aw amazon com latest xgboost tune html hyperparamet scale po weight tunabl paramet tune given link",
        "Answer_preprocessed_content":"base develop document hyperparamet tunabl paramet tune given link",
        "Answer_gpt_summary_original":"Solution: The hyperparameter \"scale_pos_weight\" is not tunable for the xgboost algorithm in Sagemaker, according to the Sagemaker developer documentation. Therefore, there is no solution to fix the error related to this parameter.",
        "Answer_gpt_summary":"solut hyperparamet scale po weight tunabl xgboost algorithm accord develop document solut fix error relat paramet"
    },
    {
        "Question_title":"Where to perform the saving of an nodeoutput in Kedro?",
        "Question_body":"<p>In Kedro, we can pipeline different nodes and partially run some nodes. When we are partially running some nodes, we need to save some inputs from the nodes somewhere so that when another node is run it can access the data that the previous node has generated. However, in which file do we write the code for this - pipeline.py, run.py or nodes.py?<\/p>\n\n<p>For instance, I am trying to save a dir path directly to the DataCatalog under a variable name 'model_path'. <\/p>\n\n<p>Snippet from pipeline.py:<\/p>\n\n<pre><code>    # A mapping from a pipeline name to a ``Pipeline`` object.\ndef create_pipelines(**kwargs) -&gt; Dict[str, Pipeline]:\nio = DataCatalog(dict(\n    model_path=MemoryDataSet()\n))\n\nio.save('model_path', \"data\/06_models\/model_test\")\nprint('****', io.exists('model_path'))\n\npipeline = Pipeline([\n    node(\n        split_files,\n        [\"data_csv\", \"parameters\"],\n        [\"train_filenames\", \"val_filenames\", \"train_labels\", \"val_labels\"],\n        name=\"splitting filenames\"\n    ),\n    # node(\n    #     create_and_train,\n    #     [\"train_filenames\", \"val_filenames\", \"train_labels\", \"val_labels\", \"parameters\"],\n    #     \"model_path\",\n    #     name=\"Create Dataset, Train and Save Model\"\n    # ),\n    node(\n        validate_model,\n        [\"val_filenames\", \"val_labels\", \"model_path\"],\n        None,\n        name=\"Validate Model\",\n    )\n\n]).decorate(decorators.log_time, decorators.mem_profile)\n\nreturn {\n    \"__default__\": pipeline\n}\n<\/code><\/pre>\n\n<p>However, I get the following error when I Kedro run:<\/p>\n\n<pre><code>ValueError: Pipeline input(s) {'model_path'} not found in the DataCatalog\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1571371429583,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1571671894436,
        "Question_score":4.0,
        "Question_view_count":1606.0,
        "Answer_body":"<p>Node inputs are automatically loaded by Kedro from the <code>DataCatalog<\/code> before being passed to the node function. Node outputs are consequently saved to the DataCatalog after the node successfully produces some data. DataCatalog configuration by default is taken from <code>conf\/base\/catalog.yml<\/code>. <\/p>\n\n<p>In your example <code>model_path<\/code> is produced by <code>Create Dataset, Train and Save Model<\/code> node and then consumed by <code>Validate Model<\/code>. If required dataset definition is not found in the <code>conf\/base\/catalog.yml<\/code>, Kedro will try to store this dataset in memory using <code>MemoryDataSet<\/code>. This will work if you run the pipeline that contains both <code>Create Dataset...<\/code> and <code>Validate Model<\/code> nodes (given no other issues arise). However, when you are trying to run <code>Validate Model<\/code> node alone, Kedro attempts to read <code>model_path<\/code> dataset from memory, which doesn't exist there.<\/p>\n\n<p>So, <strong>TLDR<\/strong>:<\/p>\n\n<p>To mitigate this, you need to:<\/p>\n\n<p>a) persist <code>model_path<\/code> by adding something like the following to your <code>conf\/base\/catalog.yml<\/code>:<\/p>\n\n<pre><code>model_path:\n  type: TextLocalDataSet\n  filepath: data\/02_intermediate\/model_path.txt\n<\/code><\/pre>\n\n<p>b) run <code>Create Dataset, Train and Save Model<\/code> node (and its dependencies) at least once<\/p>\n\n<p>After completing a) and b) you should be able to start running <code>Validate Model<\/code> separately.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1571672436532,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58443788",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1571671635460,
        "Question_original_content":"perform save nodeoutput pipelin differ node partial run node partial run node need save input node node run access data previou node gener file write code pipelin run node instanc try save dir path directli datacatalog variabl model path snippet pipelin map pipelin pipelin object def creat pipelin kwarg dict str pipelin datacatalog dict model path memorydataset save model path data model model test print exist model path pipelin pipelin node split file data csv paramet train filenam val filenam train label val label split filenam node creat train train filenam val filenam train label val label paramet model path creat dataset train save model node valid model val filenam val label model path valid model decor decor log time decor mem profil return default pipelin follow error run valueerror pipelin input model path datacatalog",
        "Question_preprocessed_content":"perform save nodeoutput pipelin differ node partial run node partial run node need save input node node run access data previou node gener file write code instanc try save dir path directli datacatalog variabl snippet follow error run",
        "Question_gpt_summary_original":"The user is facing a challenge in Kedro regarding where to save node output when partially running some nodes. They are trying to save a directory path directly to the DataCatalog under a variable name 'model_path', but are encountering an error stating that the pipeline input 'model_path' is not found in the DataCatalog. The user is unsure whether to write the code for this in pipeline.py, run.py, or nodes.py.",
        "Question_gpt_summary":"user face challeng save node output partial run node try save directori path directli datacatalog variabl model path encount error state pipelin input model path datacatalog user unsur write code pipelin run node",
        "Answer_original_content":"node input automat load datacatalog pass node function node output consequ save datacatalog node successfulli produc data datacatalog configur default taken conf base catalog yml exampl model path produc creat dataset train save model node consum valid model requir dataset definit conf base catalog yml try store dataset memori memorydataset work run pipelin contain creat dataset valid model node given issu aris try run valid model node attempt read model path dataset memori exist tldr mitig need persist model path ad like follow conf base catalog yml model path type textlocaldataset filepath data intermedi model path txt run creat dataset train save model node depend complet abl start run valid model separ",
        "Answer_preprocessed_content":"node input automat load pass node function node output consequ save datacatalog node successfulli produc data datacatalog configur default taken exampl produc node consum requir dataset definit try store dataset memori work run pipelin contain node try run node attempt read dataset memori exist tldr mitig need persist ad like follow run node complet abl start run separ",
        "Answer_gpt_summary_original":"Solution:\n\nTo mitigate the issue, the user needs to persist 'model_path' by adding it to the 'conf\/base\/catalog.yml' file and running the 'Create Dataset, Train and Save Model' node (and its dependencies) at least once. After completing these steps, the user should be able to run the 'Validate Model' node separately.",
        "Answer_gpt_summary":"solut mitig issu user need persist model path ad conf base catalog yml file run creat dataset train save model node depend complet step user abl run valid model node separ"
    },
    {
        "Question_title":"Where does AzureML run its analytics?",
        "Question_body":"<p>If I have data in a Hadoop Cluster or SQL Elastic DB, is ML bringing that data onto ML servers, or leaving it on Hadoop\/sql and running its analysis there?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1440686059093,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1440686537416,
        "Question_score":0.0,
        "Question_view_count":82.0,
        "Answer_body":"<p>Currently, Azure Machine Learning will bring that data onto ML servers.  <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/32252282",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1440686498027,
        "Question_original_content":"run analyt data hadoop cluster sql elast bring data server leav hadoop sql run analysi",
        "Question_preprocessed_content":"run analyt data hadoop cluster sql elast bring data server leav run analysi",
        "Question_gpt_summary_original":"The user is unsure whether AzureML brings data from a Hadoop Cluster or SQL Elastic DB onto its servers for analysis or if it leaves the data on Hadoop\/SQL and runs the analysis there.",
        "Question_gpt_summary":"user unsur bring data hadoop cluster sql elast server analysi leav data hadoop sql run analysi",
        "Answer_original_content":"current bring data server",
        "Answer_preprocessed_content":"current bring data server",
        "Answer_gpt_summary_original":"Solution: Azure Machine Learning brings data from a Hadoop Cluster or SQL Elastic DB onto its servers for analysis.",
        "Answer_gpt_summary":"solut bring data hadoop cluster sql elast server analysi"
    },
    {
        "Question_title":"Using mlflow.models.FlavorBackend(config, **kwargs)",
        "Question_body":"Hi,\nI am trying to use\u00a0mlflow.models.FlavorBackend(config,\u00a0**kwargs) to serve the model from python. What should be the values of config and what all params do we need to specify? Any kind of help would be appreciated.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1560305537000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":13.0,
        "Answer_body":"Hi Shevy,\n\nThere currently is no Python API for serving models - however you can use the\u00a0mlflow models serve\u00a0CLI (see docs via mlflow models serve --help to serve your Python model. If you have a use case for programmatically launching a model server from within some Python code, we'd be happy to discuss it at\u00a0https:\/\/github.com\/mlflow\/mlflow\/issues.\n\nThanks,\nSid\n\n\nOn Tue, Jun 11, 2019 at 11:12 PM Shevy Mittal <shevy....@gslab.com> wrote:\n\nHi,\nI am trying to use\u00a0mlflow.models.FlavorBackend(config,\u00a0**kwargs) to serve the model from python. What should be the values of config and what all params do we need to specify? Any kind of help would be appreciated.\n\n\nConfidentiality Notice and Disclaimer: This email (including any attachments) contains information that may be confidential, privileged and\/or copyrighted. If you are not the intended recipient, please notify the sender immediately and destroy this email. Any unauthorized use of the contents of this email in any manner whatsoever, is strictly prohibited. If improper activity is suspected, all available information may be used by the sender for possible disciplinary action, prosecution, civil claim or any remedy or lawful purpose. Email transmission cannot be guaranteed to be secure or error-free, as information could be intercepted, lost, arrive late, or contain viruses. The sender is not liable whatsoever for damage resulting from the opening of this message and\/or the use of the information contained in this message and\/or attachments. Expressions in this email cannot be treated as opined by the sender company management \u2013 they are solely expressed by the sender unless authorized.\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/f5f38111-b358-4bc6-a77d-739d45d7e642%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/mjlgu0lDAJM",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-06-12T12:04:46",
                "Answer_body":"Hi Shevy,\n\nThere currently is no Python API for serving models - however you can use the\u00a0mlflow models serve\u00a0CLI (see docs via mlflow models serve --help to serve your Python model. If you have a use case for programmatically launching a model server from within some Python code, we'd be happy to discuss it at\u00a0https:\/\/github.com\/mlflow\/mlflow\/issues.\n\nThanks,\nSid\n\n\nOn Tue, Jun 11, 2019 at 11:12 PM Shevy Mittal <shevy....@gslab.com> wrote:\n\nHi,\nI am trying to use\u00a0mlflow.models.FlavorBackend(config,\u00a0**kwargs) to serve the model from python. What should be the values of config and what all params do we need to specify? Any kind of help would be appreciated.\n\n\nConfidentiality Notice and Disclaimer: This email (including any attachments) contains information that may be confidential, privileged and\/or copyrighted. If you are not the intended recipient, please notify the sender immediately and destroy this email. Any unauthorized use of the contents of this email in any manner whatsoever, is strictly prohibited. If improper activity is suspected, all available information may be used by the sender for possible disciplinary action, prosecution, civil claim or any remedy or lawful purpose. Email transmission cannot be guaranteed to be secure or error-free, as information could be intercepted, lost, arrive late, or contain viruses. The sender is not liable whatsoever for damage resulting from the opening of this message and\/or the use of the information contained in this message and\/or attachments. Expressions in this email cannot be treated as opined by the sender company management \u2013 they are solely expressed by the sender unless authorized.\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/f5f38111-b358-4bc6-a77d-739d45d7e642%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"model flavorbackend config kwarg try us model flavorbackend config kwarg serv model python valu config param need specifi kind help appreci",
        "Question_preprocessed_content":"kwarg try serv model python valu config param need specifi kind help appreci",
        "Question_gpt_summary_original":"The user is facing challenges in using mlflow.models.FlavorBackend(config, **kwargs) to serve a model from Python. They are seeking guidance on the values of config and the necessary parameters to specify.",
        "Question_gpt_summary":"user face challeng model flavorbackend config kwarg serv model python seek guidanc valu config necessari paramet specifi",
        "Answer_original_content":"shevi current python api serv model us model servecli doc model serv help serv python model us case programmat launch model server python code happi discuss athttp github com issu thank sid tue jun shevi mittal wrote try us model flavorbackend config kwarg serv model python valu config param need specifi kind help appreci confidenti notic disclaim email includ attach contain inform confidenti privileg copyright intend recipi notifi sender immedi destroi email unauthor us content email manner whatsoev strictli prohibit improp activ suspect avail inform sender possibl disciplinari action prosecut civil claim remedi law purpos email transmiss guarante secur error free inform intercept lost arriv late contain virus sender liabl whatsoev damag result open messag us inform contain messag attach express email treat opin sender compani manag sole express sender author receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user dde googlegroup com option visit http group googl com optout",
        "Answer_preprocessed_content":"shevi current python api serv model us model servecli serv model python valu config param need specifi kind help appreci confidenti notic disclaim email contain inform confidenti privileg copyright intend recipi notifi sender immedi destroi email unauthor us content email manner whatsoev strictli prohibit improp activ suspect avail inform sender possibl disciplinari action prosecut civil claim remedi law purpos email transmiss guarante secur inform intercept lost arriv late contain virus sender liabl whatsoev damag result open messag us inform contain messag attach express email treat opin sender compani manag sole express sender author receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that there is currently no Python API for serving models. However, the user can use the mlflow models serve CLI to serve their Python model. If the user has a use case for programmatically launching a model server from within some Python code, they can discuss it at https:\/\/github.com\/mlflow\/mlflow\/issues. No specific values of config or necessary parameters are mentioned.",
        "Answer_gpt_summary":"solut discuss suggest current python api serv model user us model serv cli serv python model user us case programmat launch model server python code discuss http github com issu specif valu config necessari paramet mention"
    },
    {
        "Question_title":"How to Cancel Upload? - Azure Machine Learning Studio",
        "Question_body":"Hi,\n\nPlease will you tell me how to cancel a dataset upload?\n\n\n\n\nI tried to upload a small (321kb) CSV to Azure Machine Learning Studio. The upload has been running for more than 1 hour, but it still says uploading.\n\nI tried to upload other files (different names), and they are hanging too....same symptoms.\n\nIt seems the first problem is blocking all subsequent upload attempts.\n\nUntil today uploads worked perfectly.....multiple file types, multiple sizes, multiple dates, were all OK.\n\nI have plenty of space left in my environment.\n\nI tried closing and restarting my browser....same problem. I accessed my AMLS via a different computer...same problem.\n\n\n\n\nThanks in advance for any advice you can give.",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1592921687557,
        "Question_favorite_count":4.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Well, I don't know if you (@GiftA-MSFT) did something to help, but it's solved! Thanks if you did take that initiative, I appreciate it. :-)\n\n...or it could be that after 4-5 hours the upload just completed. My internet connection was fine (it's a 40Mb line), so I'm not sure what the solution was. Perhaps patience alone is the answer.\n\nAnyway, thanks for taking an interest in my problem either way.\n\nBest wishes. :-)",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/39201\/how-to-cancel-upload-azure-machine-learning-studio.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-23T17:33:31.233Z",
                "Answer_score":0,
                "Answer_body":"Well, I don't know if you (@GiftA-MSFT) did something to help, but it's solved! Thanks if you did take that initiative, I appreciate it. :-)\n\n...or it could be that after 4-5 hours the upload just completed. My internet connection was fine (it's a 40Mb line), so I'm not sure what the solution was. Perhaps patience alone is the answer.\n\nAnyway, thanks for taking an interest in my problem either way.\n\nBest wishes. :-)",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1592933611232,
        "Question_original_content":"cancel upload studio tell cancel dataset upload tri upload small csv studio upload run hour sai upload tri upload file differ name hang symptom problem block subsequ upload attempt todai upload work perfectli multipl file type multipl size multipl date plenti space left environ tri close restart browser problem access aml differ problem thank advanc advic",
        "Question_preprocessed_content":"cancel upload studio tell cancel dataset upload tri upload small csv studio upload run hour sai upload tri upload file hang symptom problem block subsequ upload attempt todai upload work file type multipl size multipl date plenti space left environ tri close restart problem access aml differ problem thank advanc advic",
        "Question_gpt_summary_original":"The user is facing challenges with cancelling a dataset upload on Azure Machine Learning Studio. The upload has been running for over an hour and is not completing. The user has tried uploading other files, but they are also hanging. This issue is blocking all subsequent upload attempts, and the user has tried accessing AMLS from a different computer and restarting their browser, but the problem persists.",
        "Question_gpt_summary":"user face challeng cancel dataset upload studio upload run hour complet user tri upload file hang issu block subsequ upload attempt user tri access aml differ restart browser problem persist",
        "Answer_original_content":"know gifta msft help solv thank initi appreci hour upload complet internet connect fine line sure solut patienc answer thank take problem wai best wish",
        "Answer_preprocessed_content":"know help solv thank initi appreci hour upload complet internet connect fine sure solut patienc answer thank take problem wai best wish",
        "Answer_gpt_summary_original":"No solutions were mentioned in the discussion. The user reported that the issue was resolved after waiting for 4-5 hours, but it is unclear what caused the problem or how it was resolved.",
        "Answer_gpt_summary":"solut mention discuss user report issu resolv wait hour unclear caus problem resolv"
    },
    {
        "Question_title":"Procedure for changing cache type",
        "Question_body":"<p>Because I\u2019m using large files which are tens of gigabytes, I want to use hardlinks as my filesystem doesn\u2019t support reflinks.<br>\nI executed the following commands<\/p>\n<pre><code class=\"lang-auto\">dvc config cache.type = hardlink\ndvc config cache.protected = true\ndvc checkout\n<\/code><\/pre>\n<p>However when I checked using <code>ls -i<\/code> the inodes of the file in .dvc\/cache and the file in my working directory, they are still different.<br>\nAre there any further steps I need to take to get hardlinks to be used?<\/p>\n<p>Also I noticed that both the file in the working directory and cache were still writable, when I thought that they should have been made read-only by using <code>cache.protected = true<\/code><br>\nIs my understanding here mistaken?<\/p>\n<p>Update:<br>\nI mistyped the commands above. I actually had executed them correctly, i.e.:<\/p>\n<pre><code class=\"lang-auto\">dvc config cache.type hardlink\ndvc config cache.protected true\ndvc checkout\n<\/code><\/pre>",
        "Question_answer_count":11,
        "Question_comment_count":0,
        "Question_creation_time":1566354562099,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":944.0,
        "Answer_body":"<p>The right commands should NOT have a \u2018<strong>=<\/strong>\u2019, like this:<\/p>\n<pre><code class=\"lang-bash\">dvc config cache.type hardlink\ndvc config cache.protected true\n<\/code><\/pre>\n<p>Then you can check the settings with <code>cat .dvc\/config<\/code>. You should see something like this:<\/p>\n<pre><code class=\"lang-bash\">[cache]\ntype = hardlink\nprotected = true\n<\/code><\/pre>. <p>Thanks dashohoxha.<\/p>\n<p>I\u2019d actually mistyped the commands in my question but had correctly executed them. My config file is set exactly as you said it should be, but I still have the same probem of different inodes and writable files.<br>\nIn addition I notice that files I added using <code>dvc add<\/code> after changing the configuration are using hardlinks and are read-only as expected, so I guess the question is how to apply the configuration to files added before the change.<\/p>. <aside class=\"quote no-group\" data-post=\"3\" data-topic=\"204\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/jcf76\/40\/47_2.png\" class=\"avatar\"> jcf76:<\/div>\n<blockquote>\n<p>the question is how to apply the configuration to files added before the change<\/p>\n<\/blockquote>\n<\/aside>\n<p>I think <code>dvc checkout<\/code> does the trick. Maybe you forgot it<\/p>. <p>Thanks but unfortunately <code>dvc checkout<\/code> doesn\u2019t work. I still have the same problem.<\/p>. <p>What about <code>dvc checkout file.dvc<\/code> ?<\/p>. <p>Nope, that doesn\u2019t work either. I also tried <code>pip install dvc --upgrade<\/code> also just in case it was a problem but it\u2019s still the same with the latest version (0.56.0)<\/p>. <p>I had a previous version of DVC and it seemed to work. Maybe it has changed recently.<br>\nAnyway, it seems to work like this:<\/p>\n<pre><code class=\"lang-bash\">dvc remove file.dvc\ndvc checkout file.dvc\n<\/code><\/pre>. <p>Thank you. This worked!<\/p>. <p>More generic:<\/p>\n<pre><code class=\"lang-bash\">dvc status\ndvc remove *.dvc\ndvc status\ndvc checkout *.dvc\ndvc status\n<\/code><\/pre>. <p><a class=\"mention\" href=\"\/u\/dashohoxha\">@dashohoxha<\/a> Thanks for your answers!<\/p>\n<p><a class=\"mention\" href=\"\/u\/jcf76\">@jcf76<\/a> It is a known bug in checkout, we have an issue for it in <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1371\" rel=\"nofollow noopener\">https:\/\/github.com\/iterative\/dvc\/issues\/1371<\/a> . We\u2019ve addressed a similar issue in sequential <code>dvc add<\/code>(see link to PR in that issue) and will soon proceed with improving <code>dvc checkout<\/code>. Thanks for your feedback! <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>. <p>I think it should be handled explicitly with <code>dvc checkout --relink<\/code> now.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/procedure-for-changing-cache-type\/204",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-08-21T06:59:00.828Z",
                "Answer_body":"<p>The right commands should NOT have a \u2018<strong>=<\/strong>\u2019, like this:<\/p>\n<pre><code class=\"lang-bash\">dvc config cache.type hardlink\ndvc config cache.protected true\n<\/code><\/pre>\n<p>Then you can check the settings with <code>cat .dvc\/config<\/code>. You should see something like this:<\/p>\n<pre><code class=\"lang-bash\">[cache]\ntype = hardlink\nprotected = true\n<\/code><\/pre>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-08-21T07:52:12.827Z",
                "Answer_body":"<p>Thanks dashohoxha.<\/p>\n<p>I\u2019d actually mistyped the commands in my question but had correctly executed them. My config file is set exactly as you said it should be, but I still have the same probem of different inodes and writable files.<br>\nIn addition I notice that files I added using <code>dvc add<\/code> after changing the configuration are using hardlinks and are read-only as expected, so I guess the question is how to apply the configuration to files added before the change.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-08-21T07:56:15.075Z",
                "Answer_body":"<aside class=\"quote no-group\" data-post=\"3\" data-topic=\"204\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/jcf76\/40\/47_2.png\" class=\"avatar\"> jcf76:<\/div>\n<blockquote>\n<p>the question is how to apply the configuration to files added before the change<\/p>\n<\/blockquote>\n<\/aside>\n<p>I think <code>dvc checkout<\/code> does the trick. Maybe you forgot it<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-08-21T08:02:04.531Z",
                "Answer_body":"<p>Thanks but unfortunately <code>dvc checkout<\/code> doesn\u2019t work. I still have the same problem.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-08-21T08:03:34.724Z",
                "Answer_body":"<p>What about <code>dvc checkout file.dvc<\/code> ?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-08-21T08:08:14.148Z",
                "Answer_body":"<p>Nope, that doesn\u2019t work either. I also tried <code>pip install dvc --upgrade<\/code> also just in case it was a problem but it\u2019s still the same with the latest version (0.56.0)<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-08-21T08:19:15.132Z",
                "Answer_body":"<p>I had a previous version of DVC and it seemed to work. Maybe it has changed recently.<br>\nAnyway, it seems to work like this:<\/p>\n<pre><code class=\"lang-bash\">dvc remove file.dvc\ndvc checkout file.dvc\n<\/code><\/pre>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-08-21T08:22:03.040Z",
                "Answer_body":"<p>Thank you. This worked!<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-08-21T08:30:02.559Z",
                "Answer_body":"<p>More generic:<\/p>\n<pre><code class=\"lang-bash\">dvc status\ndvc remove *.dvc\ndvc status\ndvc checkout *.dvc\ndvc status\n<\/code><\/pre>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-08-21T18:04:28.755Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/dashohoxha\">@dashohoxha<\/a> Thanks for your answers!<\/p>\n<p><a class=\"mention\" href=\"\/u\/jcf76\">@jcf76<\/a> It is a known bug in checkout, we have an issue for it in <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1371\" rel=\"nofollow noopener\">https:\/\/github.com\/iterative\/dvc\/issues\/1371<\/a> . We\u2019ve addressed a similar issue in sequential <code>dvc add<\/code>(see link to PR in that issue) and will soon proceed with improving <code>dvc checkout<\/code>. Thanks for your feedback! <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-01-01T19:32:28.995Z",
                "Answer_body":"<p>I think it should be handled explicitly with <code>dvc checkout --relink<\/code> now.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"procedur chang cach type larg file ten gigabyt want us hardlink filesystem doesnt support reflink execut follow command config cach type hardlink config cach protect true checkout check inod file cach file work directori differ step need hardlink notic file work directori cach writabl thought read cach protect true understand mistaken updat mistyp command actual execut correctli config cach type hardlink config cach protect true checkout",
        "Question_preprocessed_content":"procedur chang cach type larg file ten gigabyt want us hardlink filesystem doesnt support reflink execut follow command check inod file file work directori differ step need hardlink notic file work directori cach writabl thought understand mistaken updat mistyp command actual execut correctli",
        "Question_gpt_summary_original":"The user is facing challenges in changing the cache type to hardlink for large files. The inodes of the file in .dvc\/cache and the file in the working directory are still different even after executing the necessary commands. The user is also confused about the cache.protected command as both the file in the working directory and cache were still writable.",
        "Question_gpt_summary":"user face challeng chang cach type hardlink larg file inod file cach file work directori differ execut necessari command user confus cach protect command file work directori cach writabl",
        "Answer_original_content":"right command like config cach type hardlink config cach protect true check set cat config like cach type hardlink protect true thank dashohoxha actual mistyp command question correctli execut config file set exactli said probem differ inod writabl file addit notic file ad add chang configur hardlink read expect guess question appli configur file ad chang jcf question appli configur file ad chang think checkout trick mayb forgot thank unfortun checkout doesnt work problem checkout file nope doesnt work tri pip instal upgrad case problem latest version previou version work mayb chang recent work like remov file checkout file thank work gener statu remov statu checkout statu dashohoxha thank answer jcf known bug checkout issu http github com iter issu weve address similar issu sequenti add link issu soon proce improv checkout thank feedback think handl explicitli checkout relink",
        "Answer_preprocessed_content":"right command like check set like thank dashohoxha actual mistyp command question correctli execut config file set exactli said probem differ inod writabl file addit notic file ad chang configur hardlink expect guess question appli configur file ad jcf question appli configur file ad chang think trick mayb forgot thank unfortun doesnt work nope doesnt work tri case problem latest version previou version work mayb chang recent work like thank work gener thank answer known bug checkout issu weve address similar issu sequenti soon proce improv thank feedback think handl explicitli",
        "Answer_gpt_summary_original":"Solutions provided:\n- The correct commands for changing the cache type to hardlink are: \"dvc config cache.type hardlink\" and \"dvc config cache.protected true\". Check the settings with \"cat .dvc\/config\".\n- Use \"dvc checkout file.dvc\" or \"dvc remove file.dvc\" followed by \"dvc checkout file.dvc\" to apply the configuration to files added before the change.\n- Use \"dvc checkout --relink\" to handle the issue explicitly.",
        "Answer_gpt_summary":"solut provid correct command chang cach type hardlink config cach type hardlink config cach protect true check set cat config us checkout file remov file follow checkout file appli configur file ad chang us checkout relink handl issu explicitli"
    },
    {
        "Question_title":"Is it possible to check that the version of a file tracked by a DVC metadata file exists in remote storage without pulling the file?",
        "Question_body":"<p>My team has a set up wherein we track datasets and models in DVC, and have a GitLab repository for tracking our code and DVC metadata files. We have a job in our dev GitLab pipeline (run on each push to a merge request) that has the goal of checking to be sure that the developer remembered to run <code>dvc push<\/code> to keep DVC remote storage up-to-date. Right now, the way we do this is by running <code>dvc pull<\/code> on the GitLab runner, which will fail with errors telling you which files (new files or latest versions of existing files) were not found.<\/p>\n<p>The downside to this approach is that we are loading the entirety of our data stored in DVC onto a GitLab runner, and we've run into out-of-memory issues, not to mention lengthy run time to download all that data. Since the path and md5 hash of the objects are stored in the DVC metadata files, I would think that's all the information that DVC would need to be able to answer the question &quot;is the remote storage system up-to-date&quot;.<\/p>\n<p>It seems like <code>dvc status<\/code> is similar to what I'm asking for, but compares the cache or workspace and remote storage. In other words, it requires the files to actually be present on whatever filesystem is making the call.<\/p>\n<p>Is there some way to achieve the goal I laid out above (&quot;inform the developer that they need to run <code>dvc push<\/code>&quot;) without pulling everything from DVC?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1622232629793,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1622257491983,
        "Question_score":5.0,
        "Question_view_count":488.0,
        "Answer_body":"<blockquote>\n<p>It seems like dvc status is similar to what I'm asking for<\/p>\n<\/blockquote>\n<p><code>dvc status --cloud<\/code> will give you a list of &quot;new&quot; files if they that haven't been pushed to the (default) remote. It won't error out though, so your CI script should fail depending on the stdout message.<\/p>\n<p>More info: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/status#options\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/status#options<\/a><\/p>\n<p>I'd also ask everyone to run <code>dvc install<\/code>, which will setup some Git hooks, including automatic <code>dvc push<\/code> with <code>git push<\/code>.<\/p>\n<p>See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/install\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/install<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1622503453296,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67744934",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1622257759208,
        "Question_original_content":"possibl check version file track metadata file exist remot storag pull file team set track dataset model gitlab repositori track code metadata file job dev gitlab pipelin run push merg request goal check sure develop rememb run push remot storag date right wai run pull gitlab runner fail error tell file new file latest version exist file downsid approach load entireti data store gitlab runner run memori issu mention lengthi run time download data path hash object store metadata file think inform need abl answer question remot storag date like statu similar ask compar cach workspac remot storag word requir file actual present filesystem make wai achiev goal laid inform develop need run push pull",
        "Question_preprocessed_content":"possibl check version file track metadata file exist remot storag pull file team set track dataset model gitlab repositori track code metadata file job dev gitlab pipelin goal check sure develop rememb run remot storag right wai run gitlab runner fail error tell file downsid approach load entireti data store gitlab runner run issu mention lengthi run time download data path hash object store metadata file think inform need abl answer question remot storag like similar ask compar cach workspac remot storag word requir file actual present filesystem make wai achiev goal laid pull",
        "Question_gpt_summary_original":"The user's team tracks datasets and models in DVC and has a GitLab repository for tracking code and DVC metadata files. They have a job in their dev GitLab pipeline that checks if the developer remembered to run \"dvc push\" to keep DVC remote storage up-to-date. Currently, they run \"dvc pull\" on the GitLab runner, which loads the entirety of their data stored in DVC onto the runner, causing out-of-memory issues and lengthy run time. The user is looking for a way to check if the version of a file tracked by a DVC metadata file exists in remote storage without pulling the file.",
        "Question_gpt_summary":"user team track dataset model gitlab repositori track code metadata file job dev gitlab pipelin check develop rememb run push remot storag date current run pull gitlab runner load entireti data store runner caus memori issu lengthi run time user look wai check version file track metadata file exist remot storag pull file",
        "Answer_original_content":"like statu similar ask statu cloud list new file haven push default remot won error script fail depend stdout messag info http org doc command refer statu option ask run instal setup git hook includ automat push git push http org doc command refer instal",
        "Answer_preprocessed_content":"like statu similar ask list new file haven push remot won error script fail depend stdout messag info ask run setup git hook includ automat",
        "Answer_gpt_summary_original":"Solution:\n- Use the command \"dvc status --cloud\" to get a list of new files that haven't been pushed to the remote storage without pulling the file.\n- Set up Git hooks, including automatic \"dvc push\" with \"git push\" by running \"dvc install\".",
        "Answer_gpt_summary":"solut us command statu cloud list new file haven push remot storag pull file set git hook includ automat push git push run instal"
    },
    {
        "Question_title":"Azure Machine Learning (and cognitive services) is not supported in Region \"Germany\"?",
        "Question_body":"I'm working for a campany located in Germany. We want to use Azure Machine Learning (and other stuff like that).\nWe are only allowed to use Azure in the Region \"Germany\", because the data of our customers cannot left germany.\n\nNow I saw, that a lot of stuff in Azure Machine Learning is not available in Germany?\n\nQuestions:\n1. Is that true?\n2. Does some one now, at what time Microsoft plans to make the stuff available in Germany?\n\nThank you for a answer!\n\nPatrick",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1612860215177,
        "Question_favorite_count":5.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi @PatrickHuber-5684\nYes, Azure Machine Learning is not available in Germany region.\n\nPlease check in Azure feedback\n\nIf the Answer is helpful, please click Accept Answer and up-vote, this can be beneficial to other community members.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/265151\/azure-machine-learning-and-cognitive-services-is-n.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-09T09:16:36.647Z",
                "Answer_score":0,
                "Answer_body":"Hi @PatrickHuber-5684\nYes, Azure Machine Learning is not available in Germany region.\n\nPlease check in Azure feedback\n\nIf the Answer is helpful, please click Accept Answer and up-vote, this can be beneficial to other community members.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-02-10T08:23:01.1Z",
                "Answer_score":0,
                "Answer_body":"Thank you for your answser!\nDo you perhaps know anything about, when the features are supported?\n\nThank you!",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":1612862196647,
        "Question_original_content":"cognit servic support region germani work campani locat germani want us stuff like allow us azur region germani data custom left germani saw lot stuff avail germani question true time microsoft plan stuff avail germani thank answer patrick",
        "Question_preprocessed_content":"support region germani work campani locat germani want us allow us azur region germani data custom left germani saw lot stuff avail germani question true time microsoft plan stuff avail germani thank answer patrick",
        "Question_gpt_summary_original":"The user is facing challenges in using Azure Machine Learning and other related services in Germany due to restrictions on customer data leaving the country. They have noticed that many features of Azure Machine Learning are not available in Germany and are seeking information on when these features will be made available.",
        "Question_gpt_summary":"user face challeng relat servic germani restrict custom data leav countri notic featur avail germani seek inform featur avail",
        "Answer_original_content":"patrickhub ye avail germani region check azur feedback answer help click accept answer vote benefici commun member",
        "Answer_preprocessed_content":"ye avail germani region check azur feedback answer help click accept answer benefici commun member",
        "Answer_gpt_summary_original":"Solution: No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut solut provid discuss"
    },
    {
        "Question_title":"How to overcome TrainingException when training a large model with Azure Machine Learning service?",
        "Question_body":"<p>I'm training a large-ish model, trying to use for the purpose <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml#create-an-estimator\" rel=\"nofollow noreferrer\">Azure Machine Learning service<\/a> in Azure notebooks.<\/p>\n\n<p>I thus create an <code>Estimator<\/code> to train locally:<\/p>\n\n<pre><code>from azureml.train.estimator import Estimator\n\nestimator = Estimator(source_directory='.\/source_dir',\n                      compute_target='local',\n                      entry_script='train.py')\n<\/code><\/pre>\n\n<p>(my <code>train.py<\/code> should load and train starting from a large word vector file).<\/p>\n\n<p>When running with <\/p>\n\n<pre><code>run = experiment.submit(config=estimator)\n<\/code><\/pre>\n\n<p>I get <\/p>\n\n<blockquote>\n  <p>TrainingException: <\/p>\n  \n  <p>====================================================================<\/p>\n  \n  <p>While attempting to take snapshot of\n  \/data\/home\/username\/notebooks\/source_dir Your total\n  snapshot size exceeds the limit of 300.0 MB. Please see\n  <a href=\"http:\/\/aka.ms\/aml-largefiles\" rel=\"nofollow noreferrer\">http:\/\/aka.ms\/aml-largefiles<\/a> on how to work with large files.<\/p>\n  \n  <p>====================================================================<\/p>\n<\/blockquote>\n\n<p>The link provided in the error is likely <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/issues\/26076\" rel=\"nofollow noreferrer\">broken<\/a>. \nContents in my <code>.\/source_dir<\/code> indeed exceed 300 MB.<br>\nHow can I solve this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1554414642303,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1554642637940,
        "Question_score":1.0,
        "Question_view_count":1127.0,
        "Answer_body":"<p>You can place the training files outside <code>source_dir<\/code> so that they don't get uploaded as part of submitting the experiment, and then upload them separately to the data store (which is basically using the Azure storage associated with your workspace). All you need to do then is reference the training files from <code>train.py<\/code>. <\/p>\n\n<p>See the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml\" rel=\"nofollow noreferrer\">Train model tutorial<\/a> for an example of how to upload data to the data store and then access it from the training file.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1554449103928,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55525445",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1554445793380,
        "Question_original_content":"overcom trainingexcept train larg model servic train larg ish model try us purpos servic azur notebook creat estim train local train estim import estim estim estim sourc directori sourc dir comput target local entri script train train load train start larg word vector file run run experi submit config estim trainingexcept attempt snapshot data home usernam notebook sourc dir total snapshot size exce limit http aka aml largefil work larg file link provid error like broken content sourc dir exce solv",
        "Question_preprocessed_content":"overcom trainingexcept train larg model servic train model try us purpos servic azur notebook creat train local load train start larg word vector file run trainingexcept attempt snapshot total snapshot size exce limit work larg file link provid error like broken content exce solv",
        "Question_gpt_summary_original":"The user is encountering a TrainingException while trying to train a large model using Azure Machine Learning service in Azure notebooks. The error message indicates that the total snapshot size exceeds the limit of 300 MB, and the link provided in the error is broken. The user is seeking a solution to overcome this challenge.",
        "Question_gpt_summary":"user encount trainingexcept try train larg model servic azur notebook error messag indic total snapshot size exce limit link provid error broken user seek solut overcom challeng",
        "Answer_original_content":"place train file outsid sourc dir upload submit experi upload separ data store basic azur storag associ workspac need refer train file train train model tutori exampl upload data data store access train file",
        "Answer_preprocessed_content":"place train file outsid upload submit experi upload separ data store need refer train file train model tutori exampl upload data data store access train file",
        "Answer_gpt_summary_original":"Solution: The user can place the training files outside the source directory and upload them separately to the data store. Then, the user can reference the training files from train.py. The Train model tutorial provides an example of how to upload data to the data store and access it from the training file.",
        "Answer_gpt_summary":"solut user place train file outsid sourc directori upload separ data store user refer train file train train model tutori provid exampl upload data data store access train file"
    },
    {
        "Question_title":"Azure Auto ML JobConfigurationMaxSizeExceeded error when using a cluster",
        "Question_body":"<p>I am running into the following error when I try to run Automated ML through the studio on a GPU compute cluster:<\/p>\n<p><img src=\"https:\/\/i.stack.imgur.com\/uLyxr.png\" alt=\"Azure ML error message\" \/><\/p>\n<blockquote>\n<p>Error: AzureMLCompute job failed. JobConfigurationMaxSizeExceeded: The\nspecified job configuration exceeds the max allowed size of 32768\ncharacters. Please reduce the size of the job's command line arguments\nand environment settings<\/p>\n<\/blockquote>\n<p>The attempted run is on a registered tabulated dataset in filestore and is a simple regression case. Strangely, it works just fine with the CPU compute instance I use for my other pipelines. I have been able to run it a few times using that and wanted to upgrade to a cluster only to be hit by this error. I found online that it could be a case of having the following setting: AZUREML_COMPUTE_USE_COMMON_RUNTIME:false; but I am not sure where to put this in when just running from the web studio.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1638986274657,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1641204588636,
        "Question_score":5.0,
        "Question_view_count":171.0,
        "Answer_body":"<p>It looks like the bug was fixed. I just ran it on a cluster without changing any of the parameters. Thank you Yutong for the help!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70279636",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1639585163672,
        "Question_original_content":"azur auto jobconfigurationmaxsizeexceed error cluster run follow error try run autom studio gpu comput cluster error comput job fail jobconfigurationmaxsizeexceed specifi job configur exce max allow size charact reduc size job command line argument environ set attempt run regist tabul dataset filestor simpl regress case strang work fine cpu comput instanc us pipelin abl run time want upgrad cluster hit error onlin case have follow set comput us common runtim fals sure run web studio",
        "Question_preprocessed_content":"azur auto jobconfigurationmaxsizeexceed error cluster run follow error try run autom studio gpu comput cluster error comput job fail jobconfigurationmaxsizeexceed specifi job configur exce max allow size charact reduc size job command line argument environ set attempt run regist tabul dataset filestor simpl regress case strang work fine cpu comput instanc us pipelin abl run time want upgrad cluster hit error onlin case have follow set sure run web studio",
        "Question_gpt_summary_original":"The user encountered an error while running Automated ML through the studio on a GPU compute cluster. The error message indicates that the job configuration exceeds the maximum allowed size of 32768 characters. The user has tried to run the same job on a CPU compute instance without any issues. The user is unsure where to put the setting \"AZUREML_COMPUTE_USE_COMMON_RUNTIME:false\" when running from the web studio.",
        "Question_gpt_summary":"user encount error run autom studio gpu comput cluster error messag indic job configur exce maximum allow size charact user tri run job cpu comput instanc issu user unsur set comput us common runtim fals run web studio",
        "Answer_original_content":"look like bug fix ran cluster chang paramet thank yutong help",
        "Answer_preprocessed_content":"look like bug fix ran cluster chang paramet thank yutong help",
        "Answer_gpt_summary_original":"No solutions were mentioned in the discussion as the user reported that the issue was resolved without making any changes to the parameters.",
        "Answer_gpt_summary":"solut mention discuss user report issu resolv make chang paramet"
    },
    {
        "Question_title":"How to Find Azure ML Experiment based on Deployed Web Service",
        "Question_body":"<p>I have a list of ML experiments which I have created in Azure Machine Learning Studio.  I have deployed them as web services (the new version, not classic).  <\/p>\n\n<p>How can I go into Azure Machine Learning Web Services, click on a web service (which was deployed from an experiment), then navigate back to the experiment \/ predictive model which feeds it?<\/p>\n\n<p>The only link I can find between the two is by updating the web service from the predictive experiment, which then confirms what the web service is. I can see that the \"ExperimentId\" is a GUID in the URL when in the experiment and the web service, so hopefully this is possible.<\/p>\n\n<p>My reasoning is that relying on matching naming conventions, etc., to select the appropriate model to update is subject to human error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1489415750577,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":7.0,
        "Question_view_count":224.0,
        "Answer_body":"<p>The <em>new<\/em> web service does not store any information about the experiment or workspace that was deployed (not all <em>new<\/em> web services are deployed from an experiment).<\/p>\n\n<p>Here are the options available to track the relationship between the experiment and a <em>new<\/em> web service.<\/p>\n\n<h2>last deployment<\/h2>\n\n<p>However, the experiment keeps track of the <strong>last<\/strong> <em>new<\/em> web service that was deployed from the experiment. each deployment to a <em>new<\/em> web service overwrites this value.<\/p>\n\n<p>The value is stored in the experiment graph. One way to get the graph is to use the powershell module <a href=\"http:\/\/aka.ms\/amlps\" rel=\"nofollow noreferrer\">amlps<\/a><\/p>\n\n<p><code>Export-AmlExperimentGraph -ExperimentId &lt;Experiment Id&gt; -OutputFile e.json<\/code><\/p>\n\n<p><strong>e.json<\/strong><\/p>\n\n<pre><code>{\n\"ExperimentId\":\"&lt;Experiment Id&gt;\",\n\/\/ . . .\n\"WebService\":{\n\/\/ . . .\n\"ArmWebServiceId\":\"&lt;Arm Id&gt;\"\n},\n\/\/ . . . \n}\n<\/code><\/pre>\n\n<h2>azure resource tags<\/h2>\n\n<p>The tags feature for Azure resources is supported by the <em>new<\/em> web services. Setting a <code>tag<\/code> on the web service programmatically, with powershell or via the azure portal UX can be used to store a reference to the experiment on the <em>new<\/em> web service.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/42766263",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1490283526688,
        "Question_original_content":"experi base deploi web servic list experi creat studio deploi web servic new version classic web servic click web servic deploi experi navig experi predict model feed link updat web servic predict experi confirm web servic experimentid guid url experi web servic hopefulli possibl reason reli match name convent select appropri model updat subject human error",
        "Question_preprocessed_content":"experi base deploi web servic list experi creat studio deploi web servic web servic click web servic navig experi predict model feed link updat web servic predict experi confirm web servic experimentid guid url experi web servic hopefulli possibl reason reli match name convent select appropri model updat subject human error",
        "Question_gpt_summary_original":"The user is facing a challenge in finding the Azure ML experiment that feeds a deployed web service. They are unable to navigate back to the experiment from the web service and are relying on matching naming conventions, which is subject to human error. The user is looking for a way to link the web service to the experiment using the ExperimentId GUID in the URL.",
        "Question_gpt_summary":"user face challeng find experi feed deploi web servic unabl navig experi web servic reli match name convent subject human error user look wai link web servic experi experimentid guid url",
        "Answer_original_content":"new web servic store inform experi workspac deploi new web servic deploi experi option avail track relationship experi new web servic deploy experi keep track new web servic deploi experi deploy new web servic overwrit valu valu store experi graph wai graph us powershel modul amlp export amlexperimentgraph experimentid outputfil json json experimentid webservic armwebserviceid azur resourc tag tag featur azur resourc support new web servic set tag web servic programmat powershel azur portal store refer experi new web servic",
        "Answer_preprocessed_content":"new web servic store inform experi workspac deploi option avail track relationship experi new web servic deploy experi keep track new web servic deploi experi deploy new web servic overwrit valu valu store experi graph wai graph us powershel modul amlp azur resourc tag tag featur azur resourc support new web servic set web servic programmat powershel azur portal store refer experi new web servic",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. The experiment keeps track of the last new web service that was deployed from the experiment. The value is stored in the experiment graph, which can be obtained using the powershell module amlps.\n2. The tags feature for Azure resources is supported by the new web services. Setting a tag on the web service programmatically, with powershell or via the azure portal UX can be used to store a reference to the experiment on the new web service.",
        "Answer_gpt_summary":"possibl solut mention discuss experi keep track new web servic deploi experi valu store experi graph obtain powershel modul amlp tag featur azur resourc support new web servic set tag web servic programmat powershel azur portal store refer experi new web servic"
    },
    {
        "Question_title":"User Docker Hub registry containers in AWS Sagemaker",
        "Question_body":"<p>Is there any way to load containers stored in docker hub registry in Amazon Sagemaker.\nAccording to some documentation, it should be possible, but I have not been able to find any relevan example or guide for it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1571665775480,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":135.0,
        "Answer_body":"<p>While you can use any registry when working with Docker on a SageMaker notebook, as of this writing other SageMaker components presently only support images from Amazon ECR repositories.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58487710",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1582242007587,
        "Question_original_content":"user docker hub registri contain wai load contain store docker hub registri accord document possibl abl relevan exampl guid",
        "Question_preprocessed_content":"user docker hub registri contain wai load contain store docker hub registri accord document possibl abl relevan exampl guid",
        "Question_gpt_summary_original":"The user is facing challenges in loading containers stored in Docker Hub registry in Amazon Sagemaker and has not been able to find any relevant example or guide for it.",
        "Question_gpt_summary":"user face challeng load contain store docker hub registri abl relev exampl guid",
        "Answer_original_content":"us registri work docker notebook write compon present support imag amazon ecr repositori",
        "Answer_preprocessed_content":"us registri work docker notebook write compon present support imag amazon ecr repositori",
        "Answer_gpt_summary_original":"Solution: No solution is provided in the discussion.",
        "Answer_gpt_summary":"solut solut provid discuss"
    },
    {
        "Question_title":"Multi-file source_dir bundle with SM Training Compiler (distributed)",
        "Question_body":"I'm hoping to use SageMaker Training Compiler with a (Hugging Face Trainer API, PyTorch) program split across multiple .py files for maintainability. The job needs to run on multiple GPUs (although at the current scale, multi-device single-node would be acceptable).\n\nFollowing the docs, I added the distributed_training_launcher.py launcher script to my source_dir bundle, and passed in the true training script via a training_script hyperparameter.\n\n...But when the job tries to start, I get:\n\nTraceback (most recent call last):\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\nreturn _run_code(code, main_globals, None,\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 90, in <module>\nmain()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 86, in main\nxmp.spawn(mod._mp_fn, args=(), nprocs=args.num_gpus)\nAttributeError: module 'train' has no attribute '_mp_fn'\n\n\nAny ideas what might be causing this? Is there some particular limitation or additional requirement for training scripts that are written over multiple files?\n\nI also tried running in single-GPU mode (p3.2xlarge) instead - directly calling the train script instead of the distributed launcher - and saw the below error which seems to originate within TrainingArguments itself? Not sure why it's trying to call a 'tensorflow\/compiler' compiler when running in PT..?\n\nEDIT: Turns out the below error can be solved by explicitly setting n_gpus as mentioned on the troubleshooting doc - but that takes me back to the error message above\n\nFile \"\/opt\/ml\/code\/code\/config.py\", line 124, in __post_init__\nsuper().__post_init__()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 761, in __post_init__\nif is_torch_available() and self.device.type != \"cuda\" and (self.fp16 or self.fp16_full_eval):\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 975, in device\nreturn self._setup_devices\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1754, in __get__\ncached = self.fget(obj)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 918, in _setup_devices\ndevice = xm.xla_device()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 231, in xla_device\ndevices = get_xla_supported_devices(\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 137, in get_xla_supported_devices\nxla_devices = _DEVICES.value\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/utils\/utils.py\", line 32, in value\nself._value = self._gen_fn()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 19, in <lambda>\n_DEVICES = xu.LazyProperty(lambda: torch_xla._XLAC._xla_get_devices())\nRuntimeError: tensorflow\/compiler\/xla\/xla_client\/computation_client.cc:273 : Missing XLA configuration",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639669045329,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":29.0,
        "Answer_body":"Ahh I solved this a while ago and forgot to update -\n\nYes, the training script needs to define a _mp_fn (which can just execute the same code as gets run if __name__ == \"__main__\") and number of GPUs (at least the last time I checked - hopefully this could change in future) needs to be explicitly configured.\n\nFor my particular project the fix to enable SMTC on the existing job is available online here. For others would also suggest referring to the official SMTC example notebooks & scripts!",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwcM0XER5TcOggtQ_5cfVPw\/multi-file-source-dir-bundle-with-sm-training-compiler-distributed",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-15T08:01:47.440Z",
                "Answer_score":0,
                "Answer_body":"Ahh I solved this a while ago and forgot to update -\n\nYes, the training script needs to define a _mp_fn (which can just execute the same code as gets run if __name__ == \"__main__\") and number of GPUs (at least the last time I checked - hopefully this could change in future) needs to be explicitly configured.\n\nFor my particular project the fix to enable SMTC on the existing job is available online here. For others would also suggest referring to the official SMTC example notebooks & scripts!",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1657872107440,
        "Question_original_content":"multi file sourc dir bundl train compil distribut hope us train compil hug face trainer api pytorch program split multipl file maintain job need run multipl gpu current scale multi devic singl node accept follow doc ad distribut train launcher launcher script sourc dir bundl pass true train script train script hyperparamet job tri start traceback recent file opt conda lib python runpi line run modul main return run code code main global file opt conda lib python runpi line run code exec code run global file opt conda lib python site packag torch xla distribut xla spawn line main file opt conda lib python site packag torch xla distribut xla spawn line main xmp spawn mod arg nproc arg num gpu attributeerror modul train attribut idea caus particular limit addit requir train script written multipl file tri run singl gpu mode xlarg instead directli call train script instead distribut launcher saw error origin trainingargu sure try tensorflow compil compil run edit turn error solv explicitli set gpu mention troubleshoot doc take error messag file opt code code config line post init super post init file opt conda lib python site packag transform train arg line post init torch avail self devic type cuda self self eval file opt conda lib python site packag transform file util line wrapper return func arg kwarg file opt conda lib python site packag transform train arg line devic return self setup devic file opt conda lib python site packag transform file util line cach self fget obj file opt conda lib python site packag transform file util line wrapper return func arg kwarg file opt conda lib python site packag transform train arg line setup devic devic xla devic file opt conda lib python site packag torch xla core xla model line xla devic devic xla support devic file opt conda lib python site packag torch xla core xla model line xla support devic xla devic devic valu file opt conda lib python site packag torch xla util util line valu self valu self gen file opt conda lib python site packag torch xla core xla model line devic lazyproperti lambda torch xla xlac xla devic runtimeerror tensorflow compil xla xla client comput client miss xla configur",
        "Question_preprocessed_content":"bundl train compil hope us train compil program split multipl file maintain job need run multipl gpu follow doc ad launcher script bundl pass true train script hyperparamet job tri start traceback file line return file line exec file line main file line main arg attributeerror modul train attribut idea caus particular limit addit requir train script written multipl file tri run mode instead directli call train script instead distribut launcher saw error origin trainingargu sure try compil run edit turn error solv explicitli set mention troubleshoot doc take error messag file line file line cuda file line wrapper return func file line devic return file line cach file line wrapper return func file line devic file line devic file line file line valu file line runtimeerror miss xla configur",
        "Question_gpt_summary_original":"The user is facing challenges while using SageMaker Training Compiler with a program split across multiple .py files for maintainability. The job needs to run on multiple GPUs, but when the job tries to start, the user gets an error message. The error message suggests that there might be some particular limitation or additional requirement for training scripts that are written over multiple files. The user also tried running in single-GPU mode but encountered another error message that seems to originate within TrainingArguments itself.",
        "Question_gpt_summary":"user face challeng train compil program split multipl file maintain job need run multipl gpu job tri start user get error messag error messag suggest particular limit addit requir train script written multipl file user tri run singl gpu mode encount error messag origin trainingargu",
        "Answer_original_content":"ahh solv ago forgot updat ye train script need defin execut code get run main number gpu time check hopefulli chang futur need explicitli configur particular project fix enabl smtc exist job avail onlin suggest refer offici smtc exampl notebook script",
        "Answer_preprocessed_content":"ahh solv ago forgot updat ye train script need defin number gpu need explicitli configur particular project fix enabl smtc exist job avail onlin suggest refer offici smtc exampl notebook script",
        "Answer_gpt_summary_original":"Solution: The user needs to define a _mp_fn in the training script and explicitly configure the number of GPUs. Referring to the official SageMaker Training Compiler example notebooks and scripts is also suggested. The fix to enable SMTC on the existing job is available online.",
        "Answer_gpt_summary":"solut user need defin train script explicitli configur number gpu refer offici train compil exampl notebook script suggest fix enabl smtc exist job avail onlin"
    },
    {
        "Question_title":"Load Azure ML experiment run information from datastore",
        "Question_body":"<p>I have lots of run files created by running PyTorch estimator\/ ScriptRunStep experiments that are saved in azureml blob storage container. Previously, I'd been viewing these runs in the Experiments tab of the ml.azure.com portal and associating tags to these runs to categorise and load the desired models.<\/p>\n<p>However, a coworker recently deleted my workspace. I created a new one which is connected to the previously-existing blob container, the run files therefore still exist and can be accessed on this new workspace, but they no longer show up in the Experiment viewer on ml.azure.com. Neither can I see the tags I'd associated to the runs.<\/p>\n<p><strong>Is there any way to load these old run files into the Experiment viewer or is it only possible to view runs created inside the current workspace?<\/strong><\/p>\n<p>Sample scriptrunconfig code:<\/p>\n<pre><code>data_ref = DataReference(datastore=ds,\n                         data_reference_name=&quot;&lt;name&gt;&quot;,        \n                         path_on_datastore = &quot;&lt;path&gt;&quot;)\nargs = ['--data_dir',   str(data_ref),     \n        '--num_epochs', 30,     \n        '--lr',         0.01,          \n        '--classifier', 'int_ext' ]  \n\nsrc = ScriptRunConfig(source_directory='.',                       \n                      arguments=args,                      \n                      compute_target = compute_target,                       \n                      environment = env,                       \n                      script='train.py') \nsrc.run_config.data_references = {data_ref.data_reference_name: \n                                  data_ref.to_config()} \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1616682318310,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1617188538763,
        "Question_score":1.0,
        "Question_view_count":164.0,
        "Answer_body":"<p>Sorry for your loss! First, I'd make absolutely sure that you can't recover the deleted workspace. Definitely worthwhile to open an priority support ticket with Azure.<\/p>\n<p>Another thing you might try is:<\/p>\n<ol>\n<li>create a new workspace (which will create a new storage account for you for the new workspace's logs)<\/li>\n<li>copy your old workspace's data into the new workspace's storage account.<\/li>\n<\/ol>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66801546",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1616813814780,
        "Question_original_content":"load experi run inform datastor lot run file creat run pytorch estim scriptrunstep experi save blob storag contain previous view run experi tab azur com portal associ tag run categoris load desir model cowork recent delet workspac creat new connect previous exist blob contain run file exist access new workspac longer experi viewer azur com tag associ run wai load old run file experi viewer possibl view run creat insid current workspac sampl scriptrunconfig code data ref datarefer datastor data refer path datastor arg data dir str data ref num epoch classifi int ext src scriptrunconfig sourc directori argument arg comput target comput target environ env script train src run config data refer data ref data refer data ref config",
        "Question_preprocessed_content":"load experi run inform datastor lot run file creat run pytorch estim scriptrunstep experi save blob storag contain previous view run experi tab portal associ tag run categoris load desir model cowork recent delet workspac creat new connect blob contain run file exist access new workspac longer experi viewer tag associ run wai load old run file experi viewer possibl view run creat insid current workspac sampl scriptrunconfig code",
        "Question_gpt_summary_original":"The user has encountered a challenge in loading old run files created by PyTorch estimator\/ScriptRunStep experiments from Azure ML blob storage container into the Experiment viewer on ml.azure.com after their workspace was deleted by a coworker. The user is seeking a solution to load these old run files into the Experiment viewer or if it is only possible to view runs created inside the current workspace.",
        "Question_gpt_summary":"user encount challeng load old run file creat pytorch estim scriptrunstep experi blob storag contain experi viewer azur com workspac delet cowork user seek solut load old run file experi viewer possibl view run creat insid current workspac",
        "Answer_original_content":"sorri loss absolut sure recov delet workspac definit worthwhil open prioriti support ticket azur thing try creat new workspac creat new storag account new workspac log copi old workspac data new workspac storag account",
        "Answer_preprocessed_content":"sorri loss absolut sure recov delet workspac definit worthwhil open prioriti support ticket azur thing try creat new workspac copi old workspac data new workspac storag account",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are: \n- Trying to recover the deleted workspace by opening a priority support ticket with Azure.\n- Creating a new workspace and copying the old workspace's data into the new workspace's storage account.",
        "Answer_gpt_summary":"possibl solut mention discuss try recov delet workspac open prioriti support ticket azur creat new workspac copi old workspac data new workspac storag account"
    },
    {
        "Question_title":"Pipeline template",
        "Question_body":"<p>hey!<\/p>\n<p>I want to create a template of a pipeline  -<br>\ni.e. i want to run the same pipeline with different parameters each time.<br>\ni read the templating section documentation and looks like the only solution is<br>\n<code>foreach<\/code> in each stage,<\/p>\n<p>is it possible to run multiple stages with one foreach block?<br>\nwhat if I want to run one specific set of params?<\/p>\n<p>any suggestions?<\/p>\n<p>thanks!!!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1632646722796,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":248.0,
        "Answer_body":"<p><a class=\"mention\" href=\"\/u\/roeez\">@roeez<\/a> thank you for your question!<\/p>\n<p>There are a few discussions about this subject: <a href=\"https:\/\/github.com\/iterative\/dvc\/discussions\/5921\" class=\"inline-onebox\">Reconfigurable pipelines \u00b7 Discussion #5921 \u00b7 iterative\/dvc \u00b7 GitHub<\/a> and <a href=\"https:\/\/github.com\/iterative\/dvc\/discussions\/5922\" class=\"inline-onebox\">Reconfigurable modules \u00b7 Discussion #5922 \u00b7 iterative\/dvc \u00b7 GitHub<\/a>.<\/p>\n<p>It would be really helpful if you can provide more details on why the template is needed? Is it because you have multiple similar projects or it is about a specific use case in a single project? How an ideal API should look like in your dvc.yaml?<\/p>\n<p>I\u2019d appreciate it if you can provide more details.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/pipeline-template\/907",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-06T23:30:53.759Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/roeez\">@roeez<\/a> thank you for your question!<\/p>\n<p>There are a few discussions about this subject: <a href=\"https:\/\/github.com\/iterative\/dvc\/discussions\/5921\" class=\"inline-onebox\">Reconfigurable pipelines \u00b7 Discussion #5921 \u00b7 iterative\/dvc \u00b7 GitHub<\/a> and <a href=\"https:\/\/github.com\/iterative\/dvc\/discussions\/5922\" class=\"inline-onebox\">Reconfigurable modules \u00b7 Discussion #5922 \u00b7 iterative\/dvc \u00b7 GitHub<\/a>.<\/p>\n<p>It would be really helpful if you can provide more details on why the template is needed? Is it because you have multiple similar projects or it is about a specific use case in a single project? How an ideal API should look like in your dvc.yaml?<\/p>\n<p>I\u2019d appreciate it if you can provide more details.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"pipelin templat hei want creat templat pipelin want run pipelin differ paramet time read templat section document look like solut foreach stage possibl run multipl stage foreach block want run specif set param suggest thank",
        "Question_preprocessed_content":"pipelin templat hei want creat templat pipelin want run pipelin differ paramet time read templat section document look like solut stage possibl run multipl stage foreach block want run specif set param suggest thank",
        "Question_gpt_summary_original":"The user is facing challenges in creating a template of a pipeline that can run with different parameters each time. They have read the documentation on templating and found that the only solution is to use a foreach block in each stage. The user is seeking suggestions on how to run multiple stages with one foreach block and how to run a specific set of parameters.",
        "Question_gpt_summary":"user face challeng creat templat pipelin run differ paramet time read document templat solut us foreach block stage user seek suggest run multipl stage foreach block run specif set paramet",
        "Answer_original_content":"roeez thank question discuss subject reconfigur pipelin discuss iter github reconfigur modul discuss iter github help provid detail templat need multipl similar project specif us case singl project ideal api look like yaml appreci provid detail",
        "Answer_preprocessed_content":"thank question discuss subject reconfigur pipelin discuss iter github reconfigur modul discuss iter github help provid detail templat need multipl similar project specif us case singl project ideal api look like yaml appreci provid detail",
        "Answer_gpt_summary_original":"No solutions are provided in the discussion. The user is seeking suggestions on how to run multiple stages with one foreach block and how to run a specific set of parameters.",
        "Answer_gpt_summary":"solut provid discuss user seek suggest run multipl stage foreach block run specif set paramet"
    },
    {
        "Question_title":"Does Sagemaker pass any data other than the model itself between training and prediction steps?",
        "Question_body":"<p>I'm building a Scikit-learn model on Sagemaker.<\/p>\n\n<p>I'd like to reference the data used in training in my <code>predict_fn<\/code>. (Instead of the indices returned from NNS, I'd like to return the names and data of each neighbor.)<\/p>\n\n<p>I know this can be done by writing\/reading from S3, as in <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/<\/a> , but was wondering if there were more elegant solutions.<\/p>\n\n<p>Are there other ways to make the data used in the training job available to the prediction function?<\/p>\n\n<p>Edit: Using the advice from the accepted solution I was able to pass data as a dict.<\/p>\n\n<pre><code>model = nn.fit(train_data)\n\nmodel_dict = {\n   \"model\": model,\n   \"reference\": train_data\n}\n\njoblib.dump(model_dict, path)\n<\/code><\/pre>\n\n<p>predict_fn:<\/p>\n\n<pre><code>def predict_fn(input_data, model_dict):\n   model = model_dict[\"model\"]\n   reference = model_dict[\"reference\"]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1581546836597,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1582837925156,
        "Question_score":0.0,
        "Question_view_count":140.0,
        "Answer_body":"<p>you can bring to the endpoint instance (either in the <code>model.tar.gz<\/code> or via later download) a file storing the mapping between indexes and record names; this way you can translate from neighbor IDs to record names on the fly in the <code>predict_fn<\/code> or in the <code>output_fn<\/code>. For giant indexes this mapping (along with other metadata) can be in an external database too (eg dynamoDB, redis)<\/p>\n\n<p>the link you attach (SageMaker Batch Transform) is quite a different concept; it's for instantiating ephemeral fleet of machine(s) to run a one-time prediction task with input data in S3 and results written to s3. You question seem to refer to the alternative, permanent, real-time endpoint deployment mode.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60197897",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1581723595003,
        "Question_original_content":"pass data model train predict step build scikit learn model like refer data train predict instead indic return nn like return name data neighbor know write read http aw amazon com blog machin learn associ predict result input data amazon batch transform wonder eleg solut wai data train job avail predict function edit advic accept solut abl pass data dict model fit train data model dict model model refer train data joblib dump model dict path predict def predict input data model dict model model dict model refer model dict refer",
        "Question_preprocessed_content":"pass data model train predict step build model like refer data train know wonder eleg solut wai data train job avail predict function edit advic accept solut abl pass data dict",
        "Question_gpt_summary_original":"The user is building a Scikit-learn model on Sagemaker and wants to reference the data used in training in their predict function. They are looking for ways to make the data used in the training job available to the prediction function other than writing\/reading from S3. The user was able to pass data as a dictionary using the advice from the accepted solution.",
        "Question_gpt_summary":"user build scikit learn model want refer data train predict function look wai data train job avail predict function write read user abl pass data dictionari advic accept solut",
        "Answer_original_content":"bring endpoint instanc model tar later download file store map index record name wai translat neighbor id record name fly predict output giant index map metadata extern databas dynamodb redi link attach batch transform differ concept instanti ephemer fleet machin run time predict task input data result written question refer altern perman real time endpoint deploy mode",
        "Answer_preprocessed_content":"bring endpoint instanc file store map index record name wai translat neighbor id record name fly giant index map extern databas link attach differ concept instanti ephemer fleet machin run predict task input data result written question refer altern perman endpoint deploy mode",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to bring a file storing the mapping between indexes and record names to the endpoint instance, either in the model.tar.gz or via later download. This way, the user can translate from neighbor IDs to record names on the fly in the predict_fn or in the output_fn. For large indexes, this mapping (along with other metadata) can be stored in an external database such as DynamoDB or Redis. However, no solution was provided for making the data used in the training job available to the prediction function other than writing\/reading from S3.",
        "Answer_gpt_summary":"solut possibl solut mention discuss bring file store map index record name endpoint instanc model tar later download wai user translat neighbor id record name fly predict output larg index map metadata store extern databas dynamodb redi solut provid make data train job avail predict function write read"
    },
    {
        "Question_title":"Sage Maker Studio CPU Usage",
        "Question_body":"<p>I'm working in sage maker studio, and I have a single instance running one computationally intensive task:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/IntzJ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IntzJ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It appears that the kernel running my task is maxed out, but the actual instance is only using a small amount of its resources. Is there some sort of throttling occurring? Can I configure this so that more of the instance is utilized?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1627529779217,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":946.0,
        "Answer_body":"<p>Your ml.c5.xlarge instance comes with 4 vCPU. However, Python only uses a single CPU by default. (Source: <a href=\"https:\/\/stackoverflow.com\/questions\/64121703\/can-i-apply-multithreading-for-computationally-intensive-task-in-python\">Can I apply multithreading for computationally intensive task in python?<\/a>)<\/p>\n<p>As a result, the overall CPU utilization of your ml.c5.xlarge instance is low. To utilize all the vCPUs, you can try multiprocessing.<\/p>\n<p>The examples below are performed using a 2 vCPU + 4 GiB instance.<\/p>\n<p>In the first picture, multiprocessing is not set up. The instance CPU utilization peaks at around 50%.<\/p>\n<p>single processing:<br \/>\n<img src=\"https:\/\/i.stack.imgur.com\/lcn8K.png\" alt=\"single processing\" \/><\/p>\n<p>In the second picture, I created 50 processes to be run simultaneously. The instance CPU utilization rises to 100% immediately.<\/p>\n<p>multiprocessing:<br \/>\n<img src=\"https:\/\/i.stack.imgur.com\/tk65n.png\" alt=\"multiprocessing\" \/><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1629791630296,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68569742",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1629552761387,
        "Question_original_content":"sage maker studio cpu usag work sage maker studio singl instanc run computation intens task appear kernel run task max actual instanc small resourc sort throttl occur configur instanc util",
        "Question_preprocessed_content":"sage maker studio cpu usag work sage maker studio singl instanc run computation intens task appear kernel run task max actual instanc small resourc sort throttl occur configur instanc util",
        "Question_gpt_summary_original":"The user is facing a challenge with Sage Maker Studio where the kernel running a computationally intensive task is maxed out, but the instance is only using a small amount of its resources. The user is questioning if there is throttling occurring and if they can configure the instance to utilize more of its resources.",
        "Question_gpt_summary":"user face challeng sage maker studio kernel run computation intens task max instanc small resourc user question throttl occur configur instanc util resourc",
        "Answer_original_content":"xlarg instanc come vcpu python us singl cpu default sourc appli multithread computation intens task python result overal cpu util xlarg instanc low util vcpu try multiprocess exampl perform vcpu gib instanc pictur multiprocess set instanc cpu util peak singl process second pictur creat process run simultan instanc cpu util rise immedi multiprocess",
        "Answer_preprocessed_content":"instanc come vcpu python us singl cpu default result overal cpu util instanc low util vcpu try multiprocess exampl perform vcpu gib instanc pictur multiprocess set instanc cpu util peak singl process second pictur creat process run simultan instanc cpu util rise immedi multiprocess",
        "Answer_gpt_summary_original":"Solution: The user can try using multiprocessing to utilize all the vCPUs of the instance. By default, Python only uses a single CPU, resulting in low overall CPU utilization. The user can create multiple processes to be run simultaneously, which can increase the instance CPU utilization to 100%.",
        "Answer_gpt_summary":"solut user try multiprocess util vcpu instanc default python us singl cpu result low overal cpu util user creat multipl process run simultan increas instanc cpu util"
    },
    {
        "Question_title":"MLflow 0.8.0 released!",
        "Question_body":"MLflow 0.8.0 has been released: https:\/\/github.com\/mlflow\/mlflow\/releases\/tag\/v0.8.0\n\n\nMLflow 0.8.0 introduces several major features:\n\n\n- Dramatically improved UI for comparing experiment run results (grouping columns, showing nested runs, using run names instead of ids, and persisting table state)\n- Support for deploying models as Docker containers directly to Azure Machine Learning Service Workspace (as opposed to the previously-recommended solution of Azure ML Workbench)\n\n\nIn addition to these features, there are a host of improvements and bugfixes to the REST API, R API, Python API, tracking UI, and documentation.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1542041230000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":17.0,
        "Answer_body":"Congrats to the MLflow team!\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CAGnzRon1A-v-2LeutZzFK%2ByJUvdqhZwyAEie6142ugMBt%2BmD3A%40mail.gmail.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/TgR6lSe4gfA",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-11-12T17:45:38",
                "Answer_body":"Congrats to the MLflow team!\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CAGnzRon1A-v-2LeutZzFK%2ByJUvdqhZwyAEie6142ugMBt%2BmD3A%40mail.gmail.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"releas releas http github com releas tag introduc major featur dramat improv compar experi run result group column show nest run run name instead id persist tabl state support deploi model docker contain directli servic workspac oppos previous recommend solut workbench addit featur host improv bugfix rest api api python api track document",
        "Question_preprocessed_content":"releas releas introduc major featur dramat improv compar experi run result support deploi model docker contain directli servic workspac addit featur host improv bugfix rest api api python api track document",
        "Question_gpt_summary_original":"The user may face challenges in using MLflow 0.8.0 due to the introduction of major features such as an improved UI for comparing experiment run results and support for deploying models as Docker containers directly to Azure Machine Learning Service Workspace. However, there are also improvements and bugfixes to the REST API, R API, Python API, tracking UI, and documentation.",
        "Question_gpt_summary":"user face challeng introduct major featur improv compar experi run result support deploi model docker contain directli servic workspac improv bugfix rest api api python api track document",
        "Answer_original_content":"congrat team receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user cagnzrona leutzzfk byjuvdqhzwyaeieugmbt bmda mail gmail com option visit http group googl com optout",
        "Answer_preprocessed_content":"congrat team receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit",
        "Answer_gpt_summary_original":"No solutions are mentioned in the discussion.",
        "Answer_gpt_summary":"solut mention discuss"
    },
    {
        "Question_title":"Tracking resources used by VertexAI pipeline",
        "Question_body":"<p>Is it possible to track the resources consumed by a VertexAI pipeline run, similar to how it is possible to do for Dataflow where it shows a live graph of how many nodes are currently running to execute the pipeline?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1628219232657,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":272.0,
        "Answer_body":"<p>Vertex AI Pipeline provides a feature for <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/visualize-pipeline\" rel=\"nofollow noreferrer\">Visualizing and analyzing<\/a> the pipeline results.<\/p>\n<p>This feature can be used to check the resource utilization once the Pipeline is deployed.<\/p>\n<p><strong>steps:<\/strong><\/p>\n<pre><code>Go to vertex AI pipeline-&gt;\n         Select a pipeline-&gt;\n               pipeline step-&gt;\n                     view job(from Pipeline run analysis pane)\n<\/code><\/pre>\n<p>In the View Job pane we can check for the resources utilized i.e machine types,machine count,CPU utilization graph for the pipeline step and we can view the logs too.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BTMXZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BTMXZ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Utilizations:<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/8WQRc.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8WQRc.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As per this <a href=\"https:\/\/cloud.google.com\/monitoring\/api\/metrics_gcp#gcp-aiplatform\" rel=\"nofollow noreferrer\">document<\/a>, metrics from the Vertex AI like CPU utilization, CPU load are in the <a href=\"https:\/\/cloud.google.com\/products\/#product-launch-stages\" rel=\"nofollow noreferrer\">Beta<\/a> launch stage. However, you can examine the metrics like CPU utilization from Cloud Monitoring by referring to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/monitoring-metrics\" rel=\"nofollow noreferrer\">document<\/a> and also find the below snap for more reference.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Yyvnd.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yyvnd.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For changing the timeline of the graph you have to select the <strong>custom<\/strong> option in <strong>metrics explorer<\/strong> and provide the date and time for the duration that you want to view as shown in the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6ZUgE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6ZUgE.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1628692036932,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68675615",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1628260970907,
        "Question_original_content":"track resourc vertexai pipelin possibl track resourc consum vertexai pipelin run similar possibl dataflow show live graph node current run execut pipelin",
        "Question_preprocessed_content":"track resourc vertexai pipelin possibl track resourc consum vertexai pipelin run similar possibl dataflow show live graph node current run execut pipelin",
        "Question_gpt_summary_original":"The user is facing a challenge in tracking the resources used by a VertexAI pipeline run and is seeking a solution similar to the live graph feature available in Dataflow.",
        "Question_gpt_summary":"user face challeng track resourc vertexai pipelin run seek solut similar live graph featur avail dataflow",
        "Answer_original_content":"pipelin provid featur visual analyz pipelin result featur check resourc util pipelin deploi step pipelin select pipelin pipelin step view job pipelin run analysi pane view job pane check resourc util machin type machin count cpu util graph pipelin step view log util document metric like cpu util cpu load beta launch stage examin metric like cpu util cloud monitor refer document snap refer chang timelin graph select custom option metric explor provid date time durat want view shown screenshot",
        "Answer_preprocessed_content":"pipelin provid featur visual analyz pipelin result featur check resourc util pipelin deploi step view job pane check resourc util machin type machin count cpu util graph pipelin step view log util document metric like cpu util cpu load beta launch stage examin metric like cpu util cloud monitor refer document snap refer chang timelin graph select custom option metric explor provid date time durat want view shown screenshot",
        "Answer_gpt_summary_original":"Solution:\n- Vertex AI Pipeline provides a feature for visualizing and analyzing the pipeline results, which can be used to check the resource utilization once the Pipeline is deployed.\n- In the View Job pane, users can check for the resources utilized, such as machine types, machine count, and CPU utilization graph for the pipeline step, and view the logs too.\n- Metrics from the Vertex AI like CPU utilization, CPU load are in the Beta launch stage, but users can examine the metrics like CPU utilization from Cloud Monitoring by referring to the provided document.\n- Users can change the timeline of the graph by selecting the custom option in metrics explorer and providing the date and time for the duration that they want to view.",
        "Answer_gpt_summary":"solut pipelin provid featur visual analyz pipelin result check resourc util pipelin deploi view job pane user check resourc util machin type machin count cpu util graph pipelin step view log metric like cpu util cpu load beta launch stage user examin metric like cpu util cloud monitor refer provid document user chang timelin graph select custom option metric explor provid date time durat want view"
    },
    {
        "Question_title":"Tensorflow error. TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn",
        "Question_body":"<p>I am trying to run this on Amazon Sagemaker but I am getting this error while when I try to run it on my local machine, it works very fine.<\/p>\n<p>this is my code:<\/p>\n<pre><code>import tensorflow as tf\n\nimport IPython.display as display\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize'] = (12,12)\nmpl.rcParams['axes.grid'] = False\n\nimport numpy as np\nimport PIL.Image\nimport time\nimport functools\n    \ndef tensor_to_image(tensor):\n  tensor = tensor*255\n  tensor = np.array(tensor, dtype=np.uint8)\n  if np.ndim(tensor)&gt;3:\n    assert tensor.shape[0] == 1\n    tensor = tensor[0]\n  return PIL.Image.fromarray(tensor)\n\ncontent_path = tf.keras.utils.get_file('YellowLabradorLooking_nw4.jpg', 'https:\/\/example.com\/IMG_20200216_163015.jpg')\n\n\nstyle_path = tf.keras.utils.get_file('kandinsky3.jpg','https:\/\/example.com\/download+(2).png')\n\n\ndef load_img(path_to_img):\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img\n\n\ndef imshow(image, title=None):\n  if len(image.shape) &gt; 3:\n    image = tf.squeeze(image, axis=0)\n\n  plt.imshow(image)\n  if title:\n    plt.title(title)\n\n\ncontent_image = load_img(content_path)\nstyle_image = load_img(style_path)\n\nplt.subplot(1, 2, 1)\nimshow(content_image, 'Content Image')\n\nplt.subplot(1, 2, 2)\nimshow(style_image, 'Style Image')\n\nimport tensorflow_hub as hub\nhub_module = hub.load('https:\/\/tfhub.dev\/google\/magenta\/arbitrary-image-stylization-v1-256\/1')\nstylized_image = hub_module(tf.constant(content_image), tf.constant(style_image))[0]\ntensor_to_image(stylized_image)\n\n\nfile_name = 'stylized-image5.png'\ntensor_to_image(stylized_image).save(file_name)\n<\/code><\/pre>\n<p>This is the exact error I get:<\/p>\n<pre><code>---------------------------------------------------------------------------\n<\/code><\/pre>\n<p>TypeError                                 Traceback (most recent call last)<\/p>\n<pre><code>&lt;ipython-input-24-c47a4db4880c&gt; in &lt;module&gt;()\n     53 \n     54 \n---&gt; 55 content_image = load_img(content_path)\n     56 style_image = load_img(style_path)\n     57 \n<\/code><\/pre>\n<p> in load_img(path_to_img)<\/p>\n<pre><code>     34 \n     35     shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n---&gt; 36     long_dim = max(shape)\n     37     scale = max_dim \/ long_dim\n     38 \n<\/code><\/pre>\n<p>~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/tensorflow\/python\/framework\/ops.py in <strong>iter<\/strong>(self)<\/p>\n<pre><code>    475     if not context.executing_eagerly():\n    476       raise TypeError(\n--&gt; 477           &quot;Tensor objects are only iterable when eager execution is &quot;\n    478           &quot;enabled. To iterate over this tensor use tf.map_fn.&quot;)\n    479     shape = self._shape_tuple()\n<\/code><\/pre>\n<p>TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1594076057097,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1530.0,
        "Answer_body":"<p>Your error is being raised in this function <code>load_img<\/code>:<\/p>\n<pre><code>def load_img(path_to_img):\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img\n<\/code><\/pre>\n<p>Specifically, this line:<\/p>\n<pre><code>    long_dim = max(shape)\n<\/code><\/pre>\n<p>You are passing a tensor to the <a href=\"https:\/\/docs.python.org\/3\/library\/functions.html#max\" rel=\"nofollow noreferrer\">built-in Python max function<\/a> in graph execution mode. You can only iterate through tensors in eager-execution mode. You probably want to use <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/math\/reduce_max\" rel=\"nofollow noreferrer\">tf.reduce_max<\/a> instead:<\/p>\n<pre><code>    long_dim = tf.reduce_max(shape)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1594159994640,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62765658",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1594129639003,
        "Question_original_content":"tensorflow error typeerror tensor object iter eager execut enabl iter tensor us map try run get error try run local machin work fine code import tensorflow import ipython displai displai import matplotlib pyplot plt import matplotlib mpl mpl rcparam figur figsiz mpl rcparam ax grid fals import numpi import pil imag import time import functool def tensor imag tensor tensor tensor tensor arrai tensor dtype uint ndim tensor assert tensor shape tensor tensor return pil imag fromarrai tensor content path kera util file yellowlabradorlook jpg http exampl com img jpg style path kera util file kandinski jpg http exampl com download png def load img path img max dim img read file path img img imag decod imag img channel img imag convert imag dtype img float shape cast shape img float long dim max shape scale max dim long dim new shape cast shape scale int img imag resiz img new shape img img newaxi return img def imshow imag titl len imag shape imag squeez imag axi plt imshow imag titl plt titl titl content imag load img content path style imag load img style path plt subplot imshow content imag content imag plt subplot imshow style imag style imag import tensorflow hub hub hub modul hub load http tfhub dev googl magenta arbitrari imag styliz styliz imag hub modul constant content imag constant style imag tensor imag styliz imag file styliz imag png tensor imag styliz imag save file exact error typeerror traceback recent content imag load img content path style imag load img style path load img path img shape cast shape img float long dim max shape scale max dim long dim anaconda env amazonei tensorflow lib python site packag tensorflow python framework op iter self context execut eagerli rais typeerror tensor object iter eager execut enabl iter tensor us map shape self shape tupl typeerror tensor object iter eager execut enabl iter tensor us map",
        "Question_preprocessed_content":"tensorflow error typeerror tensor object iter eager execut enabl iter tensor us try run get error try run local machin work fine code exact error typeerror traceback iter typeerror tensor object iter eager execut enabl iter tensor us",
        "Question_gpt_summary_original":"The user is encountering a Tensorflow error while trying to run a code on Amazon Sagemaker. The error message states that Tensor objects are only iterable when eager execution is enabled and suggests using tf.map_fn to iterate over the tensor. The code works fine on the user's local machine.",
        "Question_gpt_summary":"user encount tensorflow error try run code error messag state tensor object iter eager execut enabl suggest map iter tensor code work fine user local machin",
        "Answer_original_content":"error rais function load img def load img path img max dim img read file path img img imag decod imag img channel img imag convert imag dtype img float shape cast shape img float long dim max shape scale max dim long dim new shape cast shape scale int img imag resiz img new shape img img newaxi return img specif line long dim max shape pass tensor built python max function graph execut mode iter tensor eager execut mode probabl want us reduc max instead long dim reduc max shape",
        "Answer_preprocessed_content":"error rais function specif line pass tensor python max function graph execut mode iter tensor mode probabl want us instead",
        "Answer_gpt_summary_original":"Solution:\n- Replace the line `long_dim = max(shape)` with `long_dim = tf.reduce_max(shape)` to avoid passing a tensor to the built-in Python max function in graph execution mode.",
        "Answer_gpt_summary":"solut replac line long dim max shape long dim reduc max shape avoid pass tensor built python max function graph execut mode"
    },
    {
        "Question_title":"How to execute python from conda environment by dvc run",
        "Question_body":"<p>I have an environment of conda configurated with python 3.6 and dvc is installed there, but when I try to execute dvc run with python, dvc call the python version of main installation of conda and not find the installed libraries.<\/p>\n\n<pre><code>$ conda activate py36\n$ python --version\nPython 3.6.6 :: Anaconda custom (64-bit)\n$ dvc run python --version\nRunning command:\n    python --version\nPython 3.7.0\nSaving information to 'Dvcfile'.\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1541699972677,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":6.0,
        "Question_view_count":351.0,
        "Answer_body":"<p>The version 0.24.3 of dvc correct this problem.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53213596",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1549414531500,
        "Question_original_content":"execut python conda environ run environ conda configur python instal try execut run python python version main instal conda instal librari conda activ python version python anaconda custom bit run python version run command python version python save inform file",
        "Question_preprocessed_content":"execut python conda environ run environ conda configur python instal try execut run python python version main instal conda instal librari",
        "Question_gpt_summary_original":"The user is facing challenges in executing python from a conda environment using dvc run. When attempting to execute dvc run with python, dvc calls the python version of the main installation of conda and is unable to find the installed libraries.",
        "Question_gpt_summary":"user face challeng execut python conda environ run attempt execut run python call python version main instal conda unabl instal librari",
        "Answer_original_content":"version correct problem",
        "Answer_preprocessed_content":"version correct problem",
        "Answer_gpt_summary_original":"Solution: The solution mentioned in the discussion is to upgrade to version 0.24.3 of dvc, which resolves the issue of dvc calling the wrong version of python from the conda environment.",
        "Answer_gpt_summary":"solut solut mention discuss upgrad version resolv issu call wrong version python conda environ"
    },
    {
        "Question_title":"How do I pull the pre-built docker images for SageMaker?",
        "Question_body":"<p>I'm trying to pull the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pre-built-docker-containers-frameworks.html\" rel=\"noreferrer\">pre-built docker images<\/a> for SageMaker. I am able to successfully <code>docker login<\/code> to ECR (my AWS credentials). When I try to pull the image I get the standard <code>no basic auth credentials<\/code>.<\/p>\n\n<p>Maybe I'm misunderstanding... I assumed those ECR URLs were public.<\/p>\n\n<pre><code>$(aws ecr get-login --region us-west-2 --no-include-email)\n\ndocker pull 246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-scikit-learn\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1557016435310,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1557175542127,
        "Question_score":5.0,
        "Question_view_count":2738.0,
        "Answer_body":"<p>Could you show your ECR login command and pull command in the question?<\/p>\n\n<p>For SageMaker pre-built image 520713654638.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.3.0-cpu-py3<\/p>\n\n<p>What I do is:<\/p>\n\n<ol>\n<li>Log in ECR<\/li>\n<\/ol>\n\n<p><code>$(aws ecr get-login --no-include-email --registry-ids 520713654638 --region us-west-2)<\/code><\/p>\n\n<ol start=\"2\">\n<li>Pull the image<\/li>\n<\/ol>\n\n<p><code>docker pull 520713654638.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.3.0-cpu-py3<\/code><\/p>\n\n<p>These images are public readable so you can pull them from any AWS account. I guess the reason you failed is that you did not specify --registry-ids in your login. But it's better if you can provide your scripts for others to identify what's wrong.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55987935",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1557174090812,
        "Question_original_content":"pull pre built docker imag try pull pre built docker imag abl successfulli docker login ecr aw credenti try pull imag standard basic auth credenti mayb misunderstand assum ecr url public aw ecr login region west includ email docker pull dkr ecr west amazonaw com scikit learn",
        "Question_preprocessed_content":"pull docker imag try pull docker imag abl successfulli ecr try pull imag standard mayb assum ecr url public",
        "Question_gpt_summary_original":"The user is facing challenges in pulling pre-built docker images for SageMaker. Although they are able to successfully login to ECR using their AWS credentials, they are unable to pull the image and receive a \"no basic auth credentials\" error. The user is unsure if the ECR URLs are public and is seeking assistance.",
        "Question_gpt_summary":"user face challeng pull pre built docker imag abl successfulli login ecr aw credenti unabl pull imag receiv basic auth credenti error user unsur ecr url public seek assist",
        "Answer_original_content":"ecr login command pull command question pre built imag dkr ecr west amazonaw com mxnet cpu log ecr aw ecr login includ email registri id region west pull imag docker pull dkr ecr west amazonaw com mxnet cpu imag public readabl pull aw account guess reason fail specifi registri id login better provid script identifi wrong",
        "Answer_preprocessed_content":"ecr login command pull command question imag log ecr pull imag imag public readabl pull aw account guess reason fail specifi login better provid script identifi wrong",
        "Answer_gpt_summary_original":"Solution: The solution provided is to ensure that the user specifies the --registry-ids parameter in their ECR login command. Additionally, the pre-built docker images for SageMaker are public readable and can be pulled from any AWS account.",
        "Answer_gpt_summary":"solut solut provid ensur user specifi registri id paramet ecr login command addition pre built docker imag public readabl pull aw account"
    },
    {
        "Question_title":"How to make parameters available to SageMaker Tensorflow Endpoint",
        "Question_body":"<p>I'm looking to make some hyper parameters available to the serving endpoint in SageMaker. The training instances is given access to input parameters using hyperparameters in:<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='autocat.py',\n                       role=role,\n                       output_path=params['output_path'],\n                       code_location=params['code_location'],\n                       train_instance_count=1,\n                       train_instance_type='ml.c4.xlarge',\n                       training_steps=10000,\n                       evaluation_steps=None,\n                       hyperparameters=params)\n<\/code><\/pre>\n\n<p>However, when the endpoint is deployed, there is no way to pass in parameters that are used to control the data processing in the <code>input_fn(serialized_input, content_type)<\/code> function.<\/p>\n\n<p>What would be the best way to pass parameters to the serving instance?? Is the <code>source_dir<\/code> parameter defined in the <code>sagemaker.tensorflow.TensorFlow<\/code> class copied to the serving instance? If so, I could use a config.yml or similar.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1521754623920,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1521754933880,
        "Question_score":1.0,
        "Question_view_count":1426.0,
        "Answer_body":"<p>Ah i have had a similar problem to you where I needed to download something off S3 to use in the input_fn for inference. In my case it was a dictionary.<\/p>\n\n<p>Three options:<\/p>\n\n<ol>\n<li>use your config.yml approach, and download and import the s3 file from within your entrypoint file before any function declarations. This would make it available to the input_fn <\/li>\n<li>Keep using the hyperparameter approach, download and import the vectorizer in <code>serving_input_fn<\/code> and make it available via a global variable so that <code>input_fn<\/code> has access to it.<\/li>\n<li>Download the file from s3 before training and include it in the source_dir directly.<\/li>\n<\/ol>\n\n<p>Option 3 would only work if you didnt need to make changes to the vectorizer seperately after initial training.<\/p>\n\n<p>Whatever you do, don't download the file directly in input_fn. I made that mistake and the performance is terrible as each invoking of the endpoint would result in the s3 file being downloaded.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49438903",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1523591814356,
        "Question_original_content":"paramet avail tensorflow endpoint look hyper paramet avail serv endpoint train instanc given access input paramet hyperparamet estim tensorflow entri point autocat role role output path param output path code locat param code locat train instanc count train instanc type xlarg train step evalu step hyperparamet param endpoint deploi wai pass paramet control data process input serial input content type function best wai pass paramet serv instanc sourc dir paramet defin tensorflow tensorflow class copi serv instanc us config yml similar",
        "Question_preprocessed_content":"paramet avail tensorflow endpoint look hyper paramet avail serv endpoint train instanc given access input paramet hyperparamet endpoint deploi wai pass paramet control data process function best wai pass paramet serv instanc paramet defin class copi serv instanc us similar",
        "Question_gpt_summary_original":"The user is facing a challenge in making hyperparameters available to the serving endpoint in SageMaker Tensorflow. While the training instances have access to input parameters using hyperparameters, there is no way to pass in parameters to control data processing in the input function when the endpoint is deployed. The user is seeking advice on the best way to pass parameters to the serving instance.",
        "Question_gpt_summary":"user face challeng make hyperparamet avail serv endpoint tensorflow train instanc access input paramet hyperparamet wai pass paramet control data process input function endpoint deploi user seek advic best wai pass paramet serv instanc",
        "Answer_original_content":"similar problem need download us input infer case dictionari option us config yml approach download import file entrypoint file function declar avail input hyperparamet approach download import vector serv input avail global variabl input access download file train includ sourc dir directli option work didnt need chang vector seper initi train download file directli input mistak perform terribl invok endpoint result file download",
        "Answer_preprocessed_content":"similar problem need download us infer case dictionari option us approach download import file entrypoint file function declar avail hyperparamet approach download import vector avail global variabl access download file train includ directli option work didnt need chang vector seper initi train download file directli mistak perform terribl invok endpoint result file download",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Use the config.yml approach, download and import the S3 file from within the entrypoint file before any function declarations. This would make it available to the input_fn.\n2. Keep using the hyperparameter approach, download and import the vectorizer in serving_input_fn and make it available via a global variable so that input_fn has access to it.\n3. Download the file from S3 before training and include it in the source_dir directly.\n\nIt is also mentioned that option 3 would only work if the vectorizer does not need to be changed separately after initial training. Additionally, it is advised not to download the file directly in input_fn as it would result in poor performance.",
        "Answer_gpt_summary":"possibl solut mention discuss us config yml approach download import file entrypoint file function declar avail input hyperparamet approach download import vector serv input avail global variabl input access download file train includ sourc dir directli mention option work vector need chang separ initi train addition advis download file directli input result poor perform"
    },
    {
        "Question_title":"Best practice for queuing experiments on code changes",
        "Question_body":"<p>Dear community,<\/p>\n<p>Thank you very much for DVC and the new features from the version 2!<\/p>\n<p><strong>What would be your recommendation on how to queue experiments which depend on code changes?<\/strong><\/p>\n<p>The use case is to queue as individual experiment each change of the content of a Python file declared as a <code>deps<\/code> of a DVC stage.<\/p>\n<p>For example:<\/p>\n<ol>\n<li>one is on a branch,<\/li>\n<li>makes a change on a Python file,<\/li>\n<li>do <code>dvc exp run --queue<\/code> for a DVC stage depending on this Python file,<\/li>\n<li>then do another change on the same Python file,<\/li>\n<li>then do <code>dvc exp run --queue<\/code> for the same DVC stage,<\/li>\n<li>and then do <code>dvc exp run --run-all<\/code>.<\/li>\n<\/ol>\n<p>It seems that committing each change is an anti-pattern and not very usable.<\/p>\n<p>But if the change are not committed, <code>git<\/code> shows the changed file hanging around as modified and the changes corresponding to an experiment seem then not tracked.<\/p>\n<p>The idea would be to replicate what is below but when the changes are not on a <code>params.yaml<\/code> file:<\/p>\n<aside class=\"onebox allowlistedgeneric\">\n  <header class=\"source\">\n      <img src=\"https:\/\/dvc.org\/favicon.ico\" class=\"site-icon\" width=\"64\" height=\"64\">\n      <a href=\"https:\/\/dvc.org\/doc\/start\/experiments#queueing-experiments\" target=\"_blank\" rel=\"noopener nofollow ugc\">dvc.org<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/362;\"><img src=\"https:\/\/dvc.org\/social-share.png\" class=\"thumbnail\" width=\"690\" height=\"362\"><\/div>\n\n<h3><a href=\"https:\/\/dvc.org\/doc\/start\/experiments#queueing-experiments\" target=\"_blank\" rel=\"noopener nofollow ugc\">Get Started: Experiments<\/a><\/h3>\n\n<p>Open-source version control system for Data Science and Machine Learning projects. Git-like experience to organize your data, models, and experiments.<\/p>\n\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>Thank you.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1617283616396,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":437.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/pafonta\">@pafonta<\/a>, <code>dvc exp run<\/code> (with or without <code>--queue<\/code>) should work out of the box to generate experiments with your code changes. The feature is designed to work with any changes in your DVC\/git repo, it is not specific to parameters (experimenting with modified parameters is just the common use case).<\/p>\n<blockquote>\n<p>But if the change are not committed, <code>git<\/code> shows the changed file hanging around as modified and the changes corresponding to an experiment seem then not tracked.<\/p>\n<\/blockquote>\n<p>Once you have run the experiments, you don\u2019t need to keep the code changes in your workspace (and you do not need to commit them anywhere yourself). They will still be tracked as a part of the DVC experiment.<\/p>\n<p>If\/when you decide you want to keep one of your experiments, you can use <code>dvc exp apply<\/code> to re-apply the code changes back to your workspace.<\/p>\n<p>The code changes for an experiment can also be retrieved directly in git - under the hood, experiments are essentially custom git branches. If you use <code>dvc exp show --sha<\/code>, you can see the git SHA for your experiments, and that SHA can be used in commands like <code>git diff<\/code>.<\/p>. <p>Hi <a class=\"mention\" href=\"\/u\/pmrowla\">@pmrowla<\/a>,<\/p>\n<p>Thank you very much for your prompt and detailed reply!<\/p>\n<aside class=\"quote no-group\" data-username=\"pmrowla\" data-post=\"2\" data-topic=\"710\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/p\/7ab992\/40.png\" class=\"avatar\"> pmrowla:<\/div>\n<blockquote>\n<p>If you use <code>dvc exp show --sha<\/code> , you can see the git SHA for your experiments, and that SHA can be used in commands like <code>git diff<\/code> .<\/p>\n<\/blockquote>\n<\/aside>\n<p>This helps me and solves my issue.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practice-for-queuing-experiments-on-code-changes\/710",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-01T15:23:42.295Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/pafonta\">@pafonta<\/a>, <code>dvc exp run<\/code> (with or without <code>--queue<\/code>) should work out of the box to generate experiments with your code changes. The feature is designed to work with any changes in your DVC\/git repo, it is not specific to parameters (experimenting with modified parameters is just the common use case).<\/p>\n<blockquote>\n<p>But if the change are not committed, <code>git<\/code> shows the changed file hanging around as modified and the changes corresponding to an experiment seem then not tracked.<\/p>\n<\/blockquote>\n<p>Once you have run the experiments, you don\u2019t need to keep the code changes in your workspace (and you do not need to commit them anywhere yourself). They will still be tracked as a part of the DVC experiment.<\/p>\n<p>If\/when you decide you want to keep one of your experiments, you can use <code>dvc exp apply<\/code> to re-apply the code changes back to your workspace.<\/p>\n<p>The code changes for an experiment can also be retrieved directly in git - under the hood, experiments are essentially custom git branches. If you use <code>dvc exp show --sha<\/code>, you can see the git SHA for your experiments, and that SHA can be used in commands like <code>git diff<\/code>.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-06T09:26:49.060Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/pmrowla\">@pmrowla<\/a>,<\/p>\n<p>Thank you very much for your prompt and detailed reply!<\/p>\n<aside class=\"quote no-group\" data-username=\"pmrowla\" data-post=\"2\" data-topic=\"710\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/p\/7ab992\/40.png\" class=\"avatar\"> pmrowla:<\/div>\n<blockquote>\n<p>If you use <code>dvc exp show --sha<\/code> , you can see the git SHA for your experiments, and that SHA can be used in commands like <code>git diff<\/code> .<\/p>\n<\/blockquote>\n<\/aside>\n<p>This helps me and solves my issue.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"best practic queu experi code chang dear commun thank new featur version recommend queue experi depend code chang us case queue individu experi chang content python file declar dep stage exampl branch make chang python file exp run queue stage depend python file chang python file exp run queue stage exp run run commit chang anti pattern usabl chang commit git show chang file hang modifi chang correspond experi track idea replic chang param yaml file org start experi open sourc version control data scienc machin learn project git like experi organ data model experi thank",
        "Question_preprocessed_content":"best practic queu experi code chang dear commun thank new featur version recommend queue experi depend code chang us case queue individu experi chang content python file declar stage exampl branch make chang python file stage depend python file chang python file stage commit chang usabl chang commit show chang file hang modifi chang correspond experi track idea replic chang file org start experi version control data scienc machin learn project experi organ data model experi thank",
        "Question_gpt_summary_original":"The user is seeking recommendations on how to queue experiments that depend on code changes without committing each change, as committing each change is not very usable. The user wants to queue individual experiments for each change of the content of a Python file declared as a dependency of a DVC stage. However, if the changes are not committed, Git shows the changed file hanging around as modified, and the changes corresponding to an experiment seem then not tracked.",
        "Question_gpt_summary":"user seek recommend queue experi depend code chang commit chang commit chang usabl user want queue individu experi chang content python file declar depend stage chang commit git show chang file hang modifi chang correspond experi track",
        "Answer_original_content":"pafonta exp run queue work box gener experi code chang featur design work chang git repo specif paramet experi modifi paramet common us case chang commit git show chang file hang modifi chang correspond experi track run experi dont need code chang workspac need commit track experi decid want experi us exp appli appli code chang workspac code chang experi retriev directli git hood experi essenti custom git branch us exp sha git sha experi sha command like git diff pmrowla thank prompt detail repli pmrowla us exp sha git sha experi sha command like git diff help solv issu",
        "Answer_preprocessed_content":"work box gener experi code chang featur design work chang repo specif paramet chang commit show chang file hang modifi chang correspond experi track run experi dont need code chang workspac track experi decid want experi us code chang workspac code chang experi retriev directli git hood experi essenti custom git branch us git sha experi sha command like thank prompt detail repli pmrowla us git sha experi sha command like help solv issu",
        "Answer_gpt_summary_original":"Solution:\n- Use the `dvc exp run` command to generate experiments with code changes.\n- Once the experiments are run, the code changes do not need to be kept in the workspace or committed anywhere.\n- The code changes for an experiment can be retrieved directly in Git using the `dvc exp show --sha` command, which shows the Git SHA for the experiment and can be used in commands like `git diff`.",
        "Answer_gpt_summary":"solut us exp run command gener experi code chang experi run code chang need kept workspac commit code chang experi retriev directli git exp sha command show git sha experi command like git diff"
    },
    {
        "Question_title":"Data Version Control: Absolute Paths and Project Paths in the Pipeline Parameters?",
        "Question_body":"<p>In DVC one may define pipelines.  In Unix, one typically does not work at the root level.  Further, DVC expects files to be inside the git repository.<\/p>\n<p>So, this seems like a typical problem.<\/p>\n<p>Suppose I have the following:<\/p>\n<pre><code>\/home\/user\/project\/content-folder\/data\/data-type\/cfg.json\n\/home\/user\/project\/content-folder\/app\/foo.py\n<\/code><\/pre>\n<p>Git starts at <code>\/home\/user\/project\/<\/code><\/p>\n<pre><code>cd ~\/project\/content-folder\/data\/data-type\n..\/..\/app\/foo.py do-this --with cfg.json --dest $(pwd) \n<\/code><\/pre>\n<p>Seems reasonable to me: the script takes a configuration, which is stored in a particular location, runs it against some encapsulated functionality, and outputs it to the destination using an absolute path.<\/p>\n<p>The default behavior of <code>--dest<\/code> is to output to the current working directory.  This seems like another reasonable default.<\/p>\n<hr \/>\n<p>Next, I go to configure the <code>params.yaml<\/code> file for <code>dvc<\/code>, and I am immediately confusing and unsure what is going to happen.  I write:<\/p>\n<pre><code>foodoo:\n  params: do-this --with ????\/cfg.json --dest ????\n<\/code><\/pre>\n<p>What I want to write (and would in a shell script):<\/p>\n<pre><code>#!\/usr\/bin\/env bash\norigin:=$(git rev-parse --show-toplevel)\n\nverb=do-this\nparams=--with $(origin)\/content-folder\/data\/data-type\/cfg.json --dest $(origin)\/content-folder\/data\/data-type\n<\/code><\/pre>\n<hr \/>\n<p>But, in DVC, the pathing seems to be implicit, and I do not know where to start as either:<\/p>\n<ol>\n<li>DVC will calculate the path to my script locally<\/li>\n<li>Not calculate the path to my script locally<\/li>\n<\/ol>\n<p>Which is fine -- I can discover that.  But I am reasonably sure that DVC will absolutely not prefix the directory and file params in my params.yaml with the path to my project.<\/p>\n<hr \/>\n<p>How does one achieve path control that does not assume a fixed project location, like I would in BASH?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1608672730887,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":448.0,
        "Answer_body":"<p>By default, DVC will run your stage command from the same directory as the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files#dvcyaml-file\" rel=\"nofollow noreferrer\">dvc.yaml<\/a> file. If you need to run the command from a different location, you can specify an alternate working directory via <code>wdir<\/code>, which should be a path relative to <code>dvc.yaml<\/code>'s location.<\/p>\n<p>Paths for everything else in your stage (like <code>params.yaml<\/code>) should be specified as relative to <code>wdir<\/code> (or relative to <code>dvc.yaml<\/code> if <code>wdir<\/code> is not provided).<\/p>\n<p>Looking at your example, there also seems to be a bit of confusion on parameters in DVC. In a DVC stage, <code>params<\/code> is for specifying <a href=\"https:\/\/dvc.org\/doc\/command-reference\/params\" rel=\"nofollow noreferrer\">parameter dependencies<\/a>, not used for specifying command-line flags. The full command including flags\/options should be included  the <code>cmd<\/code> section for your stage. If you wanted to make sure that your stage was rerun every time certain values in <code>cfg.json<\/code> have changed, your stage's <code>params<\/code> section would look something like:<\/p>\n<pre><code>params:\n  &lt;relpath from dvc.yaml&gt;\/cfg.json:\n    - param1\n    - param2\n    ...\n<\/code><\/pre>\n<p>So your example <code>dvc.yaml<\/code> would look something like:<\/p>\n<pre><code>stages:\n  foodoo:\n    cmd: &lt;relpath from dvc.yaml&gt;\/foo.py do-this --with &lt;relpath from dvc.yaml&gt;\/cfg.json --dest &lt;relpath from dvc.yaml&gt;\/...\n    deps:\n      &lt;relpath from dvc.yaml&gt;\/foo.py\n    params:\n      &lt;relpath from dvc.yaml&gt;\/cfg.json:\n        ...\n    ...\n<\/code><\/pre>\n<p>This would make the command <code>dvc repro<\/code> rerun your stage any time that the code in foo.py has changed, or the specified parameters in <code>cfg.json<\/code> have changed.<\/p>\n<p>You may also want to refer to the docs for <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run#run\" rel=\"nofollow noreferrer\">dvc run<\/a>, which can be used to generate or update a <code>dvc.yaml<\/code> stage (rather than writing <code>dvc.yaml<\/code> by hand)<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65416056",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1608686869860,
        "Question_original_content":"data version control absolut path project path pipelin paramet defin pipelin unix typic work root level expect file insid git repositori like typic problem suppos follow home user project content folder data data type cfg json home user project content folder app foo git start home user project project content folder data data type app foo cfg json dest pwd reason script take configur store particular locat run encapsul function output destin absolut path default behavior dest output current work directori like reason default configur param yaml file immedi confus unsur go happen write foodoo param cfg json dest want write shell script usr bin env bash origin git rev pars toplevel verb param origin content folder data data type cfg json dest origin content folder data data type path implicit know start calcul path script local calcul path script local fine discov reason sure absolut prefix directori file param param yaml path project achiev path control assum fix project locat like bash",
        "Question_preprocessed_content":"data version control absolut path project path pipelin paramet defin pipelin unix typic work root level expect file insid git repositori like typic problem suppos follow git start reason script take configur store particular locat run encapsul function output destin absolut path default behavior output current work directori like reason default configur file immedi confus unsur go happen write want write path implicit know start calcul path script local calcul path script local fine discov reason sure absolut prefix directori file param path project achiev path control assum fix project locat like bash",
        "Question_gpt_summary_original":"The user is facing challenges with defining pipelines in DVC due to the expectation that files are inside the git repository and the use of implicit pathing. The user is unsure how to configure the params.yaml file to achieve path control that does not assume a fixed project location.",
        "Question_gpt_summary":"user face challeng defin pipelin expect file insid git repositori us implicit path user unsur configur param yaml file achiev path control assum fix project locat",
        "Answer_original_content":"default run stage command directori yaml file need run command differ locat specifi altern work directori wdir path rel yaml locat path stage like param yaml specifi rel wdir rel yaml wdir provid look exampl bit confus paramet stage param specifi paramet depend specifi command line flag command includ flag option includ cmd section stage want sure stage rerun time certain valu cfg json chang stage param section look like param cfg json param param exampl yaml look like stage foodoo cmd foo cfg json dest dep foo param cfg json command repro rerun stage time code foo chang specifi paramet cfg json chang want refer doc run gener updat yaml stage write yaml hand",
        "Answer_preprocessed_content":"default run stage command directori yaml file need run command differ locat specifi altern work directori path rel locat path stage specifi rel look exampl bit confus paramet stage specifi paramet depend specifi flag command includ includ section stage want sure stage rerun time certain valu chang stage section look like exampl look like command rerun stage time code chang specifi paramet chang want refer doc run gener updat stage",
        "Answer_gpt_summary_original":"The discussion provides a solution to the challenge by suggesting that the user can specify an alternate working directory via \"wdir\" in the dvc.yaml file, which should be a path relative to dvc.yaml's location. Paths for everything else in the stage, like params.yaml, should be specified as relative to wdir (or relative to dvc.yaml if wdir is not provided). The discussion also clarifies that in a DVC stage, \"params\" is for specifying parameter dependencies, not used for specifying command-line flags. The full command including flags\/options should be included in the \"cmd\" section for the stage. The user can use \"dvc run\" to generate or update a dvc.yaml stage instead of writing it by hand",
        "Answer_gpt_summary":"discuss provid solut challeng suggest user specifi altern work directori wdir yaml file path rel yaml locat path stage like param yaml specifi rel wdir rel yaml wdir provid discuss clarifi stage param specifi paramet depend specifi command line flag command includ flag option includ cmd section stage user us run gener updat yaml stage instead write hand"
    },
    {
        "Question_title":"How to execute multiple rows in web service Azure Machine Learning Studio",
        "Question_body":"<p>I create a model in Azure ML studio. \nI deployed the web service.<\/p>\n\n<p>Now, I know how to check one record at a time, but how can I load a csv file and made the algorithm go through all records ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/1tHuM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1tHuM.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>If I click on Batch Execution - it will ask me to create an account for Azure storage. <\/p>\n\n<p>Is any way to execute multiple records from csv file without creating any other accounts?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/90zP7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/90zP7.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1515448065933,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":204.0,
        "Answer_body":"<p>Yes, there is a way and it is simple. What you need is an excel add-in. You need not create any other account.<\/p>\n\n<p>You can either read <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/excel-add-in-for-web-services\" rel=\"nofollow noreferrer\">Excel Add-in for Azure Machine Learning web services doc<\/a> or you can watch <a href=\"https:\/\/www.youtube.com\/watch?v=ju1CzDjiOMQ\" rel=\"nofollow noreferrer\">Azure ML Excel Add-in video<\/a>. <\/p>\n\n<p>If you search for <a href=\"https:\/\/www.google.co.in\/search?q=excel%20add%20in%20for%20azure%20ml&amp;client=firefox-b-ab&amp;dcr=0&amp;source=lnms&amp;tbm=vid&amp;sa=X&amp;ved=0ahUKEwinqP3a_67ZAhXBr48KHdiYAXUQ_AUICigB&amp;biw=1280&amp;bih=616\" rel=\"nofollow noreferrer\">videos on excel add in for azure ml<\/a>, you get other useful videos too. <\/p>\n\n<p>I hope this is the solution you are looking for.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48158545",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1518941429767,
        "Question_original_content":"execut multipl row web servic studio creat model studio deploi web servic know check record time load csv file algorithm record click batch execut ask creat account azur storag wai execut multipl record csv file creat account",
        "Question_preprocessed_content":"execut multipl row web servic studio creat model studio deploi web servic know check record time load csv file algorithm record click batch execut ask creat account azur storag wai execut multipl record csv file creat account",
        "Question_gpt_summary_original":"The user is facing challenges in executing multiple rows in a web service created in Azure Machine Learning Studio. They are able to check one record at a time but are unsure how to load a CSV file and make the algorithm go through all records. Clicking on Batch Execution prompts the user to create an account for Azure storage, and they are looking for a way to execute multiple records from a CSV file without creating any other accounts.",
        "Question_gpt_summary":"user face challeng execut multipl row web servic creat studio abl check record time unsur load csv file algorithm record click batch execut prompt user creat account azur storag look wai execut multipl record csv file creat account",
        "Answer_original_content":"ye wai simpl need excel add need creat account read excel add web servic doc watch excel add video search video excel add us video hope solut look",
        "Answer_preprocessed_content":"ye wai simpl need excel need creat account read excel web servic doc watch excel video search video excel add us video hope solut look",
        "Answer_gpt_summary_original":"Solution: The user can use an Excel add-in to execute multiple records from a CSV file without creating any other accounts. They can refer to the documentation or watch a video tutorial on how to use the add-in.",
        "Answer_gpt_summary":"solut user us excel add execut multipl record csv file creat account refer document watch video tutori us add"
    },
    {
        "Question_title":"dvc gc and files in remote cache",
        "Question_body":"<p>dvc documentation for <code>dvc gc<\/code> command states, the <code>-r<\/code> option indicates \"Remote storage to collect garbage in\" but I'm not sure if I understand it correctly. For example I execute this command:<\/p>\n\n<pre><code>dvc gc -r myremote\n<\/code><\/pre>\n\n<p>What exactly happens if I execute this command? I have 2 possible answers:<\/p>\n\n<ol>\n<li>dvc checks which files should be deleted, then moves these files to \"myremote\" and then deletes all these files in local cache but not in remote.<\/li>\n<li>dvc checks which files should be deleted and deletes these files both in local cache and \"myremote\"<\/li>\n<\/ol>\n\n<p>Which one of them is correct?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1569828438393,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":1617.0,
        "Answer_body":"<p>one of DVC maintainers here.<\/p>\n\n<p>Short answer: 2. is correct.<\/p>\n\n<p>A bit of additional information:\nPlease be careful when using <code>dvc gc<\/code>. It will clear your cache from all dependencies that are not mentioned in the current HEAD of your git repository. \nWe are working on making <code>dvc gc<\/code> preserving whole history by default. <\/p>\n\n<p>So if you don't want to delete files from your history commits, it would be better to wait for completion of <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2325\" rel=\"nofollow noreferrer\">this<\/a> task.<\/p>\n\n<p>[EDIT]\nPlease see comment below.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1569837069043,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58163305",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1569833846627,
        "Question_original_content":"file remot cach document command state option indic remot storag collect garbag sure understand correctli exampl execut command myremot exactli happen execut command possibl answer check file delet move file myremot delet file local cach remot check file delet delet file local cach myremot correct",
        "Question_preprocessed_content":"file remot cach document command state option indic remot storag collect garbag sure understand correctli exampl execut command exactli happen execut command possibl answer check file delet move file myremot delet file local cach remot check file delet delet file local cach myremot correct",
        "Question_gpt_summary_original":"The user is seeking clarification on the functionality of the \"dvc gc\" command with the \"-r\" option, which is used to indicate remote storage for garbage collection. The user is unsure whether executing the command will delete files only from the local cache or from both the local cache and the remote storage.",
        "Question_gpt_summary":"user seek clarif function command option indic remot storag garbag collect user unsur execut command delet file local cach local cach remot storag",
        "Answer_original_content":"maintain short answer correct bit addit inform care clear cach depend mention current head git repositori work make preserv histori default want delet file histori commit better wait complet task edit comment",
        "Answer_preprocessed_content":"maintain short answer correct bit addit inform care clear cach depend mention current head git repositori work make preserv histori default want delet file histori commit better wait complet task edit comment",
        "Answer_gpt_summary_original":"Solution:\nThe \"dvc gc\" command with the \"-r\" option will delete files from both the local cache and the remote storage. However, the user is advised to be careful when using this command as it will clear the cache from all dependencies that are not mentioned in the current HEAD of the git repository. The DVC team is working on making \"dvc gc\" preserve the whole history by default.",
        "Answer_gpt_summary":"solut command option delet file local cach remot storag user advis care command clear cach depend mention current head git repositori team work make preserv histori default"
    },
    {
        "Question_title":"How to get the version of gremlin python client on AWS SageMaker",
        "Question_body":"<p>What is the command to check the version of Gremlin Python client running on a AWS Sagemaker jupyter notebook? I would like to run the command on the jupyter notebook cell.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610823303577,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":198.0,
        "Answer_body":"<p>From a notebook cell you should be able to just ask Pip which version is being used<\/p>\n<pre><code>!pip list | grep gremlinpython\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65753455",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1610839775480,
        "Question_original_content":"version gremlin python client command check version gremlin python client run jupyt notebook like run command jupyt notebook cell",
        "Question_preprocessed_content":"version gremlin python client command check version gremlin python client run jupyt notebook like run command jupyt notebook cell",
        "Question_gpt_summary_original":"The user is seeking assistance in finding the command to check the version of Gremlin Python client running on an AWS Sagemaker Jupyter notebook.",
        "Question_gpt_summary":"user seek assist find command check version gremlin python client run jupyt notebook",
        "Answer_original_content":"notebook cell abl ask pip version pip list grep gremlinpython",
        "Answer_preprocessed_content":"notebook cell abl ask pip version",
        "Answer_gpt_summary_original":"Solution: The solution provided in the discussion is to use the command \"!pip list | grep gremlinpython\" in a notebook cell to check the version of Gremlin Python client running on an AWS Sagemaker Jupyter notebook.",
        "Answer_gpt_summary":"solut solut provid discuss us command pip list grep gremlinpython notebook cell check version gremlin python client run jupyt notebook"
    },
    {
        "Question_title":"AML - Web service TimeoutError",
        "Question_body":"<p>We created a webservice endpoint and tested it with the following code, and also with POSTMAN.<\/p>\n\n<p>We deployed the service to an AKS in the same resource group and subscription as the AML resource.<\/p>\n\n<p><strong>UPDATE: the attached AKS had a custom networking configuration and rejected external connections.<\/strong><\/p>\n\n<pre><code>import numpy\nimport os, json, datetime, sys\nfrom operator import attrgetter\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.image import Image\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.authentication import AzureCliAuthentication\n\ncli_auth = AzureCliAuthentication()\n# Get workspace\nws = Workspace.from_config(auth=cli_auth)\n\n# Get the AKS Details\ntry:\n    with open(\"..\/aml_config\/aks_webservice.json\") as f:\n        config = json.load(f)\nexcept:\n    print(\"No new model, thus no deployment on AKS\")\n    # raise Exception('No new model to register as production model perform better')\n    sys.exit(0)\n\nservice_name = config[\"aks_service_name\"]\n# Get the hosted web service\nservice = Webservice(workspace=ws, name=service_name)\n\n# Input for Model with all features\ninput_j = [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]]\nprint(input_j)\ntest_sample = json.dumps({\"data\": input_j})\ntest_sample = bytes(test_sample, encoding=\"utf8\")\ntry:\n    prediction = service.run(input_data=test_sample)\n    print(prediction)\nexcept Exception as e:\n    result = str(e)\n    print(result)\n    raise Exception(\"AKS service is not working as expected\")\n<\/code><\/pre>\n\n<p>In AML Studio, the deployment state is \"Healthy\".<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/RTB10.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RTB10.png\" alt=\"Endpoint attributes\"><\/a><\/p>\n\n<p>We get the following error when testing:<\/p>\n\n<pre><code>Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'\n<\/code><\/pre>\n\n<p><strong>Log just after deploying the AKS Webservice <a href=\"http:\/\/t.ly\/t79b\" rel=\"nofollow noreferrer\">here<\/a>.<\/strong><\/p>\n\n<p><strong>Log after running the test script <a href=\"http:\/\/t.ly\/79k5\" rel=\"nofollow noreferrer\">here<\/a>.<\/strong><\/p>\n\n<p>How can we know what is causing this problem and fix it?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1592508291480,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1592590061567,
        "Question_score":2.0,
        "Question_view_count":332.0,
        "Answer_body":"<p>We checked the AKS networking configuration and realized it has an Azure CNI profile.<\/p>\n\n<p>In order to test the webservice we need to do it from inside the created virtual network.\nIt worked well!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62457880",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1592590202852,
        "Question_original_content":"aml web servic timeouterror creat webservic endpoint test follow code postman deploi servic ak resourc group subscript aml resourc updat attach ak custom network configur reject extern connect import numpi import json datetim sy oper import attrgett core import workspac core model import model core imag import imag core webservic import webservic core authent import azurecliauthent cli auth azurecliauthent workspac workspac config auth cli auth ak detail try open aml config ak webservic json config json load print new model deploy ak rais except new model regist product model perform better sy exit servic config ak servic host web servic servic webservic workspac servic input model featur input print input test sampl json dump data input test sampl byte test sampl encod utf try predict servic run input data test sampl print predict except result str print result rais except ak servic work expect aml studio deploy state healthi follow error test fail establish new connect winerror connect attempt fail connect parti properli respond period time establish connect fail connect host fail respond log deploi ak webservic log run test script know caus problem fix",
        "Question_preprocessed_content":"aml web servic timeouterror creat webservic endpoint test follow code postman deploi servic ak resourc group subscript aml resourc updat attach ak custom network configur reject extern connect aml studio deploy state healthi follow error test log deploi ak webservic log run test script know caus problem fix",
        "Question_gpt_summary_original":"The user encountered a TimeoutError when testing a webservice endpoint that was deployed to an AKS in the same resource group and subscription as the AML resource. The deployment state in AML Studio is \"Healthy\", but the AKS had a custom networking configuration and rejected external connections. The user is seeking assistance in identifying and resolving the cause of the problem.",
        "Question_gpt_summary":"user encount timeouterror test webservic endpoint deploi ak resourc group subscript aml resourc deploy state aml studio healthi ak custom network configur reject extern connect user seek assist identifi resolv caus problem",
        "Answer_original_content":"check ak network configur realiz azur cni profil order test webservic need insid creat virtual network work",
        "Answer_preprocessed_content":"check ak network configur realiz azur cni profil order test webservic need insid creat virtual network work",
        "Answer_gpt_summary_original":"Solution: The solution mentioned in the discussion was to test the webservice from inside the virtual network created for the AKS with Azure CNI profile. This resolved the TimeoutError issue.",
        "Answer_gpt_summary":"solut solut mention discuss test webservic insid virtual network creat ak azur cni profil resolv timeouterror issu"
    },
    {
        "Question_title":"What would be the best ML method for this use case?",
        "Question_body":"<p>Within AzureML, I have a CSV file which contains <code>2 columns<\/code> of data with <code>thousands of rows<\/code>. I'm looking to run this entire file as training, and find a pattern between these 2 sets of numbers, for example:<\/p>\n\n<pre><code>x -&gt; y\n\n... 10k x\n<\/code><\/pre>\n\n<p>And after all that training, I'd want to give this one line as the score model, so It'd look like:\nx -> ? (Predict answer from training)\n-- Note, the question mark here wouldn't need to be an exact match, as long as it is somewhat around what that actual number would turn out to be like.<\/p>\n\n<p>Is their a ML method (Inside <code>Azure ML<\/code>) that does such thing? Any points would be great.<\/p>\n\n<p>tl;dr: <code>Finding any type of pattern between 2 numbers (w\/ intense training).<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1441616495073,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":32.0,
        "Answer_body":"<p>Read about <code>linear regression<\/code>. This is answer for your question. And here is the link to Azure ML tutorial <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-create-experiment\/\" rel=\"nofollow\">link<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/32434805",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1448906765136,
        "Question_original_content":"best method us case csv file contain column data thousand row look run entir file train pattern set number exampl train want line score model look like predict answer train note question mark wouldn need exact match long somewhat actual number turn like method insid thing point great find type pattern number intens train",
        "Question_preprocessed_content":"best method us case csv file contain data look run entir file train pattern set number exampl train want line score model look like note question mark wouldn need exact match long somewhat actual number turn like method thing point great",
        "Question_gpt_summary_original":"The user is looking for a machine learning method within AzureML to find a pattern between two sets of numbers in a CSV file with thousands of rows. They want to use the entire file for training and then predict the answer for a single line. The user is seeking advice on the best ML method to accomplish this task.",
        "Question_gpt_summary":"user look machin learn method pattern set number csv file thousand row want us entir file train predict answer singl line user seek advic best method accomplish task",
        "Answer_original_content":"read linear regress answer question link tutori link",
        "Answer_preprocessed_content":"read answer question link tutori link",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to use linear regression as the machine learning method within AzureML to find a pattern between two sets of numbers in a CSV file with thousands of rows. The user is also provided with a link to an Azure ML tutorial for further guidance.",
        "Answer_gpt_summary":"solut solut suggest discuss us linear regress machin learn method pattern set number csv file thousand row user provid link tutori guidanc"
    },
    {
        "Question_title":"Amazon Sage Maker: How to authenticate AWS SageMaker End-Point Request",
        "Question_body":"<p>I have an aws sagemaker end-point which need to be called from .Net core client, I have used the AWS SDK that deals with SageMaker and provided the required credentials however, always it keeps saying : <\/p>\n\n<p>The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.<\/p>\n\n<p>var requestBody = \"{'url':'\"+\"<a href=\"https:\/\/cdn.pixabay.com\/photo\/2018\/05\/28\/22\/11\/message-in-a-bottle-3437294_960_720.jpg\" rel=\"nofollow noreferrer\">https:\/\/cdn.pixabay.com\/photo\/2018\/05\/28\/22\/11\/message-in-a-bottle-3437294_960_720.jpg<\/a>\" + \"'}\";<\/p>\n\n<pre><code>        var request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest()\n        {\n            EndpointName = \"CG-model-v1-endpoint\",\n            ContentType = \"application\/json;utf-8\",\n            Body = new MemoryStream(Encoding.ASCII.GetBytes(JsonConvert.SerializeObject(requestBody)))\n\n        };\n\n\n        var awsClient = new AmazonSageMakerRuntimeClient(awsAccessKeyId: \"XXXX\", awsSecretAccessKey: \"XXX\", region: RegionEndpoint.EUCentral1);\n\n        try\n        {\n            var resposnse = await awsClient.InvokeEndpointAsync(request);\n\n        }\n        catch (Exception ex)\n        {\n\n            return ApiResponse&lt;bool&gt;.Create(false);\n        }\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1563803017410,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":222.0,
        "Answer_body":"<p>I found the error , it was simply because of the request content-type,it had to be application\/json instead of application\/json;utf-8<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57147396",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1563872105436,
        "Question_original_content":"amazon sage maker authent end point request end point need call net core client aw sdk deal provid requir credenti keep sai request signatur calcul match signatur provid check aw secret access kei sign method consult servic document detail var requestbodi url http cdn pixabai com photo messag bottl jpg var request new amazon runtim model invokeendpointrequest endpointnam model endpoint contenttyp applic json utf bodi new memorystream encod ascii getbyt jsonconvert serializeobject requestbodi var awsclient new amazonruntimecli awsaccesskeyid awssecretaccesskei region regionendpoint eucentr try var resposns await awsclient invokeendpointasync request catch except return apirespons creat fals",
        "Question_preprocessed_content":"amazon sage maker authent request need call net core client aw sdk deal provid requir credenti keep sai request signatur calcul match signatur provid check aw secret access kei sign method consult servic document detail var requestbodi",
        "Question_gpt_summary_original":"The user is facing challenges in authenticating AWS SageMaker End-Point requests from a .Net core client. Despite providing the required credentials, the user is receiving an error message indicating that the request signature does not match the provided signature. The user has shared a code snippet that includes the request body and the AWS SDK used to deal with SageMaker.",
        "Question_gpt_summary":"user face challeng authent end point request net core client despit provid requir credenti user receiv error messag indic request signatur match provid signatur user share code snippet includ request bodi aw sdk deal",
        "Answer_original_content":"error simpli request content type applic json instead applic json utf",
        "Answer_preprocessed_content":"error simpli request instead",
        "Answer_gpt_summary_original":"Solution: The user found the solution to the challenge by changing the request content-type from \"application\/json;utf-8\" to \"application\/json\".",
        "Answer_gpt_summary":"solut user solut challeng chang request content type applic json utf applic json"
    },
    {
        "Question_title":"'no SavedModel bundles found!' on tensorflow_hub model deployment to AWS SageMaker",
        "Question_body":"<p>I attempting to deploy the universal-sentence-encoder model to a aws Sagemaker endpoint and am getting the error <code>raise ValueError('no SavedModel bundles found!')<\/code><\/p>\n\n<p>I have shown my code below, I have a feeling that one of my paths is incorrect<\/p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nfrom sagemaker import get_execution_role\nfrom sagemaker.tensorflow.serving import Model\n\ndef tfhub_to_savedmodel(model_name,uri):\n    tfhub_uri = uri\n    model_path = 'encoder_model\/' + model_name\n\n    with tf.Session(graph=tf.Graph()) as sess:\n        module = hub.Module(tfhub_uri) \n        input_params = module.get_input_info_dict()\n        dtype = input_params['text'].dtype\n        shape = input_params['text'].get_shape()\n\n        # define the model inputs\n        inputs = {'text': tf.placeholder(dtype, shape, 'text')}\n\n        # define the model outputs\n        # we want the class ids and probabilities for the top 3 classes\n        logits = module(inputs['text'])\n        outputs = {\n            'vector': logits,\n        }\n\n        # export the model\n        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        tf.saved_model.simple_save(\n            sess,\n            model_path,\n            inputs=inputs,\n            outputs=outputs)  \n\n    return model_path\n\n\nsagemaker_role = get_execution_role()\n\n!tar -C \"$PWD\" -czf encoder.tar.gz encoder_model\/\nmodel_data = Session().upload_data(path='encoder.tar.gz',key_prefix='model')\n\nenv = {'SAGEMAKER_TFS_DEFAULT_MODEL_NAME': 'universal-sentence-encoder-large'}\n\nmodel = Model(model_data=model_data, role=sagemaker_role, framework_version=1.12, env=env)\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1563915680943,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":2621.0,
        "Answer_body":"<p>I suppose you started from this example? <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_serving_container\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_serving_container<\/a><\/p>\n\n<p>It looks like you're not saving the TF Serving bundle properly: the model version number is missing, because of this line:<\/p>\n\n<pre><code>model_path = 'encoder_model\/' + model_name\n<\/code><\/pre>\n\n<p>Replacing it with this should fix your problem:<\/p>\n\n<pre><code>model_path = '{}\/{}\/00000001'.format('encoder_model\/', model_name)\n<\/code><\/pre>\n\n<p>Your model artefact should look like this (I used the model in the notebook above):<\/p>\n\n<pre><code>mobilenet\/\nmobilenet\/mobilenet_v2_140_224\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/saved_model.pb\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/variables.data-00000-of-00001\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/variables.index\n<\/code><\/pre>\n\n<p>Then, upload to S3 and deploy.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57172147",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1563978858283,
        "Question_original_content":"savedmodel bundl tensorflow hub model deploy attempt deploi univers sentenc encod model endpoint get error rais valueerror savedmodel bundl shown code feel path incorrect import tensorflow import tensorflow hub hub import numpi import execut role tensorflow serv import model def tfhub savedmodel model uri tfhub uri uri model path encod model model session graph graph sess modul hub modul tfhub uri input param modul input info dict dtype input param text dtype shape input param text shape defin model input input text placehold dtype shape text defin model output want class id probabl class logit modul input text output vector logit export model sess run global variabl initi tabl initi save model simpl save sess model path input input output output return model path role execut role tar pwd czf encod tar encod model model data session upload data path encod tar kei prefix model env tf default model univers sentenc encod larg model model model data model data role role framework version env env predictor model deploi initi instanc count instanc type medium",
        "Question_preprocessed_content":"savedmodel bundl model deploy attempt deploi model endpoint get error shown code feel path incorrect",
        "Question_gpt_summary_original":"The user is encountering an error while attempting to deploy the universal-sentence-encoder model to an AWS SageMaker endpoint. The error message indicates that no SavedModel bundles were found. The user suspects that one of their paths may be incorrect. The user has provided their code and is using TensorFlow and TensorFlow Hub to convert the model to a SavedModel format.",
        "Question_gpt_summary":"user encount error attempt deploi univers sentenc encod model endpoint error messag indic savedmodel bundl user suspect path incorrect user provid code tensorflow tensorflow hub convert model savedmodel format",
        "Answer_original_content":"suppos start exampl http github com awslab amazon exampl tree master python sdk tensorflow serv contain look like save serv bundl properli model version number miss line model path encod model model replac fix problem model path format encod model model model artefact look like model notebook mobilenet mobilenet mobilenet mobilenet mobilenet mobilenet mobilenet save model mobilenet mobilenet variabl mobilenet mobilenet variabl variabl data mobilenet mobilenet variabl variabl index upload deploi",
        "Answer_preprocessed_content":"suppos start exampl look like save serv bundl properli model version number miss line replac fix problem model artefact look like upload deploi",
        "Answer_gpt_summary_original":"Solution: The error message indicates that no SavedModel bundles were found. One possible solution is to modify the code to include the model version number in the model path. The correct model artefact should have the following structure: \n\n```\nmodel_name\/\nmodel_name\/version_number\/\nmodel_name\/version_number\/saved_model.pb\nmodel_name\/version_number\/variables\/\nmodel_name\/version_number\/variables\/variables.data-00000-of-00001\nmodel_name\/version_number\/variables\/variables.index\n```\n\nAfter modifying the code, the user should upload the model to S3 and deploy it.",
        "Answer_gpt_summary":"solut error messag indic savedmodel bundl possibl solut modifi code includ model version number model path correct model artefact follow structur model model version number model version number save model model version number variabl model version number variabl variabl data model version number variabl variabl index modifi code user upload model deploi"
    },
    {
        "Question_title":"ModuleNotFoundError while using AzureML pipeline with yml file based RunConfiguration and environment.yml",
        "Question_body":"<p>I am running into a ModuleNotFoundError for pandas while using the following code to orchestrate my Azure Machine Learning Pipeline:<\/p>\n<pre><code># Loading run config\nprint(&quot;Loading run config&quot;)\ntask_1_run_config = RunConfiguration.load(\n    os.path.join(WORKING_DIR + '\/pipeline\/task_runconfigs\/T01_Test_Task.yml')\n    ) \n\ntask_1_script_run_config = ScriptRunConfig(\n    source_directory=os.path.join(WORKING_DIR + '\/pipeline\/task_scripts'),\n    run_config=task_1_run_config    \n)\n\ntask_1_py_script_step = PythonScriptStep(\n    name='Task_1_Step',\n    script_name=task_1_script_run_config.script,\n    source_directory=task_1_script_run_config.source_directory,\n    compute_target=compute_target\n)\n\npipeline_run_config = Pipeline(workspace=workspace, steps=[task_1_py_script_step])#, task_2])\n\npipeline_run = Experiment(workspace, 'Test_Run_New_Pipeline').submit(pipeline_run_config)\npipeline_run.wait_for_completion()\n<\/code><\/pre>\n<p>The environment.yml<\/p>\n<pre><code>name: phinmo_pipeline_env\ndependencies:\n- python=3.8\n- pip:\n  - pandas\n  - azureml-core==1.43.0\n  - azureml-sdk\n  - scipy\n  - scikit-learn\n  - numpy\n  - pyyaml==6.0\n  - datetime\n  - azure\nchannels:\n  - conda-forge\n<\/code><\/pre>\n<p>The loaded RunConfiguration in T01_Test_Task.yml looks like this:<\/p>\n<pre><code># The script to run.\nscript: T01_Test_Task.py\n# The arguments to the script file.\narguments: [\n  &quot;--test&quot;, False,\n  &quot;--date&quot;, &quot;2022-07-26&quot;\n]\n# The name of the compute target to use for this run.\ncompute_target: phinmo-compute-cluster\n# Framework to execute inside. Allowed values are &quot;Python&quot;, &quot;PySpark&quot;, &quot;CNTK&quot;, &quot;TensorFlow&quot;, and &quot;PyTorch&quot;.\nframework: Python\n# Maximum allowed duration for the run.\nmaxRunDurationSeconds: 6000\n# Number of nodes to use for running job.\nnodeCount: 1\n\n#Environment details.\nenvironment:\n  # Environment name\n  name: phinmo_pipeline_env\n  # Environment version\n  version:\n  # Environment variables set for the run.\n  #environmentVariables:\n  #  EXAMPLE_ENV_VAR: EXAMPLE_VALUE\n  # Python details\n  python:\n    # user_managed_dependencies=True indicates that the environmentwill be user managed. False indicates that AzureML willmanage the user environment.\n    userManagedDependencies: false\n    # The python interpreter path\n    interpreterPath: python\n    # Path to the conda dependencies file to use for this run. If a project\n    # contains multiple programs with different sets of dependencies, it may be\n    # convenient to manage those environments with separate files.\n    condaDependenciesFile: environment.yml\n    # The base conda environment used for incremental environment creation.\n    baseCondaEnvironment: AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\n  # Docker details\n  \n# History details.\nhistory:\n  # Enable history tracking -- this allows status, logs, metrics, and outputs\n  # to be collected for a run.\n  outputCollection: true\n  # Whether to take snapshots for history.\n  snapshotProject: true\n  # Directories to sync with FileWatcher.\n  directoriesToWatch:\n  - logs\n# data reference configuration details\ndataReferences: {}\n# The configuration details for data.\ndata: {}\n# Project share datastore reference.\nsourceDirectoryDataStore:\n<\/code><\/pre>\n<p>I already tried a few things like overwriting the environment attribute in the RunConfiguration object with a environment.python.conda_dependencies object or assigning a version number to pandas in the environment.yml, changing the location of the environment.yml. But I am at a loss at what else to try. the T01_Test_Task.py runs without issues on its own. But putting it into a pipeline just does not seem to work.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1658922798813,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1658924360076,
        "Question_score":0.0,
        "Question_view_count":51.0,
        "Answer_body":"<p>Okay I found the issue.\nI am unnecessarily using the ScriptRunConfig which overwrites the assigned environment with some default azureml environment. I was able to see that only in the Task description in the Azure Machine Learning Studio UI.<\/p>\n<p>I was able to just remove that part and now it works:<\/p>\n<pre><code>task_1_run_config = RunConfiguration.load(\n    os.path.join(WORKING_DIR + '\/pipeline\/task_runconfigs\/T01_Test_Task.yml')\n    ) \ntask_1_py_script_step = PythonScriptStep(\n    name='Task_1_Step',\n    script_name='T01_Test_Task.py',\n    source_directory=os.path.join(WORKING_DIR + '\/pipeline\/task_scripts'),\n    runconfig=task_1_run_config, \n    compute_target=compute_target\n)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73137433",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1658937635368,
        "Question_original_content":"modulenotfounderror pipelin yml file base runconfigur environ yml run modulenotfounderror panda follow code orchestr pipelin load run config print load run config task run config runconfigur load path join work dir pipelin task runconfig test task yml task script run config scriptrunconfig sourc directori path join work dir pipelin task script run config task run config task script step pythonscriptstep task step script task script run config script sourc directori task script run config sourc directori comput target comput target pipelin run config pipelin workspac workspac step task script step task pipelin run experi workspac test run new pipelin submit pipelin run config pipelin run wait complet environ yml phinmo pipelin env depend python pip panda core sdk scipi scikit learn numpi pyyaml datetim azur channel conda forg load runconfigur test task yml look like script run script test task argument script file argument test fals date comput target us run comput target phinmo comput cluster framework execut insid allow valu python pyspark cntk tensorflow pytorch framework python maximum allow durat run maxrundurationsecond number node us run job nodecount environ detail environ environ phinmo pipelin env environ version version environ variabl set run environmentvari exampl env var exampl valu python detail python user manag depend true indic environmentwil user manag fals indic willmanag user environ usermanageddepend fals python interpret path interpreterpath python path conda depend file us run project contain multipl program differ set depend conveni manag environ separ file condadependenciesfil environ yml base conda environ increment environ creation basecondaenviron sklearn ubuntu cpu docker detail histori detail histori enabl histori track allow statu log metric output collect run outputcollect true snapshot histori snapshotproject true directori sync filewatch directoriestowatch log data refer configur detail datarefer configur detail data data project share datastor refer sourcedirectorydatastor tri thing like overwrit environ attribut runconfigur object environ python conda depend object assign version number panda environ yml chang locat environ yml loss try test task run issu put pipelin work",
        "Question_preprocessed_content":"modulenotfounderror pipelin yml file base runconfigur run modulenotfounderror panda follow code orchestr pipelin load runconfigur look like tri thing like overwrit environ attribut runconfigur object object assign version number panda chang locat loss try run issu put pipelin work",
        "Question_gpt_summary_original":"The user is encountering a ModuleNotFoundError for pandas while using Azure Machine Learning Pipeline with a yml file based RunConfiguration and environment.yml. The T01_Test_Task.py runs without issues on its own, but putting it into a pipeline does not seem to work. The user has tried overwriting the environment attribute in the RunConfiguration object with a environment.python.conda_dependencies object or assigning a version number to pandas in the environment.yml, changing the location of the environment.yml, but is still unable to resolve the issue.",
        "Question_gpt_summary":"user encount modulenotfounderror panda pipelin yml file base runconfigur environ yml test task run issu put pipelin work user tri overwrit environ attribut runconfigur object environ python conda depend object assign version number panda environ yml chang locat environ yml unabl resolv issu",
        "Answer_original_content":"okai issu unnecessarili scriptrunconfig overwrit assign environ default environ abl task descript studio abl remov work task run config runconfigur load path join work dir pipelin task runconfig test task yml task script step pythonscriptstep task step script test task sourc directori path join work dir pipelin task script runconfig task run config comput target comput target",
        "Answer_preprocessed_content":"okai issu unnecessarili scriptrunconfig overwrit assign environ default environ abl task descript studio abl remov work",
        "Answer_gpt_summary_original":"Solution: The user found that the issue was caused by using the ScriptRunConfig which overwrites the assigned environment with some default azureml environment. The user was able to resolve the issue by removing that part and using the correct RunConfiguration object.",
        "Answer_gpt_summary":"solut user issu caus scriptrunconfig overwrit assign environ default environ user abl resolv issu remov correct runconfigur object"
    },
    {
        "Question_title":"How to retrieve the model signature from the MLflow Model Registry",
        "Question_body":"<p>I have registered a scikit learn model on my MLflow Tracking server, and I am loading it with <code>sklearn.load_model(model_uri)<\/code>.<\/p>\n<p>Now, I would like to access the signature of the model so I can get a list of the model's required inputs\/features so I can retrieve them from my feature store by name. I can't seem to find any utility or method in the <code>mlflow<\/code> API or the <code>MLFlowClient<\/code> API that will let me access a signature or inputs\/outputs attribute, even though I can see a list of inputs and outputs under each version of the model in the UI.<\/p>\n<p>I know that I can find the input sample and the model configuration in the model's artifacts, but that would require me actually downloading the artifacts and loading them manually in my script. I don't need to avoid that, but I am surprised that I can't just return the signature as a dictionary the same way I can return a run's parameters or metrics.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643655088920,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":904.0,
        "Answer_body":"<p>The way to access the model's signature without downloading the MLModel file is under the loaded model. And then you'll access the model's attributes, such as its signature or even other Pyfunc-defined methods.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmodel = mlflow.pyfunc.load_model(&quot;runs:\/&lt;run_id&gt;\/model&quot;)\nprint(model._model_meta._signature)\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70931309",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1645469817663,
        "Question_original_content":"retriev model signatur model registri regist scikit learn model track server load sklearn load model model uri like access signatur model list model requir input featur retriev featur store util method api client api let access signatur input output attribut list input output version model know input sampl model configur model artifact requir actual download artifact load manual script need avoid surpris return signatur dictionari wai return run paramet metric",
        "Question_preprocessed_content":"retriev model signatur model registri regist scikit learn model track server load like access signatur model list model requir retriev featur store util method api api let access signatur attribut list input output version model know input sampl model configur model artifact requir actual download artifact load manual script need avoid surpris return signatur dictionari wai return run paramet metric",
        "Question_gpt_summary_original":"The user is facing a challenge in retrieving the model signature from the MLflow Model Registry. They have registered a scikit learn model on their MLflow Tracking server and loaded it, but they cannot find any utility or method in the mlflow API or the MLFlowClient API that will let them access a signature or inputs\/outputs attribute. Although they can see a list of inputs and outputs under each version of the model in the UI, they need to access the signature to get a list of the model's required inputs\/features so they can retrieve them from their feature store by name. The user knows that they can find the input sample and the model configuration in the model's artifacts, but that would require them to download the artifacts and load them manually in their script.",
        "Question_gpt_summary":"user face challeng retriev model signatur model registri regist scikit learn model track server load util method api client api let access signatur input output attribut list input output version model need access signatur list model requir input featur retriev featur store user know input sampl model configur model artifact requir download artifact load manual script",
        "Answer_original_content":"wai access model signatur download mlmodel file load model access model attribut signatur pyfunc defin method import model pyfunc load model run model print model model meta signatur",
        "Answer_preprocessed_content":"wai access model signatur download mlmodel file load model access model attribut signatur method",
        "Answer_gpt_summary_original":"Solution: The user can access the model's signature by loading the model using the mlflow.pyfunc.load_model() method and then accessing the model's attributes, such as its signature or other Pyfunc-defined methods. They can print the model's signature using the following code: \n\n```\nimport mlflow\n\nmodel = mlflow.pyfunc.load_model(\"runs:\/<run_id>\/model\")\nprint(model._model_meta._signature)\n```\n\nNo other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut user access model signatur load model pyfunc load model method access model attribut signatur pyfunc defin method print model signatur follow code import model pyfunc load model run model print model model meta signatur solut mention discuss"
    },
    {
        "Question_title":"How can I generate prediction intervals for Azure AutoML timeseries forecasts?",
        "Question_body":"<p>Is it possible to generate prediction intervals for time series forecasts when using a Azure AutoML trained models? Could we get the training errors out of the process and use them for bootstrapping?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618193869380,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":205.0,
        "Answer_body":"<p>You can generate forecast quantiles. See the following notebook for more details: <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-forecast-function\/auto-ml-forecasting-function.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-forecast-function\/auto-ml-forecasting-function.ipynb<\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67051900",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1618942132740,
        "Question_original_content":"gener predict interv azur automl timeseri forecast possibl gener predict interv time seri forecast azur automl train model train error process us bootstrap",
        "Question_preprocessed_content":"gener predict interv azur automl timeseri forecast possibl gener predict interv time seri forecast azur automl train model train error process us bootstrap",
        "Question_gpt_summary_original":"The user is facing challenges in generating prediction intervals for time series forecasts using Azure AutoML trained models. They are unsure if it is possible to obtain training errors for bootstrapping purposes.",
        "Question_gpt_summary":"user face challeng gener predict interv time seri forecast azur automl train model unsur possibl obtain train error bootstrap purpos",
        "Answer_original_content":"gener forecast quantil follow notebook detail http github com azur machinelearningnotebook blob master us autom machin learn forecast forecast function auto forecast function ipynb",
        "Answer_preprocessed_content":"gener forecast quantil follow notebook detail",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to the challenge by suggesting the user to generate forecast quantiles. The user can refer to the provided notebook for more details on how to generate forecast quantiles.",
        "Answer_gpt_summary":"solut discuss provid solut challeng suggest user gener forecast quantil user refer provid notebook detail gener forecast quantil"
    },
    {
        "Question_title":"What's the difference between regular and ml AWS EC2 instances?",
        "Question_body":"<p>I'm experimenting with <a href=\"https:\/\/aws.amazon.com\/sagemaker\/\" rel=\"nofollow noreferrer\">AWS Sagemaker<\/a> using a Free Tier account. According to the <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">Sagemaker pricing<\/a>, I can use 50 hours of m4.xlarge and m5.xlarge instances for training in the free tier. (I am safely within the two-month limit.) But when I attempt to train an algorithm with the XGBoost container using m5.xlarge, I get the error shown below the code.<\/p>\n<p>Are the ml-type and non-ml-type instances the same with just a fancy prefix for those that one would use with Sagemaker or are they entirely different? The <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/\" rel=\"nofollow noreferrer\">EC2 page<\/a> doesn't even list the ml instances.<\/p>\n<pre><code>sess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(container,\n                                    role, \n                                    instance_count=1, \n                                    instance_type='m5.xlarge',\n                                    output_path=output_location,\n                                    sagemaker_session=sess)\n<\/code><\/pre>\n<blockquote>\n<p>ClientError: An error occurred (ValidationException) when calling the\nCreateTrainingJob operation: 1 validation error detected: Value\n'm5.xlarge' at 'resourceConfig.instanceType' failed to satisfy\nconstraint: Member must satisfy enum value set: [ml.p2.xlarge,\nml.m5.4xlarge, ml.m4.16xlarge, ml.p4d.24xlarge, ml.c5n.xlarge,\nml.p3.16xlarge, ml.m5.large, ml.p2.16xlarge, ml.c4.2xlarge,\nml.c5.2xlarge, ml.c4.4xlarge, ml.c5.4xlarge, ml.c5n.18xlarge,\nml.g4dn.xlarge, ml.g4dn.12xlarge, ml.c4.8xlarge, ml.g4dn.2xlarge,\nml.c5.9xlarge, ml.g4dn.4xlarge, ml.c5.xlarge, ml.g4dn.16xlarge,\nml.c4.xlarge, ml.g4dn.8xlarge, ml.c5n.2xlarge, ml.c5n.4xlarge,\nml.c5.18xlarge, ml.p3dn.24xlarge, ml.p3.2xlarge, ml.m5.xlarge,\nml.m4.10xlarge, ml.c5n.9xlarge, ml.m5.12xlarge, ml.m4.xlarge,\nml.m5.24xlarge, ml.m4.2xlarge, ml.p2.8xlarge, ml.m5.2xlarge,\nml.p3.8xlarge, ml.m4.4xlarge]<\/p>\n<\/blockquote>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1607865181387,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1608039265823,
        "Question_score":5.0,
        "Question_view_count":7690.0,
        "Answer_body":"<p>The instances with the <code>ml<\/code> prefix are instance classes specifically for use in Sagemaker.<\/p>\n<p>In addition to being used within the Sagemaker service, the instance will be running an AMI with all the necessary libraries and packages such as Jupyter.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_last_edit_time":null,
        "Answer_score":12.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65276017",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1607865492452,
        "Question_original_content":"differ regular aw instanc experi free tier account accord price us hour xlarg xlarg instanc train free tier safe month limit attempt train algorithm xgboost contain xlarg error shown code type non type instanc fanci prefix us entir differ page list instanc sess session xgb estim estim contain role instanc count instanc type xlarg output path output locat session sess clienterror error occur validationexcept call createtrainingjob oper valid error detect valu xlarg resourceconfig instancetyp fail satisfi constraint member satisfi enum valu set xlarg xlarg xlarg xlarg xlarg xlarg larg xlarg xlarg xlarg xlarg xlarg xlarg gdn xlarg gdn xlarg xlarg gdn xlarg xlarg gdn xlarg xlarg gdn xlarg xlarg gdn xlarg xlarg xlarg xlarg pdn xlarg xlarg xlarg xlarg xlarg xlarg xlarg xlarg xlarg xlarg xlarg xlarg xlarg",
        "Question_preprocessed_content":"differ regular aw instanc experi free tier account accord price us hour instanc train free tier attempt train algorithm xgboost contain error shown code instanc fanci prefix us entir differ page list instanc clienterror error occur call createtrainingjob oper valid error detect valu fail satisfi constraint member satisfi enum valu set",
        "Question_gpt_summary_original":"The user is encountering an error when attempting to train an algorithm with the XGBoost container using m5.xlarge instance type in AWS Sagemaker. The user is questioning whether ml-type and non-ml-type instances are the same or entirely different, as the EC2 page does not list the ml instances.",
        "Question_gpt_summary":"user encount error attempt train algorithm xgboost contain xlarg instanc type user question type non type instanc entir differ page list instanc",
        "Answer_original_content":"instanc prefix instanc class specif us addit servic instanc run ami necessari librari packag jupyt",
        "Answer_preprocessed_content":"instanc prefix instanc class specif us addit servic instanc run ami necessari librari packag jupyt",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to the user's question by stating that instances with the \"ml\" prefix are instance classes specifically designed for use in Sagemaker. These instances come with all the necessary libraries and packages such as Jupyter, making them suitable for machine learning tasks.",
        "Answer_gpt_summary":"solut discuss provid solut user question state instanc prefix instanc class specif design us instanc come necessari librari packag jupyt make suitabl machin learn task"
    },
    {
        "Question_title":"Azure Python SDK & Machine Learning Studio Web Service Batch Execution Snippet: TypeError",
        "Question_body":"<p><strong>First Issue resolved, please read scroll down to EDIT2<\/strong><\/p>\n\n<p>I'm trying to access a Web Service deployed via Azure Machine Learning Studio, using the Batch Execution-Sample Code for Python on the bottom of below page:<\/p>\n\n<p><a href=\"https:\/\/studio.azureml.net\/apihelp\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/webservices\/e4e3d2d32ec347ae9a829b200f7d31cd\/endpoints\/61670382104542bc9533a920830b263c\/jobs\" rel=\"nofollow noreferrer\">https:\/\/studio.azureml.net\/apihelp\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/webservices\/e4e3d2d32ec347ae9a829b200f7d31cd\/endpoints\/61670382104542bc9533a920830b263c\/jobs<\/a><\/p>\n\n<p>I have already fixed an Issue according to this question (replaced BlobService by BlobBlockService and so on):<\/p>\n\n<p><a href=\"https:\/\/studio.azureml.net\/apihelp\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/webservices\/e4e3d2d32ec347ae9a829b200f7d31cd\/endpoints\/61670382104542bc9533a920830b263c\/jobs\" rel=\"nofollow noreferrer\">https:\/\/studio.azureml.net\/apihelp\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/webservices\/e4e3d2d32ec347ae9a829b200f7d31cd\/endpoints\/61670382104542bc9533a920830b263c\/jobs<\/a><\/p>\n\n<p>And I also have entered the API-Key, Container-Name, URL, account_key and account_name according to the instructions.<\/p>\n\n<p>However it seems that today the Code Snippet is even more outdated than it was back then because I receive a different error now:<\/p>\n\n<pre><code>File \"C:\/Users\/Alex\/Desktop\/scripts\/BatchExecution.py\", line 80, in uploadFileToBlob\n    blob_service = asb.BlockBlobService(account_name=storage_account_name, account_key=storage_account_key)\n\n  File \"C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\azure\\storage\\blob\\blockblobservice.py\", line 145, in __init__\n\n  File \"C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\azure\\storage\\blob\\baseblobservice.py\", line 205, in __init__\n\nTypeError: get_service_parameters() got an unexpected keyword argument 'token_credential'\n<\/code><\/pre>\n\n<p>I also noticed, that when installing the Azure SDK for Python via pip, I get the following warnings in the end of the process (installation is successful however):<\/p>\n\n<pre><code>azure-storage-queue 1.3.0 has requirement azure-storage-common&lt;1.4.0,&gt;=1.3.0, but you'll have azure-storage-common 1.1.0 which is incompatible.\n\nazure-storage-file 1.3.0 has requirement azure-storage-common&lt;1.4.0,&gt;=1.3.0, but you'll have azure-storage-common 1.1.0 which is incompatible.\n\nazure-storage-blob 1.3.0 has requirement azure-storage-common&lt;1.4.0,&gt;=1.3.0, but you'll have azure-storage-common 1.1.0 which is incompatible.\n<\/code><\/pre>\n\n<p>I can't find anything about all this in the latest documentation for the Python SDK (the word 'token_credential' is not even contained):<\/p>\n\n<p><a href=\"https:\/\/media.readthedocs.org\/pdf\/azure-storage\/latest\/azure-storage.pdf\" rel=\"nofollow noreferrer\">https:\/\/media.readthedocs.org\/pdf\/azure-storage\/latest\/azure-storage.pdf<\/a><\/p>\n\n<p>Does anyone have a clue what's going wrong during the installation or why the type-error with the 'token_credential' pops up during execution?<\/p>\n\n<p>Or does anyone know how I can install the necessary version of azure-storage-common or azure-storage-blob?<\/p>\n\n<p>EDIT: Here's a my code (however not-reproducible because I changed the keys before posting)<\/p>\n\n<pre><code># How this works:\n#\n# 1. Assume the input is present in a local file (if the web service accepts input)\n# 2. Upload the file to an Azure blob - you\"d need an Azure storage account\n# 3. Call BES to process the data in the blob. \n# 4. The results get written to another Azure blob.\n\n# 5. Download the output blob to a local file\n#\n# Note: You may need to download\/install the Azure SDK for Python.\n# See: http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/python-how-to-install\/\n\nimport urllib\n# If you are using Python 3+, import urllib instead of urllib2\n\nimport json\nimport time\nimport azure.storage.blob as asb          # replaces BlobService by BlobBlockService\n\n\ndef printHttpError(httpError):\n    print(\"The request failed with status code: \" + str(httpError.code))\n\n    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n    print(httpError.info())\n\n    print(json.loads(httpError.read()))\n    return\n\n\ndef saveBlobToFile(blobUrl, resultsLabel):\n    output_file = \"myresults.csv\" # Replace this with the location you would like to use for your output file\n    print(\"Reading the result from \" + blobUrl)\n    try:\n        # If you are using Python 3+, replace urllib2 with urllib.request in the following code\n        response = urllib.request.urlopen(blobUrl)\n    except urllib.request.HTTPError:\n        printHttpError(urllib.HTTPError)\n        return\n\n    with open(output_file, \"w+\") as f:\n        f.write(response.read())\n    print(resultsLabel + \" have been written to the file \" + output_file)\n    return\n\n\ndef processResults(result):\n\n\n    first = True\n    results = result[\"Results\"]\n    for outputName in results:\n        result_blob_location = results[outputName]\n        sas_token = result_blob_location[\"SasBlobToken\"]\n        base_url = result_blob_location[\"BaseLocation\"]\n        relative_url = result_blob_location[\"RelativeLocation\"]\n\n        print(\"The results for \" + outputName + \" are available at the following Azure Storage location:\")\n        print(\"BaseLocation: \" + base_url)\n        print(\"RelativeLocation: \" + relative_url)\n        print(\"SasBlobToken: \" + sas_token)\n\n\n        if (first):\n            first = False\n            url3 = base_url + relative_url + sas_token\n            saveBlobToFile(url3, \"The results for \" + outputName)\n    return\n\n\n\ndef uploadFileToBlob(input_file, input_blob_name, storage_container_name, storage_account_name, storage_account_key):\n    blob_service = asb.BlockBlobService(account_name=storage_account_name, account_key=storage_account_key)\n\n    print(\"Uploading the input to blob storage...\")\n    data_to_upload = open(input_file, \"r\").read()\n    blob_service.put_blob(storage_container_name, input_blob_name, data_to_upload, x_ms_blob_type=\"BlockBlob\")\n\ndef invokeBatchExecutionService():\n    storage_account_name = \"storage1\" # Replace this with your Azure Storage Account name\n    storage_account_key = \"kOveEtQMoP5zbUGfFR47\" # Replace this with your Azure Storage Key\n    storage_container_name = \"input\" # Replace this with your Azure Storage Container name\n    connection_string = \"DefaultEndpointsProtocol=https;AccountName=\" + storage_account_name + \";AccountKey=\" + storage_account_key #\"DefaultEndpointsProtocol=https;AccountName=mayatostorage1;AccountKey=aOYA2P5VQPR3ZQCl+aWhcGhDRJhsR225teGGBKtfXWwb2fNEo0CrhlwGWdfbYiBTTXPHYoKZyMaKuEAU8A\/Fzw==;EndpointSuffix=core.windows.net\"\n    api_key = \"5wUaln7n99rt9k+enRLG2OrhSsr9VLeoCfh0q3mfYo27hfTCh32f10PsRjJtuA==\" # Replace this with the API key for the web service\n    url = \"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/306bc1f050\/services\/61670382104542bc9533a920830b263c\/jobs\" #\"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/services\/61670382104542bc9533a920830b263c\/jobs\/job_id\/start?api-version=2.0\"\n\n\n\n    uploadFileToBlob(r\"C:\\Users\\Alex\\Desktop\\16_da.csv\", # Replace this with the location of your input file\n                     \"input1datablob.csv\", # Replace this with the name you would like to use for your Azure blob; this needs to have the same extension as the input file \n                     storage_container_name, storage_account_name, storage_account_key)\n\n    payload =  {\n\n        \"Inputs\": {\n\n            \"input1\": { \"ConnectionString\": connection_string, \"RelativeLocation\": \"\/\" + storage_container_name + \"\/input1datablob.csv\" },\n        },     \n\n        \"Outputs\": {\n\n            \"output1\": { \"ConnectionString\": connection_string, \"RelativeLocation\": \"\/\" + storage_container_name + \"\/output1results.csv\" },\n        },\n        \"GlobalParameters\": {\n}\n    }\n\n    body = str.encode(json.dumps(payload))\n    headers = { \"Content-Type\":\"application\/json\", \"Authorization\":(\"Bearer \" + api_key)}\n    print(\"Submitting the job...\")\n\n    # If you are using Python 3+, replace urllib2 with urllib.request in the following code\n\n    # submit the job\n    req = urllib.request.Request(url + \"?api-version=2.0\", body, headers)\n    try:\n        response = urllib.request.urlopen(req)\n    except urllib.request.HTTPError:\n        printHttpError(urllib.HTTPError)\n        return\n\n    result = response.read()\n    job_id = result[1:-1] # remove the enclosing double-quotes\n    print(\"Job ID: \" + job_id)\n\n\n    # If you are using Python 3+, replace urllib2 with urllib.request in the following code\n    # start the job\n    print(\"Starting the job...\")\n    req = urllib.request.Request(url + \"\/\" + job_id + \"\/start?api-version=2.0\", \"\", headers)\n    try:\n        response = urllib.request.urlopen(req)\n    except urllib.request.HTTPError:\n        printHttpError(urllib.HTTPError)\n        return\n\n    url2 = url + \"\/\" + job_id + \"?api-version=2.0\"\n\n    while True:\n        print(\"Checking the job status...\")\n        # If you are using Python 3+, replace urllib2 with urllib.request in the follwing code\n        req = urllib.request.Request(url2, headers = { \"Authorization\":(\"Bearer \" + api_key) })\n\n        try:\n            response = urllib.request.urlopen(req)\n        except urllib.request.HTTPError:\n            printHttpError(urllib.HTTPError)\n            return    \n\n        result = json.loads(response.read())\n        status = result[\"StatusCode\"]\n        if (status == 0 or status == \"NotStarted\"):\n            print(\"Job \" + job_id + \" not yet started...\")\n        elif (status == 1 or status == \"Running\"):\n            print(\"Job \" + job_id + \" running...\")\n        elif (status == 2 or status == \"Failed\"):\n            print(\"Job \" + job_id + \" failed!\")\n            print(\"Error details: \" + result[\"Details\"])\n            break\n        elif (status == 3 or status == \"Cancelled\"):\n            print(\"Job \" + job_id + \" cancelled!\")\n            break\n        elif (status == 4 or status == \"Finished\"):\n            print(\"Job \" + job_id + \" finished!\")\n\n            processResults(result)\n            break\n        time.sleep(1) # wait one second\n    return\n\ninvokeBatchExecutionService()\n<\/code><\/pre>\n\n<p>EDIT 2: The above issue has been resolved thanks to jon and the csv gets uploaded in blob storage.<\/p>\n\n<p>However now there is an HTTPError, when the job gets submitted in Line 130:<\/p>\n\n<pre><code>   raise HTTPError(req.full_url, code, msg, hdrs, fp)  HTTPError: Bad Request\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1530205297260,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1530279219952,
        "Question_score":0.0,
        "Question_view_count":499.0,
        "Answer_body":"<p>I think the code they give may be pretty old at this point.<\/p>\n\n<p>The <a href=\"https:\/\/pypi.org\/project\/azure-storage-blob\/#history\" rel=\"nofollow noreferrer\">latest version<\/a> of <code>azure.storage.blob<\/code> is 1.3. So perhaps a <code>pip install azure.storage.blob --update<\/code> or simply uninstalling and reinstalling would help.<\/p>\n\n<p>Once you got the latest version, try using the <code>create_blob_from_text<\/code> method to load the file to your storage container.<\/p>\n\n<pre><code>from azure.storage.blob import BlockBlobService\n\nblobService = BlockBlobService(account_name=\"accountName\", account_key=\"accountKey)\n\nblobService.create_blob_from_text(\"containerName\", \"fileName\", csv_file)\n<\/code><\/pre>\n\n<p>Hope that works to help lead you down the right path, but if not we can work through it. :)<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51088145",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1530266278528,
        "Question_original_content":"azur python sdk machin learn studio web servic batch execut snippet typeerror issu resolv read scroll edit try access web servic deploi studio batch execut sampl code python page http studio net apihelp workspac bcfbacdbadbccccab webservic eeddecaeabfdcd endpoint bcabc job fix issu accord question replac blobservic blobblockservic http studio net apihelp workspac bcfbacdbadbccccab webservic eeddecaeabfdcd endpoint bcabc job enter api kei contain url account kei account accord instruct todai code snippet outdat receiv differ error file user alex desktop script batchexecut line uploadfiletoblob blob servic asb blockblobservic account storag account account kei storag account kei file user alex anaconda lib site packag azur storag blob blockblobservic line init file user alex anaconda lib site packag azur storag blob baseblobservic line init typeerror servic paramet got unexpect keyword argument token credenti notic instal azur sdk python pip follow warn end process instal success azur storag queue requir azur storag common azur storag common incompat azur storag file requir azur storag common azur storag common incompat azur storag blob requir azur storag common azur storag common incompat latest document python sdk word token credenti contain http media readthedoc org pdf azur storag latest azur storag pdf clue go wrong instal type error token credenti pop execut know instal necessari version azur storag common azur storag blob edit code reproduc chang kei post work assum input present local file web servic accept input upload file azur blob need azur storag account be process data blob result written azur blob download output blob local file note need download instal azur sdk python http azur microsoft com document articl python instal import urllib python import urllib instead urllib import json import time import azur storag blob asb replac blobservic blobblockservic def printhttperror httperror print request fail statu code str httperror code print header includ requert timestamp us debug failur print httperror info print json load httperror read return def saveblobtofil bloburl resultslabel output file myresult csv replac locat like us output file print read result bloburl try python replac urllib urllib request follow code respons urllib request urlopen bloburl urllib request httperror printhttperror urllib httperror return open output file write respons read print resultslabel written file output file return def processresult result true result result result outputnam result result blob locat result outputnam sa token result blob locat sasblobtoken base url result blob locat baseloc rel url result blob locat relativeloc print result outputnam avail follow azur storag locat print baseloc base url print relativeloc rel url print sasblobtoken sa token fals url base url rel url sa token saveblobtofil url result outputnam return def uploadfiletoblob input file input blob storag contain storag account storag account kei blob servic asb blockblobservic account storag account account kei storag account kei print upload input blob storag data upload open input file read blob servic blob storag contain input blob data upload blob type blockblob def invokebatchexecutionservic storag account storag replac azur storag account storag account kei koveetqmopzbugffr replac azur storag kei storag contain input replac azur storag contain connect string defaultendpointsprotocol http accountnam storag account accountkei storag account kei defaultendpointsprotocol http accountnam mayatostorag accountkei aoyapvqprzqcl awhcghdrjhsrteggbktfxwwbfneocrhlwgwdfbyibttxphyokzymakueaua fzw endpointsuffix core window net api kei wualnnrtk enrlgorhssrvleocfhqmfyohftchfpsrjjtua replac api kei web servic url http ussouthcentr servic net workspac bcf servic bcabc job http ussouthcentr servic net workspac bcfbacdbadbccccab servic bcabc job job start api version uploadfiletoblob user alex desktop csv replac locat input file inputdatablob csv replac like us azur blob need extens input file storag contain storag account storag account kei payload input input connectionstr connect string relativeloc storag contain inputdatablob csv output output connectionstr connect string relativeloc storag contain outputresult csv globalparamet bodi str encod json dump payload header content type applic json author bearer api kei print submit job python replac urllib urllib request follow code submit job req urllib request request url api version bodi header try respons urllib request urlopen req urllib request httperror printhttperror urllib httperror return result respons read job result remov enclos doubl quot print job job python replac urllib urllib request follow code start job print start job req urllib request request url job start api version header try respons urllib request urlopen req urllib request httperror printhttperror urllib httperror return url url job api version true print check job statu python replac urllib urllib request follw code req urllib request request url header author bearer api kei try respons urllib request urlopen req urllib request httperror printhttperror urllib httperror return result json load respons read statu result statuscod statu statu notstart print job job start elif statu statu run print job job run elif statu statu fail print job job fail print error detail result detail break elif statu statu cancel print job job cancel break elif statu statu finish print job job finish processresult result break time sleep wait second return invokebatchexecutionservic edit issu resolv thank jon csv get upload blob storag httperror job get submit line rais httperror req url code msg hdr httperror bad request",
        "Question_preprocessed_content":"azur python sdk machin learn studio web servic batch execut snippet typeerror issu resolv read scroll edit try access web servic deploi studio batch code python page fix issu accord question enter url accord instruct todai code snippet outdat receiv differ error notic instal azur sdk python pip follow warn end process latest document python sdk clue go wrong instal pop execut know instal necessari version edit code edit issu resolv thank jon csv get upload blob storag httperror job get submit line",
        "Question_gpt_summary_original":"The user is encountering challenges while trying to access a Web Service deployed via Azure Machine Learning Studio using the Batch Execution-Sample Code for Python. The user has already fixed one issue but is now facing a different error related to the 'token_credential'. The user is also receiving warnings during the installation of the Azure SDK for Python. The user is seeking help to resolve the installation issue and the error related to 'token_credential'. Additionally, the user is facing an HTTPError when the job gets submitted.",
        "Question_gpt_summary":"user encount challeng try access web servic deploi studio batch execut sampl code python user fix issu face differ error relat token credenti user receiv warn instal azur sdk python user seek help resolv instal issu error relat token credenti addition user face httperror job get submit",
        "Answer_original_content":"think code pretti old point latest version azur storag blob pip instal azur storag blob updat simpli uninstal reinstal help got latest version try creat blob text method load file storag contain azur storag blob import blockblobservic blobservic blockblobservic account accountnam account kei accountkei blobservic creat blob text containernam filenam csv file hope work help lead right path work",
        "Answer_preprocessed_content":"think code pretti old point latest version simpli uninstal reinstal help got latest version try method load file storag contain hope work help lead right path work",
        "Answer_gpt_summary_original":"Solutions provided:\n- Update the version of 'azure.storage.blob' using 'pip install azure.storage.blob --update' or uninstalling and reinstalling it.\n- Use the 'create_blob_from_text' method to load the file to the storage container.",
        "Answer_gpt_summary":"solut provid updat version azur storag blob pip instal azur storag blob updat uninstal reinstal us creat blob text method load file storag contain"
    },
    {
        "Question_title":"Tracking Data Provenance with DVC",
        "Question_body":"<p>I recently discovered DVC and am looking to replace my current shell script-based approach for downloading source datasets and building derived datasets with DVC. In my current process, I have a clear record of data provenance as my scripted pipelines begin with downloads of the source datasets from the web.<\/p>\n<p>The question I have is the following: does DVC provide functionality that allows me to capture data provenance somehow? Can I record the URL from which the data was originally sourced and bind that to metadata associated with the data file(s)? Or will I need to maintain scripts that allow me to easily reacquire the data from the web? In the documentation, it seems like the story begins with the source data already in hand.<\/p>\n<p>I would love to have functionality that allows me to easily reacquire the source data from the web if needed and verify that indeed the dataset is equivalent to the original form that was used in the original pipeline development (via hash comparison).<\/p>\n<p>As of yet, I\u2019m not quite seeing how I would accomplish this with existing DVC commands. Any pointers would be greatly appreciated!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1652228110629,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":170.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/diehl\">@diehl<\/a> , sorry for late response.<\/p>\n<p>If I understand your use case correctly, it looks like you might be interested in using <code>dvc import<\/code> (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/import\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/command-reference\/import<\/a>) and\/or <code>dvc import-url<\/code> (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/import-url\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/command-reference\/import-url<\/a>)<\/p>. <p>Hi <a class=\"mention\" href=\"\/u\/daavoo\">@daavoo<\/a>, No worries!<\/p>\n<p>That sounds on point. Thank you!<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/tracking-data-provenance-with-dvc\/1186",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-18T10:50:48.093Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/diehl\">@diehl<\/a> , sorry for late response.<\/p>\n<p>If I understand your use case correctly, it looks like you might be interested in using <code>dvc import<\/code> (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/import\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/command-reference\/import<\/a>) and\/or <code>dvc import-url<\/code> (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/import-url\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/command-reference\/import-url<\/a>)<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-21T18:22:52.060Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/daavoo\">@daavoo<\/a>, No worries!<\/p>\n<p>That sounds on point. Thank you!<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"track data proven recent discov look replac current shell script base approach download sourc dataset build deriv dataset current process clear record data proven script pipelin begin download sourc dataset web question follow provid function allow captur data proven record url data origin sourc bind metadata associ data file need maintain script allow easili reacquir data web document like stori begin sourc data hand love function allow easili reacquir sourc data web need verifi dataset equival origin form origin pipelin develop hash comparison see accomplish exist command pointer greatli appreci",
        "Question_preprocessed_content":"track data proven recent discov look replac current shell approach download sourc dataset build deriv dataset current process clear record data proven script pipelin begin download sourc dataset web question follow provid function allow captur data proven record url data origin sourc bind metadata associ data file need maintain script allow easili reacquir data web document like stori begin sourc data hand love function allow easili reacquir sourc data web need verifi dataset equival origin form origin pipelin develop see accomplish exist command pointer greatli appreci",
        "Question_gpt_summary_original":"The user is facing challenges in using DVC to replace their current shell script-based approach for downloading source datasets and building derived datasets. They are unsure if DVC provides functionality to capture data provenance, such as recording the URL from which the data was originally sourced and binding it to metadata associated with the data file(s). The user would like to have functionality that allows them to easily reacquire the source data from the web if needed and verify that the dataset is equivalent to the original form that was used in the original pipeline development. They are seeking guidance on how to accomplish this with existing DVC commands.",
        "Question_gpt_summary":"user face challeng replac current shell script base approach download sourc dataset build deriv dataset unsur provid function captur data proven record url data origin sourc bind metadata associ data file user like function allow easili reacquir sourc data web need verifi dataset equival origin form origin pipelin develop seek guidanc accomplish exist command",
        "Answer_original_content":"diehl sorri late respons understand us case correctli look like interest import http org doc command refer import import url http org doc command refer import url daavoo worri sound point thank",
        "Answer_preprocessed_content":"sorri late respons understand us case correctli look like interest worri sound point thank",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are to use the DVC commands \"dvc import\" and \"dvc import-url\" to capture data provenance and record the URL from which the data was originally sourced. These commands can also be used to easily reacquire the source data from the web if needed and verify that the dataset is equivalent to the original form used in the pipeline development.",
        "Answer_gpt_summary":"possibl solut mention discuss us command import import url captur data proven record url data origin sourc command easili reacquir sourc data web need verifi dataset equival origin form pipelin develop"
    },
    {
        "Question_title":"ERROR: configuration error - GDrive remote auth failed with credentials in 'GDRIVE_CREDENTIALS_DATA'",
        "Question_body":"<p>I am not sure what is going wrong here. I copy pasted the credentials files from my Gdrive service account into the secret <code>GDRIVE_CREDENTIALS_DATA<\/code>. I can see that the variable is populated when I run GH action. I tried the credentials on two other computers, but not with the environment variable.<\/p>\n<blockquote>\n<p>ERROR: configuration error - GDrive remote auth failed with credentials in \u2018GDRIVE_CREDENTIALS_DATA\u2019.<\/p>\n<\/blockquote>\n<pre><code class=\"lang-auto\">name: auto-testing\non: [push]\njobs:\n  run:\n    runs-on: [ubuntu-latest]\n    steps:\n      - uses: actions\/checkout@v2\n      - uses: actions\/setup-python@v2\n      - uses: iterative\/setup-dvc@v1\n      - name: sanity-check\n        run: |\n          dvc pull\n          pip install -r requirements.txt\n          python src\/test.py\n        env:\n          repo_token: ${{ secrets.GITHUB_TOKEN }}\n          GDRIVE_CREDENTIALS_DATA : ${{ secrets.GDRIVE_CREDENTIALS_DATA }}   \n<\/code><\/pre>",
        "Question_answer_count":13,
        "Question_comment_count":0,
        "Question_creation_time":1658153900112,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":406.0,
        "Answer_body":"<p>Hi,<br>\nwhat Pydrive2 version are you using? If running <code>1.10.2<\/code>, could you try downgrading to <code>1.10.1<\/code>?<\/p>. <p><code>dvc pull<\/code> runs before I make any changes to the container i.e. before I install dependencies. So, it should be the default version. How can I find out which version is installed?<\/p>\n<p>when I add<\/p>\n<p><code>pip list | grep pydrive<\/code><\/p>\n<p>I get an error.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/d58071514180edfe7cccfe559caf2704b7c8e299.png\" data-download-href=\"\/uploads\/short-url\/usIT4E90ZoRCU73aZA6qxOOALfX.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/d58071514180edfe7cccfe559caf2704b7c8e299.png\" alt=\"image\" data-base62-sha1=\"usIT4E90ZoRCU73aZA6qxOOALfX\" width=\"690\" height=\"324\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/d58071514180edfe7cccfe559caf2704b7c8e299_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">757\u00d7356 4.34 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>. <p>Sorry, I missed that you were using the <code>setup-dvc<\/code> action, please disregard my previous comment as it is not relevant to the issue.<\/p>\n<p>Can you get <code>dvc pull<\/code> working locally when setting <code>GDRIVE_CREDENTIALS_DATA<\/code> env variable?<\/p>. <p>Not sure how to do that. The content of the json file is not really suited for a normal environment variable. Because it is JSON. Something like this?<\/p>\n<p>GDRIVE_CREDENTIALS_DATA=\u2019<br>\n\u201c{\u2026}\u201d<br>\n\u2019<\/p>\n<p>?<\/p>\n<p>I read somewhere it is better to use the action rather than the docker file. Not sure what is recommended.<\/p>. <p>Setting the env variable to the content of the json file should be enough, see this guide: <a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote#using-service-accounts\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Setup Google Drive Remote<\/a><\/p>\n<p><code>GDRIVE_CREDENTIALS_DATA='{...}'<\/code> (no double quotes).<\/p>. <p>Thanks, I know that link. It did not really help me to set it up so far. When I do what you showed I get an error in the console:<\/p>\n<p>-bash: printf: `r\u2019: invalid format character<\/p>\n<p>probably from this entry<\/p>\n<p><code>\"client_x509_cert_url\": \"https:\/\/www.googleapis.com\/robot\/v1\/metadata\/x509\/gh-398%40lrg.iam.gserviceaccount.com\"<\/code><\/p>\n<p>When I change the $40 to @ it works locally, but not on GH.<\/p>. <p>Nope, still not working.<\/p>. <p>Is there anywhere an example file that works??<\/p>. <pre><code class=\"lang-auto\">---\nGDRIVE_CREDENTIALS_DATA: *** *** *** *** *** *** *** *** *** *** *** ***\n---\nERROR: configuration error - GDrive remote auth failed with credentials in 'GDRIVE_CREDENTIALS_DATA'.\nBackup first, remove or fix them, and run again.\nIt should do auth again and refresh the credentials.\nDetails:: name: drive  version: v2\nERROR: GDrive remote auth failed with credentials in 'GDRIVE_CREDENTIALS_DATA'.\nBackup first, remove or fix them, and run again.\nIt should do auth again and refresh the credentials.\nDetails:\nLearn more about configuration settings at &lt;https:\/\/man.dvc.org\/remote\/modify&gt;.\nError: Process completed with exit code 251.```<\/code><\/pre>. <p>I\u2019m a bit confused. Could you get it working locally? Doing so in a new git\/dvc repo would als help rule out other issues.<\/p>\n<p>Something like:<\/p>\n<pre><code class=\"lang-auto\">mkdir gdrive_test\ncd gdrive_test\ngit init\ndvc init\ngit add . &amp;&amp; git commit -m \"initialize dvc\"\ndvc remote add gdrive  gdrive:\/\/&lt;replace your path here&gt;\ndvc remote modify gdrive gdrive_use_service_account true\ntouch examplefile\ndvc add examplefile\ngit commit -m \"add example file\"\nexport GDRIVE_CREDENTIALS_DATA='{...}' # replace credentials here\ndvc push\n<\/code><\/pre>. <p>Yes, locally it works. But not on GH.<\/p>\n<aside class=\"onebox githubissue\" data-onebox-src=\"https:\/\/github.com\/iterative\/dvc\/issues\/7949\">\n  <header class=\"source\">\n\n      <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/7949\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com\/iterative\/dvc<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"><\/path><\/svg>\n  <\/div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/7949\" target=\"_blank\" rel=\"noopener nofollow ugc\">gdrive: raises unexpected error - name: drive version: v2 (again)<\/a>\n    <\/h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2022-06-29\" data-time=\"18:36:55\" data-timezone=\"UTC\">06:36PM - 29 Jun 22 UTC<\/span>\n      <\/div>\n\n\n      <div class=\"user\">\n        <a href=\"https:\/\/github.com\/BrettRyland\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n          <img alt=\"BrettRyland\" src=\"https:\/\/avatars.githubusercontent.com\/u\/18440126?v=4\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          BrettRyland\n        <\/a>\n      <\/div>\n    <\/div>\n\n    <div class=\"labels\">\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          p1-important\n        <\/span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          build\n        <\/span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          fs: gdrive\n        <\/span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          release-blocker\n        <\/span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          bug\n        <\/span>\n    <\/div>\n  <\/div>\n<\/div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\"># Bug Report\n\n&lt;!--\n## Issue name\n\nIssue names must follow the pattern `comm<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026<\/a><\/span><span class=\"excerpt hidden\">and: description` where the command is the dvc command that you are trying to run. The description should describe the consequence of the bug. \n\nExample: `repro: doesn't detect input changes`\n--&gt;\n\n## Description\n\nAs reported [here](https:\/\/github.com\/iterative\/dvc\/issues\/5618#issuecomment-1155019515), the latest version (2.11.0) of dvc on Ubuntu 22.04 LTS still has the bug reported in issue [#5618](https:\/\/github.com\/iterative\/dvc\/issues\/5618).\nDowngrading dvc to 2.10.2 allows the push to work properly.\n\n### Reproduce\n\nOn an existing repository using gdrive, add some files then try to push them.\n```console\nbrett@br-workhorse:~\/repos\/arctic-ice$ dvc status\nmodels.dvc:                                                                                                                                                                                                                                                                                           \n        changed outs:\n                modified:           models\n\nbrett@br-workhorse:~\/repos\/arctic-ice$ dvc add models\n100% Adding...|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|1\/1 [00:01,  1.52s\/file]\n\nbrett@br-workhorse:~\/repos\/arctic-ice$ dvc push                                                                                                                                                                                                                                                       \nERROR: unexpected error - name: drive  version: v2                                                                                                                                                                                                                                                    \n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n```\n\nWith extra verbosity:\n```console\nbrett@br-workhorse:~\/repos\/arctic-ice$ dvc push -vv\n2022-06-29 19:59:18,216 TRACE: Namespace(all_branches=False, all_commits=False, all_tags=False, cd='.', cmd='push', cprofile=False, cprofile_dump=None, func=&lt;class 'dvc.commands.data_sync.CmdDataPush'&gt;, glob=False, instrument=False, instrument_open=False, jobs=None, parser=DvcParser(prog='dvc', usage=None, description='Data Version Control', formatter_class=&lt;class 'argparse.RawTextHelpFormatter'&gt;, conflict_handler='error', add_help=False), pdb=False, quiet=0, recursive=False, remote=None, run_cache=False, targets=[], verbose=2, version=None, viztracer=False, viztracer_depth=None, with_deps=False, yappi=False)\n2022-06-29 19:59:18,399 TRACE:    16.49 ms in collecting stages from \/home\/brett\/repos\/arctic-ice\n2022-06-29 19:59:18,405 TRACE:     6.02 ms in collecting stages from \/home\/brett\/repos\/arctic-ice\/RadarMask\n2022-06-29 19:59:18,406 TRACE:    18.37 mks in collecting stages from \/home\/brett\/repos\/arctic-ice\/RadarMask\/RadarMask\n2022-06-29 19:59:18,420 DEBUG: Preparing to transfer data from '\/home\/brett\/repos\/arctic-ice\/.dvc\/cache' to '1uTdQZ6FntcI-JtX0ofC4LYPfFM37oZp0'\n2022-06-29 19:59:18,420 DEBUG: Preparing to collect status from '1uTdQZ6FntcI-JtX0ofC4LYPfFM37oZp0'\n2022-06-29 19:59:18,420 DEBUG: Collecting status from '1uTdQZ6FntcI-JtX0ofC4LYPfFM37oZp0'\n2022-06-29 19:59:18,423 DEBUG: Querying 11 hashes via object_exists                                                                                                                                                                                                                                   \n2022-06-29 19:59:18,635 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,637 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,638 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,639 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,641 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,642 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,642 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,643 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,644 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,645 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,645 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,647 ERROR: unexpected error - name: drive  version: v2                                                                                                                                                                                                                            \n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"dvc\/cli\/__init__.py\", line 185, in main\n  File \"dvc\/cli\/command.py\", line 22, in do_run\n  File \"dvc\/commands\/data_sync.py\", line 58, in run\n  File \"dvc\/repo\/__init__.py\", line 49, in wrapper\n  File \"dvc\/repo\/push.py\", line 68, in push\n  File \"dvc\/data_cloud.py\", line 109, in push\n  File \"dvc\/data_cloud.py\", line 88, in transfer\n  File \"dvc_data\/transfer.py\", line 159, in transfer\n  File \"dvc_data\/status.py\", line 179, in compare_status\n  File \"dvc_data\/status.py\", line 136, in status\n  File \"dvc_data\/status.py\", line 43, in _indexed_dir_hashes\n  File \"tqdm\/std.py\", line 1195, in __iter__\n  File \"dvc_objects\/db.py\", line 355, in list_hashes_exists\n  File \"concurrent\/futures\/_base.py\", line 611, in result_iterator\n  File \"concurrent\/futures\/_base.py\", line 439, in result\n  File \"concurrent\/futures\/_base.py\", line 388, in __get_result\n  File \"concurrent\/futures\/thread.py\", line 57, in run\n  File \"dvc_objects\/fs\/base.py\", line 263, in exists\n  File \"pydrive2\/fs\/spec.py\", line 225, in exists\n  File \"pydrive2\/fs\/spec.py\", line 192, in _get_item_id\n  File \"pydrive2\/fs\/spec.py\", line 174, in _path_to_item_ids\n  File \"pydrive2\/fs\/spec.py\", line 170, in _get_cached_item_ids\n  File \"funcy\/objects.py\", line 50, in __get__\n  File \"funcy\/objects.py\", line 28, in __get__\n  File \"pydrive2\/fs\/spec.py\", line 82, in _ids_cache\n  File \"pydrive2\/fs\/spec.py\", line 132, in _gdrive_list\n  File \"funcy\/objects.py\", line 28, in __get__\n  File \"pydrive2\/fs\/spec.py\", line 100, in _list_params\n  File \"funcy\/decorators.py\", line 45, in wrapper\n  File \"funcy\/flow.py\", line 127, in retry\n  File \"funcy\/decorators.py\", line 66, in __call__\n  File \"pydrive2\/fs\/spec.py\", line 121, in _gdrive_shared_drive_id\n  File \"pydrive2\/auth.py\", line 64, in _decorated\n  File \"pydrive2\/auth.py\", line 671, in Authorize\n  File \"googleapiclient\/_helpers.py\", line 130, in positional_wrapper\n  File \"googleapiclient\/discovery.py\", line 287, in build\n  File \"googleapiclient\/discovery.py\", line 404, in _retrieve_discovery_doc\ngoogleapiclient.errors.UnknownApiNameOrVersion: name: drive  version: v2\n------------------------------------------------------------\n2022-06-29 19:59:19,332 DEBUG: [Errno 95] no more link types left to try out: [Errno 95] Operation not supported\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"dvc\/cli\/__init__.py\", line 185, in main\n  File \"dvc\/cli\/command.py\", line 22, in do_run\n  File \"dvc\/commands\/data_sync.py\", line 58, in run\n  File \"dvc\/repo\/__init__.py\", line 49, in wrapper\n  File \"dvc\/repo\/push.py\", line 68, in push\n  File \"dvc\/data_cloud.py\", line 109, in push\n  File \"dvc\/data_cloud.py\", line 88, in transfer\n  File \"dvc_data\/transfer.py\", line 159, in transfer\n  File \"dvc_data\/status.py\", line 179, in compare_status\n  File \"dvc_data\/status.py\", line 136, in status\n  File \"dvc_data\/status.py\", line 43, in _indexed_dir_hashes\n  File \"tqdm\/std.py\", line 1195, in __iter__\n  File \"dvc_objects\/db.py\", line 355, in list_hashes_exists\n  File \"concurrent\/futures\/_base.py\", line 611, in result_iterator\n  File \"concurrent\/futures\/_base.py\", line 439, in result\n  File \"concurrent\/futures\/_base.py\", line 388, in __get_result\n  File \"concurrent\/futures\/thread.py\", line 57, in run\n  File \"dvc_objects\/fs\/base.py\", line 263, in exists\n  File \"pydrive2\/fs\/spec.py\", line 225, in exists\n  File \"pydrive2\/fs\/spec.py\", line 192, in _get_item_id\n  File \"pydrive2\/fs\/spec.py\", line 174, in _path_to_item_ids\n  File \"pydrive2\/fs\/spec.py\", line 170, in _get_cached_item_ids\n  File \"funcy\/objects.py\", line 50, in __get__\n  File \"funcy\/objects.py\", line 28, in __get__\n  File \"pydrive2\/fs\/spec.py\", line 82, in _ids_cache\n  File \"pydrive2\/fs\/spec.py\", line 132, in _gdrive_list\n  File \"funcy\/objects.py\", line 28, in __get__\n  File \"pydrive2\/fs\/spec.py\", line 100, in _list_params\n  File \"funcy\/decorators.py\", line 45, in wrapper\n  File \"funcy\/flow.py\", line 127, in retry\n  File \"funcy\/decorators.py\", line 66, in __call__\n  File \"pydrive2\/fs\/spec.py\", line 121, in _gdrive_shared_drive_id\n  File \"pydrive2\/auth.py\", line 64, in _decorated\n  File \"pydrive2\/auth.py\", line 671, in Authorize\n  File \"googleapiclient\/_helpers.py\", line 130, in positional_wrapper\n  File \"googleapiclient\/discovery.py\", line 287, in build\n  File \"googleapiclient\/discovery.py\", line 404, in _retrieve_discovery_doc\ngoogleapiclient.errors.UnknownApiNameOrVersion: name: drive  version: v2\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"dvc_objects\/fs\/generic.py\", line 68, in _try_links\n  File \"dvc_objects\/fs\/generic.py\", line 28, in _link\n  File \"dvc_objects\/fs\/base.py\", line 282, in reflink\n  File \"dvc_objects\/fs\/implementations\/local.py\", line 157, in reflink\n  File \"dvc_objects\/fs\/system.py\", line 115, in reflink\n  File \"dvc_objects\/fs\/system.py\", line 101, in _reflink_linux\nOSError: [Errno 95] Operation not supported\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"dvc_objects\/fs\/generic.py\", line 127, in _test_link\n  File \"dvc_objects\/fs\/generic.py\", line 76, in _try_links\nOSError: [Errno 95] no more link types left to try out\n------------------------------------------------------------\n2022-06-29 19:59:19,333 DEBUG: Removing '\/home\/brett\/repos\/.cFRhX4rL87pLC64LbFJkgt.tmp'\n2022-06-29 19:59:19,333 DEBUG: Removing '\/home\/brett\/repos\/.cFRhX4rL87pLC64LbFJkgt.tmp'\n2022-06-29 19:59:19,333 DEBUG: Removing '\/home\/brett\/repos\/.cFRhX4rL87pLC64LbFJkgt.tmp'\n2022-06-29 19:59:19,333 DEBUG: Removing '\/home\/brett\/repos\/arctic-ice\/.dvc\/cache\/.LAtLtAtBaEGib7LXwXJKVF.tmp'\n2022-06-29 19:59:19,337 DEBUG: Version info for developers:\nDVC version: 2.11.0 (deb)\n---------------------------------\nPlatform: Python 3.8.3 on Linux-5.15.0-37-generic-x86_64-with-glibc2.14\nSupports:\n        azure (adlfs = 2022.4.0, knack = 0.9.0, azure-identity = 1.10.0),\n        gdrive (pydrive2 = 1.10.1),\n        gs (gcsfs = 2022.5.0),\n        hdfs (fsspec = 2022.5.0, pyarrow = 8.0.0),\n        webhdfs (fsspec = 2022.5.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.4.8),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.4.8),\n        s3 (s3fs = 2022.5.0, boto3 = 1.21.21),\n        ssh (sshfs = 2022.6.0),\n        oss (ossfs = 2021.8.0),\n        webdav (webdav4 = 0.9.7),\n        webdavs (webdav4 = 0.9.7)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/nvme1n1p1\nCaches: local\nRemotes: gdrive\nWorkspace directory: ext4 on \/dev\/nvme1n1p1\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2022-06-29 19:59:19,338 DEBUG: Analytics is enabled.\n2022-06-29 19:59:19,339 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp0h_rt9la']'\n2022-06-29 19:59:19,341 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp0h_rt9la']'\n```\n\n### Expected\n\nThe push to succeed without errors.\n\n### Environment information\n\n**Output of `dvc doctor`:**\n\n```console\nbrett@br-workhorse:~\/repos\/arctic-ice$ dvc doctor\nDVC version: 2.11.0 (deb)\n---------------------------------\nPlatform: Python 3.8.3 on Linux-5.15.0-37-generic-x86_64-with-glibc2.14\nSupports:\n        azure (adlfs = 2022.4.0, knack = 0.9.0, azure-identity = 1.10.0),\n        gdrive (pydrive2 = 1.10.1),\n        gs (gcsfs = 2022.5.0),\n        hdfs (fsspec = 2022.5.0, pyarrow = 8.0.0),\n        webhdfs (fsspec = 2022.5.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.4.8),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.4.8),\n        s3 (s3fs = 2022.5.0, boto3 = 1.21.21),\n        ssh (sshfs = 2022.6.0),\n        oss (ossfs = 2021.8.0),\n        webdav (webdav4 = 0.9.7),\n        webdavs (webdav4 = 0.9.7)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/nvme1n1p1\nCaches: local\nRemotes: gdrive\nWorkspace directory: ext4 on \/dev\/nvme1n1p1\nRepo: dvc, git\n```\n\nThis is running with python 3.10.4 in a virtual environment set up with `direnv`.\n```console\nbrett@br-workhorse:~\/repos\/arctic-ice$ python --version\nPython 3.10.4\n\nbrett@br-workhorse:~\/repos\/arctic-ice$ cat .envrc \nlayout python-venv\n```\n\n```console\nbrett@br-workhorse:~\/repos\/arctic-ice$ cat .direnv\/python-venv-3.10.4\/pyvenv.cfg \nhome = \/usr\/bin\ninclude-system-site-packages = false\nversion = 3.10.4\n\nbrett@br-workhorse:~\/repos\/arctic-ice$ which dvc\n\/usr\/bin\/dvc\n\nbrett@br-workhorse:~\/repos\/arctic-ice$ pip freeze -l\nasttokens==2.0.5\nbackcall==0.2.0\ncertifi==2021.10.8\ncharset-normalizer==2.0.12\ncycler==0.11.0\ndecorator==5.1.1\nexecuting==0.8.3\nfonttools==4.33.3\nidna==3.3\nimageio==2.18.0\nimgaug==0.4.0\nipython==8.2.0\njedi==0.18.1\nkiwisolver==1.4.2\nmatplotlib==3.5.1\nmatplotlib-inline==0.1.3\nmemory-profiler==0.60.0\nnetworkx==2.8\nnumpy==1.22.3\nopencv-contrib-python==4.5.5.64\nopencv-python==4.5.5.64\npackaging==21.3\nparso==0.8.3\npexpect==4.8.0\npickleshare==0.7.5\nPillow==9.1.0\nPillow-SIMD==8.3.2.post0\nprompt-toolkit==3.0.29\npsutil==5.9.0\nptyprocess==0.7.0\npure-eval==0.2.2\nPygments==2.12.0\npyparsing==3.0.8\npython-dateutil==2.8.2\nPyWavelets==1.3.0\nrequests==2.27.1\nscikit-image==0.19.2\nscipy==1.8.0\nShapely==1.8.1.post1\nsix==1.16.0\nstack-data==0.2.0\ntifffile==2022.4.26\ntorch==1.11.0\ntorchvision==0.12.0\ntqdm==4.64.0\ntraitlets==5.1.1\ntyping_extensions==4.2.0\nurllib3==1.26.9\nwcwidth==0.2.5\n```<\/span><\/p>\n  <\/div>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n. <p>Could you please try to set <code>gdrive_service_account_json_file_path<\/code> to some random value?<br>\nAccording to<\/p><aside class=\"onebox githubissue\" data-onebox-src=\"https:\/\/github.com\/iterative\/dvc\/issues\/6230\">\n  <header class=\"source\">\n\n      <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/6230\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com\/iterative\/dvc<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"><\/path><\/svg>\n  <\/div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/6230\" target=\"_blank\" rel=\"noopener nofollow ugc\">push: gdrive_service_account_json_file_path must be set for GDRIVE_CREDENTIALS_DATA to work<\/a>\n    <\/h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2021-06-25\" data-time=\"22:55:50\" data-timezone=\"UTC\">10:55PM - 25 Jun 21 UTC<\/span>\n      <\/div>\n\n\n      <div class=\"user\">\n        <a href=\"https:\/\/github.com\/0x2b3bfa0\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n          <img alt=\"0x2b3bfa0\" src=\"https:\/\/avatars.githubusercontent.com\/u\/11387611?v=4\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          0x2b3bfa0\n        <\/a>\n      <\/div>\n    <\/div>\n\n    <div class=\"labels\">\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/bug.png?v=12\" title=\"bug\" class=\"emoji\" alt=\"bug\" width=\"20\" height=\"20\"> bug\n        <\/span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          p2-medium\n        <\/span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          fs: gdrive\n        <\/span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          A: data-sync\n        <\/span>\n    <\/div>\n  <\/div>\n<\/div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">## Description\n\nThe `GDRIVE_CREDENTIALS_DATA` environment variable won't be ho<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026<\/a><\/span><span class=\"excerpt hidden\">nored unless the `gdrive_service_account_json_file_path` configuration option is set beforehand. In most of the habitual use cases, this requirement doesn't make too much sense from the user standpoint.\n\nIs this the recommended way of passing Google Drive (Service Account) credentials through an environment variable?\n\nAdditionally, after reading the pertaining [documentation](https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote), I don't understand why I can't use the `GDRIVE_CREDENTIALS_DATA` environment variable to pass the DVC-specific `.dvc\/tmp\/gdrive-user-credentials.json` file contents, instead of having to pass the raw service account file contents.\n\n### How to reproduce\n\nThe following GitHub Actions workflow provides a self-contained example to reproduce this issue. Uncommenting the only line prefixed with a hash `#` will make it work as expected.\n\n```yaml\non: push\njobs:\n  run:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: iterative\/setup-dvc@v1\n      - run: |\n          git init\n          dvc init\n          \n          dvc remote add origin gdrive:\/\/root\n          dvc remote modify origin --local gdrive_use_service_account true\n          # dvc remote modify origin --local gdrive_service_account_json_file_path \/dev\/null\n\n          date &gt; file\n          dvc add file\n\n          dvc push --verbose --remote origin\n        env:\n          GDRIVE_CREDENTIALS_DATA: ${{ secrets.ORIGINAL_SERVICE_ACCOUNT_JSON }}\n```\n\n_Note: the `${{ secrets.ORIGINAL_SERVICE_ACCOUNT_JSON }}` variable represents a GCP Service Account file in the standard JSON format provided by Google, not a DVC credentials JSON file._\n\n### Actual output\n\n```console\n$ dvc push --verbose --remote origin\n2021-06-25 22:04:09,030 ERROR: failed to push data to the cloud - To use service account, set `gdrive_service_account_json_file_path`, and optionally`gdrive_service_account_user_email` in DVC config\nLearn more at &lt;https:\/\/man.dvc.org\/remote\/modify&gt;\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"dvc\/command\/data_sync.py\", line 57, in run\n  File \"dvc\/repo\/__init__.py\", line 51, in wrapper\n  File \"dvc\/repo\/push.py\", line 44, in push\n  File \"dvc\/data_cloud.py\", line 77, in push\n  File \"dvc\/data_cloud.py\", line 38, in get_remote\n  File \"dvc\/data_cloud.py\", line 58, in _init_remote\n  File \"dvc\/remote\/__init__.py\", line 8, in get_remote\n  File \"dvc\/fs\/gdrive.py\", line 128, in __init__\n  File \"dvc\/fs\/gdrive.py\", line 155, in _validate_config\ndvc.exceptions.DvcException: To use service account, set `gdrive_service_account_json_file_path`, and optionally`gdrive_service_account_user_email` in DVC config\nLearn more at &lt;https:\/\/man.dvc.org\/remote\/modify&gt;\n2021-06-25 22:04:09,033 DEBUG: Analytics is enabled.\n------------------------------------------------------------\n2021-06-25 22:04:09,076 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp8tbexj0k']'\n2021-06-25 22:04:09,077 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp8tbexj0k']'\n```\n\n### Expected output\n\n```console\n$ dvc push --verbose --remote origin\n2021-06-25 22:17:58,441 DEBUG: Preparing to upload data to 'gdrive:\/\/root'\n2021-06-25 22:17:58,441 DEBUG: Preparing to collect status from gdrive:\/\/root\n2021-06-25 22:17:58,441 DEBUG: Collecting information from local cache...\n2021-06-25 22:17:58,441 DEBUG: Collecting information from remote cache...\n2021-06-25 22:17:58,441 DEBUG: Matched '0' indexed hashes\n2021-06-25 22:17:58,442 DEBUG: Querying 1 hashes via object_exists\n2021-06-25 22:17:58,672 DEBUG: GDrive remote auth with config '***'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/runner\/work\/test\/test\/.dvc\/tmp\/.5VsCA2nJs9ZQgeBZ8pWNAk.tmp', 'get_refresh_token': True, 'service_config': ***'client_user_email': None, 'client_json_file_path': '\/home\/runner\/work\/test\/test\/.dvc\/tmp\/.mpgSFHPn7NBgMe35nngenb.tmp'***'.\n2021-06-25 22:17:59,308 DEBUG: Uploading '.dvc\/cache\/5d\/afc8549edbdc83c20f11cbfde93cc4' to 'gdrive:\/\/root\/5d\/afc8549edbdc83c20f11cbfde93cc4'\n1 file pushed\n2021-06-25 22:18:01,059 DEBUG: Analytics is enabled.\n2021-06-25 22:18:01,113 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp0ocbbm6a']'\n2021-06-25 22:18:01,116 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp0ocbbm6a']'\n```\n\n### Environment information\n\n```console\n$ dvc doctor\nDVC version: 2.4.1 (deb)\n---------------------------------\nPlatform: Python 3.8.10 on Linux-5.8.0-1033-azure-x86_64-with-glibc2.7\nSupports: All remotes\nCache types: &lt;https:\/\/error.dvc.org\/no-dvc-cache&gt;\nCaches: local\nRemotes: gdrive\nWorkspace directory: ext4 on \/dev\/root\nRepo: dvc, git\n```\n\n### Additional information\n\nhttps:\/\/github.com\/iterative\/dvc\/blob\/4e792ae61c5927ab2e5f6a6914d985d43aa705b4\/dvc\/fs\/gdrive.py#L128\n\nhttps:\/\/github.com\/iterative\/dvc\/blob\/4e792ae61c5927ab2e5f6a6914d985d43aa705b4\/dvc\/fs\/gdrive.py#L149-L162\n\n### See also\n\n- https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote\n- https:\/\/discuss.dvc.org\/t\/cml-github-actions-google-drive-service-account\/795\/9\n- https:\/\/iterativeai.slack.com\/archives\/CB41NAL8H\/p1624015633145600\n- https:\/\/iterativeai.slack.com\/archives\/CNQ95CG1K\/p1624187829201500<\/span><\/p>\n  <\/div>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n. <p>Nope, that did not help. Thanks for the suggestion.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-configuration-error-gdrive-remote-auth-failed-with-credentials-in-gdrive-credentials-data\/1254",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-18T14:21:09.726Z",
                "Answer_body":"<p>Hi,<br>\nwhat Pydrive2 version are you using? If running <code>1.10.2<\/code>, could you try downgrading to <code>1.10.1<\/code>?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-18T14:31:34.319Z",
                "Answer_body":"<p><code>dvc pull<\/code> runs before I make any changes to the container i.e. before I install dependencies. So, it should be the default version. How can I find out which version is installed?<\/p>\n<p>when I add<\/p>\n<p><code>pip list | grep pydrive<\/code><\/p>\n<p>I get an error.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/d58071514180edfe7cccfe559caf2704b7c8e299.png\" data-download-href=\"\/uploads\/short-url\/usIT4E90ZoRCU73aZA6qxOOALfX.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/d58071514180edfe7cccfe559caf2704b7c8e299.png\" alt=\"image\" data-base62-sha1=\"usIT4E90ZoRCU73aZA6qxOOALfX\" width=\"690\" height=\"324\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/d58071514180edfe7cccfe559caf2704b7c8e299_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">757\u00d7356 4.34 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-18T14:39:56.220Z",
                "Answer_body":"<p>Sorry, I missed that you were using the <code>setup-dvc<\/code> action, please disregard my previous comment as it is not relevant to the issue.<\/p>\n<p>Can you get <code>dvc pull<\/code> working locally when setting <code>GDRIVE_CREDENTIALS_DATA<\/code> env variable?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-18T14:41:54.181Z",
                "Answer_body":"<p>Not sure how to do that. The content of the json file is not really suited for a normal environment variable. Because it is JSON. Something like this?<\/p>\n<p>GDRIVE_CREDENTIALS_DATA=\u2019<br>\n\u201c{\u2026}\u201d<br>\n\u2019<\/p>\n<p>?<\/p>\n<p>I read somewhere it is better to use the action rather than the docker file. Not sure what is recommended.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-18T14:47:41.654Z",
                "Answer_body":"<p>Setting the env variable to the content of the json file should be enough, see this guide: <a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote#using-service-accounts\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Setup Google Drive Remote<\/a><\/p>\n<p><code>GDRIVE_CREDENTIALS_DATA='{...}'<\/code> (no double quotes).<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-18T14:52:54.634Z",
                "Answer_body":"<p>Thanks, I know that link. It did not really help me to set it up so far. When I do what you showed I get an error in the console:<\/p>\n<p>-bash: printf: `r\u2019: invalid format character<\/p>\n<p>probably from this entry<\/p>\n<p><code>\"client_x509_cert_url\": \"https:\/\/www.googleapis.com\/robot\/v1\/metadata\/x509\/gh-398%40lrg.iam.gserviceaccount.com\"<\/code><\/p>\n<p>When I change the $40 to @ it works locally, but not on GH.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-18T15:01:21.804Z",
                "Answer_body":"<p>Nope, still not working.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-18T15:14:02.553Z",
                "Answer_body":"<p>Is there anywhere an example file that works??<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-18T15:20:50.695Z",
                "Answer_body":"<pre><code class=\"lang-auto\">---\nGDRIVE_CREDENTIALS_DATA: *** *** *** *** *** *** *** *** *** *** *** ***\n---\nERROR: configuration error - GDrive remote auth failed with credentials in 'GDRIVE_CREDENTIALS_DATA'.\nBackup first, remove or fix them, and run again.\nIt should do auth again and refresh the credentials.\nDetails:: name: drive  version: v2\nERROR: GDrive remote auth failed with credentials in 'GDRIVE_CREDENTIALS_DATA'.\nBackup first, remove or fix them, and run again.\nIt should do auth again and refresh the credentials.\nDetails:\nLearn more about configuration settings at &lt;https:\/\/man.dvc.org\/remote\/modify&gt;.\nError: Process completed with exit code 251.```<\/code><\/pre>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-19T12:42:31.168Z",
                "Answer_body":"<p>I\u2019m a bit confused. Could you get it working locally? Doing so in a new git\/dvc repo would als help rule out other issues.<\/p>\n<p>Something like:<\/p>\n<pre><code class=\"lang-auto\">mkdir gdrive_test\ncd gdrive_test\ngit init\ndvc init\ngit add . &amp;&amp; git commit -m \"initialize dvc\"\ndvc remote add gdrive  gdrive:\/\/&lt;replace your path here&gt;\ndvc remote modify gdrive gdrive_use_service_account true\ntouch examplefile\ndvc add examplefile\ngit commit -m \"add example file\"\nexport GDRIVE_CREDENTIALS_DATA='{...}' # replace credentials here\ndvc push\n<\/code><\/pre>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-19T16:44:08.695Z",
                "Answer_body":"<p>Yes, locally it works. But not on GH.<\/p>\n<aside class=\"onebox githubissue\" data-onebox-src=\"https:\/\/github.com\/iterative\/dvc\/issues\/7949\">\n  <header class=\"source\">\n\n      <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/7949\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com\/iterative\/dvc<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"><\/path><\/svg>\n  <\/div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/7949\" target=\"_blank\" rel=\"noopener nofollow ugc\">gdrive: raises unexpected error - name: drive version: v2 (again)<\/a>\n    <\/h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2022-06-29\" data-time=\"18:36:55\" data-timezone=\"UTC\">06:36PM - 29 Jun 22 UTC<\/span>\n      <\/div>\n\n\n      <div class=\"user\">\n        <a href=\"https:\/\/github.com\/BrettRyland\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n          <img alt=\"BrettRyland\" src=\"https:\/\/avatars.githubusercontent.com\/u\/18440126?v=4\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          BrettRyland\n        <\/a>\n      <\/div>\n    <\/div>\n\n    <div class=\"labels\">\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          p1-important\n        <\/span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          build\n        <\/span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          fs: gdrive\n        <\/span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          release-blocker\n        <\/span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          bug\n        <\/span>\n    <\/div>\n  <\/div>\n<\/div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\"># Bug Report\n\n&lt;!--\n## Issue name\n\nIssue names must follow the pattern `comm<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026<\/a><\/span><span class=\"excerpt hidden\">and: description` where the command is the dvc command that you are trying to run. The description should describe the consequence of the bug. \n\nExample: `repro: doesn't detect input changes`\n--&gt;\n\n## Description\n\nAs reported [here](https:\/\/github.com\/iterative\/dvc\/issues\/5618#issuecomment-1155019515), the latest version (2.11.0) of dvc on Ubuntu 22.04 LTS still has the bug reported in issue [#5618](https:\/\/github.com\/iterative\/dvc\/issues\/5618).\nDowngrading dvc to 2.10.2 allows the push to work properly.\n\n### Reproduce\n\nOn an existing repository using gdrive, add some files then try to push them.\n```console\nbrett@br-workhorse:~\/repos\/arctic-ice$ dvc status\nmodels.dvc:                                                                                                                                                                                                                                                                                           \n        changed outs:\n                modified:           models\n\nbrett@br-workhorse:~\/repos\/arctic-ice$ dvc add models\n100% Adding...|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|1\/1 [00:01,  1.52s\/file]\n\nbrett@br-workhorse:~\/repos\/arctic-ice$ dvc push                                                                                                                                                                                                                                                       \nERROR: unexpected error - name: drive  version: v2                                                                                                                                                                                                                                                    \n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n```\n\nWith extra verbosity:\n```console\nbrett@br-workhorse:~\/repos\/arctic-ice$ dvc push -vv\n2022-06-29 19:59:18,216 TRACE: Namespace(all_branches=False, all_commits=False, all_tags=False, cd='.', cmd='push', cprofile=False, cprofile_dump=None, func=&lt;class 'dvc.commands.data_sync.CmdDataPush'&gt;, glob=False, instrument=False, instrument_open=False, jobs=None, parser=DvcParser(prog='dvc', usage=None, description='Data Version Control', formatter_class=&lt;class 'argparse.RawTextHelpFormatter'&gt;, conflict_handler='error', add_help=False), pdb=False, quiet=0, recursive=False, remote=None, run_cache=False, targets=[], verbose=2, version=None, viztracer=False, viztracer_depth=None, with_deps=False, yappi=False)\n2022-06-29 19:59:18,399 TRACE:    16.49 ms in collecting stages from \/home\/brett\/repos\/arctic-ice\n2022-06-29 19:59:18,405 TRACE:     6.02 ms in collecting stages from \/home\/brett\/repos\/arctic-ice\/RadarMask\n2022-06-29 19:59:18,406 TRACE:    18.37 mks in collecting stages from \/home\/brett\/repos\/arctic-ice\/RadarMask\/RadarMask\n2022-06-29 19:59:18,420 DEBUG: Preparing to transfer data from '\/home\/brett\/repos\/arctic-ice\/.dvc\/cache' to '1uTdQZ6FntcI-JtX0ofC4LYPfFM37oZp0'\n2022-06-29 19:59:18,420 DEBUG: Preparing to collect status from '1uTdQZ6FntcI-JtX0ofC4LYPfFM37oZp0'\n2022-06-29 19:59:18,420 DEBUG: Collecting status from '1uTdQZ6FntcI-JtX0ofC4LYPfFM37oZp0'\n2022-06-29 19:59:18,423 DEBUG: Querying 11 hashes via object_exists                                                                                                                                                                                                                                   \n2022-06-29 19:59:18,635 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,637 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,638 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,639 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,641 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,642 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,642 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,643 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,644 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,645 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,645 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/brett\/repos\/arctic-ice\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com', 'client_secret': 'a1Fz59uTpVNeG_VGuSKDLJXv', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-06-29 19:59:18,647 ERROR: unexpected error - name: drive  version: v2                                                                                                                                                                                                                            \n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"dvc\/cli\/__init__.py\", line 185, in main\n  File \"dvc\/cli\/command.py\", line 22, in do_run\n  File \"dvc\/commands\/data_sync.py\", line 58, in run\n  File \"dvc\/repo\/__init__.py\", line 49, in wrapper\n  File \"dvc\/repo\/push.py\", line 68, in push\n  File \"dvc\/data_cloud.py\", line 109, in push\n  File \"dvc\/data_cloud.py\", line 88, in transfer\n  File \"dvc_data\/transfer.py\", line 159, in transfer\n  File \"dvc_data\/status.py\", line 179, in compare_status\n  File \"dvc_data\/status.py\", line 136, in status\n  File \"dvc_data\/status.py\", line 43, in _indexed_dir_hashes\n  File \"tqdm\/std.py\", line 1195, in __iter__\n  File \"dvc_objects\/db.py\", line 355, in list_hashes_exists\n  File \"concurrent\/futures\/_base.py\", line 611, in result_iterator\n  File \"concurrent\/futures\/_base.py\", line 439, in result\n  File \"concurrent\/futures\/_base.py\", line 388, in __get_result\n  File \"concurrent\/futures\/thread.py\", line 57, in run\n  File \"dvc_objects\/fs\/base.py\", line 263, in exists\n  File \"pydrive2\/fs\/spec.py\", line 225, in exists\n  File \"pydrive2\/fs\/spec.py\", line 192, in _get_item_id\n  File \"pydrive2\/fs\/spec.py\", line 174, in _path_to_item_ids\n  File \"pydrive2\/fs\/spec.py\", line 170, in _get_cached_item_ids\n  File \"funcy\/objects.py\", line 50, in __get__\n  File \"funcy\/objects.py\", line 28, in __get__\n  File \"pydrive2\/fs\/spec.py\", line 82, in _ids_cache\n  File \"pydrive2\/fs\/spec.py\", line 132, in _gdrive_list\n  File \"funcy\/objects.py\", line 28, in __get__\n  File \"pydrive2\/fs\/spec.py\", line 100, in _list_params\n  File \"funcy\/decorators.py\", line 45, in wrapper\n  File \"funcy\/flow.py\", line 127, in retry\n  File \"funcy\/decorators.py\", line 66, in __call__\n  File \"pydrive2\/fs\/spec.py\", line 121, in _gdrive_shared_drive_id\n  File \"pydrive2\/auth.py\", line 64, in _decorated\n  File \"pydrive2\/auth.py\", line 671, in Authorize\n  File \"googleapiclient\/_helpers.py\", line 130, in positional_wrapper\n  File \"googleapiclient\/discovery.py\", line 287, in build\n  File \"googleapiclient\/discovery.py\", line 404, in _retrieve_discovery_doc\ngoogleapiclient.errors.UnknownApiNameOrVersion: name: drive  version: v2\n------------------------------------------------------------\n2022-06-29 19:59:19,332 DEBUG: [Errno 95] no more link types left to try out: [Errno 95] Operation not supported\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"dvc\/cli\/__init__.py\", line 185, in main\n  File \"dvc\/cli\/command.py\", line 22, in do_run\n  File \"dvc\/commands\/data_sync.py\", line 58, in run\n  File \"dvc\/repo\/__init__.py\", line 49, in wrapper\n  File \"dvc\/repo\/push.py\", line 68, in push\n  File \"dvc\/data_cloud.py\", line 109, in push\n  File \"dvc\/data_cloud.py\", line 88, in transfer\n  File \"dvc_data\/transfer.py\", line 159, in transfer\n  File \"dvc_data\/status.py\", line 179, in compare_status\n  File \"dvc_data\/status.py\", line 136, in status\n  File \"dvc_data\/status.py\", line 43, in _indexed_dir_hashes\n  File \"tqdm\/std.py\", line 1195, in __iter__\n  File \"dvc_objects\/db.py\", line 355, in list_hashes_exists\n  File \"concurrent\/futures\/_base.py\", line 611, in result_iterator\n  File \"concurrent\/futures\/_base.py\", line 439, in result\n  File \"concurrent\/futures\/_base.py\", line 388, in __get_result\n  File \"concurrent\/futures\/thread.py\", line 57, in run\n  File \"dvc_objects\/fs\/base.py\", line 263, in exists\n  File \"pydrive2\/fs\/spec.py\", line 225, in exists\n  File \"pydrive2\/fs\/spec.py\", line 192, in _get_item_id\n  File \"pydrive2\/fs\/spec.py\", line 174, in _path_to_item_ids\n  File \"pydrive2\/fs\/spec.py\", line 170, in _get_cached_item_ids\n  File \"funcy\/objects.py\", line 50, in __get__\n  File \"funcy\/objects.py\", line 28, in __get__\n  File \"pydrive2\/fs\/spec.py\", line 82, in _ids_cache\n  File \"pydrive2\/fs\/spec.py\", line 132, in _gdrive_list\n  File \"funcy\/objects.py\", line 28, in __get__\n  File \"pydrive2\/fs\/spec.py\", line 100, in _list_params\n  File \"funcy\/decorators.py\", line 45, in wrapper\n  File \"funcy\/flow.py\", line 127, in retry\n  File \"funcy\/decorators.py\", line 66, in __call__\n  File \"pydrive2\/fs\/spec.py\", line 121, in _gdrive_shared_drive_id\n  File \"pydrive2\/auth.py\", line 64, in _decorated\n  File \"pydrive2\/auth.py\", line 671, in Authorize\n  File \"googleapiclient\/_helpers.py\", line 130, in positional_wrapper\n  File \"googleapiclient\/discovery.py\", line 287, in build\n  File \"googleapiclient\/discovery.py\", line 404, in _retrieve_discovery_doc\ngoogleapiclient.errors.UnknownApiNameOrVersion: name: drive  version: v2\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"dvc_objects\/fs\/generic.py\", line 68, in _try_links\n  File \"dvc_objects\/fs\/generic.py\", line 28, in _link\n  File \"dvc_objects\/fs\/base.py\", line 282, in reflink\n  File \"dvc_objects\/fs\/implementations\/local.py\", line 157, in reflink\n  File \"dvc_objects\/fs\/system.py\", line 115, in reflink\n  File \"dvc_objects\/fs\/system.py\", line 101, in _reflink_linux\nOSError: [Errno 95] Operation not supported\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"dvc_objects\/fs\/generic.py\", line 127, in _test_link\n  File \"dvc_objects\/fs\/generic.py\", line 76, in _try_links\nOSError: [Errno 95] no more link types left to try out\n------------------------------------------------------------\n2022-06-29 19:59:19,333 DEBUG: Removing '\/home\/brett\/repos\/.cFRhX4rL87pLC64LbFJkgt.tmp'\n2022-06-29 19:59:19,333 DEBUG: Removing '\/home\/brett\/repos\/.cFRhX4rL87pLC64LbFJkgt.tmp'\n2022-06-29 19:59:19,333 DEBUG: Removing '\/home\/brett\/repos\/.cFRhX4rL87pLC64LbFJkgt.tmp'\n2022-06-29 19:59:19,333 DEBUG: Removing '\/home\/brett\/repos\/arctic-ice\/.dvc\/cache\/.LAtLtAtBaEGib7LXwXJKVF.tmp'\n2022-06-29 19:59:19,337 DEBUG: Version info for developers:\nDVC version: 2.11.0 (deb)\n---------------------------------\nPlatform: Python 3.8.3 on Linux-5.15.0-37-generic-x86_64-with-glibc2.14\nSupports:\n        azure (adlfs = 2022.4.0, knack = 0.9.0, azure-identity = 1.10.0),\n        gdrive (pydrive2 = 1.10.1),\n        gs (gcsfs = 2022.5.0),\n        hdfs (fsspec = 2022.5.0, pyarrow = 8.0.0),\n        webhdfs (fsspec = 2022.5.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.4.8),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.4.8),\n        s3 (s3fs = 2022.5.0, boto3 = 1.21.21),\n        ssh (sshfs = 2022.6.0),\n        oss (ossfs = 2021.8.0),\n        webdav (webdav4 = 0.9.7),\n        webdavs (webdav4 = 0.9.7)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/nvme1n1p1\nCaches: local\nRemotes: gdrive\nWorkspace directory: ext4 on \/dev\/nvme1n1p1\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2022-06-29 19:59:19,338 DEBUG: Analytics is enabled.\n2022-06-29 19:59:19,339 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp0h_rt9la']'\n2022-06-29 19:59:19,341 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp0h_rt9la']'\n```\n\n### Expected\n\nThe push to succeed without errors.\n\n### Environment information\n\n**Output of `dvc doctor`:**\n\n```console\nbrett@br-workhorse:~\/repos\/arctic-ice$ dvc doctor\nDVC version: 2.11.0 (deb)\n---------------------------------\nPlatform: Python 3.8.3 on Linux-5.15.0-37-generic-x86_64-with-glibc2.14\nSupports:\n        azure (adlfs = 2022.4.0, knack = 0.9.0, azure-identity = 1.10.0),\n        gdrive (pydrive2 = 1.10.1),\n        gs (gcsfs = 2022.5.0),\n        hdfs (fsspec = 2022.5.0, pyarrow = 8.0.0),\n        webhdfs (fsspec = 2022.5.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.4.8),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.4.8),\n        s3 (s3fs = 2022.5.0, boto3 = 1.21.21),\n        ssh (sshfs = 2022.6.0),\n        oss (ossfs = 2021.8.0),\n        webdav (webdav4 = 0.9.7),\n        webdavs (webdav4 = 0.9.7)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/nvme1n1p1\nCaches: local\nRemotes: gdrive\nWorkspace directory: ext4 on \/dev\/nvme1n1p1\nRepo: dvc, git\n```\n\nThis is running with python 3.10.4 in a virtual environment set up with `direnv`.\n```console\nbrett@br-workhorse:~\/repos\/arctic-ice$ python --version\nPython 3.10.4\n\nbrett@br-workhorse:~\/repos\/arctic-ice$ cat .envrc \nlayout python-venv\n```\n\n```console\nbrett@br-workhorse:~\/repos\/arctic-ice$ cat .direnv\/python-venv-3.10.4\/pyvenv.cfg \nhome = \/usr\/bin\ninclude-system-site-packages = false\nversion = 3.10.4\n\nbrett@br-workhorse:~\/repos\/arctic-ice$ which dvc\n\/usr\/bin\/dvc\n\nbrett@br-workhorse:~\/repos\/arctic-ice$ pip freeze -l\nasttokens==2.0.5\nbackcall==0.2.0\ncertifi==2021.10.8\ncharset-normalizer==2.0.12\ncycler==0.11.0\ndecorator==5.1.1\nexecuting==0.8.3\nfonttools==4.33.3\nidna==3.3\nimageio==2.18.0\nimgaug==0.4.0\nipython==8.2.0\njedi==0.18.1\nkiwisolver==1.4.2\nmatplotlib==3.5.1\nmatplotlib-inline==0.1.3\nmemory-profiler==0.60.0\nnetworkx==2.8\nnumpy==1.22.3\nopencv-contrib-python==4.5.5.64\nopencv-python==4.5.5.64\npackaging==21.3\nparso==0.8.3\npexpect==4.8.0\npickleshare==0.7.5\nPillow==9.1.0\nPillow-SIMD==8.3.2.post0\nprompt-toolkit==3.0.29\npsutil==5.9.0\nptyprocess==0.7.0\npure-eval==0.2.2\nPygments==2.12.0\npyparsing==3.0.8\npython-dateutil==2.8.2\nPyWavelets==1.3.0\nrequests==2.27.1\nscikit-image==0.19.2\nscipy==1.8.0\nShapely==1.8.1.post1\nsix==1.16.0\nstack-data==0.2.0\ntifffile==2022.4.26\ntorch==1.11.0\ntorchvision==0.12.0\ntqdm==4.64.0\ntraitlets==5.1.1\ntyping_extensions==4.2.0\nurllib3==1.26.9\nwcwidth==0.2.5\n```<\/span><\/p>\n  <\/div>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-20T03:27:11.842Z",
                "Answer_body":"<p>Could you please try to set <code>gdrive_service_account_json_file_path<\/code> to some random value?<br>\nAccording to<\/p><aside class=\"onebox githubissue\" data-onebox-src=\"https:\/\/github.com\/iterative\/dvc\/issues\/6230\">\n  <header class=\"source\">\n\n      <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/6230\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com\/iterative\/dvc<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"><\/path><\/svg>\n  <\/div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/6230\" target=\"_blank\" rel=\"noopener nofollow ugc\">push: gdrive_service_account_json_file_path must be set for GDRIVE_CREDENTIALS_DATA to work<\/a>\n    <\/h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2021-06-25\" data-time=\"22:55:50\" data-timezone=\"UTC\">10:55PM - 25 Jun 21 UTC<\/span>\n      <\/div>\n\n\n      <div class=\"user\">\n        <a href=\"https:\/\/github.com\/0x2b3bfa0\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n          <img alt=\"0x2b3bfa0\" src=\"https:\/\/avatars.githubusercontent.com\/u\/11387611?v=4\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          0x2b3bfa0\n        <\/a>\n      <\/div>\n    <\/div>\n\n    <div class=\"labels\">\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/bug.png?v=12\" title=\"bug\" class=\"emoji\" alt=\"bug\" width=\"20\" height=\"20\"> bug\n        <\/span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          p2-medium\n        <\/span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          fs: gdrive\n        <\/span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          A: data-sync\n        <\/span>\n    <\/div>\n  <\/div>\n<\/div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">## Description\n\nThe `GDRIVE_CREDENTIALS_DATA` environment variable won't be ho<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026<\/a><\/span><span class=\"excerpt hidden\">nored unless the `gdrive_service_account_json_file_path` configuration option is set beforehand. In most of the habitual use cases, this requirement doesn't make too much sense from the user standpoint.\n\nIs this the recommended way of passing Google Drive (Service Account) credentials through an environment variable?\n\nAdditionally, after reading the pertaining [documentation](https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote), I don't understand why I can't use the `GDRIVE_CREDENTIALS_DATA` environment variable to pass the DVC-specific `.dvc\/tmp\/gdrive-user-credentials.json` file contents, instead of having to pass the raw service account file contents.\n\n### How to reproduce\n\nThe following GitHub Actions workflow provides a self-contained example to reproduce this issue. Uncommenting the only line prefixed with a hash `#` will make it work as expected.\n\n```yaml\non: push\njobs:\n  run:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: iterative\/setup-dvc@v1\n      - run: |\n          git init\n          dvc init\n          \n          dvc remote add origin gdrive:\/\/root\n          dvc remote modify origin --local gdrive_use_service_account true\n          # dvc remote modify origin --local gdrive_service_account_json_file_path \/dev\/null\n\n          date &gt; file\n          dvc add file\n\n          dvc push --verbose --remote origin\n        env:\n          GDRIVE_CREDENTIALS_DATA: ${{ secrets.ORIGINAL_SERVICE_ACCOUNT_JSON }}\n```\n\n_Note: the `${{ secrets.ORIGINAL_SERVICE_ACCOUNT_JSON }}` variable represents a GCP Service Account file in the standard JSON format provided by Google, not a DVC credentials JSON file._\n\n### Actual output\n\n```console\n$ dvc push --verbose --remote origin\n2021-06-25 22:04:09,030 ERROR: failed to push data to the cloud - To use service account, set `gdrive_service_account_json_file_path`, and optionally`gdrive_service_account_user_email` in DVC config\nLearn more at &lt;https:\/\/man.dvc.org\/remote\/modify&gt;\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"dvc\/command\/data_sync.py\", line 57, in run\n  File \"dvc\/repo\/__init__.py\", line 51, in wrapper\n  File \"dvc\/repo\/push.py\", line 44, in push\n  File \"dvc\/data_cloud.py\", line 77, in push\n  File \"dvc\/data_cloud.py\", line 38, in get_remote\n  File \"dvc\/data_cloud.py\", line 58, in _init_remote\n  File \"dvc\/remote\/__init__.py\", line 8, in get_remote\n  File \"dvc\/fs\/gdrive.py\", line 128, in __init__\n  File \"dvc\/fs\/gdrive.py\", line 155, in _validate_config\ndvc.exceptions.DvcException: To use service account, set `gdrive_service_account_json_file_path`, and optionally`gdrive_service_account_user_email` in DVC config\nLearn more at &lt;https:\/\/man.dvc.org\/remote\/modify&gt;\n2021-06-25 22:04:09,033 DEBUG: Analytics is enabled.\n------------------------------------------------------------\n2021-06-25 22:04:09,076 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp8tbexj0k']'\n2021-06-25 22:04:09,077 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp8tbexj0k']'\n```\n\n### Expected output\n\n```console\n$ dvc push --verbose --remote origin\n2021-06-25 22:17:58,441 DEBUG: Preparing to upload data to 'gdrive:\/\/root'\n2021-06-25 22:17:58,441 DEBUG: Preparing to collect status from gdrive:\/\/root\n2021-06-25 22:17:58,441 DEBUG: Collecting information from local cache...\n2021-06-25 22:17:58,441 DEBUG: Collecting information from remote cache...\n2021-06-25 22:17:58,441 DEBUG: Matched '0' indexed hashes\n2021-06-25 22:17:58,442 DEBUG: Querying 1 hashes via object_exists\n2021-06-25 22:17:58,672 DEBUG: GDrive remote auth with config '***'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/runner\/work\/test\/test\/.dvc\/tmp\/.5VsCA2nJs9ZQgeBZ8pWNAk.tmp', 'get_refresh_token': True, 'service_config': ***'client_user_email': None, 'client_json_file_path': '\/home\/runner\/work\/test\/test\/.dvc\/tmp\/.mpgSFHPn7NBgMe35nngenb.tmp'***'.\n2021-06-25 22:17:59,308 DEBUG: Uploading '.dvc\/cache\/5d\/afc8549edbdc83c20f11cbfde93cc4' to 'gdrive:\/\/root\/5d\/afc8549edbdc83c20f11cbfde93cc4'\n1 file pushed\n2021-06-25 22:18:01,059 DEBUG: Analytics is enabled.\n2021-06-25 22:18:01,113 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp0ocbbm6a']'\n2021-06-25 22:18:01,116 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp0ocbbm6a']'\n```\n\n### Environment information\n\n```console\n$ dvc doctor\nDVC version: 2.4.1 (deb)\n---------------------------------\nPlatform: Python 3.8.10 on Linux-5.8.0-1033-azure-x86_64-with-glibc2.7\nSupports: All remotes\nCache types: &lt;https:\/\/error.dvc.org\/no-dvc-cache&gt;\nCaches: local\nRemotes: gdrive\nWorkspace directory: ext4 on \/dev\/root\nRepo: dvc, git\n```\n\n### Additional information\n\nhttps:\/\/github.com\/iterative\/dvc\/blob\/4e792ae61c5927ab2e5f6a6914d985d43aa705b4\/dvc\/fs\/gdrive.py#L128\n\nhttps:\/\/github.com\/iterative\/dvc\/blob\/4e792ae61c5927ab2e5f6a6914d985d43aa705b4\/dvc\/fs\/gdrive.py#L149-L162\n\n### See also\n\n- https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote\n- https:\/\/discuss.dvc.org\/t\/cml-github-actions-google-drive-service-account\/795\/9\n- https:\/\/iterativeai.slack.com\/archives\/CB41NAL8H\/p1624015633145600\n- https:\/\/iterativeai.slack.com\/archives\/CNQ95CG1K\/p1624187829201500<\/span><\/p>\n  <\/div>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-20T19:48:22.272Z",
                "Answer_body":"<p>Nope, that did not help. Thanks for the suggestion.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"error configur error gdrive remot auth fail credenti gdrive credenti data sure go wrong copi past credenti file gdrive servic account secret gdrive credenti data variabl popul run action tri credenti comput environ variabl error configur error gdrive remot auth fail credenti gdrive credenti data auto test push job run run ubuntu latest step us action checkout us action setup python us iter setup saniti check run pull pip instal requir txt python src test env repo token secret github token gdrive credenti data secret gdrive credenti data",
        "Question_preprocessed_content":"error configur error gdrive remot auth fail credenti sure go wrong copi past credenti file gdrive servic account secret variabl popul run action tri credenti comput environ variabl error configur error gdrive remot auth fail credenti",
        "Question_gpt_summary_original":"The user encountered a configuration error while trying to authenticate their GDrive remote with credentials in 'GDRIVE_CREDENTIALS_DATA'. The user copied and pasted the credentials files from their GDrive service account into the secret, and the variable was populated when they ran GH action. However, the user received an error message when trying to authenticate the GDrive remote. The user tried the credentials on two other computers but not with the environment variable.",
        "Question_gpt_summary":"user encount configur error try authent gdrive remot credenti gdrive credenti data user copi past credenti file gdrive servic account secret variabl popul ran action user receiv error messag try authent gdrive remot user tri credenti comput environ variabl",
        "Answer_original_content":"pydriv version run try downgrad pull run chang contain instal depend default version version instal add pip list grep pydriv error imag sorri miss setup action disregard previou comment relev issu pull work local set gdrive credenti data env variabl sure content json file suit normal environ variabl json like gdrive credenti data read better us action docker file sure recommend set env variabl content json file guid setup googl drive remot gdrive credenti data doubl quot thank know link help set far show error consol bash printf invalid format charact probabl entri client cert url http googleapi com robot metadata lrg iam gserviceaccount com chang work local nope work exampl file work gdrive credenti data error configur error gdrive remot auth fail credenti gdrive credenti data backup remov fix run auth refresh credenti detail drive version error gdrive remot auth fail credenti gdrive credenti data backup remov fix run auth refresh credenti detail learn configur set error process complet exit code bit confus work local new git repo al help rule issu like mkdir gdrive test gdrive test git init init git add git commit initi remot add gdrive gdrive remot modifi gdrive gdrive us servic account true touch examplefil add examplefil git commit add exampl file export gdrive credenti data replac credenti push ye local work github com iter gdrive rais unexpect error drive version open jun utc brettryland import build gdrive releas blocker bug bug report descript report http github com iter issu issuecom latest version ubuntu lt bug report issu http github com iter issu downgrad allow push work properli reproduc exist repositori gdrive add file try push consol brett workhors repo arctic ic statu model chang out modifi model brett workhors repo arctic ic add model ad file brett workhors repo arctic ic push error unexpect error drive version have troubl hit http org support happi help extra verbos consol brett workhors repo arctic ic push trace namespac branch fals commit fals tag fals cmd push cprofil fals cprofil dump func glob fals instrument fals instrument open fals job parser parser prog usag descript data version control formatt class conflict handler error add help fals pdb fals quiet recurs fals remot run cach fals target verbos version viztrac fals viztrac depth dep fals yappi fals trace collect stage home brett repo arctic ic trace collect stage home brett repo arctic ic radarmask trace mk collect stage home brett repo arctic ic radarmask radarmask debug prepar transfer data home brett repo arctic ic cach utdqzfntci jtxofclypffmozp debug prepar collect statu utdqzfntci jtxofclypffmozp debug collect statu utdqzfntci jtxofclypffmozp debug queri hash object exist debug gdrive remot auth config client config backend set client config file client secret json save credenti true oauth scope http googleapi com auth drive http googleapi com auth drive appdata save credenti backend file save credenti file home brett repo arctic ic tmp gdrive user credenti json refresh token true client config client iivsgbgsbuvfapdhvueioc app googleusercont com client secret afzutpvneg vguskdljxv auth uri http account googl com oauth auth token uri http oauth googleapi com token revok uri http oauth googleapi com revok redirect uri debug gdrive remot auth config client config backend set client config file client secret json save credenti true oauth scope http googleapi com auth drive http googleapi com auth drive appdata save credenti backend file save credenti file home brett repo arctic ic tmp gdrive user credenti json refresh token true client config client iivsgbgsbuvfapdhvueioc app googleusercont com client secret afzutpvneg vguskdljxv auth uri http account googl com oauth auth token uri http oauth googleapi com token revok uri http oauth googleapi com revok redirect uri debug gdrive remot auth config client config backend set client config file client secret json save credenti true oauth scope http googleapi com auth drive http googleapi com auth drive appdata save credenti backend file save credenti file home brett repo arctic ic tmp gdrive user credenti json refresh token true client config client iivsgbgsbuvfapdhvueioc app googleusercont com client secret afzutpvneg vguskdljxv auth uri http account googl com oauth auth token uri http oauth googleapi com token revok uri http oauth googleapi com revok redirect uri debug gdrive remot auth config client config backend set client config file client secret json save credenti true oauth scope http googleapi com auth drive http googleapi com auth drive appdata save credenti backend file save credenti file home brett repo arctic ic tmp gdrive user credenti json refresh token true client config client iivsgbgsbuvfapdhvueioc app googleusercont com client secret afzutpvneg vguskdljxv auth uri http account googl com oauth auth token uri http oauth googleapi com token revok uri http oauth googleapi com revok redirect uri debug gdrive remot auth config client config backend set client config file client secret json save credenti true oauth scope http googleapi com auth drive http googleapi com auth drive appdata save credenti backend file save credenti file home brett repo arctic ic tmp gdrive user credenti json refresh token true client config client iivsgbgsbuvfapdhvueioc app googleusercont com client secret afzutpvneg vguskdljxv auth uri http account googl com oauth auth token uri http oauth googleapi com token revok uri http oauth googleapi com revok redirect uri debug gdrive remot auth config client config backend set client config file client secret json save credenti true oauth scope http googleapi com auth drive http googleapi com auth drive appdata save credenti backend file save credenti file home brett repo arctic ic tmp gdrive user credenti json refresh token true client config client iivsgbgsbuvfapdhvueioc app googleusercont com client secret afzutpvneg vguskdljxv auth uri http account googl com oauth auth token uri http oauth googleapi com token revok uri http oauth googleapi com revok redirect uri debug gdrive remot auth config client config backend set client config file client secret json save credenti true oauth scope http googleapi com auth drive http googleapi com auth drive appdata save credenti backend file save credenti file home brett repo arctic ic tmp gdrive user credenti json refresh token true client config client iivsgbgsbuvfapdhvueioc app googleusercont com client secret afzutpvneg vguskdljxv auth uri http account googl com oauth auth token uri http oauth googleapi com token revok uri http oauth googleapi com revok redirect uri debug gdrive remot auth config client config backend set client config file client secret json save credenti true oauth scope http googleapi com auth drive http googleapi com auth drive appdata save credenti backend file save credenti file home brett repo arctic ic tmp gdrive user credenti json refresh token true client config client iivsgbgsbuvfapdhvueioc app googleusercont com client secret afzutpvneg vguskdljxv auth uri http account googl com oauth auth token uri http oauth googleapi com token revok uri http oauth googleapi com revok redirect uri debug gdrive remot auth config client config backend set client config file client secret json save credenti true oauth scope http googleapi com auth drive http googleapi com auth drive appdata save credenti backend file save credenti file home brett repo arctic ic tmp gdrive user credenti json refresh token true client config client iivsgbgsbuvfapdhvueioc app googleusercont com client secret afzutpvneg vguskdljxv auth uri http account googl com oauth auth token uri http oauth googleapi com token revok uri http oauth googleapi com revok redirect uri debug gdrive remot auth config client config backend set client config file client secret json save credenti true oauth scope http googleapi com auth drive http googleapi com auth drive appdata save credenti backend file save credenti file home brett repo arctic ic tmp gdrive user credenti json refresh token true client config client iivsgbgsbuvfapdhvueioc app googleusercont com client secret afzutpvneg vguskdljxv auth uri http account googl com oauth auth token uri http oauth googleapi com token revok uri http oauth googleapi com revok redirect uri debug gdrive remot auth config client config backend set client config file client secret json save credenti true oauth scope http googleapi com auth drive http googleapi com auth drive appdata save credenti backend file save credenti file home brett repo arctic ic tmp gdrive user credenti json refresh token true client config client iivsgbgsbuvfapdhvueioc app googleusercont com client secret afzutpvneg vguskdljxv auth uri http account googl com oauth auth token uri http oauth googleapi com token revok uri http oauth googleapi com revok redirect uri error unexpect error drive version traceback recent file cli init line main file cli command line run file command data sync line run file repo init line wrapper file repo push line push file data cloud line push file data cloud line transfer file data transfer line transfer file data statu line compar statu file data statu line statu file data statu line index dir hash file tqdm std line iter file object line list hash exist file concurr futur base line result iter file concurr futur base line result file concurr futur base line result file concurr futur thread line run file object base line exist file pydriv spec line exist file pydriv spec line item file pydriv spec line path item id file pydriv spec line cach item id file funci object line file funci object line file pydriv spec line id cach file pydriv spec line gdrive list file funci object line file pydriv spec line list param file funci decor line wrapper file funci flow line retri file funci decor line file pydriv spec line gdrive share drive file pydriv auth line decor file pydriv auth line author file googleapicli helper line posit wrapper file googleapicli discoveri line build file googleapicli discoveri line retriev discoveri doc googleapicli error unknownapinameorvers drive version debug errno link type left try errno oper support traceback recent file cli init line main file cli command line run file command data sync line run file repo init line wrapper file repo push line push file data cloud line push file data cloud line transfer file data transfer line transfer file data statu line compar statu file data statu line statu file data statu line index dir hash file tqdm std line iter file object line list hash exist file concurr futur base line result iter file concurr futur base line result file concurr futur base line result file concurr futur thread line run file object base line exist file pydriv spec line exist file pydriv spec line item file pydriv spec line path item id file pydriv spec line cach item id file funci object line file funci object line file pydriv spec line id cach file pydriv spec line gdrive list file funci object line file pydriv spec line list param file funci decor line wrapper file funci flow line retri file funci decor line file pydriv spec line gdrive share drive file pydriv auth line decor file pydriv auth line author file googleapicli helper line posit wrapper file googleapicli discoveri line build file googleapicli discoveri line retriev discoveri doc googleapicli error unknownapinameorvers drive version handl except except occur traceback recent file object gener line try link file object gener line link file object base line reflink file object implement local line reflink file object line reflink file object line reflink linux oserror errno oper support except direct caus follow except traceback recent file object gener line test link file object gener line try link oserror errno link type left try debug remov home brett repo cfrhxrlplclbfjkgt tmp debug remov home brett repo cfrhxrlplclbfjkgt tmp debug remov home brett repo cfrhxrlplclbfjkgt tmp debug remov home brett repo arctic ic cach latltatbaegiblxwxjkvf tmp debug version info develop version deb platform python linux gener glibc support azur adlf knack azur ident gdrive pydriv gcsf hdf fsspec pyarrow webhdf fsspec http aiohttp aiohttp retri http aiohttp aiohttp retri sf boto ssh sshf oss ossf webdav webdav webdav webdav cach type hardlink symlink cach directori ext dev nvmenp cach local remot gdrive workspac directori ext dev nvmenp repo git have troubl hit http org support happi help debug analyt enabl debug try spawn daemon analyt tmp tmph rtla debug spawn daemon analyt tmp tmph rtla expect push succe error environ inform output doctor consol brett workhors repo arctic ic doctor version deb platform python linux gener glibc support azur adlf knack azur ident gdrive pydriv gcsf hdf fsspec pyarrow webhdf fsspec http aiohttp aiohttp retri http aiohttp aiohttp retri sf boto ssh sshf oss ossf webdav webdav webdav webdav cach type hardlink symlink cach directori ext dev nvmenp cach local remot gdrive workspac directori ext dev nvmenp repo git run python virtual environ set direnv consol brett workhors repo arctic ic python version python brett workhors repo arctic ic cat envrc layout python venv consol brett workhors repo arctic ic cat direnv python venv pyvenv cfg home usr bin includ site packag fals version brett workhors repo arctic ic usr bin brett workhors repo arctic ic pip freez asttoken backcal certifi charset normal cycler decor execut fonttool idna imageio imgaug ipython jedi kiwisolv matplotlib matplotlib inlin memori profil networkx numpi opencv contrib python opencv python packag parso pexpect pickleshar pillow pillow simd post prompt toolkit psutil ptyprocess pure eval pygment pypars python dateutil pywavelet request scikit imag scipi shape post stack data tiil torch torchvis tqdm traitlet type extens urllib wcwidth try set gdrive servic account json file path random valu accord github com iter push gdrive servic account json file path set gdrive credenti data work open jun utc xbbfa bug medium gdrive data sync descript gdrive credenti data environ variabl won honor gdrive servic account json file path configur option set habitu us case requir sens user standpoint recommend wai pass googl drive servic account credenti environ variabl addition read pertain document http org doc user guid setup googl drive remot understand us gdrive credenti data environ variabl pass specif tmp gdrive user credenti json file content instead have pass raw servic account file content reproduc follow github action workflow provid self contain exampl reproduc issu uncom line prefix hash work expect yaml push job run run ubuntu latest step us iter setup run git init init remot add origin gdrive root remot modifi origin local gdrive us servic account true remot modifi origin local gdrive servic account json file path dev null date file add file push verbos remot origin env gdrive credenti data secret origin servic account json note secret origin servic account json variabl repres gcp servic account file standard json format provid googl credenti json file actual output consol push verbos remot origin error fail push data cloud us servic account set gdrive servic account json file path option gdrive servic account user email config learn traceback recent file command data sync line run file repo init line wrapper file repo push line push file data cloud line push file data cloud line remot file data cloud line init remot file remot init line remot file gdrive line init file gdrive line valid config except except us servic account set gdrive servic account json file path option gdrive servic account user email config learn debug analyt enabl debug try spawn daemon analyt tmp tmptbexjk debug spawn daemon analyt tmp tmptbexjk expect output consol push verbos remot origin debug prepar upload data gdrive root debug prepar collect statu gdrive root debug collect inform local cach debug collect inform remot cach debug match index hash debug queri hash object exist debug gdrive remot auth config client config backend set client config file client secret json save credenti true oauth scope http googleapi com auth drive http googleapi com auth drive appdata save credenti backend file save credenti file home runner work test test tmp vscanjszqgebzpwnak tmp refresh token true servic config client user email client json file path home runner work test test tmp mpgsfhpnnbgmenngenb tmp debug upload cach afcedbdccfcbfdecc gdrive root afcedbdccfcbfdecc file push debug analyt enabl debug try spawn daemon analyt tmp tmpocbbma debug spawn daemon analyt tmp tmpocbbma environ inform consol doctor version deb platform python linux azur glibc support remot cach type cach local remot gdrive workspac directori ext dev root repo git addit inform http github com iter blob eaecabefaddaab gdrive http github com iter blob eaecabefaddaab gdrive http org doc user guid setup googl drive remot http discuss org cml github action googl drive servic account http iterativeai slack com archiv cbnalh http iterativeai slack com archiv cnqcgk nope help thank suggest",
        "Answer_preprocessed_content":"pydriv version run try downgrad run chang contain instal depend default version version instal add error imag sorri miss action disregard previou comment relev issu work local set env variabl sure content json file suit normal environ variabl json like read better us action docker file sure set env variabl content json file guid setup googl drive remot thank know link help set far show error consol bash printf command descript repro detect input chang doctor direnv environ variabl won honor configur option set habitu us case requir sens user standpoint recommend wai pass googl drive credenti environ variabl addition read pertain understand us environ variabl pass specif file content instead have pass raw servic account file content reproduc follow github action workflow provid exampl reproduc issu uncom line prefix hash work expect variabl repres gcp servic account file standard json format provid googl credenti json actual output expect output environ inform addit inform nope help thank suggest",
        "Answer_gpt_summary_original":"the answer provides several possible solutions to a configuration error with gdrive remote authentication. these include downgrading the pydrive2 version, setting the gdrive_credentials_data env variable to the content of the json file, and setting the gdrive_service_account_json_file_path configuration option. the answer also suggests creating a new git\/repo to rule out other issues and provides a self-contained example to reproduce the issue.",
        "Answer_gpt_summary":"answer provid possibl solut configur error gdrive remot authent includ downgrad pydriv version set gdrive credenti data env variabl content json file set gdrive servic account json file path configur option answer suggest creat new git repo rule issu provid self contain exampl reproduc issu"
    },
    {
        "Question_title":"Vertex AI create endpoint error - FAILED_PRECONDITION: Project xxxxxxxx is not active.",
        "Question_body":"Hi, I'm stuck at following error message when I try to create vertex-ai endpoint from workbench notebook.  I have enabled aiplatform.googleapis.com.Command:\ngcloud ai endpoints create \\\n--project=XXXXX\n--region=us-central1 \\\n--display-name=ld-test-resnet-classifierUsing endpoint [https:\/\/us-central1-aiplatform.googleapis.com\/]\nERROR: (gcloud.ai.endpoints.create) FAILED_PRECONDITION: Project XXXXXXXXXX is not active.Please suggest what am I missing.   ",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661561100000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":144.0,
        "Answer_body":"Hi,\nThe issue is resolved.\nAt least one model has to be uploaded first to model registry for this command to work.\nThe official documentation titled \"Deploy a model using the Vertex AI API\" - implies deploy a model uploaded to model registry\".\n\nThanks for the views.\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-create-endpoint-error-FAILED-PRECONDITION-Project\/td-p\/460565\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-27T04:38:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Hi,\nThe issue is resolved.\nAt least one model has to be uploaded first to model registry for this command to work.\nThe official documentation titled \"Deploy a model using the Vertex AI API\" - implies deploy a model uploaded to model registry\".\n\nThanks for the views.\n\nView solution in original post"
            },
            {
                "Answer_creation_time":"2022-08-27T04:38:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Hi,\nThe issue is resolved.\nAt least one model has to be uploaded first to model registry for this command to work.\nThe official documentation titled \"Deploy a model using the Vertex AI API\" - implies deploy a model uploaded to model registry\".\n\nThanks for the views."
            }
        ],
        "Question_closed_time":1661575080000,
        "Question_original_content":"creat endpoint error fail precondit project activ stuck follow error messag try creat vertex endpoint workbench notebook enabl aiplatform googleapi com command gcloud endpoint creat project region central displai test resnet classifierus endpoint http central aiplatform googleapi com error gcloud endpoint creat fail precondit project activ suggest miss",
        "Question_preprocessed_content":"creat endpoint error project activ stuck follow error messag try creat endpoint workbench notebook enabl gcloud endpoint creat project endpoint error project suggest miss",
        "Question_gpt_summary_original":"The user encountered an error message when trying to create a Vertex AI endpoint from a workbench notebook. The error message stated that the project was not active, resulting in a FAILED_PRECONDITION error. The user is seeking assistance in resolving the issue.",
        "Question_gpt_summary":"user encount error messag try creat endpoint workbench notebook error messag state project activ result fail precondit error user seek assist resolv issu",
        "Answer_original_content":"issu resolv model upload model registri command work offici document titl deploi model api impli deploi model upload model registri thank view view solut origin post",
        "Answer_preprocessed_content":"issu resolv model upload model registri command work offici document titl deploi model api impli deploi model upload model registri thank view view solut origin post",
        "Answer_gpt_summary_original":"Solution: The issue was resolved by uploading at least one model to the model registry before creating a Vertex AI endpoint. The official documentation titled \"Deploy a model using the Vertex AI API\" implies that a model uploaded to the model registry is required for this command to work.",
        "Answer_gpt_summary":"solut issu resolv upload model model registri creat endpoint offici document titl deploi model api impli model upload model registri requir command work"
    },
    {
        "Question_title":"How do I give Vertex AI pipeline component permissions?",
        "Question_body":"<p>In a Vertex AI pipeline component,I try:<\/p>\n<pre><code>def my_comp(project_id: str, location: str, endpoint_id: str, endpoint: Output[Artifact]):\n    import google.cloud.aiplatform as aip\n    endpoints = aip.Endpoint.list()\n...\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code>'aiplatform.endpoints.list' denied on resource '\/\/aiplatform.googleapis.com\/projects\/...\n<\/code><\/pre>\n<p>My service account has owner permissions, and it works outside of the component. What do I need to do?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1662721061203,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":62.0,
        "Answer_body":"<p>This permission denied on resource issue can be resolved by using import statement:<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73661090",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1663223342092,
        "Question_original_content":"pipelin compon permiss pipelin compon try def comp project str locat str endpoint str endpoint output artifact import googl cloud aiplatform aip endpoint aip endpoint list give aiplatform endpoint list deni resourc aiplatform googleapi com project servic account owner permiss work outsid compon need",
        "Question_preprocessed_content":"pipelin compon permiss pipelin compon try give servic account owner permiss work outsid compon need",
        "Question_gpt_summary_original":"The user is encountering a challenge in giving Vertex AI pipeline component permissions. They are receiving an error message stating that 'aiplatform.endpoints.list' is denied on a resource, despite having owner permissions on their service account. The user is seeking advice on how to resolve this issue.",
        "Question_gpt_summary":"user encount challeng give pipelin compon permiss receiv error messag state aiplatform endpoint list deni resourc despit have owner permiss servic account user seek advic resolv issu",
        "Answer_original_content":"permiss deni resourc issu resolv import statement googl cloud import aiplatform aiplatform",
        "Answer_preprocessed_content":"permiss deni resourc issu resolv import statement",
        "Answer_gpt_summary_original":"Solution: The user can resolve the permission denied on resource issue by using the import statement \"from google.cloud import aiplatform_v1 as aiplatform\".",
        "Answer_gpt_summary":"solut user resolv permiss deni resourc issu import statement googl cloud import aiplatform aiplatform"
    },
    {
        "Question_title":"List all remote paths of tracked files",
        "Question_body":"<p>For a given commit, I\u2019d like to get a list of the local paths &amp; corresponding remote paths of all files tracked by DVC, possibly limited to a certain directory. Is there not an easy way to do this?<\/p>\n<p>Currently looking at a workaround involving \u201cdvc list -R --dvc-only --rev  \u2026\u201d to get all tracked files, reading the md5 out of the corresponding .dvc files, and then using those to build the remote paths. Doesn\u2019t seem very elegant.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1630558728484,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":217.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jmiller\">@jmiller<\/a>! Might I ask what is the final purpose of this operation?<\/p>\n<p>There is not really a straightforward way to get all the remote paths of a repo, probably what you are already doing would be the easiest way.<\/p>\n<p>Usually, you would be accessing those files directly with either <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\" rel=\"noopener nofollow ugc\"><code>dvc get<\/code><\/a> or the python API <a href=\"https:\/\/dvc.org\/doc\/api-reference\/read\" rel=\"noopener nofollow ugc\"><code>dvc.api.read<\/code><\/a>.<\/p>\n<p>Not sure if this can help but you can use <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get#example-getting-the-storage-url-of-a-dvc-tracked-file\" rel=\"noopener nofollow ugc\"><code>dvc get --show-url<\/code><\/a> to \u201cprint the storage location (URL) of the target data\u201d. Alternatively, you can use the python API <a href=\"https:\/\/dvc.org\/doc\/api-reference\/get_url\" rel=\"noopener nofollow ugc\"><code>dvc.api.get_url<\/code><\/a>.<\/p>. <p>Thanks! Possibly I\u2019m not using dvc for its intended purposes. I want to load the data, as it was at a certain commit, into a cloud database and\/or data visualization tool. Doing this directly with the remote copy of the data (s3) seems to make the most sense, because it doesn\u2019t require me to upload another copy of the data to the cloud first.<\/p>\n<p>Currently my repo has local cache but eventually I\u2019d like to have all data stored in the cloud, not in a local workspace or cache at all, since the data is quite large.<\/p>. <p>From Python API <a href=\"https:\/\/dvc.org\/doc\/api-reference\/open\" rel=\"noopener nofollow ugc\"><code>dvc.api.open<\/code><\/a>:<\/p>\n<blockquote>\n<p>makes a direct connection to the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/add#supported-storage-types\" rel=\"noopener nofollow ugc\">remote storage<\/a> (except for Google Drive), so the file contents can be streamed.<\/p>\n<\/blockquote>\n<p>So using the API won\u2019t create a copy of the data, but <code>dvc get<\/code> will.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/list-all-remote-paths-of-tracked-files\/863",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-02T15:10:14.876Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jmiller\">@jmiller<\/a>! Might I ask what is the final purpose of this operation?<\/p>\n<p>There is not really a straightforward way to get all the remote paths of a repo, probably what you are already doing would be the easiest way.<\/p>\n<p>Usually, you would be accessing those files directly with either <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\" rel=\"noopener nofollow ugc\"><code>dvc get<\/code><\/a> or the python API <a href=\"https:\/\/dvc.org\/doc\/api-reference\/read\" rel=\"noopener nofollow ugc\"><code>dvc.api.read<\/code><\/a>.<\/p>\n<p>Not sure if this can help but you can use <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get#example-getting-the-storage-url-of-a-dvc-tracked-file\" rel=\"noopener nofollow ugc\"><code>dvc get --show-url<\/code><\/a> to \u201cprint the storage location (URL) of the target data\u201d. Alternatively, you can use the python API <a href=\"https:\/\/dvc.org\/doc\/api-reference\/get_url\" rel=\"noopener nofollow ugc\"><code>dvc.api.get_url<\/code><\/a>.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-02T15:57:47.726Z",
                "Answer_body":"<p>Thanks! Possibly I\u2019m not using dvc for its intended purposes. I want to load the data, as it was at a certain commit, into a cloud database and\/or data visualization tool. Doing this directly with the remote copy of the data (s3) seems to make the most sense, because it doesn\u2019t require me to upload another copy of the data to the cloud first.<\/p>\n<p>Currently my repo has local cache but eventually I\u2019d like to have all data stored in the cloud, not in a local workspace or cache at all, since the data is quite large.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-06T08:37:28.957Z",
                "Answer_body":"<p>From Python API <a href=\"https:\/\/dvc.org\/doc\/api-reference\/open\" rel=\"noopener nofollow ugc\"><code>dvc.api.open<\/code><\/a>:<\/p>\n<blockquote>\n<p>makes a direct connection to the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/add#supported-storage-types\" rel=\"noopener nofollow ugc\">remote storage<\/a> (except for Google Drive), so the file contents can be streamed.<\/p>\n<\/blockquote>\n<p>So using the API won\u2019t create a copy of the data, but <code>dvc get<\/code> will.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"list remot path track file given commit like list local path correspond remot path file track possibl limit certain directori easi wai current look workaround involv list rev track file read correspond file build remot path doesnt eleg",
        "Question_preprocessed_content":"list remot path track file given commit like list local path correspond remot path file track possibl limit certain directori easi wai current look workaround involv list track file read correspond file build remot path doesnt eleg",
        "Question_gpt_summary_original":"The user is facing a challenge in obtaining a list of local and remote paths of all files tracked by DVC for a given commit. They are currently using a workaround involving \"dvc list\" command to get all tracked files and then reading the md5 out of the corresponding .dvc files to build the remote paths, which they find to be inelegant.",
        "Question_gpt_summary":"user face challeng obtain list local remot path file track given commit current workaround involv list command track file read correspond file build remot path ineleg",
        "Answer_original_content":"jmiller ask final purpos oper straightforward wai remot path repo probabl easiest wai usual access file directli python api api read sure help us url print storag locat url target data altern us python api api url thank possibl intend purpos want load data certain commit cloud databas data visual tool directli remot copi data sens doesnt requir upload copi data cloud current repo local cach eventu like data store cloud local workspac cach data larg python api api open make direct connect remot storag googl drive file content stream api wont creat copi data",
        "Answer_preprocessed_content":"ask final purpos oper straightforward wai remot path repo probabl easiest wai usual access file directli python api sure help us print storag locat target data altern us python api thank possibl intend purpos want load data certain commit cloud databas data visual tool directli remot copi data sens doesnt requir upload copi data cloud current repo local cach eventu like data store cloud local workspac cach data python api make direct connect remot storag file content stream api wont creat copi data",
        "Answer_gpt_summary_original":"Solutions provided:\n- The user can use \"dvc get --show-url\" command to print the storage location (URL) of the target data or use the python API \"dvc.api.get_url\".\n- The user can use the Python API \"dvc.api.open\" to make a direct connection to the remote storage and stream the file contents without creating a copy of the data.\n- The current workaround of using \"dvc list\" command to get all tracked files and then reading the md5 out of the corresponding .dvc files to build the remote paths is the easiest way to get all remote paths of a repo. \n\nNo personal opinions or biases were included in the response.",
        "Answer_gpt_summary":"solut provid user us url command print storag locat url target data us python api api url user us python api api open direct connect remot storag stream file content creat copi data current workaround list command track file read correspond file build remot path easiest wai remot path repo person opinion bias includ respons"
    },
    {
        "Question_title":"SageMaker deploy custom script",
        "Question_body":"<p>I'm pretty new to SageMaker, so I'm sorry if I miss something obvious.<\/p>\n\n<p>I've trained a DL model which uses frames from a video to make a prediction. The current script, that runs in the SageMaker jupyter-notebook, takes a video URL as an input and uses an FFMPEG subprocess pipe to extract the frames and predict them afterwards. This works fine, but now I want to start that script from Lambda.<\/p>\n\n<p>As far as I understood, I could deploy my model with sagemaker and make predictions for every single frame from Lambda, unfortunately this is not an option, as ffprobe, ffmpeg and numpy are too large to fit into the limited lambda space.<\/p>\n\n<p>tl;dr: Is it possible to run my custom script (ffmpeg frame extraction + tensorflow model prediction) as an endpoint in SageMaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1580859395073,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":386.0,
        "Answer_body":"<p>Sagemaker allows you to use a custom Docker image (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms.html\" rel=\"nofollow noreferrer\">AWS document<\/a>)<\/p>\n\n<blockquote>\n  <p>Build your own custom container image: If there is no pre-built Amazon\n  SageMaker container image that you can use or modify for an advanced\n  scenario, you can package your own script or algorithm to use with\n  Amazon SageMaker.You can use any programming language or framework to\n  develop your container<\/p>\n<\/blockquote>\n\n<ul>\n<li>Create a docker image with your code (FFmpeg, TensorFlow)<\/li>\n<li>Testing the docker container locally<\/li>\n<li>Deploying the image on Amazon ECR (Elastic Container Repository)<\/li>\n<li>Create a SageMaker model and point to the image<\/li>\n<\/ul>\n\n<p>For details, you can learn more from <a href=\"https:\/\/towardsdatascience.com\/brewing-up-custom-ml-models-on-aws-sagemaker-e09b64627722\" rel=\"nofollow noreferrer\">this tutorial<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60067075",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1580863000328,
        "Question_original_content":"deploi custom script pretti new sorri miss obviou train model us frame video predict current script run jupyt notebook take video url input us ffmpeg subprocess pipe extract frame predict work fine want start script lambda far understood deploi model predict singl frame lambda unfortun option ffprobe ffmpeg numpi larg fit limit lambda space possibl run custom script ffmpeg frame extract tensorflow model predict endpoint",
        "Question_preprocessed_content":"deploi custom script pretti new sorri miss obviou train model us frame video predict current script run take video url input us ffmpeg subprocess pipe extract frame predict work fine want start script lambda far understood deploi model predict singl frame lambda unfortun option ffprobe ffmpeg numpi larg fit limit lambda space possibl run custom script endpoint",
        "Question_gpt_summary_original":"The user has encountered a challenge in deploying a custom script for a DL model in SageMaker. They want to run the script from Lambda, but the required tools such as ffprobe, ffmpeg, and numpy are too large to fit into the limited Lambda space. The user is seeking a solution to run their custom script as an endpoint in SageMaker.",
        "Question_gpt_summary":"user encount challeng deploi custom script model want run script lambda requir tool ffprobe ffmpeg numpi larg fit limit lambda space user seek solut run custom script endpoint",
        "Answer_original_content":"allow us custom docker imag aw document build custom contain imag pre built amazon contain imag us modifi advanc scenario packag script algorithm us us program languag framework develop contain creat docker imag code ffmpeg tensorflow test docker contain local deploi imag amazon ecr elast contain repositori creat model point imag detail learn tutori",
        "Answer_preprocessed_content":"allow us custom docker imag build custom contain imag amazon contain imag us modifi advanc scenario packag script algorithm us us program languag framework develop contain creat docker imag code test docker contain local deploi imag amazon ecr creat model point imag detail learn tutori",
        "Answer_gpt_summary_original":"Solution: The user can create a custom Docker image with their code and required tools such as FFmpeg, TensorFlow, and numpy. They can test the Docker container locally and deploy the image on Amazon ECR (Elastic Container Repository). Then, they can create a SageMaker model and point to the image. A tutorial is provided for more details.",
        "Answer_gpt_summary":"solut user creat custom docker imag code requir tool ffmpeg tensorflow numpi test docker contain local deploi imag amazon ecr elast contain repositori creat model point imag tutori provid detail"
    },
    {
        "Question_title":"Analyzing hyperparameters without actualy performing a sweep",
        "Question_body":"<p>Hy, I\u2019m in love with wandb, but I have a problem\u2026<\/p>\n<p>I have a simple question\u2026<\/p>\n<p>How can I analyze hyperparameters\u2026As seen in this picture, without actually creating a sweep.<\/p>\n<p>In my own code\u2026<\/p>\n<p><img src=\"https:\/\/mail.google.com\/mail\/u\/0?ui=2&amp;ik=8824e8d63e&amp;attid=0.1&amp;permmsgid=msg-a:r-1242756300606160728&amp;th=181d7b1a169f2ed0&amp;view=fimg&amp;fur=ip&amp;sz=s0-l75-ft&amp;attbid=ANGjdJ9LbpPclu5VUg_KiYT_9MyY2AbgyxXn6tmqz8qoKH2kUghMnyxeJstBhkIK4wCOgqfFHueuZ6ul6juIl6zvWD3lcsPXIvZAnZatibVLxPjneVvO-xSUoWLyCpM&amp;disp=emb&amp;realattid=ii_l5aqmkag2\" alt=\"68747470733a2f2f692e696d6775722e636f6d2f5455333451465a2e706e67.png\" width=\"339\" height=\"205\"><\/p>\n<p>I\u2019m preforming learning and for every model i\u2019m sending config with hyperparams\u2026<\/p>\n<p>wandb.finish(quiet=True)<br>\nwandb.init(<br>\nentity=var.WANDB_ENTITY,<br>\nproject=f\u2019{var.version} | {var.INPUT_DATASET}',<br>\ndir=str(var.working_dir),<br>\nconfig=utils.keras.hyper_params(hp))<\/p>\n<p>But in dashboard I dont see hyperparameters dashboard\u2026 And this makes me really sad !<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1657181342018,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":95.0,
        "Answer_body":"<p>I can\u2019t see the images above, but if you would like to create a <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/parallel-coordinates\">parallel coordinates plot<\/a>, you can do so using the UI by clicking \u201cadd panel\u201d in your workspace and choosing Parallel Coordinates.<\/p>\n<p>If you need to do this programmatically, one <em>very<\/em> recent feature would be to create a W&amp;B Report using our Api. You can programatically define what plots show up. It is a very new feature so it\u2019ll become better documented and more stable over time.<\/p>\n<p>Here\u2019s how you would create a Parallel Coordinates plot programmatically and save it in a report using Python.<\/p>\n<pre><code class=\"lang-auto\">import wandb\nimport wandb.apis.reports as wb\napi = wandb.Api()\nproject = 'pytorch-sweeps-demo'\nwandb.require('report-editing') # this is needed as of version 0.12.21 but will likely not be needed in future.\nreport = wb.Report(\n    project=project,\n    title='Sweep Results',\n    blocks=[\n            wb.PanelGrid(panels=[\n                 wb.ParallelCoordinatesPlot(\n                     columns=[wb.reports.PCColumn('batch_size'), wb.reports.PCColumn('epoch'), wb.reports.PCColumn('loss')])\n            ], runsets=[wb.RunSet(project=project)]),\n    ]\n)\nreport.save()\n<\/code><\/pre>\n<p>This will then show up in the Reports tab on your project.<br>\nAs this is a very fresh API, there may be issues or features that are not supported yet. I do apologise if that happens to you, I\u2019ll be happy to follow up and provide help.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/analyzing-hyperparameters-without-actualy-performing-a-sweep\/2719",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-07T08:09:39.921Z",
                "Answer_body":"<p>This is my project view\u2026<\/p>\n<p><img src=\"https:\/\/mail.google.com\/mail\/u\/0?ui=2&amp;ik=8824e8d63e&amp;attid=0.2&amp;permmsgid=msg-a:r-1242756300606160728&amp;th=181d7b1a169f2ed0&amp;view=fimg&amp;fur=ip&amp;sz=s0-l75-ft&amp;attbid=ANGjdJ8DnhFyENGDIPIM7ASNAvo8zDMxRkPPg4T64y7n_PDGfaFafO-BtYwsjGHvK9Zir7CTjiC-Tv2Zxm7oRkDY2vip3gc5Ui1UvrWr8PG80p2mWmhrBRb9qsCs2D4&amp;disp=emb&amp;realattid=ii_l5aqthrw4\" alt=\"image.png\" width=\"543\" height=\"332\"><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-07T08:10:24.887Z",
                "Answer_body":"<p>And this is my table view\u2026<\/p>\n<p><img src=\"https:\/\/mail.google.com\/mail\/u\/0?ui=2&amp;ik=8824e8d63e&amp;attid=0.3&amp;permmsgid=msg-a:r-1242756300606160728&amp;th=181d7b1a169f2ed0&amp;view=fimg&amp;fur=ip&amp;sz=s0-l75-ft&amp;attbid=ANGjdJ8GijPoQqT-6lpCvkV5gW1tUKKzvmkiHWx6IFFMNiBMknR0Xx6_rcgbQoLZxhFDhEi_k3Lb4yHy_0BXBd3_s9lJUXHQpsfyiztimA8yKDf98gCMvdoywQj00_0&amp;disp=emb&amp;realattid=ii_l5aqv44q5\" alt=\"image.png\" width=\"543\" height=\"250\"><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-07T18:37:19.909Z",
                "Answer_body":"<p>I can\u2019t see the images above, but if you would like to create a <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/parallel-coordinates\">parallel coordinates plot<\/a>, you can do so using the UI by clicking \u201cadd panel\u201d in your workspace and choosing Parallel Coordinates.<\/p>\n<p>If you need to do this programmatically, one <em>very<\/em> recent feature would be to create a W&amp;B Report using our Api. You can programatically define what plots show up. It is a very new feature so it\u2019ll become better documented and more stable over time.<\/p>\n<p>Here\u2019s how you would create a Parallel Coordinates plot programmatically and save it in a report using Python.<\/p>\n<pre><code class=\"lang-auto\">import wandb\nimport wandb.apis.reports as wb\napi = wandb.Api()\nproject = 'pytorch-sweeps-demo'\nwandb.require('report-editing') # this is needed as of version 0.12.21 but will likely not be needed in future.\nreport = wb.Report(\n    project=project,\n    title='Sweep Results',\n    blocks=[\n            wb.PanelGrid(panels=[\n                 wb.ParallelCoordinatesPlot(\n                     columns=[wb.reports.PCColumn('batch_size'), wb.reports.PCColumn('epoch'), wb.reports.PCColumn('loss')])\n            ], runsets=[wb.RunSet(project=project)]),\n    ]\n)\nreport.save()\n<\/code><\/pre>\n<p>This will then show up in the Reports tab on your project.<br>\nAs this is a very fresh API, there may be issues or features that are not supported yet. I do apologise if that happens to you, I\u2019ll be happy to follow up and provide help.<\/p>",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-07-07T20:03:14.632Z",
                "Answer_body":"<p>I have figured it out with the help of your support team, I would really suggest that you guys build the parallel coordinates plot by default since it\u2019s really hard for newcomers to figure this out.<\/p>\n<p>Now with this my soul is satisfied! Congratulation on such a great product!!!<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-05T20:03:18.514Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":1657219039908,
        "Question_original_content":"analyz hyperparamet actuali perform sweep love problem simpl question analyz hyperparametersa seen pictur actual creat sweep code preform learn model send config hyperparam finish quiet true init entiti var entiti project var version var input dataset dir str var work dir config util kera hyper param dashboard dont hyperparamet dashboard make sad",
        "Question_preprocessed_content":"analyz hyperparamet actuali perform sweep love problem simpl question analyz hyperparametersa seen pictur actual creat sweep code preform learn model send config hyperparam finish init dashboard dont hyperparamet dashboard make sad",
        "Question_gpt_summary_original":"The user is facing a challenge in analyzing hyperparameters without actually creating a sweep in their code. They are using wandb to send config with hyperparameters for every model they are performing learning on, but they are not able to see the hyperparameters dashboard in the dashboard.",
        "Question_gpt_summary":"user face challeng analyz hyperparamet actual creat sweep code send config hyperparamet model perform learn abl hyperparamet dashboard dashboard",
        "Answer_original_content":"imag like creat parallel coordin plot click add panel workspac choos parallel coordin need programmat recent featur creat report api programat defin plot new featur itll better document stabl time here creat parallel coordin plot programmat save report python import import api report api api project pytorch sweep demo requir report edit need version like need futur report report project project titl sweep result block panelgrid panel parallelcoordinatesplot column report pccolumn batch size report pccolumn epoch report pccolumn loss runset runset project project report save report tab project fresh api issu featur support apologis happen ill happi follow provid help",
        "Answer_preprocessed_content":"imag like creat parallel coordin plot click add panel workspac choos parallel coordin need programmat recent featur creat report api programat defin plot new featur itll better document stabl time here creat parallel coordin plot programmat save report python report tab project fresh api issu featur support apologis happen ill happi follow provid help",
        "Answer_gpt_summary_original":"Solution:\n- The user can create a parallel coordinates plot using the UI by clicking \"add panel\" in their workspace and choosing Parallel Coordinates.\n- The user can programmatically create a parallel coordinates plot and save it in a report using Python and the W&B API. They can define what plots show up in the report. This will then show up in the Reports tab on their project.",
        "Answer_gpt_summary":"solut user creat parallel coordin plot click add panel workspac choos parallel coordin user programmat creat parallel coordin plot save report python api defin plot report report tab project"
    },
    {
        "Question_title":"Vertex AI scheduled notebooks doesn't recognize existence of folders",
        "Question_body":"<p>I have a managed jupyter notebook in Vertex AI that I want to schedule. The notebook works just fine as long as I start it manually, but as soon as it is scheduled, it fails. There are in fact many things that go wrong when scheduled, some of them are fixable. Before explaining what my trouble is, let me first give some details of the context.<\/p>\n<p>The notebook gathers information from an API for several stores and saves the data in different folders before processing it, saving csv-files to store-specific folders and to bigquery. So, in the location of the notebook, I have:<\/p>\n<ul>\n<li>The notebook<\/li>\n<li>Functions needed for the handling of data (as *.py files)<\/li>\n<li>A series of folders, some of which have subfolders which also have subfolders<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xhOWs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xhOWs.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>When I execute this manually, no problem. Everything works well and all files end up exactly where they should, as well as in different bigQuery tables.<\/p>\n<p>However, when scheduling the execution of the notebook, everything goes wrong. First, the files *.py cannot be read (as <code>import<\/code>). No problem, I added the functions in the notebook.<\/p>\n<p>Now, the following error is where I am at a loss, because I have no idea why it does work or how to fix it. The code that leads to the error is the following:<\/p>\n<pre><code>internal = &quot;https:\/\/api.************************&quot;\n\ndf_descriptions = [] \n\nstoress = internal\nresponse_stores = requests.get(storess,auth = HTTPBasicAuth(userInternal, keyInternal))\npathlib.Path(&quot;stores\/request_1.json&quot;).write_bytes(response_stores.content)\n\nfilepath = &quot;stores&quot;\n\nfiles = os.listdir(filepath)\n\nfor file in files:\n    with open(filepath + &quot;\/&quot;+file) as json_string:\n        jsonstr = json.load(json_string)\n        information = pd.json_normalize(jsonstr)\n    df_descriptions.append(information)\n\nStoreINFO = pd.concat(df_descriptions)\nStoreINFO = StoreINFO.dropna()\nStoreINFO = StoreINFO[StoreINFO['storeIdMappings'].map(lambda d: len(d)) &gt; 0]\n\ncloud_store_ids = list(set(StoreINFO.cloudStoreId))\n\nLastWeek = datetime.date.today()- timedelta(days=2)\nLastWeek =np.datetime64(LastWeek)\n<\/code><\/pre>\n<p>and the error reported is:<\/p>\n<pre><code>FileNotFoundError                         Traceback (most recent call last)\n\/tmp\/ipykernel_165\/2970402631.py in &lt;module&gt;\n      5 storess = internal\n      6 response_stores = requests.get(storess,auth = HTTPBasicAuth(userInternal, keyInternal))\n----&gt; 7 pathlib.Path(&quot;stores\/request_1.json&quot;).write_bytes(response_stores.content)\n      8 \n      9 filepath = &quot;stores&quot;\n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in write_bytes(self, data)\n   1228         # type-check for the buffer interface before truncating the file\n   1229         view = memoryview(data)\n-&gt; 1230         with self.open(mode='wb') as f:\n   1231             return f.write(view)\n   1232 \n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in open(self, mode, buffering, encoding, errors, newline)\n   1206             self._raise_closed()\n   1207         return io.open(self, mode, buffering, encoding, errors, newline,\n-&gt; 1208                        opener=self._opener)\n   1209 \n   1210     def read_bytes(self):\n\n\/opt\/conda\/lib\/python3.7\/pathlib.py in _opener(self, name, flags, mode)\n   1061     def _opener(self, name, flags, mode=0o666):\n   1062         # A stub for the opener argument to built-in open()\n-&gt; 1063         return self._accessor.open(self, flags, mode)\n   1064 \n   1065     def _raw_open(self, flags, mode=0o777):\n\nFileNotFoundError: [Errno 2] No such file or directory: 'stores\/request_1.json'\n<\/code><\/pre>\n<p>I would gladly find another way to do this, for instance by using GCS buckets, but my issue is the existence of sub-folders. There are many stores and I do not wish to do this operation manually because some retailers for which I am doing this have over 1000 stores. My python code generates all these folders and as I understand it, this is not feasible in GCS.<\/p>\n<p>How can I solve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1650711286300,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1650713970783,
        "Question_score":0.0,
        "Question_view_count":247.0,
        "Answer_body":"<p>GCS uses a flat namespace, so folders don't actually exist, but can be simulated as given in this <a href=\"https:\/\/cloud.google.com\/storage\/docs\/folders\" rel=\"nofollow noreferrer\">documentation<\/a>.For your requirement, you can either use absolute path (starting with &quot;\/&quot; -- not relative) or create the &quot;stores&quot; <a href=\"https:\/\/docs.python.org\/3\/library\/pathlib.html#pathlib.Path.mkdir\" rel=\"nofollow noreferrer\">directory<\/a> (with &quot;mkdir&quot;). For more information you can check this <a href=\"https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/schedule-and-execute-notebooks-with-vertex-ai-workbench\" rel=\"nofollow noreferrer\">blog<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1651402328352,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71979012",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1651402011487,
        "Question_original_content":"schedul notebook recogn exist folder manag jupyt notebook want schedul notebook work fine long start manual soon schedul fail fact thing wrong schedul fixabl explain troubl let detail context notebook gather inform api store save data differ folder process save csv file store specif folder bigqueri locat notebook notebook function need handl data file seri folder subfold subfold execut manual problem work file end exactli differ bigqueri tabl schedul execut notebook goe wrong file read import problem ad function notebook follow error loss idea work fix code lead error follow intern http api descript storess intern respons store request storess auth httpbasicauth userintern keyintern pathlib path store request json write byte respons store content filepath store file listdir filepath file file open filepath file json string jsonstr json load json string inform json normal jsonstr descript append inform storeinfo concat descript storeinfo storeinfo dropna storeinfo storeinfo storeinfo storeidmap map lambda len cloud store id list set storeinfo cloudstoreid lastweek datetim date todai timedelta dai lastweek datetim lastweek error report filenotfounderror traceback recent tmp ipykernel storess intern respons store request storess auth httpbasicauth userintern keyintern pathlib path store request json write byte respons store content filepath store opt conda lib python pathlib write byte self data type check buffer interfac truncat file view memoryview data self open mode return write view opt conda lib python pathlib open self mode buffer encod error newlin self rais close return open self mode buffer encod error newlin open self open def read byte self opt conda lib python pathlib open self flag mode def open self flag mode stub open argument built open return self accessor open self flag mode def raw open self flag mode filenotfounderror errno file directori store request json gladli wai instanc gc bucket issu exist sub folder store wish oper manual retail store python code gener folder understand feasibl gc solv issu",
        "Question_preprocessed_content":"schedul notebook recogn exist folder manag jupyt notebook want schedul notebook work fine long start manual soon schedul fail fact thing wrong schedul fixabl explain troubl let detail context notebook gather inform api store save data differ folder process save folder bigqueri locat notebook notebook function need handl data seri folder subfold subfold execut manual problem work file end exactli differ bigqueri tabl schedul execut notebook goe wrong file read problem ad function notebook follow error loss idea work fix code lead error follow error report gladli wai instanc gc bucket issu exist store wish oper manual retail store python code gener folder understand feasibl gc solv issu",
        "Question_gpt_summary_original":"The user is facing challenges with scheduling a managed Jupyter notebook in Vertex AI. The notebook gathers information from an API for several stores and saves the data in different folders before processing it, saving csv-files to store-specific folders and to bigquery. When scheduling the execution of the notebook, the files *.py cannot be read, and the code that leads to the error is related to the existence of sub-folders. The user is looking for a solution to this issue.",
        "Question_gpt_summary":"user face challeng schedul manag jupyt notebook notebook gather inform api store save data differ folder process save csv file store specif folder bigqueri schedul execut notebook file read code lead error relat exist sub folder user look solut issu",
        "Answer_original_content":"gc us flat namespac folder actual exist simul given document requir us absolut path start rel creat store directori mkdir inform check blog",
        "Answer_preprocessed_content":"gc us flat namespac folder actual exist simul given requir us absolut path creat store directori inform check blog",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are to use absolute path or create the \"stores\" directory with \"mkdir\" to simulate the existence of sub-folders in GCS. The user can refer to the documentation and blog provided for more information.",
        "Answer_gpt_summary":"possibl solut mention discuss us absolut path creat store directori mkdir simul exist sub folder gc user refer document blog provid inform"
    },
    {
        "Question_title":"Unable to parametrize ML pipeline endpoint name - Azure Data Factory",
        "Question_body":"<p>Sorry for long post, I need to explain it properly for people to undertsand.<\/p>\n<p>I have a pipeline in datafctory that triggers a published AML endpoint:\n<a href=\"https:\/\/i.stack.imgur.com\/mKIeU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mKIeU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I am trying to parametrize this ADF pipeline so that I can deploy to test and prod, but on test and prod the aml endpoints are different.<\/p>\n<p>Therefore, I have tried to edit the <strong>parameter configuration<\/strong> in ADF as shows here:\n<a href=\"https:\/\/i.stack.imgur.com\/c4g7x.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/c4g7x.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Here in the section <code>Microsoft.DataFactory\/factories\/pipelines<\/code> I add <code>&quot;*&quot;:&quot;=&quot;<\/code> so that all the pipeline parameters are parametrized:<\/p>\n<pre><code> &quot;Microsoft.DataFactory\/factories\/pipelines&quot;: {\n        &quot;*&quot;: &quot;=&quot;\n    }\n<\/code><\/pre>\n<p>After this I export the template to see which parameters are there in json, there are lot of them but I do not see any paramter that has aml endpoint name as value, but I see the endpint ID is parametrized.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/7WRUL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7WRUL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>My question is: Is it possible to parametrize the AML endpoint by name? So that, when deploying ADF to test I can just provide the AML endpoint name and it can pick the id automatically:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/0Fu1g.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0Fu1g.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661258450327,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":67.0,
        "Answer_body":"<p>i faced the similar issue when deploying adf pipelines with ml between environments. Unfortunately, As of now, adf parameter file do not have ml pipeline name as parameter value. only turn around solution is modifiying the parameter file(json) file with aligns with your pipeline design. For example, i am triggering ml pipeline endpoint inside foreach activity--&gt;if condition--&gt;ml pipeline<\/p>\n<p>Here is my parameter file values:<\/p>\n<pre><code>&quot;Microsoft.DataFactory\/factories\/pipelines&quot;: {\n    &quot;properties&quot;: {\n        &quot;activities&quot;: [\n            {\n                &quot;typeProperties&quot;: {\n                    &quot;mlPipelineEndpointId&quot;: &quot;=&quot;,\n                    &quot;url&quot;: {\n                        &quot;value&quot;: &quot;=&quot;\n                    },\n                    &quot;ifFalseActivities&quot;: [\n                        {\n                            &quot;typeProperties&quot;: {\n                                &quot;mlPipelineEndpointId&quot;: &quot;=&quot;\n                            }\n                        }\n                    ],\n                    &quot;ifTrueActivities&quot;: [\n                        {\n                            &quot;typeProperties&quot;: {\n                                &quot;mlPipelineEndpointId&quot;: &quot;=&quot;\n                            }\n                        }\n                    ],\n                    &quot;activities&quot;: [\n                        {\n                            &quot;typeProperties&quot;: {\n                                &quot;mlPipelineEndpointId&quot;: &quot;=&quot;,\n                                &quot;ifFalseActivities&quot;: [\n                                    {\n                                        &quot;typeProperties&quot;: {\n                                            &quot;mlPipelineEndpointId&quot;: &quot;=&quot;,\n                                            &quot;url&quot;: &quot;=&quot;\n                                        }\n                                    }\n                                ],\n                                &quot;ifTrueActivities&quot;: [\n                                    {\n                                        &quot;typeProperties&quot;: {\n                                            &quot;mlPipelineEndpointId&quot;: &quot;=&quot;,\n                                            &quot;url&quot;: &quot;=&quot;\n                                        }\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    }\n}\n<\/code><\/pre>\n<p>after you export the ARM template, the json file has records for your ml endpoints<\/p>\n<pre><code>&quot;ADFPIPELINE_NAME_properties_1_typeProperties_1_typeProperties_0_typeProperties_mlPipelineEndpointId&quot;: {\n        &quot;value&quot;: &quot;445xxxxx-xxxx-xxxxx-xxxxx&quot;\n<\/code><\/pre>\n<p>it is lot of manual effort to maintain if design is frequently changing so far worked for me. Hope this answers your question.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73458933",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1663188036943,
        "Question_original_content":"unabl parametr pipelin endpoint azur data factori sorri long post need explain properli peopl undertsand pipelin datafctori trigger publish aml endpoint try parametr adf pipelin deploi test prod test prod aml endpoint differ tri edit paramet configur adf show section microsoft datafactori factori pipelin add pipelin paramet parametr microsoft datafactori factori pipelin export templat paramet json lot paramt aml endpoint valu endpint parametr question possibl parametr aml endpoint deploi adf test provid aml endpoint pick automat",
        "Question_preprocessed_content":"unabl parametr pipelin endpoint azur data factori sorri long post need explain properli peopl undertsand pipelin datafctori trigger publish aml endpoint try parametr adf pipelin deploi test prod test prod aml endpoint differ tri edit paramet configur adf show section add pipelin paramet parametr export templat paramet json lot paramt aml endpoint valu endpint parametr question possibl parametr aml endpoint deploi adf test provid aml endpoint pick automat",
        "Question_gpt_summary_original":"The user is facing a challenge in parametrizing the name of an Azure Machine Learning (AML) endpoint in an Azure Data Factory (ADF) pipeline. They have tried to edit the parameter configuration in ADF to parametrize all pipeline parameters, but they cannot find any parameter that has the AML endpoint name as a value. They are asking if it is possible to parametrize the AML endpoint by name so that they can provide the endpoint name when deploying ADF to test and it can pick the ID automatically.",
        "Question_gpt_summary":"user face challeng parametr aml endpoint azur data factori adf pipelin tri edit paramet configur adf parametr pipelin paramet paramet aml endpoint valu ask possibl parametr aml endpoint provid endpoint deploi adf test pick automat",
        "Answer_original_content":"face similar issu deploi adf pipelin environ unfortun adf paramet file pipelin paramet valu turn solut modifii paramet file json file align pipelin design exampl trigger pipelin endpoint insid foreach activ condit pipelin paramet file valu microsoft datafactori factori pipelin properti activ typeproperti mlpipelineendpointid url valu iffalseact typeproperti mlpipelineendpointid iftrueact typeproperti mlpipelineendpointid activ typeproperti mlpipelineendpointid iffalseact typeproperti mlpipelineendpointid url iftrueact typeproperti mlpipelineendpointid url export arm templat json file record endpoint adfpipelin properti typeproperti typeproperti typeproperti mlpipelineendpointid valu lot manual effort maintain design frequent chang far work hope answer question",
        "Answer_preprocessed_content":"face similar issu deploi adf pipelin environ unfortun adf paramet file pipelin paramet valu turn solut modifii paramet file file align pipelin design exampl trigger pipelin endpoint insid foreach pipelin paramet file valu export arm templat json file record endpoint lot manual effort maintain design frequent chang far work hope answer question",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that currently, there is no direct way to parametrize the name of an Azure Machine Learning (AML) endpoint in an Azure Data Factory (ADF) pipeline. However, the user can modify the parameter file (JSON) to align with their pipeline design and include the AML endpoint ID. The user can export the ARM template, and the JSON file will have records for their AML endpoints. This solution requires manual effort and may not be suitable if the pipeline design frequently changes.",
        "Answer_gpt_summary":"solut discuss suggest current direct wai parametr aml endpoint azur data factori adf pipelin user modifi paramet file json align pipelin design includ aml endpoint user export arm templat json file record aml endpoint solut requir manual effort suitabl pipelin design frequent chang"
    },
    {
        "Question_title":"How to deploy AWS using CDK, sagemaker?",
        "Question_body":"<p>I want to use this <a href=\"https:\/\/github.com\/amogh147\/binance_takeHome_gemini_amogh\" rel=\"nofollow noreferrer\">repo<\/a> and I have created and activated a virtualenv and installed the required dependencies.<\/p>\n<p>I get an error when I run pytest.<\/p>\n<p>And under the file binance_cdk\/app.py it describes the following tasks:<\/p>\n<h1>App (PSVM method) entry point of the program.<\/h1>\n<h1>Note:<\/h1>\n<p>Steps tp setup CDK:<\/p>\n<ol>\n<li>install npm<\/li>\n<li>cdk -init (creates an empty project)<\/li>\n<li>Add in your infrastructure code.<\/li>\n<li>Run CDK synth<\/li>\n<li>CDK bootstrap &lt;aws_account&gt;\/<\/li>\n<li>Run CDK deploy ---&gt; This creates a cloudformation .yml file and the aws resources will be created as per the mentioned stack.<\/li>\n<\/ol>\n<p>I'm stuck on step 3, what do I add in this infrastructure code, and if I want to use this on amazon sagemaker which I am not familiar with, do I even bother doing this on my local terminal, or do I do the whole process regardless on sagemaker?<\/p>\n<p>Thank you in advance for your time and answers !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655801685383,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":100.0,
        "Answer_body":"<p>The infrastructure code is the Python code that you want to write for the resources you want to provision with SageMaker. In the example you provided for example the infra code they have is creating a Lambda function. You can do this locally on your machine, the question is what do you want to achieve with SageMaker? If you want to create an endpoint then following the CDK Python docs with SageMaker to identify the steps for creating an endpoint. Here's two guides, the first is an introduction to the AWS CDK and getting started. The second is an example of using the CDK with SageMaker to create an endpoint for  inference.<\/p>\n<p>CDK Python Starter: <a href=\"https:\/\/towardsdatascience.com\/build-your-first-aws-cdk-project-18b1fee2ed2d\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/build-your-first-aws-cdk-project-18b1fee2ed2d<\/a>\nCDK SageMaker Example: <a href=\"https:\/\/github.com\/philschmid\/cdk-samples\/tree\/master\/sagemaker-serverless-huggingface-endpoint\" rel=\"nofollow noreferrer\">https:\/\/github.com\/philschmid\/cdk-samples\/tree\/master\/sagemaker-serverless-huggingface-endpoint<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72697889",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1655849768796,
        "Question_original_content":"deploi aw cdk want us repo creat activ virtualenv instal requir depend error run pytest file binanc cdk app describ follow task app psvm method entri point program note step setup cdk instal npm cdk init creat project add infrastructur code run cdk synth cdk bootstrap run cdk deploi creat cloudform yml file aw resourc creat mention stack stuck step add infrastructur code want us familiar bother local termin process regardless thank advanc time answer",
        "Question_preprocessed_content":"deploi aw cdk want us repo creat activ virtualenv instal requir depend error run pytest file describ follow task app entri point program note step setup cdk instal npm cdk init add infrastructur code run cdk synth cdk bootstrap run cdk deploi creat cloudform yml file aw resourc creat mention stack stuck step add infrastructur code want us familiar bother local termin process regardless thank advanc time answer",
        "Question_gpt_summary_original":"The user is encountering challenges in deploying AWS using CDK and Sagemaker. They have installed the required dependencies and activated a virtualenv but are getting an error when running pytest. The user is stuck on step 3 of setting up CDK and is unsure of what to add in the infrastructure code. They are also unsure if they should do the whole process on Sagemaker or on their local terminal.",
        "Question_gpt_summary":"user encount challeng deploi aw cdk instal requir depend activ virtualenv get error run pytest user stuck step set cdk unsur add infrastructur code unsur process local termin",
        "Answer_original_content":"infrastructur code python code want write resourc want provis exampl provid exampl infra code creat lambda function local machin question want achiev want creat endpoint follow cdk python doc identifi step creat endpoint guid introduct aw cdk get start second exampl cdk creat endpoint infer cdk python starter http towardsdatasci com build aw cdk project bfeeedd cdk exampl http github com philschmid cdk sampl tree master serverless huggingfac endpoint",
        "Answer_preprocessed_content":"infrastructur code python code want write resourc want provis exampl provid exampl infra code creat lambda function local machin question want achiev want creat endpoint follow cdk python doc identifi step creat endpoint guid introduct aw cdk get start second exampl cdk creat endpoint infer cdk python starter cdk exampl",
        "Answer_gpt_summary_original":"Solutions provided in the discussion include creating the infrastructure code locally on the user's machine and following the CDK Python docs with SageMaker to identify the steps for creating an endpoint. Two guides were also provided for reference: CDK Python Starter and CDK SageMaker Example. No personal opinions or biases were included in the summary.",
        "Answer_gpt_summary":"solut provid discuss includ creat infrastructur code local user machin follow cdk python doc identifi step creat endpoint guid provid refer cdk python starter cdk exampl person opinion bias includ summari"
    },
    {
        "Question_title":"Azure ML: Include additional files during model deployment",
        "Question_body":"<p>In my AML pipeline, I've got a model built and deployed to the AciWebservice. I now have a need to include some additional data that would be used by score.py. This data is in json format (~1mb) and is specific to the model that's built. To accomplish this, I was thinking of sticking this file in blob store and updating some \"placholder\" vars in the score.py during deployment, but it seems hacky. <\/p>\n\n<p>Here are some options I was contemplating but wasn't sure on the practicality<\/p>\n\n<p><strong>Option 1:<\/strong>\nIs it possible to include this file, during the model deployment itself so that it's part of the docker image? <\/p>\n\n<p><strong>Option 2:<\/strong>\nAnother possibility I was contemplating, would it be possible to include this json data part of the Model artifacts?<\/p>\n\n<p><strong>Option 3:<\/strong>\nHow about registering it as a dataset and pull that in the score file?<\/p>\n\n<p>What is the recommended way to deploy dependent files in a model deployment scenario?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1589475408817,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1589479621848,
        "Question_score":3.0,
        "Question_view_count":866.0,
        "Answer_body":"<p>There are few ways to accomplish this:<\/p>\n\n<ol>\n<li><p>Put the additional file in the same folder as your model file, and <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none--sample-input-dataset-none--sample-output-dataset-none--resource-configuration-none-\" rel=\"nofollow noreferrer\">register<\/a> the whole folder as the model. In this approach the file is stored alongside the model.<\/p><\/li>\n<li><p>Put the file in a local folder, and specify that folder as source_directory in <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py\" rel=\"nofollow noreferrer\">InferenceConfig<\/a>. In this approach the file is re-uploaded every time you deploy a new endpoint.<\/p><\/li>\n<li><p>Use custom base image in InferenceConfig to bake the file into Docker image itself.<\/p><\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61803031",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1589484511663,
        "Question_original_content":"includ addit file model deploy aml pipelin got model built deploi aciwebservic need includ addit data score data json format specif model built accomplish think stick file blob store updat plachold var score deploy hacki option contempl wasn sure practic option possibl includ file model deploy docker imag option possibl contempl possibl includ json data model artifact option regist dataset pull score file recommend wai deploi depend file model deploy scenario",
        "Question_preprocessed_content":"includ addit file model deploy aml pipelin got model built deploi aciwebservic need includ addit data data json format specif model built accomplish think stick file blob store updat plachold var deploy hacki option contempl wasn sure practic option possibl includ file model deploy docker imag option possibl contempl possibl includ json data model artifact option regist dataset pull score file recommend wai deploi depend file model deploy scenario",
        "Question_gpt_summary_original":"The user has encountered a challenge in including additional data in json format (~1mb) that would be used by score.py during model deployment in Azure ML pipeline. The user is considering different options such as including the file during model deployment, including the json data as part of the model artifacts, or registering it as a dataset and pulling it in the score file. The user is seeking recommendations on the best way to deploy dependent files in a model deployment scenario.",
        "Question_gpt_summary":"user encount challeng includ addit data json format score model deploy pipelin user consid differ option includ file model deploy includ json data model artifact regist dataset pull score file user seek recommend best wai deploi depend file model deploy scenario",
        "Answer_original_content":"wai accomplish addit file folder model file regist folder model approach file store alongsid model file local folder specifi folder sourc directori inferenceconfig approach file upload time deploi new endpoint us custom base imag inferenceconfig bake file docker imag",
        "Answer_preprocessed_content":"wai accomplish addit file folder model file regist folder model approach file store alongsid model file local folder specifi folder inferenceconfig approach file time deploi new endpoint us custom base imag inferenceconfig bake file docker imag",
        "Answer_gpt_summary_original":"The discussion provides three possible solutions to the challenge of including additional data in json format during model deployment in Azure ML pipeline. The first solution is to put the additional file in the same folder as the model file and register the whole folder as the model. The second solution is to put the file in a local folder and specify that folder as source_directory in InferenceConfig. The third solution is to use a custom base image in InferenceConfig to bake the file into the Docker image itself.",
        "Answer_gpt_summary":"discuss provid possibl solut challeng includ addit data json format model deploy pipelin solut addit file folder model file regist folder model second solut file local folder specifi folder sourc directori inferenceconfig solut us custom base imag inferenceconfig bake file docker imag"
    },
    {
        "Question_title":"Failed to pull existing files from SSH DVC Remote",
        "Question_body":"<p>After running <code>dvc push data.csv<\/code> (to ssh-remote), when i try to dvc-pull the same file on another machine from the same remote, it won't get pulled. Below are the logs and the error:<\/p>\n<pre><code>2021-01-21 22:17:26,643 DEBUG: checking if 'data.csv'('HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)') has changed.\n2021-01-21 22:17:26,643 DEBUG: 'data.csv' doesn't exist.\n2021-01-21 22:17:26,644 WARNING: Cache 'HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)' not found. File 'data.csv' won't be created.\n2021-01-21 22:17:26,644 DEBUG: cache '\/usr\/src\/bohr\/.dvc\/cache\/27\/9936268f488e1e613f81a537f29055' expected 'HashInfo(name='md5', value='279936268f488e1e613f81a537f29055', dir_info=None, size=1458311, nfiles=None)' actual 'None'\n...\n2021-01-21 22:17:26,660 ERROR: failed to pull data from the cloud - Checkout failed for following targets:\ndata.csv\n<\/code><\/pre>\n<p>However, the file is present on the remote:<\/p>\n<pre><code>$ ls -la ~\/.dvcstorage\/bohr\/27\/9936268f488e1e613f81a537f29055\n-rw-rw-r-- 1 hbabii hbabii 1458311 Jan 22 00:19 \/home\/hbabii\/.dvcstorage\/bohr\/27\/9936268f488e1e613f81a537f29055\n<\/code><\/pre>\n<p>I double-checked that I am pulling from and pushing to the same remote. I am using DVC v1.11.11.<\/p>\n<p>Could you please give me any hints on what could be wrong?<\/p>\n<p>Cheers, Hlib<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1611327752360,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":1715.0,
        "Answer_body":"<p>In the end, the problem was that I indeed was pulling from the wrong remote (I had multiple remotes, their configuration was tricky, and local configurations differed on different machines).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65847574",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1616082756536,
        "Question_original_content":"fail pull exist file ssh remot run push data csv ssh remot try pull file machin remot won pull log error debug check data csv hashinfo valu feefaf dir info size nfile chang debug data csv exist warn cach hashinfo valu feefaf dir info size nfile file data csv won creat debug cach usr src bohr cach feefaf expect hashinfo valu feefaf dir info size nfile actual error fail pull data cloud checkout fail follow target data csv file present remot storag bohr feefaf hbabii hbabii jan home hbabii storag bohr feefaf doubl check pull push remot hint wrong cheer hlib",
        "Question_preprocessed_content":"fail pull exist file ssh remot run try pull file machin remot won pull log error file present remot pull push remot hint wrong cheer hlib",
        "Question_gpt_summary_original":"The user is facing a challenge in pulling an existing file from an SSH DVC remote. Despite pushing the file successfully, the file won't get pulled on another machine from the same remote. The logs show that the cache is not found, and an error message appears, indicating that the checkout failed for the target file. However, the file is present on the remote, and the user has confirmed that they are pulling from and pushing to the same remote. The user is seeking hints on what could be wrong.",
        "Question_gpt_summary":"user face challeng pull exist file ssh remot despit push file successfulli file won pull machin remot log cach error messag appear indic checkout fail target file file present remot user confirm pull push remot user seek hint wrong",
        "Answer_original_content":"end problem pull wrong remot multipl remot configur tricki local configur differ differ machin",
        "Answer_preprocessed_content":"end problem pull wrong remot",
        "Answer_gpt_summary_original":"Solution: The user found out that they were pulling from the wrong remote, and the configuration was tricky, with local configurations differing on different machines.",
        "Answer_gpt_summary":"solut user pull wrong remot configur tricki local configur differ differ machin"
    },
    {
        "Question_title":"Compare String with list of strings in bash",
        "Question_body":"<p>I am trying to compare the service with a list of available service names, if service is found in the list then do update the service otherwise deploy the service.\nBut below condition only deploying new service even when service available in list variable?<\/p>\n<pre><code>SERVNAME=ner\nSERVICE=$(az ml service list -g $(ml_rg) --workspace-name $(ml_ws) --model-name $(model_name) --query &quot;[].name&quot;)\n\nif [[ &quot;$SERVNAME&quot; == &quot;$SERVICE&quot; ]];\nthen\n   echo &quot;Service Found: $(SERVNAME) and updating the service&quot;\n   az ml service update --name $(AKS_DEPLOYMENT_NAME) \\\n          --model '$(MODEL_NAME):$(MODEL_VERSION)' \\\n          --dc aksDeploymentConfig.json \\\n          --ic inferenceConfig.json \\\n          -e $(ml_env_name) --ev $(ml_env_version) \\\n          -g $(ml_rg) --workspace-name $(ml_ws) -v ;\nelse\n   echo &quot;Service Not found and starting deploying new service&quot;\n   az ml model deploy --name $(AKS_DEPLOYMENT_NAME) --model \\\n   '$(MODEL_NAME):$(MODEL_VERSION)' \\\n          --compute-target $(ml_aks_name) \\\n          --ic inferenceConfig.json \\\n          -e $(ml_env_name) --ev $(ml_env_version) \\\n          --dc aksDeploymentConfig.json \\\n          -g $(ml_rg) --workspace-name $(ml_ws) \\\n          --overwrite -v ;\nfi\n<\/code><\/pre>\n<p>Example list<\/p>\n<pre><code>SERVNAME=&quot;ner&quot;\nSERVICE=[ &quot;ner&quot;, &quot;aks-gpu-ner-0306210907&quot;, &quot;aks-gpu-ner-30012231&quot;, &quot;aks-gpu-ner-1305211336&quot;]\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1633936791707,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1633953732907,
        "Question_score":1.0,
        "Question_view_count":1153.0,
        "Answer_body":"<p>Assuming the <code>az ml<\/code> command returns a json array string and you want to\ncheck if the array includes the value of variable <code>SERVNAME<\/code>, would you\nplease try:<\/p>\n<pre><code>SERVNAME=&quot;ner&quot;\nSERVICE='[ &quot;ner&quot;, &quot;aks-gpu-ner-0306210907&quot;, &quot;aks-gpu-ner-30012231&quot;, &quot;aks-gpu-ner-1305211336&quot;]'\n\nif [[ $SERVICE =~ &quot;\\&quot;$SERVNAME\\&quot;&quot; ]]; then\n    echo &quot;Service Found&quot;\n    # put your command here to update the service\nelse\n    echo &quot;Service Not Found&quot;\n    # put your command here to deploy new service\nfi\n<\/code><\/pre>\n<p>The regex operator <code>$SERVICE =~ &quot;\\&quot;$SERVNAME\\&quot;&quot;<\/code> matches if the string <code>$SERVICE<\/code>\ncontains the substring <code>$SERVNAME<\/code> enclosed with double quotes.<\/p>\n<p>If <code>jq<\/code> is available, you could also say:<\/p>\n<pre><code>result=$(echo &quot;$SERVICE&quot; | jq --arg var &quot;$SERVNAME&quot; '. | index($var)')\nif [[ $result != &quot;null&quot; ]]; then\n    echo &quot;Service Found&quot;\nelse\n    echo &quot;Service Not Found&quot;\nfi\n<\/code><\/pre>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":1634278832727,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69522401",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1634000233752,
        "Question_original_content":"compar string list string bash try compar servic list avail servic name servic list updat servic deploi servic condit deploi new servic servic avail list variabl servnam ner servic servic list workspac model model queri servnam servic echo servic servnam updat servic servic updat ak deploy model model model version aksdeploymentconfig json inferenceconfig json env env version workspac echo servic start deploi new servic model deploi ak deploy model model model version comput target ak inferenceconfig json env env version aksdeploymentconfig json workspac overwrit exampl list servnam ner servic ner ak gpu ner ak gpu ner ak gpu ner",
        "Question_preprocessed_content":"compar string list string bash try compar servic list avail servic name servic list updat servic deploi servic condit deploi new servic servic avail list variabl exampl list",
        "Question_gpt_summary_original":"The user is attempting to compare a service name with a list of available service names in a bash script. If the service is found in the list, the script should update the service, otherwise, it should deploy a new service. However, the script is only deploying a new service even when the service is available in the list variable.",
        "Question_gpt_summary":"user attempt compar servic list avail servic name bash script servic list script updat servic deploi new servic script deploi new servic servic avail list variabl",
        "Answer_original_content":"assum command return json arrai string want check arrai includ valu variabl servnam try servnam ner servic ner ak gpu ner ak gpu ner ak gpu ner servic servnam echo servic command updat servic echo servic command deploi new servic regex oper servic servnam match string servic contain substr servnam enclos doubl quot avail result echo servic arg var servnam index var result null echo servic echo servic",
        "Answer_preprocessed_content":"assum command return json arrai string want check arrai includ valu variabl try regex oper match string contain substr enclos doubl quot avail",
        "Answer_gpt_summary_original":"Solution:\nThe discussion provides two possible solutions to the challenge. The first solution suggests using a regex operator to match the string variable with the list of available services. If the service is found, the script should update the service, otherwise, it should deploy a new service. The second solution suggests using the `jq` command to check if the service is available in the list. If the service is found, the script should update the service, otherwise, it should deploy a new service.",
        "Answer_gpt_summary":"solut discuss provid possibl solut challeng solut suggest regex oper match string variabl list avail servic servic script updat servic deploi new servic second solut suggest command check servic avail list servic script updat servic deploi new servic"
    },
    {
        "Question_title":"How do I call a SageMaker Endpoint using the AWS CLI (",
        "Question_body":"<p>I'm trying to invoke the iris endpointfrom the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\/tensorflow_iris_dnn_classifier_using_estimators.ipynb\" rel=\"nofollow noreferrer\">SageMaker example notebooks<\/a> using the aws cli. I've tried using the following command:<\/p>\n\n<pre><code>!aws sagemaker-runtime invoke-endpoint \\\n--endpoint-name sagemaker-tensorflow-py2-cpu-2018-03-19-21-27-52-956 \\\n--body \"[6.4, 3.2, 4.5, 1.5]\" \\\n--content-type \"application\/json\" \\\noutput.json\n<\/code><\/pre>\n\n<p>I get the following response:<\/p>\n\n<pre><code>{\n    \"InvokedProductionVariant\": \"AllTraffic\", \n    \"ContentType\": \"*\/*\"\n}\n<\/code><\/pre>\n\n<p>What am I doing wrong?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1521505092823,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":4027.0,
        "Answer_body":"<p>If you've gotten that response, your request is successful. The output should be in the output file you specified - output.json :)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49374476",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1521567781630,
        "Question_original_content":"endpoint aw cli try invok iri endpointfrom exampl notebook aw cli tri follow command runtim invok endpoint endpoint tensorflow cpu bodi content type applic json output json follow respons invokedproductionvari alltraff contenttyp wrong",
        "Question_preprocessed_content":"endpoint aw cli try invok iri endpointfrom exampl notebook aw cli tri follow command follow respons wrong",
        "Question_gpt_summary_original":"The user is having trouble calling a SageMaker endpoint using the AWS CLI. They have tried using a command but are receiving an unexpected response and are unsure of what they are doing wrong.",
        "Question_gpt_summary":"user have troubl call endpoint aw cli tri command receiv unexpect respons unsur wrong",
        "Answer_original_content":"gotten respons request success output output file specifi output json",
        "Answer_preprocessed_content":"gotten respons request success output output file specifi",
        "Answer_gpt_summary_original":"Solution: The user has successfully made the request and the output should be in the specified output file. No further solutions are mentioned.",
        "Answer_gpt_summary":"solut user successfulli request output specifi output file solut mention"
    },
    {
        "Question_title":"Google Cloud Vertex AI with Golang: rpc error: code = Unimplemented desc = unexpected HTTP status code received from server: 404 (Not Found)",
        "Question_body":"<p>I have a Vertex AI model deployed on an endpoint and want to do some prediction from my app in Golang.<\/p>\n<p>To do this I create code inspired by this example : <a href=\"https:\/\/cloud.google.com\/go\/docs\/reference\/cloud.google.com\/go\/aiplatform\/latest\/apiv1?hl=en\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/go\/docs\/reference\/cloud.google.com\/go\/aiplatform\/latest\/apiv1?hl=en<\/a><\/p>\n<pre><code>const file = &quot;MY_BASE64_IMAGE&quot;\n\nfunc main() {\n\n    ctx := context.Background()\n\n    c, err := aiplatform.NewPredictionClient(cox)\n    if err != nil {\n        log.Printf(&quot;QueryVertex NewPredictionClient - Err:%s&quot;, err)\n    }\n    defer c.Close()\n\n    parameters, err := structpb.NewValue(map[string]interface{}{\n        &quot;confidenceThreshold&quot;: 0.2,\n        &quot;maxPredictions&quot;:      5,\n    })\n    if err != nil {\n        log.Printf(&quot;QueryVertex structpb.NewValue parameters - Err:%s&quot;, err)\n    }\n\n    instance, err := structpb.NewValue(map[string]interface{}{\n        &quot;content&quot;: file,\n    })\n    if err != nil {\n        log.Printf(&quot;QueryVertex structpb.NewValue instance - Err:%s&quot;, err)\n    }\n\n    reqP := &amp;aiplatformpb.PredictRequest{\n        Endpoint:   &quot;projects\/PROJECT_ID\/locations\/LOCATION_ID\/endpoints\/ENDPOINT_ID&quot;,\n        Instances:  []*structpb.Value{instance},\n        Parameters: parameters,\n    }\n\n    resp, err := c.Predict(cox, reqP)\n    if err != nil {\n        log.Printf(&quot;QueryVertex Predict - Err:%s&quot;, err)\n    }\n\n    log.Printf(&quot;QueryVertex Res:%+v&quot;, resp)\n}\n<\/code><\/pre>\n<p>I put the path to my service account JSON file on GOOGLE_APPLICATION_CREDENTIALS environment variable.\nBut when I run my test app I obtain this error message:<\/p>\n<pre><code>QueryVertex Predict - Err:rpc error: code = Unimplemented desc = unexpected HTTP status code received from server: 404 (Not Found); transport: received unexpected content-type &quot;text\/html; charset=UTF-8&quot;\nQueryVertex Res:&lt;nil&gt;\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_time":1651731616723,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1654669807780,
        "Question_score":3.0,
        "Question_view_count":455.0,
        "Answer_body":"<p>As @DazWilkin suggested, configure the client option to specify the specific <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest#service-endpoint\" rel=\"noreferrer\">regional endpoint<\/a> with a port 443:<\/p>\n<pre><code>option.WithEndpoint(&quot;&lt;region&gt;-aiplatform.googleapis.com:443&quot;)\n<\/code><\/pre>\n<p>Try like below:<\/p>\n<pre><code>func main() {\n \n   ctx := context.Background()\n   c, err := aiplatform.NewPredictionClient(\n       ctx,\n       option.WithEndpoint(&quot;&lt;region&gt;-aiplatform.googleapis.com:443&quot;),\n   )\n   if err != nil {\n       log.Printf(&quot;QueryVertex NewPredictionClient - Err:%s&quot;, err)\n   }\n   defer c.Close()\n       .\n       .\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1652712010567,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72122744",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1652647881720,
        "Question_original_content":"googl cloud golang rpc error code unimpl desc unexpect http statu code receiv server model deploi endpoint want predict app golang creat code inspir exampl http cloud googl com doc refer cloud googl com aiplatform latest apiv const file base imag func main ctx context background err aiplatform newpredictioncli cox err nil log printf queryvertex newpredictioncli err err defer close paramet err structpb newvalu map string interfac confidencethreshold maxpredict err nil log printf queryvertex structpb newvalu paramet err err instanc err structpb newvalu map string interfac content file err nil log printf queryvertex structpb newvalu instanc err err reqp aiplatformpb predictrequest endpoint project project locat locat endpoint endpoint instanc structpb valu instanc paramet paramet resp err predict cox reqp err nil log printf queryvertex predict err err log printf queryvertex re resp path servic account json file googl applic credenti environ variabl run test app obtain error messag queryvertex predict err rpc error code unimpl desc unexpect http statu code receiv server transport receiv unexpect content type text html charset utf queryvertex re",
        "Question_preprocessed_content":"googl cloud golang rpc error code unimpl desc unexpect http statu code receiv server model deploi endpoint want predict app golang creat code inspir exampl path servic account json file environ variabl run test app obtain error messag",
        "Question_gpt_summary_original":"The user is encountering an error while trying to do prediction from their Golang app using a Vertex AI model deployed on an endpoint. They have created code inspired by an example, but when they run their test app, they receive an error message stating \"rpc error: code = Unimplemented desc = unexpected HTTP status code received from server: 404 (Not Found).\" The error occurs despite setting the path to the service account JSON file on the GOOGLE_APPLICATION_CREDENTIALS environment variable.",
        "Question_gpt_summary":"user encount error try predict golang app model deploi endpoint creat code inspir exampl run test app receiv error messag state rpc error code unimpl desc unexpect http statu code receiv server error occur despit set path servic account json file googl applic credenti environ variabl",
        "Answer_original_content":"dazwilkin suggest configur client option specifi specif region endpoint port option withendpoint aiplatform googleapi com try like func main ctx context background err aiplatform newpredictioncli ctx option withendpoint aiplatform googleapi com err nil log printf queryvertex newpredictioncli err err defer close",
        "Answer_preprocessed_content":"suggest configur client option specifi specif region endpoint port try like",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to configure the client option to specify the specific regional endpoint with a port 443. The user can try setting the endpoint like this: \n\n```\noption.WithEndpoint(\"<region>-aiplatform.googleapis.com:443\")\n```\n\nThen, they can create a new prediction client and pass the context and endpoint option to it.",
        "Answer_gpt_summary":"solut solut suggest discuss configur client option specifi specif region endpoint port user try set endpoint like option withendpoint aiplatform googleapi com creat new predict client pass context endpoint option"
    },
    {
        "Question_title":"Is there a workaround to make opencensus work with MLFlow?",
        "Question_body":"<p>I'm not able to import mlflow after having launched a log with opencensus Azure.\nThe MLFlow import runs forever.<\/p>\n<p>My environment is the following:<\/p>\n<ul>\n<li>Python 3.7<\/li>\n<li>opencensus-ext-azure 1.0.7<\/li>\n<li>opencensus-ext-logging 0.1.0<\/li>\n<li>mlflow 1.15.0<\/li>\n<\/ul>\n<p>Here is the code to repoduce the bug:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import logging\n\nfrom opencensus.ext.azure.log_exporter import AzureLogHandler\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(AzureLogHandler(connection_string='InstrumentationKey=&lt;your-key&gt;'))\nlogger.warning('Hello, World!')\n\nimport mlflow\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619011270043,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":41.0,
        "Answer_body":"<p>I found a workaround, not the cleanest one though.<\/p>\n<p>I import mlflow at the beginning even if it's not useful this way:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\nimport logging\n\nfrom opencensus.ext.azure.log_exporter import AzureLogHandler\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(AzureLogHandler(connection_string='InstrumentationKey=&lt;your-key&gt;'))\nlogger.warning('Hello, World!')\n\nimport mlflow\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67196775",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1619077377892,
        "Question_original_content":"workaround opencensu work abl import have launch log opencensu azur import run forev environ follow python opencensu ext azur opencensu ext log code repoduc bug import log opencensu ext azur log export import azureloghandl logger log getlogg logger addhandl azureloghandl connect string instrumentationkei logger warn hello world import",
        "Question_preprocessed_content":"workaround opencensu work abl import have launch log opencensu azur import run forev environ follow python code repoduc bug",
        "Question_gpt_summary_original":"The user is facing a challenge in importing MLFlow after launching a log with opencensus Azure. The MLFlow import runs indefinitely, and the user has provided the environment details and code to reproduce the issue.",
        "Question_gpt_summary":"user face challeng import launch log opencensu azur import run indefinit user provid environ detail code reproduc issu",
        "Answer_original_content":"workaround cleanest import begin us wai import import log opencensu ext azur log export import azureloghandl logger log getlogg logger addhandl azureloghandl connect string instrumentationkei logger warn hello world import",
        "Answer_preprocessed_content":"workaround cleanest import begin us wai",
        "Answer_gpt_summary_original":"Solution: One possible workaround mentioned in the discussion is to import MLFlow at the beginning of the code, even if it's not useful in that way. This can be done by adding the line \"import mlflow\" before the logger code. However, it is mentioned that this is not the cleanest solution.",
        "Answer_gpt_summary":"solut possibl workaround mention discuss import begin code us wai ad line import logger code mention cleanest solut"
    },
    {
        "Question_title":"Auth Problems with Machine Learning Execute Pipeline Activity.",
        "Question_body":"Hello. Can anyone help with this error? Can not execute Azure ML activity from ADF.\nEverything was ok, no changes was done but suddenly(two-three days ago) I got this error.\n\n Request sent to Azure ML Service for operation 'submitMLPipelineRun' failed with http status code 'Forbidden'. Error message from Azure ML Service: '{ \"error\": { \"code\": \"UserError\", \"severity\": null, \"message\": \"Identity does not have permissions for Microsoft.MachineLearningServices\/workspaces\/experiments\/runs\/submit\/action, Microsoft.MachineLearningServices\/workspaces\/endpoints\/pipelines\/read actions.\", \"messageFormat\": null, \"messageParameters\": null, \"referenceCode\": null, \"detailsUri\": null, \"target\": null, \"details\": [], \"innerError\": { \"code\": \"ForbiddenError\", \"innerError\": null } '.",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_time":1621952386747,
        "Question_favorite_count":12.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@DenisBruk-6507\n\nWe have identified the issue and a hot fix is rolling out. It will be fixed in all regions by end of today. Sorry for the experience.\n\n\n\n\nRegards,\nYutong",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/408869\/auth-problems-with-machine-learning-execute-pipeli.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-05-25T19:35:57.963Z",
                "Answer_score":1,
                "Answer_body":"@DenisBruk-6507\n\nWe have identified the issue and a hot fix is rolling out. It will be fixed in all regions by end of today. Sorry for the experience.\n\n\n\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-05-25T17:34:38.503Z",
                "Answer_score":1,
                "Answer_body":"@YutongTie-5848 I think that this may also be related to our issue. I'm also getting a \"code\": \"ForbiddenError\" and the problem for this user arose at almost exactly the same time that it arose for us.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-26T17:41:58.203Z",
                "Answer_score":1,
                "Answer_body":"Hello everyone,\n\nThis issue should be fixed. Please check and let me know if you are still facing any issue.\n\n\n\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":1621971357963,
        "Question_original_content":"auth problem machin learn execut pipelin activ hello help error execut activ adf chang suddenli dai ago got error request sent servic oper submitmlpipelinerun fail http statu code forbidden error messag servic error code usererror sever null messag ident permiss microsoft machinelearningservic workspac experi run submit action microsoft machinelearningservic workspac endpoint pipelin read action messageformat null messageparamet null referencecod null detailsuri null target null detail innererror code forbiddenerror innererror null",
        "Question_preprocessed_content":"auth problem machin learn execut pipelin activ hello help error execut activ adf chang dai ago got error request sent servic oper submitmlpipelinerun fail http statu code forbidden error messag servic",
        "Question_gpt_summary_original":"The user is encountering an error while trying to execute an Azure ML activity from ADF. The error message indicates that the user's identity does not have the necessary permissions for certain actions related to Microsoft Machine Learning Services workspaces, experiments, runs, and pipelines. The error occurred suddenly without any changes being made.",
        "Question_gpt_summary":"user encount error try execut activ adf error messag indic user ident necessari permiss certain action relat microsoft machin learn servic workspac experi run pipelin error occur suddenli chang",
        "Answer_original_content":"denisbruk identifi issu hot fix roll fix region end todai sorri experi regard yutong",
        "Answer_preprocessed_content":"identifi issu hot fix roll fix region end todai sorri experi regard yutong",
        "Answer_gpt_summary_original":"Solution: A hot fix has been identified and is being rolled out to fix the issue. It will be fixed in all regions by the end of the day.",
        "Answer_gpt_summary":"solut hot fix identifi roll fix issu fix region end dai"
    },
    {
        "Question_title":"How to avoid error \"conda --version: conda not found\" in az ml run --submit-script command?",
        "Question_body":"<p>I would like to run a test script on an existing compute instance of Azure using the Azure Machine Learning extension to the Azure CLI:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>az ml run submit-script test.py --target compute-instance-test --experiment-name test_example --resource-group ex-test-rg\n<\/code><\/pre>\n<p>I get a Service Error with the following error message:<\/p>\n<pre><code>Unable to run conda package manager. AzureML uses conda to provision python\\nenvironments from a dependency specification. To manage the python environment\\nmanually instead, set userManagedDependencies to True in the python environment\\nconfiguration. To use system managed python environments, install conda from:\\nhttps:\/\/conda.io\/miniconda.html\n<\/code><\/pre>\n<p>But when I connect to the compute instance through the Azure portal and select the default Python kernel, <code>conda --version<\/code> prints 4.5.12. So conda is effectively already installed on the compute instance. This is why I do not understand the error message.<\/p>\n<p>Further information on the azure versions:<\/p>\n<pre><code>  &quot;azure-cli&quot;: &quot;2.12.1&quot;,\n  &quot;azure-cli-core&quot;: &quot;2.12.1&quot;,\n  &quot;azure-cli-telemetry&quot;: &quot;1.0.6&quot;,\n  &quot;extensions&quot;: {\n    &quot;azure-cli-ml&quot;: &quot;1.15.0&quot;\n  }\n<\/code><\/pre>\n<p>The image I use is:<\/p>\n<pre><code>mcr.microsoft.com\/azure-cli:latest\n<\/code><\/pre>\n<p>Can somebody please explain as to why I am getting this error and help me resolve the error? Thank you!<\/p>\n<p>EDIT: I tried to update the environment in which the <code>az ml run<\/code>-command is run.\nEssentially this is my GitLab job. The installation of miniconda is a bit complicated as the azure-cli uses an alpine Linux image (reference: <a href=\"https:\/\/stackoverflow.com\/questions\/47177538\/installing-miniconda-on-alpine-linux-fails\">Installing miniconda on alpine linux fails<\/a>). I replaced some names with ... and cut out some irrelevant pieces of code.<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>test:\n  image: 'mcr.microsoft.com\/azure-cli:latest'\n  script:\n    - echo &quot;Download conda&quot;\n    - apk --update add bash curl wget ca-certificates libstdc++ glib\n    - wget -q -O \/etc\/apk\/keys\/sgerrand.rsa.pub https:\/\/raw.githubusercontent.com\/sgerrand\/alpine-pkg-node-bower\/master\/sgerrand.rsa.pub\n    - curl -L &quot;https:\/\/github.com\/sgerrand\/alpine-pkg-glibc\/releases\/download\/2.23-r3\/glibc-2.23-r3.apk&quot; -o glibc.apk\n    - apk del libc6-compat\n    - apk add glibc.apk\n    - curl -L &quot;https:\/\/github.com\/sgerrand\/alpine-pkg-glibc\/releases\/download\/2.23-r3\/glibc-bin-2.23-r3.apk&quot; -o glibc-bin.apk \n    - apk add glibc-bin.apk \n    - curl -L &quot;https:\/\/github.com\/andyshinn\/alpine-pkg-glibc\/releases\/download\/2.25-r0\/glibc-i18n-2.25-r0.apk&quot; -o glibc-i18n.apk\n    - apk add --allow-untrusted glibc-i18n.apk \n    - \/usr\/glibc-compat\/bin\/localedef -i en_US -f UTF-8 en_US.UTF-8 \n    - \/usr\/glibc-compat\/sbin\/ldconfig \/lib \/usr\/glibc\/usr\/lib\n    - rm -rf glibc*apk \/var\/cache\/apk\/*\n    - echo &quot;yes&quot; | curl -sSL https:\/\/repo.continuum.io\/miniconda\/Miniconda3-latest-Linux-x86_64.sh -o miniconda.sh\n    - echo &quot;Install conda&quot;\n    - (echo -e &quot;\\n&quot;; echo &quot;yes&quot;; echo -e &quot;\\n&quot;; echo &quot;yes&quot;) | bash -bfp miniconda.sh\n    - echo &quot;Installing Azure Machine Learning Extension&quot;\n    - az extension add -n azure-cli-ml\n    - echo &quot;Azure Login&quot;\n    - az login\n    - az account set --subscription ...\n    - az configure --defaults group=...\n    - az ml folder attach -w ... \n    - az ml run submit-script test.py --target ... --experiment-name hello_world --resource-group ...\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":6,
        "Question_creation_time":1601897074350,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1601970763932,
        "Question_score":1.0,
        "Question_view_count":1089.0,
        "Answer_body":"<p>One needs to pass the <code>--workspace-name<\/code> argument to be able to run it on Azure's compute target and not on the local compute target:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>az ml run submit-script test.py --target compute-instance-test --experiment-name test_example --resource-group ex-test-rg --workspace-name test-ws\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64207678",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1603466200947,
        "Question_original_content":"avoid error conda version conda run submit script command like run test script exist comput instanc azur extens azur cli run submit script test target comput instanc test experi test exampl resourc group test servic error follow error messag unabl run conda packag manag us conda provis python nenviron depend specif manag python environ nmanual instead set usermanageddepend true python environ nconfigur us manag python environ instal conda nhttp conda miniconda html connect comput instanc azur portal select default python kernel conda version print conda effect instal comput instanc understand error messag inform azur version azur cli azur cli core azur cli telemetri extens azur cli imag us mcr microsoft com azur cli latest somebodi explain get error help resolv error thank edit tri updat environ run command run essenti gitlab job instal miniconda bit complic azur cli us alpin linux imag refer instal miniconda alpin linux fail replac name cut irrelev piec code test imag mcr microsoft com azur cli latest script echo download conda apk updat add bash curl wget certif libstdc glib wget apk kei sgerrand rsa pub http raw githubusercont com sgerrand alpin pkg node bower master sgerrand rsa pub curl http github com sgerrand alpin pkg glibc releas download glibc apk glibc apk apk del libc compat apk add glibc apk curl http github com sgerrand alpin pkg glibc releas download glibc bin apk glibc bin apk apk add glibc bin apk curl http github com andyshinn alpin pkg glibc releas download glibc apk glibc apk apk add allow untrust glibc apk usr glibc compat bin localedef utf utf usr glibc compat sbin ldconfig lib usr glibc usr lib glibc apk var cach apk echo ye curl ssl http repo continuum miniconda miniconda latest linux miniconda echo instal conda echo echo ye echo echo ye bash bfp miniconda echo instal extens extens add azur cli echo azur login login account set subscript configur default group folder attach run submit script test target experi hello world resourc group",
        "Question_preprocessed_content":"avoid error conda conda run command like run test script exist comput instanc azur extens azur cli servic error follow error messag connect comput instanc azur portal select default python kernel print conda effect instal comput instanc understand error messag inform azur version imag us somebodi explain get error help resolv error thank edit tri updat environ command run essenti gitlab job instal miniconda bit complic us alpin linux imag replac name cut irrelev piec code",
        "Question_gpt_summary_original":"The user is encountering an error message when trying to run a test script on an existing compute instance of Azure using the Azure Machine Learning extension to the Azure CLI. The error message states that the conda package manager is unable to run, even though conda is already installed on the compute instance. The user has tried to update the environment in which the az ml run-command is run, but the installation of miniconda is complicated as the azure-cli uses an alpine Linux image. The user is seeking help to resolve the error.",
        "Question_gpt_summary":"user encount error messag try run test script exist comput instanc azur extens azur cli error messag state conda packag manag unabl run conda instal comput instanc user tri updat environ run command run instal miniconda complic azur cli us alpin linux imag user seek help resolv error",
        "Answer_original_content":"need pass workspac argument abl run azur comput target local comput target run submit script test target comput instanc test experi test exampl resourc group test workspac test",
        "Answer_preprocessed_content":"need pass argument abl run azur comput target local comput target",
        "Answer_gpt_summary_original":"Solution: The user needs to pass the `--workspace-name` argument to be able to run the test script on Azure's compute target and not on the local compute target. The correct command is `az ml run submit-script test.py --target compute-instance-test --experiment-name test_example --resource-group ex-test-rg --workspace-name test-ws`.",
        "Answer_gpt_summary":"solut user need pass workspac argument abl run test script azur comput target local comput target correct command run submit script test target comput instanc test experi test exampl resourc group test workspac test"
    },
    {
        "Question_title":"How can I print the Canonical String which aws-requests-auth sends?",
        "Question_body":"<p>I want to have a lambda calling a Sagemaker instance in another region. If both are in the same region, everything works fine. If they are not, I get the following error:<\/p>\n\n<pre><code>The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\n\nThe Canonical String for this request should have been\n'POST\n\/endpoints\/foo-endpoint\/invocations\n\nhost:runtime.sagemaker.us-east-1.amazonaws.com\nx-amz-date:20180406T082536Z\n\nhost;x-amz-date\n1234567890foobarfoobarfoobarboofoobarfoobarfoobarfoobarfoobarfoo'\n\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20180406T082536Z\n20180406\/us-east-1\/sagemaker\/aws4_request\n987654321abcdeffoobarfoobarfoobarfoobarfoobarfoobarfoobarfoobarf'\n<\/code><\/pre>\n\n<p>I use <a href=\"https:\/\/github.com\/DavidMuller\/aws-requests-auth\" rel=\"nofollow noreferrer\"><code>aws-requests-auth<\/code><\/a> (0.4.1) with boto3 (1.5.15 - updating to 1.7.1 didn't change anything, <a href=\"https:\/\/github.com\/boto\/boto3\/blob\/develop\/CHANGELOG.rst\" rel=\"nofollow noreferrer\">changelog<\/a>) like this:<\/p>\n\n<pre><code>import requests\nfrom aws_requests_auth.aws_auth import AWSRequestsAuth\nauth = AWSRequestsAuth(aws_access_key=config['AWS']['ACCESS_KEY'],\n                       aws_secret_access_key=(\n                           config['AWS']['SECRET_ACCESS_KEY']),\n                       aws_host=config['AWS']['HOST'],\n                       aws_region=config['AWS']['REGION'],\n                       aws_service=config['AWS']['SERVICE'])\n\npayload = {'foo': 'bar'}\nresponse = requests.post(post_url,\n                         data=json.dumps(payload),\n                         headers={'content-type': 'application\/json'},\n                         auth=auth)\n<\/code><\/pre>\n\n<p>printing <code>auth<\/code> only gives <code>&lt;aws_requests_auth.aws_auth.AWSRequestsAuth object at 0x7f9d00c98390&gt;<\/code>.<\/p>\n\n<p>Is there a way to print the \"Canonical String\" mentioned in the error message?<\/p>\n\n<p>(Any other ideas how to fix this are appreciated as well)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1523005580303,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1531211963023,
        "Question_score":3.0,
        "Question_view_count":827.0,
        "Answer_body":"<p>A work-around for the asked question:<\/p>\n\n<pre><code>req = requests.request('POST', 'http:\/\/httpbin.org\/get')\nreq.body = b''\nreq.method = ''\nprint(auth.get_aws_request_headers(req,\n                                   aws_access_key=auth.aws_access_key,\n                                   aws_secret_access_key=auth.aws_secret_access_key,\n                                   aws_token=auth.aws_token))\n<\/code><\/pre>\n\n<p>The problem is not solved, though. And now I wonder what the first argument of <code>auth.get_aws_request_headers<\/code> is.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49689216",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1523012141480,
        "Question_original_content":"print canon string aw request auth send want lambda call instanc region region work fine follow error request signatur calcul match signatur provid check aw secret access kei sign method consult servic document detail canon string request post endpoint foo endpoint invoc host runtim east amazonaw com amz date host amz date foobarfoobarfoobarboofoobarfoobarfoobarfoobarfoobarfoo string sign aw hmac sha east aw request abcdeffoobarfoobarfoobarfoobarfoobarfoobarfoobarfoobarf us aw request auth boto updat chang changelog like import request aw request auth aw auth import awsrequestsauth auth awsrequestsauth aw access kei config aw access kei aw secret access kei config aw secret access kei aw host config aw host aw region config aw region aw servic config aw servic payload foo bar respons request post post url data json dump payload header content type applic json auth auth print auth give wai print canon string mention error messag idea fix appreci",
        "Question_preprocessed_content":"print canon string send want lambda call instanc region region work fine follow error us boto like print give wai print canon string mention error messag idea fix appreci",
        "Question_gpt_summary_original":"The user is encountering an error when trying to call a Sagemaker instance in another region using a lambda. The error message indicates that the request signature does not match the signature provided and provides a Canonical String and String-to-Sign that should have been used. The user is using aws-requests-auth and boto3 to make the request and is looking for a way to print the Canonical String mentioned in the error message.",
        "Question_gpt_summary":"user encount error try instanc region lambda error messag indic request signatur match signatur provid provid canon string string sign user aw request auth boto request look wai print canon string mention error messag",
        "Answer_original_content":"work ask question req request request post http httpbin org req bodi req method print auth aw request header req aw access kei auth aw access kei aw secret access kei auth aw secret access kei aw token auth aw token problem solv wonder argument auth aw request header",
        "Answer_preprocessed_content":"ask question problem solv wonder argument",
        "Answer_gpt_summary_original":"Solution: No solution is provided in the discussion.",
        "Answer_gpt_summary":"solut solut provid discuss"
    },
    {
        "Question_title":"What are the differences between AWS sagemaker and sagemaker_pyspark?",
        "Question_body":"<p>I'm currently running a quick Machine Learning proof of concept on AWS with SageMaker, and I've come across two libraries: <code>sagemaker<\/code> and <code>sagemaker_pyspark<\/code>. I would like to work with distributed data. My questions are:<\/p>\n<ol>\n<li><p>Is using <code>sagemaker<\/code> the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented <code>sagemaker_pyspark<\/code>? Based on this assumption, I do not understand what it would offer regarding using <code>scikit-learn<\/code> on a SageMaker notebook (in terms of computing capabilities).<\/p>\n<\/li>\n<li><p>Is it normal for something like <code>model = xgboost_estimator.fit(training_data)<\/code> to take 4 minutes to run with <code>sagemaker_pyspark<\/code> for a small set of test data? I see that what it does below is to train the model and also create an Endpoint to be able to offer its predictive services, and I assume that this endpoint is deployed on an EC2 instance that is created and started at the moment. Correct me if I'm wrong. I assume this from how the estimator is defined:<\/p>\n<\/li>\n<\/ol>\n<pre><code>from sagemaker import get_execution_role\nfrom sagemaker_pyspark.algorithms import XGBoostSageMakerEstimator\n\n\nxgboost_estimator = XGBoostSageMakerEstimator (\n    trainingInstanceType = &quot;ml.m4.xlarge&quot;,\n    trainingInstanceCount = 1,\n    endpointInstanceType = &quot;ml.m4.xlarge&quot;,\n    endpointInitialInstanceCount = 1,\n    sagemakerRole = IAMRole(get_execution_role())\n)\n\nxgboost_estimator.setNumRound(1)\n<\/code><\/pre>\n<p>If so, is there a way to reuse the same endpoint with different training jobs so that I don't have to wait for a new endpoint to be created each time?<\/p>\n<ol start=\"3\">\n<li><p>Does <code>sagemaker_pyspark<\/code> support custom algorithms? Or does it only allow you to use the predefined ones in the library?<\/p>\n<\/li>\n<li><p>Do you know if <code>sagemaker_pyspark<\/code> can perform hyperparameter optimization? From what I see, <code>sagemaker<\/code> offers the <code>HyperparameterTuner<\/code> class, but I can't find anything like it in <code>sagemaker_pyspark<\/code>. I suppose it is a more recent library and there is still a lot of functionality to implement.<\/p>\n<\/li>\n<li><p>I am a bit confused about the concept of <code>entry_point<\/code> and <code>container<\/code>\/<code>image_name<\/code> (both possible input arguments for the <code>Estimator<\/code> object from the <code>sagemaker<\/code> library): can you deploy models with and without containers? why would you use model containers? Do you always need to define the model externally with the <code>entry_point<\/code> script? It is also confusing that the class <code>AlgorithmEstimator<\/code> allows the input argument <code>algorithm_arn<\/code>; I see there are three different ways of passing a model as input, why? which one is better?<\/p>\n<\/li>\n<li><p>I see the <code>sagemaker<\/code> library offers SageMaker Pipelines, which seem to be very handy for deploying properly structured ML workflows. However, I don't think this is available with <code>sagemaker_pyspark<\/code>, so in that case, I would rather create my workflows with a combination of Step Functions (to orchestrate the entire thing), Glue processes (for ETL, preprocessing and feature\/target engineering) and SageMaker processes using <code>sagemaker_pyspark<\/code>.<\/p>\n<\/li>\n<li><p>I also found out that <code>sagemaker<\/code> has the <code>sagemaker.sparkml.model.SparkMLModel<\/code> object. What is the difference between this and what <code>sagemaker_pyspark<\/code> offers?<\/p>\n<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616764992750,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1618398204670,
        "Question_score":1.0,
        "Question_view_count":242.0,
        "Answer_body":"<p><code>sagemaker<\/code> is the SageMaker Python SDK. It calls SageMaker-related AWS service APIs on your behalf. You don't need to use it, but it can make life easier<\/p>\n<blockquote>\n<ol>\n<li>Is using sagemaker the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented sagemaker_pyspark?<\/li>\n<\/ol>\n<\/blockquote>\n<p>No. You can run distributed training jobs using <code>sagemaker<\/code> (see <code>instance_count<\/code> parameter)<\/p>\n<p><code>sagemaker_pyspark<\/code> facilitates calling SageMaker-related AWS service APIs from Spark. Use it if you want to use SageMaker services from Spark<\/p>\n<blockquote>\n<ol start=\"2\">\n<li>Is it normal for something like model = xgboost_estimator.fit(training_data) to take 4 minutes to run with sagemaker_pyspark for a small set of test data?<\/li>\n<\/ol>\n<\/blockquote>\n<p>Yes, it takes a few minutes for an EC2 instance to spin-up. Use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">Local Mode<\/a> if you want to iterate more quickly locally. Note: Local Mode won't work with SageMaker built-in algorithms, but you can prototype with (non AWS) XGBoost\/SciKit-Learn<\/p>\n<blockquote>\n<ol start=\"3\">\n<li>Does sagemaker_pyspark support custom algorithms? Or does it only allow you to use the predefined ones in the library?<\/li>\n<\/ol>\n<\/blockquote>\n<p>Yes, but you'd probably want to extend <a href=\"https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/api.html#sagemakerestimator\" rel=\"nofollow noreferrer\">SageMakerEstimator<\/a>. Here you can provide the <code>trainingImage<\/code> URI<\/p>\n<blockquote>\n<ol start=\"4\">\n<li>Do you know if sagemaker_pyspark can perform hyperparameter optimization?<\/li>\n<\/ol>\n<\/blockquote>\n<p>It does not appear so. It'd probably be easier just to do this from SageMaker itself though<\/p>\n<blockquote>\n<p>can you deploy models with and without containers?<\/p>\n<\/blockquote>\n<p>You can certainly host your own models any way you want. But if you want to use SageMaker model inference hosting, then containers are required<\/p>\n<blockquote>\n<p>why would you use model containers?<\/p>\n<\/blockquote>\n<blockquote>\n<p>Do you always need to define the model externally with the entry_point script?<\/p>\n<\/blockquote>\n<p>The whole Docker thing makes bundling dependencies easier, and also makes things language\/runtime-neutral. SageMaker doesn't care if your algorithm is in Python or Java or Fortran. But it needs to know how to &quot;run&quot; it, so you tell it a working directory and a command to run. This is the entry point<\/p>\n<blockquote>\n<p>It is also confusing that the class AlgorithmEstimator allows the input argument algorithm_arn; I see there are three different ways of passing a model as input, why? which one is better?<\/p>\n<\/blockquote>\n<p>Please clarify which &quot;three&quot; you are referring to<\/p>\n<p>6 is not a question, so no answer required :)<\/p>\n<blockquote>\n<ol start=\"7\">\n<li>What is the difference between this and what sagemaker_pyspark offers?<\/li>\n<\/ol>\n<\/blockquote>\n<p>sagemaker_pyspark lets you call SageMaker services from Spark, whereas SparkML Serving lets you use Spark ML services from SageMaker<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1634230982412,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66817781",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1617119678127,
        "Question_original_content":"differ pyspark current run quick machin learn proof concept aw come librari pyspark like work distribut data question equival run train job take advantag distribut comput capabl aw assum implement pyspark base assumpt understand offer scikit learn notebook term comput capabl normal like model xgboost estim fit train data minut run pyspark small set test data train model creat endpoint abl offer predict servic assum endpoint deploi instanc creat start moment correct wrong assum estim defin import execut role pyspark algorithm import xgboostestim xgboost estim xgboostestim traininginstancetyp xlarg traininginstancecount endpointinstancetyp xlarg endpointinitialinstancecount role iamrol execut role xgboost estim setnumround wai reus endpoint differ train job wait new endpoint creat time pyspark support custom algorithm allow us predefin on librari know pyspark perform hyperparamet optim offer hyperparametertun class like pyspark suppos recent librari lot function implement bit confus concept entri point contain imag possibl input argument estim object librari deploi model contain us model contain need defin model extern entri point script confus class algorithmestim allow input argument algorithm arn differ wai pass model input better librari offer pipelin handi deploi properli structur workflow think avail pyspark case creat workflow combin step function orchestr entir thing glue process etl preprocess featur target engin process pyspark sparkml model sparkmlmodel object differ pyspark offer",
        "Question_preprocessed_content":"differ current run quick machin learn proof concept aw come librari like work distribut data question equival run train job take advantag distribut comput capabl aw assum implement base assumpt understand offer notebook normal like minut run small set test data train model creat endpoint abl offer predict servic assum endpoint deploi instanc creat start moment correct wrong assum estim defin wai reus endpoint differ train job wait new endpoint creat time support custom algorithm allow us predefin on librari know perform hyperparamet optim offer class like suppos recent librari lot function implement bit confus concept deploi model contain us model contain need defin model extern script confus class allow input argument differ wai pass model input better librari offer pipelin handi deploi properli structur workflow think avail case creat workflow combin step function glue process process object differ offer",
        "Question_gpt_summary_original":"The user is encountering several challenges while working with AWS SageMaker. They are confused about the differences between the sagemaker and sagemaker_pyspark libraries, and whether sagemaker_pyspark offers distributed computing capabilities. They are also experiencing slow training times and wondering if there is a way to reuse the same endpoint with different training jobs. The user is unsure if sagemaker_pyspark supports custom algorithms and hyperparameter optimization. They are also confused about the concept of entry_point and container\/image_name and why model containers are used. Finally, the user is considering using a combination of Step Functions, Glue processes, and sagemaker_pyspark for their ML workflows and is curious about the difference between sagemaker.sparkml.model.SparkMLModel and sagemaker_pyspark.",
        "Question_gpt_summary":"user encount challeng work confus differ pyspark librari pyspark offer distribut comput capabl experienc slow train time wonder wai reus endpoint differ train job user unsur pyspark support custom algorithm hyperparamet optim confus concept entri point contain imag model contain final user consid combin step function glue process pyspark workflow curiou differ sparkml model sparkmlmodel pyspark",
        "Answer_original_content":"python sdk call relat aw servic api behalf need us life easier equival run train job take advantag distribut comput capabl aw assum implement pyspark run distribut train job instanc count paramet pyspark facilit call relat aw servic api spark us want us servic spark normal like model xgboost estim fit train data minut run pyspark small set test data ye take minut instanc spin us local mode want iter quickli local note local mode won work built algorithm prototyp non aw xgboost scikit learn pyspark support custom algorithm allow us predefin on librari ye probabl want extend estim provid trainingimag uri know pyspark perform hyperparamet optim appear probabl easier deploi model contain certainli host model wai want want us model infer host contain requir us model contain need defin model extern entri point script docker thing make bundl depend easier make thing languag runtim neutral care algorithm python java fortran need know run tell work directori command run entri point confus class algorithmestim allow input argument algorithm arn differ wai pass model input better clarifi refer question answer requir differ pyspark offer pyspark let servic spark sparkml serv let us spark servic",
        "Answer_preprocessed_content":"python sdk call relat aw servic api behalf need us life easier equival run train job take advantag distribut comput capabl aw assum implement run distribut train job facilit call relat aw servic api spark us want us servic spark normal like model minut run small set test data ye take minut instanc us local mode want iter quickli local note local mode won work algorithm prototyp support custom algorithm allow us predefin on librari ye probabl want extend estim provid uri know perform hyperparamet optim appear probabl easier deploi model contain certainli host model wai want want us model infer host contain requir us model contain need defin model extern script docker thing make bundl depend easier make thing care algorithm python java fortran need know run tell work directori command run entri point confus class algorithmestim allow input argument differ wai pass model input better clarifi refer question answer requir differ offer let servic spark sparkml serv let us spark servic",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. The user can run distributed training jobs using sagemaker by specifying the instance_count parameter.\n2. To iterate more quickly locally, the user can use Local Mode, which won't work with SageMaker built-in algorithms but can be used with XGBoost\/SciKit-Learn.\n3. sagemaker_pyspark supports custom algorithms, but the user would need to extend SageMakerEstimator and provide the trainingImage URI.\n4. sagemaker_pyspark does not support hyperparameter optimization, but it can be done from SageMaker itself.\n5. Containers are required if the user wants to use SageMaker model inference hosting.\n6. Model containers make bundling dependencies easier and make",
        "Answer_gpt_summary":"possibl solut mention discuss user run distribut train job specifi instanc count paramet iter quickli local user us local mode won work built algorithm xgboost scikit learn pyspark support custom algorithm user need extend estim provid trainingimag uri pyspark support hyperparamet optim contain requir user want us model infer host model contain bundl depend easier"
    },
    {
        "Question_title":"Consume Microsoft Cluster API using PowerBI",
        "Question_body":"<p>Thanks for getting back to me.<\/p>\n\n<p>Basically I subscribed to a Cluster API service (cortana analytics). This is the sample application as per Microsoft Machine Learning site<\/p>\n\n<p><a href=\"http:\/\/microsoftazuremachinelearning.azurewebsites.net\/ClusterModel.aspx\" rel=\"nofollow\">http:\/\/microsoftazuremachinelearning.azurewebsites.net\/ClusterModel.aspx<\/a><\/p>\n\n<p>As you could see there are 2 arguments to be passed on<\/p>\n\n<p>Input<\/p>\n\n<p>K<\/p>\n\n<p>Where input could be 10;5;2,18;1;6,7;5;5,22;3;4,12;2;1,10;3;4 (each row is separated by semi colon)<\/p>\n\n<p>And K is cluster number: 5 (for example)<\/p>\n\n<p>So to consume this API I use PowerBI Edit Query, <\/p>\n\n<p>So go to Get Data > More > Azure > Microsoft Data MarketPlace, I can see the list of APIs I subscribed to, one of them is the one I referred to in the link above.<\/p>\n\n<p>So I load that as Function lets called it \"Score\"<\/p>\n\n<p>Then I got energy table which I loaded in from a csv file, I want to cluster energy consumption into 5 clusters.<\/p>\n\n<p>So my data layout is<\/p>\n\n<p>Year   Energy<\/p>\n\n<p>2001   6.28213<\/p>\n\n<p>2002  14.12845<\/p>\n\n<p>2003   5.55851<\/p>\n\n<p>and so on, lets say I got 100 rows of the data.<\/p>\n\n<p>So I tried to pass \"6.28213;14.12845;5.55851\", \"5\" to Score function but I dont know how to <\/p>\n\n<ol>\n<li><p>Convert my table into records<\/p><\/li>\n<li><p>pass 2 argument records and constant value 5 as K.<\/p><\/li>\n<\/ol>\n\n<p>Hope this makes sense.<\/p>\n\n<p>Please help! :)<\/p>\n\n<p>Thank you in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1476795601683,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":137.0,
        "Answer_body":"<p>To convert a column of numbers into a semicolon delimited text, do this to your table:<\/p>\n\n<ol>\n<li>Convert your Energy column is type text.<\/li>\n<li>Add <code>[Energy]<\/code> after the name of your table, which gives you a list of the numbers.<\/li>\n<li>Use <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/mt253358.aspx\" rel=\"nofollow\"><code>Text.Combine<\/code><\/a> to turn the list into a text value seperated by <code>;<\/code><\/li>\n<\/ol>\n\n<p>Here's a mashup that does that:<\/p>\n\n<pre><code>let\n    Source = Table.FromRows(Json.Document(Binary.Decompress(Binary.FromText(\"NcjBCQAgDAPAXfKWYqKR7iLdfw1F8J63N9Q70bBCKQ5Ue6VbnEHl9L9xz2GniaoD\", BinaryEncoding.Base64), Compression.Deflate)), let _t = ((type text) meta [Serialized.Text = true]) in type table [Year = _t, Energy = _t]),\n    #\"Changed Type\" = Table.TransformColumnTypes(Source,{{\"Year\", Int64.Type}, {\"Energy\", type text}}),\n    Custom1 = #\"Changed Type\"[Energy],\n    Custom2 = Text.Combine(Custom1, \";\")\nin\n    Custom2\n<\/code><\/pre>\n\n<hr>\n\n<p>Once you have a function, you'll invoke it like <code>YourFunction(Custum2, 5)<\/code><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40108999",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1476896300823,
        "Question_original_content":"consum microsoft cluster api powerbi thank get basic subscrib cluster api servic cortana analyt sampl applic microsoft machin learn site http microsoftazuremachinelearn azurewebsit net clustermodel aspx argument pass input input row separ semi colon cluster number exampl consum api us powerbi edit queri data azur microsoft data marketplac list api subscrib refer link load function let call score got energi tabl load csv file want cluster energi consumpt cluster data layout year energi let got row data tri pass score function dont know convert tabl record pass argument record constant valu hope make sens help thank advanc",
        "Question_preprocessed_content":"consum microsoft cluster api powerbi thank get basic subscrib cluster api servic sampl applic microsoft machin learn site argument pass input input cluster number consum api us powerbi edit queri data azur microsoft data marketplac list api subscrib refer link load function let call score got energi tabl load csv file want cluster energi consumpt cluster data layout year energi let got row data tri pass score function dont know convert tabl record pass argument record constant valu hope make sens help thank advanc",
        "Question_gpt_summary_original":"The user is facing challenges in consuming a Cluster API service using PowerBI Edit Query. They are struggling to convert their energy consumption data table into records and pass two argument records and a constant value of 5 as K to the API.",
        "Question_gpt_summary":"user face challeng consum cluster api servic powerbi edit queri struggl convert energi consumpt data tabl record pass argument record constant valu api",
        "Answer_original_content":"convert column number semicolon delimit text tabl convert energi column type text add energi tabl give list number us text combin turn list text valu seper mashup let sourc tabl fromrow json document binari decompress binari fromtext ncjbcqagdapaxfkwyqkrildfwfjnqbbckquevbnehllxzgniaod binaryencod base compress deflat let type text meta serial text true type tabl year energi chang type tabl transformcolumntyp sourc year int type energi type text custom chang type energi custom text combin custom custom function invok like yourfunct custum",
        "Answer_preprocessed_content":"convert column number semicolon delimit text tabl convert energi column type text add tabl give list number us turn list text valu seper mashup function invok like",
        "Answer_gpt_summary_original":"Solution:\n- Convert the Energy column to type text.\n- Add [Energy] after the name of the table to get a list of numbers.\n- Use Text.Combine to turn the list into a text value separated by \";\".\n- Invoke the function with the converted Energy column and a constant value of 5 as arguments.",
        "Answer_gpt_summary":"solut convert energi column type text add energi tabl list number us text combin turn list text valu separ invok function convert energi column constant valu argument"
    },
    {
        "Question_title":"Permission \"artifactregistry.repositories.downloadArtifacts\" denied on resource",
        "Question_body":"<p>While the artifact repository was successfully creating, running a docker push to push the image to the google artifact registry fails with a permissions error even after granting all artifact permissions to the accounting I am using on gcloud cli.<\/p>\n<p><strong>Command used to push image:<\/strong><\/p>\n<pre><code>docker push us-central1-docker.pkg.dev\/project-id\/repo-name:v2\n<\/code><\/pre>\n<p><strong>Error message:<\/strong><\/p>\n<pre><code>The push refers to repository [us-central1-docker.pkg.dev\/project-id\/repo-name]\n6f6f4a472f31: Preparing\nbc096d7549c4: Preparing\n5f70bf18a086: Preparing\n20bed28d4def: Preparing\n2a3255c6d9fb: Preparing\n3f5d38b4936d: Waiting\n7be8268e2fb0: Waiting\nb889a93a79dd: Waiting\n9d4550089a93: Waiting\na7934564e6b9: Waiting\n1b7cceb6a07c: Waiting\nb274e8788e0c: Waiting\n78658088978a: Waiting\ndenied: Permission &quot;artifactregistry.repositories.downloadArtifacts&quot; denied on resource &quot;projects\/project-id\/locations\/us-central1\/repositories\/repo-name&quot; (or it may not exist)\n\n\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1652644857243,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1652667678670,
        "Question_score":12.0,
        "Question_view_count":5722.0,
        "Answer_body":"<p>I was able to recreate your use case. This happens when you are trying to push an image on a <code>repository<\/code> in which its specific hostname (associated with it's repository location) is not yet  added to the credential helper configuration for authentication. You may refer to this <a href=\"https:\/\/cloud.google.com\/artifact-registry\/docs\/docker\/authentication\" rel=\"noreferrer\">Setting up authentication for Docker <\/a> as also provided by @DazWilkin in the comments for more details.<\/p>\n<p>In my example, I was trying to push an image on a repository that has a location of <code>us-east1<\/code> and got the same error since it is not yet added to the credential helper configuration.\n<a href=\"https:\/\/i.stack.imgur.com\/NQeIf.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NQeIf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>And after I ran the authentication using below command (specifically for us-east1 since it is the <code>location<\/code> of my repository), the image was successfully pushed:<\/p>\n<pre><code>gcloud auth configure-docker us-east1-docker.pkg.dev\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q2Q9x.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q2Q9x.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><em><strong>QUICK TIP<\/strong><\/em>: You may  get your authentication command specific for your repository when you open your desired repository in the <a href=\"https:\/\/console.cloud.google.com\/artifacts\" rel=\"noreferrer\">console<\/a>, and then click on the <code>SETUP INSTRUCTIONS<\/code>.\n<a href=\"https:\/\/i.stack.imgur.com\/KBjqa.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KBjqa.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":1654468583460,
        "Answer_score":23.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72251787",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1652683203067,
        "Question_original_content":"permiss artifactregistri repositori downloadartifact deni resourc artifact repositori successfulli creat run docker push push imag googl artifact registri fail permiss error grant artifact permiss account gcloud cli command push imag docker push central docker pkg dev project repo error messag push refer repositori central docker pkg dev project repo ffaf prepar bcdc prepar fbfa prepar bedddef prepar acdfb prepar fdbd wait beefb wait baadd wait wait aeb wait bccebac wait beec wait wait deni permiss artifactregistri repositori downloadartifact deni resourc project project locat central repositori repo exist",
        "Question_preprocessed_content":"permiss deni resourc artifact repositori successfulli creat run docker push push imag googl artifact registri fail permiss error grant artifact permiss account gcloud cli command push imag error messag",
        "Question_gpt_summary_original":"The user encountered a permissions error while trying to push an image to the Google Artifact Registry using the \"docker push\" command. The error message indicates that the user was denied permission to download artifacts on the resource \"projects\/project-id\/locations\/us-central1\/repositories\/repo-name\". Despite granting all artifact permissions to the accounting being used on gcloud cli, the error persisted.",
        "Question_gpt_summary":"user encount permiss error try push imag googl artifact registri docker push command error messag indic user deni permiss download artifact resourc project project locat central repositori repo despit grant artifact permiss account gcloud cli error persist",
        "Answer_original_content":"abl recreat us case happen try push imag repositori specif hostnam associ repositori locat ad credenti helper configur authent refer set authent docker provid dazwilkin comment detail exampl try push imag repositori locat east got error ad credenti helper configur ran authent command specif east locat repositori imag successfulli push gcloud auth configur docker east docker pkg dev quick tip authent command specif repositori open desir repositori consol click setup instruct",
        "Answer_preprocessed_content":"abl recreat us case happen try push imag specif hostnam ad credenti helper configur authent refer set authent docker provid comment detail exampl try push imag repositori locat got error ad credenti helper configur ran authent command imag successfulli push quick tip authent command specif repositori open desir repositori consol click",
        "Answer_gpt_summary_original":"Solution:\n- The user needs to add the specific hostname associated with the repository location to the credential helper configuration for authentication.\n- The user can refer to the Setting up authentication for Docker documentation for more details.\n- The user can run the authentication command specific to their repository location, which can be found in the SETUP INSTRUCTIONS section of the repository in the console.",
        "Answer_gpt_summary":"solut user need add specif hostnam associ repositori locat credenti helper configur authent user refer set authent docker document detail user run authent command specif repositori locat setup instruct section repositori consol"
    },
    {
        "Question_title":"Azure Machine Learning Compute quota?",
        "Question_body":"<p>The <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-manage-quotas#azure-machine-learning-compute\" rel=\"nofollow noreferrer\">Manage and request quotas for Azure resources<\/a> documentation page states that the default quota depends \"on your subscription offer type\". The quota doesn't show up in Azure web portal. Is there a way to find out current quota values using SDK, CLI, REST API?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1570636386640,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":354.0,
        "Answer_body":"<p>You probably want to try something like this command : <\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>az vm list-usage --location eastus --out table\n<\/code><\/pre>\n\n<p>It would get you the core usage for the region, which is what is important for deployment of resources.<\/p>\n\n<p>Other choices (az + Powershell) are available <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/networking\/check-usage-against-limits\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Hope this helps!<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58307950",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1570669528590,
        "Question_original_content":"comput quota manag request quota azur resourc document page state default quota depend subscript offer type quota azur web portal wai current quota valu sdk cli rest api",
        "Question_preprocessed_content":"comput quota manag request quota azur resourc document page state default quota depend subscript offer type quota azur web portal wai current quota valu sdk cli rest api",
        "Question_gpt_summary_original":"The user is facing challenges in finding out the current quota values for Azure Machine Learning Compute using SDK, CLI, or REST API as the default quota depends on the subscription offer type and does not show up in the Azure web portal.",
        "Question_gpt_summary":"user face challeng find current quota valu comput sdk cli rest api default quota depend subscript offer type azur web portal",
        "Answer_original_content":"probabl want try like command list usag locat eastu tabl core usag region import deploy resourc choic powershel avail hope help",
        "Answer_preprocessed_content":"probabl want try like command core usag region import deploy resourc choic avail hope help",
        "Answer_gpt_summary_original":"Solution:\n- Use the command \"az vm list-usage --location eastus --out table\" to get the core usage for the region, which is important for deployment of resources. Other options are also available using az and Powershell.",
        "Answer_gpt_summary":"solut us command list usag locat eastu tabl core usag region import deploy resourc option avail powershel"
    },
    {
        "Question_title":"What type of protocole for huge dataset",
        "Question_body":"<p>Hello,<\/p>\n<p>I want to use dvc for my dataset versioning. My dataset can go up to 10Go and I want to setup my protocole to retrieve my data.<br>\nI see that protocole to be able to retrieve data:<br>\n(<a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/user-guide\/managing-external-data<\/a>)<\/p>\n<ul>\n<li>Amazon S3<\/li>\n<li>SSH<\/li>\n<li>HDFS<\/li>\n<li>Local files and directories outside the workspace<\/li>\n<\/ul>\n<p>What protocole should I use to be able to pull that quantity of data?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1616517263719,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":282.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/xavier\">@xavier<\/a> !<\/p>\n<p>What do you mean by <code>retrieve<\/code>? <code>dvc pull<\/code>? The article you\u2019ve linked talks about an experimental workflow, which we don\u2019t recommend using (there is a note about add\/import at the top of it, as that is what people usually are looking for).<\/p>. <p>Yes it is dvc pull. What is the best way to manage huge dataset? Is it a good solution to use DVC?<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/what-type-of-protocole-for-huge-dataset\/707",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-23T17:54:33.676Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/xavier\">@xavier<\/a> !<\/p>\n<p>What do you mean by <code>retrieve<\/code>? <code>dvc pull<\/code>? The article you\u2019ve linked talks about an experimental workflow, which we don\u2019t recommend using (there is a note about add\/import at the top of it, as that is what people usually are looking for).<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-23T19:52:51.824Z",
                "Answer_body":"<p>Yes it is dvc pull. What is the best way to manage huge dataset? Is it a good solution to use DVC?<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"type protocol huge dataset hello want us dataset version dataset want setup protocol retriev data protocol abl retriev data http org doc user guid manag extern data amazon ssh hdf local file directori outsid workspac protocol us abl pull quantiti data",
        "Question_preprocessed_content":"type protocol huge dataset hello want us dataset version dataset want setup protocol retriev data protocol abl retriev data amazon ssh hdf local file directori outsid workspac protocol us abl pull quantiti data",
        "Question_gpt_summary_original":"The user is facing a challenge in selecting the appropriate protocol to retrieve a huge dataset of up to 10GB using dvc for versioning. They are considering different protocols such as Amazon S3, SSH, HDFS, and local files and directories outside the workspace, and are seeking advice on which protocol to use for pulling such a large amount of data.",
        "Question_gpt_summary":"user face challeng select appropri protocol retriev huge dataset version consid differ protocol amazon ssh hdf local file directori outsid workspac seek advic protocol us pull larg data",
        "Answer_original_content":"xavier mean retriev pull articl youv link talk experiment workflow dont recommend note add import peopl usual look ye pull best wai manag huge dataset good solut us",
        "Answer_preprocessed_content":"mean articl youv link talk experiment workflow dont recommend ye pull best wai manag huge dataset good solut us",
        "Answer_gpt_summary_original":"No solutions are provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Scatter plot instead of Line plot",
        "Question_body":"<p>Hi everyone,<\/p>\n<p>regarding the different chart types, I am somehow missing the option \u2018<strong>Scatter plot<\/strong>\u2019 next to \u2018Line plot\u2019, \u2018Area plot\u2019, and \u2018Percent area plot\u2019. Would be great if you could add this feature in the future (or in case it can easily be done somehow else, please let me know how it works \u2013 I already tried custom scatter plots, but it seems as if they are meant for comparing different runs, not matrices from a single run).<\/p>\n<p>Thanks <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Question_answer_count":7,
        "Question_comment_count":0,
        "Question_creation_time":1657120094265,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":203.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/aichberger\">@aichberger<\/a> ,<\/p>\n<p>You can create a line chart with a dotted non connected line through the legend category within chart edit.  I believe that is what you are looking for, see image below. In terms of modifying how a metric is being logged, can you expand on your meaning behind this? You can currently update metrics for a run, after it has finished., see <a href=\"https:\/\/wandb.ai\/mohammadbakir\/Finance-Prediction?workspace=user-mohammadbakir\">here<\/a>.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea.png\" data-download-href=\"\/uploads\/short-url\/wDbjAXgfUtaaLAXhq2VTVU9YU6C.png?dl=1\" title=\"DotChart\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea_2_690x198.png\" alt=\"DotChart\" data-base62-sha1=\"wDbjAXgfUtaaLAXhq2VTVU9YU6C\" width=\"690\" height=\"198\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea_2_690x198.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea_2_1035x297.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea_2_1380x396.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">DotChart<\/span><span class=\"informations\">1819\u00d7524 30 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/scatter-plot-instead-of-line-plot\/2706",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-06T15:17:29.543Z",
                "Answer_body":"<p>I have been struggling with this as well. Would be very thankful about anyone\u2019s help!<br>\nThanks <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-07T21:39:29.598Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/aichberger\">@aichberger<\/a>  and <a class=\"mention\" href=\"\/u\/sflattinger\">@sflattinger<\/a> , you can log a custom scatter plot\u2014a list of points (x, y) on a pair of arbitrary axes x and y, see <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log\/plots#basic-charts\">here<\/a>, or directly in the UI under <strong>+ Add Panel<\/strong>, see screenshot.  Please let me know if you have additional questions.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/8af4bfd941934dbb8a3f15acd328a48055cebd3c.png\" data-download-href=\"\/uploads\/short-url\/jPggnfhgHQp1P9staS6HiVjut6Q.png?dl=1\" title=\"ScatterPlot\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/8af4bfd941934dbb8a3f15acd328a48055cebd3c_2_283x375.png\" alt=\"ScatterPlot\" data-base62-sha1=\"jPggnfhgHQp1P9staS6HiVjut6Q\" width=\"283\" height=\"375\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/8af4bfd941934dbb8a3f15acd328a48055cebd3c_2_283x375.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/8af4bfd941934dbb8a3f15acd328a48055cebd3c.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/8af4bfd941934dbb8a3f15acd328a48055cebd3c.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/8af4bfd941934dbb8a3f15acd328a48055cebd3c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">ScatterPlot<\/span><span class=\"informations\">407\u00d7537 27.2 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-11T13:22:51.755Z",
                "Answer_body":"<p>Thank you for the reply!<br>\nI have already tried to create scatter plots directly in the UI, but if I choose \u2018Step\u2019 on the x-axis and the metric of interest on the y-axis, each data point represents a total run within the project. However, I would like to plot the metric at each step of a single run (just like the line plot but without connecting the points). Also, changing the way this metric is logged isn\u2019t possible retrospectively.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-14T20:45:41.559Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/aichberger\">@aichberger<\/a> ,<\/p>\n<p>You can create a line chart with a dotted non connected line through the legend category within chart edit.  I believe that is what you are looking for, see image below. In terms of modifying how a metric is being logged, can you expand on your meaning behind this? You can currently update metrics for a run, after it has finished., see <a href=\"https:\/\/wandb.ai\/mohammadbakir\/Finance-Prediction?workspace=user-mohammadbakir\">here<\/a>.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea.png\" data-download-href=\"\/uploads\/short-url\/wDbjAXgfUtaaLAXhq2VTVU9YU6C.png?dl=1\" title=\"DotChart\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea_2_690x198.png\" alt=\"DotChart\" data-base62-sha1=\"wDbjAXgfUtaaLAXhq2VTVU9YU6C\" width=\"690\" height=\"198\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea_2_690x198.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea_2_1035x297.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea_2_1380x396.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e4b37129b3b2dca4efe8af5082b5662e1e7fe5ea_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">DotChart<\/span><span class=\"informations\">1819\u00d7524 30 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-07-15T15:22:24.698Z",
                "Answer_body":"<p>Yeah that is exactly what I was looking for, thank you!<br>\nHowever, this has to be set for each run individually. This is why I suggested adding the option \u2018Scatter plot \u2019 next to \u2018Line plot\u2019, \u2018Area plot\u2019, and \u2018Percent area plot\u2019.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-20T18:17:17.462Z",
                "Answer_body":"<p>Thank you <a class=\"mention\" href=\"\/u\/aichberger\">@aichberger<\/a> , we are currently enhancing and improving our chart options, I will make note of your comments and share them with our app team. If you have any additional questions, please do reach back out again. I will mark this matter closed for now.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-18T18:17:28.737Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":1657831541559,
        "Question_original_content":"scatter plot instead line plot differ chart type miss option scatter plot line plot area plot percent area plot great add featur futur case easili let know work tri custom scatter plot meant compar differ run matric singl run thank",
        "Question_preprocessed_content":"scatter plot instead line plot differ chart type miss option scatter plot line plot area plot percent area plot great add featur futur thank",
        "Question_gpt_summary_original":"The user is facing a challenge with the chart types available as they are missing the option for a scatter plot. They have attempted to create custom scatter plots but have not been successful in comparing matrices from a single run. The user is requesting the addition of a scatter plot feature or guidance on how to create one.",
        "Question_gpt_summary":"user face challeng chart type avail miss option scatter plot attempt creat custom scatter plot success compar matric singl run user request addit scatter plot featur guidanc creat",
        "Answer_original_content":"aichberg creat line chart dot non connect line legend categori chart edit believ look imag term modifi metric log expand mean current updat metric run finish dotchart",
        "Answer_preprocessed_content":"creat line chart dot non connect line legend categori chart edit believ look imag term modifi metric log expand mean current updat metric run dotchart",
        "Answer_gpt_summary_original":"Solution: No solution provided.",
        "Answer_gpt_summary":"solut solut provid"
    },
    {
        "Question_title":"How to solve the error with deploying a model in aws sagemaker?",
        "Question_body":"<p>I have to deploy a custom keras model in AWS Sagemaker. I have a created a notebook instance and I have the following files:<\/p>\n\n<pre><code>AmazonSagemaker-Codeset16\n   -ann\n      -nginx.conf\n      -predictor.py\n      -serve\n      -train.py\n      -wsgi.py\n   -Dockerfile\n<\/code><\/pre>\n\n<p>I now open the AWS terminal and build the docker image and push the image in the ECR repository. Then I open a new jupyter python notebook and try to fit the model and deploy the same. The training is done correctly but while deploying I get the following error:<\/p>\n\n<blockquote>\n  <p>\"Error hosting endpoint sagemaker-example-2019-10-25-06-11-22-366: Failed. >Reason: The primary container for production variant AllTraffic did not pass >the ping health check. Please check CloudWatch logs for this endpoint...\"<\/p>\n<\/blockquote>\n\n<p>When I check the logs, I find the following:<\/p>\n\n<blockquote>\n  <p>2019\/11\/11 11:53:32 [crit] 19#19: *3 connect() to unix:\/tmp\/gunicorn.sock >failed (2: No such file or directory) while connecting to upstream, client: >10.32.0.4, server: , request: \"GET \/ping HTTP\/1.1\", upstream: >\"<a href=\"http:\/\/unix:\/tmp\/gunicorn.sock:\/ping\" rel=\"nofollow noreferrer\">http:\/\/unix:\/tmp\/gunicorn.sock:\/ping<\/a>\", host: \"model.aws.local:8080\"<\/p>\n<\/blockquote>\n\n<p>and <\/p>\n\n<blockquote>\n  <p>Traceback (most recent call last):\n   File \"\/usr\/local\/bin\/serve\", line 8, in \n     sys.exit(main())\n   File \"\/usr\/local\/lib\/python2.7\/dist->packages\/sagemaker_containers\/cli\/serve.py\", line 19, in main\n     server.start(env.ServingEnv().framework_module)\n   File \"\/usr\/local\/lib\/python2.7\/dist->packages\/sagemaker_containers\/_server.py\", line 107, in start\n     module_app,\n   File \"\/usr\/lib\/python2.7\/subprocess.py\", line 711, in <strong>init<\/strong>\n     errread, errwrite)\n   File \"\/usr\/lib\/python2.7\/subprocess.py\", line 1343, in _execute_child\n     raise child_exception<\/p>\n<\/blockquote>\n\n<p>I tried to deploy the same model in AWS Sagemaker with these files in my local computer and the model was deployed successfully but inside AWS, I am facing this problem.<\/p>\n\n<p>Here is my serve file code:<\/p>\n\n<pre><code>from __future__ import print_function\nimport multiprocessing\nimport os\nimport signal\nimport subprocess\nimport sys\n\ncpu_count = multiprocessing.cpu_count()\n\nmodel_server_timeout = os.environ.get('MODEL_SERVER_TIMEOUT', 60)\nmodel_server_workers = int(os.environ.get('MODEL_SERVER_WORKERS', cpu_count))\n\n\ndef sigterm_handler(nginx_pid, gunicorn_pid):\n    try:\n        os.kill(nginx_pid, signal.SIGQUIT)\n    except OSError:\n        pass\n    try:\n        os.kill(gunicorn_pid, signal.SIGTERM)\n    except OSError:\n        pass\n\n    sys.exit(0)\n\n\ndef start_server():\n    print('Starting the inference server with {} workers.'.format(model_server_workers))\n\n\n    # link the log streams to stdout\/err so they will be logged to the container logs\n    subprocess.check_call(['ln', '-sf', '\/dev\/stdout', '\/var\/log\/nginx\/access.log'])\n    subprocess.check_call(['ln', '-sf', '\/dev\/stderr', '\/var\/log\/nginx\/error.log'])\n\n    nginx = subprocess.Popen(['nginx', '-c', '\/opt\/ml\/code\/nginx.conf'])\n    gunicorn = subprocess.Popen(['gunicorn',\n                                 '--timeout', str(model_server_timeout),\n                                 '-b', 'unix:\/tmp\/gunicorn.sock',\n                                 '-w', str(model_server_workers),\n                                 'wsgi:app'])\n\n    signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))\n\n    # If either subprocess exits, so do we.\n    pids = set([nginx.pid, gunicorn.pid])\n    while True:\n        pid, _ = os.wait()\n        if pid in pids:\n            break\n\n    sigterm_handler(nginx.pid, gunicorn.pid)\n    print('Inference server exiting')\n\n\n# The main routine just invokes the start function.\nif __name__ == '__main__':\n    start_server()\n<\/code><\/pre>\n\n<p>I deploy the model using the following:<\/p>\n\n<blockquote>\n  <p>predictor = classifier.deploy(1, 'ml.t2.medium', serializer=csv_serializer)<\/p>\n<\/blockquote>\n\n<p>Kindly let me know the mistake I am doing while deploying.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1573549904240,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1573550204732,
        "Question_score":1.0,
        "Question_view_count":4605.0,
        "Answer_body":"<p>Using Sagemaker script mode can be much simpler than dealing with container and nginx low-level stuff like you're trying to do, have you considered that?<br>\nYou only need to provide the keras script:   <\/p>\n\n<blockquote>\n  <p>With Script Mode, you can use training scripts similar to those you would use outside SageMaker with SageMaker's prebuilt containers for various deep learning frameworks such TensorFlow, PyTorch, and Apache MXNet.<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-sentiment-script-mode\/sentiment-analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-sentiment-script-mode\/sentiment-analysis.ipynb<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58815367",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1573630807523,
        "Question_original_content":"solv error deploi model deploi custom kera model creat notebook instanc follow file amazon codeset ann nginx conf predictor serv train wsgi dockerfil open aw termin build docker imag push imag ecr repositori open new jupyt python notebook try fit model deploi train correctli deploi follow error error host endpoint exampl fail reason primari contain product variant alltraff pass ping health check check cloudwatch log endpoint check log follow crit connect unix tmp gunicorn sock fail file directori connect upstream client server request ping http upstream http unix tmp gunicorn sock ping host model aw local traceback recent file usr local bin serv line sy exit main file usr local lib python dist packag contain cli serv line main server start env servingenv framework modul file usr local lib python dist packag contain server line start modul app file usr lib python subprocess line init eead errwrit file usr lib python subprocess line execut child rais child except tri deploi model file local model deploi successfulli insid aw face problem serv file code futur import print function import multiprocess import import signal import subprocess import sy cpu count multiprocess cpu count model server timeout environ model server timeout model server worker int environ model server worker cpu count def sigterm handler nginx pid gunicorn pid try kill nginx pid signal sigquit oserror pass try kill gunicorn pid signal sigterm oserror pass sy exit def start server print start infer server worker format model server worker link log stream stdout err log contain log subprocess check dev stdout var log nginx access log subprocess check dev stderr var log nginx error log nginx subprocess popen nginx opt code nginx conf gunicorn subprocess popen gunicorn timeout str model server timeout unix tmp gunicorn sock str model server worker wsgi app signal signal signal sigterm lambda sigterm handler nginx pid gunicorn pid subprocess exit pid set nginx pid gunicorn pid true pid wait pid pid break sigterm handler nginx pid gunicorn pid print infer server exit main routin invok start function main start server deploi model follow predictor classifi deploi medium serial csv serial kindli let know mistak deploi",
        "Question_preprocessed_content":"solv error deploi model deploi custom kera model creat notebook instanc follow file open aw termin build docker imag push imag ecr repositori open new jupyt python notebook try fit model deploi train correctli deploi follow error error host endpoint fail reason primari contain product variant alltraff pass ping health check check cloudwatch log check log follow connect fail connect upstream client server request upstream host traceback file line file line main file line start file line init eead errwrit file line rais tri deploi model file local model deploi successfulli insid aw face problem serv file code deploi model follow predictor kindli let know mistak deploi",
        "Question_gpt_summary_original":"The user is facing an error while deploying a custom Keras model in AWS Sagemaker. The training is done correctly, but while deploying, the primary container for production variant AllTraffic did not pass the ping health check. The user checked the logs and found that the connection to unix:\/tmp\/gunicorn.sock failed, and the subprocess failed to execute child. The user tried to deploy the same model in AWS Sagemaker with the same files on their local computer, and the model was deployed successfully, but inside AWS, they are facing this problem. The user has provided the serve file code and the deployment code used.",
        "Question_gpt_summary":"user face error deploi custom kera model train correctli deploi primari contain product variant alltraff pass ping health check user check log connect unix tmp gunicorn sock fail subprocess fail execut child user tri deploi model file local model deploi successfulli insid aw face problem user provid serv file code deploy code",
        "Answer_original_content":"script mode simpler deal contain nginx low level stuff like try consid need provid kera script script mode us train script similar us outsid prebuilt contain deep learn framework tensorflow pytorch apach mxnet http github com aw sampl amazon script mode blob master sentiment script mode sentiment analysi ipynb",
        "Answer_preprocessed_content":"script mode simpler deal contain nginx stuff like try consid need provid kera script script mode us train script similar us outsid prebuilt contain deep learn framework tensorflow pytorch apach mxnet",
        "Answer_gpt_summary_original":"Solution: One possible solution suggested in the discussion is to use Sagemaker script mode instead of dealing with container and nginx low-level stuff. With script mode, the user can use training scripts similar to those used outside Sagemaker with Sagemaker's prebuilt containers for various deep learning frameworks such as TensorFlow, PyTorch, and Apache MXNet. The user can refer to the provided link for more information on how to use script mode.",
        "Answer_gpt_summary":"solut possibl solut suggest discuss us script mode instead deal contain nginx low level stuff script mode user us train script similar outsid prebuilt contain deep learn framework tensorflow pytorch apach mxnet user refer provid link inform us script mode"
    },
    {
        "Question_title":"Multi user on one DB ok?",
        "Question_body":"When I use tracking directly on a Database (PostgreSQL) would it cause problems when two computer do tracking into the same DB at the same time?\n\n\n\nSame question with MLFlow UI: Is it ok when we connect two UIs from two different computers to one (the same) DB?\n\n\nMy guess is that it is no problem but just want to make sure.\n\n\nThanks\nPhilip",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1583597894000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":23.0,
        "Answer_body":"When I use tracking directly on a Database (PostgreSQL) would it cause problems when two computers do tracking into the same DB at the same time?\n\n\n\nI don't believe that would be an issue, given that PostgresSQL is a relational DB supporting transactions, and each run has a unique ID; even within an umbrella experiment name, your run will be different from the other person's run on a different computer.\n\n\nSame question with MLFlow UI: Is it ok when we connect two UIs from two different computers to one (the same) DB?\n\n\nThe same idea\n\n\nJules\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/98d2d583-19bf-4198-bc73-ebaefb64cd62%40googlegroups.com.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/VQc6nWUNjYM",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-03-07T16:54:42",
                "Answer_body":"When I use tracking directly on a Database (PostgreSQL) would it cause problems when two computers do tracking into the same DB at the same time?\n\n\n\nI don't believe that would be an issue, given that PostgresSQL is a relational DB supporting transactions, and each run has a unique ID; even within an umbrella experiment name, your run will be different from the other person's run on a different computer.\n\n\nSame question with MLFlow UI: Is it ok when we connect two UIs from two different computers to one (the same) DB?\n\n\nThe same idea\n\n\nJules\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/98d2d583-19bf-4198-bc73-ebaefb64cd62%40googlegroups.com."
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"multi user us track directli databas postgresql caus problem track time question connect ui differ comput guess problem want sure thank philip",
        "Question_preprocessed_content":"multi user us track directli databas caus problem track time question connect ui differ comput guess problem want sure thank philip",
        "Question_gpt_summary_original":"The user is inquiring about the potential challenges of having multiple users tracking directly on a PostgreSQL database at the same time. They also ask if there would be any issues if two different computers connect to the same database through MLFlow UI. The user is seeking confirmation that there will be no problems with these scenarios.",
        "Question_gpt_summary":"user inquir potenti challeng have multipl user track directli postgresql databas time ask issu differ comput connect databas user seek confirm problem scenario",
        "Answer_original_content":"us track directli databas postgresql caus problem comput track time believ issu given postgressql relat support transact run uniqu umbrella experi run differ person run differ question connect ui differ comput idea jule receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user ebaefbcd googlegroup com",
        "Answer_preprocessed_content":"us track directli databas caus problem comput track time believ issu given postgressql relat support transact run uniqu umbrella experi run differ person run differ question connect ui differ comput idea jule receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion include that there should be no issues with having multiple users tracking directly on a PostgreSQL database at the same time, as PostgreSQL is a relational database supporting transactions and each run has a unique ID. The same applies to connecting two different computers to the same database through MLFlow UI. No issues or problems were mentioned in the discussion.",
        "Answer_gpt_summary":"possibl solut mention discuss includ issu have multipl user track directli postgresql databas time postgresql relat databas support transact run uniqu appli connect differ comput databas issu problem mention discuss"
    },
    {
        "Question_title":"Pipeline can't find nodes in kedro",
        "Question_body":"<p>I was following <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/03_tutorial\/04_create_pipelines.html\" rel=\"nofollow noreferrer\">pipelines tutorial<\/a>, create all needed files, started the kedro with <code>kedro run --node=preprocessing_data<\/code> but got stuck with such error message:<\/p>\n\n<pre><code>ValueError: Pipeline does not contain nodes named ['preprocessing_data'].\n<\/code><\/pre>\n\n<p>If I run kedro without <code>node<\/code> parameter, I receive<\/p>\n\n<pre><code>kedro.context.context.KedroContextError: Pipeline contains no nodes\n<\/code><\/pre>\n\n<p>Contents of the files:<\/p>\n\n<pre><code>src\/project\/pipelines\/data_engineering\/nodes.py\ndef preprocess_data(data: SparkDataSet) -&gt; None:\n    print(data)\n    return\n<\/code><\/pre>\n\n<pre><code>src\/project\/pipelines\/data_engineering\/pipeline.py\ndef create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                func=preprocess_data,\n                inputs=\"data\",\n                outputs=\"preprocessed_data\",\n                name=\"preprocessing_data\",\n            ),\n        ]\n    )\n<\/code><\/pre>\n\n<pre><code>src\/project\/pipeline.py\ndef create_pipelines(**kwargs) -&gt; Dict[str, Pipeline]:\n    de_pipeline = de.create_pipeline()\n    return {\n        \"de\": de_pipeline,\n        \"__default__\": Pipeline([])\n    }\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1582395101757,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1583175186660,
        "Question_score":4.0,
        "Question_view_count":2155.0,
        "Answer_body":"<p>I think it looks like you need to have the pipeline in <code>__default__<\/code>.\ne.g.<\/p>\n\n<pre><code>def create_pipelines(**kwargs) -&gt; Dict[str, Pipeline]:\n    de_pipeline = de.create_pipeline()\n    return {\n        \"de\": data_engineering_pipeline,\n        \"__default__\": data_engineering_pipeline\n    }\n<\/code><\/pre>\n\n<p>Then <code>kedro run --node=preprocessing_data<\/code> works for me.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":8.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60355240",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1582427680590,
        "Question_original_content":"pipelin node follow pipelin tutori creat need file start run node preprocess data got stuck error messag valueerror pipelin contain node name preprocess data run node paramet receiv context context contexterror pipelin contain node content file src project pipelin data engin node def preprocess data data sparkdataset print data return src project pipelin data engin pipelin def creat pipelin kwarg return pipelin node func preprocess data input data output preprocess data preprocess data src project pipelin def creat pipelin kwarg dict str pipelin pipelin creat pipelin return pipelin default pipelin",
        "Question_preprocessed_content":"pipelin node follow pipelin tutori creat need file start got stuck error messag run paramet receiv content file",
        "Question_gpt_summary_original":"The user encountered an error while following the pipelines tutorial in Kedro. They created all the necessary files and started Kedro with the node parameter, but received an error message stating that the pipeline does not contain nodes named 'preprocessing_data'. Running Kedro without the node parameter resulted in an error message stating that the pipeline contains no nodes. The user provided the contents of the relevant files, including the function to preprocess data, the pipeline creation function, and the function to create pipelines.",
        "Question_gpt_summary":"user encount error follow pipelin tutori creat necessari file start node paramet receiv error messag state pipelin contain node name preprocess data run node paramet result error messag state pipelin contain node user provid content relev file includ function preprocess data pipelin creation function function creat pipelin",
        "Answer_original_content":"think look like need pipelin default def creat pipelin kwarg dict str pipelin pipelin creat pipelin return data engin pipelin default data engin pipelin run node preprocess data work",
        "Answer_preprocessed_content":"think look like need pipelin work",
        "Answer_gpt_summary_original":"Solution: The user can try adding the pipeline to the `__default__` dictionary in the `create_pipelines` function. This can be done by modifying the function to include the following line of code: `\"__default__\": data_engineering_pipeline`. After making this change, running Kedro with the node parameter should work.",
        "Answer_gpt_summary":"solut user try ad pipelin default dictionari creat pipelin function modifi function includ follow line code default data engin pipelin make chang run node paramet work"
    },
    {
        "Question_title":"Dealing with Kaggle data",
        "Question_body":"<p>What is the best way to deal with outside data, such as a well known data set such as Imagenet or a Kaggle dataset?<\/p>\n<p>More specifically, I want to download the data from the source as the first stage in the pipeline and be able to use them as dependencies for the next stage in the pipeline. But, I don\u2019t want to upload those (potentially) huge data sets to my s3 bucket.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1541129343230,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":1048.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/noenzyme\">@noenzyme<\/a> !<\/p>\n<p>There are a few ways you can go about it:<\/p>\n<ol>\n<li>If you don\u2019t need your dataset at all but rather need some much smaller transformed version of it, you can combine your <code>download<\/code> and <code>transform<\/code> pipeline stages into a single stage, so that dvc itself doesn\u2019t even know about the raw dataset, E.g.<\/li>\n<\/ol>\n<pre><code class=\"lang-auto\">dvc run -d download_and_transform.py -o transformed.data python download_and_transform.py\n<\/code><\/pre>\n<ol start=\"2\">\n<li>If you do indeed need that raw data(or if your <code>transform<\/code> stage is not that deterministic that you could just glue it together with the <code>download<\/code> stage), but just don\u2019t want to push the cache for it to your remote, you could simply tell dvc not to cache it in the <code>download<\/code> stage of your pipeline. E.g.<\/li>\n<\/ol>\n<pre><code class=\"lang-auto\"># Notice `-O|--outs-no-cache` flag, that tells dvc to not cache `raw.data`. Also note that you will need need to add it to .gitignore yourself.\ndvc run -d download.py -O raw.data python download.py\n<\/code><\/pre>\n<p>Would something like that suit your scenario?<\/p>\n<p>Thanks,<br>\nRuslan<\/p>. <p>Thanks <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a>!<\/p>\n<p>I hadn\u2019t thought of using the code <code>download.py<\/code> as a dependency. Option number 2 will work nicely I\u2019m thinking.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dealing-with-kaggle-data\/103",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-11-02T10:55:41.603Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/noenzyme\">@noenzyme<\/a> !<\/p>\n<p>There are a few ways you can go about it:<\/p>\n<ol>\n<li>If you don\u2019t need your dataset at all but rather need some much smaller transformed version of it, you can combine your <code>download<\/code> and <code>transform<\/code> pipeline stages into a single stage, so that dvc itself doesn\u2019t even know about the raw dataset, E.g.<\/li>\n<\/ol>\n<pre><code class=\"lang-auto\">dvc run -d download_and_transform.py -o transformed.data python download_and_transform.py\n<\/code><\/pre>\n<ol start=\"2\">\n<li>If you do indeed need that raw data(or if your <code>transform<\/code> stage is not that deterministic that you could just glue it together with the <code>download<\/code> stage), but just don\u2019t want to push the cache for it to your remote, you could simply tell dvc not to cache it in the <code>download<\/code> stage of your pipeline. E.g.<\/li>\n<\/ol>\n<pre><code class=\"lang-auto\"># Notice `-O|--outs-no-cache` flag, that tells dvc to not cache `raw.data`. Also note that you will need need to add it to .gitignore yourself.\ndvc run -d download.py -O raw.data python download.py\n<\/code><\/pre>\n<p>Would something like that suit your scenario?<\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-11-02T17:03:00.835Z",
                "Answer_body":"<p>Thanks <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a>!<\/p>\n<p>I hadn\u2019t thought of using the code <code>download.py<\/code> as a dependency. Option number 2 will work nicely I\u2019m thinking.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"deal kaggl data best wai deal outsid data known data set imagenet kaggl dataset specif want download data sourc stage pipelin abl us depend stage pipelin dont want upload potenti huge data set bucket",
        "Question_preprocessed_content":"deal kaggl data best wai deal outsid data known data set imagenet kaggl dataset specif want download data sourc stage pipelin abl us depend stage pipelin dont want upload huge data set bucket",
        "Question_gpt_summary_original":"The user is facing challenges in dealing with outside data, such as well-known datasets like Imagenet or Kaggle datasets. They want to download the data from the source as the first stage in the pipeline and use them as dependencies for the next stage, but they do not want to upload the potentially huge datasets to their s3 bucket.",
        "Question_gpt_summary":"user face challeng deal outsid data known dataset like imagenet kaggl dataset want download data sourc stage pipelin us depend stage want upload potenti huge dataset bucket",
        "Answer_original_content":"noenzym wai dont need dataset need smaller transform version combin download transform pipelin stage singl stage doesnt know raw dataset run download transform transform data python download transform need raw data transform stage determinist glue download stage dont want push cach remot simpli tell cach download stage pipelin notic out cach flag tell cach raw data note need need add gitignor run download raw data python download like suit scenario thank ruslan thank kuprus hadnt thought code download depend option number work nice think",
        "Answer_preprocessed_content":"wai dont need dataset need smaller transform version combin pipelin stage singl stage doesnt know raw dataset need raw data dont want push cach remot simpli tell cach stage pipelin like suit scenario thank ruslan thank hadnt thought code depend option number work nice think",
        "Answer_gpt_summary_original":"Solutions:\n- Combine the download and transform pipeline stages into a single stage.\n- Tell DVC not to cache the raw data in the download stage of the pipeline.",
        "Answer_gpt_summary":"solut combin download transform pipelin stage singl stage tell cach raw data download stage pipelin"
    },
    {
        "Question_title":"SageMaker Managed Spot Training with Object Detection algorithm",
        "Question_body":"<p>I'm trying to train an object detection model starting from an existing model using the new Managed Spot Training feature,  The paramters used when creating my Estimator are as follows:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>od_model = sagemaker.estimator.Estimator(get_image_uri(sagemaker.Session().boto_region_name, 'object-detection', repo_version=\"latest\"),\n                                         Config['role'],\n                                         train_instance_count = 1,\n                                         train_instance_type = 'ml.p3.16xlarge',\n                                         train_volume_size = 50,\n                                         train_max_run = (48 * 60 * 60),\n                                         train_use_spot_instances = True,\n                                         train_max_wait = (72 * 60 * 60),\n                                         input_mode = 'File',\n                                         checkpoint_s3_uri = Config['train_checkpoint_uri'],\n                                         output_path = Config['s3_output_location'],\n                                         sagemaker_session = sagemaker.Session()\n                                         )\n<\/code><\/pre>\n\n<p>(The references to <code>Config<\/code> in the above are a config data structure I'm using to extract\/centralise some parameters)<\/p>\n\n<p>When I run the above, I get the following exception:<\/p>\n\n<blockquote>\n  <p>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: MaxWaitTimeInSeconds above 3600 is not supported for the given algorithm.<\/p>\n<\/blockquote>\n\n<p>If I change <code>train_max_wait<\/code> to 3600 I get this exception instead:<\/p>\n\n<blockquote>\n  <p>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: Invalid MaxWaitTimeInSeconds. It must be present and be greater than or equal to MaxRuntimeInSeconds<\/p>\n<\/blockquote>\n\n<p>However changing <code>max_run_time<\/code> to 3600 or less isn't going to work for me as I expect this model to take several days to train (large data set), in fact a single epoch takes more than an hour.<\/p>\n\n<p>The <a href=\"https:\/\/aws.amazon.com\/blogs\/aws\/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs\/\" rel=\"nofollow noreferrer\">AWS blog post on Managed Spot Training<\/a> say that <code>MaxWaitTimeInSeconds<\/code> is limited to an 60 minutes for:<\/p>\n\n<blockquote>\n  <p>For built-in algorithms and AWS Marketplace algorithms that don\u2019t use checkpointing, we\u2019re enforcing a maximum training time of 60 minutes (MaxWaitTimeInSeconds parameter).<\/p>\n<\/blockquote>\n\n<p>Earlier, the same blog post says:<\/p>\n\n<blockquote>\n  <p>Built-in algorithms: computer vision algorithms support checkpointing (Object Detection, Semantic Segmentation, and very soon Image Classification).<\/p>\n<\/blockquote>\n\n<p>So I don't think it's that my algorithm doesn't support Checkpointing.  In fact that blog post uses object detection and max run times of 48 hours.  So I don't think it's an algorithm limitation.<\/p>\n\n<p>As you can see above, I've set up a S3 URL for the checkpoints.  The S3 bucket does exist, and the training container has access to it (it's the same bucket that the training data and model outputs are placed, and I had no problems with access to those before turning on spot training.<\/p>\n\n<p>My boto and sagemaker libraries are current versions:<\/p>\n\n<pre><code>boto3 (1.9.239)\nbotocore (1.12.239)\nsagemaker (1.42.3)\n<\/code><\/pre>\n\n<p>As best I can tell from reading various docs, I've got everything set up correctly.  My use case is almost exactly what's described in the blog post linked above, but I'm using the SageMaker Python SDK instead of the console.<\/p>\n\n<p>I'd really like to try Managed Spot Training to save some money, as I have a very long training run coming up.  But limiting timeouts to an hour isn't going to work for my use case.  Any suggestions?<\/p>\n\n<p><strong>Update:<\/strong>  If I comment out the <code>train_use_spot_instances<\/code> and <code>train_max_wait<\/code> options to train on regular on-demand instances my training job is created successfully.  If I then try to use the console to clone the job and turn on Spot instances on the clone I get the same ValidationException.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1569898766770,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1569900383487,
        "Question_score":4.0,
        "Question_view_count":1234.0,
        "Answer_body":"<p>I ran my script again today and it worked fine, no <code>botocore.exceptions.ClientError<\/code> exceptions.  Given that this issue affected both the Python SDK for Sagemaker and the console, I suspect it might have been an issue with the backend API and not my client code.<\/p>\n<p>Either way, it's working now.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1646271695950,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58177548",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1570492274472,
        "Question_original_content":"manag spot train object detect algorithm try train object detect model start exist model new manag spot train featur paramt creat estim follow model estim estim imag uri session boto region object detect repo version latest config role train instanc count train instanc type xlarg train volum size train max run train us spot instanc true train max wait input mode file checkpoint uri config train checkpoint uri output path config output locat session session refer config config data structur extract centralis paramet run follow except botocor except clienterror error occur validationexcept call createtrainingjob oper maxwaittimeinsecond support given algorithm chang train max wait except instead botocor except clienterror error occur validationexcept call createtrainingjob oper invalid maxwaittimeinsecond present greater equal maxruntimeinsecond chang max run time isn go work expect model dai train larg data set fact singl epoch take hour aw blog post manag spot train maxwaittimeinsecond limit minut built algorithm aw marketplac algorithm dont us checkpoint enforc maximum train time minut maxwaittimeinsecond paramet earlier blog post sai built algorithm vision algorithm support checkpoint object detect semant segment soon imag classif think algorithm support checkpoint fact blog post us object detect max run time hour think algorithm limit set url checkpoint bucket exist train contain access bucket train data model output place problem access turn spot train boto librari current version boto botocor best tell read doc got set correctli us case exactli describ blog post link python sdk instead consol like try manag spot train save monei long train run come limit timeout hour isn go work us case suggest updat comment train us spot instanc train max wait option train regular demand instanc train job creat successfulli try us consol clone job turn spot instanc clone validationexcept",
        "Question_preprocessed_content":"manag spot train object detect algorithm try train object detect model start exist model new manag spot train featur paramt creat estim follow refer config data structur paramet run follow except error occur call createtrainingjob oper maxwaittimeinsecond support given algorithm chang except instead error occur call createtrainingjob oper invalid maxwaittimeinsecond present greater equal maxruntimeinsecond chang isn go work expect model dai train fact singl epoch take hour aw blog post manag spot train limit minut algorithm aw marketplac algorithm dont us checkpoint enforc maximum train time minut earlier blog post sai algorithm vision algorithm support checkpoint think algorithm support checkpoint fact blog post us object detect max run time hour think algorithm limit set url checkpoint bucket exist train contain access bucket train data model output place problem access turn spot train boto librari current version best tell read doc got set correctli us case exactli describ blog post link python sdk instead consol like try manag spot train save monei long train run come limit timeout hour isn go work us case suggest updat comment option train regular instanc train job creat successfulli try us consol clone job turn spot instanc clone validationexcept",
        "Question_gpt_summary_original":"The user is encountering challenges when trying to use the SageMaker Managed Spot Training feature with an object detection algorithm. They are receiving a ValidationException error when setting the train_max_wait parameter to a value greater than 3600, and when they set it to 3600 or less, they receive another ValidationException error. The user needs to train their model for several days, and the algorithm they are using supports checkpointing. The AWS blog post on Managed Spot Training states that the MaxWaitTimeInSeconds parameter is limited to 60 minutes for built-in algorithms and AWS Marketplace algorithms that do not use checkpointing, but the user's algorithm supports checkpointing. The user has checked that their S3 bucket exists and that the training container has access to it. When they comment out the train_use_spot_instances and train_max_wait options, they can create a training job successfully on regular on-demand instances. However, when they try to clone the job and turn on Spot instances on the clone, they receive the",
        "Question_gpt_summary":"user encount challeng try us manag spot train featur object detect algorithm receiv validationexcept error set train max wait paramet valu greater set receiv validationexcept error user need train model dai algorithm support checkpoint aw blog post manag spot train state maxwaittimeinsecond paramet limit minut built algorithm aw marketplac algorithm us checkpoint user algorithm support checkpoint user check bucket exist train contain access comment train us spot instanc train max wait option creat train job successfulli regular demand instanc try clone job turn spot instanc clone receiv",
        "Answer_original_content":"ran script todai work fine botocor except clienterror except given issu affect python sdk consol suspect issu backend api client code wai work",
        "Answer_preprocessed_content":"ran script todai work fine except given issu affect python sdk consol suspect issu backend api client code wai work",
        "Answer_gpt_summary_original":"There are no solutions provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"ModuleNotFoundError when starting a training job on Sagemaker",
        "Question_body":"I want to submit a training job on sagemaker. I tried it on notebook and it works. When I try the following I get ModuleNotFoundError: No module named 'nltk'\n\nMy code is\n\nimport sagemaker  \nfrom sagemaker.pytorch import PyTorch\n\nJOB_PREFIX   = 'pyt-ic'\nFRAMEWORK_VERSION = '1.3.1'\n\nestimator = PyTorch(entry_point='finetune-T5.py',\n                   source_dir='..\/src',\n                   train_instance_type='ml.p2.xlarge' ,\n                   train_instance_count=1,\n                   role=sagemaker.get_execution_role(),\n                   framework_version=FRAMEWORK_VERSION, \n                   debugger_hook_config=False,  \n                   py_version='py3',\n                   base_job_name=JOB_PREFIX)\n\nestimator.fit()\n\n\nfinetune-T5.py have many other libraries that are not installed. How can I install the missing library? Or is there a better way to run the training job?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1598912648000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":357.0,
        "Answer_body":"Check out this link (Using third-party libraries section) on how to install third-party libraries for training jobs. You need to create requirement.txt file in the same directory as your training script to install other dependencies at runtime.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJMd_4s52RpWXDXITXFsQdw\/module-not-found-error-when-starting-a-training-job-on-sagemaker",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-08-31T22:47:15.000Z",
                "Answer_score":0,
                "Answer_body":"Check out this link (Using third-party libraries section) on how to install third-party libraries for training jobs. You need to create requirement.txt file in the same directory as your training script to install other dependencies at runtime.",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1598914035000,
        "Question_original_content":"modulenotfounderror start train job want submit train job tri notebook work try follow modulenotfounderror modul name nltk code import pytorch import pytorch job prefix pyt framework version estim pytorch entri point finetun sourc dir src train instanc type xlarg train instanc count role execut role framework version framework version debugg hook config fals version base job job prefix estim fit finetun librari instal instal miss librari better wai run train job",
        "Question_preprocessed_content":"modulenotfounderror start train job want submit train job tri notebook work try follow modulenotfounderror modul name nltk code import pytorch import pytorch estim librari instal instal miss librari better wai run train job",
        "Question_gpt_summary_original":"The user is encountering a \"ModuleNotFoundError\" when attempting to submit a training job on Sagemaker. The error message specifically states that the \"nltk\" module is missing. The user's code includes an import statement for Sagemaker and PyTorch, and specifies various parameters for the training job. The user's \"finetune-T5.py\" file requires additional libraries that are not installed, and the user is seeking advice on how to install these missing libraries or if there is a better way to run the training job.",
        "Question_gpt_summary":"user encount modulenotfounderror attempt submit train job error messag specif state nltk modul miss user code includ import statement pytorch specifi paramet train job user finetun file requir addit librari instal user seek advic instal miss librari better wai run train job",
        "Answer_original_content":"check link parti librari section instal parti librari train job need creat requir txt file directori train script instal depend runtim",
        "Answer_preprocessed_content":"check link instal librari train job need creat file directori train script instal depend runtim",
        "Answer_gpt_summary_original":"Solution: The user can create a requirement.txt file in the same directory as their training script to install other dependencies at runtime. They can refer to the \"Using third-party libraries\" section in the provided link to learn how to install third-party libraries for training jobs.",
        "Answer_gpt_summary":"solut user creat requir txt file directori train script instal depend runtim refer parti librari section provid link learn instal parti librari train job"
    },
    {
        "Question_title":"How to debug my init git container",
        "Question_body":"From slack\n\nMy job is stack with a warning status, I configured a private bitbucket connection and the cloning fails.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649328334000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Before updating the connections or changing anything about your current deployment, please perform the following debugging steps:\n\nEnable logs from all containers:\n\n\n\nYou can also inspect the operations from the statuses page to get more information (for distributed runs you can select the correct pod)\n\n\nYou can suspend the init container using :\n\n  - connection: my-connection\n    git: {...}\n    container:\n      command: [\"\/bin\/bash\", \"-c\"]\n      args: [\"sleep 3600\"]\nUse shell to get inside the container (for distributed runs you can select the correct pod and container):",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1472",
        "Tool":"Polyaxon",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-07T10:51:48Z",
                "Answer_score":1,
                "Answer_body":"Before updating the connections or changing anything about your current deployment, please perform the following debugging steps:\n\nEnable logs from all containers:\n\n\n\nYou can also inspect the operations from the statuses page to get more information (for distributed runs you can select the correct pod)\n\n\nYou can suspend the init container using :\n\n  - connection: my-connection\n    git: {...}\n    container:\n      command: [\"\/bin\/bash\", \"-c\"]\n      args: [\"sleep 3600\"]\nUse shell to get inside the container (for distributed runs you can select the correct pod and container):"
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"debug init git contain slack job stack warn statu configur privat bitbucket connect clone fail",
        "Question_preprocessed_content":"debug init git contain slack job stack warn statu configur privat bitbucket connect clone fail",
        "Question_gpt_summary_original":"The user is facing challenges with their job being stuck in a warning status due to a failed cloning process after configuring a private bitbucket connection. They are seeking help with debugging their init git container.",
        "Question_gpt_summary":"user face challeng job stuck warn statu fail clone process configur privat bitbucket connect seek help debug init git contain",
        "Answer_original_content":"updat connect chang current deploy perform follow debug step enabl log contain inspect oper status page inform distribut run select correct pod suspend init contain connect connect git contain command bin bash arg sleep us shell insid contain distribut run select correct pod contain",
        "Answer_preprocessed_content":"updat connect chang current deploy perform follow debug step enabl log contain inspect oper status page inform suspend init contain connect git contain command arg us shell insid contain",
        "Answer_gpt_summary_original":"Solution: The discussion provides some debugging steps to help the user resolve their issue. These include enabling logs from all containers, inspecting operations from the statuses page, suspending the init container using a specific command, and using shell to get inside the container. No specific solution is mentioned, but these steps can help the user identify and fix the problem.",
        "Answer_gpt_summary":"solut discuss provid debug step help user resolv issu includ enabl log contain inspect oper status page suspend init contain specif command shell insid contain specif solut mention step help user identifi fix problem"
    },
    {
        "Question_title":"Best compute cluster for training large image datasets !",
        "Question_body":"Good morning,\nI have a a dataset that consist of 99000 (256 x 256 pixels) images. I am trying to use this dataset to training a generative advesarial network (GAN) for at least a 1,000 epoch.\nCurrently, I am using a standard_NC24r (24 cores, 224 GB RAM, 1440 GB disk) GPU (4 x NVIDIA Tesla K80) cluster but the training is slow. It takes about 3000 seconds to train 1 epoch. This implies it would take at least a month to complete training.\nIs a cluster that I can used to speed up training?\n\nThanks for your help in advance\n\nMany thanks\n\nRoland",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1633703536287,
        "Question_favorite_count":7.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@OkwenRolandT-6377 Thanks, Instead of bigger machines with more memory, there are techniques to be used with Aml Compute for larger datasets. The Parallel Run Step is an AzureML Pipeline Step which enables parallel processing or data partitions across multiple workers on multiple nodes. PRS (ParallelRunStep) is designed for embarrassingly parallel workload, e.g. train many models, batch inference, etc.\n\nAlso look into using some of the curated images provided for compute clusters.\nSpecifically look into the DASK image.\n\nCurated environments - Azure Machine Learning | Microsoft Docs",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/583349\/best-compute-cluster-for-training-large-image-data.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-11T05:01:12.66Z",
                "Answer_score":0,
                "Answer_body":"@OkwenRolandT-6377 Thanks, Instead of bigger machines with more memory, there are techniques to be used with Aml Compute for larger datasets. The Parallel Run Step is an AzureML Pipeline Step which enables parallel processing or data partitions across multiple workers on multiple nodes. PRS (ParallelRunStep) is designed for embarrassingly parallel workload, e.g. train many models, batch inference, etc.\n\nAlso look into using some of the curated images provided for compute clusters.\nSpecifically look into the DASK image.\n\nCurated environments - Azure Machine Learning | Microsoft Docs",
                "Answer_comment_count":4,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1633928472660,
        "Question_original_content":"best comput cluster train larg imag dataset good morn dataset consist pixel imag try us dataset train gener advesari network gan epoch current standard ncr core ram disk gpu nvidia tesla cluster train slow take second train epoch impli month complet train cluster speed train thank help advanc thank roland",
        "Question_preprocessed_content":"best comput cluster train larg imag dataset good morn dataset consist imag try us dataset train gener advesari network epoch current gpu cluster train slow take second train epoch impli month complet train cluster speed train thank help advanc thank roland",
        "Question_gpt_summary_original":"The user is facing challenges in training a generative adversarial network (GAN) using a standard_NC24r GPU cluster due to slow training time of 3000 seconds per epoch, which would take at least a month to complete training for 1000 epochs. The user is seeking advice on a better compute cluster to speed up the training process.",
        "Question_gpt_summary":"user face challeng train gener adversari network gan standard ncr gpu cluster slow train time second epoch month complet train epoch user seek advic better comput cluster speed train process",
        "Answer_original_content":"okwenrolandt thank instead bigger machin memori techniqu aml comput larger dataset parallel run step pipelin step enabl parallel process data partit multipl worker multipl node pr parallelrunstep design embarrassingli parallel workload train model batch infer look curat imag provid comput cluster specif look dask imag curat environ microsoft doc",
        "Answer_preprocessed_content":"thank instead bigger machin memori techniqu aml comput larger dataset parallel run step pipelin step enabl parallel process data partit multipl worker multipl node pr design embarrassingli parallel workload train model batch infer look curat imag provid comput cluster specif look dask imag curat environ microsoft doc",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Using the Parallel Run Step (PRS) in AzureML Pipeline to enable parallel processing or data partitions across multiple workers on multiple nodes.\n2. Looking into using some of the curated images provided for compute clusters, specifically the DASK image.\n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss parallel run step pr pipelin enabl parallel process data partit multipl worker multipl node look curat imag provid comput cluster specif dask imag person opinion bias includ summari"
    },
    {
        "Question_title":"Read only mode add dataset",
        "Question_body":"<p>My dataset folder only read mode. I dont want to active write mode. So, how can \u0131 add and track my dataset?<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1613568661758,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":677.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/merve\">@merve<\/a>,<\/p>\n<p>Are you getting some specific error when <code>dvc add<\/code>ing it? Generally, to add data into dvc, you need to own the file\/directory, so if you don\u2019t - you need to copy it and then <code>dvc add<\/code> it.<\/p>. <p>\u0131 used simplicity dvc add mydatasetpath<br>\nerror : ERROR: unexpected error - [Errno 30] Read-only file system: mydatasetpath\/abc.jpg<\/p>. <p>Could you show full log for <code>dvc add mydatasetopath -v<\/code>, please? Also, please show <code>dvc doctor<\/code> output.<\/p>. <h2>dvc doctor output;<br>\nDVC version: 1.11.16 (pip)<\/h2>\n<p>Platform: Python 3.6.9 on Linux-4.15.0-135-generic-x86_64-with-Ubuntu-18.04-bionic<br>\nSupports: http, https<br>\nCache types: hardlink, symlink<br>\nCache directory: overlay on overlay<br>\nCaches: local<br>\nRemotes: None<br>\nWorkspace directory: overlay on overlay<br>\nRepo: dvc (no_scm)<\/p>\n<p>and dvc add mydatasetopath -v output is<br>\n2021-02-17 15:28:59,058 DEBUG: Analytics is enabled.<br>\n2021-02-17 15:28:59,120 DEBUG: Trying to spawn \u2018[\u2018daemon\u2019, \u2018-q\u2019, \u2018analytics\u2019, \u2018\/tmp\/tmpt3or8f1b\u2019]\u2019<br>\n2021-02-17 15:28:59,122 DEBUG: Spawned \u2018[\u2018daemon\u2019, \u2018-q\u2019, \u2018analytics\u2019, \u2018\/tmp\/tmpt3or8f1b\u2019]\u2019<\/p>. <p>Thanks!<\/p>\n<p>Unfortunately that <code>dvc add mydatasetopath -v<\/code> log doesn\u2019t look right, try <code>dvc add mydatasetopath -v &amp;&gt; log.log<\/code> and post the contents of log.log<\/p>. <pre><code class=\"lang-auto\">2021-02-17 15:50:06,877 DEBUG: Check for update is enabled.\n2021-02-17 15:50:06,878 DEBUG: fetched: [(3,)]\n2021-02-17 15:50:22,785 DEBUG: Path '\/jup\/datasets\/TRAIN\/PLATE\/LOCATION\/VERSION2\/FRONT\/jenoptic-1' inode '768770473'\n2021-02-17 15:50:22,785 DEBUG: fetched: [('41882c1af0439fd46eeb5f0f90a11338', '2718846001', 'e27dcd51611a843f0744ba096799f52e.dir', '1613576997068446720')]\n2021-02-17 15:50:22,817 DEBUG: Assuming '\/jup\/.dvc\/cache\/e2\/7dcd51611a843f0744ba096799f52e.dir' is unchanged since it is read-only\n2021-02-17 15:50:22,817 DEBUG: {'datasets\/TRAIN\/PLATE\/LOCATION\/VERSION2\/FRONT\/jenoptic-1': 'modified'}\n2021-02-17 15:50:22,867 DEBUG: Path '\/jup\/datasets\/TRAIN\/PLATE\/LOCATION\/VERSION2\/FRONT\/jenoptic-1' inode '768770473'\n2021-02-17 15:50:22,867 DEBUG: fetched: [('41882c1af0439fd46eeb5f0f90a11338', '2718846001', 'e27dcd51611a843f0744ba096799f52e.dir', '1613577022786244352')]\n2021-02-17 15:50:22,868 DEBUG: Assuming '\/jup\/.dvc\/cache\/e2\/7dcd51611a843f0744ba096799f52e.dir' is unchanged since it is read-only\n2021-02-17 15:50:22,868 DEBUG: Computed stage: 'datasets\/TRAIN\/PLATE\/LOCATION\/VERSION2\/FRONT\/jenoptic-1.dvc' md5: 'None'\n2021-02-17 15:50:22,871 DEBUG: Saving 'datasets\/TRAIN\/PLATE\/LOCATION\/VERSION2\/FRONT\/jenoptic-1' to '.dvc\/cache\/e2\/7dcd51611a843f0744ba096799f52e.dir'.\n2021-02-17 15:50:22,871 DEBUG: Assuming '\/jup\/.dvc\/cache\/e2\/7dcd51611a843f0744ba096799f52e.dir' is unchanged since it is read-only\n2021-02-17 15:50:22,871 DEBUG: cache '\/jup\/.dvc\/cache\/2f\/6cc7cf7fab02868e12f9d4707d719e' expected 'HashInfo(name='md5', value='2f6cc7cf7fab02868e12f9d4707d719e', dir_info=None, size=None, nfiles=None)' actual 'None'\n2021-02-17 15:50:22,882 DEBUG: fetched: [(2,)]\n2021-02-17 15:50:22,894 ERROR: unexpected error - [Errno 30] Read-only file system: '\/jup\/datasets\/TRAIN\/PLATE\/LOCATION\/VERSION2\/FRONT\/jenoptic-1\/images\/PLATE_LOC_F_D_jenoptic-1_V2-01BLJ36_3511612000020101_2020-12-10_12-50-18-150_0_0_0_73_0-0-0-0_0_0_0_D60DD87C000015372101_1_0.jpg'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/usr\/lib\/python3.6\/shutil.py\", line 550, in move\n    os.rename(src, real_dst)\nOSError: [Errno 18] Invalid cross-device link: '\/jup\/datasets\/TRAIN\/PLATE\/LOCATION\/VERSION2\/FRONT\/jenoptic-1\/images\/PLATE_LOC_F_D_jenoptic-1_V2-01BLJ36_3511612000020101_2020-12-10_12-50-18-150_0_0_0_73_0-0-0-0_0_0_0_D60DD87C000015372101_1_0.jpg' -&gt; '\/jup\/.dvc\/cache\/2f\/6cc7cf7fab02868e12f9d4707d719e.iV3dGZaD64MuEyY4E4Y4QZ'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/main.py\", line 90, in main\n    ret = cmd.run()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/command\/add.py\", line 24, in run\n    desc=self.args.desc,\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/repo\/__init__.py\", line 54, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/repo\/scm_context.py\", line 4, in run\n    result = method(repo, *args, **kw)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/repo\/add.py\", line 85, in add\n    _process_stages(repo, stages, no_commit, pbar)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/repo\/add.py\", line 124, in _process_stages\n    stage.commit()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/funcy\/decorators.py\", line 39, in wrapper\n    return deco(call, *dargs, **dkwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/stage\/decorators.py\", line 36, in rwlocked\n    return call()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/funcy\/decorators.py\", line 60, in __call__\n    return self._func(*self._args, **self._kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/stage\/__init__.py\", line 478, in commit\n    out.commit()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/output\/base.py\", line 297, in commit\n    self.cache.save(self.path_info, self.cache.tree, self.hash_info)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/funcy\/decorators.py\", line 39, in wrapper\n    return deco(call, *dargs, **dkwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/cache\/base.py\", line 40, in use_state\n    return call()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/funcy\/decorators.py\", line 60, in __call__\n    return self._func(*self._args, **self._kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/cache\/base.py\", line 317, in save\n    self._save(path_info, tree, hash_info, save_link, **kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/cache\/base.py\", line 324, in _save\n    self._save_dir(path_info, tree, hash_info, save_link, **kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/cache\/base.py\", line 298, in _save_dir\n    entry_info, tree, entry_hash, save_link=False, **kwargs\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/cache\/base.py\", line 194, in _save_file\n    self.tree.move(path_info, cache_info, mode=self.CACHE_MODE)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/tree\/local.py\", line 176, in move\n    move(from_info, to_info, mode=mode)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/utils\/fs.py\", line 102, in move\n    shutil.move(src, tmp)\n  File \"\/usr\/lib\/python3.6\/shutil.py\", line 565, in move\n    os.unlink(src)\nOSError: [Errno 30] Read-only file system: '\/jup\/datasets\/TRAIN\/PLATE\/LOCATION\/VERSION2\/FRONT\/jenoptic-1\/images\/PLATE_LOC_F_D_jenoptic-1_V2-01BLJ36_3511612000020101_2020-12-10_12-50-18-150_0_0_0_73_0-0-0-0_0_0_0_D60DD87C000015372101_1_0.jpg'\n------------------------------------------------------------\n2021-02-17 15:50:22,920 DEBUG: Version info for developers:\nDVC version: 1.11.16 (pip)\n---------------------------------\nPlatform: Python 3.6.9 on Linux-4.15.0-135-generic-x86_64-with-Ubuntu-18.04-bionic\nSupports: http, https\nCache types: hardlink, symlink\nCache directory: overlay on overlay\nCaches: local\nRemotes: None\nWorkspace directory: overlay on overlay\nRepo: dvc (no_scm)\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2021-02-17 15:50:22,922 DEBUG: Analytics is enabled.\n2021-02-17 15:50:22,989 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp8z6k9p1y']'\n2021-02-17 15:50:22,991 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp8z6k9p1y']'\n<\/code><\/pre>. <p>Thanks!<\/p>\n<p>Ok, so your whole dvc repository is on a read-only filesystem. That won\u2019t work, as dvc needs to move files around and that is not permitted by the read-only filesystem. You need to move your dvc project to a normal filesystem and then copy the data you want to add into it and then <code>dvc add<\/code> it there.<\/p>. <p>thank you I will try<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/read-only-mode-add-dataset\/674",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-17T13:44:28.180Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/merve\">@merve<\/a>,<\/p>\n<p>Are you getting some specific error when <code>dvc add<\/code>ing it? Generally, to add data into dvc, you need to own the file\/directory, so if you don\u2019t - you need to copy it and then <code>dvc add<\/code> it.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-17T15:13:06.043Z",
                "Answer_body":"<p>\u0131 used simplicity dvc add mydatasetpath<br>\nerror : ERROR: unexpected error - [Errno 30] Read-only file system: mydatasetpath\/abc.jpg<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-17T15:22:54.116Z",
                "Answer_body":"<p>Could you show full log for <code>dvc add mydatasetopath -v<\/code>, please? Also, please show <code>dvc doctor<\/code> output.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-17T15:29:57.325Z",
                "Answer_body":"<h2>dvc doctor output;<br>\nDVC version: 1.11.16 (pip)<\/h2>\n<p>Platform: Python 3.6.9 on Linux-4.15.0-135-generic-x86_64-with-Ubuntu-18.04-bionic<br>\nSupports: http, https<br>\nCache types: hardlink, symlink<br>\nCache directory: overlay on overlay<br>\nCaches: local<br>\nRemotes: None<br>\nWorkspace directory: overlay on overlay<br>\nRepo: dvc (no_scm)<\/p>\n<p>and dvc add mydatasetopath -v output is<br>\n2021-02-17 15:28:59,058 DEBUG: Analytics is enabled.<br>\n2021-02-17 15:28:59,120 DEBUG: Trying to spawn \u2018[\u2018daemon\u2019, \u2018-q\u2019, \u2018analytics\u2019, \u2018\/tmp\/tmpt3or8f1b\u2019]\u2019<br>\n2021-02-17 15:28:59,122 DEBUG: Spawned \u2018[\u2018daemon\u2019, \u2018-q\u2019, \u2018analytics\u2019, \u2018\/tmp\/tmpt3or8f1b\u2019]\u2019<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-17T15:33:05.726Z",
                "Answer_body":"<p>Thanks!<\/p>\n<p>Unfortunately that <code>dvc add mydatasetopath -v<\/code> log doesn\u2019t look right, try <code>dvc add mydatasetopath -v &amp;&gt; log.log<\/code> and post the contents of log.log<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-17T15:52:32.172Z",
                "Answer_body":"<pre><code class=\"lang-auto\">2021-02-17 15:50:06,877 DEBUG: Check for update is enabled.\n2021-02-17 15:50:06,878 DEBUG: fetched: [(3,)]\n2021-02-17 15:50:22,785 DEBUG: Path '\/jup\/datasets\/TRAIN\/PLATE\/LOCATION\/VERSION2\/FRONT\/jenoptic-1' inode '768770473'\n2021-02-17 15:50:22,785 DEBUG: fetched: [('41882c1af0439fd46eeb5f0f90a11338', '2718846001', 'e27dcd51611a843f0744ba096799f52e.dir', '1613576997068446720')]\n2021-02-17 15:50:22,817 DEBUG: Assuming '\/jup\/.dvc\/cache\/e2\/7dcd51611a843f0744ba096799f52e.dir' is unchanged since it is read-only\n2021-02-17 15:50:22,817 DEBUG: {'datasets\/TRAIN\/PLATE\/LOCATION\/VERSION2\/FRONT\/jenoptic-1': 'modified'}\n2021-02-17 15:50:22,867 DEBUG: Path '\/jup\/datasets\/TRAIN\/PLATE\/LOCATION\/VERSION2\/FRONT\/jenoptic-1' inode '768770473'\n2021-02-17 15:50:22,867 DEBUG: fetched: [('41882c1af0439fd46eeb5f0f90a11338', '2718846001', 'e27dcd51611a843f0744ba096799f52e.dir', '1613577022786244352')]\n2021-02-17 15:50:22,868 DEBUG: Assuming '\/jup\/.dvc\/cache\/e2\/7dcd51611a843f0744ba096799f52e.dir' is unchanged since it is read-only\n2021-02-17 15:50:22,868 DEBUG: Computed stage: 'datasets\/TRAIN\/PLATE\/LOCATION\/VERSION2\/FRONT\/jenoptic-1.dvc' md5: 'None'\n2021-02-17 15:50:22,871 DEBUG: Saving 'datasets\/TRAIN\/PLATE\/LOCATION\/VERSION2\/FRONT\/jenoptic-1' to '.dvc\/cache\/e2\/7dcd51611a843f0744ba096799f52e.dir'.\n2021-02-17 15:50:22,871 DEBUG: Assuming '\/jup\/.dvc\/cache\/e2\/7dcd51611a843f0744ba096799f52e.dir' is unchanged since it is read-only\n2021-02-17 15:50:22,871 DEBUG: cache '\/jup\/.dvc\/cache\/2f\/6cc7cf7fab02868e12f9d4707d719e' expected 'HashInfo(name='md5', value='2f6cc7cf7fab02868e12f9d4707d719e', dir_info=None, size=None, nfiles=None)' actual 'None'\n2021-02-17 15:50:22,882 DEBUG: fetched: [(2,)]\n2021-02-17 15:50:22,894 ERROR: unexpected error - [Errno 30] Read-only file system: '\/jup\/datasets\/TRAIN\/PLATE\/LOCATION\/VERSION2\/FRONT\/jenoptic-1\/images\/PLATE_LOC_F_D_jenoptic-1_V2-01BLJ36_3511612000020101_2020-12-10_12-50-18-150_0_0_0_73_0-0-0-0_0_0_0_D60DD87C000015372101_1_0.jpg'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/usr\/lib\/python3.6\/shutil.py\", line 550, in move\n    os.rename(src, real_dst)\nOSError: [Errno 18] Invalid cross-device link: '\/jup\/datasets\/TRAIN\/PLATE\/LOCATION\/VERSION2\/FRONT\/jenoptic-1\/images\/PLATE_LOC_F_D_jenoptic-1_V2-01BLJ36_3511612000020101_2020-12-10_12-50-18-150_0_0_0_73_0-0-0-0_0_0_0_D60DD87C000015372101_1_0.jpg' -&gt; '\/jup\/.dvc\/cache\/2f\/6cc7cf7fab02868e12f9d4707d719e.iV3dGZaD64MuEyY4E4Y4QZ'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/main.py\", line 90, in main\n    ret = cmd.run()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/command\/add.py\", line 24, in run\n    desc=self.args.desc,\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/repo\/__init__.py\", line 54, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/repo\/scm_context.py\", line 4, in run\n    result = method(repo, *args, **kw)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/repo\/add.py\", line 85, in add\n    _process_stages(repo, stages, no_commit, pbar)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/repo\/add.py\", line 124, in _process_stages\n    stage.commit()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/funcy\/decorators.py\", line 39, in wrapper\n    return deco(call, *dargs, **dkwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/stage\/decorators.py\", line 36, in rwlocked\n    return call()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/funcy\/decorators.py\", line 60, in __call__\n    return self._func(*self._args, **self._kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/stage\/__init__.py\", line 478, in commit\n    out.commit()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/output\/base.py\", line 297, in commit\n    self.cache.save(self.path_info, self.cache.tree, self.hash_info)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/funcy\/decorators.py\", line 39, in wrapper\n    return deco(call, *dargs, **dkwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/cache\/base.py\", line 40, in use_state\n    return call()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/funcy\/decorators.py\", line 60, in __call__\n    return self._func(*self._args, **self._kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/cache\/base.py\", line 317, in save\n    self._save(path_info, tree, hash_info, save_link, **kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/cache\/base.py\", line 324, in _save\n    self._save_dir(path_info, tree, hash_info, save_link, **kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/cache\/base.py\", line 298, in _save_dir\n    entry_info, tree, entry_hash, save_link=False, **kwargs\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/cache\/base.py\", line 194, in _save_file\n    self.tree.move(path_info, cache_info, mode=self.CACHE_MODE)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/tree\/local.py\", line 176, in move\n    move(from_info, to_info, mode=mode)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/dvc\/utils\/fs.py\", line 102, in move\n    shutil.move(src, tmp)\n  File \"\/usr\/lib\/python3.6\/shutil.py\", line 565, in move\n    os.unlink(src)\nOSError: [Errno 30] Read-only file system: '\/jup\/datasets\/TRAIN\/PLATE\/LOCATION\/VERSION2\/FRONT\/jenoptic-1\/images\/PLATE_LOC_F_D_jenoptic-1_V2-01BLJ36_3511612000020101_2020-12-10_12-50-18-150_0_0_0_73_0-0-0-0_0_0_0_D60DD87C000015372101_1_0.jpg'\n------------------------------------------------------------\n2021-02-17 15:50:22,920 DEBUG: Version info for developers:\nDVC version: 1.11.16 (pip)\n---------------------------------\nPlatform: Python 3.6.9 on Linux-4.15.0-135-generic-x86_64-with-Ubuntu-18.04-bionic\nSupports: http, https\nCache types: hardlink, symlink\nCache directory: overlay on overlay\nCaches: local\nRemotes: None\nWorkspace directory: overlay on overlay\nRepo: dvc (no_scm)\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2021-02-17 15:50:22,922 DEBUG: Analytics is enabled.\n2021-02-17 15:50:22,989 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp8z6k9p1y']'\n2021-02-17 15:50:22,991 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp8z6k9p1y']'\n<\/code><\/pre>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-17T15:55:13.360Z",
                "Answer_body":"<p>Thanks!<\/p>\n<p>Ok, so your whole dvc repository is on a read-only filesystem. That won\u2019t work, as dvc needs to move files around and that is not permitted by the read-only filesystem. You need to move your dvc project to a normal filesystem and then copy the data you want to add into it and then <code>dvc add<\/code> it there.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-17T15:58:37.081Z",
                "Answer_body":"<p>thank you I will try<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"read mode add dataset dataset folder read mode dont want activ write mode add track dataset",
        "Question_preprocessed_content":"read mode add dataset dataset folder read mode dont want activ write mode add track dataset",
        "Question_gpt_summary_original":"The user's dataset folder is in read-only mode and they are seeking advice on how to add and track their dataset without activating write mode.",
        "Question_gpt_summary":"user dataset folder read mode seek advic add track dataset activ write mode",
        "Answer_original_content":"merv get specif error ad gener add data need file directori dont need copi add simplic add mydatasetpath error error unexpect error errno read file mydatasetpath abc jpg log add mydatasetopath doctor output doctor output version pip platform python linux gener ubuntu bionic support http http cach type hardlink symlink cach directori overlai overlai cach local remot workspac directori overlai overlai repo scm add mydatasetopath output debug analyt enabl debug try spawn daemon analyt tmp tmptorfb debug spawn daemon analyt tmp tmptorfb thank unfortun add mydatasetopath log doesnt look right try add mydatasetopath log log post content log log debug check updat enabl debug fetch debug path jup dataset train plate locat version jenopt inod debug fetch caffdeebffa edcdafbaf dir debug assum jup cach dcdafbaf dir unchang read debug dataset train plate locat version jenopt modifi debug path jup dataset train plate locat version jenopt inod debug fetch caffdeebffa edcdafbaf dir debug assum jup cach dcdafbaf dir unchang read debug comput stage dataset train plate locat version jenopt debug save dataset train plate locat version jenopt cach dcdafbaf dir debug assum jup cach dcdafbaf dir unchang read debug cach jup cach cccffabefdd expect hashinfo valu fcccffabefdd dir info size nfile actual debug fetch error unexpect error errno read file jup dataset train plate locat version jenopt imag plate loc jenopt blj dddc jpg traceback recent file usr lib python shutil line renam src real dst oserror errno invalid cross devic link jup dataset train plate locat version jenopt imag plate loc jenopt blj dddc jpg jup cach cccffabefdd ivdgzadmueyyeyqz handl except except occur traceback recent file usr local lib python dist packag main line main ret cmd run file usr local lib python dist packag command add line run desc self arg desc file usr local lib python dist packag repo init line wrapper return repo arg kwarg file usr local lib python dist packag repo scm context line run result method repo arg file usr local lib python dist packag repo add line add process stage repo stage commit pbar file usr local lib python dist packag repo add line process stage stage commit file usr local lib python dist packag funci decor line wrapper return deco darg dkwarg file usr local lib python dist packag stage decor line rwlock return file usr local lib python dist packag funci decor line return self func self arg self kwarg file usr local lib python dist packag stage init line commit commit file usr local lib python dist packag output base line commit self cach save self path info self cach tree self hash info file usr local lib python dist packag funci decor line wrapper return deco darg dkwarg file usr local lib python dist packag cach base line us state return file usr local lib python dist packag funci decor line return self func self arg self kwarg file usr local lib python dist packag cach base line save self save path info tree hash info save link kwarg file usr local lib python dist packag cach base line save self save dir path info tree hash info save link kwarg file usr local lib python dist packag cach base line save dir entri info tree entri hash save link fals kwarg file usr local lib python dist packag cach base line save file self tree path info cach info mode self cach mode file usr local lib python dist packag tree local line info info mode mode file usr local lib python dist packag util line shutil src tmp file usr lib python shutil line unlink src oserror errno read file jup dataset train plate locat version jenopt imag plate loc jenopt blj dddc jpg debug version info develop version pip platform python linux gener ubuntu bionic support http http cach type hardlink symlink cach directori overlai overlai cach local remot workspac directori overlai overlai repo scm have troubl hit http org support happi help debug analyt enabl debug try spawn daemon analyt tmp tmpzkpy debug spawn daemon analyt tmp tmpzkpy thank repositori read filesystem wont work need file permit read filesystem need project normal filesystem copi data want add add thank try",
        "Answer_preprocessed_content":"get specif error ing gener add data need dont need copi simplic add mydatasetpath error error unexpect error file log doctor output version platform python support http http cach type hardlink symlink cach directori overlai overlai cach local remot workspac directori overlai overlai repo add mydatasetopath output debug analyt enabl debug try spawn debug spawn thank unfortun log doesnt look right try post content thank repositori filesystem wont work need file permit filesystem need project normal filesystem copi data want add thank try",
        "Answer_gpt_summary_original":"Solution: The user's whole dvc repository is on a read-only filesystem, which won't work as dvc needs to move files around. The user needs to move their dvc project to a normal filesystem and then copy the data they want to add into it and then use the dvc add command. No other solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut user repositori read filesystem won work need file user need project normal filesystem copi data want add us add command solut provid discuss"
    },
    {
        "Question_title":"How to create a new VertexAI Workbench Managed Notebook using gcloud",
        "Question_body":"<p>There is a gcloud command to create a user-managed notebook instance.<\/p>\n<pre><code>gcloud notebooks instances create \n<\/code><\/pre>\n<p>Is is possible to create a managed notebook with gcloud?<\/p>\n<p>It looks to be possible in the <a href=\"https:\/\/stackoverflow.com\/questions\/70223161\/how-to-create-a-new-workbench-managed-notebook-using-rest-api\">API<\/a>. I can't find a gcloud reference.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1659584245230,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":107.0,
        "Answer_body":"<p>As mentioned by @gogasca, the gcloud SDK for creating managed notebook is currently under work. Meanwhile you can try client libraries and REST API to create the same.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73230096",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1660204307547,
        "Question_original_content":"creat new vertexai workbench manag notebook gcloud gcloud command creat user manag notebook instanc gcloud notebook instanc creat possibl creat manag notebook gcloud look possibl api gcloud refer",
        "Question_preprocessed_content":"creat new vertexai workbench manag notebook gcloud gcloud command creat notebook instanc possibl creat manag notebook gcloud look possibl api gcloud refer",
        "Question_gpt_summary_original":"The user is facing a challenge in creating a managed notebook instance using gcloud command as there is no reference available for it. They are exploring the possibility of creating it using the API.",
        "Question_gpt_summary":"user face challeng creat manag notebook instanc gcloud command refer avail explor possibl creat api",
        "Answer_original_content":"mention gogasca gcloud sdk creat manag notebook current work try client librari rest api creat",
        "Answer_preprocessed_content":"mention gcloud sdk creat manag notebook current work try client librari rest api creat",
        "Answer_gpt_summary_original":"Solution: The gcloud SDK for creating a managed notebook instance is currently under development. In the meantime, the user can try using client libraries or the REST API to create the instance.",
        "Answer_gpt_summary":"solut gcloud sdk creat manag notebook instanc current develop meantim user try client librari rest api creat instanc"
    },
    {
        "Question_title":"How to get model references logged by a specific run?",
        "Question_body":"We are trying to save a model using log_model_ref and add a name to it, i.e. best_auc. Then we want to be able to retrieve this model from the latest run.\nHowever, if we use RunClient.client.runs_v1.get_runs_artifacts_lineage this returns all the artifacts ever generated for that project. And if we use RunClient.get_artifacts_tree, we do have more control about which run we are looking at, but we lose the name information we set when using log_model_ref?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649410139000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"To get the logged model refs:\n\nfrom polyaxon.client import RunClient\n\nrun_client = RunClient(project=\"PROJECT_NAME\", run_uuid=\"RUN_UUID\")\n\n# Query the lineage information\nlineages = run_client.get_artifacts_lineage(query=\"kind: model\").results\n\n# Download the lineage assets\nfor lineage in lineages:\n    run_client.download_artifact_for_lineage(lineage=lineage)\n\nYou can restrict the ref to specific lineage by filtering further by name:\n\nlineages = run_client.get_artifacts_lineage(query=\"kind: model, name: best_auc\").results",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1485",
        "Tool":"Polyaxon",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-08T09:30:38Z",
                "Answer_score":1,
                "Answer_body":"To get the logged model refs:\n\nfrom polyaxon.client import RunClient\n\nrun_client = RunClient(project=\"PROJECT_NAME\", run_uuid=\"RUN_UUID\")\n\n# Query the lineage information\nlineages = run_client.get_artifacts_lineage(query=\"kind: model\").results\n\n# Download the lineage assets\nfor lineage in lineages:\n    run_client.download_artifact_for_lineage(lineage=lineage)\n\nYou can restrict the ref to specific lineage by filtering further by name:\n\nlineages = run_client.get_artifacts_lineage(query=\"kind: model, name: best_auc\").results"
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"model refer log specif run try save model log model ref add best auc want abl retriev model latest run us runclient client run run artifact lineag return artifact gener project us runclient artifact tree control run look lose inform set log model ref",
        "Question_preprocessed_content":"model refer log specif run try save model add want abl retriev model latest run us return artifact gener project us control run look lose inform set",
        "Question_gpt_summary_original":"The user is facing challenges in retrieving a specific model reference named \"best_auc\" that was saved using log_model_ref during a particular run. The methods they have tried, such as using RunClient.client.runs_v1.get_runs_artifacts_lineage and RunClient.get_artifacts_tree, have not been successful in providing the desired results. They are unable to retrieve the model reference from the latest run and have lost the name information set during log_model_ref.",
        "Question_gpt_summary":"user face challeng retriev specif model refer name best auc save log model ref particular run method tri runclient client run run artifact lineag runclient artifact tree success provid desir result unabl retriev model refer latest run lost inform set log model ref",
        "Answer_original_content":"log model ref client import runclient run client runclient project project run uuid run uuid queri lineag inform lineag run client artifact lineag queri kind model result download lineag asset lineag lineag run client download artifact lineag lineag lineag restrict ref specif lineag filter lineag run client artifact lineag queri kind model best auc result",
        "Answer_preprocessed_content":"log model ref client import runclient queri lineag inform lineag download lineag asset lineag lineag restrict ref specif lineag filter lineag model",
        "Answer_gpt_summary_original":"Solution: The user can retrieve the logged model reference named \"best_auc\" by using the Polyaxon client's RunClient module. They can query the lineage information using the get_artifacts_lineage method and filter it further by name to restrict the reference to a specific lineage. Once the desired lineage is obtained, the user can download the lineage assets using the download_artifact_for_lineage method.",
        "Answer_gpt_summary":"solut user retriev log model refer name best auc client runclient modul queri lineag inform artifact lineag method filter restrict refer specif lineag desir lineag obtain user download lineag asset download artifact lineag method"
    },
    {
        "Question_title":"Voice\/language options during conversion of long text files to speech",
        "Question_body":"The voice\/language options during conversion of long text files to speech. Can anyone help with the doc\/sample for the same.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1627934100000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":375.0,
        "Answer_body":"Howdy Ram56.\u00a0 Could you perhaps elaborate on what you are looking for?\u00a0 We'll be delighted to try and assist.\u00a0\u00a0\n\nHere is the home page for the GCP Text To Speech materials with links to docs:\n\nhttps:\/\/cloud.google.com\/text-to-speech\n\nI fully realize that is a fluffy response ... so if you can add a little more detail to the voice\/language query in your question, we'll get back to you ASAP.\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Voice-language-options-during-conversion-of-long-text-files-to\/td-p\/165947\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-03T15:53:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Howdy Ram56.\u00a0 Could you perhaps elaborate on what you are looking for?\u00a0 We'll be delighted to try and assist.\u00a0\u00a0\n\nHere is the home page for the GCP Text To Speech materials with links to docs:\n\nhttps:\/\/cloud.google.com\/text-to-speech\n\nI fully realize that is a fluffy response ... so if you can add a little more detail to the voice\/language query in your question, we'll get back to you ASAP.\n\nView solution in original post"
            },
            {
                "Answer_creation_time":"2021-08-03T15:53:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Howdy Ram56.\u00a0 Could you perhaps elaborate on what you are looking for?\u00a0 We'll be delighted to try and assist.\u00a0\u00a0\n\nHere is the home page for the GCP Text To Speech materials with links to docs:\n\nhttps:\/\/cloud.google.com\/text-to-speech\n\nI fully realize that is a fluffy response ... so if you can add a little more detail to the voice\/language query in your question, we'll get back to you ASAP."
            }
        ],
        "Question_closed_time":1628005980000,
        "Question_original_content":"voic languag option convers long text file speech voic languag option convers long text file speech help doc sampl",
        "Question_preprocessed_content":"option convers long text file speech option convers long text file speech help",
        "Question_gpt_summary_original":"The user is seeking assistance with voice and language options for converting long text files to speech and is requesting a sample or documentation to help with this task.",
        "Question_gpt_summary":"user seek assist voic languag option convert long text file speech request sampl document help task",
        "Answer_original_content":"howdi ram elabor look delight try assist home page gcp text speech materi link doc http cloud googl com text speech fulli realiz fluffi respons add littl voic languag queri question asap view solut origin post",
        "Answer_preprocessed_content":"howdi ram elabor look delight try assist home page gcp text speech materi link doc fulli realiz fluffi respons add littl queri question asap view solut origin post",
        "Answer_gpt_summary_original":"Solution: The discussion provides a link to the home page for the GCP Text To Speech materials with links to documentation. No other solutions are mentioned.",
        "Answer_gpt_summary":"solut discuss provid link home page gcp text speech materi link document solut mention"
    },
    {
        "Question_title":"Contextual version conflict error, Microsoft Azure Machine Learning Studio",
        "Question_body":"<p>I'm trying to run this 10 line .ipynb file from Google Colab in Microsoft Azure Machine Learning Studio<\/p>\n<p><a href=\"https:\/\/colab.research.google.com\/drive\/1o_-QIR8yVphfnbNZGYemyEr111CHHxSv?usp=sharing\" rel=\"nofollow noreferrer\">https:\/\/colab.research.google.com\/drive\/1o_-QIR8yVphfnbNZGYemyEr111CHHxSv?usp=sharing<\/a><\/p>\n<p>When I get to this step:<\/p>\n<pre><code>import gradio as gr\nimport tensorflow as tf\nfrom transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n<\/code><\/pre>\n<p>I get this error:<\/p>\n<pre><code>ContextualVersionConflict: (Flask 1.0.3 (\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages), Requirement.parse('Flask&gt;=1.1.1'), {'gradio'})\n<\/code><\/pre>\n<p>I tried to install the Flask 1.1.1 version but I get more errors. Any idea what I should do to get past this step in Azure ML Studio?<\/p>\n<pre><code>!pip install \u2013force-reinstall Flask==1.1.1\n\/\/ More errors\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630103458310,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1630103807136,
        "Question_score":2.0,
        "Question_view_count":237.0,
        "Answer_body":"<p>The issue is because <code>gradio<\/code> package using existing Flask package (version 1.0.3). But as your application required Flask&gt;=1.1.1, therefore it is showing error. You need to uninstall the existing Flask package and then install the latest required version.<\/p>\n<p>To uninstall the existing package:\n<code>!pip uninstall Flask -y<\/code><\/p>\n<p>To install latest package:\n<code>!pip install Flask&gt;=1.1.1<\/code><\/p>\n<p><strong>Then, make sure to restart your runtime to pick up the new Flask using the Runtime -&gt; Restart runtime menu.<\/strong><\/p>\n<p>Finally, import gradio.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68959934",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1630303052768,
        "Question_original_content":"contextu version conflict error studio try run line ipynb file googl colab studio http colab research googl com drive qiryvphfnbnzgyemyerchhxsv usp share step import gradio import tensorflow transform import tfgptlmheadmodel gpttoken error contextualversionconflict flask anaconda env lib python site packag requir pars flask gradio tri instal flask version error idea past step studio pip instal forc reinstal flask error",
        "Question_preprocessed_content":"contextu version conflict error studio try run line ipynb file googl colab studio step error tri instal flask version error idea past step studio",
        "Question_gpt_summary_original":"The user is encountering a ContextualVersionConflict error while trying to run a 10 line .ipynb file from Google Colab in Microsoft Azure Machine Learning Studio. The error occurs when importing gradio, tensorflow, and transformers. The user attempted to install Flask 1.1.1 version but encountered more errors. The user is seeking advice on how to resolve the issue and proceed with the step in Azure ML Studio.",
        "Question_gpt_summary":"user encount contextualversionconflict error try run line ipynb file googl colab studio error occur import gradio tensorflow transform user attempt instal flask version encount error user seek advic resolv issu proce step studio",
        "Answer_original_content":"issu gradio packag exist flask packag version applic requir flask show error need uninstal exist flask packag instal latest requir version uninstal exist packag pip uninstal flask instal latest packag pip instal flask sure restart runtim pick new flask runtim restart runtim menu final import gradio",
        "Answer_preprocessed_content":"issu packag exist flask packag applic requir show error need uninstal exist flask packag instal latest requir version uninstal exist packag instal latest packag sure restart runtim pick new flask runtim restart runtim menu final import gradio",
        "Answer_gpt_summary_original":"Solution:\n- Uninstall the existing Flask package using \"!pip uninstall Flask -y\" command.\n- Install the latest required version of Flask using \"!pip install Flask>=1.1.1\" command.\n- Restart the runtime to pick up the new Flask using the Runtime -> Restart runtime menu.\n- Import gradio.",
        "Answer_gpt_summary":"solut uninstal exist flask packag pip uninstal flask command instal latest requir version flask pip instal flask command restart runtim pick new flask runtim restart runtim menu import gradio"
    },
    {
        "Question_title":"Does DVC duplicate dataset in storage blobs when the same dataset is used in different tags?",
        "Question_body":"<p>Hi,<\/p>\n<p>Suppose I have a dataset of 5 GB (images and jsons). I version this dataset using DVC and give a tag name dataset_1. I push the dataset to S3.<\/p>\n<p>Now I add a new dataset of 7GB (images and jsons) to the old dataset, take a tag out called dataset_2 and push to the same S3 bucket. So dataset_2 tag contains the old 5GB and the new 7GB data.<\/p>\n<p>How much storage space is being taken by S3. Will it be 12GB (5 +7) or 15 GB (5 + (5 + 7))?<br>\nIn other words, is the older dataset being duplicated in S3?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635503758715,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":194.0,
        "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/subhankar\">@subhankar<\/a>,<br>\nThe answer to your question depends on how are your datasets stored. DVC asserts granularity on a file level - which means that if given files shows up in different datasets, it will still be stored as single file in the remote. You can read a bit on our cache structure here: <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory\">https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory<\/a><\/p>\n<p>So if your datasets are directories containing files, your data will take (5+7) GB (assuming that none of the files repeat between those two versions - if they do it\u2019s even better, because DVC will know to store only one copy.<\/p>\n<p>If your dataset is some kind of archive (for example a <code>.tar<\/code> file) DVC will store each file separately, which means that your remote will have 5 + (5+7) GB.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/does-dvc-duplicate-dataset-in-storage-blobs-when-the-same-dataset-is-used-in-different-tags\/938",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-29T10:55:51.053Z",
                "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/subhankar\">@subhankar<\/a>,<br>\nThe answer to your question depends on how are your datasets stored. DVC asserts granularity on a file level - which means that if given files shows up in different datasets, it will still be stored as single file in the remote. You can read a bit on our cache structure here: <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory\">https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory<\/a><\/p>\n<p>So if your datasets are directories containing files, your data will take (5+7) GB (assuming that none of the files repeat between those two versions - if they do it\u2019s even better, because DVC will know to store only one copy.<\/p>\n<p>If your dataset is some kind of archive (for example a <code>.tar<\/code> file) DVC will store each file separately, which means that your remote will have 5 + (5+7) GB.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"duplic dataset storag blob dataset differ tag suppos dataset imag json version dataset tag dataset push dataset add new dataset imag json old dataset tag call dataset push bucket dataset tag contain old new data storag space taken word older dataset duplic",
        "Question_preprocessed_content":"duplic dataset storag blob dataset differ tag suppos dataset version dataset tag push dataset add new dataset old dataset tag call push bucket tag contain old new data storag space taken word older dataset duplic",
        "Question_gpt_summary_original":"The user is questioning whether DVC duplicates datasets in storage blobs when the same dataset is used in different tags. They have added a new dataset to an old one and pushed it to S3, and are unsure if the older dataset is being duplicated in S3, resulting in a total storage space of either 12GB or 15GB.",
        "Question_gpt_summary":"user question duplic dataset storag blob dataset differ tag ad new dataset old push unsur older dataset duplic result total storag space",
        "Answer_original_content":"hello subhankar answer question depend dataset store assert granular file level mean given file show differ dataset store singl file remot read bit cach structur http org doc user guid project structur intern file structur cach directori dataset directori contain file data assum file repeat version better know store copi dataset kind archiv exampl tar file store file separ mean remot",
        "Answer_preprocessed_content":"hello answer question depend dataset store assert granular file level mean given file show differ dataset store singl file remot read bit cach structur dataset directori contain file data store file separ mean remot",
        "Answer_gpt_summary_original":"Solution: The answer to the user's question depends on how their datasets are stored. DVC asserts granularity on a file level, which means that if given files show up in different datasets, it will still be stored as a single file in the remote. If the datasets are directories containing files, the data will take 5+7 GB (assuming that none of the files repeat between those two versions). If the dataset is some kind of archive (for example a .tar file), DVC will store each file separately, which means that the remote will have 5 + (5+7) GB.",
        "Answer_gpt_summary":"solut answer user question depend dataset store assert granular file level mean given file differ dataset store singl file remot dataset directori contain file data assum file repeat version dataset kind archiv exampl tar file store file separ mean remot"
    },
    {
        "Question_title":"Azure ML Tabular Dataset : missing 1 required positional argument: 'stream_column'",
        "Question_body":"<p>For the Python API for tabular dataset of AzureML (<code>azureml.data.TabularDataset<\/code>), there are two experimental methods which have been introduced:<\/p>\n<ol>\n<li><code>download(stream_column, target_path=None, overwrite=False, ignore_not_found=True)<\/code><\/li>\n<li><code>mount(stream_column, mount_point=None)<\/code><\/li>\n<\/ol>\n<p>Parameter <code>stream_column<\/code> has been defined as The stream column to mount or download.<\/p>\n<p>What is the actual meaning of <code>stream_column<\/code>? I don't see any example any where?<\/p>\n<p>Any pointer will be helpful.<\/p>\n<p>The stack trace:<\/p>\n<pre><code>Method download: This is an experimental method, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n\/tmp\/ipykernel_11561\/3904436543.py in &lt;module&gt;\n----&gt; 1 tab_dataset.download(target_path=&quot;..\/data\/tabular&quot;)\n\n\/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/_base_sdk_common\/_docstring_wrapper.py in wrapped(*args, **kwargs)\n     50     def wrapped(*args, **kwargs):\n     51         module_logger.warning(&quot;Method {0}: {1} {2}&quot;.format(func.__name__, _method_msg, _experimental_link_msg))\n---&gt; 52         return func(*args, **kwargs)\n     53     return wrapped\n     54 \n\n\/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/azureml\/data\/_loggerfactory.py in wrapper(*args, **kwargs)\n    130             with _LoggerFactory.track_activity(logger, func.__name__, activity_type, custom_dimensions) as al:\n    131                 try:\n--&gt; 132                     return func(*args, **kwargs)\n    133                 except Exception as e:\n    134                     if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):\n\nTypeError: download() missing 1 required positional argument: 'stream_column'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1644217302490,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1645197572643,
        "Question_score":0.0,
        "Question_view_count":356.0,
        "Answer_body":"<p><strong>Update on 5th March, 2022<\/strong><\/p>\n<p>I posted this as a support ticket with Azure. Following is the answer I have received:<\/p>\n<blockquote>\n<p>As you can see from our documentation of <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">TabularDataset Class<\/a>,\nthe \u201cstream_column\u201d parameter is required. So, that error is occurring\nbecause you are not passing any parameters when you are calling the\ndownload method.    The \u201cstream_column\u201d parameter should have the\nstream column to download\/mount. So, you need to pass the column name\nthat contains the paths from which the data will be streamed.<br \/>\nPlease find an example <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-labeled-dataset#explore-labeled-datasets-via-pandas-dataframe\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1648643627872,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71014584",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1646484376340,
        "Question_original_content":"tabular dataset miss requir posit argument stream column python api tabular dataset data tabulardataset experiment method introduc download stream column target path overwrit fals ignor true mount stream column mount point paramet stream column defin stream column mount download actual mean stream column exampl pointer help stack trace method download experiment method chang time http aka experiment inform typeerror traceback recent tmp ipykernel tab dataset download target path data tabular anaconda env lib python site packag base sdk common docstr wrapper wrap arg kwarg def wrap arg kwarg modul logger warn method format func method msg experiment link msg return func arg kwarg return wrap anaconda env lib python site packag data loggerfactori wrapper arg kwarg loggerfactori track activ logger func activ type custom dimens try return func arg kwarg except hasattr activ info hasattr error code typeerror download miss requir posit argument stream column",
        "Question_preprocessed_content":"tabular dataset miss requir posit argument python api tabular dataset experiment method introduc paramet defin stream column mount download actual mean exampl pointer help stack trace",
        "Question_gpt_summary_original":"The user is facing a challenge with the Python API for tabular dataset of AzureML, specifically with the experimental methods 'download' and 'mount'. The user is unsure about the meaning of the parameter 'stream_column' and is seeking clarification. The user has encountered an error message stating that the 'download' method is missing one required positional argument: 'stream_column'.",
        "Question_gpt_summary":"user face challeng python api tabular dataset specif experiment method download mount user unsur mean paramet stream column seek clarif user encount error messag state download method miss requir posit argument stream column",
        "Answer_original_content":"updat march post support ticket azur follow answer receiv document tabulardataset class stream column paramet requir error occur pass paramet call download method stream column paramet stream column download mount need pass column contain path data stream exampl",
        "Answer_preprocessed_content":"updat march post support ticket azur follow answer receiv document tabulardataset class paramet requir error occur pass paramet call download method paramet stream column need pass column contain path data stream exampl",
        "Answer_gpt_summary_original":"Solution: The user needs to pass the 'stream_column' parameter with the column name that contains the paths from which the data will be streamed while calling the 'download' method. The 'stream_column' parameter is required as per the documentation of the TabularDataset Class. An example is provided in the documentation for further clarification.",
        "Answer_gpt_summary":"solut user need pass stream column paramet column contain path data stream call download method stream column paramet requir document tabulardataset class exampl provid document clarif"
    },
    {
        "Question_title":"Vertex AI Endpoints scales to 0 before increasing number of replicas",
        "Question_body":"<p>I have an endpoint in <code>us-east<\/code> which serves a custom imported model (docker image).<\/p>\n<p>This endpoint uses <code>min replicas = 1<\/code> and <code>max replicas = 100<\/code>.<\/p>\n<p>Sometimes, Vertex AI will require the model to scale from 1 to 2.<\/p>\n<p>However, there seems to be an issue causing the number of replicas to go from <code>1 -&gt; 0 -&gt; 2<\/code> instead of <code>1 -&gt; 2<\/code>.<\/p>\n<p>This causes several 504 (Gateway Timeout) errors in my API and the way to solve that was setting <code>min replicas &gt; 1<\/code>, highly impacting the monthly cost of the application.<\/p>\n<p>Is this some known issue to Vertex AI\/GCP services, is there anyway to fix it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640176354140,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":77.0,
        "Answer_body":"<p>The intermittent <code>504<\/code> errors could be a result of an endpoint that is under-provisioned to handle the load. It can also happen if too many prediction requests are sent to the endpoint before it has a chance to scale up.<\/p>\n<p>Traffic splitting of the incoming prediction requests is done randomly. So, multiple requests may end up on the same model server at the same time. This can happen even if the overall Queries Per Second (QPS) is low, and especially when the QPS is spiky. This contributes to the requests being queued up if the model server isn't able to handle the load. This is what results in a 504 error.<\/p>\n<p><strong>Recommendations to mitigate the <code>504<\/code> errors are as follows:<\/strong><\/p>\n<ul>\n<li><p>Improve the container's ability to use all resources in the container. One thing to keep in mind about resource utilization is whether the model server is single-threaded or multi-threaded. The container may not be using up all the cores and\/or requests may be queuing up, hence are served only one-at-a-time.<\/p>\n<\/li>\n<li><p>Autoscaling is happening, it just might need to be tuned to the prediction workload and expectations. A lower utilization threshold would trigger autoscaling sooner.<\/p>\n<\/li>\n<li><p>Perform an exponential backoff while the deployment is scaling. This way, there is a retry mechanism to handle failed requests.<\/p>\n<\/li>\n<li><p>Provision a higher minimum replica count for the endpoint, which you have already implemented.<\/p>\n<\/li>\n<\/ul>\n<p>If the above recommendations do not solve the problem or in general require further investigation of these errors, please reach out to <a href=\"https:\/\/cloud.google.com\/support-hub#section-2\" rel=\"nofollow noreferrer\">GCP support<\/a> in case you have a <a href=\"https:\/\/cloud.google.com\/support\/\" rel=\"nofollow noreferrer\">support plan<\/a>. Otherwise, please open an issue in the <a href=\"https:\/\/issuetracker.google.com\/issues\/new?component=1134259&amp;template=1640573\" rel=\"nofollow noreferrer\">issue tracker<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1641204696607,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70449117",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1641201707430,
        "Question_original_content":"endpoint scale increas number replica endpoint east serv custom import model docker imag endpoint us min replica max replica requir model scale issu caus number replica instead caus gatewai timeout error api wai solv set min replica highli impact monthli cost applic known issu gcp servic fix",
        "Question_preprocessed_content":"endpoint scale increas number replica endpoint serv custom import model endpoint us requir model scale issu caus number replica instead caus error api wai solv set highli impact monthli cost applic known issu servic fix",
        "Question_gpt_summary_original":"The user is encountering challenges with Vertex AI Endpoints scaling from 1 to 2 replicas. Instead of going from 1 to 2, the number of replicas goes from 1 to 0 to 2, causing 504 errors in the API. The user had to set the minimum replicas to a value greater than 1 to solve the issue, which increased the monthly cost of the application. The user is seeking a solution to this issue and wondering if it is a known issue with Vertex AI\/GCP services.",
        "Question_gpt_summary":"user encount challeng endpoint scale replica instead go number replica goe caus error api user set minimum replica valu greater solv issu increas monthli cost applic user seek solut issu wonder known issu gcp servic",
        "Answer_original_content":"intermitt error result endpoint provis handl load happen predict request sent endpoint chanc scale traffic split incom predict request randomli multipl request end model server time happen overal queri second qp low especi qp spiki contribut request queu model server isn abl handl load result error recommend mitig error follow improv contain abil us resourc contain thing mind resourc util model server singl thread multi thread contain core request queu serv time autosc happen need tune predict workload expect lower util threshold trigger autosc sooner perform exponenti backoff deploy scale wai retri mechan handl fail request provis higher minimum replica count endpoint implement recommend solv problem gener requir investig error reach gcp support case support plan open issu issu tracker",
        "Answer_preprocessed_content":"intermitt error result endpoint handl load happen predict request sent endpoint chanc scale traffic split incom predict request randomli multipl request end model server time happen overal queri second low especi qp spiki contribut request queu model server isn abl handl load result error recommend mitig error follow improv contain abil us resourc contain thing mind resourc util model server contain core request queu serv autosc happen need tune predict workload expect lower util threshold trigger autosc sooner perform exponenti backoff deploy scale wai retri mechan handl fail request provis higher minimum replica count endpoint implement recommend solv problem gener requir investig error reach gcp support case support plan open issu issu tracker",
        "Answer_gpt_summary_original":"The discussion provides several recommendations to mitigate the 504 errors encountered when scaling Vertex AI Endpoints, including improving container resource utilization, tuning autoscaling, performing exponential backoff, and provisioning a higher minimum replica count. It is suggested to reach out to GCP support or open an issue in the issue tracker if these recommendations do not solve the problem.",
        "Answer_gpt_summary":"discuss provid recommend mitig error encount scale endpoint includ improv contain resourc util tune autosc perform exponenti backoff provis higher minimum replica count suggest reach gcp support open issu issu tracker recommend solv problem"
    },
    {
        "Question_title":"Dvc metrics diff on metrics stored in dvc-cache?",
        "Question_body":"<p>Hello!<\/p>\n<p>Does the <code>dvc metrics diff ref_a ref_b<\/code> command work on metrics which are tracked <em>not<\/em> by git, but instead by dvc?<\/p>\n<p>Our current setup is the following:<\/p>\n<ul>\n<li>We have a dvc-pipeline-stage, which produces a <code>eval.json<\/code>, which is cached by dvc<br>\n<code> dvc run -d dep1 -d dep2 -m eval.json  -f data\/evaluation.dvc python my_script.py<\/code>\n<ul>\n<li>According to the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run#options\" rel=\"nofollow noopener\">docs<\/a>, the lowercase <code>-m<\/code> puts <code>eval.json<\/code> on the gitignore and into the dvc cache<\/li>\n<\/ul>\n<\/li>\n<li>Our model is trained remotely via a CI-Pipeline, which after a successful run executes a <code>git add . &amp;&amp; git commit &amp;&amp; git push &amp;&amp; dvc push<\/code>\n<\/li>\n<li>On our local machines then we execute a <code>git pull &amp;&amp; dvc pull<\/code>. We now have the trained model and our <code>eval.json<\/code> locally available<\/li>\n<li>When we now execute <code>dvc metrics diff some_other_commit_hash <\/code> the command does not find the <code>eval.json<\/code> for the <code>some_other_commit_hash<\/code>:<pre><code class=\"lang-auto\">$ dvc metrics diff 5f036012bc9d35b7240eab3b7a42792093612ba8 \nWARNING: Metrics file 'data\/evaluation\/eval.json' does not exist at the reference '5f036012bc9d35b7240eab3b7a42792093612ba8'.\n        Path               Metric          Value                Change      \ndvc_pipeline\/data\/evaluation   f1       0.11549247896660152   diff not supported\n\/eval.json                                                                      \ndvc_pipeline\/data\/evaluation   acc      0.6784822940826416    diff not supported\n\/eval.json                                                                      \ndvc_pipeline\/data\/evaluation   loss     1.1063932177428895    diff not supported\n\/eval.json                                                                      \ndata\/evaluation\/eval.json      f1       0.43913782532829426   diff not supported\ndata\/evaluation\/eval.json      acc      0.750374436378479     diff not supported\ndata\/evaluation\/eval.json      loss     0.6777220140143865    diff not supported\n<\/code><\/pre>\n<\/li>\n<li>I\u2019m assuming this command fails, since (I\u2019m guessing) dvc looks for the <code>eval.json<\/code> in the git-cache, but not in the dvc-cache (where it is stored)<\/li>\n<\/ul>\n<p>Is there a flag, to tell <code>dvc metrics diff ...<\/code> that the metrics-file is stored in the dvc cache? Or is this expected to work, and we have an error somewhere else?<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1589548601677,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":754.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/rabefabi\">@rabefabi<\/a>,<\/p>\n<blockquote>\n<p>I\u2019m assuming this command fails, since (I\u2019m guessing) dvc looks for the eval.json in the git-cache, but not in the dvc-cache (where it is stored)<\/p>\n<\/blockquote>\n<p>No, DVC should not be looking in Git for this metric (if it does, it would be a bug). It should just use the command hash as a reference to find the corresponding metric file in the DVC cache. But I think probably you\u2019re just not pulling all the historic metrics into your local DVC cache \u2014 you could explore .dvc\/cache to try and figure this out BTW but it\u2019s kind of tricky knowing what the metrics file name should be.<\/p>\n<blockquote>\n<ul>\n<li>On our local machines then we execute a  <code>git pull &amp;&amp; dvc pull<\/code> . We now have the trained model and our  <code>eval.json<\/code>  locally available<\/li>\n<\/ul>\n<\/blockquote>\n<p>Try <code>dvc pull --all-commits<\/code> here instead. More details in <a href=\"https:\/\/dvc.org\/doc\/command-reference\/pull#options\">https:\/\/dvc.org\/doc\/command-reference\/pull#options<\/a><\/p>\n<p>Please let me know if that helped and we\u2019ll take it from there.<\/p>. <p>Thank you, you\u2019re right and I did not pull the complete history, which is why the command failed.<\/p>\n<p>For anyone else running into this issue:<br>\nI didn\u2019t actually need my complete history since I switched to working with git tags.<br>\nTo pull the data for all my tags I did the following:<\/p>\n<pre><code class=\"lang-auto\">$ dvc pull -T\n$ dvc metrics diff my_first_tag my_second_tag\n<\/code><\/pre>\n<p>This ensures that all necessary data is pulled and can be found by the <code>dvc metrics diff<\/code> command.<\/p>\n<p>Thanks again for the swift response, this forum helps a lot to find my way into dvc <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-metrics-diff-on-metrics-stored-in-dvc-cache\/386",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-05-15T18:36:19.775Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/rabefabi\">@rabefabi<\/a>,<\/p>\n<blockquote>\n<p>I\u2019m assuming this command fails, since (I\u2019m guessing) dvc looks for the eval.json in the git-cache, but not in the dvc-cache (where it is stored)<\/p>\n<\/blockquote>\n<p>No, DVC should not be looking in Git for this metric (if it does, it would be a bug). It should just use the command hash as a reference to find the corresponding metric file in the DVC cache. But I think probably you\u2019re just not pulling all the historic metrics into your local DVC cache \u2014 you could explore .dvc\/cache to try and figure this out BTW but it\u2019s kind of tricky knowing what the metrics file name should be.<\/p>\n<blockquote>\n<ul>\n<li>On our local machines then we execute a  <code>git pull &amp;&amp; dvc pull<\/code> . We now have the trained model and our  <code>eval.json<\/code>  locally available<\/li>\n<\/ul>\n<\/blockquote>\n<p>Try <code>dvc pull --all-commits<\/code> here instead. More details in <a href=\"https:\/\/dvc.org\/doc\/command-reference\/pull#options\">https:\/\/dvc.org\/doc\/command-reference\/pull#options<\/a><\/p>\n<p>Please let me know if that helped and we\u2019ll take it from there.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-05-18T11:27:35.047Z",
                "Answer_body":"<p>Thank you, you\u2019re right and I did not pull the complete history, which is why the command failed.<\/p>\n<p>For anyone else running into this issue:<br>\nI didn\u2019t actually need my complete history since I switched to working with git tags.<br>\nTo pull the data for all my tags I did the following:<\/p>\n<pre><code class=\"lang-auto\">$ dvc pull -T\n$ dvc metrics diff my_first_tag my_second_tag\n<\/code><\/pre>\n<p>This ensures that all necessary data is pulled and can be found by the <code>dvc metrics diff<\/code> command.<\/p>\n<p>Thanks again for the swift response, this forum helps a lot to find my way into dvc <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"metric diff metric store cach hello metric diff ref ref command work metric track git instead current setup follow pipelin stage produc eval json cach run dep dep eval json data evalu python script accord doc lowercas put eval json gitignor cach model train remot pipelin success run execut git add git commit git push push local machin execut git pull pull train model eval json local avail execut metric diff commit hash command eval json commit hash metric diff fbcdbeabbaba warn metric file data evalu eval json exist refer fbcdbeabbaba path metric valu chang pipelin data evalu diff support eval json pipelin data evalu acc diff support eval json pipelin data evalu loss diff support eval json data evalu eval json diff support data evalu eval json acc diff support data evalu eval json loss diff support assum command fail guess look eval json git cach cach store flag tell metric diff metric file store cach expect work error thank advanc",
        "Question_preprocessed_content":"metric diff metric store cach hello command work metric track git instead current setup follow produc cach accord doc lowercas put gitignor cach model train remot success run execut local machin execut train model local avail execut command assum command fail look cach flag tell store cach expect work error thank advanc",
        "Question_gpt_summary_original":"The user is facing a challenge with the \"dvc metrics diff\" command not finding the \"eval.json\" file for a specific commit hash. The file is stored in the dvc-cache, but the command seems to be looking for it in the git-cache. The user is asking if there is a flag to tell the command that the metrics-file is stored in the dvc-cache.",
        "Question_gpt_summary":"user face challeng metric diff command find eval json file specif commit hash file store cach command look git cach user ask flag tell command metric file store cach",
        "Answer_original_content":"rabefabi assum command fail guess look eval json git cach cach store look git metric bug us command hash refer correspond metric file cach think probabl your pull histor metric local cach explor cach try figur btw kind tricki know metric file local machin execut git pull pull train model eval json local avail try pull commit instead detail http org doc command refer pull option let know help thank your right pull complet histori command fail run issu didnt actual need complet histori switch work git tag pull data tag follow pull metric diff tag second tag ensur necessari data pull metric diff command thank swift respons forum help lot wai",
        "Answer_preprocessed_content":"assum command fail look cach look git metric us command hash refer correspond metric file cach think probabl your pull histor metric local cach explor try figur btw kind tricki know metric file local machin execut train model local avail try instead detail let know help thank your right pull complet histori command fail run issu didnt actual need complet histori switch work git tag pull data tag follow ensur necessari data pull command thank swift respons forum help lot wai",
        "Answer_gpt_summary_original":"Solution:\n- The user can try using the command \"dvc pull --all-commits\" instead of \"git pull && dvc pull\" to pull all the historic metrics into the local DVC cache.\n- The user can also try using the command \"dvc pull -T\" to pull the data for all the tags and then use the \"dvc metrics diff\" command to find the necessary data.",
        "Answer_gpt_summary":"solut user try command pull commit instead git pull pull pull histor metric local cach user try command pull pull data tag us metric diff command necessari data"
    },
    {
        "Question_title":"AML Studio: Register mutliple gateways on the same server",
        "Question_body":"<p>I am struggling to find a way to register multiple gateways. I have a local instance of my SQL server and have created a gateway to access to it from the AML Studio workspace. It works fine but now I would like to access to the same SQL server instance from another workspace. So the question is: how to register a new gateway without removing the previous one?\nI followed this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/use-data-from-an-on-premises-sql-server\" rel=\"nofollow noreferrer\">documentation<\/a>.\nDoes the following explanation mean that there is no way to do that?<\/p>\n\n<blockquote>\n  <p>You can create and set up multiple gateways in Studio for each workspace. For example, you may have a gateway that you want to connect to your test data sources during development, and a different gateway for your production data sources. Azure Machine Learning gives you the flexibility to set up multiple gateways depending upon your corporate environment. Currently you can\u2019t share a gateway between workspaces and only one gateway can be installed on a single computer.<\/p>\n<\/blockquote>\n\n<p>It is quite limiting as connecting to the same server from multiple workspaces may be sometimes crucial.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1538466090420,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1538485474347,
        "Question_score":0.0,
        "Question_view_count":40.0,
        "Answer_body":"<p>Well, finally I have found a way to bypass this limitation. From this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/use-data-from-an-on-premises-sql-server\" rel=\"nofollow noreferrer\">documentation<\/a> I have found that: <\/p>\n\n<blockquote>\n  <p>The IR does not need to be on the same machine as the data source. But staying closer to the data source reduces the time for the gateway to connect to the data source. We recommend that you install the IR on a machine that's different from the one that hosts the on-premises data source so that the gateway and data source don't compete for resources.<\/p>\n<\/blockquote>\n\n<p>So the  logic is pretty simple. You provide access to your local server to another machine on vpn and install your gateway there. Important: I have set up the firewall rules on the server before, to be able to establish the connection remotely.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52603929",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1538648158568,
        "Question_original_content":"aml studio regist mutlipl gatewai server struggl wai regist multipl gatewai local instanc sql server creat gatewai access aml studio workspac work fine like access sql server instanc workspac question regist new gatewai remov previou follow document follow explan mean wai creat set multipl gatewai studio workspac exampl gatewai want connect test data sourc develop differ gatewai product data sourc give flexibl set multipl gatewai depend corpor environ current share gatewai workspac gatewai instal singl limit connect server multipl workspac crucial",
        "Question_preprocessed_content":"aml studio regist mutlipl gatewai server struggl wai regist multipl gatewai local instanc sql server creat gatewai access aml studio workspac work fine like access sql server instanc workspac question regist new gatewai remov previou follow document follow explan mean wai creat set multipl gatewai studio workspac exampl gatewai want connect test data sourc develop differ gatewai product data sourc give flexibl set multipl gatewai depend corpor environ current share gatewai workspac gatewai instal singl limit connect server multipl workspac crucial",
        "Question_gpt_summary_original":"The user is facing a challenge in registering multiple gateways on the same server in AML Studio. They have successfully created a gateway to access their local SQL server instance from one workspace but are struggling to access it from another workspace without removing the previous gateway. The documentation suggests that only one gateway can be installed on a single computer and cannot be shared between workspaces, which limits the user's ability to connect to the same server from multiple workspaces.",
        "Question_gpt_summary":"user face challeng regist multipl gatewai server aml studio successfulli creat gatewai access local sql server instanc workspac struggl access workspac remov previou gatewai document suggest gatewai instal singl share workspac limit user abil connect server multipl workspac",
        "Answer_original_content":"final wai bypass limit document need machin data sourc stai closer data sourc reduc time gatewai connect data sourc recommend instal machin differ host premis data sourc gatewai data sourc compet resourc logic pretti simpl provid access local server machin vpn instal gatewai import set firewal rule server abl establish connect remot",
        "Answer_preprocessed_content":"final wai bypass limit document need machin data sourc stai closer data sourc reduc time gatewai connect data sourc recommend instal machin differ host data sourc gatewai data sourc compet resourc logic pretti simpl provid access local server machin vpn instal gatewai import set firewal rule server abl establish connect remot",
        "Answer_gpt_summary_original":"Solution: The user found a way to bypass the limitation of only being able to install one gateway on a single computer by installing the Integration Runtime (IR) on a different machine that has access to the local server through VPN. The documentation suggests that the IR does not need to be on the same machine as the data source, and it is recommended to install it on a different machine to avoid resource competition between the gateway and data source. The user also set up firewall rules on the server to establish the connection remotely.",
        "Answer_gpt_summary":"solut user wai bypass limit abl instal gatewai singl instal integr runtim differ machin access local server vpn document suggest need machin data sourc recommend instal differ machin avoid resourc competit gatewai data sourc user set firewal rule server establish connect remot"
    },
    {
        "Question_title":"How to log artifacts in wandb while using saimpletransformers?",
        "Question_body":"<p>I am creating a Question Answering model using <a href=\"https:\/\/simpletransformers.ai\/docs\/qa-specifics\/\" rel=\"nofollow noreferrer\">simpletransformers<\/a>. I would also like to use wandb to track model artifacts. As I understand from <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/other\/simpletransformers\" rel=\"nofollow noreferrer\">wandb docs<\/a>, there is an integration touchpoint for simpletransformers but there is no mention of logging artifacts.<\/p>\n<p>I would like to log artifacts generated at the train, validation, and test phase such as train.json, eval.json, test.json, output\/nbest_predictions_test.json and best performing model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634705743473,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":53.0,
        "Answer_body":"<p>Currently simpleTransformers doesn't support logging artifacts within the training\/testing scripts. But you can do it manually:<\/p>\n<pre><code>import os \n\nwith wandb.init(id=model.wandb_run_id, resume=&quot;allow&quot;, project=wandb_project) as training_run:\n    for dir in sorted(os.listdir(&quot;outputs&quot;)):\n        if &quot;checkpoint&quot; in dir:\n            artifact = wandb.Artifact(&quot;model-checkpoints&quot;, type=&quot;checkpoints&quot;)\n            artifact.add_dir(&quot;outputs&quot; + &quot;\/&quot; + dir)\n            training_run.log_artifact(artifact)\n<\/code><\/pre>\n<p>For more info, you can follow along with the W&amp;B notebook in the SimpleTransofrmer's README.md<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69640534",
        "Tool":"Weights & Biases",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1634729204572,
        "Question_original_content":"log artifact saimpletransform creat question answer model simpletransform like us track model artifact understand doc integr touchpoint simpletransform mention log artifact like log artifact gener train valid test phase train json eval json test json output nbest predict test json best perform model",
        "Question_preprocessed_content":"log artifact saimpletransform creat question answer model simpletransform like us track model artifact understand doc integr touchpoint simpletransform mention log artifact like log artifact gener train valid test phase best perform model",
        "Question_gpt_summary_original":"The user is facing a challenge in logging artifacts while using simpletransformers and wandb for a Question Answering model. Although there is an integration touchpoint for simpletransformers in wandb, there is no mention of logging artifacts. The user wants to log artifacts generated during the train, validation, and test phases, including train.json, eval.json, test.json, output\/nbest_predictions_test.json, and the best performing model.",
        "Question_gpt_summary":"user face challeng log artifact simpletransform question answer model integr touchpoint simpletransform mention log artifact user want log artifact gener train valid test phase includ train json eval json test json output nbest predict test json best perform model",
        "Answer_original_content":"current simpletransform support log artifact train test script manual import init model run resum allow project project train run dir sort listdir output checkpoint dir artifact artifact model checkpoint type checkpoint artifact add dir output dir train run log artifact artifact info follow notebook simpletransofrm readm",
        "Answer_preprocessed_content":"current simpletransform support log artifact script manual info follow notebook simpletransofrm",
        "Answer_gpt_summary_original":"Solution: Currently, simpleTransformers does not support logging artifacts within the training\/testing scripts. However, the user can manually log artifacts by importing os and using the wandb.Artifact() function to add the directories containing the artifacts. The user can follow the W&B notebook in the SimpleTransformers' README.md for more information.",
        "Answer_gpt_summary":"solut current simpletransform support log artifact train test script user manual log artifact import artifact function add directori contain artifact user follow notebook simpletransform readm inform"
    },
    {
        "Question_title":"AzureML: experiment working for a subset and not for the whole dataset",
        "Question_body":"<p>some times ago I had written a code in AzureML meeting \"out of memory\" issues. So I tried to split the code in three different codes and that partially worked. It remains a part that (I think) is affected by memory issues too.<\/p>\n\n<p>I have created an experiment that I have published in this <a href=\"http:\/\/gallery.cortanaintelligence.com\/Experiment\/TextMining-sample-NA-v1-1\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>\n\n<p>There is a module that considers only a sample of my dataset, and it does work. This means that the code is supposed to work correctly. If you remove the sampling code (the second module starting from the top) <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Cbzhj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Cbzhj.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>and you connect directly the original dataset you have the following situation<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/XOo8e.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XOo8e.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>producing the following error:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/mRSSQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mRSSQ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Does someone have some way to understand where Azure crashes?<\/p>\n\n<p>Thanks you,<\/p>\n\n<p>Andrea<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1472651915230,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":102.0,
        "Answer_body":"<p>Thanks so much for publishing the example -- this really helped to understand the issue. I suspect that you want to modify the <code>gsub()<\/code> calls in your script by adding the argument \"<code>fixed=TRUE<\/code>\" to each. (The documentation for this function is <a href=\"https:\/\/stat.ethz.ch\/R-manual\/R-devel\/library\/base\/html\/grep.html\" rel=\"nofollow\">here<\/a>.)<\/p>\n\n<p>What appears to have happened is that somewhere in your full dataset -- but not in the subsampled dataset -- there is some text that winds up being included in <code>df[i, \"names\"]<\/code> as \"<code>(art.<\/code>\".  Your script pads this into \"<code>\\\\b(art.\\\\b<\/code>\". The <code>gsub()<\/code> function tries to interpret this as a regular expression instead of a simple string, then throws an error because it is not a valid regular expression: it contains an opening parenthesis but no closing parenthesis. I believe that you actually did not want <code>gsub()<\/code> to interpret the input as a regular expression in the first place, and specifying <code>gsub(..., fixed=TRUE)<\/code> will correct that.<\/p>\n\n<p>I believe the reason why this error disappears when you add the sample\/partition module is because, by chance, the problematic input value was dropped on subsampling. I do not think it is an issue of available resources on Azure ML. (Caveat: I cannot confirm the fix works yet; I made the suggested update and started running the experiment, but it has not yet completed successfully.)<\/p>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/39251701",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1472676812496,
        "Question_original_content":"experi work subset dataset time ago written code meet memori issu tri split code differ code partial work remain think affect memori issu creat experi publish link modul consid sampl dataset work mean code suppos work correctli remov sampl code second modul start connect directli origin dataset follow situat produc follow error wai understand azur crash thank andrea",
        "Question_preprocessed_content":"experi work subset dataset time ago written code meet memori issu tri split code differ code partial work remain affect memori issu creat experi publish link modul consid sampl dataset work mean code suppos work correctli remov sampl code connect directli origin dataset follow situat produc follow error wai understand azur crash thank andrea",
        "Question_gpt_summary_original":"The user has encountered memory issues while working with AzureML. They tried to split the code into three different codes, which partially worked, but there is still a part that is affected by memory issues. The experiment created by the user works for a subset of the dataset, but not for the whole dataset, resulting in an error. The user is seeking help to understand where Azure crashes.",
        "Question_gpt_summary":"user encount memori issu work tri split code differ code partial work affect memori issu experi creat user work subset dataset dataset result error user seek help understand azur crash",
        "Answer_original_content":"thank publish exampl help understand issu suspect want modifi gsub call script ad argument fix true document function appear happen dataset subsampl dataset text wind includ name art script pad art gsub function tri interpret regular express instead simpl string throw error valid regular express contain open parenthesi close parenthesi believ actual want gsub interpret input regular express place specifi gsub fix true correct believ reason error disappear add sampl partit modul chanc problemat input valu drop subsampl think issu avail resourc caveat confirm fix work suggest updat start run experi complet successfulli",
        "Answer_preprocessed_content":"thank publish exampl help understand issu suspect want modifi call script ad argument appear happen dataset subsampl dataset text wind includ script pad function tri interpret regular express instead simpl string throw error valid regular express contain open parenthesi close parenthesi believ actual want interpret input regular express place specifi correct believ reason error disappear add modul chanc problemat input valu drop subsampl think issu avail resourc",
        "Answer_gpt_summary_original":"Solution:\n- Modify the `gsub()` calls in the script by adding the argument \"fixed=TRUE\" to each. This will prevent `gsub()` from interpreting the input as a regular expression and correct the error caused by an invalid regular expression.\n- The error disappears when adding the sample\/partition module because the problematic input value was dropped on subsampling, not because of available resources on Azure ML.",
        "Answer_gpt_summary":"solut modifi gsub call script ad argument fix true prevent gsub interpret input regular express correct error caus invalid regular express error disappear ad sampl partit modul problemat input valu drop subsampl avail resourc"
    },
    {
        "Question_title":"Updating pandas to version 0.19 in Azure ML Studio",
        "Question_body":"<p>I would really like to get access to some of the updated functions in pandas 0.19, but Azure ML studio uses pandas 0.18 as part of the Anaconda 4.0 bundle. Is there a way to update the version that is used within the \"Execute Python Script\" components?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1505401641617,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":1903.0,
        "Answer_body":"<p>I offer the below steps for you to show how to update the version of pandas  library in <code>Execute Python Script<\/code>.<\/p>\n\n<p><strong><em>Step 1<\/em><\/strong> : Use the <code>virtualenv<\/code> component to create an independent python runtime environment in your system.Please install it first with command <code>pip install virtualenv<\/code> if you don't have it.<\/p>\n\n<p>If you installed it successfully ,you could see it in your python\/Scripts file.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZFI2t.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZFI2t.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step2<\/em><\/strong> : Run the commad to create independent python runtime environment.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/nzDqz.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/nzDqz.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 3<\/em><\/strong> : Then go into the created directory's Scripts folder and activate it (this step is important , don't miss it)<\/p>\n\n<p>Please don't close this command window and use <code>pip install pandas==0.19<\/code> to download external libraries in this command window.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Wj857.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Wj857.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 4<\/em><\/strong> : Compress all of the files in the Lib\/site-packages folder into a zip package (I'm calling it pandas - package here)<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Ch9Oo.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ch9Oo.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 5<\/em><\/strong> \uff1aUpload the zip package into the Azure Machine Learning WorkSpace DataSet.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/efRkK.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/efRkK.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>specific steps please refer to the <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/cdb56f95-7f4c-404d-bde7-5bb972e6f232\/\" rel=\"noreferrer\">Technical Notes<\/a>.<\/p>\n\n<p>After success, you will see the uploaded package in the DataSet List<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ngGCu.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ngGCu.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 6<\/em><\/strong> \uff1a Before the defination of method <code>azureml_main<\/code> in the Execute Python Script module, you need to remove the old <code>pandas<\/code> modules &amp; its dependencies, then to import <code>pandas<\/code> again, as the code below.<\/p>\n\n<pre><code>import sys\nimport pandas as pd\nprint(pd.__version__)\ndel sys.modules['pandas']\ndel sys.modules['numpy']\ndel sys.modules['pytz']\ndel sys.modules['six']\ndel sys.modules['dateutil']\nsys.path.insert(0, '.\\\\Script Bundle')\nfor td in [m for m in sys.modules if m.startswith('pandas.') or m.startswith('numpy.') or m.startswith('pytz.') or m.startswith('dateutil.') or m.startswith('six.')]:\n    del sys.modules[td]\nimport pandas as pd\nprint(pd.__version__)\n# The entry point function can contain up to two input arguments:\n#   Param&lt;dataframe1&gt;: a pandas.DataFrame\n#   Param&lt;dataframe2&gt;: a pandas.DataFrame\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n<\/code><\/pre>\n\n<p>Then you can see the result from logs as below, first print the old version <code>0.14.0<\/code>, then print the new version <code>0.19.0<\/code> from the uploaded zip file.<\/p>\n\n<pre><code>[Information]         0.14.0\n[Information]         0.19.0\n<\/code><\/pre>\n\n<p>You could also refer to these threads: <a href=\"https:\/\/stackoverflow.com\/questions\/45749479\/access-blob-file-using-time-stamp-in-azure\/45814318#45814318\">Access blob file using time stamp in Azure<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/12669546\/reload-with-reset\">reload with reset<\/a>.<\/p>\n\n<p>Hope it helps you.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1505805193896,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46222606",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1505456513227,
        "Question_original_content":"updat panda version studio like access updat function panda studio us panda anaconda bundl wai updat version execut python script compon",
        "Question_preprocessed_content":"updat panda version studio like access updat function panda studio us panda anaconda bundl wai updat version execut python script compon",
        "Question_gpt_summary_original":"The user is facing a challenge in updating pandas to version 0.19 in Azure ML Studio as it currently uses pandas 0.18 as part of the Anaconda 4.0 bundle. The user is seeking a way to update the version used within the \"Execute Python Script\" components.",
        "Question_gpt_summary":"user face challeng updat panda version studio current us panda anaconda bundl user seek wai updat version execut python script compon",
        "Answer_original_content":"offer step updat version panda librari execut python script step us virtualenv compon creat independ python runtim environ instal command pip instal virtualenv instal successfulli python script file step run commad creat independ python runtim environ step creat directori script folder activ step import miss close command window us pip instal panda download extern librari command window step compress file lib site packag folder zip packag call panda packag step upload zip packag workspac dataset specif step refer technic note success upload packag dataset list step defin method main execut python script modul need remov old panda modul depend import panda code import sy import panda print version del sy modul panda del sy modul numpi del sy modul pytz del sy modul del sy modul dateutil sy path insert script bundl sy modul startswith panda startswith numpi startswith pytz startswith dateutil startswith del sy modul import panda print version entri point function contain input argument param panda datafram param panda datafram def main datafram datafram result log print old version print new version upload zip file inform inform refer thread access blob file time stamp azur reload reset hope help",
        "Answer_preprocessed_content":"offer step updat version panda librari step us compon creat independ python runtim environ instal command instal successfulli file step run commad creat independ python runtim environ step creat directori script folder activ close command window us download extern librari command window step compress file folder zip packag step upload zip packag workspac dataset specif step refer technic note success upload packag dataset list step defin method execut python script modul need remov old modul depend import code result log print old version print new version upload zip file refer thread access blob file time stamp azur reload reset hope help",
        "Answer_gpt_summary_original":"The solution provided involves creating an independent python runtime environment using the virtualenv component, installing pandas 0.19 in this environment, compressing the files in the Lib\/site-packages folder into a zip package, uploading the zip package into the Azure Machine Learning WorkSpace DataSet, removing the old pandas modules and dependencies, and importing pandas again. The logs will show the old version 0.14.0 and the new version 0.19.0 from the uploaded zip file.",
        "Answer_gpt_summary":"solut provid involv creat independ python runtim environ virtualenv compon instal panda environ compress file lib site packag folder zip packag upload zip packag workspac dataset remov old panda modul depend import panda log old version new version upload zip file"
    },
    {
        "Question_title":"Can I make Neptune talk to git?",
        "Question_body":"<p>In <a href=\"https:\/\/neptune.ml\/\" rel=\"nofollow noreferrer\">Neptune<\/a> (this machine learning experiment tracker) is it possible to make it git-aware? I mean - using <code>.gitignore<\/code> for excluded files and saving commit hashes for each run?<\/p>\n\n<p>In particular, when I review an already finished job, can I go directly to GitHub commit?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1495124614783,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1503919276067,
        "Question_score":0.0,
        "Question_view_count":140.0,
        "Answer_body":"<p>Starting form version 2.0 Neptune provides integration with git, see: <a href=\"https:\/\/docs.neptune.ml\/advanced-topics\/git-integration\/\" rel=\"nofollow noreferrer\">https:\/\/docs.neptune.ml\/advanced-topics\/git-integration\/<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44053141",
        "Tool":"Neptune",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1503919410423,
        "Question_original_content":"talk git machin learn experi tracker possibl git awar mean gitignor exclud file save commit hash run particular review finish job directli github commit",
        "Question_preprocessed_content":"talk git possibl mean exclud file save commit hash run particular review finish job directli github commit",
        "Question_gpt_summary_original":"The user is facing challenges in making Neptune, a machine learning experiment tracker, git-aware. They want to use .gitignore for excluded files and save commit hashes for each run. Additionally, they want to be able to go directly to GitHub commit when reviewing an already finished job.",
        "Question_gpt_summary":"user face challeng make machin learn experi tracker git awar want us gitignor exclud file save commit hash run addition want abl directli github commit review finish job",
        "Answer_original_content":"start form version provid integr git http doc advanc topic git integr",
        "Answer_preprocessed_content":"start form version provid integr git",
        "Answer_gpt_summary_original":"Solution: The user can use Neptune's integration with git starting from version 2.0. This integration allows the user to use .gitignore for excluded files and save commit hashes for each run. Additionally, the user can go directly to the GitHub commit when reviewing an already finished job. The documentation for this integration can be found at https:\/\/docs.neptune.ml\/advanced-topics\/git-integration\/.",
        "Answer_gpt_summary":"solut user us integr git start version integr allow user us gitignor exclud file save commit hash run addition user directli github commit review finish job document integr http doc advanc topic git integr"
    },
    {
        "Question_title":"Autoscaling VertexAI pipeline components",
        "Question_body":"<p>I am exploring VertexAI pipelines and understand that it is a managed alternative to, say, AI Platform pipelines (where you have to deploy a GKE cluster to be able to run Kubeflow pipelines). What I am not clear on is whether VertexAI will autoscale the cluster depending on the load. In the answer to a <a href=\"https:\/\/stackoverflow.com\/questions\/68343475\/how-to-scale-out-kubeflow-pipelines-using-vertex-ai-or-it-just-done-automatic\">similar question<\/a>, it is mentioned that for pipeline steps that use GCP resources such as Dataflow etc., autoscaling will be done automatically. In the google docs, it is mentioned that for components, one can <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/build-pipeline#specify-machine-type\" rel=\"nofollow noreferrer\">set resources<\/a>, such as CPU_LIMIT GPU_LIMIT etc. My question is, can these limits be set for any type of component, i.e., Google Cloud pipeline components or Custom components, whether Python function-based or those packaged as a container image? Secondly, do these limits mean that the components resources will autoscale till they hit those limits? And what happens if these options are not even specified, how are the resources allocated then, will they autoscale as VertexAI sees fit?<\/p>\n<p>Links to relevant docs and resources would be really helpful.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1628219043833,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":524.0,
        "Answer_body":"<p>To answer your questions,<\/p>\n<p><strong>1. Can these limits be set for any type of components?<\/strong><\/p>\n<blockquote>\n<p>Yes. Because, these limits are applicable to all Kubeflow components and are not specific to any particular type of component.\nThese components could be implemented to perform tasks with a set amount of resources.<\/p>\n<\/blockquote>\n<br \/>\n<p><strong>2. Do these limits mean that the component resources will autoscale till they hit the limits?<\/strong><\/p>\n<blockquote>\n<p>No, there is no autoscaling performed by Vertex AI. Based on the limits set, Vertex AI chooses one suitable VM to perform the task.\nHaving a pool of workers is supported in Google Cloud Pipeline Components such as \u201c<a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.1.4\/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.CustomContainerTrainingJobRunOp\" rel=\"nofollow noreferrer\">CustomContainerTrainingJobRunOp<\/a>\u201d and \u201c<a href=\"https:\/\/google-cloud-pipeline-components.readthedocs.io\/en\/google-cloud-pipeline-components-0.1.4\/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.CustomPythonPackageTrainingJobRunOp\" rel=\"nofollow noreferrer\">CustomPythonPackageTrainingJobRunOp<\/a>\u201d as part of Distributed Training in Vertex AI. Otherwise, only 1 machine is used per step.<\/p>\n<\/blockquote>\n<br \/>\n<p><strong>3. What happens if these limits are not specified? Does Vertex AI scale the resources as it sees fit?<\/strong><\/p>\n<blockquote>\n<p>If the limits are not specified, an \u201c<a href=\"https:\/\/cloud.google.com\/compute\/docs\/general-purpose-machines#e2_machine_types\" rel=\"nofollow noreferrer\">e2-standard-4<\/a>\u201d VM is used for task execution as the default option.<\/p>\n<\/blockquote>\n<br \/>\n<p><strong>EDIT:<\/strong> I have updated the links with the latest version of the documentation.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1632070029070,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68675606",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1628594911847,
        "Question_original_content":"autosc vertexai pipelin compon explor vertexai pipelin understand manag altern platform pipelin deploi gke cluster abl run kubeflow pipelin clear vertexai autoscal cluster depend load answer similar question mention pipelin step us gcp resourc dataflow autosc automat googl doc mention compon set resourc cpu limit gpu limit question limit set type compon googl cloud pipelin compon custom compon python function base packag contain imag secondli limit mean compon resourc autoscal till hit limit happen option specifi resourc alloc autoscal vertexai see fit link relev doc resourc help",
        "Question_preprocessed_content":"autosc vertexai pipelin compon explor vertexai pipelin understand manag altern platform pipelin clear vertexai autoscal cluster depend load answer similar question mention pipelin step us gcp resourc dataflow autosc automat googl doc mention compon set resourc question limit set type compon googl cloud pipelin compon custom compon python packag contain imag secondli limit mean compon resourc autoscal till hit limit happen option specifi resourc alloc autoscal vertexai see fit link relev doc resourc help",
        "Question_gpt_summary_original":"The user is exploring VertexAI pipelines and is unsure if the cluster will autoscale depending on the load. They are also unclear if the limits for resources such as CPU_LIMIT and GPU_LIMIT can be set for any type of component, and if these limits mean that the components resources will autoscale till they hit those limits. The user is also unsure about what happens if these options are not specified and how resources are allocated in that case. They are looking for links to relevant documentation and resources.",
        "Question_gpt_summary":"user explor vertexai pipelin unsur cluster autoscal depend load unclear limit resourc cpu limit gpu limit set type compon limit mean compon resourc autoscal till hit limit user unsur happen option specifi resourc alloc case look link relev document resourc",
        "Answer_original_content":"answer question limit set type compon ye limit applic kubeflow compon specif particular type compon compon implement perform task set resourc limit mean compon resourc autoscal till hit limit autosc perform base limit set choos suitabl perform task have pool worker support googl cloud pipelin compon customcontainertrainingjobrunop custompythonpackagetrainingjobrunop distribut train machin step happen limit specifi scale resourc see fit limit specifi standard task execut default option edit updat link latest version document",
        "Answer_preprocessed_content":"answer question limit set type compon ye limit applic kubeflow compon specif particular type compon compon implement perform task set resourc limit mean compon resourc autoscal till hit limit autosc perform base limit set choos suitabl perform task have pool worker support googl cloud pipelin compon customcontainertrainingjobrunop custompythonpackagetrainingjobrunop distribut train machin step happen limit specifi scale resourc see fit limit specifi task execut default option edit updat link latest version document",
        "Answer_gpt_summary_original":"Solutions provided:\n- Resource limits can be set for any type of Kubeflow component.\n- Resource limits do not mean that the component resources will autoscale till they hit the limits. Vertex AI chooses one suitable VM to perform the task based on the limits set.\n- If resource limits are not specified, an \u201ce2-standard-4\u201d VM is used for task execution as the default option.",
        "Answer_gpt_summary":"solut provid resourc limit set type kubeflow compon resourc limit mean compon resourc autoscal till hit limit choos suitabl perform task base limit set resourc limit specifi standard task execut default option"
    },
    {
        "Question_title":"Kubeflow Pipeline Training Component Failing | Unknown return type: <class 'inspect._empty'>",
        "Question_body":"<p>I am running an ML pipeline and the training component\/step (see code below) continues to fail with the following error: &quot;RuntimeError: Unknown return type: &lt;class 'inspect._empty'&gt;. Must be one of <code>str<\/code>, <code>int<\/code>, <code>float<\/code>, a subclass of <code>Artifact<\/code>, or a NamedTuple collection of these types.&quot;<\/p>\n<p>Any ideas on what might be causing the issue\/error and how to resolve it?<\/p>\n<p>Thank you!<\/p>\n<ul>\n<li>RE<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>\n@component(\n    # this component builds an xgboost classifier with xgboost\n    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;\n)\n\ndef build_xgb_xgboost(project: str, \n                            bq_dataset: str, \n                            test_view_name: str,\n                            bq_location: str,\n                            metrics: Output[Metrics],\n                            model: Output[Model]\n\n):\n    from google.cloud import bigquery\n    import xgboost as xgb\n    import pandas as pd\n    from xgboost import XGBRegressor\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.metrics import mean_squared_error as MSE\n    from sklearn.metrics import mean_absolute_error\n    import joblib\n    import pyarrow\n    import db_dtypes\n     \n\n    client = bigquery.Client(project=project) \n\n    view_uri = f&quot;{project}.{bq_dataset}.{test_view_name}&quot; #replace view_name with test_view_name\n    \n    build_df_for_xgboost = '''\n    SELECT * FROM `{view_uri}`\n    '''.format(view_uri = view_uri)\n\n    job_config = bigquery.QueryJobConfig()\n    df_1 = client.query(build_df_for_xgboost).to_dataframe()\n    \n    #client.query(build_df_for_xgboost, job_config=job_config).to_dataframe()  \n    \n    df = df_1.drop(['int64_field_0'], axis=1)\n    \n    def onehot_encode(df, column):\n        df = df.copy()\n        dummies = pd.get_dummies(df[column], prefix=column)\n        df = pd.concat([df, dummies], axis=1)\n        df = df.drop(column, axis=1)\n    return df\n    \n    # Binary encoding\n    df['preferred_foot'] = df['preferred_foot'].replace({'left': 0, 'right': 1})\n    \n    # One-hot encoding\n    for column in ['attacking_work_rate', 'defensive_work_rate']:\n        df = onehot_encode(df, column=column)\n    \n    # Split df into X and y\n    y = df['overall_rating']\n    X = df.drop('overall_rating', axis=1)\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    # Scale X\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n\n    #specify parameters\n    \n    #define your model \n    bst = XGBRegressor(\n    objective='reg:linear',\n    learning_rate = '.1',\n    alpha = '0.001'\n    )\n    \n    #fit your model\n    bst.fit(X_train, y_train)\n    \n    # Predict the model \n    y_pred = bst.predict(X_test)\n    rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n    mae = mean_absolute_error(y_test, y_pred)\n    \n    metrics.log_metric(&quot;RMSE&quot;, rmse)\n    metrics.log_metric(&quot;framework&quot;, &quot;xgboost&quot;)\n    metrics.log_metric(&quot;dataset_size&quot;, len(df))\n    metrics.log_metric(&quot;MAE&quot;, mae)\n    \n    dump(bst, model.path + &quot;.joblib&quot;)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655350561587,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1659256989790,
        "Question_score":0.0,
        "Question_view_count":107.0,
        "Answer_body":"<p>I think this might just be a bug in the version of KFP v2 SDK code you're using.<\/p>\n<p>I mostly use the stable KFPv1 methods to avoid problems.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\n\n\ndef train_xgboost_model(\n    project: str, \n    bq_dataset: str, \n    test_view_name: str,\n    bq_location: str,\n    metrics_path: OutputPath(Metrics),\n    model_path: OutputPath(Model),\n):\n    import json\n    from pathlib import Path\n\n    metrics = {\n       ...\n    }\n    Path(metrics_path).write_text(json.dumps(metrics))\n\n    dump(bst, model_path)\n\ntrain_xgboost_model_op = create_component_from_func(\n    func=train_xgboost_model,\n    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;,\n)\n\n<\/code><\/pre>\n<p>You can also find many examples of real-world components in this repo: <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/master\/components\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/master\/components<\/a><\/p>\n<p>including an XGBoost trainer <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/d8c4cf5\/components\/XGBoost\/Train\/component.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/d8c4cf5\/components\/XGBoost\/Train\/component.py<\/a><\/p>\n<p>and a full XGBoost pipeline: <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/4f19be6f26eaaf85ba251110d10d103b17e54a17\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI\/pipeline.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/4f19be6f26eaaf85ba251110d10d103b17e54a17\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI\/pipeline.py<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72640182",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1659257407332,
        "Question_original_content":"kubeflow pipelin train compon fail unknown return type run pipelin train compon step code continu fail follow error runtimeerror unknown return type str int float subclass artifact namedtupl collect type idea caus issu error resolv thank compon compon build xgboost classifi xgboost packag instal googl cloud bigqueri xgboost panda sklearn joblib pyarrow dtype base imag python output compon file creat xgb model xgboost yaml def build xgb xgboost project str dataset str test view str locat str metric output metric model output model googl cloud import bigqueri import xgboost xgb import panda xgboost import xgbregressor sklearn model select import train test split sklearn preprocess import standardscal sklearn metric import mean squar error mse sklearn metric import mean absolut error import joblib import pyarrow import dtype client bigqueri client project project view uri project dataset test view replac view test view build xgboost select view uri format view uri view uri job config bigqueri queryjobconfig client queri build xgboost datafram client queri build xgboost job config job config datafram drop int field axi def onehot encod column copi dummi dummi column prefix column concat dummi axi drop column axi return binari encod prefer foot prefer foot replac left right hot encod column attack work rate defens work rate onehot encod column column split overal rate drop overal rate axi train test split train test train test train test split train size shuffl true random state scale scaler standardscal scaler fit train train datafram scaler transform train index train index column train column test datafram scaler transform test index test index column test column specifi paramet defin model bst xgbregressor object reg linear learn rate alpha fit model bst fit train train predict model pred bst predict test rmse sqrt mean test pred mae mean absolut error test pred metric log metric rmse rmse metric log metric framework xgboost metric log metric dataset size len metric log metric mae mae dump bst model path joblib",
        "Question_preprocessed_content":"kubeflow pipelin train compon fail unknown return type run pipelin train continu fail follow error runtimeerror unknown return type subclass namedtupl collect idea caus resolv thank",
        "Question_gpt_summary_original":"The user is encountering an error while running an ML pipeline's training component\/step. The error message states that the return type is unknown and must be one of the specified types. The user is seeking help to identify the cause of the issue and resolve it. The code for the training component is also provided.",
        "Question_gpt_summary":"user encount error run pipelin train compon step error messag state return type unknown specifi type user seek help identifi caus issu resolv code train compon provid",
        "Answer_original_content":"think bug version kfp sdk code us stabl kfpv method avoid problem kfp compon import inputpath outputpath creat compon func def train xgboost model project str dataset str test view str locat str metric path outputpath metric model path outputpath model import json pathlib import path metric path metric path write text json dump metric dump bst model path train xgboost model creat compon func func train xgboost model packag instal googl cloud bigqueri xgboost panda sklearn joblib pyarrow dtype base imag python output compon file creat xgb model xgboost yaml exampl real world compon repo http github com ark kun pipelin compon tree master compon includ xgboost trainer http github com ark kun pipelin compon blob dccf compon xgboost train compon xgboost pipelin http github com ark kun pipelin compon blob fbefeaafbaddbea sampl googl cloud vertex train tabular regress model xgboost import vertex pipelin",
        "Answer_preprocessed_content":"think bug version kfp sdk code us stabl kfpv method avoid problem exampl compon repo includ xgboost trainer xgboost pipelin",
        "Answer_gpt_summary_original":"No solutions are provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Bay Area MLflow Meetup @ Mesoshere",
        "Question_body":"If you can't flow with us in person, do so remotely:\n\n\nMEETUP @ Mesosphere :\u00a0 https:\/\/www.meetup.com\/Bay-Area-MLflow\/events\/254124344\/\n\n\nZOOM:\u00a0https:\/\/mesosphere.zoom.us\/j\/823248629\n\n\nCheers\nJules",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1537454038000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":30.0,
        "Answer_body":"Is there a plan for another meetup in the Bay area soon?\n\n\n-Alex\n\ue5d3. Hello Alex,\n\n\nWe are in the process of finding a spot or host in the South Bay, either in December or Jan. But coming soon.\u00a0\n\n\nCheers\u00a0\nJules\u00a0\n\n\nSent from my iPhone\nPardon the dumb thumb typos :)\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/3ac5db44-a295-4e02-8050-5a5ac00f99fe%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.. If anyone would like to host one or speak at one by the way, definitely contact Jules and let him know.\n\nMatei\n\n\ue5d3\n> To view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CF31FBF2-2668-4337-9570-DD8602873C41%40databricks.com.\n\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/h8MFLcRq11M",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-11-29T22:47:20",
                "Answer_body":"Is there a plan for another meetup in the Bay area soon?\n\n\n-Alex\n\ue5d3"
            },
            {
                "Answer_creation_time":"2018-11-30T01:00:48",
                "Answer_body":"Hello Alex,\n\n\nWe are in the process of finding a spot or host in the South Bay, either in December or Jan. But coming soon.\u00a0\n\n\nCheers\u00a0\nJules\u00a0\n\n\nSent from my iPhone\nPardon the dumb thumb typos :)\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/3ac5db44-a295-4e02-8050-5a5ac00f99fe%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2018-11-30T02:49:51",
                "Answer_body":"If anyone would like to host one or speak at one by the way, definitely contact Jules and let him know.\n\nMatei\n\n\ue5d3\n> To view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CF31FBF2-2668-4337-9570-DD8602873C41%40databricks.com.\n\n\ue5d3"
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"bai area meetup mesosher flow person remot meetup mesospher http meetup com bai area event zoom http mesospher zoom cheer jule",
        "Question_preprocessed_content":"bai area meetup mesosher flow person remot meetup mesospher cheer jule",
        "Question_gpt_summary_original":"The user is facing the challenge of not being able to attend the Bay Area MLflow Meetup in person and is being provided with a remote option through Zoom.",
        "Question_gpt_summary":"user face challeng abl attend bai area meetup person provid remot option zoom",
        "Answer_original_content":"plan meetup bai area soon alex hello alex process find spot host south bai decemb jan come soon cheer jule sent iphon pardon dumb thumb typo receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user acdb aacff googlegroup com option visit http group googl com optout like host speak wai definit contact jule let know matei view discuss web visit http group googl com msgid user cffbf ddc databrick com",
        "Answer_preprocessed_content":"plan meetup bai area soon alex hello alex process find spot host south bai decemb jan come soon cheer jule sent iphon pardon dumb thumb typo receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit like host speak wai definit contact jule let know matei view discuss web visit",
        "Answer_gpt_summary_original":"Solution: \nThe discussion mentions that there is a plan to organize another meetup in the South Bay, either in December or January. However, no specific details are provided about the date, time, or location of the meetup.",
        "Answer_gpt_summary":"solut discuss mention plan organ meetup south bai decemb januari specif detail provid date time locat meetup"
    },
    {
        "Question_title":"Compose model",
        "Question_body":"As the document\nA composed model is created by taking a collection of custom models and assigning them to a single model ID. You can assign up to 100 trained custom models to a single composed model ID. When a document is submitted to a composed model, the service performs a classification step to decide which custom model accurately represents the form presented for analysis.\n\nWhat\u2019s the price for the classification step?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1669041105107,
        "Question_favorite_count":13.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @KenSmith-3969\n\nThanks for reaching out to us and sorry for the confusion of the document.\n\nThere is no extra fee for the classification you mentioned in the document. You only pay for the custom model you finally run for your document.\n\nI will raise a ticket to fix the document, thanks a lot for pointing out it.\n\nI hope this helps!\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks!",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1098169\/compose-model.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-21T15:31:45.523Z",
                "Answer_score":0,
                "Answer_body":"Hello @KenSmith-3969\n\nThanks for reaching out to us and sorry for the confusion of the document.\n\nThere is no extra fee for the classification you mentioned in the document. You only pay for the custom model you finally run for your document.\n\nI will raise a ticket to fix the document, thanks a lot for pointing out it.\n\nI hope this helps!\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks!",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1669044705523,
        "Question_original_content":"compos model document compos model creat take collect custom model assign singl model assign train custom model singl compos model document submit compos model servic perform classif step decid custom model accur repres form present analysi what price classif step",
        "Question_preprocessed_content":"compos model document compos model creat take collect custom model assign singl model assign train custom model singl compos model document submit compos model servic perform classif step decid custom model accur repres form present analysi what price classif step",
        "Question_gpt_summary_original":"The given text does not provide any information about challenges faced by the user. It only explains the concept of a composed model and how it works. Therefore, a summary of challenges cannot be provided.",
        "Question_gpt_summary":"given text provid inform challeng face user explain concept compos model work summari challeng provid",
        "Answer_original_content":"hello kensmith thank reach sorri confus document extra fee classif mention document pai custom model final run document rais ticket fix document thank lot point hope help regard yutong kindli accept answer feel help support commun thank",
        "Answer_preprocessed_content":"hello thank reach sorri confus document extra fee classif mention document pai custom model final run document rais ticket fix document thank lot point hope help regard yutong kindli accept answer feel help support commun thank",
        "Answer_gpt_summary_original":"No solutions were mentioned in the discussion as the challenge did not provide any information about challenges faced by the user. The discussion only addressed a clarification regarding the pricing of a custom model.",
        "Answer_gpt_summary":"solut mention discuss challeng provid inform challeng face user discuss address clarif price custom model"
    },
    {
        "Question_title":"start, monitor and define script of SageMaker processing job from local machine",
        "Question_body":"<p>I am looking at <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb\" rel=\"nofollow noreferrer\">this<\/a>, which makes all sense. Let us focus on this bit of code:<\/p>\n<pre><code>from sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor.run(\n    code=&quot;preprocessing.py&quot;,\n    inputs=[\n        ProcessingInput(source=&quot;s3:\/\/your-bucket\/path\/to\/your\/data&quot;, destination=&quot;\/opt\/ml\/processing\/input&quot;),\n    ],\n    outputs=[\n        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n    ],\n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],\n)\n\npreprocessing_job_description = sklearn_processor.jobs[-1].describe() \n<\/code><\/pre>\n<p>Here preprocessing.py has to be obviously in the cloud. I am curious, could one also put scripts into an S3 bucket and trigger the job remotely. I can easily to this with hyper parameter optimisation, which does not require dedicated scripts though as I use an OOTB training image.<\/p>\n<p>In this case I can fire off the job like so:<\/p>\n<pre><code>tuning_job_name = &quot;amazing-hpo-job-&quot; + strftime(&quot;%d-%H-%M-%S&quot;, gmtime())\n\nsmclient = boto3.Session().client(&quot;sagemaker&quot;)\nsmclient.create_hyper_parameter_tuning_job(\n    HyperParameterTuningJobName=tuning_job_name,\n    HyperParameterTuningJobConfig=tuning_job_config,\n    TrainingJobDefinition=training_job_definition\n)\n<\/code><\/pre>\n<p>and then monitor the job's progress:<\/p>\n<pre><code>smclient = boto3.Session().client(&quot;sagemaker&quot;)\n\ntuning_job_result = smclient.describe_hyper_parameter_tuning_job(\n    HyperParameterTuningJobName=tuning_job_name\n)\n\nstatus = tuning_job_result[&quot;HyperParameterTuningJobStatus&quot;]\nif status != &quot;Completed&quot;:\n    print(&quot;Reminder: the tuning job has not been completed.&quot;)\n\njob_count = tuning_job_result[&quot;TrainingJobStatusCounters&quot;][&quot;Completed&quot;]\nprint(&quot;%d training jobs have completed&quot; % job_count)\n\nobjective = tuning_job_result[&quot;HyperParameterTuningJobConfig&quot;][&quot;HyperParameterTuningJobObjective&quot;]\nis_minimize = objective[&quot;Type&quot;] != &quot;Maximize&quot;\nobjective_name = objective[&quot;MetricName&quot;]\n\nif tuning_job_result.get(&quot;BestTrainingJob&quot;, None):\n    print(&quot;Best model found so far:&quot;)\n    pprint(tuning_job_result[&quot;BestTrainingJob&quot;])\nelse:\n    print(&quot;No training jobs have reported results yet.&quot;) \n<\/code><\/pre>\n<p>I would think starting and monitoring a SageMaker processing job from a local machine should be possible as with an HPO job but what about the script(s)? Ideally I would like to develop and test them locally and the run remotely. Hope this makes sense?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662209372887,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":38.0,
        "Answer_body":"<p>Im not sure I understand the comparison to a Tuning Job.<\/p>\n<p>Based on what you have described, in this case the <code>preprocessing.py<\/code> is actually stored locally. The SageMaker SDK will upload it to S3 for the remote Processing Job to access it. I suggest launching the Job and then taking a look at the inputs in the SageMaker Console.<\/p>\n<p>If you wanted to test the Processing Job locally you can do so using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">Local Mode<\/a>. This will basically imitate the Job locally which aids in debugging the script before kicking off a remote Processing Job. Kindly note docker is required to make use of Local Mode.<\/p>\n<p>Example code for local mode:<\/p>\n<pre><code>from sagemaker.local import LocalSession\nfrom sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n\nsagemaker_session = LocalSession()\nsagemaker_session.config = {'local': {'local_code': True}}\n\n# For local training a dummy role will be sufficient\nrole = 'arn:aws:iam::111111111111:role\/service-role\/AmazonSageMaker-ExecutionRole-20200101T000001'\n\nprocessor = ScriptProcessor(command=['python3'],\n                    image_uri='sagemaker-scikit-learn-processing-local',\n                    role=role,\n                    instance_count=1,\n                    instance_type='local')\n\nprocessor.run(code='processing_script.py',\n                    inputs=[ProcessingInput(\n                        source='.\/input_data\/',\n                        destination='\/opt\/ml\/processing\/input_data\/')],\n                    outputs=[ProcessingOutput(\n                        output_name='word_count_data',\n                        source='\/opt\/ml\/processing\/processed_data\/')],\n                    arguments=['job-type', 'word-count']\n                    )\n\npreprocessing_job_description = processor.jobs[-1].describe()\noutput_config = preprocessing_job_description['ProcessingOutputConfig']\n\nprint(output_config)\n\nfor output in output_config['Outputs']:\n    if output['OutputName'] == 'word_count_data':\n        word_count_data_file = output['S3Output']['S3Uri']\n\nprint('Output file is located on: {}'.format(word_count_data_file))\n\n\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73592371",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1662504468407,
        "Question_original_content":"start monitor defin script process job local machin look make sens let focu bit code process import processinginput processingoutput sklearn processor run code preprocess input processinginput sourc bucket path data destin opt process input output processingoutput output train data sourc opt process train processingoutput output test data sourc opt process test argument train test split ratio preprocess job descript sklearn processor job preprocess obvious cloud curiou script bucket trigger job remot easili hyper paramet optimis requir dedic script us ootb train imag case job like tune job amaz hpo job strftime gmtime smclient boto session client smclient creat hyper paramet tune job hyperparametertuningjobnam tune job hyperparametertuningjobconfig tune job config trainingjobdefinit train job definit monitor job progress smclient boto session client tune job result smclient hyper paramet tune job hyperparametertuningjobnam tune job statu tune job result hyperparametertuningjobstatu statu complet print remind tune job complet job count tune job result trainingjobstatuscount complet print train job complet job count object tune job result hyperparametertuningjobconfig hyperparametertuningjobobject minim object type maxim object object metricnam tune job result besttrainingjob print best model far pprint tune job result besttrainingjob print train job report result think start monitor process job local machin possibl hpo job script ideal like develop test local run remot hope make sens",
        "Question_preprocessed_content":"start monitor defin script process job local machin look make sens let focu bit code obvious cloud curiou script bucket trigger job remot easili hyper paramet optimis requir dedic script us ootb train imag case job like monitor job progress think start monitor process job local machin possibl hpo job script ideal like develop test local run remot hope make sens",
        "Question_gpt_summary_original":"The user is trying to start, monitor, and define a script for a SageMaker processing job from their local machine. They are able to trigger hyperparameter optimization remotely using an out-of-the-box training image, but they are unsure if they can put scripts into an S3 bucket and trigger the job remotely. The user would like to develop and test the scripts locally before running them remotely.",
        "Question_gpt_summary":"user try start monitor defin script process job local machin abl trigger hyperparamet optim remot box train imag unsur script bucket trigger job remot user like develop test script local run remot",
        "Answer_original_content":"sure understand comparison tune job base describ case preprocess actual store local sdk upload remot process job access suggest launch job take look input consol want test process job local local mode basic imit job local aid debug script kick remot process job kindli note docker requir us local mode exampl code local mode local import localsess process import scriptprocessor processinginput processingoutput session localsess session config local local code true local train dummi role suffici role arn aw iam role servic role amazon executionrol processor scriptprocessor command python imag uri scikit learn process local role role instanc count instanc type local processor run code process script input processinginput sourc input data destin opt process input data output processingoutput output word count data sourc opt process process data argument job type word count preprocess job descript processor job output config preprocess job descript processingoutputconfig print output config output output config output output outputnam word count data word count data file output soutput suri print output file locat format word count data file",
        "Answer_preprocessed_content":"sure understand comparison tune job base describ case actual store local sdk upload remot process job access suggest launch job take look input consol want test process job local local mode basic imit job local aid debug script kick remot process job kindli note docker requir us local mode exampl code local mode",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. The user can store the `preprocessing.py` script locally and the SageMaker SDK will upload it to S3 for the remote Processing Job to access it. They can launch the job and then check the inputs in the SageMaker Console.\n2. The user can test the Processing Job locally using Local Mode, which imitates the job locally and aids in debugging the script before kicking off a remote Processing Job. Docker is required to make use of Local Mode. \n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss user store preprocess script local sdk upload remot process job access launch job check input consol user test process job local local mode imit job local aid debug script kick remot process job docker requir us local mode person opinion bias includ summari"
    },
    {
        "Question_title":"How to cancel a running job from the UI?",
        "Question_body":"<p>Am I missing something but how can I cancel a run in my workspace from <a href=\"https:\/\/ms.portal.azure.com\/\" rel=\"nofollow noreferrer\">https:\/\/ms.portal.azure.com\/<\/a> ? The cancel button is always greyed out.<\/p>\n\n<p>I know I can use use the sdk to cancel a run using:<\/p>\n\n<pre><code>run = [ r for r in Experiment(ws, 'myExp').get_runs() if r.id == '899b8314-26b6-458f-9f5c-539ffbf01b91'].pop()\nrun.cancel()\n<\/code><\/pre>\n\n<p>But it would be more convenient to be able to do it from the UI<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1568993659347,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":204.0,
        "Answer_body":"<p>What kind of run is this? Canceling is not currently enabled for pipeline runs in the UI, but is supported for other run types.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1569508172863,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58031370",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1569333175230,
        "Question_original_content":"cancel run job miss cancel run workspac http portal azur com cancel button grei know us us sdk cancel run run experi myexp run ffbfb pop run cancel conveni abl",
        "Question_preprocessed_content":"cancel run job miss cancel run workspac cancel button grei know us us sdk cancel run conveni abl",
        "Question_gpt_summary_original":"The user is facing a challenge in cancelling a running job from the UI in their workspace on https:\/\/ms.portal.azure.com\/. The cancel button is always greyed out, and they have to use the SDK to cancel a run, which is inconvenient.",
        "Question_gpt_summary":"user face challeng cancel run job workspac http portal azur com cancel button grei us sdk cancel run inconveni",
        "Answer_original_content":"kind run cancel current enabl pipelin run support run type",
        "Answer_preprocessed_content":"kind run cancel current enabl pipelin run support run type",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that cancelling pipeline runs from the UI is not currently supported, but cancelling other run types is possible. No other solutions are mentioned.",
        "Answer_gpt_summary":"solut discuss suggest cancel pipelin run current support cancel run type possibl solut mention"
    },
    {
        "Question_title":"MLflow: INVALID_PARAMETER_VALUE: Unsupported URI '.\/mlruns' for model registry store",
        "Question_body":"<p>I got this error when I was trying to have a model registered in the model registry. Could someone help me?<\/p>\n<pre><code>RestException: INVALID_PARAMETER_VALUE: Unsupported URI '.\/mlruns' for model registry store. \nSupported schemes are: ['postgresql', 'mysql', 'sqlite', 'mssql']. \nSee https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#storage for how to setup a compatible server.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596578095187,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1598815085667,
        "Question_score":10.0,
        "Question_view_count":12594.0,
        "Answer_body":"<p>Mlflow required DB as datastore for Model Registry\nSo you have to run tracking server with DB as backend-store and log model to this tracking server.\nThe easiest way to use DB is to use SQLite.<\/p>\n<pre><code>mlflow server \\\n    --backend-store-uri sqlite:\/\/\/mlflow.db \\\n    --default-artifact-root .\/artifacts \\\n    --host 0.0.0.0\n<\/code><\/pre>\n<p>And set MLFLOW_TRACKING_URI environment variable to <em>http:\/\/localhost:5000<\/em> or<\/p>\n<pre><code>mlflow.set_tracking_uri(&quot;http:\/\/localhost:5000&quot;)\n<\/code><\/pre>\n<p>After got to http:\/\/localhost:5000 and you can register a logged model from UI or from the code.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":null,
        "Answer_score":27.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63255631",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1596626369480,
        "Question_original_content":"invalid paramet valu unsupport uri mlrun model registri store got error try model regist model registri help restexcept invalid paramet valu unsupport uri mlrun model registri store support scheme postgresql mysql sqlite mssql http org doc latest track html storag setup compat server",
        "Question_preprocessed_content":"unsupport uri model registri store got error try model regist model registri help",
        "Question_gpt_summary_original":"The user encountered an error while trying to register a model in the model registry. The error message stated that the URI '.\/mlruns' is unsupported for the model registry store and provided a list of supported schemes. The user requested assistance in resolving the issue.",
        "Question_gpt_summary":"user encount error try regist model model registri error messag state uri mlrun unsupport model registri store provid list support scheme user request assist resolv issu",
        "Answer_original_content":"requir datastor model registri run track server backend store log model track server easiest wai us us sqlite server backend store uri sqlite default artifact root artifact host set track uri environ variabl http localhost set track uri http localhost got http localhost regist log model code",
        "Answer_preprocessed_content":"requir datastor model registri run track server log model track server easiest wai us us sqlite set environ variabl got regist log model code",
        "Answer_gpt_summary_original":"Solution: The user needs to run the tracking server with a database as the backend store and log the model to this tracking server. The easiest way to use a database is to use SQLite. The user can set the MLFLOW_TRACKING_URI environment variable to \"http:\/\/localhost:5000\" or use the \"mlflow.set_tracking_uri\" command. After that, the user can register a logged model from the UI or from the code by going to http:\/\/localhost:5000.",
        "Answer_gpt_summary":"solut user need run track server databas backend store log model track server easiest wai us databas us sqlite user set track uri environ variabl http localhost us set track uri command user regist log model code go http localhost"
    },
    {
        "Question_title":"Sagemaker API to list Hyperparameters",
        "Question_body":"<p>I'm currently trying to implement MLFlow Tracking into my training pipeline and would like to log the hyperparameters of my hyperparameter Tuning of each training job.<\/p>\n\n<p>Does anyone know, how to pull the list of hyperparameters that can be seen on the sagemaker training job interface (on the AWS console)? Is there any other smarter way to list how models perform in comparison in Sagemaker (and displayed)?<\/p>\n\n<p>I would assume there must be an easy and Pythonic way to do this (either boto3 or the sagemaker api) to get this data. I wasn't able to find it in Cloudwatch.<\/p>\n\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1591864702897,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":484.0,
        "Answer_body":"<p>there is indeed a rather pythonic way in the SageMaker python SDK:<\/p>\n\n<pre><code>tuner = sagemaker.tuner.HyperparameterTuner.attach('&lt; your tuning jobname&gt;')\n\nresults = tuner.analytics().dataframe()  # all your tuning metadata, in pandas!\n<\/code><\/pre>\n\n<p>See full example here <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62320331",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1592173650467,
        "Question_original_content":"api list hyperparamet current try implement track train pipelin like log hyperparamet hyperparamet tune train job know pull list hyperparamet seen train job interfac aw consol smarter wai list model perform comparison displai assum easi python wai boto api data wasn abl cloudwatch thank advanc",
        "Question_preprocessed_content":"api list hyperparamet current try implement track train pipelin like log hyperparamet hyperparamet tune train job know pull list hyperparamet seen train job interfac smarter wai list model perform comparison assum easi python wai data wasn abl cloudwatch thank advanc",
        "Question_gpt_summary_original":"The user is facing challenges in implementing MLFlow Tracking into their training pipeline and is seeking help to log the hyperparameters of their hyperparameter tuning for each training job. They are looking for a way to pull the list of hyperparameters seen on the Sagemaker training job interface and display how models perform in comparison in Sagemaker. The user has tried to find this data in Cloudwatch but was unsuccessful.",
        "Question_gpt_summary":"user face challeng implement track train pipelin seek help log hyperparamet hyperparamet tune train job look wai pull list hyperparamet seen train job interfac displai model perform comparison user tri data cloudwatch unsuccess",
        "Answer_original_content":"python wai python sdk tuner tuner hyperparametertun attach result tuner analyt datafram tune metadata panda exampl http github com aw sampl amazon tuneranalyt sampl blob master tune job analyt ipynb",
        "Answer_preprocessed_content":"python wai python sdk exampl",
        "Answer_gpt_summary_original":"Solution: The user can use the SageMaker python SDK to pull the list of hyperparameters seen on the Sagemaker training job interface and display how models perform in comparison in Sagemaker. They can attach the hyperparameter tuner to their tuning job name and use the analytics() function to get all the tuning metadata in pandas. A full example is provided in the given link.",
        "Answer_gpt_summary":"solut user us python sdk pull list hyperparamet seen train job interfac displai model perform comparison attach hyperparamet tuner tune job us analyt function tune metadata panda exampl provid given link"
    },
    {
        "Question_title":"create sagemaker notebook instance via Terraform",
        "Question_body":"<p>I am taking my first steps into the Terraform world so please be gentle with me. I have a user with AmazonSageMakerFullAccess, which I stored via AWS CLI in a profile called terraform. I can create an S3 bucket as follows no problem referring this user in Windows in VSC:<\/p>\n<pre><code>provider &quot;aws&quot; {\n    region = &quot;eu-west-2&quot;\n    shared_credentials_files = [&quot;C:\\\\Users\\\\amazinguser\\\\.aws\\\\credentials&quot;]\n    profile = &quot;terraform&quot;\n}\n\nresource &quot;aws_s3_bucket&quot; &quot;b&quot; {\n  bucket = &quot;blabla-test-bucket&quot;\n\n  tags = {\n    Name        = &quot;amazing_tag&quot;\n    Environment = &quot;dev&quot;\n  }\n}\n<\/code><\/pre>\n<p>I try to implement <a href=\"https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\/tree\/main\/Part%202a%20-%20Creating%20a%20SageMaker%20Notebook\" rel=\"nofollow noreferrer\">this<\/a> documented <a href=\"https:\/\/towardsdatascience.com\/terraform-sagemaker-part-2a-creating-a-custom-sagemaker-notebook-instance-1d68c90b192b\" rel=\"nofollow noreferrer\">here<\/a> and try to this:<\/p>\n<pre><code>resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot; {\n  name = &quot;titanic-sagemaker-byoc-notebook&quot;\n  role_arn = aws_iam_role.notebook_iam_role.arn\n  instance_type = &quot;ml.t2.medium&quot;\n  #lifecycle_config_name = aws_sagemaker_notebook_instance_lifecycle_configuration.notebook_config.name\n  #default_code_repository = aws_sagemaker_code_repository.git_repo.code_repository_name\n}\n<\/code><\/pre>\n<p>I am a bit confused about the role_arn which is defined here:<\/p>\n<p><a href=\"https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\/blob\/main\/Part%202a%20-%20Creating%20a%20SageMaker%20Notebook\/terraform\/iam.tf\" rel=\"nofollow noreferrer\">https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\/blob\/main\/Part%202a%20-%20Creating%20a%20SageMaker%20Notebook\/terraform\/iam.tf<\/a><\/p>\n<p>Can I not use the above user? Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_time":1649680247017,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":338.0,
        "Answer_body":"<p>AWS services trying to call other AWS services and perform actions are not allowed to do so by default. For example, SageMaker Notebooks are basically EC2 instances. In order for SageMaker to create EC2 instances, it has to have a policy which allows e.g., injecting ENIs to a VPC. Since you probably do not want to do all that by yourself (it is a managed Notebook service after all), you have to give SageMaker permissions to perform actions on your behalf. Enter <strong>execution roles<\/strong>. For SageMaker, you can read more in [1]. Other services that you will commonly find using execution roles are Lambda, ECS and many others. An IAM role usually consists of two parts:<\/p>\n<ol>\n<li>Trust relationship (I like to call it trust policy)<\/li>\n<li>Permissions policy<\/li>\n<\/ol>\n<p>The first one decides which principal (AWS identifier, Service etc. [2]) will be able to assume the role. In your example, that is:<\/p>\n<pre><code>data &quot;aws_iam_policy_document&quot; &quot;sm_assume_role_policy&quot; {\n  statement {\n    actions = [&quot;sts:AssumeRole&quot;]\n    \n    principals {\n      type = &quot;Service&quot;\n      identifiers = [&quot;sagemaker.amazonaws.com&quot;]\n    }\n  }\n}\n<\/code><\/pre>\n<p>What this policy says is &quot;I am going to allow SageMaker (which is of type <code>Service<\/code>) to assume any role to which this policy is attached and perform actions that are defined in the permissions policy&quot;. The permissions policy is:<\/p>\n<pre><code># Attaching the AWS default policy, &quot;AmazonSageMakerFullAccess&quot;\nresource &quot;aws_iam_policy_attachment&quot; &quot;sm_full_access_attach&quot; {\n  name = &quot;sm-full-access-attachment&quot;\n  roles = [aws_iam_role.notebook_iam_role.name]\n  policy_arn = &quot;arn:aws:iam::aws:policy\/AmazonSageMakerFullAccess&quot;\n}\n<\/code><\/pre>\n<p>Without going into too much details about what the AWS managed policy for SageMaker does, it is enough to see the <code>FullAccess<\/code> part for it to be clear. What you could do if you want to be extra careful is to define a customer managed policy [3] for SageMaker notebooks. This permissions policy will be attached to the IAM role(s) defined in the <code>roles<\/code> argument. Note that it is a list, so multiple roles can have the same permissions policy attached.<\/p>\n<p>Last, but not the least, the glue between the trust and permissions policy is the role itself:<\/p>\n<pre><code>resource &quot;aws_iam_role&quot; &quot;notebook_iam_role&quot; {\n  name = &quot;sm_notebook_role&quot;\n  assume_role_policy = data.aws_iam_policy_document.sm_assume_role_policy.json\n}\n<\/code><\/pre>\n<p>As you can see, the <code>assume_role_policy<\/code> is the policy which will allow SageMaker to perform actions in the AWS account based on the permissions defined in the permissions policy.<\/p>\n<p>This topic is much more complex than in this answer, but it should give you a fair amount of information.<\/p>\n<p>NOTE: In theory, the same role accessing information in AWS and running the AWS API actions when using Terraform could be used for SageMaker, but I would strongly advise against it. Always keep in mind separation of concerns and principle of least privilege.<\/p>\n<hr \/>\n<p>[1] <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html#sagemaker-roles-create-execution-role\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html#sagemaker-roles-create-execution-role<\/a><\/p>\n<p>[2] <a href=\"https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/reference_policies_elements_principal.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/reference_policies_elements_principal.html<\/a><\/p>\n<p>[3] <a href=\"https:\/\/docs.aws.amazon.com\/acm\/latest\/userguide\/authen-custmanagedpolicies.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/acm\/latest\/userguide\/authen-custmanagedpolicies.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71827884",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1649687933128,
        "Question_original_content":"creat notebook instanc terraform take step terraform world gentl user amazonfullaccess store aw cli profil call terraform creat bucket follow problem refer user window vsc provid aw region west share credenti file user amazingus aw credenti profil terraform resourc aw bucket bucket blabla test bucket tag amaz tag environ dev try implement document try resourc aw notebook instanc notebook instanc titan byoc notebook role arn aw iam role notebook iam role arn instanc type medium lifecycl config aw notebook instanc lifecycl configur notebook config default code repositori aw code repositori git repo code repositori bit confus role arn defin http github com dkhundlei terraform tutori blob main creat notebook terraform iam us user thank",
        "Question_preprocessed_content":"creat notebook instanc terraform take step terraform world gentl user amazonfullaccess store aw cli profil call terraform creat bucket follow problem refer user window vsc try implement document try bit confus defin us user thank",
        "Question_gpt_summary_original":"The user is facing challenges in creating a SageMaker notebook instance via Terraform. They have successfully created an S3 bucket but are confused about the role_arn required for the notebook instance. They are unsure if they can use the user they have already created for this purpose.",
        "Question_gpt_summary":"user face challeng creat notebook instanc terraform successfulli creat bucket confus role arn requir notebook instanc unsur us user creat purpos",
        "Answer_original_content":"aw servic try aw servic perform action allow default exampl notebook basic instanc order creat instanc polici allow inject eni vpc probabl want manag notebook servic permiss perform action behalf enter execut role read servic commonli execut role lambda ec iam role usual consist part trust relationship like trust polici permiss polici decid princip aw identifi servic abl assum role exampl data aw iam polici document assum role polici statement action st assumerol princip type servic identifi amazonaw com polici sai go allow type servic assum role polici attach perform action defin permiss polici permiss polici attach aw default polici amazonfullaccess resourc aw iam polici attach access attach access attach role aw iam role notebook iam role polici arn arn aw iam aw polici amazonfullaccess go detail aw manag polici fullaccess clear want extra care defin custom manag polici notebook permiss polici attach iam role defin role argument note list multipl role permiss polici attach glue trust permiss polici role resourc aw iam role notebook iam role notebook role assum role polici data aw iam polici document assum role polici json assum role polici polici allow perform action aw account base permiss defin permiss polici topic complex answer fair inform note theori role access inform aw run aw api action terraform strongli advis mind separ concern principl privileg http doc aw amazon com latest role html role creat execut role http doc aw amazon com iam latest userguid refer polici element princip html http doc aw amazon com acm latest userguid authen custmanagedpolici html",
        "Answer_preprocessed_content":"aw servic try aw servic perform action allow default exampl notebook basic instanc order creat instanc polici allow inject eni vpc probabl want permiss perform action behalf enter execut role read servic commonli execut role lambda ec iam role usual consist part trust relationship permiss polici decid princip abl assum role exampl polici sai go allow assum role polici attach perform action defin permiss polici permiss polici go detail aw manag polici clear want extra care defin custom manag polici notebook permiss polici attach iam role defin argument note list multipl role permiss polici attach glue trust permiss polici role polici allow perform action aw account base permiss defin permiss polici topic complex answer fair inform note theori role access inform aw run aw api action terraform strongli advis mind separ concern principl privileg",
        "Answer_gpt_summary_original":"Solution:\nThe user needs to create an execution role for SageMaker to perform actions on their behalf. The IAM role consists of two parts: trust relationship and permissions policy. The trust relationship decides which principal will be able to assume the role, and the permissions policy defines the actions that can be performed. The user can define a customer-managed policy for SageMaker notebooks and attach it to the IAM role(s) defined in the roles argument. The same role accessing information in AWS and running the AWS API actions when using Terraform could be used for SageMaker, but it is advised to keep in mind separation of concerns and the principle of least privilege.",
        "Answer_gpt_summary":"solut user need creat execut role perform action behalf iam role consist part trust relationship permiss polici trust relationship decid princip abl assum role permiss polici defin action perform user defin custom manag polici notebook attach iam role defin role argument role access inform aw run aw api action terraform advis mind separ concern principl privileg"
    },
    {
        "Question_title":"Add model description when registering model after hyperdrive successful run",
        "Question_body":"<p>I have successfully trained a model on Azure Machine Learning Service using Hyperdrive that has now yielded a hyperdrive run instance<\/p>\n\n<pre><code>hyperdrive_run = exp.submit(config=hypertune_config)\nhyperdrive_run\nbest_run = hyperdrive_run.get_best_run_by_primary_metric()\n<\/code><\/pre>\n\n<p>As a next step, I would like to register a model while adding a description to the model.:<\/p>\n\n<pre><code>pumps_rf = best_run.register_model(model_name='pumps_rf', model_path='outputs\/rf.pkl')\n<\/code><\/pre>\n\n<p>There is a <code>description<\/code> column in the Models section of my AML Workspace on Azure portal but the <code>register_model<\/code> method does not seem to have a <code>description<\/code> flag. So how do I go about adding a description to the model so I see it in Azure Portal?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1550539380057,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":453.0,
        "Answer_body":"<p>Good question :).<\/p>\n\n<p>Looking at the current version of the API, it doesn't look like you can add the description using <code>Run.register_model<\/code>, as confirmed <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#register-model-model-name--model-path-none--tags-none--properties-none----kwargs-\" rel=\"nofollow noreferrer\">by the docs<\/a>. <\/p>\n\n<p>You can go around this however by registering the model using the <code>Model.register<\/code> method which, fortunately, includes an argument for <code>description<\/code> as detailed <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none-\" rel=\"nofollow noreferrer\">here<\/a>. In your case, you also need to <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run(class)?view=azure-ml-py#download-file-name--output-file-path-none-\" rel=\"nofollow noreferrer\">download the files<\/a> first.<\/p>\n\n<p>In short, use something like:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>best_run.download_file('outputs\/rf.pkl', output_file_path='.\/rf.pkl')\n\nModel.register(workspace=ws, model_path='.\/rf.pkl', model_name=\"pumps_rf\", description=\"There are many models like it, but this one is mine.\")\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1550686459150,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54757598",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1550668517447,
        "Question_original_content":"add model descript regist model hyperdr success run successfulli train model servic hyperdr yield hyperdr run instanc hyperdr run exp submit config hypertun config hyperdr run best run hyperdr run best run primari metric step like regist model ad descript model pump best run regist model model pump model path output pkl descript column model section aml workspac azur portal regist model method descript flag ad descript model azur portal",
        "Question_preprocessed_content":"add model descript regist model hyperdr success run successfulli train model servic hyperdr yield hyperdr run instanc step like regist model ad descript column model section aml workspac azur portal method flag ad descript model azur portal",
        "Question_gpt_summary_original":"The user has successfully trained a model on Azure Machine Learning Service using Hyperdrive, and has registered a model using the <code>register_model<\/code> method. However, the user is facing a challenge in adding a description to the model as the method does not have a <code>description<\/code> flag, and is unsure how to add a description to the model in the Azure Portal.",
        "Question_gpt_summary":"user successfulli train model servic hyperdr regist model regist model method user face challeng ad descript model method descript flag unsur add descript model azur portal",
        "Answer_original_content":"good question look current version api look like add descript run regist model confirm doc regist model model regist method fortun includ argument descript detail case need download file short us like best run download file output pkl output file path pkl model regist workspac model path pkl model pump descript model like",
        "Answer_preprocessed_content":"good question look current version api look like add descript confirm doc regist model method fortun includ argument detail case need download file short us like",
        "Answer_gpt_summary_original":"Solution: The user can register the model using the <code>Model.register<\/code> method which includes an argument for <code>description<\/code>. The user needs to download the files first and then use the <code>Model.register<\/code> method to register the model with a description.",
        "Answer_gpt_summary":"solut user regist model model regist method includ argument descript user need download file us model regist method regist model descript"
    },
    {
        "Question_title":"Is there a specification document for server sizing?",
        "Question_body":"Hello,\n\n\nI'll give MLflow a try within my team: that'll be a few data scientists trying out some use cases to evaluate MLflow in our environment.\n\n\n\nI couldn't find a specification document for sizing servers for MLflow installation. Is there some kind of guideline that could tell me amount of CPU, RAM, etc. for the server on which MLflow runs? It's of course not very important in this early stage, but nevertheless I think having such a guideline would be good.\n\n\nCheers,\nEmre",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1530070766000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":42.0,
        "Answer_body":"For your reference, at least in a test environment, I have my MLflow server running on a t2.micro (free-tier) instance.\u00a0 Of course, its just me hitting it ;-).\n\ue5d3. Hey guys,\n\n\nI have got ML flow working in a docker image on an AWS EC2 instance (t2.medium). We are able to have multiple data scientists send jobs to the tracker. However, I am unable to store artifacts on a remote VM from different local machines.\n\n\nAny ideas if this is in the pipeline?\n\n\nCheers,\nVivek\n\n\n\ue5d3\n\ue5d3\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users+unsubscribe@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/094a339c-e77c-4220-80df-a1e5693e5092%40googlegroups.com.\n\nFor more options, visit https:\/\/groups.google.com\/d\/optout.\n\n\n\n\n\n\n--\n\nVivek Katial\nData Scientist\n\n\n\n\n\nLevel 1, 155 Karangahape Road, Auckland Central,\u00a01010\nvivek....@quantiful.co.nz | \u00a00210435892\nwww.quantiful.co.nz",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/f0hQ6k4w8dA",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-06-27T13:04:18",
                "Answer_body":"For your reference, at least in a test environment, I have my MLflow server running on a t2.micro (free-tier) instance.\u00a0 Of course, its just me hitting it ;-).\n\ue5d3"
            },
            {
                "Answer_creation_time":"2018-06-27T17:17:51",
                "Answer_body":"Hey guys,\n\n\nI have got ML flow working in a docker image on an AWS EC2 instance (t2.medium). We are able to have multiple data scientists send jobs to the tracker. However, I am unable to store artifacts on a remote VM from different local machines.\n\n\nAny ideas if this is in the pipeline?\n\n\nCheers,\nVivek\n\n\n\ue5d3\n\ue5d3\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users+unsubscribe@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/094a339c-e77c-4220-80df-a1e5693e5092%40googlegroups.com.\n\nFor more options, visit https:\/\/groups.google.com\/d\/optout.\n\n\n\n\n\n\n--\n\nVivek Katial\nData Scientist\n\n\n\n\n\nLevel 1, 155 Karangahape Road, Auckland Central,\u00a01010\nvivek....@quantiful.co.nz | \u00a00210435892\nwww.quantiful.co.nz"
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"specif document server size hello try team data scientist try us case evalu environ couldn specif document size server instal kind guidelin tell cpu ram server run cours import earli stage think have guidelin good cheer emr",
        "Question_preprocessed_content":"specif document server size hello try team data scientist try us case evalu environ couldn specif document size server instal kind guidelin tell cpu ram server run cours import earli stage think have guidelin good cheer emr",
        "Question_gpt_summary_original":"The user is encountering a challenge in finding a specification document for server sizing for MLflow installation. They are seeking guidance on the amount of CPU, RAM, etc. required for the server on which MLflow runs.",
        "Question_gpt_summary":"user encount challeng find specif document server size instal seek guidanc cpu ram requir server run",
        "Answer_original_content":"refer test environ server run micro free tier instanc cours hit hei gui got flow work docker imag aw instanc medium abl multipl data scientist send job tracker unabl store artifact remot differ local machin idea pipelin cheer vivek receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user unsubscrib googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user aee googlegroup com option visit http group googl com optout vivek katial data scientist level karangahap road auckland central vivek quanti quanti",
        "Answer_preprocessed_content":"refer test environ server run instanc cours hit hei gui got flow work docker imag aw instanc abl multipl data scientist send job tracker unabl store artifact remot differ local machin idea pipelin cheer vivek receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit vivek katial data scientist level karangahap road auckland central",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"dataset.to_pandas_dataframe() throws a ScriptExecution.StreamAccess.Authentication error",
        "Question_body":"Azure fails to connect with the Dataset citing 403 inspite of SAS token\nThis appears when we try to load the data as a pandas dataframe . dataset = Dataset.get_by_name() works\n\nError message:\n\n\n\n\n{\n\"error\": {\n\"code\": \"UserError\",\n\"message\": \"Execution failed in operation 'to_pandas_dataframe' for Dataset(id='data id', name='dataset name', error_code=ScriptExecution.StreamAccess.Authentication,error_message=ScriptExecutionException was caused by StreamAccessException.\\r\\n StreamAccessException was caused by AuthenticationException.\\r\\n Authentication failed for 'AzureBlob GetReference' operation at '[REDACTED]' with '403: AuthenticationFailed'. Please make sure the SAS token or the account key is correct.\\r\\n Failed due to inner exception of type: StorageException\\r\\n| session_id=session_id) ErrorCode: ScriptExecution.StreamAccess.Authentication\"\n}\n}",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1638194224630,
        "Question_favorite_count":13.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"The problem was solved by updating the account keys in the workspace.\naz ml workspace sync-keys -w mlw-kundenscore -g rg-datascience",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/644562\/datasetto-pandas-dataframe-throws-a-scriptexecutio.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-30T13:29:23.5Z",
                "Answer_score":1,
                "Answer_body":"The problem was solved by updating the account keys in the workspace.\naz ml workspace sync-keys -w mlw-kundenscore -g rg-datascience",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-11-30T01:44:50.843Z",
                "Answer_score":1,
                "Answer_body":"Hi, some suggestions include:\n\nCheck your Network and Firewall settings\n\n\nEnsure that you are providing the correct SAS token and in the correct format\n\n\nWhen generating a new SAS token, try to adjust your start time to be at least 15mins in the past (review Be careful with SAS start time)\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":1638278963500,
        "Question_original_content":"dataset panda datafram throw scriptexecut streamaccess authent error azur fail connect dataset cite inspit sa token appear try load data panda datafram dataset dataset work error messag error code usererror messag execut fail oper panda datafram dataset data dataset error code scriptexecut streamaccess authent error messag scriptexecutionexcept caus streamaccessexcept streamaccessexcept caus authenticationexcept authent fail azureblob getrefer oper redact authenticationfail sure sa token account kei correct fail inner except type storageexcept session session errorcod scriptexecut streamaccess authent",
        "Question_preprocessed_content":"throw error azur fail connect dataset cite inspit sa token appear try load data panda datafram dataset work error messag error",
        "Question_gpt_summary_original":"The user is encountering an error when trying to load data as a pandas dataframe using dataset.to_pandas_dataframe(). The error message indicates that Azure is failing to connect with the dataset due to a 403 error, despite having a valid SAS token. The error is specifically related to authentication for the 'AzureBlob GetReference' operation. The user is able to successfully retrieve the dataset using Dataset.get_by_name().",
        "Question_gpt_summary":"user encount error try load data panda datafram dataset panda datafram error messag indic azur fail connect dataset error despit have valid sa token error specif relat authent azureblob getrefer oper user abl successfulli retriev dataset dataset",
        "Answer_original_content":"problem solv updat account kei workspac workspac sync kei mlw kundenscor datasci",
        "Answer_preprocessed_content":"problem solv updat account kei workspac workspac",
        "Answer_gpt_summary_original":"Solution: The problem was solved by updating the account keys in the workspace using the command \"az ml workspace sync-keys -w mlw-kundenscore -g rg-datascience\".",
        "Answer_gpt_summary":"solut problem solv updat account kei workspac command workspac sync kei mlw kundenscor datasci"
    },
    {
        "Question_title":"Anyone trying backend-store-uri ?",
        "Question_body":"I am trying to use backend-store-uri for a mysql db.\nI pulled from master today and built wheel\n\n\n\u00a0 \u00a0 \u00a0 \u00a0python3.6 setup.py bdist_wheel\n\n\nI am using the wheel in this Dockerfile below.\n\n\nGetting 403 error when I go to localhost:5000\n\n\nI checked my db connections using a local Sqlachemy script and it works fine.\n\n\nAnyone trying backend-store-uri ?\n------------------------\n\n\nFROM centos:6\n\n\nENV LC_ALL=en_US.utf-8\nENV LANG=en_US.utf-8\n\n\nRUN yum update -y\nRUN yum install yum-utils -y\nRUN yum install -y https:\/\/centos6.iuscommunity.org\/ius-release.rpm\nRUN yum install -y python36u python36u-libs python36u-devel python36u-pip\n\n\nRUN yum install -y which gcc\n\n\nRUN ln -fs \/usr\/bin\/pip3.6 \/bin\/pip\nRUN ln -fs \/usr\/bin\/python3.6 \/usr\/bin\/python\n\n\nRUN python --version\nRUN pip --version\n\n\n\n\nENV TERM linux\nENV BUCKET #####\n\n\n#ENVs for mysql Aurora\nENV USERNAME #######\nENV PASSWORD #######\nENV HOST #######\nENV DATABASE #######\n\n\nCOPY mlflow-0.8.3.dev0-py3-none-any.whl .\/\nRUN pip install mlflow-0.8.3.dev0-py3-none-any.whl\n\n\nRUN mkdir -p \/mlflow\/\n\n\nEXPOSE 5000\n\n\nCMD mlflow server \\\n\u00a0 \u00a0 --backend-store-uri mysql:\/\/${USERNAME}:${PASSWORD}@${HOST}:${PORT}\/${DATABASE} \\\n\u00a0 \u00a0 --default-artifact-root s3:\/\/${BUCKET}\/mlflow-artifacts \\\n\u00a0 \u00a0 --host 0.0.0.0 --gunicorn-opts \"--access-logfile -\"",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_creation_time":1552644949000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":43.0,
        "Answer_body":"Hi\u00a0Paul,\n\n\nThe command seems to be set correctly. We have tested a local MySQL and RDS MySQL.\u00a0\n\n\nYou mentioned being able to connect with a SQLAlchemy script. Were you able to your Aurora instance from a local machine? Was the connection string format different? If you look at the SqlAlchemyStore constructor code, it passes in the exact argument connection string to create engine. If there is a different want to connect to Aurora, would be great if you could let us know so we can update the documentation and any internal code checks and setups, accordingly.\n\n\nThanks,\n\nMani Parkhe\n\nma...@databricks.com\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/69fa7001-ca1f-47b2-b584-2219e8b31f7b%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.. I am testing with an RDS MySQL Aurora DB.\n\n\nExample of creating table from local machine.\n\n\n\n\nimport sqlalchemy\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import Table, Column, Integer, String, MetaData, ForeignKey\nfrom sqlalchemy import inspect\n\n\nUSERNAME = '#####'\nPASSWORD = '#####'\nHOST = '#####'\nDATABASE = '#####'\nPORT = '3306'\n\n\nmetadata = MetaData()\nbooks = Table('book', metadata,\n\u00a0 Column('id', Integer, primary_key=True),\n\u00a0 Column('title', String(20)),\n\u00a0 Column('primary_author', String(20)),\n)\n\n\nconnection_string = 'mysql:\/\/' + USERNAME + ':' + PASSWORD + '@' + HOST + ':' + PORT + '\/' + DATABASE\n\n\nengine = create_engine(connection_string)\n\n\nmetadata.create_all(engine)\n\ue5d3. This matches how SqlAlchemyStore calls sqlalchemy.create_engine. Could this be related to IAM roles and ACLs on machine you are running the server? Can you try running this command on the same (local) machine which you ran this script on.\n\n\n\u00a0 \u00a0mlflow ui\u00a0--backend-store-uri\u00a0mysql:\/\/${USERNAME}:${PASSWORD}@${HOST}:${PORT}\/${DATABASE}\n\n\nand then pull up the UI that this command is listening to (most probably\u00a0http:\/\/127.0.0.1:5000). You may not have any data in the tables, but want to see if there are any errors that pop up when you run the above command.\n\n\nOne more question. Does that ${DATABASE} on Aurora? If not, does it help to manually create it. If I use the wrong name database name for a local MySQL -- I get this error message. Not sure if RDS\/Aurora drops that message.\n\n\n\u00a0 \u00a0 super(Connection, self).__init__(*args, **kwargs2)\nOperationalError: (_mysql_exceptions.OperationalError) (1049, \"Unknown database 'mlflow3'\") (Background on this error at: http:\/\/sqlalche.me\/e\/e3q8)\n\n\nReally appreciate you helping us test this.\n\n\n\nMani Parkhe\n\nma...@databricks.com\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/aa731c3c-d323-49fa-8e2a-7b3a1fe261b4%40googlegroups.com.\n\ue5d3. Yes, the DB is already in Aurora.\n\n\nOn the terminal...looks ok\n\n\nEXPORT USERNAME #####\nEXPORT PASSWORD #####\nEXPORT HOST #####\nEXPORT DATABASE #####\n\n\n$ mlflow ui --backend-store-uri mysql:\/\/${USERNAME}:${PASSWORD}@${HOST}:${PORT}\/${DATABASE}\n[2019-03-15 11:39:31 -0700] [18566] [INFO] Starting gunicorn 19.9.0\n[2019-03-15 11:39:31 -0700] [18566] [INFO] Listening at: http:\/\/127.0.0.1:5000 (18566)\n[2019-03-15 11:39:31 -0700] [18566] [INFO] Using worker: sync\n[2019-03-15 11:39:31 -0700] [18570] [INFO] Booting worker with pid: 18570\n\n\n\n\nBrowser at http:\/\/localhost:5000\n\n\nNot Found\nThe requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.\n\ue5d3. \ue5d3. I am in Python3.6 Anaconda venv.\npip install mlflow-0.8.3.dev0-py3-none-any.whl\n\n\nOn the terminal...looks ok\n\n\n$ EXPORT USERNAME #####\n$ EXPORT PASSWORD #####\n$ EXPORT HOST #####\n$ EXPORT DATABASE #####\n\n\n$ mlflow ui --backend-store-uri mysql:\/\/${USERNAME}:${PASSWORD}@${HOST}:${PORT}\/${DATABASE}\n[2019-03-15 11:39:31 -0700] [18566] [INFO] Starting gunicorn 19.9.0\n[2019-03-15 11:39:31 -0700] [18566] [INFO] Listening at: http:\/\/127.0.0.1:5000 (18566)\n[2019-03-15 11:39:31 -0700] [18566] [INFO] Using worker: sync\n[2019-03-15 11:39:31 -0700] [18570] [INFO] Booting worker with pid: 18570\n\n\n--------------------------------------------------\nBrowser at http:\/\/localhost:5000\n\n\nNot Found\nThe requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.\n\n\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/T6lt0HQxGr4",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-03-15T13:46:46",
                "Answer_body":"Hi\u00a0Paul,\n\n\nThe command seems to be set correctly. We have tested a local MySQL and RDS MySQL.\u00a0\n\n\nYou mentioned being able to connect with a SQLAlchemy script. Were you able to your Aurora instance from a local machine? Was the connection string format different? If you look at the SqlAlchemyStore constructor code, it passes in the exact argument connection string to create engine. If there is a different want to connect to Aurora, would be great if you could let us know so we can update the documentation and any internal code checks and setups, accordingly.\n\n\nThanks,\n\nMani Parkhe\n\nma...@databricks.com\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/69fa7001-ca1f-47b2-b584-2219e8b31f7b%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2019-03-15T14:02:43",
                "Answer_body":"I am testing with an RDS MySQL Aurora DB.\n\n\nExample of creating table from local machine.\n\n\n\n\nimport sqlalchemy\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import Table, Column, Integer, String, MetaData, ForeignKey\nfrom sqlalchemy import inspect\n\n\nUSERNAME = '#####'\nPASSWORD = '#####'\nHOST = '#####'\nDATABASE = '#####'\nPORT = '3306'\n\n\nmetadata = MetaData()\nbooks = Table('book', metadata,\n\u00a0 Column('id', Integer, primary_key=True),\n\u00a0 Column('title', String(20)),\n\u00a0 Column('primary_author', String(20)),\n)\n\n\nconnection_string = 'mysql:\/\/' + USERNAME + ':' + PASSWORD + '@' + HOST + ':' + PORT + '\/' + DATABASE\n\n\nengine = create_engine(connection_string)\n\n\nmetadata.create_all(engine)\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-03-15T14:17:27",
                "Answer_body":"This matches how SqlAlchemyStore calls sqlalchemy.create_engine. Could this be related to IAM roles and ACLs on machine you are running the server? Can you try running this command on the same (local) machine which you ran this script on.\n\n\n\u00a0 \u00a0mlflow ui\u00a0--backend-store-uri\u00a0mysql:\/\/${USERNAME}:${PASSWORD}@${HOST}:${PORT}\/${DATABASE}\n\n\nand then pull up the UI that this command is listening to (most probably\u00a0http:\/\/127.0.0.1:5000). You may not have any data in the tables, but want to see if there are any errors that pop up when you run the above command.\n\n\nOne more question. Does that ${DATABASE} on Aurora? If not, does it help to manually create it. If I use the wrong name database name for a local MySQL -- I get this error message. Not sure if RDS\/Aurora drops that message.\n\n\n\u00a0 \u00a0 super(Connection, self).__init__(*args, **kwargs2)\nOperationalError: (_mysql_exceptions.OperationalError) (1049, \"Unknown database 'mlflow3'\") (Background on this error at: http:\/\/sqlalche.me\/e\/e3q8)\n\n\nReally appreciate you helping us test this.\n\n\n\nMani Parkhe\n\nma...@databricks.com\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/aa731c3c-d323-49fa-8e2a-7b3a1fe261b4%40googlegroups.com.\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-03-15T14:43:58",
                "Answer_body":"Yes, the DB is already in Aurora.\n\n\nOn the terminal...looks ok\n\n\nEXPORT USERNAME #####\nEXPORT PASSWORD #####\nEXPORT HOST #####\nEXPORT DATABASE #####\n\n\n$ mlflow ui --backend-store-uri mysql:\/\/${USERNAME}:${PASSWORD}@${HOST}:${PORT}\/${DATABASE}\n[2019-03-15 11:39:31 -0700] [18566] [INFO] Starting gunicorn 19.9.0\n[2019-03-15 11:39:31 -0700] [18566] [INFO] Listening at: http:\/\/127.0.0.1:5000 (18566)\n[2019-03-15 11:39:31 -0700] [18566] [INFO] Using worker: sync\n[2019-03-15 11:39:31 -0700] [18570] [INFO] Booting worker with pid: 18570\n\n\n\n\nBrowser at http:\/\/localhost:5000\n\n\nNot Found\nThe requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-03-15T14:49:24",
                "Answer_body":"\ue5d3"
            },
            {
                "Answer_creation_time":"2019-03-15T14:53:32",
                "Answer_body":"I am in Python3.6 Anaconda venv.\npip install mlflow-0.8.3.dev0-py3-none-any.whl\n\n\nOn the terminal...looks ok\n\n\n$ EXPORT USERNAME #####\n$ EXPORT PASSWORD #####\n$ EXPORT HOST #####\n$ EXPORT DATABASE #####\n\n\n$ mlflow ui --backend-store-uri mysql:\/\/${USERNAME}:${PASSWORD}@${HOST}:${PORT}\/${DATABASE}\n[2019-03-15 11:39:31 -0700] [18566] [INFO] Starting gunicorn 19.9.0\n[2019-03-15 11:39:31 -0700] [18566] [INFO] Listening at: http:\/\/127.0.0.1:5000 (18566)\n[2019-03-15 11:39:31 -0700] [18566] [INFO] Using worker: sync\n[2019-03-15 11:39:31 -0700] [18570] [INFO] Booting worker with pid: 18570\n\n\n--------------------------------------------------\nBrowser at http:\/\/localhost:5000\n\n\nNot Found\nThe requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.\n\n\n\ue5d3"
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"try backend store uri try us backend store uri mysql pull master todai built wheel python setup bdist wheel wheel dockerfil get error localhost check connect local sqlachemi script work fine try backend store uri cento env utf env lang utf run yum updat run yum instal yum util run yum instal http cento iuscommun org iu releas rpm run yum instal pythonu pythonu lib pythonu devel pythonu pip run yum instal gcc run usr bin pip bin pip run usr bin python usr bin python run python version run pip version env term linux env bucket env mysql aurora env usernam env password env host env databas copi dev whl run pip instal dev whl run mkdir expos cmd server backend store uri mysql usernam password host port databas default artifact root bucket artifact host gunicorn opt access logfil",
        "Question_preprocessed_content":"try try us mysql pull master todai built wheel wheel dockerfil get error localhost check connect local sqlachemi script work fine try cento env env run yum updat run yum instal run yum instal run yum instal python run yum instal gcc run run run python run pip env term linux env bucket env mysql aurora env usernam env password env host env databas copi run pip instal run mkdir expos cmd server",
        "Question_gpt_summary_original":"The user is encountering a 403 error when trying to access localhost:5000 while using backend-store-uri for a MySQL database. The user has checked their database connections using a local Sqlalchemy script and it works fine. The user is seeking help from anyone who has tried backend-store-uri.",
        "Question_gpt_summary":"user encount error try access localhost backend store uri mysql databas user check databas connect local sqlalchemi script work fine user seek help tri backend store uri",
        "Answer_original_content":"hipaul command set correctli test local mysql rd mysql mention abl connect sqlalchemi script abl aurora instanc local machin connect string format differ look sqlalchemystor constructor code pass exact argument connect string creat engin differ want connect aurora great let know updat document intern code check setup accordingli thank mani parkh databrick com receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user caf ebfb googlegroup com option visit http group googl com optout test rd mysql aurora exampl creat tabl local machin import sqlalchemi sqlalchemi import creat engin sqlalchemi import tabl column integ string metadata foreignkei sqlalchemi import inspect usernam password host databas port metadata metadata book tabl book metadata column integ primari kei true column titl string column primari author string connect string mysql usernam password host port databas engin creat engin connect string metadata creat engin match sqlalchemystor call sqlalchemi creat engin relat iam role acl machin run server try run command local machin ran script backend store urimysql usernam password host port databas pull command listen probablyhttp data tabl want error pop run command question databas aurora help manual creat us wrong databas local mysql error messag sure rd aurora drop messag super connect self init arg kwarg operationalerror mysql except operationalerror unknown databas background error http sqlalch appreci help test mani parkh databrick com view discuss web visit http group googl com msgid user aacc bafeb googlegroup com ye aurora termin look export usernam export password export host export databas backend store uri mysql usernam password host port databas info start gunicorn info listen http info worker sync info boot worker pid browser http localhost request url server enter url manual check spell try python anaconda venv pip instal dev whl termin look export usernam export password export host export databas backend store uri mysql usernam password host port databas info start gunicorn info listen http info worker sync info boot worker pid browser http localhost request url server enter url manual check spell try",
        "Answer_preprocessed_content":"hipaul command set correctli test local mysql rd mysql mention abl connect sqlalchemi script abl aurora instanc local machin connect string format differ look sqlalchemystor constructor code pass exact argument connect string creat engin differ want connect aurora great let know updat document intern code check setup accordingli thank mani parkh receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit test rd mysql aurora exampl creat tabl local machin import sqlalchemi sqlalchemi import sqlalchemi import tabl column integ string metadata foreignkei sqlalchemi import inspect usernam password host databas port metadata metadata book tabl column string usernam password host port databas engin match sqlalchemystor call relat iam role acl machin run server try run command machin ran script pull command listen data tabl want error pop run command question aurora help manual creat us wrong databas local mysql error messag sure drop messag super kwarg operationalerror appreci help test mani parkh view discuss web visit ye aurora export usernam export password export host export databas start gunicorn listen worker sync boot worker pid browser request url server enter url manual check spell try anaconda venv pip instal export usernam export password export host export databas start gunicorn listen worker sync boot worker pid browser request url server enter url manual check spell try",
        "Answer_gpt_summary_original":"No solutions are provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"How do I use Service Principal authentication with an Azure Machine Learning Pipeline Endpoint in C#?",
        "Question_body":"<p>I'm trying to call an Azure Machine Learning Pipeline Endpoint I've set up using C# &amp; the Machine Learning REST api.<\/p>\n<p>I am certain that I have the Service Principal configured correctly, as I can successfully authenticate &amp; hit the endpoint using the <code>azureml-core<\/code> python sdk:<\/p>\n<pre><code>sp = ServicePrincipalAuthentication(\n    tenant_id=tenant_id,\n    service_principal_id=service_principal_id,\n    service_principal_password=service_principal_password)\nws =Workspace.get(\n    name=workspace_name, \n    resource_group=resource_group, \n    subscription_id=subscription_id, \n    auth=sp)\n\nendpoint = PipelineEndpoint.get(ws, name='MyEndpoint')\nendpoint.submit('Test_Experiment')\n<\/code><\/pre>\n<p>I'm using the following example in C# to attempt to run my endpoint: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-pipelines#run-a-published-pipeline-using-c\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-pipelines#run-a-published-pipeline-using-c<\/a><\/p>\n<p>I'm attempting to fill <code>auth_key<\/code> with the following code:<\/p>\n<pre><code>var clientId = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_ID&quot;);\nvar clientSecret = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_SECRET&quot;);\nvar tenantId = Environment.GetEnvironmentVariable(&quot;AZURE_TENANT_ID&quot;);\n\nvar cred = new ClientSecretCredential(tenantId, clientId, clientSecret);\nvar auth_key = cred.GetToken(new Azure.Core.TokenRequestContext(new string[] {&quot;.default&quot; }));\n<\/code><\/pre>\n<p>I receive a 401 (unauthorized).<\/p>\n<p>What am I am doing wrong?<\/p>\n<ul>\n<li>UPDATE *<\/li>\n<\/ul>\n<p>I changed the 'scopes' param in the <code>TokenRequestContext<\/code> to look like:<\/p>\n<pre><code>var auth_key = cred.GetToken(new Azure.Core.TokenRequestContext(new string[] { &quot;http:\/\/DataTriggerApp\/.default&quot; }));\n<\/code><\/pre>\n<p><code>http:\/\/DataTriggerApp<\/code> is one of the <code>servicePrincipalNames<\/code> that shows up when i query my Service Principal from the azure CLI.<\/p>\n<p>Now, when I attempt to use the returned token to call the Machine Learning Pipeline Endpoint, I receive a 403 instead of a 401.  Maybe some progress?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634153827710,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1634156473112,
        "Question_score":1.0,
        "Question_view_count":752.0,
        "Answer_body":"<p>Ok, through a lot of trial-and-error I was able to come up with two ways of acquiring a token that allows me to hit my Azure Machine Learning Pipeline Endpoint through the REST api.  One uses Microsoft.Identity.Client &amp; one uses Azure.Identity.<\/p>\n<pre><code>using Microsoft.Identity.Client;\n\n...\n\npublic static async Task&lt;string&gt; GetAccessToken()\n{\n      var clientId = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_ID&quot;);\n      var clientSecret = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_SECRET&quot;);\n      var tenantId = Environment.GetEnvironmentVariable(&quot;AZURE_TENANT_ID&quot;);\n\n   \n      var app = ConfidentialClientApplicationBuilder.Create(clientId)\n                                                .WithClientSecret(clientSecret)                                                \n                                                .WithAuthority(AzureCloudInstance.AzurePublic, tenantId)\n                                                .Build();\n      var result = await app.AcquireTokenForClient(new string[] { &quot;https:\/\/ml.azure.com\/.default&quot; }).ExecuteAsync();\n      return result.AccessToken;\n}\n<\/code><\/pre>\n<p>Or:<\/p>\n<pre><code>using Azure.Identity;\n...\n\npublic static async Task&lt;string&gt; GetAccessToken()\n{\n      var clientId = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_ID&quot;);\n      var clientSecret = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_SECRET&quot;);\n      var tenantId = Environment.GetEnvironmentVariable(&quot;AZURE_TENANT_ID&quot;);\n\n\n      var cred = new ClientSecretCredential(tenantId, clientId, clientSecret);\n      var token =  await cred.GetTokenAsync(new Azure.Core.TokenRequestContext(new string[] { &quot;https:\/\/ml.azure.com\/.default&quot; }));\n      return token.Token;\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1634160459928,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69561386",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1634160031172,
        "Question_original_content":"us servic princip authent pipelin endpoint try pipelin endpoint set machin learn rest api certain servic princip configur correctli successfulli authent hit endpoint core python sdk serviceprincipalauthent tenant tenant servic princip servic princip servic princip password servic princip password workspac workspac resourc group resourc group subscript subscript auth endpoint pipelineendpoint myendpoint endpoint submit test experi follow exampl attempt run endpoint http doc microsoft com azur machin learn deploi pipelin run publish pipelin attempt auth kei follow code var clientid environ getenvironmentvari azur client var clientsecret environ getenvironmentvari azur client secret var tenantid environ getenvironmentvari azur tenant var cred new clientsecretcredenti tenantid clientid clientsecret var auth kei cred gettoken new azur core tokenrequestcontext new string default receiv unauthor wrong updat chang scope param tokenrequestcontext look like var auth kei cred gettoken new azur core tokenrequestcontext new string http datatriggerapp default http datatriggerapp serviceprincipalnam show queri servic princip azur cli attempt us return token machin learn pipelin endpoint receiv instead mayb progress",
        "Question_preprocessed_content":"us servic princip authent pipelin endpoint try pipelin endpoint set machin learn rest api certain servic princip configur correctli successfulli authent hit endpoint python sdk follow exampl attempt run endpoint attempt follow code receiv wrong updat chang scope param look like show queri servic princip azur cli attempt us return token machin learn pipelin endpoint receiv instead mayb progress",
        "Question_gpt_summary_original":"The user is facing challenges in using Service Principal authentication with an Azure Machine Learning Pipeline Endpoint in C#. They have configured the Service Principal correctly and can successfully authenticate and hit the endpoint using the azureml-core python sdk. However, when attempting to run the endpoint in C#, they receive a 401 (unauthorized) error. They have tried changing the 'scopes' param in the TokenRequestContext to include one of the servicePrincipalNames that shows up when they query their Service Principal from the azure CLI, but now receive a 403 error instead of a 401.",
        "Question_gpt_summary":"user face challeng servic princip authent pipelin endpoint configur servic princip correctli successfulli authent hit endpoint core python sdk attempt run endpoint receiv unauthor error tri chang scope param tokenrequestcontext includ serviceprincipalnam show queri servic princip azur cli receiv error instead",
        "Answer_original_content":"lot trial error abl come wai acquir token allow hit pipelin endpoint rest api us microsoft ident client us azur ident microsoft ident client public static async task getaccesstoken var clientid environ getenvironmentvari azur client var clientsecret environ getenvironmentvari azur client secret var tenantid environ getenvironmentvari azur tenant var app confidentialclientapplicationbuild creat clientid withclientsecret clientsecret withauthor azurecloudinst azurepubl tenantid build var result await app acquiretokenforcli new string http azur com default executeasync return result accesstoken azur ident public static async task getaccesstoken var clientid environ getenvironmentvari azur client var clientsecret environ getenvironmentvari azur client secret var tenantid environ getenvironmentvari azur tenant var cred new clientsecretcredenti tenantid clientid clientsecret var token await cred gettokenasync new azur core tokenrequestcontext new string http azur com default return token token",
        "Answer_preprocessed_content":"lot abl come wai acquir token allow hit pipelin endpoint rest api us us",
        "Answer_gpt_summary_original":"The discussion provides two possible solutions to the challenge of using Service Principal authentication with an Azure Machine Learning Pipeline Endpoint in C#. The first solution uses Microsoft.Identity.Client and the second solution uses Azure.Identity. Both solutions involve acquiring a token that allows the user to hit the Azure Machine Learning Pipeline Endpoint through the REST API.",
        "Answer_gpt_summary":"discuss provid possibl solut challeng servic princip authent pipelin endpoint solut us microsoft ident client second solut us azur ident solut involv acquir token allow user hit pipelin endpoint rest api"
    },
    {
        "Question_title":"how to train and deploy YOLOv5 on aws sagemaker",
        "Question_body":"<p>I want to train YOLOv5 on aws sagemaker also deploy the model on sagemaker itself,need to know about entrypoint python script as well. how can I build a pipeline for this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1632686467053,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1632754422356,
        "Question_score":0.0,
        "Question_view_count":2510.0,
        "Answer_body":"<p>This official AWS <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/speed-up-yolov4-inference-to-twice-as-fast-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">blog post<\/a> has information on how to deploy YOLOv4. I wonder if you can use it as a guide and change the model to v5.<\/p>\n<p>If not, there is a 3rd party implementation of YOLOv5 <a href=\"https:\/\/github.com\/HKT-SSA\/yolov5-on-sagemaker\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69338516",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1632853607047,
        "Question_original_content":"train deploi yolov want train yolov deploi model need know entrypoint python script build pipelin",
        "Question_preprocessed_content":"train deploi yolov want train yolov deploi model need know entrypoint python script build pipelin",
        "Question_gpt_summary_original":"The user is facing challenges in training and deploying YOLOv5 on AWS Sagemaker. They are specifically seeking information on the entrypoint python script and how to build a pipeline for this process.",
        "Question_gpt_summary":"user face challeng train deploi yolov specif seek inform entrypoint python script build pipelin process",
        "Answer_original_content":"offici aw blog post inform deploi yolov wonder us guid chang model parti implement yolov",
        "Answer_preprocessed_content":"offici aw blog post inform deploi yolov wonder us guid chang model parti implement yolov",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Using the official AWS blog post on deploying YOLOv4 as a guide and modifying it for YOLOv5.\n2. Utilizing a third-party implementation of YOLOv5 available on GitHub.",
        "Answer_gpt_summary":"possibl solut mention discuss offici aw blog post deploi yolov guid modifi yolov util parti implement yolov avail github"
    },
    {
        "Question_title":"Log custom metrics for a run outside of the training loop",
        "Question_body":"<p>Hi,<br>\nI\u2019m currently working on a self-supervised representation learning project, and to evaluate the quality of my models I train a linear classifier on the outputs of my (frozen) trained encoder and look at the downstream classification accuracy.<\/p>\n<p>This evaluation procedure is done separately from the training of the encoder, however is there still a way to add the metrics computed during this evaluation phase to the standard metrics I log during the training phase, in the same run panel?<\/p>\n<p>More generally, can I add metrics to a run that is already finished?<\/p>\n<p>Thanks a lot!<\/p>",
        "Question_answer_count":9,
        "Question_comment_count":0,
        "Question_creation_time":1673613970984,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":97.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ari0u\">@ari0u<\/a> , appreciate your your additional feedback.<\/p>\n<p>This approach of first logging , <code>loss<\/code>, to a run, then revisiting\/resuming a run to log different metric, <code>accuracy<\/code>, starting from <strong>step zero<\/strong> again is not supported. The wandb logging step must be monotonically increasing in each call, otherwise the <code>step<\/code> value is ignored during your call to <code>log()<\/code>. Now if you are not interested in logging accuracy at step 0, you <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/resuming#resuming-guidance\">could resume<\/a> the previously finished run using its un id and log additional metrics to the run. this however is problematic as the new metric is logged starting at the last known\/registered step for the run.<\/p>\n<p>One approach to get around the issue you are running into  is to assign each of the runs to a <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/grouping\">specific group<\/a>. Example set <code>group = version_0<\/code> for any runs that logs metrics for this specific version of the model. You could then set grouping in the workspace to help with tracking  the different metrics for each experiment, <a href=\"https:\/\/wandb.ai\/mohammadbakir\/Group-Viz-Test\/groups\/L2\/workspace?workspace=user-mohammadbakir\">see this example workspace<\/a>.<\/p>\n<p>Hope this helps and please let us know if you have additional questions.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/log-custom-metrics-for-a-run-outside-of-the-training-loop\/3696",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-13T23:55:34.388Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ari0u\">@ari0u<\/a>, if I understood your question correctly, you are attempting to plot two separate metrics on a chart from two different runs?<\/p>\n<p>This can be done by adding a secondary Y metric key in the <code>Data<\/code>tab of a charts option panel.<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/9\/938472e0466c758a383d8da8813a7023d682b9df.png\" alt=\"ChartsDualY\" data-base62-sha1=\"l2ZXpyuWhP3yvh2waBauTKf4eED\" width=\"246\" height=\"131\"><\/p>\n<p>Please let me know if you have additional questions.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-16T11:55:14.339Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mohammadbakir\">@mohammadbakir<\/a>, thanks a lot for your reply! In fact, my problem is a bit different, let me try to describe it better:<\/p>\n<p>First I have a training phase during which metrics are plot, e.g. loss, etc. Let\u2019s say I have three runs <code>version_0<\/code>, <code>version_1<\/code> and <code>version_2<\/code>. For each metric I therefore have one chart in which there is one curve for each run. For example, I have a panel <code>loss<\/code> containing 3 curves, one for each of the version<\/p>\n<p>Then, I have a second phase, during which new metrics are computed, e.g. downstream accuracy. What I\u2019d like to do is therefore to have a new panel in my project called <code>accuracy<\/code> also with 3 curves, one for <code>version_0<\/code>, <code>version_1<\/code> and <code>version_2<\/code>. Moreover, these metrics should really be added to the same run created during the training phase, so that e.g metrics computed during both phases are displayed together on the params tables, both curves should become invisible if I untick the run, etc.<\/p>\n<p>However the tricky part here is that I really cannot compute the <code>accuracy<\/code> on-the-fly during the training phase since it\u2019s self-supervised learning, and if I would just log accuracy afterwards it would create a new run instead of adding the metric to the existing one, which would make things quite unreadable.<\/p>\n<p>I was therefore wondering if there is a way to somehow \u201creload\u201d a wandb logger for a specific run so that I can add the new metrics to this run even if it\u2019s already finished.<\/p>\n<p>Hope it\u2019s a bit clearer, thanks a lot!<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-19T00:18:59.911Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ari0u\">@ari0u<\/a> , appreciate your your additional feedback.<\/p>\n<p>This approach of first logging , <code>loss<\/code>, to a run, then revisiting\/resuming a run to log different metric, <code>accuracy<\/code>, starting from <strong>step zero<\/strong> again is not supported. The wandb logging step must be monotonically increasing in each call, otherwise the <code>step<\/code> value is ignored during your call to <code>log()<\/code>. Now if you are not interested in logging accuracy at step 0, you <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/resuming#resuming-guidance\">could resume<\/a> the previously finished run using its un id and log additional metrics to the run. this however is problematic as the new metric is logged starting at the last known\/registered step for the run.<\/p>\n<p>One approach to get around the issue you are running into  is to assign each of the runs to a <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/grouping\">specific group<\/a>. Example set <code>group = version_0<\/code> for any runs that logs metrics for this specific version of the model. You could then set grouping in the workspace to help with tracking  the different metrics for each experiment, <a href=\"https:\/\/wandb.ai\/mohammadbakir\/Group-Viz-Test\/groups\/L2\/workspace?workspace=user-mohammadbakir\">see this example workspace<\/a>.<\/p>\n<p>Hope this helps and please let us know if you have additional questions.<\/p>",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2023-01-19T10:20:17.567Z",
                "Answer_body":"<p>Hi,<br>\nThanks again for your help, it\u2019s a bit sad that my issue cannot be solved as is but the workaround you suggested with the groups looks fine.<br>\nDo you have any idea why logging has to be monotonically increasing in W&amp;B? Is there fundamental implementation constraints making it impossible, or is it just not implemented for now and could eventually be supported one day? Should I consider sending a feature request?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-19T11:01:40.267Z",
                "Answer_body":"<p>You can also use the W&amp;B API to update or add metrics to a run after it is finished.<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#update-config-for-an-existing-run\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/c\/c74534a2df0c93028c85f5aa85cbe3b185c39893.png\" class=\"site-icon\" width=\"132\" height=\"132\">\n\n      <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#update-config-for-an-existing-run\" target=\"_blank\" rel=\"noopener\">docs.wandb.ai<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/b\/bc8da779e3d547476a8cbe7e4bd3888df6f5ad36.png\" class=\"thumbnail onebox-avatar\" width=\"500\" height=\"500\" data-dominant-color=\"FFC924\">\n\n<h3><a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#update-config-for-an-existing-run\" target=\"_blank\" rel=\"noopener\">Import &amp; Export Data<\/a><\/h3>\n\n  <p>Best practices and common use cases for our public API to export data and update existing runs<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<pre><code class=\"lang-auto\">import wandb\napi = wandb.Api()\n\nrun = api.run(\"&lt;entity&gt;\/&lt;project&gt;\/&lt;run_id&gt;\")\nrun.config[\"key\"] = updated_value\nrun.update()\n<\/code><\/pre>\n<p>You could do this for each of versions you\u2019ve logged to attach your accuracy.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-24T13:03:02.048Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ari0u\">@ari0u<\/a>, I\u2019m jumping in here as Mohammad is out now. If you\u2019d like to log metrics to a previous step I can create a new feature request for this. Could you please share with me a little bit about your use-case? Thanks!<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-24T13:31:59.985Z",
                "Answer_body":"<p>Hi,<br>\nThat would be great, yes! My use case is to log so-called \u201cdownstream accuracy\u201d when working on self-supervised representation learning methods. The point of those methods is basically to train a neural network to learn semantic representations of some input data, i.e. map each data sample from a dataset to a vector in a latent space, in an unsupervised way, so without using any labels. The output vector corresponding to an input data is called its representation.<\/p>\n<p>Then, a common practice to evaluate the quality of the produced representations is to train a linear classifier with these representations as inputs on a supervised classification task. The idea here is that if the original encoder is able to produce semantically relevant representations, then classes are likely to be linearly separable. For example, for images, that would mean that e.g. all dogs from the dataset are mapped to vectors that are close from each other in the latent space, but far from the vectors that represent planes.<\/p>\n<p>However, because of the self-supervised setting, training the encoder that produces the representations and the linear classifier that learns to separate the classes are two different phases that are run independently, it is really a posteriori evaluation. Therefore to log the values in wandb I would need to be able to add to wandb the <code>downstream accuracy<\/code> metric after the training and log the values for this metric on past steps.<\/p>\n<p>In terms of wandb features, what I would require is to be able to use <code>wandb.log(..., step=my_step)<\/code> even if <code>my_step<\/code> is a past step. Since I know when I have to log past values, there could be a kwarg in <code>wandb.init<\/code> like <code>enable_log_past_steps<\/code>, which can be set to <code>True<\/code> to enable what I want to do but defaults to <code>False<\/code> to keep the current behaviour unless stated otherwise.<\/p>\n<p>Since self-supervised learning is becoming a more and more active field of research, I\u2019m sure that such feature would benefit to many people. Please do not hesitate to tell me if something is unclear in my explanations or if you want more details about the usage.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-24T13:37:33.235Z",
                "Answer_body":"<p>That could be kind a solution, but if I update the config that would mean that downstream accuracy would be a parameter instead of a metric, right? Then I\u2019d lose cool features like logging it for different epochs, or use it for computing parameter importances, etc.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-24T13:55:19.283Z",
                "Answer_body":"<p>Thank you very much <a class=\"mention\" href=\"\/u\/ari0u\">@ari0u<\/a>, this is a great feedback and I\u2019ll submit internally. Regarding your last question, this is correct, the accuracy would then be a parameter under the config and not a metric.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":1674087539911,
        "Question_original_content":"log custom metric run outsid train loop current work self supervis represent learn project evalu qualiti model train linear classifi output frozen train encod look downstream classif accuraci evalu procedur separ train encod wai add metric comput evalu phase standard metric log train phase run panel gener add metric run finish thank lot",
        "Question_preprocessed_content":"log custom metric run outsid train loop current work represent learn project evalu qualiti model train linear classifi output train encod look downstream classif accuraci evalu procedur separ train encod wai add metric comput evalu phase standard metric log train phase run panel gener add metric run finish thank lot",
        "Question_gpt_summary_original":"The user is facing challenges in logging custom metrics for a run outside of the training loop. They are working on a self-supervised representation learning project and evaluating the quality of their models by training a linear classifier on the outputs of their trained encoder. The evaluation procedure is done separately from the training of the encoder, and the user wants to know if there is a way to add the metrics computed during this evaluation phase to the standard metrics logged during the training phase in the same run panel. Additionally, the user wants to know if they can add metrics to a run that is already finished.",
        "Question_gpt_summary":"user face challeng log custom metric run outsid train loop work self supervis represent learn project evalu qualiti model train linear classifi output train encod evalu procedur separ train encod user want know wai add metric comput evalu phase standard metric log train phase run panel addition user want know add metric run finish",
        "Answer_original_content":"ariu appreci addit feedback approach log loss run revisit resum run log differ metric accuraci start step zero support log step monoton increas step valu ignor log interest log accuraci step resum previous finish run log addit metric run problemat new metric log start known regist step run approach issu run assign run specif group exampl set group version run log metric specif version model set group workspac help track differ metric experi exampl workspac hope help let know addit question",
        "Answer_preprocessed_content":"appreci addit feedback approach log run run log differ metric start step zero support log step monoton increas valu ignor interest log accuraci step resum previous finish run log addit metric run problemat new metric log start step run approach issu run assign run specif group exampl set run log metric specif version model set group workspac help track differ metric experi exampl workspac hope help let know addit question",
        "Answer_gpt_summary_original":"Solutions provided:\n- Resuming a previously finished run using its un id and logging additional metrics to the run.\n- Assigning each of the runs to a specific group to help with tracking different metrics for each experiment.",
        "Answer_gpt_summary":"solut provid resum previous finish run log addit metric run assign run specif group help track differ metric experi"
    },
    {
        "Question_title":"Saving an Matlabplot as an MLFlow artifact",
        "Question_body":"<p>I am using DataBricks and Spark 7.4ML,<\/p>\n<p>The following code successfully logs the params and metrics, and I can see the ROCcurve.png in the MLFLOW gui (just the item in the tree below the model). But the actually plot is blank. Why?<\/p>\n<pre><code>with mlflow.start_run(run_name=&quot;logistic-regression&quot;) as run:\n  pipeModel = pipe.fit(trainDF)\n  mlflow.spark.log_model(pipeModel, &quot;model&quot;)\n  predTest = pipeModel.transform(testDF)\n  predTrain = pipeModel.transform(trainDF)\n  evaluator=BinaryClassificationEvaluator(labelCol=&quot;arrivedLate&quot;)\n  trainROC = evaluator.evaluate(predTrain)\n  testROC = evaluator.evaluate(predTest)\n  print(f&quot;Train ROC: {trainROC}&quot;)\n  print(f&quot;Test ROC: {testROC}&quot;)\n  mlflow.log_param(&quot;Dataset Name&quot;, &quot;Flights &quot; + datasetName)\n  mlflow.log_metric(key=&quot;Train ROC&quot;, value=trainROC)\n  mlflow.log_metric(key=&quot;Test ROC&quot;, value=testROC)\n\n  lrModel = pipeModel.stages[3]\n  trainingSummary = lrModel.summary\n  roc = trainingSummary.roc.toPandas()\n  plt.plot(roc['FPR'],roc['TPR'])\n  plt.ylabel('False Positive Rate')\n  plt.xlabel('True Positive Rate')\n  plt.title('ROC Curve')\n  plt.show()\n  plt.savefig(&quot;ROCcurve.png&quot;)\n  mlflow.log_artifact(&quot;ROCcurve.png&quot;)\n  plt.close()\n  \n  display(predTest.select(stringCols + [&quot;arrivedLate&quot;, &quot;prediction&quot;]))\n<\/code><\/pre>\n<p>What the notebook shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What the MLFlow shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1607094596640,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1607191847983,
        "Question_score":8.0,
        "Question_view_count":5219.0,
        "Answer_body":"<p>Put <code>plt.show()<\/code> after <code>plt.savefig()<\/code> - <code>plt.show()<\/code> will remove your plot because it is shown already.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":7.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65145994",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1607094854147,
        "Question_original_content":"save matlabplot artifact databrick spark follow code successfulli log param metric roccurv png gui item tree model actual plot blank start run run logist regress run pipemodel pipe fit traindf spark log model pipemodel model predtest pipemodel transform testdf predtrain pipemodel transform traindf evalu binaryclassificationevalu labelcol arrivedl trainroc evalu evalu predtrain testroc evalu evalu predtest print train roc trainroc print test roc testroc log param dataset flight datasetnam log metric kei train roc valu trainroc log metric kei test roc valu testroc lrmodel pipemodel stage trainingsummari lrmodel summari roc trainingsummari roc topanda plt plot roc fpr roc tpr plt ylabel fals posit rate plt xlabel true posit rate plt titl roc curv plt plt savefig roccurv png log artifact roccurv png plt close displai predtest select stringcol arrivedl predict notebook show show",
        "Question_preprocessed_content":"save matlabplot artifact databrick spark follow code successfulli log param metric gui actual plot blank notebook show show",
        "Question_gpt_summary_original":"The user is facing a challenge in saving a Matplotlib plot as an MLFlow artifact. The code successfully logs the parameters and metrics, and the ROC curve is visible in the MLFlow GUI, but the actual plot is blank. The user is seeking assistance in resolving this issue.",
        "Question_gpt_summary":"user face challeng save matplotlib plot artifact code successfulli log paramet metric roc curv visibl gui actual plot blank user seek assist resolv issu",
        "Answer_original_content":"plt plt savefig plt remov plot shown",
        "Answer_preprocessed_content":"remov plot shown",
        "Answer_gpt_summary_original":"Solution: The solution mentioned in the discussion is to add the line of code \"plt.show()\" after \"plt.savefig()\" to resolve the issue of the blank plot being saved as an MLFlow artifact.",
        "Answer_gpt_summary":"solut solut mention discuss add line code plt plt savefig resolv issu blank plot save artifact"
    },
    {
        "Question_title":"ClearML how to change clearml.conf file in AWS Sagemaker",
        "Question_body":"<p>I am working in AWS Sagemaker Jupyter notebook.\nI have installed clearml package in AWS Sagemaker in Jupyter.\nClearML server was installed on AWS EC2.\nI need to store artifacts and models in AWS S3 bucket, so I want to specify credentials to S3 in clearml.conf file.\nHow can I change clearml.conf file in AWS Sagemaker instance? looks like permission denied to all folders on it.\nOr maybe somebody can suggest a better approach.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1613428648627,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":276.0,
        "Answer_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p>To set credentials (and <code>clearml-server<\/code> hosts) you can use <code>Task.set_credentials<\/code>.\nTo specify the S3 bucket as output for all artifacts (and debug images for that matter) you can just set it as the <code>files_server<\/code>.<\/p>\n<p>For example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\n\nTask.set_credentials(api_host='http:\/\/clearml-server:8008', web_host='http:\/\/clearml-server:8080', files_host='s3:\/\/my_bucket\/folder\/',\nkey='add_clearml_key_here', secret='add_clearml_key_secret_here')\n<\/code><\/pre>\n<p>To pass your S3 credentials, just add a cell at the top of your jupyter notebook, and set the standard AWS S3 environment variables:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nos.environ['AWS_ACCESS_KEY_ID'] = 's3_bucket_key_here'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 's3_bucket_secret_here'\n# optional\nos.environ['AWS_DEFAULT_REGION'] = 's3_bucket_region'\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66216294",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1613774515323,
        "Question_original_content":"chang conf file work jupyt notebook instal packag jupyt server instal aw need store artifact model aw bucket want specifi credenti conf file chang conf file instanc look like permiss deni folder mayb somebodi suggest better approach",
        "Question_preprocessed_content":"chang conf file work jupyt notebook instal packag jupyt server instal aw need store artifact model aw bucket want specifi credenti conf file chang conf file instanc look like permiss deni folder mayb somebodi suggest better approach",
        "Question_gpt_summary_original":"The user is facing challenges in changing the clearml.conf file in AWS Sagemaker Jupyter notebook to specify credentials to S3 for storing artifacts and models. The user is encountering permission denied errors while attempting to change the file and is seeking suggestions for a better approach.",
        "Question_gpt_summary":"user face challeng chang conf file jupyt notebook specifi credenti store artifact model user encount permiss deni error attempt chang file seek suggest better approach",
        "Answer_original_content":"disclaim train team set credenti server host us task set credenti specifi bucket output artifact debug imag matter set file server exampl import task task set credenti api host http server web host http server file host bucket folder kei add kei secret add kei secret pass credenti add cell jupyt notebook set standard aw environ variabl import environ aw access kei bucket kei environ aw secret access kei bucket secret option environ aw default region bucket region",
        "Answer_preprocessed_content":"disclaim team set credenti us specifi bucket output artifact set exampl pass credenti add cell jupyt notebook set standard aw environ variabl",
        "Answer_gpt_summary_original":"Solution:\n- Use `Task.set_credentials` to set credentials and `files_server` to specify the S3 bucket as output for all artifacts.\n- Add a cell at the top of the Jupyter notebook and set the standard AWS S3 environment variables to pass S3 credentials.",
        "Answer_gpt_summary":"solut us task set credenti set credenti file server specifi bucket output artifact add cell jupyt notebook set standard aw environ variabl pass credenti"
    },
    {
        "Question_title":"Sagemaker Hyperparameter Tuning Job Mechanism",
        "Question_body":"<p>Does anyone know what's the mechanism behind hyperparameter tuning job in AWS Sagemaker?<\/p>\n<p>In specific, I am trying to do the following:<\/p>\n<ol>\n<li>Bring my own container<\/li>\n<li>Minimize cross entropy loss (this will be the objective metric of the tuner)<\/li>\n<\/ol>\n<p>My question is when we define the hyper parameter in <code>HyperParameterTuner<\/code> class, does that get copied into <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code>?<\/p>\n<p>If so, should one adjust the training image so that it uses the hyper parameters from <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code>?<\/p>\n<p>Edit: I've looked into some sample HPO notebooks that AWS provides and they seem to confuse me more. Sometimes they'd use <code>argparser<\/code> to pass in the HPs. How is that passed into the training code?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1644569855830,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1644570751860,
        "Question_score":1.0,
        "Question_view_count":281.0,
        "Answer_body":"<p>So i finally figured it out and had it wrong all the time.<\/p>\n<p>The file <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code> is there. It just has slightly different content compared to a regular training-job. The params to be tuned as well as static params are contained there. As well as the metric-name.<\/p>\n<p>So here is the structure, i hope it helps:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    '_tuning_objective_metric': 'your-metric', \n    'dynamic-param1': '0.3', \n    'dynamic-param2': '1',\n    'static-param1': 'some-value', \n    'static-paramN': 'another-value'\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71077397",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1660920166467,
        "Question_original_content":"hyperparamet tune job mechan know mechan hyperparamet tune job specif try follow bring contain minim cross entropi loss object metric tuner question defin hyper paramet hyperparametertun class copi opt input config hyperparamet json adjust train imag us hyper paramet opt input config hyperparamet json edit look sampl hpo notebook aw provid confus us argpars pass hp pass train code",
        "Question_preprocessed_content":"hyperparamet tune job mechan know mechan hyperparamet tune job specif try follow bring contain minim cross entropi loss question defin hyper paramet class copi adjust train imag us hyper paramet edit look sampl hpo notebook aw provid confus us pass hp pass train code",
        "Question_gpt_summary_original":"The user is seeking information about the mechanism behind hyperparameter tuning job in AWS Sagemaker. They are specifically trying to bring their own container and minimize cross entropy loss. They are unsure if the hyperparameters defined in the HyperParameterTuner class get copied into \/opt\/ml\/input\/config\/hyperparameters.json and whether they should adjust the training image to use the hyperparameters from that file. The user is also confused about how hyperparameters are passed into the training code, as some sample HPO notebooks use argparser to pass in the hyperparameters.",
        "Question_gpt_summary":"user seek inform mechan hyperparamet tune job specif try bring contain minim cross entropi loss unsur hyperparamet defin hyperparametertun class copi opt input config hyperparamet json adjust train imag us hyperparamet file user confus hyperparamet pass train code sampl hpo notebook us argpars pass hyperparamet",
        "Answer_original_content":"final figur wrong time file opt input config hyperparamet json slightli differ content compar regular train job param tune static param contain metric structur hope help tune object metric metric dynam param dynam param static param valu static paramn valu",
        "Answer_preprocessed_content":"final figur wrong time file slightli differ content compar regular param tune static param contain structur hope help",
        "Answer_gpt_summary_original":"Solution:\n- The file \/opt\/ml\/input\/config\/hyperparameters.json contains the parameters to be tuned as well as static parameters and the metric name.\n- The HyperParameterTuner class defines the hyperparameters to be tuned and the metric to optimize for.\n- The hyperparameters can be passed into the training code using argparser.",
        "Answer_gpt_summary":"solut file opt input config hyperparamet json contain paramet tune static paramet metric hyperparametertun class defin hyperparamet tune metric optim hyperparamet pass train code argpars"
    },
    {
        "Question_title":"Replacing values in dataset within Azure Machine Learning Studio",
        "Question_body":"<p>In Azure Machine Learning studio I need to convert a column of data that has three categorical values 'yes', 'no' and 'maybe', and wish to combine the 'no' and 'maybe' values as just 'no'. <\/p>\n\n<p>I can do this easily using SQL, R, or Python but for these purposes I need to show if it is possible to do this without using these languages. I can't seem to find a way to do this. <\/p>\n\n<p>Does anyone have any ideas? I'm fine if the answer is no but I don't want to say it's not possible if it is. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1527572052450,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":484.0,
        "Answer_body":"<p>It can be done! :)<\/p>\n\n<p>You would just use the \"Group Categorical Values\" module. Choose the column that has the data you want to group, and you can set the values like the following:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/UGhrR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/UGhrR.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>What's going on here is that the default, which will get used if the other levels aren't caught, is set to \"yes\". Then when any values are \"no\", or \"maybe\", it gets grouped into a category of \"no\".<\/p>\n\n<p>However, this will error unless you make that column a categorical type, so you would need to use the \"Edit Metadata\" module to do that.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/45s2Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/45s2Q.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The example I used is <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Replace-Values-in-Dataset\" rel=\"nofollow noreferrer\">published to the gallery<\/a>, if you need to reference it.<\/p>\n\n<p>If you need more info, just let me know.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50576929",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1527597469392,
        "Question_original_content":"replac valu dataset studio studio need convert column data categor valu ye mayb wish combin mayb valu easili sql python purpos need possibl languag wai idea fine answer want possibl",
        "Question_preprocessed_content":"replac valu dataset studio studio need convert column data categor valu ye mayb wish combin mayb valu easili sql python purpos need possibl languag wai idea fine answer want possibl",
        "Question_gpt_summary_original":"The user is facing a challenge in replacing values in a dataset within Azure Machine Learning Studio. They need to combine the 'no' and 'maybe' values in a column of categorical data as just 'no', but they are unable to find a way to do this without using SQL, R, or Python. They are seeking ideas or solutions to accomplish this task within Azure Machine Learning Studio.",
        "Question_gpt_summary":"user face challeng replac valu dataset studio need combin mayb valu column categor data unabl wai sql python seek idea solut accomplish task studio",
        "Answer_original_content":"us group categor valu modul choos column data want group set valu like follow go default level aren caught set ye valu mayb get group categori error column categor type need us edit metadata modul exampl publish galleri need refer need info let know",
        "Answer_preprocessed_content":"us group categor valu modul choos column data want group set valu like follow go default level aren caught set ye valu mayb get group categori error column categor type need us edit metadata modul exampl publish galleri need refer need info let know",
        "Answer_gpt_summary_original":"Solution: The user can use the \"Group Categorical Values\" module in Azure Machine Learning Studio to combine the 'no' and 'maybe' values in a column of categorical data as just 'no'. They need to choose the column that has the data they want to group and set the values accordingly. However, they need to make sure that the column is a categorical type, which can be done using the \"Edit Metadata\" module. An example experiment is also available in the gallery for reference.",
        "Answer_gpt_summary":"solut user us group categor valu modul studio combin mayb valu column categor data need choos column data want group set valu accordingli need sure column categor type edit metadata modul exampl experi avail galleri refer"
    },
    {
        "Question_title":"Azure Machine Learning Services - AutoMl - Error running experiment.submit: \"\/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\"",
        "Question_body":"Hello,\n\nI created a notebook in the workspace and when I sent the experiment for training I received error message undefined symbol: XGDMatrixSetDenseInfo for algorithm Xgboost. Do you know how to fix the problem?\n\n\n\n\nAzure ML Version: 1.22.0\nCompute Instance: Standard_DS3_v2\n\nCode:\n\nimport logging\nfrom azureml.train.automl import AutoMLConfig\nfrom azureml.core.experiment import Experiment\n\nautoml_settings = {\n\"iteration_timeout_minutes\": 10,\n\"experiment_timeout_hours\": 0.3,\n\"enable_early_stopping\": True,\n\"primary_metric\": 'normalized_root_mean_squared_error',\n\"featurization\": 'auto',\n\"verbosity\": logging.INFO,\n\"n_cross_validations\": 5\n}\n\nautoml_config = AutoMLConfig(task='regression',\ndebug_log='automated_ml_errors.log',\ntraining_data=x_train,\nlabel_column_name=\"production_time\",\n**automl_settings)\n\nexperiment = Experiment(ws, \"train-model\")\nlocal_run = experiment.submit(automl_config, show_output=True)\n\nFull Error Message:\n\n\n\n\nERROR: FitException:\nMessage: \/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\nInnerException: AttributeError: \/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\nErrorResponse\n{\n\"error\": {\n\"code\": \"SystemError\",\n\"message\": \"Encountered an internal AutoML error. Error Message\/Code: FitException. Additional Info: FitException:\\n\\tMessage: \/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\\n\\tInnerException: None\\n\\tErrorResponse \\n{\\n \\\"error\\\": {\\n \\\"message\\\": \\\"\/anaconda\/envs\/azureml_py36\/lib\/libxgboost.so: undefined symbol: XGDMatrixSetDenseInfo\\\",\\n \\\"target\\\": \\\"Xgboost\\\",\\n \\\"reference_code\\\": \\\"Xgboost\\\"\\n }\\n}\",\n\"details_uri\": \"https:\/\/docs.microsoft.com\/azure\/machine-learning\/resource-known-issues#automated-machine-learning\",\n\"target\": \"Xgboost\",\n\"inner_error\": {\n\"code\": \"ClientError\",\n\"inner_error\": {\n\"code\": \"AutoMLInternal\"\n}\n},\n\"reference_code\": \"Xgboost\"\n}\n}\n\n\n\n\n\nBest regards,\nCristina\n\n\n\n\n\n\n\n\n79658-packages.txt",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616170731780,
        "Question_favorite_count":7.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, can you try uninstalling and reinstalling Xgboost (try versions <= 0.90 if you continue to get errors).",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/322886\/azure-machine-learning-services-automl-error-runni.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-19T23:29:59.997Z",
                "Answer_score":0,
                "Answer_body":"Hi, can you try uninstalling and reinstalling Xgboost (try versions <= 0.90 if you continue to get errors).",
                "Answer_comment_count":3,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1616196599996,
        "Question_original_content":"servic automl error run experi submit anaconda env lib libxgboost undefin symbol xgdmatrixsetdenseinfo hello creat notebook workspac sent experi train receiv error messag undefin symbol xgdmatrixsetdenseinfo algorithm xgboost know fix problem version comput instanc standard code import log train automl import automlconfig core experi import experi automl set iter timeout minut experi timeout hour enabl earli stop true primari metric normal root mean squar error featur auto verbos log info cross valid automl config automlconfig task regress debug log autom error log train data train label column product time automl set experi experi train model local run experi submit automl config output true error messag error fitexcept messag anaconda env lib libxgboost undefin symbol xgdmatrixsetdenseinfo innerexcept attributeerror anaconda env lib libxgboost undefin symbol xgdmatrixsetdenseinfo errorrespons error code systemerror messag encount intern automl error error messag code fitexcept addit info fitexcept tmessag anaconda env lib libxgboost undefin symbol xgdmatrixsetdenseinfo tinnerexcept terrorrespons error messag anaconda env lib libxgboost undefin symbol xgdmatrixsetdenseinfo target xgboost refer code xgboost detail uri http doc microsoft com azur machin learn resourc known issu autom machin learn target xgboost inner error code clienterror inner error code automlintern refer code xgboost best regard cristina packag txt",
        "Question_preprocessed_content":"servic automl error run undefin symbol xgdmatrixsetdenseinfo hello creat notebook workspac sent experi train receiv error messag undefin symbol xgdmatrixsetdenseinfo algorithm xgboost know fix problem version comput instanc code import log import automlconfig import experi automlconfig experi experi error messag error fitexcept messag undefin symbol xgdmatrixsetdenseinfo innerexcept attributeerror undefin symbol xgdmatrixsetdenseinfo errorrespons error target xgboost xgboost best regard cristina",
        "Question_gpt_summary_original":"The user encountered an error message \"undefined symbol: XGDMatrixSetDenseInfo\" while running an experiment for training in Azure Machine Learning Services AutoML with Xgboost algorithm. The error occurred during the execution of the code and the user is seeking help to fix the problem.",
        "Question_gpt_summary":"user encount error messag undefin symbol xgdmatrixsetdenseinfo run experi train servic automl xgboost algorithm error occur execut code user seek help fix problem",
        "Answer_original_content":"try uninstal reinstal xgboost try version continu error",
        "Answer_preprocessed_content":"try uninstal reinstal xgboost",
        "Answer_gpt_summary_original":"Solution: The user can try uninstalling and reinstalling Xgboost, and if the error persists, they can try using versions of Xgboost that are less than or equal to 0.90.",
        "Answer_gpt_summary":"solut user try uninstal reinstal xgboost error persist try version xgboost equal"
    },
    {
        "Question_title":"WandB not using user PID when updating",
        "Question_body":"<p>Hello,<\/p>\n<p>I used  <code>tempfile.mkdtemp() <\/code> to create a temporary directory for my runs (as I don\u2019t want a persistent folder with tons of runs)<\/p>\n<p>For training everything works fine but when resuming the run to do some validation \/ evaluation updates, and using <code>run.summary.update({\"key\": value})<\/code> I got a<\/p>\n<pre><code class=\"lang-auto\">wandb: WARNING Path \/tmp\/tmpq5uafy4d\/wandb\/ wasn't writable, using system temp directory\n<\/code><\/pre>\n<p>with obviously<\/p>\n<pre><code class=\"lang-auto\">File \"\/mnt\/Projets\/nlp\/.venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 855, in _update_summary\n    with open(summary_path, \"w\") as f:\nFileNotFoundError: [Errno 2] No such file or directory: '\/tmp\/tmpq5uafy4d\/wandb\/run-20211102_153311-37264m5k\/files\/wandb-summary.json'\n<\/code><\/pre>\n<p>As in the doc of <a href=\"https:\/\/docs.python.org\/3.9\/library\/tempfile.html#tempfile.mkdtemp\" rel=\"noopener nofollow ugc\"><code>mkdtemp<\/code><\/a> :<\/p>\n<pre><code class=\"lang-auto\"> The directory is readable, writable, and searchable only by the creating user ID.\n<\/code><\/pre>\n<p>So I guess WandB is not using the user ID and thus is not able to write in the directory for updating.<br>\nNote that this directory is different from the training one (as it\u2019s random at each init)<\/p>\n<p>Thanks in advance for any help.<br>\nHave a great day.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1635882593147,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":289.0,
        "Answer_body":"<p>As of now, there isn\u2019t an option to enable that by default. One issue you should be aware of is that if you are currently logging a run when you call <code>wandb sync --clean<\/code> bad things will happen. We\u2019re working on improving the the robustness of these features and will likely support an automatic clean option in the future.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-not-using-user-pid-when-updating\/1204",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-03T15:31:05.874Z",
                "Answer_body":"<p>For the posterity :<\/p>\n<p>After searching for a long time with it, wandb firstly deletes my class object (which call deletion of the temp folder) <strong>and then<\/strong> try to update the run.<\/p>\n<p>To avoid that you need to first call <code>run.finish() or wandb.finish()<\/code> which will first update and then delete your object (or let the garbage collector do it)  thus it will be synced before the object is destroyed and the temp file removed.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-03T16:45:59.689Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ierezell\">@ierezell<\/a>, there are some reasons why you don\u2019t want to automatically delete your run folders, especially if there is an issue with the run. However, I do understand your desire to manage the clutter in your file system. It sounds like having a feature where you could tell wandb to delete the local files it created after a successful run would be the most preferable option for you. Currently, calling <code>wandb sync --clean<\/code> will sync any unsynced runs and then remove those run folders from your computer. The wandb dir will still be there and there but it will only be taking up bytes of space as most of the information will have been deleted. If you still find it annoying to have mostly empty <code>.\/wandb<\/code> folders in your project dirs, you can set the <code>WANDB_DIR<\/code> environment variable to an absolute path where all of your run data will be stored until you call <code>wandb sync --clean<\/code>.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-03T18:08:15.337Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/aidanjd\">@aidanjd<\/a>,<\/p>\n<p>I didn\u2019t know the <code>wandb sync --clean<\/code> option!<br>\nIt\u2019s quite what I wanted to do, saving space and your solution will only delete finished runs which is nice <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>I don\u2019t mind deleting failed run as I can just relaunch them again (it\u2019s small models on only one machine). This is why I put all my wandb folders in <code>\/tmp<\/code> which means I keep the folder until I reboot.<\/p>\n<p>Is there any option to have the <code>clean<\/code> feature enabled by default? I mean for any run if sync is complete: delete the folder.<\/p>\n<p>Thanks for the response, I guess it solves it but we can continue discussing it.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-03T18:21:20.198Z",
                "Answer_body":"<p>As of now, there isn\u2019t an option to enable that by default. One issue you should be aware of is that if you are currently logging a run when you call <code>wandb sync --clean<\/code> bad things will happen. We\u2019re working on improving the the robustness of these features and will likely support an automatic clean option in the future.<\/p>",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-01-02T18:22:08.008Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":1635963680198,
        "Question_original_content":"user pid updat hello tempfil mkdtemp creat temporari directori run dont want persist folder ton run train work fine resum run valid evalu updat run summari updat kei valu got warn path tmp tmpquafyd wasn writabl temp directori obvious file mnt projet nlp venv lib python site packag sdk intern sender line updat summari open summari path filenotfounderror errno file directori tmp tmpquafyd run file summari json doc mkdtemp directori readabl writabl searchabl creat user guess user abl write directori updat note directori differ train random init thank advanc help great dai",
        "Question_preprocessed_content":"user pid updat hello creat temporari directori run train work fine resum run valid evalu updat got obvious doc guess user abl write directori updat note directori differ train thank advanc help great dai",
        "Question_gpt_summary_original":"The user encountered a challenge when using WandB to update their runs. They created a temporary directory using tempfile.mkdtemp() for their runs, but when resuming the run to do some validation\/evaluation updates and using run.summary.update({\"key\": value}), they received a warning that the path wasn't writable, and a FileNotFoundError occurred. The user suspects that WandB is not using the user ID and thus is not able to write in the directory for updating.",
        "Question_gpt_summary":"user encount challeng updat run creat temporari directori tempfil mkdtemp run resum run valid evalu updat run summari updat kei valu receiv warn path wasn writabl filenotfounderror occur user suspect user abl write directori updat",
        "Answer_original_content":"isnt option enabl default issu awar current log run sync clean bad thing happen work improv robust featur like support automat clean option futur",
        "Answer_preprocessed_content":"isnt option enabl default issu awar current log run bad thing happen work improv robust featur like support automat clean option futur",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Azure ML inference pipeline deployment authorization token error",
        "Question_body":"<p>When deploying a real-time inferencing pipeline in Azure ML (as per <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy#deploy-the-real-time-endpoint\" rel=\"nofollow noreferrer\">this<\/a> tutorial), I receive the below error. I've tried forcibly logging out using OAuth. Tried creating a new Azure workspace but continue to receive the same error.<\/p>\n\n<p>It looks like the tenant id causing the problem is example.onmicrosoft.com (72f988bf-86f1-41af-91ab-2d7cd011db47)<\/p>\n\n<hr>\n\n<p><em>Deploy: Failed on step CreateServiceFromModels. Details: AzureML service API error. Error calling ServiceCreate: {\"code\":\"Unauthorized\",\"statusCode\":401,\"message\":\"Unauthorized\",\"details\":[{\"code\":\"EmptyOrInvalidToken\",\"message\":\"Error: Service invocation failed!\\r\\nRequest: GET <a href=\"https:\/\/management.azure.com\/subscriptions\/subscription_id\/resourceGroups\/dev-rg\/providers\/Microsoft.MachineLearningServices\/workspaces\/dev-ws\/providers\/Microsoft.Authorization\/permissions?api-version=2015-07-01\" rel=\"nofollow noreferrer\">https:\/\/management.azure.com\/subscriptions\/subscription_id\/resourceGroups\/dev-rg\/providers\/Microsoft.MachineLearningServices\/workspaces\/dev-ws\/providers\/Microsoft.Authorization\/permissions?api-version=2015-07-01<\/a>\\r\\nStatus Code: 401 Unauthorized\\r\\nReason Phrase: Unauthorized\\r\\nResponse Body: {\\\"error\\\":{\\\"code\\\":\\\"InvalidAuthenticationTokenTenant\\\",\\\"message\\\":\\\"The access token is from the wrong issuer '<a href=\"https:\/\/sts.windows.net\/72f988bf-86f1-41af-91ab-2d7cd011db47\/\" rel=\"nofollow noreferrer\">https:\/\/sts.windows.net\/72f988bf-86f1-41af-91ab-2d7cd011db47\/<\/a>'. It must match the tenant '<a href=\"https:\/\/sts.windows.net\/correct_tenant_id\/\" rel=\"nofollow noreferrer\">https:\/\/sts.windows.net\/correct_tenant_id\/<\/a>' associated with this subscription. Please use the authority (URL) '<a href=\"https:\/\/login.windows.net\/correct_tenant_id\" rel=\"nofollow noreferrer\">https:\/\/login.windows.net\/correct_tenant_id<\/a>' to get the token. Note, if the subscription is transferred to another tenant there i<\/em><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1582302156893,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1582395411243,
        "Question_score":2.0,
        "Question_view_count":481.0,
        "Answer_body":"<p>I appear to have had User Access Administrator role only (in addition to Classic Service Administrator). As soon as I added myself to the Owner role in the Access Control (IAM) section of the Azure Portal, the deployment succeeded.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60342645",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1582556597823,
        "Question_original_content":"infer pipelin deploy author token error deploi real time inferenc pipelin tutori receiv error tri forcibl log oauth tri creat new azur workspac continu receiv error look like tenant caus problem exampl onmicrosoft com fbf dcddb deploi fail step createservicefrommodel detail servic api error error call servicecr code unauthor statuscod messag unauthor detail code emptyorinvalidtoken messag error servic invoc fail nrequest http manag azur com subscript subscript resourcegroup dev provid microsoft machinelearningservic workspac dev provid microsoft author permiss api version nstatu code unauthor nreason phrase unauthor nrespons bodi error code invalidauthenticationtokenten messag access token wrong issuer http st window net fbf dcddb match tenant http st window net correct tenant associ subscript us author url http login window net correct tenant token note subscript transfer tenant",
        "Question_preprocessed_content":"infer pipelin deploy author token error deploi inferenc pipelin receiv error tri forcibl log oauth tri creat new azur workspac continu receiv error look like tenant caus problem deploi fail step createservicefrommodel detail servic api error error call servicecr code unauthor statuscod messag unauthor detail code emptyorinvalidtoken messag error servic invoc code phrase bodi access token wrong issuer match tenant associ subscript us author token note subscript transfer tenant",
        "Question_gpt_summary_original":"The user is encountering an error when deploying a real-time inferencing pipeline in Azure ML, receiving an \"Unauthorized\" error message with details indicating an issue with the authorization token. The error message suggests that the access token is from the wrong issuer and must match the correct tenant ID associated with the subscription. The user has attempted to resolve the issue by logging out and creating a new Azure workspace, but the error persists.",
        "Question_gpt_summary":"user encount error deploi real time inferenc pipelin receiv unauthor error messag detail indic issu author token error messag suggest access token wrong issuer match correct tenant associ subscript user attempt resolv issu log creat new azur workspac error persist",
        "Answer_original_content":"appear user access administr role addit classic servic administr soon ad owner role access control iam section azur portal deploy succeed",
        "Answer_preprocessed_content":"appear user access administr role soon ad owner role access control section azur portal deploy succeed",
        "Answer_gpt_summary_original":"Solution: Adding the user to the Owner role in the Access Control (IAM) section of the Azure Portal resolved the issue.",
        "Answer_gpt_summary":"solut ad user owner role access control iam section azur portal resolv issu"
    },
    {
        "Question_title":"Without ECR being enabled in AWS account at Organization Level, what's the impact to SageMaker Studio?",
        "Question_body":"I have a customer already standardized Artifactory as the centralized image registration. They disabled ECR service at Org level. Now we want to understand the potential impact on customer's day-2-day use of SageMaker Studio as a platform to support their full ML lifecycle.\n\n(If customer only use built-in SageMaker algorithm or framework and use prebuilt SageMaker container images)\n\nEspecially when customer trying to deploy the model to endpoint, does that need ECR service to be enabled in customer account?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649820151027,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":95.0,
        "Answer_body":"Hi, yes, if you're restricting the user to only built-in algorithms and frameworks, and prebuilt images for Studio, you should be able to use it seamlessly (to deploy endpoints as well). That said, it severely restricts the data scientist from using custom images that could be built to their needs and packages, or bringing their own container for machine learning.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXZB0Oki3QamlQ5ijtVSuzQ\/without-ecr-being-enabled-in-aws-account-at-organization-level-whats-the-impact-to-sage-maker-studio",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-18T16:46:25.539Z",
                "Answer_score":1,
                "Answer_body":"Hi, yes, if you're restricting the user to only built-in algorithms and frameworks, and prebuilt images for Studio, you should be able to use it seamlessly (to deploy endpoints as well). That said, it severely restricts the data scientist from using custom images that could be built to their needs and packages, or bringing their own container for machine learning.",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1650300385539,
        "Question_original_content":"ecr enabl aw account organ level impact studio custom standard artifactori central imag registr disabl ecr servic org level want understand potenti impact custom dai dai us studio platform support lifecycl custom us built algorithm framework us prebuilt contain imag especi custom try deploi model endpoint need ecr servic enabl custom account",
        "Question_preprocessed_content":"ecr enabl aw account organ level impact studio custom standard artifactori central imag registr disabl ecr servic org level want understand potenti impact custom us studio platform support lifecycl custom us algorithm framework us prebuilt contain imag especi custom try deploi model endpoint need ecr servic enabl custom account",
        "Question_gpt_summary_original":"The user is facing challenges related to the potential impact on their day-to-day use of SageMaker Studio as a platform to support their full ML lifecycle due to the disabling of ECR service at the organization level. They are specifically concerned about the need for ECR service to be enabled in their account when deploying the model to an endpoint.",
        "Question_gpt_summary":"user face challeng relat potenti impact dai dai us studio platform support lifecycl disabl ecr servic organ level specif concern need ecr servic enabl account deploi model endpoint",
        "Answer_original_content":"ye restrict user built algorithm framework prebuilt imag studio abl us seamlessli deploi endpoint said sever restrict data scientist custom imag built need packag bring contain machin learn",
        "Answer_preprocessed_content":"ye restrict user algorithm framework prebuilt imag studio abl us seamlessli said sever restrict data scientist custom imag built need packag bring contain machin learn",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to restrict the user to only built-in algorithms and frameworks, and prebuilt images for SageMaker Studio. This would allow the user to use the platform seamlessly, including deploying endpoints, without the need for ECR service. However, this solution would severely restrict the data scientist from using custom images that could be built to their needs and packages, or bringing their own container for machine learning. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut possibl solut mention discuss restrict user built algorithm framework prebuilt imag studio allow user us platform seamlessli includ deploi endpoint need ecr servic solut sever restrict data scientist custom imag built need packag bring contain machin learn solut mention discuss"
    },
    {
        "Question_title":"Azure Machine Learning Designer - Webservice input\/output disappear",
        "Question_body":"Hi all,\n\nI'm trying to create an inference pipeline with the AML designer.\nI clicked on the \"Create inference pipeline\" button:\n\n\n\n\nand now I want to do some changes in the pipeline. I added at the end two more steps and linked the Webservice output component to the last step:\n\n\n\n\nI clicked on save and submit it.\nThe result is the following:\n\n\n\n\nThe two new steps are present and executed, but the webservice output step is disappeared! I've tried multiple time with the same result.\nThe webservice input step is correctly present at the beginning of the pipeline.\n\nAlso, after making the change and saving correctly, if I exit and reopen the pipeline the step \"Web Service Output\" is no longer there\n\nCan you help me?\n\nThanks!\n\nG",
        "Question_answer_count":8,
        "Question_comment_count":1,
        "Question_creation_time":1653385681273,
        "Question_favorite_count":14.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi @Antonio-9417,\n\nSorry for the inconvenience caused.\nThis is a known bug and we've fixed. Could you please retry to see if you can still repro? I tried from my side either manually build an inference pipeline or modify the auto-gen inference pipeline, the web service input\/output components are still there.\n\nIf you can still repro, could you please provide following info for us to investigate?\n- your inference pipeline draft URL\n- inference pipeline job URL of which the webservice input\/output components disappear\n- Is your workspace in Vnet?\n\nWe're also happy to set up a call to investigate, could you please send me an email so that I can send the meeting request?\nWe're based in Beijing (UTC+8).\n\nThanks!",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/861882\/azure-machine-learning-designer-webservice-inoputo.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-07T13:36:56.26Z",
                "Answer_score":2,
                "Answer_body":"Hi @Antonio-9417,\n\nSorry for the inconvenience caused.\nThis is a known bug and we've fixed. Could you please retry to see if you can still repro? I tried from my side either manually build an inference pipeline or modify the auto-gen inference pipeline, the web service input\/output components are still there.\n\nIf you can still repro, could you please provide following info for us to investigate?\n- your inference pipeline draft URL\n- inference pipeline job URL of which the webservice input\/output components disappear\n- Is your workspace in Vnet?\n\nWe're also happy to set up a call to investigate, could you please send me an email so that I can send the meeting request?\nWe're based in Beijing (UTC+8).\n\nThanks!",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-05-28T23:49:14.153Z",
                "Answer_score":0,
                "Answer_body":"I have the exact same problem, Web Service Input disappears. Because of that (I guess), after submitting the pipeline there is no \"deploy\" option available.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-30T13:47:05.607Z",
                "Answer_score":0,
                "Answer_body":"I'm having the same issue here! There's something wrong with the transition between training and inference pipelines!",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-31T09:51:44.197Z",
                "Answer_score":0,
                "Answer_body":"Same for me. Also when following https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/create-classification-model-azure-machine-learning-designer\/inference-pipeline , sometimes deleting the connection between \"Score Model\" and \"Web Service Output\" has no effect (the pipeline displays as modified in the designer, but after submitting, or reloading from the drafts, shows the old connection)",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-06T12:59:22.007Z",
                "Answer_score":0,
                "Answer_body":"Hi, this is Blanca, a PM from AML Designer team.\nSorry for the inconvenience caused.\nWe will investigate this issue, and firstly we'd like to know more details about how to repro such issue.\nCould you please send an email to me and let me know what time works for you to have a quick call?\nMy email is keli19@microsoft.com, and our team is based in Beijing (UTC+8).\nThanks!",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-06T13:39:37.06Z",
                "Answer_score":0,
                "Answer_body":"Hi @BlancaLi-3542 , I'm following https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/create-classification-model-azure-machine-learning-designer\/inference-pipeline.\n\nDoes it work for you?\n\nRegards",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-07T14:02:47.9Z",
                "Answer_score":1,
                "Answer_body":"I've tested it on the clustering example. It seems to be working now. Thanks",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-07T15:14:43.58Z",
                "Answer_score":1,
                "Answer_body":"Hi @BlancaLi-3542,\n\nI did more tests in my environment and the bug didn't appear anymore, thanks!",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":1654609016260,
        "Question_original_content":"design webservic input output disappear try creat infer pipelin aml design click creat infer pipelin button want chang pipelin ad end step link webservic output compon step click save submit result follow new step present execut webservic output step disappear tri multipl time result webservic input step correctli present begin pipelin make chang save correctli exit reopen pipelin step web servic output longer help thank",
        "Question_preprocessed_content":"design webservic disappear try creat infer pipelin aml design click creat infer pipelin button want chang pipelin ad end step link webservic output compon step click save submit result follow new step present execut webservic output step disappear tri multipl time result webservic input step correctli present begin pipelin make chang save correctli exit reopen pipelin step web servic output longer help thank",
        "Question_gpt_summary_original":"The user is encountering a challenge with the Azure Machine Learning Designer where the Webservice output component disappears after making changes to the inference pipeline and saving it. The Webservice input step is correctly present at the beginning of the pipeline, but the output step disappears even after multiple attempts. The issue persists even after exiting and reopening the pipeline.",
        "Question_gpt_summary":"user encount challeng design webservic output compon disappear make chang infer pipelin save webservic input step correctli present begin pipelin output step disappear multipl attempt issu persist exit reopen pipelin",
        "Answer_original_content":"antonio sorri inconveni caus known bug fix retri repro tri manual build infer pipelin modifi auto gen infer pipelin web servic input output compon repro provid follow info investig infer pipelin draft url infer pipelin job url webservic input output compon disappear workspac vnet happi set investig send email send meet request base beij utc thank",
        "Answer_preprocessed_content":"sorri inconveni caus known bug fix retri repro tri manual build infer pipelin modifi infer pipelin web servic compon repro provid follow info investig infer pipelin draft url infer pipelin job url webservic compon disappear workspac vnet happi set investig send email send meet request base beij thank",
        "Answer_gpt_summary_original":"Solution: The person responding to the challenge acknowledged that this is a known bug and has been fixed. They suggested that the user retry to see if they can still reproduce the issue. If the issue persists, they requested the user to provide some information for them to investigate further. They also offered to set up a call to investigate the issue.",
        "Answer_gpt_summary":"solut person respond challeng acknowledg known bug fix suggest user retri reproduc issu issu persist request user provid inform investig offer set investig issu"
    },
    {
        "Question_title":"S3 Dataset versioning with SageMaker?",
        "Question_body":"Is there any standard for ML S3 dataset tracking or versioning? Basically, what setup allows to track a given model training execution to a given dataset? Interested to hear about proven or state-of-the-art ideas",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1549396058000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":386.0,
        "Answer_body":"Unfortunately, managing versions of datasets and which models used them is not embedded in SageMaker. But, you can use SageMaker search to manage the differences in data location between experiments. In that case, if your dataset isn't too big, my recommendation will be to create a standard for data structure in S3. i.e. for each new dataset, create a new prefix in S3 with your logic. Using SageMaker search you'll be able to find all your jobs and compare between datasets.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhYC1EJQuSWqpwTByAtB_fg\/s-3-dataset-versioning-with-sage-maker",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-02-06T07:18:29.000Z",
                "Answer_score":0,
                "Answer_body":"Unfortunately, managing versions of datasets and which models used them is not embedded in SageMaker. But, you can use SageMaker search to manage the differences in data location between experiments. In that case, if your dataset isn't too big, my recommendation will be to create a standard for data structure in S3. i.e. for each new dataset, create a new prefix in S3 with your logic. Using SageMaker search you'll be able to find all your jobs and compare between datasets.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-09-28T12:51:12.007Z",
                "Answer_score":0,
                "Answer_body":"Nowadays, there are 3rd party tool that can be used alongside SageMaker. One example is Data Version Control (DVC), and we have discussed it how to integrate within SageMaker Processing jobs and SageMaker Training Jobs in this blogpost. As an alternative, you can leverage SageMaker Pipelines when your data preparation step is executed as a processing step within a pipeline execution. Pipelines allows you to achieve data versioning in a programmatic way by using execution-specific variables like ExecutionVariables.PIPELINE_EXECUTION_ID, which is the unique ID of a pipeline run. We can, for example, create a unique key for storing the output datasets in S3 that ties them to a specific pipeline run. We have also discussed this possibility as part of this blogpost.",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":1549437509000,
        "Question_original_content":"dataset version standard dataset track version basic setup allow track given model train execut given dataset interest hear proven state art idea",
        "Question_preprocessed_content":"dataset version standard dataset track version basic setup allow track given model train execut given dataset interest hear proven idea",
        "Question_gpt_summary_original":"The user is seeking information on a standard method for tracking or versioning S3 datasets used in machine learning, in order to link a specific model training execution to a particular dataset. They are interested in hearing about established or cutting-edge approaches to this challenge.",
        "Question_gpt_summary":"user seek inform standard method track version dataset machin learn order link specif model train execut particular dataset interest hear establish cut edg approach challeng",
        "Answer_original_content":"unfortun manag version dataset model embed us search manag differ data locat experi case dataset isn big recommend creat standard data structur new dataset creat new prefix logic search abl job compar dataset",
        "Answer_preprocessed_content":"unfortun manag version dataset model embed us search manag differ data locat experi case dataset isn big recommend creat standard data structur new dataset creat new prefix logic search abl job compar dataset",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to use SageMaker search to manage the differences in data location between experiments. Another solution is to create a standard for data structure in S3, where for each new dataset, a new prefix is created in S3 with a specific logic. Using SageMaker search, all jobs can be found and compared between datasets.",
        "Answer_gpt_summary":"solut possibl solut mention discuss us search manag differ data locat experi solut creat standard data structur new dataset new prefix creat specif logic search job compar dataset"
    },
    {
        "Question_title":"Azure ML's web service asking for label?",
        "Question_body":"<p>I built a linear regression algorithm in Azure ML. On the &quot;Score Model&quot; module I can actually see the predictions and the rest of the features. However, when I deploy this project as a web service, the service is expecting the actual label of the data (e.g. I'm trying to predict a house's price and it asks me for the price of the house to make the prediction), which doesn't make any sense to me... What am I doing wrong? On the &quot;Train Model&quot; module I set that the label column is the HousePrice, which is what I'm trying to predict...<\/p>\n<p>This is my model:\n<a href=\"https:\/\/i.stack.imgur.com\/kI8qu.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kI8qu.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I tried leaving that field blank but the prediction returns null...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1476382562787,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1592644375060,
        "Question_score":4.0,
        "Question_view_count":1012.0,
        "Answer_body":"<p>The input schema (names\/types of required input) based on the location in the graph where you attach the \"Web Service Input\" module. To get the schema you want, you will need to find -- or if necessary, create -- a place in the experiment where the data has the column names\/types you desire.<\/p>\n\n<p>Consider this simple example experiment that predicts whether a field called \"income\" will be above or below $50k\/year:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/nWaN2.png\" rel=\"nofollow\"><img src=\"https:\/\/i.stack.imgur.com\/nWaN2.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When we click \"Set up web service\", the following graph is automatically generated:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/NMMpV.png\" rel=\"nofollow\"><img src=\"https:\/\/i.stack.imgur.com\/NMMpV.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Since the input dataset and \"Web service input\" modules are connected to the same port, the web service schema will perfectly match the schema of the input dataset. This is unfortunate because the input dataset contains a column called \"income\", which is what our web service is supposed to predict -- this is equivalent to the problem that you are having.<\/p>\n\n<p>To get around it, we need to create a place in our experiment graph where we've dropped the unneeded \"income\" field from the input dataset, and attach the \"Web service input\" module there:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WPeSB.png\" rel=\"nofollow\"><img src=\"https:\/\/i.stack.imgur.com\/WPeSB.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>With this arrangement, the web service only requests the features actually needed to score the model. I'm sure you can use a similar method to create a predictive experiment with whatever input schema you need for your own work.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40028165",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1476730527012,
        "Question_original_content":"web servic ask label built linear regress algorithm score model modul actual predict rest featur deploi project web servic servic expect actual label data try predict hous price ask price hous predict sens wrong train model modul set label column housepric try predict model tri leav field blank predict return null",
        "Question_preprocessed_content":"web servic ask label built linear regress algorithm score model modul actual predict rest featur deploi project web servic servic expect actual label data sens wrong train model modul set label column housepric try model tri leav field blank predict return",
        "Question_gpt_summary_original":"The user built a linear regression algorithm in Azure ML and is facing challenges while deploying the project as a web service. The web service is asking for the actual label of the data, which is not making sense to the user. The user has set the label column as HousePrice in the Train Model module, but leaving the field blank results in null predictions.",
        "Question_gpt_summary":"user built linear regress algorithm face challeng deploi project web servic web servic ask actual label data make sens user user set label column housepric train model modul leav field blank result null predict",
        "Answer_original_content":"input schema name type requir input base locat graph attach web servic input modul schema want need necessari creat place experi data column name type desir consid simpl exampl experi predict field call incom year click set web servic follow graph automat gener input dataset web servic input modul connect port web servic schema perfectli match schema input dataset unfortun input dataset contain column call incom web servic suppos predict equival problem have need creat place experi graph drop unneed incom field input dataset attach web servic input modul arrang web servic request featur actual need score model sure us similar method creat predict experi input schema need work",
        "Answer_preprocessed_content":"input schema base locat graph attach web servic input modul schema want need necessari creat place experi data column desir consid simpl exampl experi predict field call incom click set web servic follow graph automat gener input dataset web servic input modul connect port web servic schema perfectli match schema input dataset unfortun input dataset contain column call incom web servic suppos predict equival problem have need creat place experi graph drop unneed incom field input dataset attach web servic input modul arrang web servic request featur actual need score model sure us similar method creat predict experi input schema need work",
        "Answer_gpt_summary_original":"Solution: The user needs to create a place in the experiment graph where the unneeded column is dropped from the input dataset and attach the \"Web service input\" module there. This will ensure that the web service only requests the features actually needed to score the model.",
        "Answer_gpt_summary":"solut user need creat place experi graph unneed column drop input dataset attach web servic input modul ensur web servic request featur actual need score model"
    },
    {
        "Question_title":"power by dashboard with machine learning azure data",
        "Question_body":"<p>Hi I have a web service which is the result of my machine learning azure training. I would like to set a new datasource in power bi, which calls the web service with the current datetime as a parameter in order to create a report with the result predictions. I cannot find a way to call the api. Is this any? I am thinking another solution of creating a service and execute the api, and insert the result in a table in order to connect to this table. But, I would like to avoid doing this.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1462889379010,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":444.0,
        "Answer_body":"<p>I used something called Azure Data Factory (ADF). It allows you to schedule a job by defining a pipeline with activities. There are activities for training your model or scoring your predictive ML. The scoring result, I am storing it in Azure DB (it could be another storage) and connected it to Power BI.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37140987",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1463150592243,
        "Question_original_content":"power dashboard machin learn azur data web servic result machin learn azur train like set new datasourc power call web servic current datetim paramet order creat report result predict wai api think solut creat servic execut api insert result tabl order connect tabl like avoid",
        "Question_preprocessed_content":"power dashboard machin learn azur data web servic result machin learn azur train like set new datasourc power call web servic current datetim paramet order creat report result predict wai api think solut creat servic execut api insert result tabl order connect tabl like avoid",
        "Question_gpt_summary_original":"The user is facing challenges in setting a new datasource in Power BI that calls a web service with the current datetime as a parameter to create a report with the result predictions from their machine learning Azure training. They are unable to find a way to call the API and are considering creating a service to execute the API and insert the result in a table to connect to it, but would like to avoid doing so.",
        "Question_gpt_summary":"user face challeng set new datasourc power call web servic current datetim paramet creat report result predict machin learn azur train unabl wai api consid creat servic execut api insert result tabl connect like avoid",
        "Answer_original_content":"call azur data factori adf allow schedul job defin pipelin activ activ train model score predict score result store azur storag connect power",
        "Answer_preprocessed_content":"call azur data factori allow schedul job defin pipelin activ activ train model score predict score result store azur connect power",
        "Answer_gpt_summary_original":"Solution: The user can use Azure Data Factory (ADF) to schedule a job by defining a pipeline with activities. There are activities for training the model or scoring the predictive ML. The scoring result can be stored in Azure DB or another storage and connected to Power BI.",
        "Answer_gpt_summary":"solut user us azur data factori adf schedul job defin pipelin activ activ train model score predict score result store azur storag connect power"
    },
    {
        "Question_title":"Overview of the resumable workspaces",
        "Question_body":"Domino 4.4 introduced Resumable Workspaces. In our environment the maximum number of workspaces per user is limited to 8. When this limit is reached, I have to delete one of my workspaces first, so that I can create a new one.\u00a0\n\nA user asked the following question in our legacy community system:\n\nI would have expected to see, for example, in the project overview or in the Control Center in which projects my 8 user workspaces are, so that I can specifically archive workspaces that are no longer needed. So I have to go through all my projects and manually check where workspaces exist.\u00a0\n\nDid I miss something? Is there a place where I as a Data Scientist can see in which projects I currently have how many workspaces?\n\nAnswering below.\u00a0\n\nThank you very much for your feedback.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624935390000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi and thanks for using our community! Apologies for the delayed response here. The ability to see your workspaces in a central location has been added in Domino v 4.5 -- each project will display the number of active workspaces on your projects homepage, so you can see at a glance which projects contain workspaces that you may want to revisit or delete\/retire.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/tickets.dominodatalab.com\/hc\/en-us\/community\/posts\/4402871565076-Overview-of-the-resumable-workspaces-",
        "Tool":"Domino",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-29T02:57:02Z",
                "Answer_score":0,
                "Answer_body":"Hi and thanks for using our community! Apologies for the delayed response here. The ability to see your workspaces in a central location has been added in Domino v 4.5 -- each project will display the number of active workspaces on your projects homepage, so you can see at a glance which projects contain workspaces that you may want to revisit or delete\/retire."
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"overview resum workspac introduc resum workspac environ maximum number workspac user limit limit reach delet workspac creat new user ask follow question legaci commun expect exampl project overview control center project user workspac specif archiv workspac longer need project manual check workspac exist miss place data scientist project current workspac answer thank feedback",
        "Question_preprocessed_content":"overview resum workspac introduc resum workspac environ maximum number workspac user limit limit reach delet workspac creat new user ask follow question legaci commun expect exampl project overview control center project user workspac specif archiv workspac longer need project manual check workspac exist miss place data scientist project current workspac answer thank feedback",
        "Question_gpt_summary_original":"The user is facing a challenge with the maximum limit of 8 workspaces per user in Domino 4.4. When the limit is reached, the user has to delete one workspace to create a new one. The user is requesting a feature to see which projects have their 8 workspaces so they can specifically archive workspaces that are no longer needed.",
        "Question_gpt_summary":"user face challeng maximum limit workspac user limit reach user delet workspac creat new user request featur project workspac specif archiv workspac longer need",
        "Answer_original_content":"thank commun apolog delai respons abil workspac central locat ad project displai number activ workspac project homepag glanc project contain workspac want revisit delet retir",
        "Answer_preprocessed_content":"thank commun apolog delai respons abil workspac central locat ad project displai number activ workspac project homepag glanc project contain workspac want revisit",
        "Answer_gpt_summary_original":"Solution: The user can upgrade to Domino v 4.5, which has a feature that displays the number of active workspaces on each project's homepage. This will allow the user to see which projects have their 8 workspaces and which ones can be deleted or retired.",
        "Answer_gpt_summary":"solut user upgrad featur displai number activ workspac project homepag allow user project workspac on delet retir"
    },
    {
        "Question_title":"Install graphiz on AWS Sagemaker",
        "Question_body":"<p>I'm on a Jupyter notebook using Python3 and trying to plot a tree with code like this:<\/p>\n\n<pre><code>import xgboost as xgb\nfrom xgboost import plot_tree\n\nplot_tree(model, num_trees=4)\n<\/code><\/pre>\n\n<p>On the last line I get:<\/p>\n\n<pre><code>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/xgboost\/plotting.py in to_graphviz(booster, fmap, num_trees, rankdir, yes_color, no_color, **kwargs)\n196         from graphviz import Digraph\n197     except ImportError:\n--&gt; 198         raise ImportError('You must install graphviz to plot tree')\n199 \n200     if not isinstance(booster, (Booster, XGBModel)):\n\nImportError: You must install graphviz to plot tree\n<\/code><\/pre>\n\n<p>How do I install graphviz so I can see the plot_tree?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1552345636840,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":572.0,
        "Answer_body":"<p>I was finally able to learn that Conda has a package which can install it for you. I was able to get it installed by running the command:<\/p>\n\n<pre><code>!conda install python-graphviz --yes\n<\/code><\/pre>\n\n<p>Note the <code>--yes<\/code> is only needed if the installation needs to verify adding\/changing other packages since the Jupyter notebook is not interactive once it is running.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55112494",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1552366926347,
        "Question_original_content":"instal graphiz jupyt notebook python try plot tree code like import xgboost xgb xgboost import plot tree plot tree model num tree line anaconda env python lib python site packag xgboost plot graphviz booster fmap num tree rankdir ye color color kwarg graphviz import digraph importerror rais importerror instal graphviz plot tree isinst booster booster xgbmodel importerror instal graphviz plot tree instal graphviz plot tree",
        "Question_preprocessed_content":"instal graphiz jupyt notebook python try plot tree code like line instal graphviz",
        "Question_gpt_summary_original":"The user is trying to plot a tree using Python3 on a Jupyter notebook with xgboost library. However, the user encountered an error message stating that graphviz needs to be installed to plot the tree. The user is seeking guidance on how to install graphviz to resolve the issue.",
        "Question_gpt_summary":"user try plot tree python jupyt notebook xgboost librari user encount error messag state graphviz need instal plot tree user seek guidanc instal graphviz resolv issu",
        "Answer_original_content":"final abl learn conda packag instal abl instal run command conda instal python graphviz ye note ye need instal need verifi ad chang packag jupyt notebook interact run",
        "Answer_preprocessed_content":"final abl learn conda packag instal abl instal run command note need instal need verifi packag jupyt notebook interact run",
        "Answer_gpt_summary_original":"Solution: The user can install graphviz by running the command \"!conda install python-graphviz --yes\" in the Jupyter notebook. The \"--yes\" flag is only necessary if the installation needs to verify adding\/changing other packages.",
        "Answer_gpt_summary":"solut user instal graphviz run command conda instal python graphviz ye jupyt notebook ye flag necessari instal need verifi ad chang packag"
    },
    {
        "Question_title":"Azure machine learning",
        "Question_body":"Is there any way to integrate MS Dynamics Customer Insights with Azure Machine Learning (designer)?I know there is an integration between CI and Azure Machine Learning studio (classic). Please help to integrate these two services.",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1656618921300,
        "Question_favorite_count":12.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @Yasuo-9899\n\nThanks for reaching out to us for this question. Are you looking for this document? https:\/\/docs.microsoft.com\/en-us\/dynamics365\/customer-insights\/azure-machine-learning-experiments\n\nI have found one pic which is described the structure well:\n\n\n\n\n\nAnd also a repo you may want to refer to: https:\/\/github.com\/ArtisConsulting\/customer-insights-azure-data-workshop\/blob\/main\/README.md\n\nPlease let us know more details you are interested in so that we can help. Thanks.\n\nRegards,\nYutong",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/909965\/azure-machine-learning-2.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-30T23:43:14.977Z",
                "Answer_score":0,
                "Answer_body":"Hello @Yasuo-9899\n\nThanks for reaching out to us for this question. Are you looking for this document? https:\/\/docs.microsoft.com\/en-us\/dynamics365\/customer-insights\/azure-machine-learning-experiments\n\nI have found one pic which is described the structure well:\n\n\n\n\n\nAnd also a repo you may want to refer to: https:\/\/github.com\/ArtisConsulting\/customer-insights-azure-data-workshop\/blob\/main\/README.md\n\nPlease let us know more details you are interested in so that we can help. Thanks.\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-07-01T22:59:10.557Z",
                "Answer_score":0,
                "Answer_body":"Hi @Yasuo-9899 ,\n\nI believe below document will be helpful.\nGeneral Documentation: https:\/\/docs.microsoft.com\/en-us\/dynamics365\/customer-insights\/custom-models\n\nPre-requisites for correct configuration of a pipeline: https:\/\/docs.microsoft.com\/en-us\/dynamics365\/customer-insights\/azure-machine-learning-experiments\n\nTutorial Documentation: https:\/\/github.com\/naravill\/CustomerInsightsML\n\nRegards,\nPritee",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":1656632594976,
        "Question_original_content":" wai integr dynam custom insight design know integr studio classic help integr servic",
        "Question_preprocessed_content":" wai integr dynam custom insight know integr studio help integr servic",
        "Question_gpt_summary_original":"The user is facing a challenge in integrating MS Dynamics Customer Insights with Azure Machine Learning (designer) and is seeking help to integrate these two services. They are aware of the integration between CI and Azure Machine Learning studio (classic).",
        "Question_gpt_summary":"user face challeng integr dynam custom insight design seek help integr servic awar integr studio classic",
        "Answer_original_content":"hello yasuo thank reach question look document http doc microsoft com dynam custom insight azur machin learn experi pic describ structur repo want refer http github com artisconsult custom insight azur data workshop blob main readm let know detail interest help thank regard yutong",
        "Answer_preprocessed_content":"hello thank reach question look document pic describ structur repo want refer let know detail interest help thank regard yutong",
        "Answer_gpt_summary_original":"Solution: The discussion provides a link to a Microsoft document that explains the integration between MS Dynamics Customer Insights and Azure Machine Learning (designer). Additionally, a GitHub repository is shared that may be helpful in integrating these two services.",
        "Answer_gpt_summary":"solut discuss provid link microsoft document explain integr dynam custom insight design addition github repositori share help integr servic"
    },
    {
        "Question_title":"Practice Exam for DP-100",
        "Question_body":"Hello,\n\nno practice exam for the Azure certification DP-100 seems to be available in the official channels. It would, however, be very helpful for preparing.\nBy any chance, do you plan to introduce such a resource any time soon?\n\nThanks and best regards\nTim",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593082618390,
        "Question_favorite_count":4.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Hi,\n\nMicrosoft Certification \/ Exams are currently not supported in the Q&A forums, the supported products are listed over here https:\/\/docs.microsoft.com\/en-us\/answers\/products (more to be added later on).\n\nYou can ask the experts in the dedicated Microsoft Certification - Preparation Resources forum over here:\nhttps:\/\/trainingsupport.microsoft.com\/en-us\/mcp\/forum\/mcp_exams-mcp_prep\n\n(Please don't forget to accept helpful replies as answer)\n\nBest regards,\nLeon",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/39948\/practice-exam-for-dp-100.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-25T11:08:59.537Z",
                "Answer_score":2,
                "Answer_body":"Hi,\n\nMicrosoft Certification \/ Exams are currently not supported in the Q&A forums, the supported products are listed over here https:\/\/docs.microsoft.com\/en-us\/answers\/products (more to be added later on).\n\nYou can ask the experts in the dedicated Microsoft Certification - Preparation Resources forum over here:\nhttps:\/\/trainingsupport.microsoft.com\/en-us\/mcp\/forum\/mcp_exams-mcp_prep\n\n(Please don't forget to accept helpful replies as answer)\n\nBest regards,\nLeon",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1593083339536,
        "Question_original_content":"practic exam hello practic exam azur certif avail offici channel help prepar chanc plan introduc resourc time soon thank best regard tim",
        "Question_preprocessed_content":"practic exam hello practic exam azur certif avail offici channel help prepar chanc plan introduc resourc time soon thank best regard tim",
        "Question_gpt_summary_original":"The user is facing a challenge in finding a practice exam for the Azure certification DP-100, which is not available in official channels. They are seeking information on whether such a resource will be introduced in the near future.",
        "Question_gpt_summary":"user face challeng find practic exam azur certif avail offici channel seek inform resourc introduc near futur",
        "Answer_original_content":"microsoft certif exam current support forum support product list http doc microsoft com answer product ad later ask expert dedic microsoft certif prepar resourc forum http trainingsupport microsoft com mcp forum mcp exam mcp prep forget accept help repli answer best regard leon",
        "Answer_preprocessed_content":"microsoft certif exam current support forum support product list ask expert dedic microsoft certif prepar resourc forum forget accept help repli answer best regard leon",
        "Answer_gpt_summary_original":"Solution: The user is advised to ask the experts in the dedicated Microsoft Certification - Preparation Resources forum to find a practice exam for the Azure certification DP-100. No information is provided on whether such a resource will be introduced in the near future.",
        "Answer_gpt_summary":"solut user advis ask expert dedic microsoft certif prepar resourc forum practic exam azur certif inform provid resourc introduc near futur"
    },
    {
        "Question_title":"Azure Machine Learning: I cannot find experiment's user logs located in logs\/user folder",
        "Question_body":"I am running experiments in Azure Machine Learning using ParallelRunStep, and I cannot get the user folder with logs as defined in readme.txt file with the log folder structure.\nI cannot find log\/user folder with \"Logs generated when loading and running user's scripts.\"\n\nreadme.txt file states:\nParallelRunStep has two major parts:\n1. Scheduling, progress tracking and file concatenation for append_row.\n2. Processing mini batch by calling the entry script.\nThe agent manager on each node start agents.\nAn agent gets mini batch and calls the entry script against the mini batch.\n\n The \"logs\" folder has user, sys and perf sub folders.\n The user folder includes messages from the entry script in processing mini batches.\n The sys folder includes messages from #1 and non-entry script log from #2.\n The perf folder includes periodical checking result of resource usage.\n\n\n\nIn majority case, users can find the processing messages from the user folder.\nUsers need to check sys folder for messages beyond processing mini batches.\nlogs\/\nazureml\/: Logs from azureml dependencies. e.g. azureml.dataprep\nuser\/ : Logs generated when loading and running user's scripts.\nerror\/ : Logs of errors encountered while loading and running entry script.\nstderr\/ : stderr output of user's scripts.\nstdout\/ : stdout output of user's scripts.\nentry_script_log\/ : Logs generated by loggers of EntryScript()\n<node seq> :\nprocessNNN.log.txt : Logs generated by loggers of EntryScript() from each process.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1645621539517,
        "Question_favorite_count":8.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"@CalabriaMonteroSalvadorSGRESEDFPDC-5704 Thanks for the question. Please follow the doc to view and log files for a run. Interactive logging sessions are typically used in notebook environments. The method Experiment.start_logging() starts an interactive logging session. Any metrics logged during the session are added to the run record in the experiment. The method run.complete() ends the sessions and marks the run as completed.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics#view-and-download-log-files-for-a-run",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/747549\/azure-machine-learning-i-cannot-find-experiment39s.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-24T02:37:03.417Z",
                "Answer_score":0,
                "Answer_body":"@CalabriaMonteroSalvadorSGRESEDFPDC-5704 Thanks for the question. Please follow the doc to view and log files for a run. Interactive logging sessions are typically used in notebook environments. The method Experiment.start_logging() starts an interactive logging session. Any metrics logged during the session are added to the run record in the experiment. The method run.complete() ends the sessions and marks the run as completed.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics#view-and-download-log-files-for-a-run",
                "Answer_comment_count":4,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1645670223416,
        "Question_original_content":"experi user log locat log user folder run experi parallelrunstep user folder log defin readm txt file log folder structur log user folder log gener load run user script readm txt file state parallelrunstep major part schedul progress track file concaten append row process mini batch call entri script agent manag node start agent agent get mini batch call entri script mini batch log folder user sy perf sub folder user folder includ messag entri script process mini batch sy folder includ messag non entri script log perf folder includ period check result resourc usag major case user process messag user folder user need check sy folder messag process mini batch log log depend dataprep user log gener load run user script error log error encount load run entri script stderr stderr output user script stdout stdout output user script entri script log log gener logger entryscript process log txt log gener logger entryscript process",
        "Question_preprocessed_content":"experi user log locat folder run experi parallelrunstep user folder log defin file log folder structur folder log gener load run user file state parallelrunstep major part schedul progress track file concaten process mini batch call entri script agent manag node start agent agent get mini batch call entri script mini batch log folder user sy perf sub folder user folder includ messag entri script process mini batch sy folder includ messag script log perf folder includ period check result resourc usag major case user process messag user folder user need check sy folder messag process mini batch log log depend dataprep user log gener load run user script error log error encount load run entri script stderr stderr output user script stdout stdout output user script log gener logger entryscript log gener logger entryscript process",
        "Question_gpt_summary_original":"The user is encountering challenges in finding the user logs folder located in logs\/user while running experiments in Azure Machine Learning using ParallelRunStep. The readme.txt file provides information on the log folder structure, which includes user, sys, and perf subfolders. The user folder contains messages from the entry script in processing mini batches, while the sys folder includes messages from scheduling, progress tracking, and non-entry script logs. The user needs to check the sys folder for messages beyond processing mini batches.",
        "Question_gpt_summary":"user encount challeng find user log folder locat log user run experi parallelrunstep readm txt file provid inform log folder structur includ user sy perf subfold user folder contain messag entri script process mini batch sy folder includ messag schedul progress track non entri script log user need check sy folder messag process mini batch",
        "Answer_original_content":"calabriamonterosalvadorsgresedfpdc thank question follow doc view log file run interact log session typic notebook environ method experi start log start interact log session metric log session ad run record experi method run complet end session mark run complet http doc microsoft com azur machin learn log view metric view download log file run",
        "Answer_preprocessed_content":"thank question follow doc view log file run interact log session typic notebook environ method start interact log session metric log session ad run record experi method end session mark run complet",
        "Answer_gpt_summary_original":"Solution: The user can follow the documentation provided by Microsoft to view and log files for a run. The documentation suggests using interactive logging sessions, which can be started using the method Experiment.start_logging(). Any metrics logged during the session are added to the run record in the experiment. The method run.complete() ends the sessions and marks the run as completed. However, no solution is provided for finding the user logs folder located in logs\/user while running experiments in Azure Machine Learning using ParallelRunStep.",
        "Answer_gpt_summary":"solut user follow document provid microsoft view log file run document suggest interact log session start method experi start log metric log session ad run record experi method run complet end session mark run complet solut provid find user log folder locat log user run experi parallelrunstep"
    },
    {
        "Question_title":"What would stop credentials from validation on a ClearML server?",
        "Question_body":"<p>I've set up a ClearML server in GCP using the sub-domain approach. I can access all three domains (<code>https:\/\/app.clearml.mydomain.com<\/code>, <code>https:\/\/api.clearml.mydomain.com<\/code> and <code>https:\/\/files.clearml.mydomain.com<\/code>) in a browser and see what I think is the correct response, but when connecting with the python SDK via <code>clearml-init<\/code> I get the following error:<\/p>\n<pre><code>clearml.backend_api.session.session.LoginError: Failed getting token (error 400 from https:\/\/api.clearml.mydomain.com): Bad Request\n<\/code><\/pre>\n<p>Are there any likely causes of this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639763350487,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":194.0,
        "Answer_body":"<p>Following the discussion <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/517\" rel=\"nofollow noreferrer\">here<\/a>, it seemed that the load balancer being used was blocking <code>GET<\/code> requests with a payload which are used by ClearML. A <a href=\"https:\/\/github.com\/allegroai\/clearml\/pull\/521\" rel=\"nofollow noreferrer\">fix<\/a> is being worked on to allow the method to be changed to a <code>POST<\/code> request via an environment variable.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70397010",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1640162037740,
        "Question_original_content":"stop credenti valid server set server gcp sub domain approach access domain http app mydomain com http api mydomain com http file mydomain com browser think correct respons connect python sdk init follow error backend api session session loginerror fail get token error http api mydomain com bad request like caus error",
        "Question_preprocessed_content":"stop credenti valid server set server gcp approach access domain browser think correct respons connect python sdk follow error like caus error",
        "Question_gpt_summary_original":"The user is encountering an error while trying to connect to a ClearML server using the python SDK via clearml-init. The error message indicates that the credentials are not being validated due to a Bad Request error. The user is seeking assistance in identifying the likely causes of this error.",
        "Question_gpt_summary":"user encount error try connect server python sdk init error messag indic credenti valid bad request error user seek assist identifi like caus error",
        "Answer_original_content":"follow discuss load balanc block request payload fix work allow method chang post request environ variabl",
        "Answer_preprocessed_content":"follow discuss load balanc block request payload fix work allow method chang request environ variabl",
        "Answer_gpt_summary_original":"Solution: A fix is being worked on to allow the method to be changed to a POST request via an environment variable.",
        "Answer_gpt_summary":"solut fix work allow method chang post request environ variabl"
    },
    {
        "Question_title":"Is it possible to share compute instance with other user?",
        "Question_body":"<p>I create one compute instance 'yhd-notebook' in Azure Machine Learning compute with user1. When I login with user2, and try to open the JupyterLab of this compute instance, it shows an error message like below.<\/p>\n\n<blockquote>\n  <p>User user2 does not have access to compute instance yhd-notebook.<\/p>\n  \n  <p>Only the creator can access a compute instance.<\/p>\n  \n  <p>Click here to sign out and sign in again with a different account.<\/p>\n<\/blockquote>\n\n<p>Is it possible to share compute instance with another user? BTW, both user1 and user2 have Owner role with the Azure subscription.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1583388412943,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":8.0,
        "Question_view_count":3705.0,
        "Answer_body":"<p>According to MS, all users in the workspace contributor and owner role can create, delete, start, stop, and restart compute instances across the workspace. However, only the creator of a specific compute instance is allowed to access Jupyter, JupyterLab, and RStudio on that compute instance. The creator of the compute instance has the compute instance dedicated to them, have root access, and can terminal in through Jupyter. Compute instance will have single-user login of creator user and all actions will use that user\u2019s identity for RBAC and attribution of experiment runs. SSH access is controlled through public\/private key mechanism.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":7.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60539094",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1583393458820,
        "Question_original_content":"possibl share comput instanc user creat comput instanc yhd notebook comput user login user try open jupyterlab comput instanc show error messag like user user access comput instanc yhd notebook creator access comput instanc click sign sign differ account possibl share comput instanc user btw user user owner role azur subscript",
        "Question_preprocessed_content":"possibl share comput instanc user creat comput instanc comput user login user try open jupyterlab comput instanc show error messag like user user access comput instanc creator access comput instanc click sign sign differ account possibl share comput instanc user btw user user owner role azur subscript",
        "Question_gpt_summary_original":"The user is facing a challenge in sharing a compute instance named 'yhd-notebook' with another user. When the second user tries to access the JupyterLab of the compute instance, an error message appears stating that only the creator has access to it. Both users have the Owner role with the Azure subscription, and the user is seeking a solution to share the compute instance with another user.",
        "Question_gpt_summary":"user face challeng share comput instanc name yhd notebook user second user tri access jupyterlab comput instanc error messag appear state creator access user owner role azur subscript user seek solut share comput instanc user",
        "Answer_original_content":"accord user workspac contributor owner role creat delet start stop restart comput instanc workspac creator specif comput instanc allow access jupyt jupyterlab rstudio comput instanc creator comput instanc comput instanc dedic root access termin jupyt comput instanc singl user login creator user action us user ident rbac attribut experi run ssh access control public privat kei mechan",
        "Answer_preprocessed_content":"accord user workspac contributor owner role creat delet start stop restart comput instanc workspac creator specif comput instanc allow access jupyt jupyterlab rstudio comput instanc creator comput instanc comput instanc dedic root access termin jupyt comput instanc login creator user action us user ident rbac attribut experi run ssh access control kei mechan",
        "Answer_gpt_summary_original":"Solution: The creator of the compute instance needs to share the JupyterLab URL and token with the other user. The other user can then use the URL and token to access the JupyterLab of the compute instance. Alternatively, the creator of the compute instance can add the other user as a collaborator to the workspace, which will give them access to the compute instance.",
        "Answer_gpt_summary":"solut creator comput instanc need share jupyterlab url token user user us url token access jupyterlab comput instanc altern creator comput instanc add user collabor workspac access comput instanc"
    },
    {
        "Question_title":"Invoking SageMaker Endpoint for PyTorch Model",
        "Question_body":"<p>I'm trying to call my SageMaker model endpoint both from Postman and the AWS CLI. The endpoint's status is &quot;in service&quot; but whenever I try to call it it gives me an error. When I try to use the predict function in the SageMaker notebook and provide it a numpy array (ex. <code>np.array([1,2,3,4])<\/code>), it successfully gives me an output. I'm unsure what I'm doing wrong.<\/p>\n<pre><code>$ aws2 sagemaker-runtime invoke-endpoint \\\n$ --endpoint-name=pytorch-model \\\n$ --body=1,2 \\\n$ --content-type=text\/csv \\\n$ --cli-binary-format=raw-in-base64-out \\\n$ output.json\n\nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message &quot;tensors used as indices must be long, byte or bool tensors\nTraceback (most recent call last):\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py&quot;, line 125, in transform\n    result = self._transform_fn(self._model, input_data, content_type, accept)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py&quot;, line 215, in _default_transform_fn\n    prediction = self._predict_fn(data, model)\n  File &quot;\/opt\/ml\/model\/code\/pytorch-model-reco.py&quot;, line 268, in predict_fn\n    return torch.argsort(- final_matrix[input_data, :], dim = 1)\nIndexError: tensors used as indices must be long, byte or bool tensors\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596530497410,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":394.0,
        "Answer_body":"<p>The clue is in the final few lines of your stacktrace:<\/p>\n<pre><code>  File &quot;\/opt\/ml\/model\/code\/pytorch-model-reco.py&quot;, line 268, in predict_fn\n    return torch.argsort(- final_matrix[input_data, :], dim = 1)\nIndexError: tensors used as indices must be long, byte or bool tensors\n<\/code><\/pre>\n<p>In your <code>predict_fn<\/code> in <code>pytorch-model-reco.py<\/code> on line 268, you're trying to use <code>input_data<\/code> as indices for <code>final_matrix<\/code>, but <code>input_data<\/code> is the wrong type.<\/p>\n<p>I would guess there is some type casting that your <code>predict_fn<\/code> should be doing when the input type is <code>text\/csv<\/code>. This type casting is happening outside of the <code>predict_fn<\/code> when your input type is numpy data. Taking a look at the <a href=\"https:\/\/github.com\/aws\/sagemaker-inference-toolkit\/tree\/master\/src\/sagemaker_inference\" rel=\"nofollow noreferrer\"><code>sagemaker_inference<\/code><\/a> source code might reveal more.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63243154",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1596555829307,
        "Question_original_content":"invok endpoint pytorch model try model endpoint postman aw cli endpoint statu servic try give error try us predict function notebook provid numpi arrai arrai successfulli give output unsur wrong aw runtim invok endpoint endpoint pytorch model bodi content type text csv cli binari format raw base output json error occur modelerror call invokeendpoint oper receiv server error model messag tensor indic long byte bool tensor traceback recent file opt conda lib python site packag infer transform line transform result self transform self model input data content type accept file opt conda lib python site packag infer transform line default transform predict self predict data model file opt model code pytorch model reco line predict return torch argsort final matrix input data dim indexerror tensor indic long byte bool tensor",
        "Question_preprocessed_content":"invok endpoint pytorch model try model endpoint postman aw cli endpoint statu servic try give error try us predict function notebook provid numpi arrai successfulli give output unsur wrong",
        "Question_gpt_summary_original":"The user is encountering challenges while trying to call their SageMaker model endpoint from Postman and AWS CLI. Although the endpoint's status is \"in service,\" it gives an error whenever the user tries to call it. However, when the user uses the predict function in the SageMaker notebook and provides it with a numpy array, it successfully gives an output. The error message received while invoking the endpoint indicates that tensors used as indices must be long, byte, or bool tensors.",
        "Question_gpt_summary":"user encount challeng try model endpoint postman aw cli endpoint statu servic give error user tri user us predict function notebook provid numpi arrai successfulli give output error messag receiv invok endpoint indic tensor indic long byte bool tensor",
        "Answer_original_content":"clue final line stacktrac file opt model code pytorch model reco line predict return torch argsort final matrix input data dim indexerror tensor indic long byte bool tensor predict pytorch model reco line try us input data indic final matrix input data wrong type guess type cast predict input type text csv type cast happen outsid predict input type numpi data take look infer sourc code reveal",
        "Answer_preprocessed_content":"clue final line stacktrac line try us indic wrong type guess type cast input type type cast happen outsid input type numpi data take look sourc code reveal",
        "Answer_gpt_summary_original":"Solution: The error message received while invoking the endpoint indicates that tensors used as indices must be long, byte, or bool tensors. The solution is to do some type casting that the predict_fn should be doing when the input type is text\/csv. This type casting is happening outside of the predict_fn when the input type is numpy data. Taking a look at the sagemaker_inference source code might reveal more.",
        "Answer_gpt_summary":"solut error messag receiv invok endpoint indic tensor indic long byte bool tensor solut type cast predict input type text csv type cast happen outsid predict input type numpi data take look infer sourc code reveal"
    },
    {
        "Question_title":"Azure machine learning designer - edit columns stuck on loading",
        "Question_body":"Azure machine learning designer :\n\nI have a dataset on the designer connected to a Normalize data module but it keeps loading when i try to Edit columns on Normalize data module with no result or errors.\nThe same thing happens with Select columns in dataset module.\n\nI have tried to recreate and restart and even deleted the whole resource group but no luck.\nI tried on both mac and windows with different browsers but still getting stuck on the same place.\n\nany idea on how to solve this issue?\n\nThanks!\n\nScreenshot:\nhttps:\/\/i.imgur.com\/P0oWrGR.png",
        "Question_answer_count":1,
        "Question_comment_count":12,
        "Question_creation_time":1617486098190,
        "Question_favorite_count":10.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@AyushBhardwaj-4354 @yazeenjasim-8837 @AnshulSharma-6861 This issue is now fixed in all regions and it does not require an additional parameter to be added to the URL. Please try and let us know if it works fine.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/343332\/azure-machine-learning-designer-edit-columns-stuck.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-06T12:14:07.023Z",
                "Answer_score":1,
                "Answer_body":"@AyushBhardwaj-4354 @yazeenjasim-8837 @AnshulSharma-6861 This issue is now fixed in all regions and it does not require an additional parameter to be added to the URL. Please try and let us know if it works fine.",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1617711247023,
        "Question_original_content":"design edit column stuck load design dataset design connect normal data modul keep load try edit column normal data modul result error thing happen select column dataset modul tri recreat restart delet resourc group luck tri mac window differ browser get stuck place idea solv issu thank screenshot http imgur com powrgr png",
        "Question_preprocessed_content":"design edit column stuck load design dataset design connect normal data modul keep load try edit column normal data modul result error thing happen select column dataset modul tri recreat restart delet resourc group luck tri mac window differ browser get stuck place idea solv issu thank screenshot",
        "Question_gpt_summary_original":"The user is facing challenges with the Azure machine learning designer as the Edit columns feature on the Normalize data module and the Select columns feature in the dataset module are stuck on loading with no results or errors. The user has tried recreating, restarting, and deleting the resource group, but the issue persists on both Mac and Windows with different browsers. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user face challeng design edit column featur normal data modul select column featur dataset modul stuck load result error user tri recreat restart delet resourc group issu persist mac window differ browser user seek help resolv issu",
        "Answer_original_content":"ayushbhardwaj yazeenjasim anshulsharma issu fix region requir addit paramet ad url try let know work fine",
        "Answer_preprocessed_content":"issu fix region requir addit paramet ad url try let know work fine",
        "Answer_gpt_summary_original":"Solution: The issue with the Edit columns feature on the Normalize data module and the Select columns feature in the dataset module being stuck on loading with no results or errors has been fixed in all regions. The user is advised to try again and see if the issue is resolved. No additional parameter needs to be added to the URL.",
        "Answer_gpt_summary":"solut issu edit column featur normal data modul select column featur dataset modul stuck load result error fix region user advis try issu resolv addit paramet need ad url"
    },
    {
        "Question_title":"Render hyperlink in pandas df in jupyterlab",
        "Question_body":"<p>I am trying to render a url inside a pandas dataframe output.  I followed along with some of the other examples out there, here is my implementation:<\/p>\n\n<pre><code>def create_url(product_name):\n    search = 'http:\/\/www.example.com\/search'\n    url = 'http:\/\/www.example.com\/search\/'+product_name\n    return url\n\ndef make_clickable(url):\n    return '&lt;a target=\"_blank\" href=\"{}\"&gt;{}&lt;\/a&gt;'.format(url, url)\n\n...\n\ndf['url'] = df['product_name'].apply(format_url)\ndf.style.format({'url': make_clickable})\n<\/code><\/pre>\n\n<p>This produces a correctly formatted raw text hyperlink, however its not clickable within the output.<\/p>\n\n<p>I should add that I'm doing this in an AWS sagemaker jupyterlab notebook which potentially disables hyperlinking in the output.  Not sure how I would check that though.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1557848494950,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":241.0,
        "Answer_body":"<p>If this doesn't work, I'm guessing it's an AWS thing<\/p>\n\n<ol>\n<li><code>IPython.display.HTML<\/code><\/li>\n<li><code>pandas.DataFrame.to_html<\/code> with <code>escape=False<\/code><\/li>\n<li><code>pandas.set_option('display.max_colwidth', 2000)<\/code> Must be a large number to accommodate length of link tag.  I'll say that I think this is broken.  It shouldn't be necessary to set <code>'display.max_colwidth'<\/code> in order to make sure <code>to_html<\/code> outputs properly.  But it is :-\/<\/li>\n<\/ol>\n\n<hr>\n\n<pre><code>from IPython import display\n\npd.set_option('display.max_colwidth', 2000)\n\ndisplay.HTML(df.assign(url=[*map(make_clickable, df.url)]).to_html(escape=False))\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YcVj6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YcVj6.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1557931098116,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56134165",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1557849422180,
        "Question_original_content":"render hyperlink panda jupyterlab try render url insid panda datafram output follow exampl implement def creat url product search http exampl com search url http exampl com search product return url def clickabl url return format url url url product appli format url style format url clickabl produc correctli format raw text hyperlink clickabl output add jupyterlab notebook potenti disabl hyperlink output sure check",
        "Question_preprocessed_content":"render hyperlink panda jupyterlab try render url insid panda datafram output follow exampl implement produc correctli format raw text hyperlink clickabl output add jupyterlab notebook potenti disabl hyperlink output sure check",
        "Question_gpt_summary_original":"The user is facing a challenge in rendering a clickable hyperlink in a pandas dataframe output in JupyterLab. The user has implemented a function to create a URL and another function to make the URL clickable, but the hyperlink is not clickable in the output. The user suspects that the issue may be due to AWS Sagemaker JupyterLab notebook potentially disabling hyperlinking in the output.",
        "Question_gpt_summary":"user face challeng render clickabl hyperlink panda datafram output jupyterlab user implement function creat url function url clickabl hyperlink clickabl output user suspect issu jupyterlab notebook potenti disabl hyperlink output",
        "Answer_original_content":"work guess aw thing ipython displai html panda datafram html escap fals panda set option displai max colwidth larg number accommod length link tag think broken shouldn necessari set displai max colwidth order sure html output properli ipython import displai set option displai max colwidth displai html assign url map clickabl url html escap fals",
        "Answer_preprocessed_content":"work guess aw thing larg number accommod length link tag think broken shouldn necessari set order sure output properli",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Using `IPython.display.HTML` to render the dataframe output as HTML and make the hyperlink clickable.\n2. Using `pandas.DataFrame.to_html` with `escape=False` to render the dataframe output as HTML and make the hyperlink clickable.\n3. Setting `pandas.set_option('display.max_colwidth', 2000)` to a large number to accommodate the length of the link tag.\n\nAll of these solutions involve rendering the dataframe output as HTML and making the hyperlink clickable. The user suspects that the issue may be due to AWS Sagemaker JupyterLab notebook potentially disabling hyperlinking in the output.",
        "Answer_gpt_summary":"possibl solut mention discuss ipython displai html render datafram output html hyperlink clickabl panda datafram html escap fals render datafram output html hyperlink clickabl set panda set option displai max colwidth larg number accommod length link tag solut involv render datafram output html make hyperlink clickabl user suspect issu jupyterlab notebook potenti disabl hyperlink output"
    },
    {
        "Question_title":"Rerun a deleted run in wandb sweep",
        "Question_body":"<p>Hi,<\/p>\n<p>Assuming I have a sweep of runs. For some reason, I wanna rerun a few of the runs. So I go ahead and delete those runs in the dashboard. But then even if I rerun the command (<code>wandb agent ...<\/code>), wandb is not able to rerun those runs. It will show all runs have been completed. Could wandb add the feature to rerun the runs that are not in the dashboards (for example, those that are deleted)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1676042396664,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":29.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/taochen\">@taochen<\/a>, rerunning deleted runs of a sweep is supported for grid search only. Please see the <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/faq#can-i-rerun-a-grid-search\">following guide<\/a> on the steps to take to execute correctly. If you find that does not work for you, provide a link to your workspace and we\u2019ll take a closer look.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/rerun-a-deleted-run-in-wandb-sweep\/3860",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-10T23:12:55.536Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/taochen\">@taochen<\/a>, rerunning deleted runs of a sweep is supported for grid search only. Please see the <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/faq#can-i-rerun-a-grid-search\">following guide<\/a> on the steps to take to execute correctly. If you find that does not work for you, provide a link to your workspace and we\u2019ll take a closer look.<\/p>",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1676070775536,
        "Question_original_content":"rerun delet run sweep assum sweep run reason wanna rerun run ahead delet run dashboard rerun command agent abl rerun run run complet add featur rerun run dashboard exampl delet",
        "Question_preprocessed_content":"rerun delet run sweep assum sweep run reason wanna rerun run ahead delet run dashboard rerun command abl rerun run run complet add featur rerun run dashboard",
        "Question_gpt_summary_original":"The user encountered a challenge with rerunning deleted runs in a wandb sweep. After deleting some runs in the dashboard, the user was unable to rerun those runs using the wandb agent command as wandb showed all runs as completed. The user suggests adding a feature to rerun deleted runs.",
        "Question_gpt_summary":"user encount challeng rerun delet run sweep delet run dashboard user unabl rerun run agent command show run complet user suggest ad featur rerun delet run",
        "Answer_original_content":"taochen rerun delet run sweep support grid search follow guid step execut correctli work provid link workspac closer look",
        "Answer_preprocessed_content":"rerun delet run sweep support grid search follow guid step execut correctli work provid link workspac closer look",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that rerunning deleted runs of a sweep is supported for grid search only. The user is directed to follow the steps provided in the guide on how to execute it correctly. If the suggested solution does not work, the user is advised to provide a link to their workspace for further assistance.",
        "Answer_gpt_summary":"solut discuss suggest rerun delet run sweep support grid search user direct follow step provid guid execut correctli suggest solut work user advis provid link workspac assist"
    },
    {
        "Question_title":"Freeze stage with parameter substitution",
        "Question_body":"<p>Hi, I am not sure if this is expected or not, but freezing a stage with parameter substitution gives:<br>\n<code>ERROR: failed to freeze 'Test' - cannot dump a parametrized stage: 'Test'<\/code><\/p>\n<pre><code class=\"lang-auto\"># dvc.yaml\nvars:\n  - folder: .\n\nstages:\n  Test:\n    cmd: dir ${folder}\n    always_changed: true\n<\/code><\/pre>\n<p>Command to run: <code>dvc freeze Test<\/code>. Is this normal behaviour? If yes, why?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1622098167167,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":390.0,
        "Answer_body":"<p>Hi, Jimmy.<\/p>\n<p>We decided to not implement supporting it for parametrized stages for now. Instead, we want users to be able to edit\/modify <code>dvc.yaml<\/code>. Here, you can set <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#stage-entries\"><code>frozen: true<\/code><\/a>.<\/p>\n<p>Please feel free to open an issue in the repo. Thanks.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/freeze-stage-with-parameter-substitution\/761",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-05-27T06:55:58.253Z",
                "Answer_body":"<p>Hi, Jimmy.<\/p>\n<p>We decided to not implement supporting it for parametrized stages for now. Instead, we want users to be able to edit\/modify <code>dvc.yaml<\/code>. Here, you can set <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#stage-entries\"><code>frozen: true<\/code><\/a>.<\/p>\n<p>Please feel free to open an issue in the repo. Thanks.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"freez stage paramet substitut sure expect freez stage paramet substitut give error fail freez test dump parametr stage test yaml var folder stage test cmd dir folder chang true command run freez test normal behaviour ye thank",
        "Question_preprocessed_content":"freez stage paramet substitut sure expect freez stage paramet substitut give command run normal behaviour ye thank",
        "Question_gpt_summary_original":"The user is encountering an error when trying to freeze a stage with parameter substitution. The error message states that it is not possible to freeze a parametrized stage. The user is unsure if this is expected behavior and is seeking clarification on why this is happening.",
        "Question_gpt_summary":"user encount error try freez stage paramet substitut error messag state possibl freez parametr stage user unsur expect behavior seek clarif happen",
        "Answer_original_content":"jimmi decid implement support parametr stage instead want user abl edit modifi yaml set frozen true feel free open issu repo thank",
        "Answer_preprocessed_content":"jimmi decid implement support parametr stage instead want user abl set feel free open issu repo thank",
        "Answer_gpt_summary_original":"Solution: The user can set \"frozen: true\" in the dvc.yaml file instead of trying to freeze a parametrized stage. The discussion suggests that supporting freezing for parametrized stages is not currently implemented. The user is also encouraged to open an issue in the repository if they have further questions or concerns.",
        "Answer_gpt_summary":"solut user set frozen true yaml file instead try freez parametr stage discuss suggest support freez parametr stage current implement user encourag open issu repositori question concern"
    },
    {
        "Question_title":"Using Hyperparameter Tuning Jobs over Training and Preprocessing",
        "Question_body":"Some data science teams want to tune the hyperparameters of their preprocessing jobs alongside ML model training jobs.\n\nDoes AWS have a recommended approach to establish this using Sagemaker Hyperparameter tuning?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610658074000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":65.0,
        "Answer_body":"It depends on the dataset and the question for ML to answer.\n\nYes, it is feasible to do HPO with preprocessing. However, to run a HPO job, it is required to define to a specific target to achieve, e.g. maximize\/minimize certain values during the whole HPO process. Thus, it is important to understand what is the target during preprocessing. If the answer is yes, they should be able to leverage Hyperparameter Tuning Jobs.\n\nHere is how HPO works in SageMaker. Firstly, we define each training Job with output in a container and specify the hyperparameters in \/opt\/ml\/input\/config\/hyperparameters.json. When we run the pipeline using HyperparameterTuner in SageMaker, the initial Job can pass the hyperparameters to the Pipeline for HPO, and return the model with highest score.\n\nOption 1, if there is a clear defined target for preprocessing to achieve, we can also do HPO separately for data preprocessing through defining the function and outputs in a container and use HyperparameterTuner fit to tune the preprocessing.\n\nOption 2. include the preprocessing + training code in the whole SageMaker Training Job. But then you can't use separate infrastructure for training and preprocessing.\n\nSo it depends on what exactly they are looking for, but they can likely use SageMaker HPO.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3xcWKDPHR8ylWSaX83lNKQ\/using-hyperparameter-tuning-jobs-over-training-and-preprocessing",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-15T20:17:42.000Z",
                "Answer_score":0,
                "Answer_body":"It depends on the dataset and the question for ML to answer.\n\nYes, it is feasible to do HPO with preprocessing. However, to run a HPO job, it is required to define to a specific target to achieve, e.g. maximize\/minimize certain values during the whole HPO process. Thus, it is important to understand what is the target during preprocessing. If the answer is yes, they should be able to leverage Hyperparameter Tuning Jobs.\n\nHere is how HPO works in SageMaker. Firstly, we define each training Job with output in a container and specify the hyperparameters in \/opt\/ml\/input\/config\/hyperparameters.json. When we run the pipeline using HyperparameterTuner in SageMaker, the initial Job can pass the hyperparameters to the Pipeline for HPO, and return the model with highest score.\n\nOption 1, if there is a clear defined target for preprocessing to achieve, we can also do HPO separately for data preprocessing through defining the function and outputs in a container and use HyperparameterTuner fit to tune the preprocessing.\n\nOption 2. include the preprocessing + training code in the whole SageMaker Training Job. But then you can't use separate infrastructure for training and preprocessing.\n\nSo it depends on what exactly they are looking for, but they can likely use SageMaker HPO.",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1610741862000,
        "Question_original_content":"hyperparamet tune job train preprocess data scienc team want tune hyperparamet preprocess job alongsid model train job aw recommend approach establish hyperparamet tune",
        "Question_preprocessed_content":"hyperparamet tune job train preprocess data scienc team want tune hyperparamet preprocess job alongsid model train job aw recommend approach establish hyperparamet tune",
        "Question_gpt_summary_original":"The user is seeking guidance on how to use Sagemaker Hyperparameter tuning to tune the hyperparameters of their preprocessing jobs alongside ML model training jobs.",
        "Question_gpt_summary":"user seek guidanc us hyperparamet tune tune hyperparamet preprocess job alongsid model train job",
        "Answer_original_content":"depend dataset question answer ye feasibl hpo preprocess run hpo job requir defin specif target achiev maxim minim certain valu hpo process import understand target preprocess answer ye abl leverag hyperparamet tune job hpo work firstli defin train job output contain specifi hyperparamet opt input config hyperparamet json run pipelin hyperparametertun initi job pass hyperparamet pipelin hpo return model highest score option clear defin target preprocess achiev hpo separ data preprocess defin function output contain us hyperparametertun fit tune preprocess option includ preprocess train code train job us separ infrastructur train preprocess depend exactli look like us hpo",
        "Answer_preprocessed_content":"depend dataset question answer ye feasibl hpo preprocess run hpo job requir defin specif target achiev certain valu hpo process import understand target preprocess answer ye abl leverag hyperparamet tune job hpo work firstli defin train job output contain specifi hyperparamet run pipelin hyperparametertun initi job pass hyperparamet pipelin hpo return model highest score option clear defin target preprocess achiev hpo separ data preprocess defin function output contain us hyperparametertun fit tune preprocess option includ preprocess train code train job us separ infrastructur train preprocess depend exactli look like us hpo",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. It is feasible to do HPO with preprocessing, but it is important to define a specific target to achieve during the whole HPO process.\n2. HPO works in SageMaker by defining each training job with output in a container and specifying the hyperparameters in \/opt\/ml\/input\/config\/hyperparameters.json. The initial job can pass the hyperparameters to the pipeline for HPO and return the model with the highest score.\n3. Option 1 is to do HPO separately for data preprocessing by defining the function and outputs in a container and using HyperparameterTuner fit to tune the preprocessing.\n4. Option 2 is to include the preprocessing + training code in the whole SageMaker Training Job",
        "Answer_gpt_summary":"possibl solut mention discuss feasibl hpo preprocess import defin specif target achiev hpo process hpo work defin train job output contain specifi hyperparamet opt input config hyperparamet json initi job pass hyperparamet pipelin hpo return model highest score option hpo separ data preprocess defin function output contain hyperparametertun fit tune preprocess option includ preprocess train code train job"
    },
    {
        "Question_title":"Set Github as Artifacts location",
        "Question_body":"Im testing MLFLOW 1.0.0. I run MLFLOW in docker container, myArifacts location is Minio bucket (Minio is running in its own docker container), backend store is Postgres ( running in its own docker container); I train my model in Jupiter notebook ( also in its own docker container). Is it possible to use Github as an artifact location?\n\n\nThank you.",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1561645149000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":20.0,
        "Answer_body":"No, you can\u2019t use GitHub by default, but it may be possible to write an artifact store plugin that does that. Our idea was that artifacts can be very large, however (multi-gigabyte models or multi-terabyte datasets), so it might not make sense to use GitHub.\n\n\nMatei\n\n\n\nOn Jun 27, 2019, at 11:19 AM, SoniaK <sofia....@8451.com> wrote:\n\n\nIm testing MLFLOW 1.0.0. I run MLFLOW in docker container, myArifacts location is Minio bucket (Minio is running in its own docker container), backend store is Postgres ( running in its own docker container); I train my model in Jupiter notebook ( also in its own docker container). Is it possible to use Github as an artifact location?\n\n\nThank you.\n\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/59f7455b-4b7b-41f5-8058-6bb231bb0807%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.. Hi, I think you should use the right tool for the right job. GitHub is for sharing code.\nThx\n\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/F25604B5-40AA-4907-960B-C588609BC72A%40databricks.com.\n\ue5d3. Thank you, Matei and Zahir.\n\n\nOn Thursday, June 27, 2019 at 2:36:20 PM UTC-5, zahir hamroune wrote:\nHi, I think you should use the right tool for the right job. GitHub is for sharing code.\nThx\n\n\n\nOn 27 Jun 2019, at 21:34, Matei Zaharia <ma...@databricks.com> wrote:\n\n\nNo, you can\u2019t use GitHub by default, but it may be possible to write an artifact store plugin that does that. Our idea was that artifacts can be very large, however (multi-gigabyte models or multi-terabyte datasets), so it might not make sense to use GitHub.\n\n\nMatei\n\n\n\nOn Jun 27, 2019, at 11:19 AM, SoniaK <sofia...@8451.com> wrote:\n\n\nIm testing MLFLOW 1.0.0. I run MLFLOW in docker container, myArifacts location is Minio bucket (Minio is running in its own docker container), backend store is Postgres ( running in its own docker container); I train my model in Jupiter notebook ( also in its own docker container). Is it possible to use Github as an artifact location?\n\n\nThank you.\n\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow...@googlegroups.com.\n\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/59f7455b-4b7b-41f5-8058-6bb231bb0807%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.\n\n\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow...@googlegroups.com.\n\ue5d3. Hi Zahir.\u00a0 If GitHub is not the right tool for the job (I don't disagree with that), what would you say IS the right tool for persisting artifacts?\n\nThanks in advance!\n\n\nOn Thursday, June 27, 2019 at 2:36:20 PM UTC-5, zahir hamroune wrote:\n\ue5d3. You can use a shared filesystem or blob store, such as AWS S3, Azure Blob Storage, HDFS, or a POSIX file system set up through NFS.\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\n\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/9bd4f3f5-4574-473e-8fd9-81361945f52f%40googlegroups.com.\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/QKRO33wr3hM",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-06-27T15:34:12",
                "Answer_body":"No, you can\u2019t use GitHub by default, but it may be possible to write an artifact store plugin that does that. Our idea was that artifacts can be very large, however (multi-gigabyte models or multi-terabyte datasets), so it might not make sense to use GitHub.\n\n\nMatei\n\n\n\nOn Jun 27, 2019, at 11:19 AM, SoniaK <sofia....@8451.com> wrote:\n\n\nIm testing MLFLOW 1.0.0. I run MLFLOW in docker container, myArifacts location is Minio bucket (Minio is running in its own docker container), backend store is Postgres ( running in its own docker container); I train my model in Jupiter notebook ( also in its own docker container). Is it possible to use Github as an artifact location?\n\n\nThank you.\n\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/59f7455b-4b7b-41f5-8058-6bb231bb0807%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2019-06-27T15:36:20",
                "Answer_body":"Hi, I think you should use the right tool for the right job. GitHub is for sharing code.\nThx\n\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/F25604B5-40AA-4907-960B-C588609BC72A%40databricks.com.\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-06-27T16:03:55",
                "Answer_body":"Thank you, Matei and Zahir.\n\n\nOn Thursday, June 27, 2019 at 2:36:20 PM UTC-5, zahir hamroune wrote:\nHi, I think you should use the right tool for the right job. GitHub is for sharing code.\nThx\n\n\n\nOn 27 Jun 2019, at 21:34, Matei Zaharia <ma...@databricks.com> wrote:\n\n\nNo, you can\u2019t use GitHub by default, but it may be possible to write an artifact store plugin that does that. Our idea was that artifacts can be very large, however (multi-gigabyte models or multi-terabyte datasets), so it might not make sense to use GitHub.\n\n\nMatei\n\n\n\nOn Jun 27, 2019, at 11:19 AM, SoniaK <sofia...@8451.com> wrote:\n\n\nIm testing MLFLOW 1.0.0. I run MLFLOW in docker container, myArifacts location is Minio bucket (Minio is running in its own docker container), backend store is Postgres ( running in its own docker container); I train my model in Jupiter notebook ( also in its own docker container). Is it possible to use Github as an artifact location?\n\n\nThank you.\n\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow...@googlegroups.com.\n\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/59f7455b-4b7b-41f5-8058-6bb231bb0807%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.\n\n\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow...@googlegroups.com.\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-07-01T18:14:36",
                "Answer_body":"Hi Zahir.\u00a0 If GitHub is not the right tool for the job (I don't disagree with that), what would you say IS the right tool for persisting artifacts?\n\nThanks in advance!\n\n\nOn Thursday, June 27, 2019 at 2:36:20 PM UTC-5, zahir hamroune wrote:\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-07-02T06:37:31",
                "Answer_body":"You can use a shared filesystem or blob store, such as AWS S3, Azure Blob Storage, HDFS, or a POSIX file system set up through NFS.\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\n\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/9bd4f3f5-4574-473e-8fd9-81361945f52f%40googlegroups.com.\n\ue5d3"
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"set github artifact locat test run docker contain myarifact locat minio bucket minio run docker contain backend store postgr run docker contain train model jupit notebook docker contain possibl us github artifact locat thank",
        "Question_preprocessed_content":"set github artifact locat test run docker contain myarifact locat minio bucket backend store postgr train model jupit notebook possibl us github artifact locat thank",
        "Question_gpt_summary_original":"The user is facing a challenge of whether it is possible to use Github as an artifact location while testing MLFLOW 1.0.0. They are currently running MLFLOW, Minio, Postgres, and Jupiter notebook in separate docker containers, with Minio serving as the current artifact location.",
        "Question_gpt_summary":"user face challeng possibl us github artifact locat test current run minio postgr jupit notebook separ docker contain minio serv current artifact locat",
        "Answer_original_content":"us github default possibl write artifact store plugin idea artifact larg multi gigabyt model multi terabyt dataset sens us github matei jun soniak wrote test run docker contain myarifact locat minio bucket minio run docker contain backend store postgr run docker contain train model jupit notebook docker contain possibl us github artifact locat thank receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user bbbb googlegroup com option visit http group googl com optout think us right tool right job github share code thx view discuss web visit http group googl com msgid user cbca databrick com thank matei zahir thursdai june utc zahir hamroun wrote think us right tool right job github share code thx jun matei zaharia wrote us github default possibl write artifact store plugin idea artifact larg multi gigabyt model multi terabyt dataset sens us github matei jun soniak wrote test run docker contain myarifact locat minio bucket minio run docker contain backend store postgr run docker contain train model jupit notebook docker contain possibl us github artifact locat thank receiv messag subscrib googl group user group unsubscrib group stop receiv email send email googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user bbbb googlegroup com option visit http group googl com optout receiv messag subscrib googl group user group unsubscrib group stop receiv email send email googlegroup com zahir github right tool job disagre right tool persist artifact thank advanc thursdai june utc zahir hamroun wrote us share filesystem blob store aw azur blob storag hdf posix file set nf receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user bdff googlegroup com",
        "Answer_preprocessed_content":"us github default possibl write artifact store plugin idea artifact larg sens us github matei jun soniak wrote test run docker contain myarifact locat minio bucket backend store postgr train model jupit notebook possibl us github artifact locat thank receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit think us right tool right job github share code thx view discuss web visit thank matei zahir thursdai june zahir hamroun wrote think us right tool right job github share code thx jun matei zaharia wrote us github default possibl write artifact store plugin idea artifact larg sens us github matei jun soniak wrote test run docker contain myarifact locat minio bucket backend store postgr train model jupit notebook possibl us github artifact locat thank receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit receiv messag subscrib googl group group unsubscrib group stop receiv email send email zahir github right tool job right tool persist artifact thank advanc thursdai june zahir hamroun wrote us share filesystem blob store aw azur blob storag hdf posix file set nf receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit",
        "Answer_gpt_summary_original":"The discussion suggests that it is not possible to use Github as an artifact location by default, but it may be possible to write an artifact store plugin that does that. However, it might not make sense to use Github as artifacts can be very large. The participants suggest using a shared filesystem or blob store, such as AWS S3, Azure Blob Storage, HDFS, or a POSIX file system set up through NFS, as a better alternative for persisting artifacts.",
        "Answer_gpt_summary":"discuss suggest possibl us github artifact locat default possibl write artifact store plugin sens us github artifact larg particip suggest share filesystem blob store aw azur blob storag hdf posix file set nf better altern persist artifact"
    },
    {
        "Question_title":"Hyperparam search on huggingface with optuna fails with wandb error",
        "Question_body":"<p>I'm using this simple script, using the example blog post. However, it fails because of <code>wandb<\/code>. It was of no use to make <code>wandb<\/code> OFFLINE as well.<\/p>\n<pre><code>from datasets import load_dataset, load_metric\nfrom transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n                          Trainer, TrainingArguments)\nimport wandb\n\n\nwandb.init()\n\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\ndataset = load_dataset('glue', 'mrpc')\nmetric = load_metric('glue', 'mrpc')\n\ndef encode(examples):\n    outputs = tokenizer(\n        examples['sentence1'], examples['sentence2'], truncation=True)\n    return outputs\n\nencoded_dataset = dataset.map(encode, batched=True)\n\ndef model_init():\n    return AutoModelForSequenceClassification.from_pretrained(\n        'distilbert-base-uncased', return_dict=True)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = predictions.argmax(axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n# Evaluate during training and a bit more often\n# than the default to be able to prune bad trials early.\n# Disabling tqdm is a matter of preference.\ntraining_args = TrainingArguments(\n    &quot;test&quot;, eval_steps=500, disable_tqdm=True,\n    evaluation_strategy='steps',)\n\ntrainer = Trainer(\n    args=training_args,\n    tokenizer=tokenizer,\n    train_dataset=encoded_dataset[&quot;train&quot;],\n    eval_dataset=encoded_dataset[&quot;validation&quot;],\n    model_init=model_init,\n    compute_metrics=compute_metrics,\n)\n\ndef my_hp_space(trial):\n    return {\n        &quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 1e-4, 1e-2, log=True),\n        &quot;weight_decay&quot;: trial.suggest_float(&quot;weight_decay&quot;, 0.1, 0.3),\n        &quot;num_train_epochs&quot;: trial.suggest_int(&quot;num_train_epochs&quot;, 5, 10),\n        &quot;seed&quot;: trial.suggest_int(&quot;seed&quot;, 20, 40),\n        &quot;per_device_train_batch_size&quot;: trial.suggest_categorical(&quot;per_device_train_batch_size&quot;, [32, 64]),\n    }\n\n\ntrainer.hyperparameter_search(\n    direction=&quot;maximize&quot;,\n    backend=&quot;optuna&quot;,\n    n_trials=10,\n    hp_space=my_hp_space\n)\n<\/code><\/pre>\n<p><code>Trail 0<\/code> finishes successfully, but next <code>Trail 1<\/code> crashes with following error:<\/p>\n<pre><code>  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/transformers\/integrations.py&quot;, line 138, in _objective\n    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/transformers\/trainer.py&quot;, line 1376, in train\n    self.log(metrics)\n  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/transformers\/trainer.py&quot;, line 1688, in log\n    self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)\n  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/transformers\/trainer_callback.py&quot;, line 371, in on_log\n    return self.call_event(&quot;on_log&quot;, args, state, control, logs=logs)\n  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/transformers\/trainer_callback.py&quot;, line 378, in call_event\n    result = getattr(callback, event)(\n  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/transformers\/integrations.py&quot;, line 754, in on_log\n    self._wandb.log({**logs, &quot;train\/global_step&quot;: state.global_step})\n  File &quot;\/home\/user123\/anaconda3\/envs\/iza\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/preinit.py&quot;, line 38, in preinit_wrapper\n    raise wandb.Error(&quot;You must call wandb.init() before {}()&quot;.format(name))\nwandb.errors.Error: You must call wandb.init() before wandb.log()\n<\/code><\/pre>\n<p>Any help is highly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_time":1627015026463,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":532.0,
        "Answer_body":"<p>Please check running the code on the latest versions of wandb and transformers. Works fine for me with <code>wandb 0.11.0<\/code> and <code>transformers 4.9.0<\/code><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68494108",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1627224502390,
        "Question_original_content":"hyperparam search huggingfac fail error simpl script exampl blog post fail us offlin dataset import load dataset load metric transform import automodelforsequenceclassif autotoken trainer trainingargu import init token autotoken pretrain distilbert base uncas dataset load dataset glue mrpc metric load metric glue mrpc def encod exampl output token exampl sentenc exampl sentenc truncat true return output encod dataset dataset map encod batch true def model init return automodelforsequenceclassif pretrain distilbert base uncas return dict true def comput metric eval pred predict label eval pred predict predict argmax axi return metric comput predict predict refer label evalu train bit default abl prune bad trial earli disabl tqdm matter prefer train arg trainingargu test eval step disabl tqdm true evalu strategi step trainer trainer arg train arg token token train dataset encod dataset train eval dataset encod dataset valid model init model init comput metric comput metric def space trial return learn rate trial suggest float learn rate log true weight decai trial suggest float weight decai num train epoch trial suggest int num train epoch seed trial suggest int seed devic train batch size trial suggest categor devic train batch size trainer hyperparamet search direct maxim backend trial space space trail finish successfulli trail crash follow error file home user anaconda env iza lib python site packag transform integr line object trainer train resum checkpoint checkpoint trial trial file home user anaconda env iza lib python site packag transform trainer line train self log metric file home user anaconda env iza lib python site packag transform trainer line log self control self callback handler log self arg self state self control log file home user anaconda env iza lib python site packag transform trainer callback line log return self event log arg state control log log file home user anaconda env iza lib python site packag transform trainer callback line event result getattr callback event file home user anaconda env iza lib python site packag transform integr line log self log log train global step state global step file home user anaconda env iza lib python site packag sdk lib preinit line preinit wrapper rais error init format error error init log help highli appreci",
        "Question_preprocessed_content":"hyperparam search huggingfac fail error simpl script exampl blog post fail us offlin finish successfulli crash follow error help highli appreci",
        "Question_gpt_summary_original":"The user is encountering an error while performing hyperparameter search on Huggingface with Optuna due to a Wandb error. The error message indicates that the user must call Wandb.init() before Wandb.log(). The user has tried making Wandb offline, but it did not resolve the issue. The error occurs during the second trial, and the user is seeking help to resolve the problem.",
        "Question_gpt_summary":"user encount error perform hyperparamet search huggingfac error error messag indic user init log user tri make offlin resolv issu error occur second trial user seek help resolv problem",
        "Answer_original_content":"check run code latest version transform work fine transform",
        "Answer_preprocessed_content":"check run code latest version transform work fine",
        "Answer_gpt_summary_original":"Solution: The user can try running the code on the latest versions of wandb and transformers, specifically with wandb 0.11.0 and transformers 4.9.0.",
        "Answer_gpt_summary":"solut user try run code latest version transform specif transform"
    },
    {
        "Question_title":"How to combine pipeline and hyperparameter in AzureML SDK in the training step",
        "Question_body":"<p>Short form:\nI am trying to figure out how can I run the hyperparam within a <strong>training step<\/strong> (i.e. train_step = PythonScriptStep(...)) in the pipeline, I am not sure where shall I put the &quot;config=hyperdrive&quot;<\/p>\n<p>Long form:<\/p>\n<p>General:<\/p>\n<pre><code># Register the environment \ndiabetes_env.register(workspace=ws)\nregistered_env = Environment.get(ws, 'diabetes-pipeline-env')\n\n# Create a new runconfig object for the pipeline\nrun_config = RunConfiguration()\n\n# Use the compute you created above. \nrun_config.target = ComputerTarget_Crea\n\n# Assign the environment to the run configuration\nrun_config.environment = registered_env\n<\/code><\/pre>\n<p>Hyperparam:<\/p>\n<pre><code>script_config = ScriptRunConfig(source_directory=experiment_folder,\n                                script='diabetes_training.py',\n                                # Add non-hyperparameter arguments -in this case, the training dataset\n                                arguments = ['--input-data', diabetes_ds.as_named_input('training_data')],\n                                environment=sklearn_env,\n                                compute_target = training_cluster)\n\n# Sample a range of parameter values\nparams = GridParameterSampling(\n    {\n        # Hyperdrive will try 6 combinations, adding these as script arguments\n        '--learning_rate': choice(0.01, 0.1, 1.0),\n        '--n_estimators' : choice(10, 100)\n    }\n)\n\n# Configure hyperdrive settings\nhyperdrive = HyperDriveConfig(run_config=script_config, \n                          hyperparameter_sampling=params, \n                          policy=None, # No early stopping policy\n                          primary_metric_name='AUC', # Find the highest AUC metric\n                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n                          max_total_runs=6, # Restict the experiment to 6 iterations\n                          max_concurrent_runs=2) # Run up to 2 iterations in parallel\n\n# Run the experiment if I only want to run hyperparam alone without the pipeline\n#experiment = Experiment(workspace=ws, name='mslearn-diabetes-hyperdrive')\n#run = experiment.submit(**config=hyperdrive**)\n<\/code><\/pre>\n<p>PipeLine:<\/p>\n<pre><code>prep_step = PythonScriptStep(name = &quot;Prepare Data&quot;,\n                                source_directory = experiment_folder,\n                                script_name = &quot;prep_diabetes.py&quot;,\n                                arguments = ['--input-data', diabetes_ds.as_named_input('raw_data'),\n                                             '--prepped-data', prepped_data_folder],\n                                outputs=[prepped_data_folder],\n                                compute_target = ComputerTarget_Crea,\n                                runconfig = run_config,\n                                allow_reuse = True)\n\n# Step 2, run the training script\ntrain_step = PythonScriptStep(name = &quot;Train and Register Model&quot;,\n                                source_directory = experiment_folder,\n                                script_name = &quot;train_diabetes.py&quot;,\n                                arguments = ['--training-folder', prepped_data_folder],\n                                inputs=[prepped_data_folder],\n                                compute_target = ComputerTarget_Crea,\n                                runconfig = run_config,\n                                allow_reuse = True)\n# Construct the pipeline\npipeline_steps = [prep_step, train_step]\npipeline = Pipeline(workspace=ws, steps=pipeline_steps)\nprint(&quot;Pipeline is built.&quot;)\n\n# Create an experiment and run the pipeline\n**#How do I need to change these below lines to use hyperdrive????**\nexperiment = Experiment(workspace=ws, name = 'mslearn-diabetes-pipeline')\npipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n<\/code><\/pre>\n<p>Not sure where I need to put <strong>config=hyperdrive<\/strong> in the Pipeline section?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619929277080,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1619929940227,
        "Question_score":1.0,
        "Question_view_count":276.0,
        "Answer_body":"<p>here's how to combine hyperparameters with an AML pipeline: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.hyperdrivestep?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.hyperdrivestep?view=azure-ml-py<\/a><\/p>\n<p>Alternatively, here's a sample notebook: <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67352949",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1633922581063,
        "Question_original_content":"combin pipelin hyperparamet sdk train step short form try figur run hyperparam train step train step pythonscriptstep pipelin sure shall config hyperdr long form gener regist environ diabet env regist workspac regist env environ diabet pipelin env creat new runconfig object pipelin run config runconfigur us comput creat run config target computertarget crea assign environ run configur run config environ regist env hyperparam script config scriptrunconfig sourc directori experi folder script diabet train add non hyperparamet argument case train dataset argument input data diabet name input train data environ sklearn env comput target train cluster sampl rang paramet valu param gridparametersampl hyperdr try combin ad script argument learn rate choic estim choic configur hyperdr set hyperdr hyperdriveconfig run config script config hyperparamet sampl param polici earli stop polici primari metric auc highest auc metric primari metric goal primarymetricgo maxim max total run restict experi iter max concurr run run iter parallel run experi want run hyperparam pipelin experi experi workspac mslearn diabet hyperdr run experi submit config hyperdr pipelin prep step pythonscriptstep prepar data sourc directori experi folder script prep diabet argument input data diabet name input raw data prep data prep data folder output prep data folder comput target computertarget crea runconfig run config allow reus true step run train script train step pythonscriptstep train regist model sourc directori experi folder script train diabet argument train folder prep data folder input prep data folder comput target computertarget crea runconfig run config allow reus true construct pipelin pipelin step prep step train step pipelin pipelin workspac step pipelin step print pipelin built creat experi run pipelin need chang line us hyperdr experi experi workspac mslearn diabet pipelin pipelin run experi submit pipelin regener output true sure need config hyperdr pipelin section",
        "Question_preprocessed_content":"combin pipelin hyperparamet sdk train step short form try figur run hyperparam train step pipelin sure shall config hyperdr long form gener hyperparam pipelin sure need config hyperdr pipelin section",
        "Question_gpt_summary_original":"The user is trying to figure out how to run hyperparameters within a training step in a pipeline using AzureML SDK. They have successfully created a hyperparameter configuration and a pipeline with two steps, but they are unsure where to include the hyperparameter configuration in the pipeline.",
        "Question_gpt_summary":"user try figur run hyperparamet train step pipelin sdk successfulli creat hyperparamet configur pipelin step unsur includ hyperparamet configur pipelin",
        "Answer_original_content":"combin hyperparamet aml pipelin http doc microsoft com python api pipelin step pipelin step hyperdrivestep view azur altern sampl notebook http github com azur machinelearningnotebook blob master us machin learn pipelin intro pipelin aml pipelin paramet tune hyperdr ipynb",
        "Answer_preprocessed_content":"combin hyperparamet aml pipelin altern sampl notebook",
        "Answer_gpt_summary_original":"Solution: The discussion provides two possible solutions for combining hyperparameters with an AML pipeline. The first solution is to use the AzureML SDK's HyperDriveStep class, which is documented in the provided link. The second solution is to refer to a sample notebook that demonstrates how to use hyperparameters with an AML pipeline.",
        "Answer_gpt_summary":"solut discuss provid possibl solut combin hyperparamet aml pipelin solut us sdk hyperdrivestep class document provid link second solut refer sampl notebook demonstr us hyperparamet aml pipelin"
    },
    {
        "Question_title":"Does dvc work for live streaming data versioning and batch data versioning ? If yes, can someone explain briefly",
        "Question_body":"<p>I couldn\u2019t find any dvc documentation for batch data, and live streaming data versioning.<br>\nIs it possible in dvc to track streaming data and also fetch data in batch or time travel data?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1619414992269,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":796.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/veeresh\">@veeresh<\/a>,<\/p>\n<p>DVC could be used to maintain snapshots of a growing dataset every X time (prob not real-time for a raw stream though) IF the dataset growth looks like adding\/removing files. That would work efficiently because DVC de-duplicates data storage at the file level (see <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\">https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization<\/a>). So in that narrow case, it\u2019s technically possible.<\/p>\n<p>That said, there may be better tools for timed data back-ups. And I would question whether snapshots of a data stream should be considered versions: You could say they\u2019re all parts of the same version, which you just haven\u2019t obtained completely yet (maybe never will). If the stream includes updates\/corrections to previously received data points though, then that would more clearly represent versioning IMO.<\/p>\n<p>This is all pretty conceptual though. Feel free to share a more specific use case you have in mind for more concrete answer <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>. <p>Hi <a class=\"mention\" href=\"\/u\/jorgeorpinel\">@jorgeorpinel<\/a> ,<\/p>\n<p>Thanks for explaining.<\/p>\n<p>I was looking if something similar to delta lake time travel is possible in dvc. (<a href=\"https:\/\/databricks.com\/blog\/2019\/02\/04\/introducing-delta-time-travel-for-large-scale-data-lakes.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Introducing Delta Time Travel for Large Scale Data Lakes - The Databricks Blog<\/a>)<\/p>\n<p>Lets say there are 2 folders which keep updating every day (steaming),<br>\n-dogs<br>\n-cats<br>\nBasically, I want to get data between some time period (between 2 dates) , this is not possible in dvc right?<\/p>. <p>DVC uses Git as the underlying versioning layer. In Git you decide when and what to include in your commits. You can only switch between the commits that you have registered yourself.<\/p>\n<p>The new Experiments features do include some automatic tracking of multiple project versions but again that has a different purpose (ML <a href=\"https:\/\/dvc.org\/doc\/user-guide\/experiment-management\">experiment management<\/a>).<\/p>\n<p>Thanks<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/does-dvc-work-for-live-streaming-data-versioning-and-batch-data-versioning-if-yes-can-someone-explain-briefly\/738",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-26T05:52:19.927Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/veeresh\">@veeresh<\/a>,<\/p>\n<p>DVC could be used to maintain snapshots of a growing dataset every X time (prob not real-time for a raw stream though) IF the dataset growth looks like adding\/removing files. That would work efficiently because DVC de-duplicates data storage at the file level (see <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\">https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization<\/a>). So in that narrow case, it\u2019s technically possible.<\/p>\n<p>That said, there may be better tools for timed data back-ups. And I would question whether snapshots of a data stream should be considered versions: You could say they\u2019re all parts of the same version, which you just haven\u2019t obtained completely yet (maybe never will). If the stream includes updates\/corrections to previously received data points though, then that would more clearly represent versioning IMO.<\/p>\n<p>This is all pretty conceptual though. Feel free to share a more specific use case you have in mind for more concrete answer <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-26T06:40:24.955Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jorgeorpinel\">@jorgeorpinel<\/a> ,<\/p>\n<p>Thanks for explaining.<\/p>\n<p>I was looking if something similar to delta lake time travel is possible in dvc. (<a href=\"https:\/\/databricks.com\/blog\/2019\/02\/04\/introducing-delta-time-travel-for-large-scale-data-lakes.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Introducing Delta Time Travel for Large Scale Data Lakes - The Databricks Blog<\/a>)<\/p>\n<p>Lets say there are 2 folders which keep updating every day (steaming),<br>\n-dogs<br>\n-cats<br>\nBasically, I want to get data between some time period (between 2 dates) , this is not possible in dvc right?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-26T06:44:01.795Z",
                "Answer_body":"<p>DVC uses Git as the underlying versioning layer. In Git you decide when and what to include in your commits. You can only switch between the commits that you have registered yourself.<\/p>\n<p>The new Experiments features do include some automatic tracking of multiple project versions but again that has a different purpose (ML <a href=\"https:\/\/dvc.org\/doc\/user-guide\/experiment-management\">experiment management<\/a>).<\/p>\n<p>Thanks<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"work live stream data version batch data version ye explain briefli document batch data live stream data version possibl track stream data fetch data batch time travel data",
        "Question_preprocessed_content":"work live stream data version batch data version ye explain briefli document batch data live stream data version possibl track stream data fetch data batch time travel data",
        "Question_gpt_summary_original":"The user is facing challenges in finding documentation for using dvc for batch data versioning and live streaming data versioning. They are unsure if it is possible to track streaming data and fetch data in batch or time travel data using dvc.",
        "Question_gpt_summary":"user face challeng find document batch data version live stream data version unsur possibl track stream data fetch data batch time travel data",
        "Answer_original_content":"veeresh maintain snapshot grow dataset time prob real time raw stream dataset growth look like ad remov file work effici duplic data storag file level http org doc user guid larg dataset optim narrow case technic possibl said better tool time data up question snapshot data stream consid version theyr part version havent obtain complet mayb stream includ updat correct previous receiv data point clearli repres version imo pretti conceptu feel free share specif us case mind concret answer jorgeorpinel thank explain look similar delta lake time travel possibl introduc delta time travel larg scale data lake databrick blog let folder updat dai steam dog cat basic want data time period date possibl right us git underli version layer git decid includ commit switch commit regist new experi featur includ automat track multipl project version differ purpos experi manag thank",
        "Answer_preprocessed_content":"maintain snapshot grow dataset time dataset growth look like file work effici data storag file level narrow case technic possibl said better tool time data question snapshot data stream consid version theyr part version havent obtain complet stream includ previous receiv data point clearli repres version imo pretti conceptu feel free share specif us case mind concret answer thank explain look similar delta lake time travel possibl let folder updat dai dog cat basic want data time period possibl right us git underli version layer git decid includ commit switch commit regist new experi featur includ automat track multipl project version differ purpos thank",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- DVC can be used to maintain snapshots of a growing dataset every X time if the dataset growth looks like adding\/removing files. However, it may not work efficiently for real-time raw streams. \n- There may be better tools for timed data backups.\n- Snapshots of a data stream should not be considered versions, but if the stream includes updates\/corrections to previously received data points, then it would more clearly represent versioning.\n- DVC uses Git as the underlying versioning layer, and you can only switch between the commits that you have registered yourself.\n- The new Experiments features in DVC include some automatic tracking of multiple project versions, but it has a different purpose, which is",
        "Answer_gpt_summary":"possibl solut mention discuss maintain snapshot grow dataset time dataset growth look like ad remov file work effici real time raw stream better tool time data backup snapshot data stream consid version stream includ updat correct previous receiv data point clearli repres version us git underli version layer switch commit regist new experi featur includ automat track multipl project version differ purpos"
    },
    {
        "Question_title":"Version control of azure machine learning workspace notebooks",
        "Question_body":"<p>I'm trying to work with the capacities of the new Azure ML Workspace and I can't find any option to track my notebooks on git. <\/p>\n\n<p>It's this possible as well as you can do with Azure notebooks? If not is possible... how it's suposed to work with this notebooks? Only inside this workspace? <\/p>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1582811011920,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":344.0,
        "Answer_body":"<p>AFAIK, Git isn't currently supported by Azure Machine Learning Notebooks. If you're looking for a more fully-featured development environment, I suggest setting one up locally. There's more work up front, but it will give you the ability to version control. Check out this development environment set-up guide. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-environment\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-environment<\/a><\/p>\n\n<pre><code>| Environment                                                   | Pros                                                                                                                                                                                                                                    | Cons                                                                                                                                                                                 |\n|---------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Cloud-based Azure Machine Learning compute instance (preview) | Easiest way to get started. The entire SDK is already installed in your workspace VM, and notebook tutorials are pre-cloned and ready to run.                                                                                           | Lack of control over your development environment and dependencies. Additional cost incurred for Linux VM (VM can be stopped when not in use to avoid charges). See pricing details. |\n| Local environment                                             | Full control of your development environment and dependencies. Run with any build tool, environment, or IDE of your choice.                                                                                                             | Takes longer to get started. Necessary SDK packages must be installed, and an environment must also be installed if you don't already have one.                                      |\n| Azure Databricks                                              | Ideal for running large-scale intensive machine learning workflows on the scalable Apache Spark platform.                                                                                                                               | Overkill for experimental machine learning, or smaller-scale experiments and workflows. Additional cost incurred for Azure Databricks. See pricing details.                          |\n| The Data Science Virtual Machine (DSVM)                       | Similar to the cloud-based compute instance (Python and the SDK are pre-installed), but with additional popular data science and machine learning tools pre-installed. Easy to scale and combine with other custom tools and workflows. | A slower getting started experience compared to the cloud-based compute instance.                                                                                                    |\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60434642",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1582827641316,
        "Question_original_content":"version control workspac notebook try work capac new workspac option track notebook git possibl azur notebook possibl supos work notebook insid workspac thank",
        "Question_preprocessed_content":"version control workspac notebook try work capac new workspac option track notebook git possibl azur notebook supos work notebook insid workspac thank",
        "Question_gpt_summary_original":"The user is facing challenges in tracking their Azure ML Workspace notebooks on git and is unsure if it is possible to do so. They are seeking clarification on whether the notebooks can only be used within the workspace or if there is another way to work with them.",
        "Question_gpt_summary":"user face challeng track workspac notebook git unsur possibl seek clarif notebook workspac wai work",
        "Answer_original_content":"afaik git isn current support notebook look fulli featur develop environ suggest set local work abil version control check develop environ set guid http doc microsoft com azur machin learn configur environ environ pro con cloud base comput instanc preview easiest wai start entir sdk instal workspac notebook tutori pre clone readi run lack control develop environ depend addit cost incur linux stop us avoid charg price detail local environ control develop environ depend run build tool environ id choic take longer start necessari sdk packag instal environ instal azur databrick ideal run larg scale intens machin learn workflow scalabl apach spark platform overkil experiment machin learn smaller scale experi workflow addit cost incur azur databrick price detail data scienc virtual machin dsvm similar cloud base comput instanc python sdk pre instal addit popular data scienc machin learn tool pre instal easi scale combin custom tool workflow slower get start experi compar cloud base comput instanc",
        "Answer_preprocessed_content":"afaik git isn current support notebook look develop environ suggest set local work abil version control check develop environ guid",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that Git is not currently supported by Azure Machine Learning Notebooks. However, the user can set up a local development environment to version control their notebooks. The discussion also provides a link to a development environment set-up guide.",
        "Answer_gpt_summary":"solut discuss suggest git current support notebook user set local develop environ version control notebook discuss provid link develop environ set guid"
    },
    {
        "Question_title":"Unable to use GPU to train a NN model in azure machine learning service using P100-NC6s-V2 compute. Fails wth CUDA error",
        "Question_body":"<p>I\u2019ve recently started working with azure for ML and am trying to use machine learning service workspace.\nI\u2019ve set up a workspace with the compute set to NC6s-V2 machines since I need train a NN using images on GPU. <\/p>\n\n<p>The issue is that the training still happens on the CPU \u2013 the logs say it\u2019s not able to find CUDA. Here\u2019s the warning log when running my script.\nAny clues how to solve this issue?<\/p>\n\n<p>I\u2019ve also mentioned explicitly tensorflow-gpu package in the conda packages option of the estimator. <\/p>\n\n<p>Here's my code for the estimator,<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>script_params = {\n         '--input_data_folder': ds.path('dataset').as_mount(),\n         '--zip_file_name': 'train.zip',\n         '--run_mode': 'train'\n    }\n\n\nest = Estimator(source_directory='.\/scripts',\n                     script_params=script_params,\n                     compute_target=compute_target,\n                     entry_script='main.py',\n                     conda_packages=['scikit-image', 'keras', 'tqdm', 'pillow', 'matplotlib', 'scipy', 'tensorflow-gpu']\n                     )\n\nrun = exp.submit(config=est)\n\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n\n<p>The compute target was made as per the sample code on github:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>compute_name = \"P100-NC6s-V2\"\ncompute_min_nodes = 0\ncompute_max_nodes = 4\n\nvm_size = \"STANDARD_NC6S_V2\"\n\nif compute_name in ws.compute_targets:\n    compute_target = ws.compute_targets[compute_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('found compute target. just use it. ' + compute_name)\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n                                                                min_nodes=compute_min_nodes,\n                                                                max_nodes=compute_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(\n        ws, compute_name, provisioning_config)\n\n    # can poll for a minimum number of nodes and for a specific timeout.\n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(\n        show_output=True, min_node_count=None, timeout_in_minutes=20)\n\n    # For a more detailed view of current AmlCompute status, use get_status()\n    print(compute_target.get_status().serialize())\n\n<\/code><\/pre>\n\n<p>This is the warning with which it fails to use the GPU:<\/p>\n\n<pre><code>2019-08-12 14:50:16.961247: I tensorflow\/compiler\/xla\/service\/service.cc:168] XLA service 0x55a7ce570830 executing computations on platform Host. Devices:\n2019-08-12 14:50:16.961278: I tensorflow\/compiler\/xla\/service\/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;\n2019-08-12 14:50:16.971025: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:53] Could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/azureml-envs\/azureml_5fdf05c5671519f307e0f43128b8610e\/lib:\n2019-08-12 14:50:16.971054: E tensorflow\/stream_executor\/cuda\/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n2019-08-12 14:50:16.971081: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 4bd815dfb0e74e3da901861a4746184f000000\n2019-08-12 14:50:16.971089: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:176] hostname: 4bd815dfb0e74e3da901861a4746184f000000\n2019-08-12 14:50:16.971164: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:200] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\n2019-08-12 14:50:16.971202: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:204] kernel reported version is: 418.40.4\nDevice mapping:\n\/job:localhost\/replica:0\/task:0\/device:XLA_CPU:0 -&gt; device: XLA_CPU device\n2019-08-12 14:50:16.973301: I tensorflow\/core\/common_runtime\/direct_session.cc:296] Device mapping:\n\/job:localhost\/replica:0\/task:0\/device:XLA_CPU:0 -&gt; device: XLA_CPU device\n\n<\/code><\/pre>\n\n<p>It's currently using the CPU as per the logs. Any clues how to resolve the issue here?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1565670597500,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1565670701316,
        "Question_score":5.0,
        "Question_view_count":1402.0,
        "Answer_body":"<p>Instead of base Estimator, you can use the Tensorflow Estimator with Keras and other libraries layered on top. That way you don't have to worry about setting up and configuring the GPU libraries, as the Tensorflow Estimator uses a Docker image with GPU libraries pre-configured. <\/p>\n\n<p>See here for documentation:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.dnn.tensorflow?view=azure-ml-py\" rel=\"nofollow noreferrer\">API Reference<\/a> You can use <code>conda_packages<\/code> argument to specify additional libraries. Also set argument <code>use_gpu = True<\/code>.<\/p>\n\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training-with-deep-learning\/train-hyperparameter-tune-deploy-with-keras\/train-hyperparameter-tune-deploy-with-keras.ipynb\" rel=\"nofollow noreferrer\">Example Notebook<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57471129",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1565705412680,
        "Question_original_content":"unabl us gpu train model servic nc comput fail wth cuda error iv recent start work azur try us machin learn servic workspac iv set workspac comput set nc machin need train imag gpu issu train happen cpu log abl cuda here warn log run script clue solv issu iv mention explicitli tensorflow gpu packag conda packag option estim code estim script param input data folder path dataset mount zip file train zip run mode train est estim sourc directori script script param script param comput target comput target entri script main conda packag scikit imag kera tqdm pillow matplotlib scipi tensorflow gpu run exp submit config est run wait complet output true comput target sampl code github comput nc comput min node comput max node size standard nc comput comput target comput target comput target comput comput target type comput target amlcomput print comput target us comput print creat new comput target provis config amlcomput provis configur size size min node comput min node max node comput max node creat cluster comput target computetarget creat comput provis config poll minimum number node specif timeout min node count provid us scale set cluster comput target wait complet output true min node count timeout minut detail view current amlcomput statu us statu print comput target statu serial warn fail us gpu tensorflow compil xla servic servic xla servic xace execut comput platform host devic tensorflow compil xla servic servic streamexecutor devic tensorflow stream executor platform default dso loader dlopen librari libcuda dlerror libcuda open share object file file directori librari path opt intel compil librari linux mpi intel lib opt intel compil librari linux mpi mic lib opt intel compil librari linux mpi intel lib opt intel compil librari linux mpi mic lib env fdfcfefb lib tensorflow stream executor cuda cuda driver fail cuinit unknown error tensorflow stream executor cuda cuda diagnost retriev cuda diagnost inform host bddfbeedaaf tensorflow stream executor cuda cuda diagnost hostnam bddfbeedaaf tensorflow stream executor cuda cuda diagnost libcuda report version unabl libcuda dso load program tensorflow stream executor cuda cuda diagnost kernel report version devic map job localhost replica task devic xla cpu devic xla cpu devic tensorflow core common runtim direct session devic map job localhost replica task devic xla cpu devic xla cpu devic current cpu log clue resolv issu",
        "Question_preprocessed_content":"unabl us gpu train model servic comput fail wth cuda error iv recent start work azur try us machin learn servic workspac iv set workspac comput set machin need train imag gpu issu train happen cpu log abl cuda here warn log run script clue solv issu iv mention explicitli packag conda packag option estim code estim comput target sampl code github warn fail us gpu current cpu log clue resolv issu",
        "Question_gpt_summary_original":"The user is facing challenges in using GPU to train a neural network model in Azure Machine Learning Service using P100-NC6s-V2 compute. The training is happening on the CPU and the logs show that it's not able to find CUDA. The user has mentioned explicitly tensorflow-gpu package in the conda packages option of the estimator. The warning log shows that it fails to use the GPU and is currently using the CPU.",
        "Question_gpt_summary":"user face challeng gpu train neural network model servic nc comput train happen cpu log abl cuda user mention explicitli tensorflow gpu packag conda packag option estim warn log show fail us gpu current cpu",
        "Answer_original_content":"instead base estim us tensorflow estim kera librari layer wai worri set configur gpu librari tensorflow estim us docker imag gpu librari pre configur document api refer us conda packag argument specifi addit librari set argument us gpu true exampl notebook",
        "Answer_preprocessed_content":"instead base estim us tensorflow estim kera librari layer wai worri set configur gpu librari tensorflow estim us docker imag gpu librari document api refer us argument specifi addit librari set argument exampl notebook",
        "Answer_gpt_summary_original":"Solution: The user can use the Tensorflow Estimator with pre-configured GPU libraries instead of the base Estimator. They can specify additional libraries using the `conda_packages` argument and set `use_gpu=True`. Documentation and an example notebook are provided for reference.",
        "Answer_gpt_summary":"solut user us tensorflow estim pre configur gpu librari instead base estim specifi addit librari conda packag argument set us gpu true document exampl notebook provid refer"
    },
    {
        "Question_title":"Google Colab freezes my browser and pc when trying to reconnect to a notebook",
        "Question_body":"<p>I am training a Machine learning model in google colab, to be more specific I am training a GAN with PyTorch-lightning. The problem occurs is when I get disconnected from my current runtime due to inactivity. When I try to reconnect my Browser(tried on firefox and chrome) becomes first laggy and than freezes, my pc starts to lag so that I am not able to close my browser and it doesn't go away. I am forced to press the power button of my PC in order to restart the PC.\nI have no clue why this happens.\nI tried various batch sizes(also the size 1) but it still happens. It can't be that my dataset is too big either(since i tried it on a dataset with 10images for testing puposes).\nI hope someone can help me.<\/p>\n\n<p>Here is my code (For using the code you will need comet.nl and enter the comet.ml api key):<\/p>\n\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision  \nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\n\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport pytorch_lightning as pl\nfrom pytorch_lightning import loggers\n\nimport numpy as np\nfrom numpy.random import choice\n\nfrom PIL import Image\n\nimport os\nfrom pathlib import Path\nimport shutil\n\nfrom collections import OrderedDict\n\n# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n# randomly flip some labels\ndef noisy_labels(y, p_flip=0.05):  # # flip labels with 5% probability\n    # determine the number of labels to flip\n    n_select = int(p_flip * y.shape[0])\n    # choose labels to flip\n    flip_ix = choice([i for i in range(y.shape[0])], size=n_select)\n    # invert the labels in place\n    y[flip_ix] = 1 - y[flip_ix]\n    return y\n\nclass AddGaussianNoise(object):\n    def __init__(self, mean=0.0, std=0.1):\n        self.std = std\n        self.mean = mean\n\n    def __call__(self, tensor):\n        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n\ndef get_valid_labels(img):\n  return (0.8 - 1.1) * torch.rand(img.shape[0], 1, 1, 1) + 1.1  # soft labels\n\ndef get_unvalid_labels(img):\n  return noisy_labels((0.0 - 0.3) * torch.rand(img.shape[0], 1, 1, 1) + 0.3)  # soft labels\n\nclass Generator(nn.Module):\n    def __init__(self, ngf, nc, latent_dim):\n        super(Generator, self).__init__()\n        self.ngf = ngf\n        self.latent_dim = latent_dim\n        self.nc = nc\n\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d(latent_dim, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n             nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass Discriminator(nn.Module):\n    def __init__(self, ndf, nc):\n        super(Discriminator, self).__init__()\n        self.nc = nc\n        self.ndf = ndf\n\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass DCGAN(pl.LightningModule):\n\n    def __init__(self, hparams, logger, checkpoint_folder, experiment_name):\n        super().__init__()\n        self.hparams = hparams\n        self.logger = logger  # only compatible with comet_logger at the moment\n        self.checkpoint_folder = checkpoint_folder\n        self.experiment_name = experiment_name\n\n        # networks\n        self.generator = Generator(ngf=hparams.ngf, nc=hparams.nc, latent_dim=hparams.latent_dim)\n        self.discriminator = Discriminator(ndf=hparams.ndf, nc=hparams.nc)\n        self.generator.apply(weights_init)\n        self.discriminator.apply(weights_init)\n\n        # cache for generated images\n        self.generated_imgs = None\n        self.last_imgs = None\n\n        # For experience replay\n        self.exp_replay_dis = torch.tensor([])\n\n        # creating checkpoint folder\n        dirpath = Path(self.checkpoint_folder)\n        if not dirpath.exists():\n          os.makedirs(dirpath, 0o755)\n\n    def forward(self, z):\n        return self.generator(z)\n\n    def adversarial_loss(self, y_hat, y):\n        return F.binary_cross_entropy(y_hat, y)\n\n    def training_step(self, batch, batch_nb, optimizer_idx):\n        # For adding Instance noise for more visit: https:\/\/www.inference.vc\/instance-noise-a-trick-for-stabilising-gan-training\/\n        std_gaussian = max(0, self.hparams.level_of_noise - ((self.hparams.level_of_noise * 1.5) * (self.current_epoch \/ self.hparams.epochs)))\n        AddGaussianNoiseInst = AddGaussianNoise(std=std_gaussian) # the noise decays over time\n\n        imgs, _ = batch\n        imgs = AddGaussianNoiseInst(imgs) # Adding instance noise to real images\n        self.last_imgs = imgs\n\n        # train generator\n        if optimizer_idx == 0:\n            # sample noise\n            z = torch.randn(imgs.shape[0], self.hparams.latent_dim, 1, 1)\n\n            # generate images\n            self.generated_imgs = self(z)\n            self.generated_imgs = AddGaussianNoiseInst(self.generated_imgs) # Adding instance noise to fake images\n\n            # Experience replay\n            # for discriminator\n            perm = torch.randperm(self.generated_imgs.size(0))  # Shuffeling\n            r_idx = perm[:max(1, self.hparams.experience_save_per_batch)]  # Getting the index\n            self.exp_replay_dis = torch.cat((self.exp_replay_dis, self.generated_imgs[r_idx]), 0).detach()  # Add our new example to the replay buffer\n\n            # ground truth result (ie: all fake)\n            g_loss = self.adversarial_loss(self.discriminator(self.generated_imgs), get_valid_labels(self.generated_imgs)) # adversarial loss is binary cross-entropy\n\n            tqdm_dict = {'g_loss': g_loss}\n            log = {'g_loss': g_loss, \"std_gaussian\": std_gaussian}\n            output = OrderedDict({\n                'loss': g_loss,\n                'progress_bar': tqdm_dict,\n                'log': log\n            })\n            return output\n\n        # train discriminator\n        if optimizer_idx == 1:\n            # Measure discriminator's ability to classify real from generated samples\n            # how well can it label as real?\n            real_loss = self.adversarial_loss(self.discriminator(imgs), get_valid_labels(imgs))\n\n            # Experience replay\n            if self.exp_replay_dis.size(0) &gt;= self.hparams.experience_batch_size:\n              fake_loss = self.adversarial_loss(self.discriminator(self.exp_replay_dis.detach()), get_unvalid_labels(self.exp_replay_dis))  # train on already seen images\n\n              self.exp_replay_dis = torch.tensor([]) # Reset experience replay\n\n              # discriminator loss is the average of these\n              d_loss = (real_loss + fake_loss) \/ 2\n\n              tqdm_dict = {'d_loss': d_loss}\n              log = {'d_loss': d_loss, \"d_exp_loss\": fake_loss, \"std_gaussian\": std_gaussian}\n              output = OrderedDict({\n                  'loss': d_loss,\n                  'progress_bar': tqdm_dict,\n                  'log': log\n              })\n              return output\n\n            else:\n              fake_loss = self.adversarial_loss(self.discriminator(self.generated_imgs.detach()), get_unvalid_labels(self.generated_imgs))  # how well can it label as fake?\n\n              # discriminator loss is the average of these\n              d_loss = (real_loss + fake_loss) \/ 2\n\n              tqdm_dict = {'d_loss': d_loss}\n              log = {'d_loss': d_loss, \"std_gaussian\": std_gaussian}\n              output = OrderedDict({\n                  'loss': d_loss,\n                  'progress_bar': tqdm_dict,\n                  'log': log\n              })\n              return output\n\n    def configure_optimizers(self):\n        lr = self.hparams.lr\n        b1 = self.hparams.b1\n        b2 = self.hparams.b2\n\n        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n        return [opt_g, opt_d], []\n\n    def train_dataloader(self):\n        transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize([0.5], [0.5])])\n        dataset = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n        return DataLoader(dataset, batch_size=self.hparams.batch_size)\n        # transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n        #                                 transforms.ToTensor(),\n        #                                 transforms.Normalize([0.5], [0.5])\n        #                                 ])\n\n        # train_dataset = torchvision.datasets.ImageFolder(\n        #     root=\".\/drive\/My Drive\/datasets\/ghibli_dataset_small_overfit\/\",\n        #     transform=transform\n        # )\n        # return DataLoader(train_dataset, num_workers=self.hparams.num_workers, shuffle=True, batch_size=self.hparams.batch_size)\n\n    def on_epoch_end(self):\n        z = torch.randn(4, self.hparams.latent_dim, 1, 1)\n        # match gpu device (or keep as cpu)\n        if self.on_gpu:\n            z = z.cuda(self.last_imgs.device.index)\n\n        # log sampled images\n        sample_imgs = self.generator(z)\n        sample_imgs = sample_imgs.view(-1, self.hparams.nc, self.hparams.image_size, self.hparams.image_size)\n        grid = torchvision.utils.make_grid(sample_imgs, nrow=2)\n        self.logger.experiment.log_image(grid.permute(1, 2, 0), f'generated_images_epoch{self.current_epoch}', step=self.current_epoch)\n\n        # save model\n        if self.current_epoch % self.hparams.save_model_every_epoch == 0:\n          trainer.save_checkpoint(self.checkpoint_folder + \"\/\" + self.experiment_name + \"_epoch_\" + str(self.current_epoch) + \".ckpt\")\n          comet_logger.experiment.log_asset_folder(self.checkpoint_folder, step=self.current_epoch)\n\n          # Deleting the folder where we saved the model so that we dont upload a thing twice\n          dirpath = Path(self.checkpoint_folder)\n          if dirpath.exists() and dirpath.is_dir():\n                shutil.rmtree(dirpath)\n\n          # creating checkpoint folder\n          access_rights = 0o755\n          os.makedirs(dirpath, access_rights)\n\nfrom argparse import Namespace\n\nargs = {\n    'batch_size': 48,\n    'lr': 0.0002,\n    'b1': 0.5,\n    'b2': 0.999,\n    'latent_dim': 128, # tested value which worked(in V4_1): 100\n    'nc': 1,\n    'ndf': 32,\n    'ngf': 32,\n    'epochs': 10,\n    'save_model_every_epoch': 5,\n    'image_size': 64,\n    'num_workers': 2,\n    'level_of_noise': 0.15,\n    'experience_save_per_batch': 1, # this value should be very low; tested value which works: 1\n    'experience_batch_size': 50 # this value shouldnt be too high; tested value which works: 50\n}\nhparams = Namespace(**args)\n\n# Parameters\nexperiment_name = \"DCGAN_V4_2_MNIST\"\ndataset_name = \"MNIST\"\ncheckpoint_folder = \"DCGAN\/\"\ntags = [\"DCGAN\", \"MNIST\", \"OVERFIT\", \"64x64\"]\ndirpath = Path(checkpoint_folder)\n\n# init logger\ncomet_logger = loggers.CometLogger(\n    api_key=\"\",\n    rest_api_key=\"\",\n    project_name=\"gan\",\n    experiment_name=experiment_name,\n    #experiment_key=\"f23d00c0fe3448ee884bfbe3fc3923fd\"  # used for resuming trained id can be found in comet.ml\n)\n\n#defining net\nnet = DCGAN(hparams, comet_logger, checkpoint_folder, experiment_name)\n\n#logging\ncomet_logger.experiment.set_model_graph(str(net))\ncomet_logger.experiment.add_tags(tags=tags)\ncomet_logger.experiment.log_dataset_info(dataset_name)\n\ntrainer = pl.Trainer(#resume_from_checkpoint=\"GHIBLI_DCGAN_OVERFIT_64px_epoch_6000.ckpt\",\n                     logger=comet_logger,\n                     max_epochs=args[\"epochs\"]\n                     )\ntrainer.fit(net)\ncomet_logger.experiment.end()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586987562523,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1587130797008,
        "Question_score":0.0,
        "Question_view_count":1013.0,
        "Answer_body":"<p>I fixed it with importing this:<\/p>\n\n<pre><code>from IPython.display import clear_output \n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61239274",
        "Tool":"Comet",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1587034953183,
        "Question_original_content":"googl colab freez browser try reconnect notebook train machin learn model googl colab specif train gan pytorch lightn problem occur disconnect current runtim inact try reconnect browser tri firefox chrome laggi freez start lag abl close browser awai forc press power button order restart clue happen tri batch size size happen dataset big tri dataset imag test pupos hope help code code need enter api kei import torch import torch import torch function import torchvis import torchvis transform transform torch util data import dataload torchvis dataset import mnist pytorch lightn callback import modelcheckpoint import pytorch lightn pytorch lightn import logger import numpi numpi random import choic pil import imag import pathlib import path import shutil collect import ordereddict custom weight initi call netg netd def weight init classnam class classnam conv init normal weight data elif classnam batchnorm init normal weight data init constant bia data randomli flip label def noisi label flip flip label probabl determin number label flip select int flip shape choos label flip flip choic rang shape size select invert label place flip flip return class addgaussiannois object def init self mean std self std std self mean mean def self tensor return tensor torch randn tensor size self std self mean def repr self return self class mean std format self mean self std def valid label img return torch rand img shape soft label def unvalid label img return noisi label torch rand img shape soft label class gener modul def init self ngf latent dim super gener self init self ngf ngf self latent dim latent dim self self main sequenti input go convolut convtranspos latent dim ngf bia fals batchnormd ngf leakyrelu inplac true state size ngf convtranspos ngf ngf bia fals batchnormd ngf leakyrelu inplac true state size ngf convtranspos ngf ngf bia fals batchnormd ngf leakyrelu inplac true state size ngf convtranspos ngf ngf bia fals batchnormd ngf leakyrelu inplac true state size ngf convtranspos ngf bia fals tanh state size def forward self input return self main input class discrimin modul def init self ndf super discrimin self init self self ndf ndf self main sequenti input convd ndf bia fals leakyrelu inplac true state size ndf convd ndf ndf bia fals batchnormd ndf leakyrelu inplac true state size ndf convd ndf ndf bia fals batchnormd ndf leakyrelu inplac true state size ndf convd ndf ndf bia fals batchnormd ndf leakyrelu inplac true state size ndf convd ndf bia fals sigmoid def forward self input return self main input class dcgan lightningmodul def init self hparam logger checkpoint folder experi super init self hparam hparam self logger logger compat logger moment self checkpoint folder checkpoint folder self experi experi network self gener gener ngf hparam ngf hparam latent dim hparam latent dim self discrimin discrimin ndf hparam ndf hparam self gener appli weight init self discrimin appli weight init cach gener imag self gener img self img experi replai self exp replai di torch tensor creat checkpoint folder dirpath path self checkpoint folder dirpath exist makedir dirpath def forward self return self gener def adversari loss self hat return binari cross entropi hat def train step self batch batch optim idx ad instanc nois visit http infer instanc nois trick stabilis gan train std gaussian max self hparam level nois self hparam level nois self current epoch self hparam epoch addgaussiannoiseinst addgaussiannois std std gaussian nois decai time img batch img addgaussiannoiseinst img ad instanc nois real imag self img img train gener optim idx sampl nois torch randn img shape self hparam latent dim gener imag self gener img self self gener img addgaussiannoiseinst self gener img ad instanc nois fake imag experi replai discrimin perm torch randperm self gener img size shuffel idx perm max self hparam experi save batch get index self exp replai di torch cat self exp replai di self gener img idx detach add new exampl replai buffer ground truth result fake loss self adversari loss self discrimin self gener img valid label self gener img adversari loss binari cross entropi tqdm dict loss loss log loss loss std gaussian std gaussian output ordereddict loss loss progress bar tqdm dict log log return output train discrimin optim idx measur discrimin abil classifi real gener sampl label real real loss self adversari loss self discrimin img valid label img experi replai self exp replai di size self hparam experi batch size fake loss self adversari loss self discrimin self exp replai di detach unvalid label self exp replai di train seen imag self exp replai di torch tensor reset experi replai discrimin loss averag loss real loss fake loss tqdm dict loss loss log loss loss exp loss fake loss std gaussian std gaussian output ordereddict loss loss progress bar tqdm dict log log return output fake loss self adversari loss self discrimin self gener img detach unvalid label self gener img label fake discrimin loss averag loss real loss fake loss tqdm dict loss loss log loss loss std gaussian std gaussian output ordereddict loss loss progress bar tqdm dict log log return output def configur optim self self hparam self hparam self hparam opt torch optim adam self gener paramet beta opt torch optim adam self discrimin paramet beta return opt opt def train dataload self transform transform compos transform resiz self hparam imag size self hparam imag size transform totensor transform normal dataset mnist getcwd train true download true transform transform return dataload dataset batch size self hparam batch size transform transform compos transform resiz self hparam imag size self hparam imag size transform totensor transform normal train dataset torchvis dataset imagefold root drive drive dataset ghibli dataset small overfit transform transform return dataload train dataset num worker self hparam num worker shuffl true batch size self hparam batch size def epoch end self torch randn self hparam latent dim match gpu devic cpu self gpu cuda self img devic index log sampl imag sampl img self gener sampl img sampl img view self hparam self hparam imag size self hparam imag size grid torchvis util grid sampl img nrow self logger experi log imag grid permut gener imag epoch self current epoch step self current epoch save model self current epoch self hparam save model epoch trainer save checkpoint self checkpoint folder self experi epoch str self current epoch ckpt logger experi log asset folder self checkpoint folder step self current epoch delet folder save model dont upload thing twice dirpath path self checkpoint folder dirpath exist dirpath dir shutil rmtree dirpath creat checkpoint folder access right makedir dirpath access right argpars import namespac arg batch size latent dim test valu work ndf ngf epoch save model epoch imag size num worker level nois experi save batch valu low test valu work experi batch size valu shouldnt high test valu work hparam namespac arg paramet experi dcgan mnist dataset mnist checkpoint folder dcgan tag dcgan mnist overfit dirpath path checkpoint folder init logger logger logger logger api kei rest api kei project gan experi experi experi kei fdcfeeebfbefcfd resum train defin net net dcgan hparam logger checkpoint folder experi log logger experi set model graph str net logger experi add tag tag tag logger experi log dataset info dataset trainer trainer resum checkpoint ghibli dcgan overfit epoch ckpt logger logger max epoch arg epoch trainer fit net logger experi end",
        "Question_preprocessed_content":"googl colab freez browser try reconnect notebook train machin learn model googl colab specif train gan problem occur disconnect current runtim inact try reconnect browser laggi freez start lag abl close browser awai forc press power button order restart clue happen tri batch size happen dataset big hope help code",
        "Question_gpt_summary_original":"The user is encountering issues with Google Colab freezing their browser and PC when trying to reconnect to a notebook while training a machine learning model, specifically a GAN with PyTorch-lightning. The issue occurs when the user gets disconnected from their current runtime due to inactivity. They have tried various batch sizes and datasets, but the problem persists. The user has provided their code for reference.",
        "Question_gpt_summary":"user encount issu googl colab freez browser try reconnect notebook train machin learn model specif gan pytorch lightn issu occur user get disconnect current runtim inact tri batch size dataset problem persist user provid code refer",
        "Answer_original_content":"fix import ipython displai import clear output",
        "Answer_preprocessed_content":"fix import",
        "Answer_gpt_summary_original":"Solution: The user found a solution to their problem by importing \"clear_output\" from IPython.display.",
        "Answer_gpt_summary":"solut user solut problem import clear output ipython displai"
    },
    {
        "Question_title":"Problem with top-level plot definitions",
        "Question_body":"<p>Hey!<\/p>\n<p>I am currently having problems with a top-level plot definition.<br>\nMy dvc.yaml file contains a stage producing<br>\n<code>output_data\/plot_metrics\/metrics_plot_dict.json<\/code> as output (also defined as outs in dvc.yaml)<\/p>\n<p>I am trying to define a plot like this in dvc.yaml<\/p>\n<pre><code class=\"lang-auto\">plots:\n  acceptance_rate_histogram:\n    template: plot_templates\/histogram_template.json\n    x: \n      output_data\/plot_metrics\/metrics_plot_dict.json: acceptance_rate\n    x_label: Acceptance_Rate\n    title: Histogram\n<\/code><\/pre>\n<p>But with <code>dvc plots show<\/code> I receive :<\/p>\n<pre><code class=\"lang-auto\">2022-09-14 12:37:13,109 WARNING: 'acceptance_rate_histogram' was not found in current workspace.\n2022-09-14 12:37:13,113 DEBUG: 'acceptance_rate_histogram' - file type error\nOnly JSON, YAML, CSV and TSV formats are supported.\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/home\/tim\/miniconda3\/envs\/pymc_stable\/lib\/python3.10\/site-packages\/dvc\/utils\/__init__.py\", line 410, in wrapper\n    vals = func(*args, **kwargs)\n  File \"\/home\/tim\/miniconda3\/envs\/pymc_stable\/lib\/python3.10\/site-packages\/dvc\/repo\/plots\/__init__.py\", line 548, in parse\n    raise PlotMetricTypeError(path)\ndvc.repo.plots.PlotMetricTypeError: 'acceptance_rate_histogram' - file type error\nOnly JSON, YAML, CSV and TSV formats are supported.\n------------------------------------------------------------\nDVC failed to load some plots for following revisions: 'workspace'.\nfile:\/\/\/home\/tim\/Workspaces\/pystoms\/pipelines\/pipe_A\/dvc_plots\/index.html\n2022-09-14 12:37:13,120 DEBUG: Analytics is enabled.\n2022-09-14 12:37:13,190 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmpaooxh7mo']'\n2022-09-14 12:37:13,191 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmpaooxh7mo']'\n<\/code><\/pre>\n<p>It seems as if dvc tries to find a file named <code>acceptance_rate_histogram<\/code>, but this string is supposed to be an id. If I exchange the id for the file\u2019s path, it works.<\/p>\n<p>My dvc version: 2.25.0<br>\nMy python version: 3.10.6<br>\nMy OS: Ubuntu 22.04<\/p>\n<p>Thank you very much for any help!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1663152137956,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":135.0,
        "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/timolivermaier\">@TimOliverMaier<\/a>!<br>\nThe reason for that is that currently DVC tries to get the file by looking at <code>y<\/code>. In other case it will try to treat id as a file - hence your problem, we need to allow for locating the <code>x<\/code> too.<br>\nHere is relevant issue:<\/p><aside class=\"onebox githubissue\" data-onebox-src=\"https:\/\/github.com\/iterative\/dvc\/issues\/7754\">\n  <header class=\"source\">\n\n      <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/7754\" target=\"_blank\" rel=\"noopener\">github.com\/iterative\/dvc<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"><\/path><\/svg>\n  <\/div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/7754\" target=\"_blank\" rel=\"noopener\">plots: support for flexible x-axis values<\/a>\n    <\/h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2022-05-16\" data-time=\"17:32:06\" data-timezone=\"UTC\">05:32PM - 16 May 22 UTC<\/span>\n      <\/div>\n\n\n      <div class=\"user\">\n        <a href=\"https:\/\/github.com\/dberenbaum\" target=\"_blank\" rel=\"noopener\">\n          <img alt=\"dberenbaum\" src=\"https:\/\/avatars.githubusercontent.com\/u\/2308172?v=4\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          dberenbaum\n        <\/a>\n      <\/div>\n    <\/div>\n\n    <div class=\"labels\">\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          p2-medium\n        <\/span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          A: plots\n        <\/span>\n    <\/div>\n  <\/div>\n<\/div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">In https:\/\/github.com\/iterative\/dvc\/issues\/7086#issuecomment-1125169586, there a<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026<\/a><\/span><span class=\"excerpt hidden\">re plots where it can be useful to specify the `x` source file separately from `y`. For example, if I keep the actual independent variable saved with my original data, but save the predictions for each model in a separate file, I might need something like:\n\n```yaml\nplots:\n  confusion:\n    x: \n      dir\/actual.csv: actual\n    y:\n      dir\/preds.csv: predicted\n    template: confusion\n```\n\nWe might also need to change `x` between files. For example, maybe I want to plot ROC and precision-recall on the same plot to see how they correlate:\n\n```yaml\nplots:\n  roc_vs_prc:\n    x:\n      precision_recall.json: recall\n      roc.json: fpr\n    y:\n      precision_recall.json: precision\n      roc.json: tpr\n```\n\nAnother use case for this would be if I named `x` differently between files, like:\n\n```yaml\nplots:\n  train_vs_val:\n    x:\n      train_loss.csv: epoch\n      val_loss.csv: step\n    y: \n      train_loss.csv: loss\n      val_loss.csv: loss\n```<\/span><\/p>\n  <\/div>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n. <p>Hi <a class=\"mention\" href=\"\/u\/paffciu\">@Paffciu<\/a> !<br>\nThank you very much for your super quick answer! Knowing that, I will be able to make myself a quick-fix.<\/p>\n<p>Cheers!<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/problem-with-top-level-plot-definitions\/1342",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-14T10:50:48.914Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/timolivermaier\">@TimOliverMaier<\/a>!<br>\nThe reason for that is that currently DVC tries to get the file by looking at <code>y<\/code>. In other case it will try to treat id as a file - hence your problem, we need to allow for locating the <code>x<\/code> too.<br>\nHere is relevant issue:<\/p><aside class=\"onebox githubissue\" data-onebox-src=\"https:\/\/github.com\/iterative\/dvc\/issues\/7754\">\n  <header class=\"source\">\n\n      <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/7754\" target=\"_blank\" rel=\"noopener\">github.com\/iterative\/dvc<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"><\/path><\/svg>\n  <\/div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/7754\" target=\"_blank\" rel=\"noopener\">plots: support for flexible x-axis values<\/a>\n    <\/h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2022-05-16\" data-time=\"17:32:06\" data-timezone=\"UTC\">05:32PM - 16 May 22 UTC<\/span>\n      <\/div>\n\n\n      <div class=\"user\">\n        <a href=\"https:\/\/github.com\/dberenbaum\" target=\"_blank\" rel=\"noopener\">\n          <img alt=\"dberenbaum\" src=\"https:\/\/avatars.githubusercontent.com\/u\/2308172?v=4\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          dberenbaum\n        <\/a>\n      <\/div>\n    <\/div>\n\n    <div class=\"labels\">\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          p2-medium\n        <\/span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          A: plots\n        <\/span>\n    <\/div>\n  <\/div>\n<\/div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">In https:\/\/github.com\/iterative\/dvc\/issues\/7086#issuecomment-1125169586, there a<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026<\/a><\/span><span class=\"excerpt hidden\">re plots where it can be useful to specify the `x` source file separately from `y`. For example, if I keep the actual independent variable saved with my original data, but save the predictions for each model in a separate file, I might need something like:\n\n```yaml\nplots:\n  confusion:\n    x: \n      dir\/actual.csv: actual\n    y:\n      dir\/preds.csv: predicted\n    template: confusion\n```\n\nWe might also need to change `x` between files. For example, maybe I want to plot ROC and precision-recall on the same plot to see how they correlate:\n\n```yaml\nplots:\n  roc_vs_prc:\n    x:\n      precision_recall.json: recall\n      roc.json: fpr\n    y:\n      precision_recall.json: precision\n      roc.json: tpr\n```\n\nAnother use case for this would be if I named `x` differently between files, like:\n\n```yaml\nplots:\n  train_vs_val:\n    x:\n      train_loss.csv: epoch\n      val_loss.csv: step\n    y: \n      train_loss.csv: loss\n      val_loss.csv: loss\n```<\/span><\/p>\n  <\/div>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-14T10:56:38.970Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/paffciu\">@Paffciu<\/a> !<br>\nThank you very much for your super quick answer! Knowing that, I will be able to make myself a quick-fix.<\/p>\n<p>Cheers!<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"problem level plot definit hei current have problem level plot definit yaml file contain stage produc output data plot metric metric plot dict json output defin out yaml try defin plot like yaml plot accept rate histogram templat plot templat histogram templat json output data plot metric metric plot dict json accept rate label accept rate titl histogram plot receiv warn accept rate histogram current workspac debug accept rate histogram file type error json yaml csv tsv format support traceback recent file home tim miniconda env pymc stabl lib python site packag util init line wrapper val func arg kwarg file home tim miniconda env pymc stabl lib python site packag repo plot init line pars rais plotmetrictypeerror path repo plot plotmetrictypeerror accept rate histogram file type error json yaml csv tsv format support fail load plot follow revis workspac file home tim workspac pystom pipelin pipe plot index html debug analyt enabl debug try spawn daemon analyt tmp tmpaooxhmo debug spawn daemon analyt tmp tmpaooxhmo tri file name accept rate histogram string suppos exchang file path work version python version ubuntu thank help",
        "Question_preprocessed_content":"problem plot definit hei current have problem plot definit yaml file contain stage produc output try defin plot like yaml receiv tri file name string suppos exchang file path work version python version ubuntu thank help",
        "Question_gpt_summary_original":"The user is encountering challenges with defining a top-level plot in their dvc.yaml file. They are receiving an error message when trying to define the plot and it seems that dvc is trying to find a file named after the plot's id. The user has provided their dvc version, python version, and OS.",
        "Question_gpt_summary":"user encount challeng defin level plot yaml file receiv error messag try defin plot try file name plot user provid version python version",
        "Answer_original_content":"hei timolivermai reason current tri file look case try treat file problem need allow locat relev issu github com iter plot support flexibl axi valu open utc dberenbaum medium plot http github com iter issu issuecom plot us specifi sourc file separ exampl actual independ variabl save origin data save predict model separ file need like yaml plot confus dir actual csv actual dir pred csv predict templat confus need chang file exampl mayb want plot roc precis recal plot correl yaml plot roc prc precis recal json recal roc json fpr precis recal json precis roc json tpr us case name differ file like yaml plot train val train loss csv epoch val loss csv step train loss csv loss val loss csv loss paffciu thank super quick answer know abl quick fix cheer",
        "Answer_preprocessed_content":"hei reason current tri file look case try treat file problem need allow locat relev issu plot support flexibl valu open utc dberenbaum plot plot us specifi sourc file separ exampl actual independ variabl save origin data save predict model separ file need like need chang file exampl mayb want plot roc plot correl us case name differ file like thank super quick answer know abl cheer",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"How to early stop bad runs in sweeps to save time",
        "Question_body":"<p>Hello,<br>\nthat\u2019s my first topic in the community, so I hope I am posting that in the correct category <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>I started exploring sweeps last week for a university project, and it is incredible! As we also got a new PyTorch version with support for the new apple silicon, I wanted to try that on my M1 Pro. As this is not as powerful as, for example, using GoogleColab for a fraction of the time, I wanted to ask if it is somehow possible to stop bad runs after a few epochs.<\/p>\n<p>As you can see in the report linked below, the run hopeful-sweep-2 does not look promising. It would be nice to cancel that run and start a new one instead.<\/p>\n<p>Thanks,<br>\nMarkus<\/p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/markuskarner\/AILS-Challenge%203%20Microscopic%20Images\/reports\/Sweep-on-some-image-data--VmlldzoyMTI2MDA3?accessToken=9h1rd20e7e6smpxm2cvtm3plk3rrauhkbb39w2rs46fv0htwvf0tx9r4ixjhkolk\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/18b592222b97bc09df3743831bb4929dec23611c.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/markuskarner\/AILS-Challenge%203%20Microscopic%20Images\/reports\/Sweep-on-some-image-data--VmlldzoyMTI2MDA3?accessToken=9h1rd20e7e6smpxm2cvtm3plk3rrauhkbb39w2rs46fv0htwvf0tx9r4ixjhkolk\" target=\"_blank\" rel=\"noopener\">W&amp;B<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_500x500.jpeg\" class=\"thumbnail onebox-avatar\" width=\"500\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_500x500.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_750x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/cf9e1ba973281ce03a0112b895e3f727d93c3e20_2_10x10.png\">\n\n<h3><a href=\"https:\/\/wandb.ai\/markuskarner\/AILS-Challenge%203%20Microscopic%20Images\/reports\/Sweep-on-some-image-data--VmlldzoyMTI2MDA3?accessToken=9h1rd20e7e6smpxm2cvtm3plk3rrauhkbb39w2rs46fv0htwvf0tx9r4ixjhkolk\" target=\"_blank\" rel=\"noopener\">Weights &amp; Biases<\/a><\/h3>\n\n  <p>Weights &amp; Biases, developer tools for machine learning<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1654584250764,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":270.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/markuskarner\">@markuskarner<\/a> ,<\/p>\n<p>Thank you for writing in with your question. We do support early termination of sweeps, this reference <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#early_terminate\">doc<\/a> covers this. When the early stopping is triggered, the agent stops the current run and gets the next set of hyperparameters to try. Here is a <a href=\"https:\/\/github.com\/wandb\/examples\/blob\/master\/examples\/keras\/keras-cnn-fashion\/sweep-bayes-hyperband.yaml\" rel=\"noopener nofollow ugc\">link<\/a> to an example sweep configuration for reference. If after setting up your configuration and your require review \/ feedback. Please do write back in this thread and we can review your work more closely.<\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-early-stop-bad-runs-in-sweeps-to-save-time\/2563",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-07T18:41:53.182Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/markuskarner\">@markuskarner<\/a> ,<\/p>\n<p>Thank you for writing in with your question. We do support early termination of sweeps, this reference <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#early_terminate\">doc<\/a> covers this. When the early stopping is triggered, the agent stops the current run and gets the next set of hyperparameters to try. Here is a <a href=\"https:\/\/github.com\/wandb\/examples\/blob\/master\/examples\/keras\/keras-cnn-fashion\/sweep-bayes-hyperband.yaml\" rel=\"noopener nofollow ugc\">link<\/a> to an example sweep configuration for reference. If after setting up your configuration and your require review \/ feedback. Please do write back in this thread and we can review your work more closely.<\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-06-08T05:11:46.045Z",
                "Answer_body":"<p>Thanks a lot <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/smiley.png?v=12\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\" loading=\"lazy\" width=\"20\" height=\"20\"><br>\nThat was exactly what I was looking for, just searched for early stopping and not terminating and somehow didn\u2019t find it <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>Is there also a way to change the configuration of a running sweep? Or stop it and continue it with a new configuration?<\/p>\n<p>Thanks &amp; Regards,<br>\nMarkus<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-08T19:54:18.740Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/markuskarner\">@markuskarner<\/a>,<\/p>\n<p>You\u2019re welcome, glad that helped.<\/p>\n<p>Currently we do not allow the modification of a running sweep. A sweep is inherently tied to its config, so once it is set there is no way to change it. You can however, create a new sweep and seed it with existing runs, see <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project#seed-a-new-sweep-with-existing-runs\">here<\/a>. Hope this helps.<\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-09T08:13:38.558Z",
                "Answer_body":"<p>Thanks for your kind reply <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-08T08:14:38.630Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":1654627313182,
        "Question_original_content":"earli stop bad run sweep save time hello that topic commun hope post correct categori start explor sweep week univers project incred got new pytorch version support new appl silicon want try pro power exampl googlecolab fraction time want ask possibl stop bad run epoch report link run hope sweep look promis nice cancel run start new instead thank marku develop tool machin learn",
        "Question_preprocessed_content":"earli stop bad run sweep save time hello that topic commun hope post correct categori start explor sweep week univers project incred got new pytorch version support new appl silicon want try pro power exampl googlecolab fraction time want ask possibl stop bad run epoch report link run look promis nice cancel run start new instead thank marku develop tool machin learn",
        "Question_gpt_summary_original":"The user is exploring sweeps for a university project and wants to know if it is possible to stop bad runs after a few epochs to save time. They have provided a report showing a run that does not look promising and would like to cancel it and start a new one instead.",
        "Question_gpt_summary":"user explor sweep univers project want know possibl stop bad run epoch save time provid report show run look promis like cancel start new instead",
        "Answer_original_content":"markuskarn thank write question support earli termin sweep refer doc cover earli stop trigger agent stop current run get set hyperparamet try link exampl sweep configur refer set configur requir review feedback write thread review work close regard mohammad",
        "Answer_preprocessed_content":"thank write question support earli termin sweep refer doc cover earli stop trigger agent stop current run get set hyperparamet try link exampl sweep configur refer set configur requir review feedback write thread review work close regard mohammad",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to the user's challenge. The solution is to use early termination of sweeps, which is supported by the platform. The reference document provided in the discussion covers this feature. When the early stopping is triggered, the agent stops the current run and gets the next set of hyperparameters to try. The discussion also provides a link to an example sweep configuration for reference.",
        "Answer_gpt_summary":"solut discuss provid solut user challeng solut us earli termin sweep support platform refer document provid discuss cover featur earli stop trigger agent stop current run get set hyperparamet try discuss provid link exampl sweep configur refer"
    },
    {
        "Question_title":"Is it possible to predict in sagemaker without using s3",
        "Question_body":"<p>I have a .pkl which I would like to put into production. I would like to do a daily query of my SQL server and do a prediction on about 1000 rows. The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html\" rel=\"nofollow noreferrer\">documentation<\/a> implies I have to load the daily data into s3. Is there a way around this? It should be able to fit in memory no problem. <\/p>\n\n<p>The answer to \" <a href=\"https:\/\/stackoverflow.com\/questions\/48319893\/is-there-some-kind-of-persistent-local-storage-in-aws-sagemaker-model-training\">is there some kind of persistent local storage in aws sagemaker model training?<\/a> \" says that \"<em>The notebook instance is coming with a local EBS (5GB) that you can use to copy some data into it and run the fast development iterations without copying the data every time from S3.<\/em>\" The 5GB could be enough but I am not sure you can actually run from a notebook in this manner. If I set up a VPN could I just query using pyodbc?<\/p>\n\n<p>Is there sagemaker integration with AWS Lambda? That in combination with a docker container would suit my needs.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1531871216253,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1531893078607,
        "Question_score":2.0,
        "Question_view_count":1576.0,
        "Answer_body":"<p>While you need to to specify a s3 \"folder\" as input, this folder can contain only a dummy file. \nAlso if you bring your own docker container for training like in <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">this example<\/a>, you can do pretty much everthing in it. So you could do your daily query inside your docker container, because they have access to the internet. <\/p>\n\n<p>Also inside this container you have access to all the other aws services. Your access is defined by the role you're passing to your training job.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51391639",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1531919252070,
        "Question_original_content":"possibl predict pkl like product like daili queri sql server predict row document impli load daili data wai abl fit memori problem answer kind persist local storag model train sai notebook instanc come local eb us copi data run fast develop iter copi data time sure actual run notebook manner set vpn queri pyodbc integr aw lambda combin docker contain suit need",
        "Question_preprocessed_content":"possibl predict pkl like product like daili queri sql server predict row document impli load daili data wai abl fit memori problem answer kind persist local storag model train sai notebook instanc come local eb us copi data run fast develop iter copi data time sure actual run notebook manner set vpn queri pyodbc integr aw lambda combin docker contain suit need",
        "Question_gpt_summary_original":"The user wants to put a .pkl file into production and do a daily query of their SQL server to predict on about 1000 rows. However, the documentation implies that the daily data needs to be loaded into s3, which the user wants to avoid. They are unsure if they can run from a notebook with local EBS storage or query using pyodbc with a VPN. They are also wondering if there is sagemaker integration with AWS Lambda, which would suit their needs.",
        "Question_gpt_summary":"user want pkl file product daili queri sql server predict row document impli daili data need load user want avoid unsur run notebook local eb storag queri pyodbc vpn wonder integr aw lambda suit need",
        "Answer_original_content":"need specifi folder input folder contain dummi file bring docker contain train like exampl pretti everth daili queri insid docker contain access internet insid contain access aw servic access defin role pass train job",
        "Answer_preprocessed_content":"need specifi folder input folder contain dummi file bring docker contain train like exampl pretti everth daili queri insid docker contain access internet insid contain access aw servic access defin role pass train job",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. The user can specify a dummy file in the s3 folder as input and bring their own docker container for training to do the daily query inside the container, which has access to the internet and all other AWS services. \n\nThere is no mention of any solution related to using a notebook with local EBS storage or querying using pyodbc with a VPN or sagemaker integration with AWS Lambda.",
        "Answer_gpt_summary":"possibl solut mention discuss user specifi dummi file folder input bring docker contain train daili queri insid contain access internet aw servic mention solut relat notebook local eb storag queri pyodbc vpn integr aw lambda"
    },
    {
        "Question_title":"Is it necessary to commit DVC files from our CI pipelines?",
        "Question_body":"<p>DVC uses git commits to save the experiments and navigate between experiments.<\/p>\n<p>Is it possible to avoid making auto-commits in CI\/CD (to save data artifacts after <code>dvc repro<\/code> in CI\/CD side).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1587023782283,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1594640021920,
        "Question_score":6.0,
        "Question_view_count":1047.0,
        "Answer_body":"<blockquote>\n  <p>will you make it part of CI pipeline<\/p>\n<\/blockquote>\n\n<p>DVC often serves as a part of MLOps infrastructure. There is a popular <a href=\"https:\/\/martinfowler.com\/articles\/cd4ml.html\" rel=\"noreferrer\">blog post about CI\/CD for ML<\/a> where DVC is used under the hood. <a href=\"https:\/\/blog.codecentric.de\/en\/2020\/01\/remote-training-gitlab-ci-dvc\/\" rel=\"noreferrer\">Another example<\/a> but with GitLab CI\/CD.<\/p>\n\n<blockquote>\n  <p>scenario where you will integrate dvc commit command with CI\n  pipelines?<\/p>\n<\/blockquote>\n\n<p>If you mean <code>git commit<\/code> of DVC files (not <code>dvc commit<\/code>) then yes, you need to commit dvc-files into Git during CI\/CD process. Auto-commit is not the best practice.<\/p>\n\n<p>How to avoid Git commit in CI\/CD:<\/p>\n\n<ol>\n<li>After ML model training in CI\/CD, save changed dvc-files in external storage (for example GitLab artifact\/releases), then get the files to a developer machine and commit there. Users usually write scripts to automate it.<\/li>\n<li>Wait for DVC 1.0 release when <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1234\" rel=\"noreferrer\">run-cache (like build-cache)<\/a> will be implemented. Run-cache makes dvc-files ephemeral and no additional Git commits will be required. Technically, run-cache is an associative storage <code>repo state --&gt; run results<\/code> outside of Git repo (in data remote).<\/li>\n<\/ol>\n\n<p>Disclaimer: I'm one of the creators of DVC.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":null,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61245284",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1587029761967,
        "Question_original_content":"necessari commit file pipelin us git commit save experi navig experi possibl avoid make auto commit save data artifact repro",
        "Question_preprocessed_content":"necessari commit file pipelin us git commit save experi navig experi possibl avoid make",
        "Question_gpt_summary_original":"The user is facing a challenge of whether or not to commit DVC files from their CI pipelines, as DVC uses git commits to save experiments and navigate between them. They are exploring the possibility of avoiding auto-commits in CI\/CD to save data artifacts after running \"dvc repro\" in the CI\/CD side.",
        "Question_gpt_summary":"user face challeng commit file pipelin us git commit save experi navig explor possibl avoid auto commit save data artifact run repro",
        "Answer_original_content":"pipelin serv mlop infrastructur popular blog post hood exampl gitlab scenario integr commit command pipelin mean git commit file commit ye need commit file git process auto commit best practic avoid git commit model train save chang file extern storag exampl gitlab artifact releas file develop machin commit user usual write script autom wait releas run cach like build cach implement run cach make file ephemer addit git commit requir technic run cach associ storag repo state run result outsid git repo data remot disclaim creator",
        "Answer_preprocessed_content":"pipelin serv mlop infrastructur popular blog post hood exampl gitlab scenario integr commit command pipelin mean file ye need commit file git process best practic avoid git commit model train save chang file extern storag file develop machin commit user usual write script autom wait releas implement make file ephemer addit git commit requir technic associ storag outsid git repo disclaim creator",
        "Answer_gpt_summary_original":"Two possible solutions were discussed in the conversation. The first solution is to save changed DVC files in external storage after ML model training in CI\/CD, then get the files to a developer machine and commit there. The second solution is to wait for DVC 1.0 release when run-cache will be implemented, which makes DVC files ephemeral and no additional Git commits will be required.",
        "Answer_gpt_summary":"possibl solut discuss convers solut save chang file extern storag model train file develop machin commit second solut wait releas run cach implement make file ephemer addit git commit requir"
    },
    {
        "Question_title":"Sagemaker with tensorflow 2 not saving model",
        "Question_body":"<p>I am working with Keras and I am trying to train a model using Sagemaker. I have the following issue:\nWhen I train my model using TensorFlow 1.12 everything works fine:<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='entrypoint-2.py',\n                            base_job_name='mlearning-test',\n                         role=role,\n                         train_instance_count=1,\n                         input_mode='Pipe',\n                         train_instance_type='ml.p2.xlarge',\n                         framework_version='1.12.0')\n<\/code><\/pre>\n\n<p>My model is trained and the model is saved in S3. Not problems.<\/p>\n\n<p>However, if I changed the framework version to be 2.0.0<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='entrypoint-2.py',\n                                base_job_name='mlearning-test',\n                             role=role,\n                             train_instance_count=1,\n                             input_mode='Pipe',\n                             train_instance_type='ml.p2.xlarge',\n                             framework_version='2.0.0')\n<\/code><\/pre>\n\n<p>I get the following error: <\/p>\n\n<pre><code>2020-02-12 13:54:36,601 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path \/opt\/ml\/model. Your training job will not save any model files to S3.\nFor details of how to construct your training script see:\nhttps:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html#adapting-your-local-tensorflow-script\n<\/code><\/pre>\n\n<p>The training job is marked as successful but there is nothing in the S3 bucket and indeed there was not training.<\/p>\n\n<p>As an alternative, I tried putting the py_version='py3' but this keeps happening. is there a major difference that I am not aware of when using TF2 on sagemaker? <\/p>\n\n<p>I don't think the entry point is needed since it works fine with version 1.12 but in case you are curious or can spot something here it is:<\/p>\n\n<pre><code>import tensorflow as tf\nfrom sagemaker_tensorflow import PipeModeDataset\n#from tensorflow.contrib.data import map_and_batch\n\nINPUT_TENSOR_NAME = 'inputs_input'  \nBATCH_SIZE = 64\nNUM_CLASSES = 5\nBUFFER_SIZE = 50\nPREFETCH_SIZE = 1\nLENGHT = 512\nSEED = 26\nEPOCHS = 1\nWIDTH = 512\n\ndef keras_model_fn(hyperparameters):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(WIDTH, 'relu', input_shape=(None, WIDTH), name = 'inputs'),\n        #tf.keras.layers.InputLayer(input_shape=(None, WIDTH), name=INPUT_TENSOR_NAME),\n        tf.keras.layers.Dense(256, 'relu'),\n        tf.keras.layers.Dense(128, 'relu'),\n        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n    ])\n\n    opt = tf.keras.optimizers.RMSprop()\n\n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n    return model\n\ndef serving_input_fn(hyperparameters):\n    # Notice that the input placeholder has the same input shape as the Keras model input\n    tensor = tf.placeholder(tf.float32, shape=[None, WIDTH])\n\n    # The inputs key INPUT_TENSOR_NAME matches the Keras InputLayer name\n    inputs = {INPUT_TENSOR_NAME: tensor}\n    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n\ndef train_input_fn(training_dir, params):\n    \"\"\"Returns input function that would feed the model during training\"\"\"\n    return _input_fn('train')\n\ndef eval_input_fn(training_dir, params):\n    \"\"\"Returns input function that would feed the model during evaluation\"\"\"\n    return _input_fn('eval')\n\ndef _input_fn(channel):\n    \"\"\"Returns a Dataset for reading from a SageMaker PipeMode channel.\"\"\"\n    print(\"DATA \"+channel)\n    features={\n        'question': tf.FixedLenFeature([WIDTH], tf.float32),\n        'label': tf.FixedLenFeature([1], tf.int64)\n    }\n\n    def parse(record):\n        parsed = tf.parse_single_example(record, features)\n        #print(\"--------&gt;\"+str(tf.cast(parsed['question'], tf.float32))\n        return {\n            INPUT_TENSOR_NAME: tf.cast(parsed['question'], tf.float32)\n        }, parsed['label']\n\n    ds = PipeModeDataset(channel)\n    if EPOCHS &gt; 1:\n        ds = ds.repeat(EPOCHS)\n    ds = ds.prefetch(PREFETCH_SIZE)\n    #ds = ds.apply(map_and_batch(parse, batch_size=BATCH_SIZE,\n    #                            num_parallel_batches=BUFFER_SIZE))\n    ds = ds.map(parse, num_parallel_calls=NUM_PARALLEL_BATCHES)\n    ds = ds.batch(BATCH_SIZE)\n\n    return ds\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1581516927893,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":913.0,
        "Answer_body":"<p>you're correct, <strong>there has been a major, beneficial change last year in the SageMaker TensorFlow experience named the <em>Script Mode<\/em> formalism<\/strong>. As you can see in the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">SDK Documentation<\/a>:<\/p>\n\n<p><em>\"Warning.\nWe have added a new format of your TensorFlow training script with TensorFlow version 1.11. This new way gives the user script more flexibility. This new format is called Script Mode, as opposed to Legacy Mode, which is what we support with TensorFlow 1.11 and older versions. In addition we are adding Python 3 support with Script Mode. The last supported version of Legacy Mode will be TensorFlow 1.12. Script Mode is available with TensorFlow version 1.11 and newer. Make sure you refer to the correct version of this README when you prepare your script. You can find the Legacy Mode README here.\"<\/em><\/p>\n\n<p>with TensorFlow 2, you need to follow that <em>Script Mode<\/em> formalism and save your model in the <code>opt\/ml\/model<\/code> path, otherwise nothing will be sent to S3. <em>Script Mode<\/em> is quite straightforward to implement and gives better flexibility and portability, and this spec is shared with SageMaker Sklearn container, SageMaker Pytorch container and SageMaker MXNet container so definitely worth adopting<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60190365",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1581724456863,
        "Question_original_content":"tensorflow save model work kera try train model follow issu train model tensorflow work fine estim tensorflow entri point entrypoint base job mlearn test role role train instanc count input mode pipe train instanc type xlarg framework version model train model save problem chang framework version estim tensorflow entri point entrypoint base job mlearn test role role train instanc count input mode pipe train instanc type xlarg framework version follow error tensorflow contain train warn model artifact save path opt model train job save model file detail construct train script http readthedoc stabl html adapt local tensorflow script train job mark success bucket train altern tri put version keep happen major differ awar think entri point need work fine version case curiou spot import tensorflow tensorflow import pipemodedataset tensorflow contrib data import map batch input tensor input input batch size num class buffer size prefetch size lenght seed epoch width def kera model hyperparamet model kera sequenti kera layer dens width relu input shape width input kera layer inputlay input shape width input tensor kera layer dens relu kera layer dens relu kera layer dens num class activ softmax opt kera optim rmsprop model compil loss categor crossentropi optim opt metric accuraci return model def serv input hyperparamet notic input placehold input shape kera model input tensor placehold float shape width input kei input tensor match kera inputlay input input tensor tensor return estim export servinginputreceiv input input def train input train dir param return input function feed model train return input train def eval input train dir param return input function feed model evalu return input eval def input channel return dataset read pipemod channel print data channel featur question fixedlenfeatur width float label fixedlenfeatur int def pars record pars pars singl exampl record featur print str cast pars question float return input tensor cast pars question float pars label pipemodedataset channel epoch repeat epoch prefetch prefetch size appli map batch pars batch size batch size num parallel batch buffer size map pars num parallel call num parallel batch batch batch size return",
        "Question_preprocessed_content":"tensorflow save model work kera try train model follow issu train model tensorflow work fine model train model save problem chang framework version follow error train job mark success bucket train altern tri put keep happen major differ awar think entri point need work fine version case curiou spot",
        "Question_gpt_summary_original":"The user is facing challenges while trying to save a model in Sagemaker using TensorFlow 2.0.0. The training job is marked as successful but the model is not saved in the S3 bucket. The user has tried changing the py_version to 'py3' but the issue persists. The user has shared the entry point code for reference.",
        "Question_gpt_summary":"user face challeng try save model tensorflow train job mark success model save bucket user tri chang version issu persist user share entri point code refer",
        "Answer_original_content":"correct major benefici chang year tensorflow experi name script mode formal sdk document warn ad new format tensorflow train script tensorflow version new wai give user script flexibl new format call script mode oppos legaci mode support tensorflow older version addit ad python support script mode support version legaci mode tensorflow script mode avail tensorflow version newer sure refer correct version readm prepar script legaci mode readm tensorflow need follow script mode formal save model opt model path sent script mode straightforward implement give better flexibl portabl spec share sklearn contain pytorch contain mxnet contain definit worth adopt",
        "Answer_preprocessed_content":"correct major benefici chang year tensorflow experi name script mode formal sdk document warn ad new format tensorflow train script tensorflow version new wai give user script flexibl new format call script mode oppos legaci mode support tensorflow older version addit ad python support script mode support version legaci mode tensorflow script mode avail tensorflow version newer sure refer correct version readm prepar script legaci mode readm tensorflow need follow script mode formal save model path sent script mode straightforward implement give better flexibl portabl spec share sklearn contain pytorch contain mxnet contain definit worth adopt",
        "Answer_gpt_summary_original":"Solution: The user needs to follow the Script Mode formalism while saving the model in Sagemaker using TensorFlow 2.0.0. The model needs to be saved in the opt\/ml\/model path, otherwise, it will not be sent to S3. The Script Mode is easy to implement and provides better flexibility and portability.",
        "Answer_gpt_summary":"solut user need follow script mode formal save model tensorflow model need save opt model path sent script mode easi implement provid better flexibl portabl"
    },
    {
        "Question_title":"When I get a prediction from sagemaker endpoint, what does the endpoint do?",
        "Question_body":"<p>In sagemaker, the docs talk about inference scripts requiring to have 4 specific functions. When we get a prediction, the python SDK sends a request to the endpoint.<\/p>\n<p>Then the inference script runs. But I cannot find where in the SDK the inference script is run.<\/p>\n<p>When I navigate through the sdk code the <code>Predictor.predict()<\/code> method calls the sagemaker session to post a request to the endpoint and get a response. That is the final step in the sdk. Sagemaker is obviously doing something when it receives that request.<\/p>\n<p>What is the code that it runs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646326739290,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":405.0,
        "Answer_body":"<p>The endpoint is essentially a Flask web server running in a Docker container<\/p>\n<p>If it's a scikit-learn image, when you invoke the endpoint, it loads your script from S3, then...<\/p>\n<p>It calls <code>input_fn(request_body: bytearray, content_type) -&gt; np.ndarray<\/code> to parse the <code>request_body<\/code> into a numpy array<\/p>\n<p>Then it calls your <code>model_fn(model_dir: str) -&gt; object<\/code> function to load the model from <code>model_dir<\/code> and return the model<\/p>\n<p>Then it calls <code>predict_fn(input_object: np.ndarray, model: object) -&gt; np.array<\/code>, which calls your <code>model.predict()<\/code> function and returns the prediction<\/p>\n<p>Then it calls <code>output_fn(prediction: np.array, accept: str)<\/code> to take the result from <code>predict_fn<\/code> and encode it to the <code>accept<\/code> type<\/p>\n<p>You don't need to implement all of these functions yourself, as there are defaults<\/p>\n<p>You <strong>do<\/strong> need to implement <code>model_fn<\/code><\/p>\n<p>You only need to implement <code>input_fn<\/code> if you have non numeric data<\/p>\n<p>You only need to implement <code>predict_fn<\/code> if your model uses something other than <code>.predict()<\/code><\/p>\n<p>You can see how the default function implementations work <a href=\"https:\/\/github.com\/aws\/sagemaker-scikit-learn-container\/blob\/master\/src\/sagemaker_sklearn_container\/serving.py\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1646761223947,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71340893",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1646329084320,
        "Question_original_content":"predict endpoint endpoint doc talk infer script requir specif function predict python sdk send request endpoint infer script run sdk infer script run navig sdk code predictor predict method call session post request endpoint respons final step sdk obvious receiv request code run",
        "Question_preprocessed_content":"predict endpoint endpoint doc talk infer script requir specif function predict python sdk send request endpoint infer script run sdk infer script run navig sdk code method call session post request endpoint respons final step sdk obvious receiv request code run",
        "Question_gpt_summary_original":"The user is facing a challenge in understanding the process of getting a prediction from a Sagemaker endpoint. They are unable to locate where the inference script is run in the Python SDK and are curious about the code that Sagemaker runs when it receives a request for prediction.",
        "Question_gpt_summary":"user face challeng understand process get predict endpoint unabl locat infer script run python sdk curiou code run receiv request predict",
        "Answer_original_content":"endpoint essenti flask web server run docker contain scikit learn imag invok endpoint load script call input request bodi bytearrai content type ndarrai pars request bodi numpi arrai call model model dir str object function load model model dir return model call predict input object ndarrai model object arrai call model predict function return predict call output predict arrai accept str result predict encod accept type need implement function default need implement model need implement input non numer data need implement predict model us predict default function implement work",
        "Answer_preprocessed_content":"endpoint essenti flask web server run docker contain imag invok endpoint load script call pars numpi arrai call function load model return model call call function return predict call result encod type need implement function default need implement need implement non numer data need implement model us default function implement work",
        "Answer_gpt_summary_original":"Solution:\n- The endpoint is essentially a Flask web server running in a Docker container.\n- If it's a scikit-learn image, when you invoke the endpoint, it loads your script from S3, then calls input_fn() to parse the request_body into a numpy array, then it calls your model_fn() function to load the model from model_dir and return the model, then it calls predict_fn() which calls your model.predict() function and returns the prediction, and finally it calls output_fn() to encode the result to the accept type.\n- You only need to implement model_fn() function.\n- You only need to implement input_fn() if you have non-numeric data.\n- You only need to implement predict_fn() if your",
        "Answer_gpt_summary":"solut endpoint essenti flask web server run docker contain scikit learn imag invok endpoint load script call input pars request bodi numpi arrai call model function load model model dir return model call predict call model predict function return predict final call output encod result accept type need implement model function need implement input non numer data need implement predict"
    },
    {
        "Question_title":"Validation error on Role name when running AWS SageMaker linear-learner locally",
        "Question_body":"<p>I'm trying to build a machine learning model locally using AWS SageMaker, but I got a validation error on IAM Role name. Although it's the exact role name that I created on the console.<\/p>\n<p>This is my code<\/p>\n<pre><code>    import boto3\n    import sagemaker\n    from sagemaker import get_execution_role\n    from sagemaker.amazon.amazon_estimator import image_uris\n    from sagemaker.amazon.amazon_estimator import RecordSet\n\n    sess = sagemaker.Session()\n\n\n    bucket = sagemaker.Session().default_bucket()\n    prefix = 'sagemaker\/ccard19'\n\n    role ='arn:aws:iam::94911111111542:role\/SageMaker-Full-Access '\n\n    container = image_uris.retrieve('linear-learner',boto3.Session().region_name)\n    \n    # Some other code\n\n   linear = sagemaker.LinearLearner(role=role,\n                                               instance_count=1,\n                                               instance_type='ml.m4.xlarge',\n                                               predictor_type='binary_classifier')\n  \n  # Some other code\n\n  ### Fit the classifier\n  linear.fit([train_records,val_records,test_records], wait=True, logs='All')\n\n<\/code><\/pre>\n<p>And this is the error message<\/p>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: 1 validation error detected: Value 'arn:aws:iam::949010940542:role\/SageMaker-Full-Access ' at 'roleArn' failed to satisfy constraint: Member must satisfy regular expression pattern: ^arn:aws[a-z\\-]*:iam::\\d{12}:role\/?[a-zA-Z_0-9+=,.@\\-_\/]+$\n<\/code><\/pre>\n<p>Any Help please?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617256728180,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":560.0,
        "Answer_body":"<p>You have <strong>space<\/strong> in the name. It should be:<\/p>\n<pre><code>role ='arn:aws:iam::94911111111542:role\/SageMaker-Full-Access'\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66899120",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1617256809808,
        "Question_original_content":"valid error role run linear learner local try build machin learn model local got valid error iam role exact role creat consol code import boto import import execut role amazon amazon estim import imag uri amazon amazon estim import recordset sess session bucket session default bucket prefix ccard role arn aw iam role access contain imag uri retriev linear learner boto session region code linear linearlearn role role instanc count instanc type xlarg predictor type binari classifi code fit classifi linear fit train record val record test record wait true log error messag clienterror error occur validationexcept call createtrainingjob oper valid error detect valu arn aw iam role access rolearn fail satisfi constraint member satisfi regular express pattern arn aw iam role help",
        "Question_preprocessed_content":"valid error role run local try build machin learn model local got valid error iam role exact role creat consol code error messag help",
        "Question_gpt_summary_original":"The user encountered a validation error on IAM Role name when trying to build a machine learning model locally using AWS SageMaker. The error message indicated that the value of the roleArn failed to satisfy the regular expression pattern. The user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount valid error iam role try build machin learn model local error messag indic valu rolearn fail satisfi regular express pattern user seek help resolv issu",
        "Answer_original_content":"space role arn aw iam role access",
        "Answer_preprocessed_content":"space",
        "Answer_gpt_summary_original":"Solution: The user should remove the space in the IAM Role name and use the correct format: 'arn:aws:iam::account-id:role\/role-name'.",
        "Answer_gpt_summary":"solut user remov space iam role us correct format arn aw iam account role role"
    },
    {
        "Question_title":"Polyaxon CE fresh deployment never finishes",
        "Question_body":"From slack\n\nHi, I\u2019m trying to deploy Polyaxon CE with helm\/argocd and the polyaxon-api keeps logging in loop:\n\nSystem check identified some issues:\n\nPreparing...\n\n\nand the pod never becomes ready.\nAny idea what could cause that?\n\nMore\n\nIt seems the polyaxon helm chart installation always fails with the message:\n\nError: timed out waiting for the condition\nThis is what I see:\n\npolyaxon-polyaxon-api-d8d7c8b5f-spsb2           1\/1     Running   0          3d5h\npolyaxon-polyaxon-api-dbb84b79c-6jqm9           0\/1     Running   2          13m\n\n\nThe polyaxon api pods take a long time to become ready. It fails liveness probe:\n\nEvents:\n  Type     Reason     Age                   From               Message\n  ----     ------     ----                  ----               -------\n  Normal   Scheduled  15m                   default-scheduler  Successfully assigned polyaxon\/polyaxon-polyaxon-api-dbb84b79c-6jqm9 to ip-192-168-165-30.us-east-2.compute.internal\n  Normal   Pulling    14m                   kubelet            Pulling image \"polyaxon\/polyaxon-api:xx\"\n  Normal   Pulled     14m                   kubelet            Successfully pulled image \"polyaxon\/polyaxon-api:xx\" in 33.387487209s\n  Normal   Created    14m                   kubelet            Created container polyaxon-api\n  Normal   Started    14m                   kubelet            Started container polyaxon-api\n  Normal   Killing    8m49s                 kubelet            Container polyaxon-api failed liveness probe, will be restarted\n  Warning  Unhealthy  8m37s (x10 over 13m)  kubelet            Readiness probe failed: Get \"[http:\/\/192.168.184.124:80\/healthz\/](http:\/\/192.168.184.124\/healthz\/)\": dial tcp 192.168.184.124:80: connect: connection refused\n  Normal   Pulled     8m19s                 kubelet            Container image \"polyaxon\/polyaxon-api:1.9.5\" already present on machine\n  Warning  Unhealthy  4m49s (x16 over 13m)  kubelet            Liveness probe failed: Get \"[http:\/\/192.168.184.124:80\/healthz\/](http:\/\/192.168.184.124\/healthz\/)\": dial tcp 192.168.184.124:80: connect: connection refused",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649328918000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"The firts step is to cehck if you are using the --wait flag with Helm, if yes the deployment will not pass because there's helm hook.\n\nThe reason of the deadlock: helm waits forever for API to be healthy and the API is waiting for the hook to initialize the database.\n\nMore info about the helm deadlock issue: helm\/helm#5118\n\nFor ArgoCD, the flag is set to true automatically: argoproj\/argo-cd#6880\n\nFor the Terraform provider for helm release, the flag is set to true by default: https:\/\/registry.terraform.io\/providers\/hashicorp\/helm\/latest\/docs\/resources\/release#wait and should be set it to false.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1473",
        "Tool":"Polyaxon",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-07T11:02:58Z",
                "Answer_score":1,
                "Answer_body":"The firts step is to cehck if you are using the --wait flag with Helm, if yes the deployment will not pass because there's helm hook.\n\nThe reason of the deadlock: helm waits forever for API to be healthy and the API is waiting for the hook to initialize the database.\n\nMore info about the helm deadlock issue: helm\/helm#5118\n\nFor ArgoCD, the flag is set to true automatically: argoproj\/argo-cd#6880\n\nFor the Terraform provider for helm release, the flag is set to true by default: https:\/\/registry.terraform.io\/providers\/hashicorp\/helm\/latest\/docs\/resources\/release#wait and should be set it to false."
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"fresh deploy finish slack try deploi helm argocd api keep log loop check identifi issu prepar pod readi idea caus helm chart instal fail messag error time wait condit api ddcbf spsb run api dbbbc jqm run api pod long time readi fail live probe event type reason ag messag normal schedul default schedul successfulli assign api dbbbc jqm east comput intern normal pull kubelet pull imag api normal pull kubelet successfulli pull imag api normal creat kubelet creat contain api normal start kubelet start contain api normal kill kubelet contain api fail live probe restart warn unhealthi kubelet readi probe fail http healthz http healthz dial tcp connect connect refus normal pull kubelet contain imag api present machin warn unhealthi kubelet live probe fail http healthz http healthz dial tcp connect connect refus",
        "Question_preprocessed_content":"fresh deploy finish slack try deploi api keep log loop check identifi issu pod readi idea caus helm chart instal fail messag error time wait condit run run api pod long time readi fail live probe event type reason ag messag normal schedul successfulli assign normal pull kubelet pull imag normal pull kubelet successfulli pull imag normal creat kubelet creat contain api normal start kubelet start contain api normal kill kubelet contain api fail live probe restart warn unhealthi kubelet readi probe fail dial tcp connect connect refus normal pull kubelet contain imag present machin warn unhealthi kubelet live probe fail dial tcp connect connect refus",
        "Question_gpt_summary_original":"The user is encountering challenges while trying to deploy Polyaxon CE with helm\/argocd. The polyaxon-api pod is not becoming ready and is failing the liveness probe, resulting in a timed-out error message. The readiness probe is also failing due to a connection refused error.",
        "Question_gpt_summary":"user encount challeng try deploi helm argocd api pod readi fail live probe result time error messag readi probe fail connect refus error",
        "Answer_original_content":"firt step cehck wait flag helm ye deploy pass helm hook reason deadlock helm wait forev api healthi api wait hook initi databas info helm deadlock issu helm helm argocd flag set true automat argoproj argo terraform provid helm releas flag set true default http registri terraform provid hashicorp helm latest doc resourc releas wait set fals",
        "Answer_preprocessed_content":"firt step cehck flag helm ye deploy pass helm hook reason deadlock helm wait forev api healthi api wait hook initi databas info helm deadlock issu argocd flag set true automat terraform provid helm releas flag set true default set fals",
        "Answer_gpt_summary_original":"The discussion provides a solution for the challenge of encountering issues while deploying Polyaxon CE with helm\/argocd. The solution involves checking if the --wait flag is being used with Helm, as it can cause a deadlock issue where the API is waiting for the hook to initialize the database. The flag is set to true automatically for ArgoCD and by default for the Terraform provider for helm release, but it should be set to false to resolve the issue.",
        "Answer_gpt_summary":"discuss provid solut challeng encount issu deploi helm argocd solut involv check wait flag helm caus deadlock issu api wait hook initi databas flag set true automat argocd default terraform provid helm releas set fals resolv issu"
    },
    {
        "Question_title":"MLFlow tracking ui not showing experiments on local machine (laptop)",
        "Question_body":"<p>I am a beginner in mlflow and was trying to set it up locally using Anaconda 3.\nI have created a new environment in anaconda and install mlflow and sklearn in it. Now I am using jupyter notebook to run my sample code for mlflow.<\/p>\n<p>'''<\/p>\n<pre><code>import os\nimport warnings\nimport sys\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import ElasticNet\nfrom urllib.parse import urlparse\nimport mlflow\nimport mlflow.sklearn\n\nimport logging\n\nlogging.basicConfig(level=logging.WARN)\nlogger = logging.getLogger(__name__)\n\nwarnings.filterwarnings(&quot;ignore&quot;)\nnp.random.seed(40)\n\n\nmlflow.set_tracking_uri(&quot;file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun&quot;)\n\nmlflow.get_tracking_uri()\n\nmlflow.get_experiment\n\n#experiment_id = mlflow.create_experiment(&quot;Mlflow_demo&quot;)\nexperiment_id = mlflow.create_experiment(&quot;Demo3&quot;)\nexperiment = mlflow.get_experiment(experiment_id)\nprint(&quot;Name: {}&quot;.format(experiment.name))\nprint(&quot;Experiment_id: {}&quot;.format(experiment.experiment_id))\nprint(&quot;Artifact Location: {}&quot;.format(experiment.artifact_location))\nprint(&quot;Tags: {}&quot;.format(experiment.tags))\nprint(&quot;Lifecycle_stage: {}&quot;.format(experiment.lifecycle_stage))\n\nmlflow.set_experiment(&quot;Demo3&quot;)\n\ndef eval_metrics(actual, pred):\n    rmse = np.sqrt(mean_squared_error(actual, pred))\n    mae = mean_absolute_error(actual, pred)\n    r2 = r2_score(actual, pred)\n    return rmse, mae, r2\n\n# Read the wine-quality csv file from the URL\ncsv_url =\\\n    'http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/wine-quality\/winequality-red.csv'\ntry:\n    data = pd.read_csv(csv_url, sep=';')\nexcept Exception as e:\n    logger.exception(\n        &quot;Unable to download training &amp; test CSV, check your internet connection. Error: %s&quot;, e)\n\ndata.head(2)\n\n\ndef train_model(data, alpha, l1_ratio):\n    \n    # Split the data into training and test sets. (0.75, 0.25) split.\n    train, test = train_test_split(data)\n\n    # The predicted column is &quot;quality&quot; which is a scalar from [3, 9]\n    train_x = train.drop([&quot;quality&quot;], axis=1)\n    test_x = test.drop([&quot;quality&quot;], axis=1)\n    train_y = train[[&quot;quality&quot;]]\n    test_y = test[[&quot;quality&quot;]]\n\n    # Set default values if no alpha is provided\n    alpha = alpha\n    l1_ratio = l1_ratio\n\n\n    # Execute ElasticNet\n    lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n    lr.fit(train_x, train_y)\n\n    # Evaluate Metrics\n    predicted_qualities = lr.predict(test_x)\n    (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n    # Print out metrics\n    print(&quot;Elasticnet model (alpha=%f, l1_ratio=%f):&quot; % (alpha, l1_ratio))\n    print(&quot;  RMSE: %s&quot; % rmse)\n    print(&quot;  MAE: %s&quot; % mae)\n    print(&quot;  R2: %s&quot; % r2)\n    \n    # Log parameter, metrics, and model to MLflow\n    with mlflow.start_run(experiment_id = experiment_id):\n        mlflow.log_param(&quot;alpha&quot;, alpha)\n        mlflow.log_param(&quot;l1_ratio&quot;, l1_ratio)\n        mlflow.log_metric(&quot;rmse&quot;, rmse)\n        mlflow.log_metric(&quot;r2&quot;, r2)\n        mlflow.log_metric(&quot;mae&quot;, mae)\n        mlflow.sklearn.log_model(lr, &quot;model&quot;)\n        \n\ntrain_model(data, 0.5, 0.5)\n\ntrain_model(data, 0.5, 0.3)\n\ntrain_model(data, 0.4, 0.3)\n<\/code><\/pre>\n<p>'''<\/p>\n<p>using above code, I am successfully able to create 3 different experiment as I can see the folders created in my local directory as shown below:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jKqgX.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Now, I am trying to run the mlflow ui using the jupyter terminal in my chrome browser and I am able to open the mlflow ui but cannot see and experiments as shown below:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6KaQK.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Could you help me in finding where I am going wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1648821712310,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":936.0,
        "Answer_body":"<p>Where do you run <code>mlflow ui<\/code> command?<\/p>\n<p>I think if you pass tracking ui path in the arguments, it would work:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>mlflow ui --backend-store-uri file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1659992652860,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71708147",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1650761211590,
        "Question_original_content":"track show experi local machin laptop beginn try set local anaconda creat new environ anaconda instal sklearn jupyt notebook run sampl code import import warn import sy import panda import numpi sklearn metric import mean squar error mean absolut error score sklearn model select import train test split sklearn linear model import elasticnet urllib pars import urlpars import import sklearn import log log basicconfig level log warn logger log getlogg warn filterwarn ignor random seed set track uri file user swapnil document localpython demo mlrun track uri experi experi creat experi demo experi creat experi demo experi experi experi print format experi print experi format experi experi print artifact locat format experi artifact locat print tag format experi tag print lifecycl stage format experi lifecycl stage set experi demo def eval metric actual pred rmse sqrt mean squar error actual pred mae mean absolut error actual pred score actual pred return rmse mae read wine qualiti csv file url csv url http archiv ic uci edu machin learn databas wine qualiti winequ red csv try data read csv csv url sep except logger except unabl download train test csv check internet connect error data head def train model data alpha ratio split data train test set split train test train test split data predict column qualiti scalar train train drop qualiti axi test test drop qualiti axi train train qualiti test test qualiti set default valu alpha provid alpha alpha ratio ratio execut elasticnet elasticnet alpha alpha ratio ratio random state fit train train evalu metric predict qualiti predict test rmse mae eval metric test predict qualiti print metric print elasticnet model alpha ratio alpha ratio print rmse rmse print mae mae print log paramet metric model start run experi experi log param alpha alpha log param ratio ratio log metric rmse rmse log metric log metric mae mae sklearn log model model train model data train model data train model data code successfulli abl creat differ experi folder creat local directori shown enter imag descript try run jupyt termin chrome browser abl open experi shown enter imag descript help find go wrong",
        "Question_preprocessed_content":"track show experi local machin beginn try set local anaconda creat new environ anaconda instal sklearn jupyt notebook run sampl code code successfulli abl creat differ experi folder creat local directori shown enter imag descript try run jupyt termin chrome browser abl open experi shown enter imag descript help find go wrong",
        "Question_gpt_summary_original":"The user is a beginner in MLFlow and has set it up locally using Anaconda 3. They have created a new environment in Anaconda and installed MLFlow and Sklearn in it. The user has successfully created three different experiments using Jupyter Notebook, but when trying to run the MLFlow UI using the Jupyter terminal in their Chrome browser, they are unable to see any experiments. The user is seeking help in finding where they are going wrong.",
        "Question_gpt_summary":"user beginn set local anaconda creat new environ anaconda instal sklearn user successfulli creat differ experi jupyt notebook try run jupyt termin chrome browser unabl experi user seek help find go wrong",
        "Answer_original_content":"run command think pass track path argument work backend store uri file user swapnil document localpython demo mlrun",
        "Answer_preprocessed_content":"run command think pass track path argument work",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to pass the tracking ui path in the arguments when running the \"mlflow ui\" command. The suggested command is \"mlflow ui --backend-store-uri file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun\".",
        "Answer_gpt_summary":"solut possibl solut mention discuss pass track path argument run command suggest command backend store uri file user swapnil document localpython demo mlrun"
    },
    {
        "Question_title":"Difference between tracking_uri and the backend store uri in MLFLOW",
        "Question_body":"<p>I am using Mlflow for my project hosting it in an EC2 instance. I was wondering in MlFlow what is the difference between the backend_store_uri we set when we launch the server and the trarcking_uri ?<\/p>\n<p>Thanks,<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629231900840,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":682.0,
        "Answer_body":"<p><code>tracking_uri<\/code> is the URL of the MLflow server (remote, or built-in in Databricks) that will be used to log metadata &amp; model (see <a href=\"https:\/\/mlflow.org\/docs\/latest\/quickstart.html#launch-a-tracking-server-on-a-remote-machine\" rel=\"nofollow noreferrer\">doc<\/a>).  In your case, this will be the URL pointing to your EC2 instance that should be configured in programs that will log parameters into your server.<\/p>\n<p><code>backend_store_uri<\/code> - is used by MLflow server to configure where to store this data - on filesystem, in SQL-compatible database, etc. (see <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#cmdoption-mlflow-server-backend-store-uri\" rel=\"nofollow noreferrer\">doc<\/a>). If you use SQL database, then you also need to provide the <code>--default-artifact-root<\/code> option to point where to store generated artifacts (images, model files, etc.)<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68823606",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1629269923972,
        "Question_original_content":"differ track uri backend store uri project host instanc wonder differ backend store uri set launch server trarck uri thank",
        "Question_preprocessed_content":"differ backend store uri project host instanc wonder differ set launch server thank",
        "Question_gpt_summary_original":"The user is facing a challenge in understanding the difference between the backend_store_uri and the tracking_uri in MLFlow while hosting it on an EC2 instance for their project.",
        "Question_gpt_summary":"user face challeng understand differ backend store uri track uri host instanc project",
        "Answer_original_content":"track uri url server remot built databrick log metadata model doc case url point instanc configur program log paramet server backend store uri server configur store data filesystem sql compat databas doc us sql databas need provid default artifact root option point store gener artifact imag model file",
        "Answer_preprocessed_content":"url server log metadata model case url point instanc configur program log paramet server server configur store data filesystem databas us sql databas need provid option point store gener artifact",
        "Answer_gpt_summary_original":"Solution: The discussion provides a clear explanation of the difference between the backend_store_uri and the tracking_uri in MLFlow. The tracking_uri is the URL of the MLflow server that will be used to log metadata and models, while the backend_store_uri is used by the MLflow server to configure where to store this data. If a SQL-compatible database is used, the --default-artifact-root option should also be provided to point where to store generated artifacts. No further solutions are mentioned in the discussion.",
        "Answer_gpt_summary":"solut discuss provid clear explan differ backend store uri track uri track uri url server log metadata model backend store uri server configur store data sql compat databas default artifact root option provid point store gener artifact solut mention discuss"
    },
    {
        "Question_title":"Adding a {serve} metagraph to existing Tensorflow model",
        "Question_body":"<p>The situation:<\/p>\n\n<p>I've already created several models, trained over several days each, that we're ready to move from local testing to a serving environment.<\/p>\n\n<p>The models were saved using the function<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def save_graph_to_file(sess, graph, graph_file_name):\n    \"\"\"Saves an graph to file, creating a valid quantized one if necessary.\"\"\"\n    output_graph_def = graph_util.convert_variables_to_constants(sess, graph.as_graph_def(), [final_tensor_name])\n    with gfile.FastGFile(graph_file_name, 'wb') as f:\n        f.write(output_graph_def.SerializeToString())\n<\/code><\/pre>\n\n<p>Now when attempting to deploy to a serving environment (Sagemaker, using a correct directory structure and file naming convention), the system returns<\/p>\n\n<pre><code>2019-06-04 22:38:53.794056: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\n2019-06-04 22:38:53.798096: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:259] SavedModel load for tags { serve }; Status: fail. Took 83297 microseconds.\n2019-06-04 22:38:53.798132: E tensorflow_serving\/util\/retrier.cc:37] Loading servable: {name: model version: 1} failed: Not found: Could not find meta graph def matching supplied tags: { serve }. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`\n<\/code><\/pre>\n\n<p>All I have are the <code>*.pb<\/code> files and their label textfiles. These work lovely across multiple computers in local environments. <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def load_graph(model_file):\n    \"\"\"\n    Code from v1.6.0 of Tensorflow's label_image.py example\n    \"\"\"\n    graph = tf.Graph()\n    graph_def = tf.GraphDef()\n    with open(model_file, \"rb\") as f:\n        graph_def.ParseFromString(f.read())\n    with graph.as_default():\n        tf.import_graph_def(graph_def)\n    return graph\n\ninputLayer = \"Mul\"\noutputLayer = \"final_result\"\ninputName = \"import\/\" + inputLayer\noutputName = \"import\/\" + outputLayer\ngraph = load_graph(modelPath)\ninputOperation = graph.get_operation_by_name(inputName)\noutputOperation = graph.get_operation_by_name(outputName)\nwith tf.Session(graph= graph) as sess:\n    # ... make a tensor t\n    results = sess.run(outputOperation.outputs[0], {\n        inputOperation.outputs[0]: t\n    })\n    # lovely functional results here\n<\/code><\/pre>\n\n<p>All I want to do is to take these existing files, add the \"serve\" tag needed, and re-save them, but everything I see seems to be related to doing this from scratch.<\/p>\n\n<p>I tried to use the builder to append a graph to a model like so:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Load the graph\ngraph = load_graph(modelPath)\nimport shutil\nif os.path.exists(exportDir):\n    shutil.rmtree(exportDir)\n# Add the serving metagraph tag\nbuilder = tf.saved_model.builder.SavedModelBuilder(exportDir)\nfrom tensorflow.saved_model import tag_constants\nwith tf.Session(graph= graph) as sess:\n    builder.add_meta_graph_and_variables(sess, [tag_constants.SERVING, tag_constants.GPU], strip_default_attrs= True)\nbuilder.save()\nprint(\"Built a SavedModel\")\n<\/code><\/pre>\n\n<p>but got the same error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559778036413,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":1140.0,
        "Answer_body":"<p>Finally solved it. This contains some S3 specific code and S3 instance calls (the <code>!<\/code> commands) but you should pretty much be able to slice that out to run this.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#!python3\n\"\"\"\nAssumes we've defined:\n\n- A directory for our working files to live in, CONTAINER_DIR\n- an arbitrary integer VERSION_INT\n- We have established local and S3 paths for our model and their labels as variables, particularly `modelLabel` and `modelPath`\n\"\"\"\n\n# Create a versioned path for the models to live in\n# See https:\/\/stackoverflow.com\/a\/54014480\/1877527\nexportDir = os.path.join(CONTAINER_DIR, VERSION_INT)\nif os.path.exists(exportDir):\n    shutil.rmtree(exportDir)\nos.mkdir(exportDir)\nimport tensorflow as tf\ndef load_graph(model_file, returnElements= None):\n    \"\"\"\n    Code from v1.6.0 of Tensorflow's label_image.py example\n    \"\"\"\n    graph = tf.Graph()\n    graph_def = tf.GraphDef()\n    with open(model_file, \"rb\") as f:\n        graph_def.ParseFromString(f.read())\n    returns = None\n    with graph.as_default():\n        returns = tf.import_graph_def(graph_def, return_elements= returnElements)\n    if returnElements is None:\n        return graph\n    return graph, returns\n\n# Add the serving metagraph tag\n# We need the inputLayerName; in Inception we're feeding the resized tensor\n# corresponding to resized_input_tensor_name\n# May be able to get away with auto-determining this if not using Inception,\n# but for Inception this is the 11th layer\ninputLayerName = \"Mul:0\"\n# Load the graph\nif inputLayerName is None:\n    graph = load_graph(modelPath)\n    inputTensor = None\nelse:\n    graph, returns = load_graph(modelPath, returnElements= [inputLayerName])\n    inputTensor = returns[0]\nwith tf.Session(graph= graph) as sess:\n    # Read the layers\n    try:\n        from tensorflow.compat.v1.saved_model import simple_save\n    except (ModuleNotFoundError, ImportError):\n        from tensorflow.saved_model import simple_save\n    with graph.as_default():\n        layers = [n.name for n in graph.as_graph_def().node]\n        outName = layers.pop() + \":0\"\n        if inputLayerName is None:\n            inputLayerName = layers.pop(0) + \":0\"\n    print(\"Checking outlayer\", outName)\n    outLayer = tf.get_default_graph().get_tensor_by_name(outName)\n    if inputTensor is None:\n        print(\"Checking inlayer\", inputLayerName)\n        inputTensor = tf.get_default_graph().get_tensor_by_name(inputLayerName)\n    inputs = {\n        inputLayerName: inputTensor\n    }\n    outputs = {\n        outName: outLayer\n    }\n    simple_save(sess, exportDir, inputs, outputs)\nprint(\"Built a SavedModel\")\n# Put the model label into the artifact dir\nmodelLabelDest = os.path.join(exportDir, \"saved_model.txt\")\n!cp {modelLabel} {modelLabelDest}\n# Prep for serving\nimport datetime as dt\nmodelArtifact = f\"livemodel_{dt.datetime.now().timestamp()}.tar.gz\"\n# Copy the version directory here to package\n!cp -R {exportDir} .\/\n# gziptar it\n!tar -czvf {modelArtifact} {VERSION_INT}\n# Shove it back to S3 for serving\n!aws s3 cp {modelArtifact} {bucketPath}\nshutil.rmtree(VERSION_INT) # Cleanup\nshutil.rmtree(exportDir) # Cleanup\n<\/code><\/pre>\n\n<p>This model is then deployable as a Sagemaker endpoint (and any other Tensorflow serving environment)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1571867961112,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56469341",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1560534456823,
        "Question_original_content":"ad serv metagraph exist tensorflow model situat creat model train dai readi local test serv environ model save function def save graph file sess graph graph file save graph file creat valid quantiz necessari output graph def graph util convert variabl constant sess graph graph def final tensor gfile fastgfil graph file write output graph def serializetostr attempt deploi serv environ correct directori structur file name convent return extern org tensorflow tensorflow save model reader read meta graph tag serv extern org tensorflow tensorflow save model loader savedmodel load tag serv statu fail took microsecond tensorflow serv util retrier load servabl model version fail meta graph def match suppli tag serv inspect avail tag set savedmodel us savedmodel cli save model cli file label textfil work love multipl comput local environ def load graph model file code tensorflow label imag exampl graph graph graph def graphdef open model file graph def parsefromstr read graph default import graph def graph def return graph inputlay mul outputlay final result inputnam import inputlay outputnam import outputlay graph load graph modelpath inputoper graph oper inputnam outputoper graph oper outputnam session graph graph sess tensor result sess run outputoper output inputoper output love function result want exist file add serv tag need save relat scratch tri us builder append graph model like load graph graph load graph modelpath import shutil path exist exportdir shutil rmtree exportdir add serv metagraph tag builder save model builder savedmodelbuild exportdir tensorflow save model import tag constant session graph graph sess builder add meta graph variabl sess tag constant serv tag constant gpu strip default attr true builder save print built savedmodel got error",
        "Question_preprocessed_content":"ad metagraph exist tensorflow model situat creat model train dai readi local test serv environ model save function attempt deploi serv environ return file label textfil work love multipl comput local environ want exist file add serv tag need relat scratch tri us builder append graph model like got error",
        "Question_gpt_summary_original":"The user has encountered challenges when attempting to deploy their existing Tensorflow models to a serving environment. The system returns an error message indicating that it could not find the meta graph def matching the supplied tags. The user has tried to add the \"serve\" tag to the existing files using the builder, but still encountered the same error.",
        "Question_gpt_summary":"user encount challeng attempt deploi exist tensorflow model serv environ return error messag indic meta graph def match suppli tag user tri add serv tag exist file builder encount error",
        "Answer_original_content":"final solv contain specif code instanc call command pretti abl slice run python assum defin directori work file live contain dir arbitrari integ version int establish local path model label variabl particularli modellabel modelpath creat version path model live http stackoverflow com exportdir path join contain dir version int path exist exportdir shutil rmtree exportdir mkdir exportdir import tensorflow def load graph model file returnel code tensorflow label imag exampl graph graph graph def graphdef open model file graph def parsefromstr read return graph default return import graph def graph def return element returnel returnel return graph return graph return add serv metagraph tag need inputlayernam incept feed resiz tensor correspond resiz input tensor abl awai auto determin incept incept layer inputlayernam mul load graph inputlayernam graph load graph modelpath inputtensor graph return load graph modelpath returnel inputlayernam inputtensor return session graph graph sess read layer try tensorflow compat save model import simpl save modulenotfounderror importerror tensorflow save model import simpl save graph default layer graph graph def node outnam layer pop inputlayernam inputlayernam layer pop print check outlay outnam outlay default graph tensor outnam inputtensor print check inlay inputlayernam inputtensor default graph tensor inputlayernam input inputlayernam inputtensor output outnam outlay simpl save sess exportdir input output print built savedmodel model label artifact dir modellabeldest path join exportdir save model txt modellabel modellabeldest prep serv import datetim modelartifact livemodel datetim timestamp tar copi version directori packag exportdir gziptar tar czvf modelartifact version int shove serv aw modelartifact bucketpath shutil rmtree version int cleanup shutil rmtree exportdir cleanup model deploy endpoint tensorflow serv environ",
        "Answer_preprocessed_content":"final solv contain specif code instanc call pretti abl slice run model deploy endpoint",
        "Answer_gpt_summary_original":"Solution:\nThe user has shared a code snippet that can be used to deploy a Tensorflow model to a serving environment. The code involves creating a versioned path for the models to live in, loading the graph, adding the serving metagraph tag, reading the layers, and preparing for serving. The model can then be deployed as a Sagemaker endpoint or any other Tensorflow serving environment.",
        "Answer_gpt_summary":"solut user share code snippet deploi tensorflow model serv environ code involv creat version path model live load graph ad serv metagraph tag read layer prepar serv model deploi endpoint tensorflow serv environ"
    },
    {
        "Question_title":"Objects Not Visible Within S3 Bucket for GroundTruth Labeling Job",
        "Question_body":"<p>I am currently creating a GroundTruth Labeling job, and am following the tutorial\n<a href=\"https:\/\/www.youtube.com\/watch?v=_FPI6KjDlCI&amp;t=210s\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=_FPI6KjDlCI&amp;t=210s<\/a>\nI have created the same bucket <code>ground-truth-example-labeling-job<\/code> and uploaded jpg files within the bucket. Within this tutorial, under Select S3 bucket or resource, they were able to go within the S3 Bucket and access the jpg files inside.<\/p>\n<p>However, I am able to go inside the <code>ground-truth-example-labeling-job<\/code> bucket, but no jpg files are visible for me to select. The entire bucket is empty with nothing to select.<\/p>\n<p>Is this a permissions settings problem?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/sdXZw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sdXZw.png\" alt=\"enter image description here\" \/><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/s10qk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s10qk.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649824909493,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":66.0,
        "Answer_body":"<p>You cannot select the files.<\/p>\n<p>But if you have a folder within a bucket then you can select that folder which consists of the input data.<\/p>\n<p>In the video they selected the bucket but not the files.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71851981",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1651560562667,
        "Question_original_content":"object visibl bucket groundtruth label job current creat groundtruth label job follow tutori http youtub com watch fpikjdlci creat bucket ground truth exampl label job upload jpg file bucket tutori select bucket resourc abl bucket access jpg file insid abl insid ground truth exampl label job bucket jpg file visibl select entir bucket select permiss set problem",
        "Question_preprocessed_content":"object visibl bucket groundtruth label job current creat groundtruth label job follow tutori creat bucket upload jpg file bucket tutori select bucket resourc abl bucket access jpg file insid abl insid bucket jpg file visibl select entir bucket select permiss set problem",
        "Question_gpt_summary_original":"The user is facing a challenge in accessing jpg files within an S3 bucket for a GroundTruth Labeling job, despite having uploaded the files. The entire bucket appears to be empty, and the user is unsure if this is due to a permissions settings problem.",
        "Question_gpt_summary":"user face challeng access jpg file bucket groundtruth label job despit have upload file entir bucket appear user unsur permiss set problem",
        "Answer_original_content":"select file folder bucket select folder consist input data video select bucket file",
        "Answer_preprocessed_content":"select file folder bucket select folder consist input data video select bucket file",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that the user should create a folder within the S3 bucket and upload the jpg files into that folder. Then, the user can select the folder as the input data for the GroundTruth Labeling job. It is also mentioned that in a video tutorial, the bucket was selected but not the files, which may have caused the issue.",
        "Answer_gpt_summary":"solut discuss suggest user creat folder bucket upload jpg file folder user select folder input data groundtruth label job mention video tutori bucket select file caus issu"
    },
    {
        "Question_title":"SageMaker MXNet local mode not working",
        "Question_body":"Hi, I am trying to fit an MXNet model locally. I am adapting this https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/ and doing the following:\n\nbucket = 'XXXXXXXXXXX'\nprefix = 'sagemaker\/cifar-bench\/data'\n\ninputs = sagemaker_session.upload_data(\n    path='data',\n    bucket=bucket, \n    key_prefix=prefix)\n\nprint('data sent to ' + inputs)\n\n\nInception = MXNet('gluon_cifar_net.py', \n          role=role, \n          train_instance_count=1, \n          train_instance_type='local_gpu',\n          framework_version='1.2.1',\n          base_job_name='cifar10-inception-',\n          hyperparameters={'batch_size': 256, \n                           'optimizer': 'sgd',\n                           'epochs': 100, \n                           'learning_rate': 0.1, \n                           'momentum': 0.9})\n\n\nInception.fit(inputs)\n\n\nwhich returns an OSError: [Errno 2] No such file or directory\n\nIn the error log I can see that there seems to be error at self.latest_training_job = _TrainingJob.start_new(self, inputs) and self.sagemaker_client.create_training_job(**train_request)\n\nHow can I make the local mode work?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1537534843000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":186.0,
        "Answer_body":"It is very likely that you don't have docker-compose (or docker) installed in the box, that is why you are getting a No such file or directory.\n\nIf you want to use the GPU setup I would recommend running on a sagemaker notebook instance. Navigate to one of the example notebooks such as: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_cifar10\/mxnet_cifar10_local_mode.ipynb\n\nAnd run the setup.sh cell. This will install and configure all the docker dependencies correctly and then you should be able to use MXNet locally on GPU without any issue.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQu1fDak6RL2wmivZ5UJwUw\/sage-maker-mx-net-local-mode-not-working",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-09-21T17:24:55.000Z",
                "Answer_score":0,
                "Answer_body":"It is very likely that you don't have docker-compose (or docker) installed in the box, that is why you are getting a No such file or directory.\n\nIf you want to use the GPU setup I would recommend running on a sagemaker notebook instance. Navigate to one of the example notebooks such as: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_cifar10\/mxnet_cifar10_local_mode.ipynb\n\nAnd run the setup.sh cell. This will install and configure all the docker dependencies correctly and then you should be able to use MXNet locally on GPU without any issue.",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1537550695000,
        "Question_original_content":"mxnet local mode work try fit mxnet model local adapt http aw amazon com blog machin learn us amazon local mode train notebook instanc follow bucket prefix cifar bench data input session upload data path data bucket bucket kei prefix prefix print data sent input incept mxnet gluon cifar net role role train instanc count train instanc type local gpu framework version base job cifar incept hyperparamet batch size optim sgd epoch learn rate momentum incept fit input return oserror errno file directori error log error self latest train job trainingjob start new self input self client creat train job train request local mode work",
        "Question_preprocessed_content":"mxnet local mode work try fit mxnet model local adapt follow bucket prefix input path data bucket bucket print incept role role optim sgd epoch momentum return oserror file directori error log error input local mode work",
        "Question_gpt_summary_original":"The user is encountering challenges with SageMaker MXNet local mode not working. They are trying to fit an MXNet model locally but are receiving an OSError: [Errno 2] No such file or directory error. The error log indicates that there seems to be an error at self.latest_training_job = _TrainingJob.start_new(self, inputs) and self.sagemaker_client.create_training_job(**train_request). The user is seeking assistance to make the local mode work.",
        "Question_gpt_summary":"user encount challeng mxnet local mode work try fit mxnet model local receiv oserror errno file directori error error log indic error self latest train job trainingjob start new self input self client creat train job train request user seek assist local mode work",
        "Answer_original_content":"like docker compos docker instal box get file directori want us gpu setup recommend run notebook instanc navig exampl notebook http github com awslab amazon exampl blob master python sdk mxnet gluon cifar mxnet cifar local mode ipynb run setup cell instal configur docker depend correctli abl us mxnet local gpu issu",
        "Answer_preprocessed_content":"like instal box get file directori want us gpu setup recommend run notebook instanc navig exampl notebook run cell instal configur docker depend correctli abl us mxnet local gpu issu",
        "Answer_gpt_summary_original":"Solution: The user can try installing docker-compose (or docker) in their system to resolve the OSError: [Errno 2] No such file or directory error. Alternatively, they can use a SageMaker notebook instance and run the setup.sh cell in an example notebook to install and configure all the docker dependencies correctly, which should allow them to use MXNet locally on GPU without any issue.",
        "Answer_gpt_summary":"solut user try instal docker compos docker resolv oserror errno file directori error altern us notebook instanc run setup cell exampl notebook instal configur docker depend correctli allow us mxnet local gpu issu"
    },
    {
        "Question_title":"security setup - username\/password experiment level",
        "Question_body":"Hi\n\n\nCurrently the way mlflow works is that we can set an experiment ID for a MLFLOW tracker server URL and call it using our training model code. We would like to see if we can add security to this - say setup username\/password and also assign users with permissions for certain experiments\/projects. Is it possible to do that?",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1542243319000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":4154.0,
        "Answer_body":"Hi Maheshwar,\n\nCurrently, we recommend adding authentication in front of your MLflow server using a HTTP proxy such as nginx. You can configure this to use some other internal authentication mechanism, such as verifying whether the user is part of a group. This won\u2019t isolate things per user, but you could run a separate tracking server for each team using ML for example.\n\nMatei\n\n\n\n> On Nov 14, 2018, at 9:55 PM, Maheshwar Dattatri <mahes...@gmail.com> wrote:\n>\n>\n> Hi\n>\n> Currently the way mlflow works is that we can set an experiment ID for a MLFLOW tracker server URL and call it using our training model code. We would like to see if we can add security to this - say setup username\/password and also assign users with permissions for certain experiments\/projects. Is it possible to do that?\n>\n\n> --\n> You received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n> To unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\n> To post to this group, send email to mlflow...@googlegroups.com.\n> To view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/de8d707f-fee9-4450-b906-89142f5d9827%40googlegroups.com.\n> For more options, visit https:\/\/groups.google.com\/d\/optout.. Hi Matei\n\n\nIn \"rest_utils.py\" , there is \"http_request\" function, which receives host_creds (include username, password) to make http request for authentication\nhttps:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/utils\/rest_utils.py#L20\u00a0\u00a0\n\nThe \"http_request\" fucntion is also used in \"databricks.py\"\nhttps:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/projects\/databricks.py#L88\n\n\n\nCould we use this function to make authentication for MLflow?\nDoes MLflow support authentication for next versions ?\n\n\nTriet Nguyen\n\ue5d3. Yes, you can actually pass these already using the environment variables MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD or MLFLOW_TRACKING_TOKEN (see https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/tracking\/utils.py). These will be used with all HTTP requests to the tracking server. You can run the server behind a proxy such as nginx to perform authentication before passing requests through (the goal here is to make it possible for people to integrate it with their own company\u2019s auth systems instead of baking one into our server).\n\nMatei\n\n\ue5d3\n> To view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/a25a55cb-9d64-438d-9e84-4f5061b59aed%40googlegroups.com.\n\n\ue5d3. Hello, this is an old question but we tried the solution suggested by @matei and we hit a dead end because of the following reasons\n\n\n\n- Nginx `auth_request` erases the data before sending `\/authorize` requests and MLFlow sends the `experiment_id`\/`run_id` in the data of `POST` and `UPDATE` requests instead of the URL (`POST \/tracking\/experiments\/1`), this makes it impossible to authorize such requests, we are denying all them right now.\n- We can't filter out the list of the experiments a user can see, we can only allow them to do a request or not. If we don't allow them to see the list of experiments, the front page of MLFlow will broken.\n\n\nI think access control mechanics over MLFlow resources should implemented in MLFlow itself or in an MLFlow plugin or at least think about how third party application can do that and makes it possible for them, that way we'll have access to the database, the UI and everything we need to implement such features.\n\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/E9QW4HdS8a8",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-11-20T21:46:46",
                "Answer_body":"Hi Maheshwar,\n\nCurrently, we recommend adding authentication in front of your MLflow server using a HTTP proxy such as nginx. You can configure this to use some other internal authentication mechanism, such as verifying whether the user is part of a group. This won\u2019t isolate things per user, but you could run a separate tracking server for each team using ML for example.\n\nMatei\n\n\n\n> On Nov 14, 2018, at 9:55 PM, Maheshwar Dattatri <mahes...@gmail.com> wrote:\n>\n>\n> Hi\n>\n> Currently the way mlflow works is that we can set an experiment ID for a MLFLOW tracker server URL and call it using our training model code. We would like to see if we can add security to this - say setup username\/password and also assign users with permissions for certain experiments\/projects. Is it possible to do that?\n>\n\n> --\n> You received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n> To unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\n> To post to this group, send email to mlflow...@googlegroups.com.\n> To view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/de8d707f-fee9-4450-b906-89142f5d9827%40googlegroups.com.\n> For more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2018-11-22T03:28:48",
                "Answer_body":"Hi Matei\n\n\nIn \"rest_utils.py\" , there is \"http_request\" function, which receives host_creds (include username, password) to make http request for authentication\nhttps:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/utils\/rest_utils.py#L20\u00a0\u00a0\n\nThe \"http_request\" fucntion is also used in \"databricks.py\"\nhttps:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/projects\/databricks.py#L88\n\n\n\nCould we use this function to make authentication for MLflow?\nDoes MLflow support authentication for next versions ?\n\n\nTriet Nguyen\n\ue5d3"
            },
            {
                "Answer_creation_time":"2018-11-22T18:50:09",
                "Answer_body":"Yes, you can actually pass these already using the environment variables MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD or MLFLOW_TRACKING_TOKEN (see https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/tracking\/utils.py). These will be used with all HTTP requests to the tracking server. You can run the server behind a proxy such as nginx to perform authentication before passing requests through (the goal here is to make it possible for people to integrate it with their own company\u2019s auth systems instead of baking one into our server).\n\nMatei\n\n\ue5d3\n> To view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/a25a55cb-9d64-438d-9e84-4f5061b59aed%40googlegroups.com.\n\n\ue5d3"
            },
            {
                "Answer_creation_time":"2021-11-29T10:21:54",
                "Answer_body":"Hello, this is an old question but we tried the solution suggested by @matei and we hit a dead end because of the following reasons\n\n\n\n- Nginx `auth_request` erases the data before sending `\/authorize` requests and MLFlow sends the `experiment_id`\/`run_id` in the data of `POST` and `UPDATE` requests instead of the URL (`POST \/tracking\/experiments\/1`), this makes it impossible to authorize such requests, we are denying all them right now.\n- We can't filter out the list of the experiments a user can see, we can only allow them to do a request or not. If we don't allow them to see the list of experiments, the front page of MLFlow will broken.\n\n\nI think access control mechanics over MLFlow resources should implemented in MLFlow itself or in an MLFlow plugin or at least think about how third party application can do that and makes it possible for them, that way we'll have access to the database, the UI and everything we need to implement such features.\n\n\ue5d3"
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"secur setup usernam password experi level current wai work set experi tracker server url train model code like add secur setup usernam password assign user permiss certain experi project possibl",
        "Question_preprocessed_content":"secur setup experi level current wai work set experi tracker server url train model code like add secur setup assign user permiss certain possibl",
        "Question_gpt_summary_original":"The user is facing a challenge with the security setup of their MLFLOW tracker server URL. They want to add a username\/password and assign permissions to users for certain experiments\/projects. They are unsure if this is possible.",
        "Question_gpt_summary":"user face challeng secur setup tracker server url want add usernam password assign permiss user certain experi project unsur possibl",
        "Answer_original_content":"maheshwar current recommend ad authent server http proxi nginx configur us intern authent mechan verifi user group wont isol thing user run separ track server team exampl matei nov maheshwar dattatri wrote current wai work set experi tracker server url train model code like add secur setup usernam password assign user permiss certain experi project possibl receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user dedf fee googlegroup com option visit http group googl com optout matei rest util http request function receiv host cred includ usernam password http request authent http github com blob master util rest util http request fucntion databrick http github com blob master project databrick us function authent support authent version triet nguyen ye actual pass environ variabl track usernam track password track token http github com blob master track util http request track server run server proxi nginx perform authent pass request goal possibl peopl integr compani auth system instead bake server matei view discuss web visit http group googl com msgid user aacb fba googlegroup com hello old question tri solut suggest matei hit dead end follow reason nginx auth request eras data send author request send experi run data post updat request instead url post track experi make imposs author request deni right filter list experi user allow request allow list experi page broken think access control mechan resourc implement plugin think parti applic make possibl wai access databas need implement featur",
        "Answer_preprocessed_content":"maheshwar current recommend ad authent server http proxi nginx configur us intern authent mechan verifi user group wont isol thing user run separ track server team exampl matei nov maheshwar dattatri wrote current wai work set experi tracker server url train model code like add secur setup assign user permiss certain possibl receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit matei function receiv http request authent fucntion us function authent support authent version triet nguyen ye actual pass environ variabl http request track server run server proxi nginx perform authent pass request matei view discuss web visit hello old question tri solut suggest hit dead end follow reason nginx eras data send request send data request instead url make imposs author request deni right filter list experi user allow request allow list experi page broken think access control mechan resourc implement plugin think parti applic make possibl wai access databas need implement featur",
        "Answer_gpt_summary_original":"Solution:\n- It is recommended to add authentication in front of the MLflow server using a HTTP proxy such as nginx. This can be configured to use some other internal authentication mechanism, such as verifying whether the user is part of a group. However, this won't isolate things per user, but a separate tracking server can be run for each team using ML.\n- MLflow supports authentication by passing environment variables MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD or MLFLOW_TRACKING_TOKEN. These will be used with all HTTP requests to the tracking server. The server can be run behind a proxy such as nginx to perform authentication before passing requests through.\n- However, there are some limitations with using Nginx `auth_request` as it er",
        "Answer_gpt_summary":"solut recommend add authent server http proxi nginx configur us intern authent mechan verifi user group won isol thing user separ track server run team support authent pass environ variabl track usernam track password track token http request track server server run proxi nginx perform authent pass request limit nginx auth request"
    },
    {
        "Question_title":"Tracking predictions with ml flow",
        "Question_body":"Hello all,\n\n\nTL;DR - Can you track the inference side of an ml pipeline using ml flow? if so whats the recommended way?\n\n\nI was thinking of using ml-flow-tracking to track model pipelines. ml-flow looks like a really good fit for tracking the training side, but I also want to track the prediction\/inference side. I didn't see any functionality for this built in to ml-flow.\n\n\nLet's define a \"problem\" has a single point solution that results in a model, that model will then go into production and then be used it to make batch predictions. Giving that definition, I was thinking of doing it in two ways.\nHave a single experiment per \"problem\" and a tag\/parameter that says if the run is a training or a prediction run.\nHave two experiments per problem, one for training and one for prediction. Each would get their own series of runs.\nDo either of those solutions make sense from ml flow perspective? Is one preferable?\nIs there an alternative?\nThe down side of 1) is that most of the metrics for training would be not be sent for prediction and vice versa. e.g. auc and hyper-parameters wouldn't apply to prediction, and something like mean inference score wouldn't apply to training.\nThe down side of 2) is that there is no link between training or prediction for a single problem in ml-flow. This makes it hard to compare things between the two, like comparing feature drift between training and prediction.\n\n\nLittle info that might help\n\nFor any experiment we will do multiple trainings that result in a best model.\nUsing that best model we could do one or multiple predictions.\u00a0\nWe will then repeat steps 1 and 2 every so often likely due to feature drift.\nThanks!",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1620341372000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":128.0,
        "Answer_body":"i'm interested in the same issue\u00a0 , how can i do model drift using mlflow\u00a0\nif there's any paper to suggest , it would be grateful\u00a0\nthank you\u00a0\n\n\n\ue5d3. Here is interesting idea explored using tracking for monitoring.\u00a0\n\n\nhttps:\/\/link.medium.com\/ugnTO7b13fb\n\n\nI\u2019ll respond to Adrian\u2019s mail shortly.\u00a0\n\n\n\nSent from my iPhone\nPardon the dumb thumb typos :)\n\n\nOn May 6, 2021, at 10:25 PM, nadine ben harrath <nadinebe...@gmail.com> wrote:\n\n\n\ufeffi'm interested in the same issue\u00a0 , how can i do model drift using mlflow\u00a0\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/bdf9f8e3-0779-49d1-8427-c06d34fa77f1n%40googlegroups.com.. TL;DR\u00a0- Can you track the inference side of an ml pipeline using ml flow? if so whats the recommended way?\n\n\nCurrently, not part of the MLflow yet per se, for instance, model performance monitoring in staging\/production. Theoretically, you can with the tracking API if you wanted.\u00a0\nOne benefit\u00a0of MLflow Tracking APIs is that, though intended for tracking experiments, they can be used for other tracking needs:\u00a0Misusing MLflow To Help Deduplicate Data At Scale\n\n\nThere are other vendors who can monitor data and concept drift or monitor model inference performance. For instance, MLflow models deployed to\u00a0\nSeldon, SageMaker, Alogirthmia, or KFServing captures metrics. This talk at DAIS might of interest for general ML performance monitoring during inference.\n\n\nLet's define a \"problem\" has a single point solution that results in a model, that model will then go into production and then be used it to make batch predictions. Giving that definition, I was thinking of doing it in two ways.\nHave a single experiment per \"problem\" and a tag\/parameter that says if the run is a training or a prediction run.\nHave two experiments per problem, one for training and one for prediction. Each would get their own series of runs.\n\n\nBoth are viable ideas, but the second keeps them separate and distinct.\u00a0\u00a0\n\n\nDoes either of those solutions make sense from an MLflow perspective? Is one preferable?\nIs there an alternative?\nThe downside of 1) is that most of the metrics for training would be not be sent for prediction and vice versa. E.g., AUC and hyper-parameters wouldn't apply to prediction, and something like mean inference score wouldn't apply to training.\nThe downside of 2) is that there is no link between training or prediction for a single problem in ml-flow. This makes it hard to compare things between the two, like comparing feature drift between training and prediction.\n\n\nCouldn't the same identifiable\/searchable tag for a specific run be the link between a run in experiments 1 and 2?\n\n\nLittle info that might help\n\nFor any experiment, we will do multiple trainings that result in the best model.\nUsing that best model we could do one or multiple predictions.\u00a0\nWe will then repeat steps 1 and 2 every so often likely due to feature drift.\n\n\nI think this could be a viable POC. I'm sure others will opine on this.\n\n\nCheers\nJules\n\n\u2013\u2013\n\nThe Best Ideas are Simple\n\nJules S. Damji\n\nSr. Developer Advocate\n\nDatabricks, Inc.\n\nju...@databricks.com\n\n(510) 304-7686\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\n\n\n\ue5d3\n\ue5d3\n--\n\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\n\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/37d68734-aa45-49aa-bbbf-7e163624da35n%40googlegroups.com.. I forgot to include this blog in the previous answer:\n\n\nhttps:\/\/databricks.com\/blog\/2019\/09\/18\/productionizing-machine-learning-from-deployment-to-drift-detection.html\n\n\n\nSent from my iPhone\nPardon the dumb thumb typos :)\n\n\nOn May 7, 2021, at 4:46 AM, Jules Damji <ju...@databricks.com> wrote:\n\n\n\ufeffHere is interesting idea explored using tracking for monitoring.\u00a0\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/ru9XgXqIMCA",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-05-07T01:25:25",
                "Answer_body":"i'm interested in the same issue\u00a0 , how can i do model drift using mlflow\u00a0\nif there's any paper to suggest , it would be grateful\u00a0\nthank you\u00a0\n\n\n\ue5d3"
            },
            {
                "Answer_creation_time":"2021-05-07T07:46:58",
                "Answer_body":"Here is interesting idea explored using tracking for monitoring.\u00a0\n\n\nhttps:\/\/link.medium.com\/ugnTO7b13fb\n\n\nI\u2019ll respond to Adrian\u2019s mail shortly.\u00a0\n\n\n\nSent from my iPhone\nPardon the dumb thumb typos :)\n\n\nOn May 6, 2021, at 10:25 PM, nadine ben harrath <nadinebe...@gmail.com> wrote:\n\n\n\ufeffi'm interested in the same issue\u00a0 , how can i do model drift using mlflow\u00a0\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/bdf9f8e3-0779-49d1-8427-c06d34fa77f1n%40googlegroups.com."
            },
            {
                "Answer_creation_time":"2021-05-07T11:43:15",
                "Answer_body":"TL;DR\u00a0- Can you track the inference side of an ml pipeline using ml flow? if so whats the recommended way?\n\n\nCurrently, not part of the MLflow yet per se, for instance, model performance monitoring in staging\/production. Theoretically, you can with the tracking API if you wanted.\u00a0\nOne benefit\u00a0of MLflow Tracking APIs is that, though intended for tracking experiments, they can be used for other tracking needs:\u00a0Misusing MLflow To Help Deduplicate Data At Scale\n\n\nThere are other vendors who can monitor data and concept drift or monitor model inference performance. For instance, MLflow models deployed to\u00a0\nSeldon, SageMaker, Alogirthmia, or KFServing captures metrics. This talk at DAIS might of interest for general ML performance monitoring during inference.\n\n\nLet's define a \"problem\" has a single point solution that results in a model, that model will then go into production and then be used it to make batch predictions. Giving that definition, I was thinking of doing it in two ways.\nHave a single experiment per \"problem\" and a tag\/parameter that says if the run is a training or a prediction run.\nHave two experiments per problem, one for training and one for prediction. Each would get their own series of runs.\n\n\nBoth are viable ideas, but the second keeps them separate and distinct.\u00a0\u00a0\n\n\nDoes either of those solutions make sense from an MLflow perspective? Is one preferable?\nIs there an alternative?\nThe downside of 1) is that most of the metrics for training would be not be sent for prediction and vice versa. E.g., AUC and hyper-parameters wouldn't apply to prediction, and something like mean inference score wouldn't apply to training.\nThe downside of 2) is that there is no link between training or prediction for a single problem in ml-flow. This makes it hard to compare things between the two, like comparing feature drift between training and prediction.\n\n\nCouldn't the same identifiable\/searchable tag for a specific run be the link between a run in experiments 1 and 2?\n\n\nLittle info that might help\n\nFor any experiment, we will do multiple trainings that result in the best model.\nUsing that best model we could do one or multiple predictions.\u00a0\nWe will then repeat steps 1 and 2 every so often likely due to feature drift.\n\n\nI think this could be a viable POC. I'm sure others will opine on this.\n\n\nCheers\nJules\n\n\u2013\u2013\n\nThe Best Ideas are Simple\n\nJules S. Damji\n\nSr. Developer Advocate\n\nDatabricks, Inc.\n\nju...@databricks.com\n\n(510) 304-7686\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\n\n\n\ue5d3\n\ue5d3\n--\n\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\n\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/37d68734-aa45-49aa-bbbf-7e163624da35n%40googlegroups.com."
            },
            {
                "Answer_creation_time":"2021-05-08T21:10:35",
                "Answer_body":"I forgot to include this blog in the previous answer:\n\n\nhttps:\/\/databricks.com\/blog\/2019\/09\/18\/productionizing-machine-learning-from-deployment-to-drift-detection.html\n\n\n\nSent from my iPhone\nPardon the dumb thumb typos :)\n\n\nOn May 7, 2021, at 4:46 AM, Jules Damji <ju...@databricks.com> wrote:\n\n\n\ufeffHere is interesting idea explored using tracking for monitoring.\u00a0\n\ue5d3"
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"track predict flow hello track infer pipelin flow what recommend wai think flow track track model pipelin flow look like good fit track train want track predict infer function built flow let defin problem singl point solut result model model product batch predict give definit think wai singl experi problem tag paramet sai run train predict run experi problem train predict seri run solut sens flow perspect prefer altern metric train sent predict vice versa auc hyper paramet wouldn appli predict like mean infer score wouldn appli train link train predict singl problem flow make hard compar thing like compar featur drift train predict littl info help experi multipl train result best model best model multipl predict repeat step like featur drift thank",
        "Question_preprocessed_content":"track predict flow hello track infer pipelin flow what recommend wai think track model pipelin look like good fit track train want track function built let defin problem singl point solut result model model product batch predict give definit think wai singl experi problem sai run train predict run experi problem train predict seri run solut sens flow perspect prefer altern metric train sent predict vice versa auc wouldn appli predict like mean infer score wouldn appli train link train predict singl problem make hard compar thing like compar featur drift train predict littl info help experi multipl train result best model best model multipl predict repeat step like featur drift thank",
        "Question_gpt_summary_original":"The user is seeking advice on how to track the inference side of an ml pipeline using ml flow. They are considering two solutions: having a single experiment per problem with a tag for training or prediction, or having two experiments per problem for training and prediction. The downside of the first solution is that metrics for training and prediction would not be sent to each other, while the downside of the second solution is that there is no link between training and prediction for a single problem in ml-flow. The user is seeking an alternative solution.",
        "Question_gpt_summary":"user seek advic track infer pipelin flow consid solut have singl experi problem tag train predict have experi problem train predict downsid solut metric train predict sent downsid second solut link train predict singl problem flow user seek altern solut",
        "Answer_original_content":"interest issu model drift paper suggest grate thank interest idea explor track monitor http link medium com ugntobfb ill respond adrian mail shortli sent iphon pardon dumb thumb typo nadin ben harrath wrote interest issu model drift receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user bdffe cdfafn googlegroup com track infer pipelin flow what recommend wai current instanc model perform monitor stage product theoret track api want benefitof track api intend track experi track need misus help dedupl data scale vendor monitor data concept drift monitor model infer perform instanc model deploi seldon alogirthmia kfserv captur metric talk dai gener perform monitor infer let defin problem singl point solut result model model product batch predict give definit think wai singl experi problem tag paramet sai run train predict run experi problem train predict seri run viabl idea second keep separ distinct solut sens perspect prefer altern downsid metric train sent predict vice versa auc hyper paramet wouldn appli predict like mean infer score wouldn appli train downsid link train predict singl problem flow make hard compar thing like compar featur drift train predict couldn identifi searchabl tag specif run link run experi littl info help experi multipl train result best model best model multipl predict repeat step like featur drift think viabl poc sure opin cheer jule best idea simpl jule damji develop advoc databrick databrick com receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user edan googlegroup com forgot includ blog previou answer http databrick com blog production machin learn deploy drift detect html sent iphon pardon dumb thumb typo jule damji wrote interest idea explor track monitor",
        "Answer_preprocessed_content":"interest issu model drift paper suggest grate thank interest idea explor track monitor ill respond adrian mail shortli sent iphon pardon dumb thumb typo nadin ben harrath wrote interest issu model drift receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit track infer pipelin flow what recommend wai current instanc model perform monitor theoret track api want benefitof track api intend track experi track need misus help dedupl data scale vendor monitor data concept drift monitor model infer perform instanc model deploi seldon alogirthmia kfserv captur metric talk dai gener perform monitor infer let defin problem singl point solut result model model product batch predict give definit think wai singl experi problem sai run train predict run experi problem train predict seri run viabl idea second keep separ distinct solut sens perspect prefer altern downsid metric train sent predict vice versa auc wouldn appli predict like mean infer score wouldn appli train downsid link train predict singl problem make hard compar thing like compar featur drift train predict couldn tag specif run link run experi littl info help experi multipl train result best model best model multipl predict repeat step like featur drift think viabl poc sure opin cheer jule best idea simpl jule damji develop advoc databrick receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit forgot includ blog previou answer sent iphon pardon dumb thumb typo jule damji wrote interest idea explor track monitor",
        "Answer_gpt_summary_original":"No solutions are provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"SageMaker Studio projects in VpcOnly mode without internet access",
        "Question_body":"A customer is using SageMaker Studio in VpcOnly mode (VPC, protected subnets without internet access, NO NAT gateways). The all functionality is fine. However, when I try create a SageMaker projects - as described here, SageMaker Studio is unable to list the project templates (timeout and unspecified error) resulting in empty list of the available project templates.\n\nProjects are enabled for the users - as described here. The problem is with project creation.\n\nIs internet access (e.g. via NAT gateways) is needed for SageMaker projects?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615480055000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":323.0,
        "Answer_body":"Figured it out. SageMaker Studio projects need Service Catalog access and VPCE for com.amazonaws.${AWS::Region}.servicecatalog",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcyhpq1pxRTmtjkDRAh_MDA\/sage-maker-studio-projects-in-vpc-only-mode-without-internet-access",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-10T20:10:40.000Z",
                "Answer_score":0,
                "Answer_body":"Figured it out. SageMaker Studio projects need Service Catalog access and VPCE for com.amazonaws.${AWS::Region}.servicecatalog",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1618085440000,
        "Question_original_content":"studio project vpconli mode internet access custom studio vpconli mode vpc protect subnet internet access nat gatewai function fine try creat project describ studio unabl list project templat timeout unspecifi error result list avail project templat project enabl user describ problem project creation internet access nat gatewai need project",
        "Question_preprocessed_content":"studio project vpconli mode internet access custom studio vpconli mode function fine try creat project describ studio unabl list project templat result list avail project templat project enabl user describ problem project creation internet access need project",
        "Question_gpt_summary_original":"The user is facing challenges in creating SageMaker projects in VpcOnly mode without internet access. Although all other functionalities are working fine, SageMaker Studio is unable to list project templates resulting in an empty list of available project templates. The user has enabled projects for the users, but the problem persists with project creation. The user is unsure if internet access is required for SageMaker projects.",
        "Question_gpt_summary":"user face challeng creat project vpconli mode internet access function work fine studio unabl list project templat result list avail project templat user enabl project user problem persist project creation user unsur internet access requir project",
        "Answer_original_content":"figur studio project need servic catalog access vpce com amazonaw aw region servicecatalog",
        "Answer_preprocessed_content":"figur studio project need servic catalog access vpce",
        "Answer_gpt_summary_original":"Solution: The user found a solution to the challenge by enabling Service Catalog access and VPCE for com.amazonaws.${AWS::Region}.servicecatalog. This resolved the issue with SageMaker Studio not being able to list project templates and allowed for project creation in VpcOnly mode without internet access.",
        "Answer_gpt_summary":"solut user solut challeng enabl servic catalog access vpce com amazonaw aw region servicecatalog resolv issu studio abl list project templat allow project creation vpconli mode internet access"
    },
    {
        "Question_title":"ML flow and Airflow Integration",
        "Question_body":"Hi\n\n\nIs there any\u00a0 integration of airflow and mlflow\u00a0 for scheduling\n\n\n\nThank you and Warm Regards",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1569290907000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":18.0,
        "Answer_body":"Hey Preetam,\n\n\nDo you mind sharing more about your use-case with airflow? Do you hope to schedule model training, deployment, or something else?\n\n\nWe plan to develop some airflow scripts that pull metrics from MLFlow into a data warehouse, to enable analytics in other tools (BI dashboards, etc..). Happy to contribute that to open source once we're done.\n\n\n-Adam\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CAH1WR8rOR%2B8tfWMGBSqaMCo2%3DG1PATSQpD4b5R%2Bp8izwaKLHEw%40mail.gmail.com.. Sounds good, thanks! with respect to airflow scripts that pull metrics from Ml-flow into a data warehouse. I am definitely looking at this use case.\u00a0\u00a0\n\n\n\nCurrently I am looking at model training and deployment .\u00a0\n\ue5d3",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/nuxDrEAXseg",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-09-24T16:17:18",
                "Answer_body":"Hey Preetam,\n\n\nDo you mind sharing more about your use-case with airflow? Do you hope to schedule model training, deployment, or something else?\n\n\nWe plan to develop some airflow scripts that pull metrics from MLFlow into a data warehouse, to enable analytics in other tools (BI dashboards, etc..). Happy to contribute that to open source once we're done.\n\n\n-Adam\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CAH1WR8rOR%2B8tfWMGBSqaMCo2%3DG1PATSQpD4b5R%2Bp8izwaKLHEw%40mail.gmail.com."
            },
            {
                "Answer_creation_time":"2019-09-27T06:52:11",
                "Answer_body":"Sounds good, thanks! with respect to airflow scripts that pull metrics from Ml-flow into a data warehouse. I am definitely looking at this use case.\u00a0\u00a0\n\n\n\nCurrently I am looking at model training and deployment .\u00a0\n\ue5d3"
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"flow airflow integr integr airflow schedul thank warm regard",
        "Question_preprocessed_content":"flow airflow integr integr airflow schedul thank warm regard",
        "Question_gpt_summary_original":"The user is inquiring about the integration of Airflow and MLflow for scheduling purposes. No specific challenges are mentioned.",
        "Question_gpt_summary":"user inquir integr airflow schedul purpos specif challeng mention",
        "Answer_original_content":"hei preetam mind share us case airflow hope schedul model train deploy plan develop airflow script pull metric data warehous enabl analyt tool dashboard happi contribut open sourc adam receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user cahwrror btfwmgbsqamco dgpatsqpdbr bpizwaklhew mail gmail com sound good thank respect airflow script pull metric flow data warehous definit look us case current look model train deploy",
        "Answer_preprocessed_content":"hei preetam mind share airflow hope schedul model train deploy plan develop airflow script pull metric data warehous enabl analyt tool happi contribut open sourc adam receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit sound good thank respect airflow script pull metric data warehous definit look us case current look model train deploy",
        "Answer_gpt_summary_original":"Solutions provided: \n- One solution mentioned is to develop some Airflow scripts that pull metrics from MLFlow into a data warehouse to enable analytics in other tools such as BI dashboards. The person who suggested this is willing to contribute it to open source once they are done. \n\nNo specific challenges were mentioned in the discussion.",
        "Answer_gpt_summary":"solut provid solut mention develop airflow script pull metric data warehous enabl analyt tool dashboard person suggest will contribut open sourc specif challeng mention discuss"
    },
    {
        "Question_title":"Convert web service output to a dataset Azure MLS classic",
        "Question_body":"Is it possible to convert a web service output as a dataset or a csv file ? I want to consume this in another experiment.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592408319737,
        "Question_favorite_count":3.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"You can delete or export in-product data stored by Azure Machine Learning Studio (classic) by using the Azure portal, the Studio (classic) interface, PowerShell, and authenticated REST APIs. This article tells you how.\n\nTelemetry data can be accessed through the Azure Privacy portal.\n\nMore details please refer to: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/export-delete-personal-data-dsr\n\nAnd also you can use one of the Azure Machine Learning Studio Module - \"Export Data\" to do it : https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/export-data?redirectedfrom=MSDN\n\nLet me know if you have more questions.\n\nRegards,\nYutong",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/37214\/convert-web-service-output-to-a-dataset-azure-mls.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-17T17:56:25.02Z",
                "Answer_score":0,
                "Answer_body":"You can delete or export in-product data stored by Azure Machine Learning Studio (classic) by using the Azure portal, the Studio (classic) interface, PowerShell, and authenticated REST APIs. This article tells you how.\n\nTelemetry data can be accessed through the Azure Privacy portal.\n\nMore details please refer to: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/export-delete-personal-data-dsr\n\nAnd also you can use one of the Azure Machine Learning Studio Module - \"Export Data\" to do it : https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/export-data?redirectedfrom=MSDN\n\nLet me know if you have more questions.\n\nRegards,\nYutong",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1592416585020,
        "Question_original_content":"convert web servic output dataset classic possibl convert web servic output dataset csv file want consum experi",
        "Question_preprocessed_content":"convert web servic output dataset classic possibl convert web servic output dataset csv file want consum experi",
        "Question_gpt_summary_original":"The user is facing a challenge of converting a web service output into a dataset or a CSV file in order to consume it in another experiment.",
        "Question_gpt_summary":"user face challeng convert web servic output dataset csv file order consum experi",
        "Answer_original_content":"delet export product data store studio classic azur portal studio classic interfac powershel authent rest api articl tell telemetri data access azur privaci portal detail refer http doc microsoft com azur machin learn studio export delet person data dsr us studio modul export data http doc microsoft com azur machin learn studio modul refer export data redirectedfrom msdn let know question regard yutong",
        "Answer_preprocessed_content":"delet export data store studio azur portal studio interfac powershel authent rest api articl tell telemetri data access azur privaci portal detail refer us studio modul export data let know question regard yutong",
        "Answer_gpt_summary_original":"Solution: The discussion provides two solutions to the challenge. The first solution is to delete or export in-product data stored by Azure Machine Learning Studio (classic) using the Azure portal, the Studio (classic) interface, PowerShell, and authenticated REST APIs. The second solution is to use the \"Export Data\" module of Azure Machine Learning Studio to convert the web service output into a dataset or a CSV file. The discussion also suggests referring to the provided links for more details.",
        "Answer_gpt_summary":"solut discuss provid solut challeng solut delet export product data store studio classic azur portal studio classic interfac powershel authent rest api second solut us export data modul studio convert web servic output dataset csv file discuss suggest refer provid link detail"
    },
    {
        "Question_title":"Why AWS Lambda Internel Server Error 500 but successfully \/invocations POST 200 in Endpoint SageMaker?",
        "Question_body":"<pre><code>import os\nimport io\nimport boto3\nimport json\nimport csv\n\n\n# grab environment variables\nENDPOINT_NAME = os.environ['ENDPOINT_NAME']\n# grab runtime client\nruntime = boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    # Load data from POST request\n    data = json.loads(json.dumps(event))\n    \n    # Grab the payload\n    payload = data['body']\n    \n    # Invoke the model. In this case the data type is a JSON but can be other things such as a CSV\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                   ContentType='application\/json',\n                                   Body=payload)\n    \n    # Get the body of the response from the model\n    result = response['Body'].read().decode()\n\n    # Return it along with the status code of 200 meaning this was succesful \n    return {\n        'statusCode': 200,\n        'body': result\n    }\n<\/code><\/pre>\n<p><strong>response from AWS Lambda<\/strong><\/p>\n<pre><code>{\n  &quot;errorMessage&quot;: &quot;'body'&quot;,\n  &quot;errorType&quot;: &quot;KeyError&quot;,\n  &quot;stackTrace&quot;: [\n    [\n      &quot;\/var\/task\/lambda_function.py&quot;,\n      18,\n      &quot;lambda_handler&quot;,\n      &quot;payload = data['body']&quot;\n    ]\n  ]\n}\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/h8wvA.png\" rel=\"nofollow noreferrer\">response from Postman 500 Internal Server Error<\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/cuknX.png\" rel=\"nofollow noreferrer\">but successfully invoke POST 200 in SageMaker Endpoint<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1626360722473,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1626360827356,
        "Question_score":0.0,
        "Question_view_count":277.0,
        "Answer_body":"<p>The issue is when you are trying to parse your payload with data['body']. The data is not being passed in the format that the endpoint is expecting. Use the following code snippet to properly format\/serialize your data for the endpoint. Also to make all this clearer make sure to check for your payload type to make sure you have not serialized again by accident.<\/p>\n<pre><code>    data = json.loads(json.dumps(event))\n    payload = json.dumps(data)\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                       ContentType='application\/json',\n                                       Body=payload)\n    result = json.loads(response['Body'].read().decode())\n<\/code><\/pre>\n<p>I work for AWS &amp; my opinions are my own<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1626979396583,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68396088",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1626975923230,
        "Question_original_content":"aw lambda internel server error successfulli invoc post endpoint import import import boto import json import csv grab environ variabl endpoint environ endpoint grab runtim client runtim boto client runtim def lambda handler event context load data post request data json load json dump event grab payload payload data bodi invok model case data type json thing csv respons runtim invok endpoint endpointnam endpoint contenttyp applic json bodi payload bodi respons model result respons bodi read decod return statu code mean succes return statuscod bodi result respons aw lambda errormessag bodi errortyp keyerror stacktrac var task lambda function lambda handler payload data bodi respons postman intern server error successfulli invok post endpoint",
        "Question_preprocessed_content":"aw lambda internel server error successfulli post endpoint respons aw lambda respons postman intern server error successfulli invok post endpoint",
        "Question_gpt_summary_original":"The user is facing a challenge where they are receiving an Internal Server Error 500 response from AWS Lambda when using a POST request with a JSON payload. The error is caused by a KeyError in the Lambda function, specifically with the 'body' parameter. However, the user is able to successfully invoke a POST request with a JSON payload to the SageMaker Endpoint and receive a 200 response.",
        "Question_gpt_summary":"user face challeng receiv intern server error respons aw lambda post request json payload error caus keyerror lambda function specif bodi paramet user abl successfulli invok post request json payload endpoint receiv respons",
        "Answer_original_content":"issu try pars payload data bodi data pass format endpoint expect us follow code snippet properli format serial data endpoint clearer sure check payload type sure serial accid data json load json dump event payload json dump data respons runtim invok endpoint endpointnam endpoint contenttyp applic json bodi payload result json load respons bodi read decod work aw opinion",
        "Answer_preprocessed_content":"issu try pars payload data data pass format endpoint expect us follow code snippet properli data endpoint clearer sure check payload type sure serial accid work aw opinion",
        "Answer_gpt_summary_original":"Solution: The discussion provides a code snippet that can be used to properly format and serialize the data for the endpoint. The user should use this code snippet to parse the payload and check for the payload type to ensure that it has not been serialized again by accident.",
        "Answer_gpt_summary":"solut discuss provid code snippet properli format serial data endpoint user us code snippet pars payload check payload type ensur serial accid"
    },
    {
        "Question_title":"Deploy Pre-Trained model to SageMaker Endpoint from CloudFormation",
        "Question_body":"<p>This seems to be a tricky thing to do, as I haven't found too much documentation for it. I'm trying to deploy a Huggingface pre-trained model for NLU to a SageMaker endpoint. Naturally, I don't want to do this manually, I'd like to automate it through CloudFormation. I found a somewhat <a href=\"https:\/\/faun.pub\/mastering-the-mystical-art-of-model-deployment-part-2-deploying-amazon-sagemaker-endpoints-with-cf9539dc2579\" rel=\"nofollow noreferrer\">useful article<\/a> on how to deploy, but the name of the training model is confusing and I don't know where I would find the right name for the model I want to deploy or where I would put that name (I want to deploy an <a href=\"https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L6-v2\" rel=\"nofollow noreferrer\">all-MiniLM-L6-v2<\/a> model).<\/p>\n<p>Is this possible to do? Do I need to deploy a container? If so, how do I set up the container to process requests and return the text embeddings from the model? I've looked into doing this with just a lambda (which would satisfy the automated deployment process), but the packages I need to use greatly exceed the 250MB limit for lambda+layers.<\/p>\n<p>How do I deploy an endpoint from CloudFormation? Does anyone have experience doing this? If so, please share your wisdom.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1658934967237,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":103.0,
        "Answer_body":"<p>To anyone curious, this is how I ended up solving this issue:<\/p>\n<p>I ran a Jupyter notebook locally to create the model artifact. Once complete, I zipped the model artifact into a tar.gz file.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModel, AutoTokenizer\nfrom os import makedirs\n\nsaved_model_dir = 'saved_model_dir'\nmakedirs(saved_model_dir, exist_ok=True)\n\n# models were obtained from https:\/\/huggingface.co\/models\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers\/multi-qa-MiniLM-L6-cos-v1')\nmodel = AutoModel.from_pretrained('sentence-transformers\/multi-qa-MiniLM-L6-cos-v1')\n\ntokenizer.save_pretrained(saved_model_dir)\nmodel.save_pretrained(saved_model_dir)\n<\/code><\/pre>\n<pre class=\"lang-bash prettyprint-override\"><code>cd saved_model_dir &amp;&amp; tar czvf ..\/model.tar.gz *\n<\/code><\/pre>\n<p>I included a script in my pipeline to then upload that artifact to S3.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>aws s3 cp path\/to\/model.tar.gz s3:\/\/bucket-name\/prefix\n<\/code><\/pre>\n<p>I also created a CloudFormation template that would stand up my SageMaker resources. The tricky part of this was finding a container image to use, and a colleague was able to point me to <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">this repo<\/a> that contained a massive list of AWS-maintained container images for deep learning and inference. From there, I just needed to select the one that fit my needs.<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>Resources:\n  SageMakerModel:\n    Type: AWS::SageMaker::Model\n    Properties:\n      PrimaryContainer:\n        Image: 763104351884.dkr.ecr.us-east-1.amazonaws.com\/pytorch-inference:1.12.0-cpu-py38-ubuntu20.04-sagemaker # image resource found at https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\n        Mode: SingleModel\n        ModelDataUrl: s3:\/\/path\/to\/model.tar.gz\n      ExecutionRole: \n      ModelName: inference-model\n\n  SageMakerEndpointConfig:\n    Type: AWS::SageMaker::EndpointConfig\n    Properties:\n      EndpointConfigName: endpoint-config-name\n      ProductionVariants:\n        - ModelName: inference-model\n          InitialInstanceCount: 1\n          InstanceType: ml.t2.medium\n          VariantName: dev\n  \n  SageMakerEndpoint:\n    Type: AWS::SageMaker::Endpoint\n    Properties:\n      EndpointName: endpoint-name\n      EndpointConfigName: !GetAtt SageMakerEndpointConfig.EndpointConfigName\n<\/code><\/pre>\n<p>Once the PyTorch model is created locally, this solution essentially automates the process of provisioning and deploying a SageMaker endpoint for inference. If I need to switch the model, I just need to run my notebook code and it will overwrite my existing model artifact. Then I can redeploy and my pipeline will re-upload the artifact to S3, modify the existing SageMaker resources, and the solution will begin operating using the new model.<\/p>\n<p>This may not be the most elegant solution out there, so any suggestions or pointers would be much appreciated!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1659976426276,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73140531",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1659976220936,
        "Question_original_content":"deploi pre train model endpoint cloudform tricki thing haven document try deploi huggingfac pre train model nlu endpoint natur want manual like autom cloudform somewhat us articl deploi train model confus know right model want deploi want deploi minilm model possibl need deploi contain set contain process request return text embed model look lambda satisfi autom deploy process packag need us greatli exce limit lambda layer deploi endpoint cloudform experi share wisdom",
        "Question_preprocessed_content":"deploi model endpoint cloudform tricki thing haven document try deploi huggingfac model nlu endpoint natur want manual like autom cloudform somewhat us articl deploi train model confus know right model want deploi possibl need deploi contain set contain process request return text embed model look lambda packag need us greatli exce limit lambda layer deploi endpoint cloudform experi share wisdom",
        "Question_gpt_summary_original":"The user is facing challenges in deploying a Huggingface pre-trained model for NLU to a SageMaker endpoint through CloudFormation. They are struggling to find documentation on how to do this and are unsure of the name of the training model they want to deploy. They are also unsure if they need to deploy a container and how to set it up to process requests and return text embeddings from the model. The user has looked into using a lambda but the packages they need exceed the 250MB limit. They are seeking advice on how to deploy an endpoint from CloudFormation.",
        "Question_gpt_summary":"user face challeng deploi huggingfac pre train model nlu endpoint cloudform struggl document unsur train model want deploi unsur need deploi contain set process request return text embed model user look lambda packag need exce limit seek advic deploi endpoint cloudform",
        "Answer_original_content":"curiou end solv issu ran jupyt notebook local creat model artifact complet zip model artifact tar file transform import automodel autotoken import makedir save model dir save model dir makedir save model dir exist true model obtain http huggingfac model token autotoken pretrain sentenc transform multi minilm co model automodel pretrain sentenc transform multi minilm co token save pretrain save model dir model save pretrain save model dir save model dir tar czvf model tar includ script pipelin upload artifact aw path model tar bucket prefix creat cloudform templat stand resourc tricki find contain imag us colleagu abl point repo contain massiv list aw maintain contain imag deep learn infer need select fit need resourc model type aw model properti primarycontain imag dkr ecr east amazonaw com pytorch infer cpu ubuntu imag resourc http github com aw deep learn contain blob master avail imag mode singlemodel modeldataurl path model tar executionrol modelnam infer model endpointconfig type aw endpointconfig properti endpointconfignam endpoint config productionvari modelnam infer model initialinstancecount instancetyp medium variantnam dev endpoint type aw endpoint properti endpointnam endpoint endpointconfignam getatt endpointconfig endpointconfignam pytorch model creat local solut essenti autom process provis deploi endpoint infer need switch model need run notebook code overwrit exist model artifact redeploi pipelin upload artifact modifi exist resourc solut begin oper new model eleg solut suggest pointer appreci",
        "Answer_preprocessed_content":"curiou end solv issu ran jupyt notebook local creat model artifact complet zip model artifact file includ script pipelin upload artifact creat cloudform templat stand resourc tricki find contain imag us colleagu abl point repo contain massiv list contain imag deep learn infer need select fit need pytorch model creat local solut essenti autom process provis deploi endpoint infer need switch model need run notebook code overwrit exist model artifact redeploi pipelin artifact modifi exist resourc solut begin oper new model eleg solut suggest pointer appreci",
        "Answer_gpt_summary_original":"Solution:\nThe user solved the issue by creating a model artifact using a Jupyter notebook locally and zipping it into a tar.gz file. They then uploaded the artifact to S3 and created a CloudFormation template to stand up SageMaker resources. They found a container image from an AWS-maintained repository that fit their needs and used it in the CloudFormation template. This solution automates the process of provisioning and deploying a SageMaker endpoint for inference.",
        "Answer_gpt_summary":"solut user solv issu creat model artifact jupyt notebook local zip tar file upload artifact creat cloudform templat stand resourc contain imag aw maintain repositori fit need cloudform templat solut autom process provis deploi endpoint infer"
    },
    {
        "Question_title":"Unable to compile model to Neuron: no error message, no output",
        "Question_body":"Hi. We are trying to convert all our in-house pytorch models to aws-neuron on inferentia. We successfully converted one, but the second model we tried did not compile. Unfortunately, compilation did not generate any error message nor log of any kind, so we are stuck. The model is rather simple, but large, U-Net, with partial convolutions instead of regular ones, but otherwise no fancy operators. Conversion of this model to torchscript is ok on the same instance. Could it be a memory problem ?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1663166637969,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":71.0,
        "Answer_body":"Hi, in order to see more information about the error, you can enable debugging during tracing by passing 'verbose' to the tracing command like this:\n\nimport torch\nimport torch.neuron\ntorch.neuron.trace(\n    model,\n    example_inputs=inp,\n    verbose=\"debug\",\n    compiler_workdir=\"logs\" # dir where debugging logs will be saved\n)\n\n\nYou'll see the error messages in the console and they will also be saved to the \"logs\" dir.\n\nIt is always good to run the NeuronSDK analyzer first to make sure the model is: 1\/ torch.jit traceable; 2\/ supported by the compiler\n\nimport torch\nimport torch.neuron\ntorch.neuron.analyze_model(model, example_inputs=inp)\n\n\nYou can also see a sample that shows how to compile an U-net Pytorch (3rd party implementation) to Inf1 instances here: https:\/\/github.com\/samir-souza\/laboratory\/blob\/master\/05_Inferentia\/03_UnetPytorch\/03_UnetPytorch.ipynb\n\nRef: https:\/\/awsdocs-neuron.readthedocs-hosted.com\/en\/latest\/neuron-guide\/neuron-frameworks\/pytorch-neuron\/api-compilation-python-api.html\n\nIf everything fails, try to look for something like this in the logs:\n\nINFO:Neuron:Compile command returned: -11\nWARNING:Neuron:torch.neuron.trace failed on _NeuronGraph$647; falling back to native python function call\nERROR:Neuron:neuron-cc failed with the following command line call:\n\n\nAnd paste here, please. With the \"Compile command returned:\" code it is possible to identify the error. You are suspecting that there is some issue related to memory, maybe Out of Memory. Normally when that is the case, you'll find the code: -9 in this part of the error.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUA_oVwSPQReCt96QyX4cz-g\/unable-to-compile-model-to-neuron-no-error-message-no-output",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-15T09:25:42.678Z",
                "Answer_score":2,
                "Answer_body":"Hi, in order to see more information about the error, you can enable debugging during tracing by passing 'verbose' to the tracing command like this:\n\nimport torch\nimport torch.neuron\ntorch.neuron.trace(\n    model,\n    example_inputs=inp,\n    verbose=\"debug\",\n    compiler_workdir=\"logs\" # dir where debugging logs will be saved\n)\n\n\nYou'll see the error messages in the console and they will also be saved to the \"logs\" dir.\n\nIt is always good to run the NeuronSDK analyzer first to make sure the model is: 1\/ torch.jit traceable; 2\/ supported by the compiler\n\nimport torch\nimport torch.neuron\ntorch.neuron.analyze_model(model, example_inputs=inp)\n\n\nYou can also see a sample that shows how to compile an U-net Pytorch (3rd party implementation) to Inf1 instances here: https:\/\/github.com\/samir-souza\/laboratory\/blob\/master\/05_Inferentia\/03_UnetPytorch\/03_UnetPytorch.ipynb\n\nRef: https:\/\/awsdocs-neuron.readthedocs-hosted.com\/en\/latest\/neuron-guide\/neuron-frameworks\/pytorch-neuron\/api-compilation-python-api.html\n\nIf everything fails, try to look for something like this in the logs:\n\nINFO:Neuron:Compile command returned: -11\nWARNING:Neuron:torch.neuron.trace failed on _NeuronGraph$647; falling back to native python function call\nERROR:Neuron:neuron-cc failed with the following command line call:\n\n\nAnd paste here, please. With the \"Compile command returned:\" code it is possible to identify the error. You are suspecting that there is some issue related to memory, maybe Out of Memory. Normally when that is the case, you'll find the code: -9 in this part of the error.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-09-21T07:33:00.671Z",
                "Answer_score":0,
                "Answer_body":"Following your answer we were able to check the log and got\n\nINFO:Neuron:Compile command returned: -9\n\nwhich is apparently an out of memory error. Switching to a 6x instance solved the problem",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":1663233942678,
        "Question_original_content":"unabl compil model neuron error messag output try convert hous pytorch model aw neuron inferentia successfulli convert second model tri compil unfortun compil gener error messag log kind stuck model simpl larg net partial convolut instead regular on fanci oper convers model torchscript instanc memori problem",
        "Question_preprocessed_content":"unabl compil model neuron error messag output try convert pytorch model inferentia successfulli convert second model tri compil unfortun compil gener error messag log kind stuck model simpl larg partial convolut instead regular on fanci oper convers model torchscript instanc memori problem",
        "Question_gpt_summary_original":"The user is facing challenges in converting their in-house PyTorch models to AWS-Neuron on Inferentia. They were successful in converting one model, but the second model, a U-Net with partial convolutions, did not compile. The user did not receive any error messages or logs, and they suspect it may be a memory problem.",
        "Question_gpt_summary":"user face challeng convert hous pytorch model aw neuron inferentia success convert model second model net partial convolut compil user receiv error messag log suspect memori problem",
        "Answer_original_content":"order inform error enabl debug trace pass verbos trace command like import torch import torch neuron torch neuron trace model exampl input inp verbos debug compil workdir log dir debug log save error messag consol save log dir good run neuronsdk analyz sure model torch jit traceabl support compil import torch import torch neuron torch neuron analyz model model exampl input inp sampl show compil net pytorch parti implement inf instanc http github com samir souza laboratori blob master inferentia unetpytorch unetpytorch ipynb ref http awsdoc neuron readthedoc host com latest neuron guid neuron framework pytorch neuron api compil python api html fail try look like log info neuron compil command return warn neuron torch neuron trace fail neurongraph fall nativ python function error neuron neuron fail follow command line past compil command return code possibl identifi error suspect issu relat memori mayb memori normal case code error",
        "Answer_preprocessed_content":"order inform error enabl debug trace pass verbos trace command like import torch import model verbos debug dir debug log save error messag consol save log dir good run neuronsdk analyz sure model traceabl support compil import torch import sampl show compil pytorch inf instanc ref fail try look like log info neuron compil command return fail fall nativ python function fail follow command line past compil command return code possibl identifi error suspect issu relat memori mayb memori normal case code error",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Enable debugging during tracing to see error messages and save them to a directory.\n2. Run the NeuronSDK analyzer to ensure the model is traceable and supported by the compiler.\n3. Refer to a sample implementation of U-Net PyTorch on Inf1 instances.\n4. Look for error codes in the logs, such as \"Compile command returned:\", to identify the error. \n\nNo solution was provided for the suspected memory issue.",
        "Answer_gpt_summary":"possibl solut mention discuss enabl debug trace error messag save directori run neuronsdk analyz ensur model traceabl support compil refer sampl implement net pytorch inf instanc look error code log compil command return identifi error solut provid suspect memori issu"
    },
    {
        "Question_title":"Is there a way to install R libraries in SageMaker that receive a non-zero exit status?",
        "Question_body":"Hi all, I'm having an issue with an R kernel\/Jupyter notebook. I've come across two different libraries that result in the following error:\n\nWarning message in install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\"):\n\u201cinstallation of package \u2018XML\u2019 had non-zero exit status\u201dUpdating HTML index of packages in '.Library'\nMaking 'packages.html' ... done\n\n\nXML is the second package that I have run into this issue with. The other is rJava.\n\nI found a workaround that could work if I had root access, which is installing via the command line in a terminal. Which involves commands such as:\n\nyum install r-cran-rjava\n\n\nHowever, I don't have root access and cannot install as I get the message \"You need to be root to perform this command.\" So this workaround hasn't been possible.\n\nAfter checking the documentation for rJava and XML, I am running the requirements for JDK and other system requirements in SageMaker. This issue wasn't reproducible on a local RStudio environment. XML is a dependency for multiple R libraries (as is rJava). Is there a way that I can still install these packages?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1527798496000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":464.0,
        "Answer_body":"For Amazon SageMaker notebook Instances, you have the ability to assume root privileges, so instead of:\n\n$ yum install r-cran-rjava\n\nyou can try:\n\n$ sudo yum install r-cran-rjava\n\nwhich will allow you to impersonate the superuser (ie. root) for that command 1\n\nBut I don't believe that package exists in the available repos (ie. may be valid for another distro of Linux, but does not appear to be available in the yum repos -- running yum search 'r-cran-rjava' returned no results 2)\n\nInstead, from a prompt, install R + the necessary development files for later installation of R packages:\n\n$ sudo yum install -y R-java-devel.x86_64\n\nAnd finally, install the necessary XML libraries to support the XML package in R:\n\n$ sudo yum install -y libxml2-devel 3\n\nAfter which you can then open R (either as root user...)\n\n$ sudo R\n\nor personal\/local user\n\n$ R\n\nand execute the package installation:\n\n> install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\")\n\nEDITED TO FIX RJAVA PACKAGE INSTALLATION\n\nIt looks like the installation is requiring libgomp.spec\/libgomp.a files, so you can first find that file:\n\n$ sudo find \/ -iname libgomp.spec\n\nwhich should be located at \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec -- if so, you can manually create symlinks to fix this:\n\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec \/usr\/lib64\/\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.a \/usr\/lib64\/\n\n\nIf that ran correctly, you should now see both files in \/usr\/lib64 path:\n\n$ ls \/usr\/lib64\/libgomp*\n\nOnce confirmed, you can run the install.package('rJava') command.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE-0c9SwxRViGhd-lAYtsuw\/is-there-a-way-to-install-r-libraries-in-sage-maker-that-receive-a-non-zero-exit-status",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-06-01T02:52:15.000Z",
                "Answer_score":0,
                "Answer_body":"For Amazon SageMaker notebook Instances, you have the ability to assume root privileges, so instead of:\n\n$ yum install r-cran-rjava\n\nyou can try:\n\n$ sudo yum install r-cran-rjava\n\nwhich will allow you to impersonate the superuser (ie. root) for that command 1\n\nBut I don't believe that package exists in the available repos (ie. may be valid for another distro of Linux, but does not appear to be available in the yum repos -- running yum search 'r-cran-rjava' returned no results 2)\n\nInstead, from a prompt, install R + the necessary development files for later installation of R packages:\n\n$ sudo yum install -y R-java-devel.x86_64\n\nAnd finally, install the necessary XML libraries to support the XML package in R:\n\n$ sudo yum install -y libxml2-devel 3\n\nAfter which you can then open R (either as root user...)\n\n$ sudo R\n\nor personal\/local user\n\n$ R\n\nand execute the package installation:\n\n> install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\")\n\nEDITED TO FIX RJAVA PACKAGE INSTALLATION\n\nIt looks like the installation is requiring libgomp.spec\/libgomp.a files, so you can first find that file:\n\n$ sudo find \/ -iname libgomp.spec\n\nwhich should be located at \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec -- if so, you can manually create symlinks to fix this:\n\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec \/usr\/lib64\/\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.a \/usr\/lib64\/\n\n\nIf that ran correctly, you should now see both files in \/usr\/lib64 path:\n\n$ ls \/usr\/lib64\/libgomp*\n\nOnce confirmed, you can run the install.package('rJava') command.",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1527821535000,
        "Question_original_content":"wai instal librari receiv non zero exit statu have issu kernel jupyt notebook come differ librari result follow error warn messag instal packag xml repo http cran project org instal packag xml non zero exit statusupd html index packag librari make packag html xml second packag run issu rjava workaround work root access instal command line termin involv command yum instal cran rjava root access instal messag need root perform command workaround hasn possibl check document rjava xml run requir jdk requir issu wasn reproduc local rstudio environ xml depend multipl librari rjava wai instal packag",
        "Question_preprocessed_content":"wai instal librari receiv exit statu have issu notebook come differ librari result follow error warn messag repo instal packag xml exit statusupd html index packag make xml second packag run issu rjava workaround work root access instal command line termin involv command yum instal root access instal messag need root perform workaround hasn possibl check document rjava xml run requir jdk requir issu wasn reproduc local rstudio environ xml depend multipl librari wai instal packag",
        "Question_gpt_summary_original":"The user is facing challenges in installing R libraries, specifically XML and rJava, in SageMaker due to non-zero exit status errors. The user has tried a workaround that involves installing via the command line in a terminal, but cannot do so as they do not have root access. The issue was not reproducible on a local RStudio environment, and the user is running the requirements for JDK and other system requirements in SageMaker. The user is seeking a solution to install these packages without root access.",
        "Question_gpt_summary":"user face challeng instal librari specif xml rjava non zero exit statu error user tri workaround involv instal command line termin root access issu reproduc local rstudio environ user run requir jdk requir user seek solut instal packag root access",
        "Answer_original_content":"notebook instanc abil assum root privileg instead yum instal cran rjava try sudo yum instal cran rjava allow imperson superus root command believ packag exist avail repo valid distro linux appear avail yum repo run yum search cran rjava return result instead prompt instal necessari develop file later instal packag sudo yum instal java devel final instal necessari xml librari support xml packag sudo yum instal libxml devel open root user sudo person local user execut packag instal instal packag xml repo http cran project org edit fix rjava packag instal look like instal requir libgomp spec libgomp file file sudo inam libgomp spec locat usr lib gcc amazon linux libgomp spec manual creat symlink fix sudo usr lib gcc amazon linux libgomp spec usr lib sudo usr lib gcc amazon linux libgomp usr lib ran correctli file usr lib path usr lib libgomp confirm run instal packag rjava command",
        "Answer_preprocessed_content":"notebook instanc abil assum root privileg instead yum instal try sudo yum instal allow imperson superus command believ packag exist avail repo instead prompt instal necessari develop file later instal packag sudo yum instal final instal necessari xml librari support xml packag sudo yum instal open sudo user execut packag instal repo edit fix rjava packag instal look like instal requir file file sudo inam locat manual creat symlink fix sudo sudo ran correctli file path confirm run command",
        "Answer_gpt_summary_original":"The solution suggested in the discussion involves assuming root privileges by using the command \"sudo yum install\" instead of \"yum install\" to install the necessary packages. If the required package is not available in the available repositories, the user can install R and the necessary development files, followed by installing the necessary XML libraries to support the XML package in R. The user can then open R and execute the package installation. Additionally, the user can manually create symlinks to fix the issue of missing libgomp.spec\/libgomp.a files and then run the install.package('rJava') command.",
        "Answer_gpt_summary":"solut suggest discuss involv assum root privileg command sudo yum instal instead yum instal instal necessari packag requir packag avail avail repositori user instal necessari develop file follow instal necessari xml librari support xml packag user open execut packag instal addition user manual creat symlink fix issu miss libgomp spec libgomp file run instal packag rjava command"
    },
    {
        "Question_title":"Python in AzureML fail to pass dataframe without changes",
        "Question_body":"<p>when trying to pass data without doing anything in python, getting this error:<\/p>\n\n<pre><code>Error 0085: The following error occurred during script evaluation, please view the output log for more information:\n---------- Start of error message from Python interpreter ----------\nCaught exception while executing function: Traceback (most recent call last):\n  File \"C:\\server\\invokepy.py\", line 175, in batch\n    rutils.RUtils.DataFrameToRFile(outlist[i], outfiles[i])\n  File \"C:\\server\\RReader\\rutils.py\", line 28, in DataFrameToRFile\n    rwriter.write_attribute_list(attributes)\n  File \"C:\\server\\RReader\\rwriter.py\", line 59, in write_attribute_list\n    self.write_object(value);\n  File \"C:\\server\\RReader\\rwriter.py\", line 121, in write_object\n    write_function(flags, value.values())\n  File \"C:\\server\\RReader\\rwriter.py\", line 104, in write_objects\n    self.write_object(value)\n  File \"C:\\server\\RReader\\rwriter.py\", line 121, in write_object\n    write_function(flags, value.values())\n  File \"C:\\server\\RReader\\rwriter.py\", line 71, in write_integers\n    self.write_integer(value)\n  File \"C:\\server\\RReader\\rwriter.py\", line 147, in write_integer\n    self.writer.WriteInt32(value)\n  File \"C:\\server\\RReader\\BinaryIO\\binarywriter.py\", line 26, in WriteInt32\n    self.WriteData(self.Int32Format, data)\n  File \"C:\\server\\RReader\\BinaryIO\\binarywriter.py\", line 14, in WriteData\n    self.stream.write(pack(format, data))\nerror: cannot convert argument to integer\n\n---------- End of error message from Python  interpreter  ----------\nStart time: UTC 05\/26\/2016 13:16:01\nEnd time: UTC 05\/26\/2016 13:16:13\n<\/code><\/pre>\n\n<p>here is the data i'm trying to pass:\n<a href=\"https:\/\/i.stack.imgur.com\/ysG36.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ysG36.png\" alt=\"the data\"><\/a><\/p>\n\n<p>here is the experiment:\n<a href=\"https:\/\/i.stack.imgur.com\/vgdSn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vgdSn.png\" alt=\"the experiment\"><\/a><\/p>\n\n<p>and the python code:\n<a href=\"https:\/\/i.stack.imgur.com\/LRSAE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LRSAE.png\" alt=\"python code\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1464269301867,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":739.0,
        "Answer_body":"<p>after talking to Microsoft support, the problem was that the \"Execute Python Script\" module cannot return empty values.\nthis can be solved by adding a \"Clean Missing Data\" module before reading it from python:\n<a href=\"https:\/\/i.stack.imgur.com\/BzCUZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BzCUZ.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37462268",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1468139774183,
        "Question_original_content":"python fail pass datafram chang try pass data python get error error follow error occur script evalu view output log inform start error messag python interpret caught except execut function traceback recent file server invokepi line batch rutil rutil dataframetorfil outlist outfil file server rreader rutil line dataframetorfil rwriter write attribut list attribut file server rreader rwriter line write attribut list self write object valu file server rreader rwriter line write object write function flag valu valu file server rreader rwriter line write object self write object valu file server rreader rwriter line write object write function flag valu valu file server rreader rwriter line write integ self write integ valu file server rreader rwriter line write integ self writer writeint valu file server rreader binaryio binarywrit line writeint self writedata self intformat data file server rreader binaryio binarywrit line writedata self stream write pack format data error convert argument integ end error messag python interpret start time utc end time utc data try pass experi python code",
        "Question_preprocessed_content":"python fail pass datafram chang try pass data python get error data try pass experi python code",
        "Question_gpt_summary_original":"The user encountered an error when trying to pass data without making any changes in Python in AzureML. The error message suggests that there was an issue with converting an argument to an integer. The user provided a screenshot of the data and experiment, as well as the Python code used.",
        "Question_gpt_summary":"user encount error try pass data make chang python error messag suggest issu convert argument integ user provid screenshot data experi python code",
        "Answer_original_content":"talk microsoft support problem execut python script modul return valu solv ad clean miss data modul read python",
        "Answer_preprocessed_content":"talk microsoft support problem execut python script modul return valu solv ad clean miss data modul read python",
        "Answer_gpt_summary_original":"Solution: The issue was that the \"Execute Python Script\" module cannot return empty values. This can be solved by adding a \"Clean Missing Data\" module before reading it from Python.",
        "Answer_gpt_summary":"solut issu execut python script modul return valu solv ad clean miss data modul read python"
    },
    {
        "Question_title":"Help with Inference Script for Amazon Sagemaker Neo Compiled Models",
        "Question_body":"Hello everyone, I was trying to execute the example mentioned in the docs - https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_neo_compilation_jobs\/pytorch_torchvision\/pytorch_torchvision_neo.html. I was able to successfully run this example but as soon as I changed the target_device to jetson_tx2, after which I ran the entire script again, keeping the rest of the code as it is, the model stopped working. I was not getting any inferences from the deployed model and it always errors out with the message:\n\nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from <users-sagemaker-endpoint> with message \"Your invocation timed out while waiting for a response from container model. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\"                \n\n\nAccording to the troubleshoot docs https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo-troubleshooting-inference.html, this seems to be an issue of model_fn() function. The inference script used by this example is mentioned here https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_neo_compilation_jobs\/pytorch_torchvision\/code\/resnet18.py , which itself doesn't contain any model_fn() definition but it still worked for target device ml_c5. So could anyone please help me with the following questions:\n\nWhat changes does SageMaker Neo do to the model depending on target_device type? Since it seems the same model is loaded in a different way for different target device.\nIs there any way to determine how the model is expected to load for a certain target_device type so that I could define the model_fn() function myself in the same inference script mentioned above?\nAt-last, can anyone please help with the inference script for this very same model as mentioned in docs above which works for jetson_tx2 device as well.\n\nAny suggestions or links on how to resolve this issue would be really helpful.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1668430718144,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":32.0,
        "Answer_body":"As you mentioned, you changed the Neo compiling target from ml_c5 to jetson_tx2, the compiled model will require runtime from jetson_tx2. If you kept other code unchanged, the model will be deployed to a ml.c5.9xlarge EC2 instance, which doesn't provide Nvida Jeston.\n\nThe model can't be loaded and will error out since Jestion is a device Nvidia GPU structure while c5 is only equipped with CPU. No CUDA environment.\n\nIf you compile the model with jeston_tx2 as target, you should download the model and run the compiled model in a real Nvidia Jeston device.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJvbkzp91TGSZO1GwW-r90w\/help-with-inference-script-for-amazon-sagemaker-neo-compiled-models",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-18T10:08:12.280Z",
                "Answer_score":1,
                "Answer_body":"As you mentioned, you changed the Neo compiling target from ml_c5 to jetson_tx2, the compiled model will require runtime from jetson_tx2. If you kept other code unchanged, the model will be deployed to a ml.c5.9xlarge EC2 instance, which doesn't provide Nvida Jeston.\n\nThe model can't be loaded and will error out since Jestion is a device Nvidia GPU structure while c5 is only equipped with CPU. No CUDA environment.\n\nIf you compile the model with jeston_tx2 as target, you should download the model and run the compiled model in a real Nvidia Jeston device.",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1668766092280,
        "Question_original_content":"help infer script neo compil model hello try execut exampl mention doc http exampl readthedoc latest neo compil job pytorch torchvis pytorch torchvis neo html abl successfulli run exampl soon chang target devic jetson ran entir script keep rest code model stop work get infer deploi model error messag error occur modelerror call invokeendpoint oper receiv server error messag invoc time wait respons contain model review latenc metric contain amazon cloudwatch resolv issu try accord troubleshoot doc http doc aw amazon com latest neo troubleshoot infer html issu model function infer script exampl mention http github com aw amazon exampl blob main neo compil job pytorch torchvis code resnet contain model definit work target devic help follow question chang neo model depend target devic type model load differ wai differ target devic wai determin model expect load certain target devic type defin model function infer script mention help infer script model mention doc work jetson devic suggest link resolv issu help",
        "Question_preprocessed_content":"help infer script neo compil model hello try execut exampl mention doc abl successfulli run exampl soon chang ran entir script keep rest code model stop work get infer deploi model error messag error occur call invokeendpoint oper receiv server error messag invoc time wait respons contain model review latenc metric contain amazon cloudwatch resolv issu try accord troubleshoot doc issu function infer script exampl mention contain definit work target devic help follow question chang neo model depend type model load differ wai differ target devic wai determin model expect load certain type defin function infer script mention help infer script model mention doc work devic suggest link resolv issu help",
        "Question_gpt_summary_original":"The user encountered challenges while trying to execute an example in Amazon Sagemaker Neo Compiled Models. After changing the target_device to jetson_tx2, the model stopped working and the user was not getting any inferences from the deployed model. The troubleshoot docs suggest that this could be an issue with the model_fn() function. The user is seeking help with understanding the changes that SageMaker Neo does to the model depending on the target_device type, determining how the model is expected to load for a certain target_device type, and help with the inference script for the same model that works for jetson_tx2 device.",
        "Question_gpt_summary":"user encount challeng try execut exampl neo compil model chang target devic jetson model stop work user get infer deploi model troubleshoot doc suggest issu model function user seek help understand chang neo model depend target devic type determin model expect load certain target devic type help infer script model work jetson devic",
        "Answer_original_content":"mention chang neo compil target jetson compil model requir runtim jetson kept code unchang model deploi xlarg instanc provid nvida jeston model load error jestion devic nvidia gpu structur equip cpu cuda environ compil model jeston target download model run compil model real nvidia jeston devic",
        "Answer_preprocessed_content":"mention chang neo compil target compil model requir runtim kept code unchang model deploi instanc provid nvida jeston model load error jestion devic nvidia gpu structur equip cpu cuda environ compil model target download model run compil model real nvidia jeston devic",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to compile the model with the target_device set to jetson_tx2 and then download the compiled model to run it on a real Nvidia Jetson device. It is also mentioned that if the model is compiled with a target_device that is not available on the deployment instance, the model will not work and will error out. No other solutions are provided in the discussion.",
        "Answer_gpt_summary":"solut solut suggest discuss compil model target devic set jetson download compil model run real nvidia jetson devic mention model compil target devic avail deploy instanc model work error solut provid discuss"
    },
    {
        "Question_title":"Dvc exp show: experiment not showing \/ wrong position",
        "Question_body":"<p>Hi there,<br>\nI am trying out dvc for a few days now and have a question regarding dvc exp show. My setup is:<\/p>\n<ul>\n<li>Have a git &lt;branch 1&gt; with a head commit &lt;commit 1&gt;<\/li>\n<li>Run an experiment on a remote machine (checkout &lt;commit 1&gt;, dvc pull, dvc exp run)<\/li>\n<li>Create a new branch (dvc exp branch) which creates &lt;branch 2&gt; and &lt;commit 2&gt; in it<\/li>\n<li>I push the experiment (dvc exp push)<\/li>\n<li>On my local machine, I fetch the experiment (dvc exp pull)<br>\n(* In the mean time, I have more commits on &lt;branch 1&gt;)<\/li>\n<\/ul>\n<p>Then, dvc exp show behaves unexpectedly:<\/p>\n<ul>\n<li>If I run dvc exp show -a, it shows &lt;branch 1&gt; and &lt;branch 2&gt;, but no experiment<\/li>\n<li>If I run dvc exp show -A, it shows the experiment under &lt;commit 1&gt;<\/li>\n<\/ul>\n<p>Expected behavior:<\/p>\n<ul>\n<li>If I run dvc exp show -a, it shows the experiment under &lt;branch 2&gt;<\/li>\n<\/ul>\n<p>Hope I got this across. Am I doing something wrong here?<\/p>",
        "Question_answer_count":14,
        "Question_comment_count":0,
        "Question_creation_time":1641551077155,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":504.0,
        "Answer_body":"<p>Hi again,<\/p>\n<p>so after stepping through the dvc code to see what is happening, I realized I must have misunderstood the concept of experiments.<\/p>\n<p>What I would like to have is an overview of all my experiments, grouped by the branches they are in and sorted by metrics, parameters or whatever. Apparently, dvc exp show is not the right tool to do so. Does someone have a suggestion how I can achieve what I want?<\/p>\n<p>I am also very open to suggestions on how a better workflow would look like. My \u201cspecial requirement\u201d is that I need to run experiements on another machine inside a docker container (Which I tried to do by doing inside the container: git clone, dvc exp run, [dvc exp branch], dvc exp push, git push).<br>\nThis requirement brings another requirement, i.e. that all experiments must be pushed to git automatically after experiment run and I need to somehow remove unwanted experiments later.<\/p>\n<p>I tried to solve this by having each experiment create its own branch, then either merging the branch into my main or feature branch afterwards or deleting the branch. When I could then exp show all experiments for some branch, I\u2019d be fine. But this does not seem to be the dvc way.<\/p>. <p>Hi <a class=\"mention\" href=\"\/u\/gugar\">@gugar<\/a> ,<\/p>\n<p>And what does plain <code>dvc exp show<\/code> show if you <code>git checkout<\/code> to <code>branch 2<\/code>?<\/p>. <p>DVC experiments are always associated with parent commits (and not branches). <code>dvc exp show -a\/--all-branches<\/code> shows your current branch heads, and experiments associated with those branch head commits. So in your scenario, your experiment will always be associated with <code>commit 1<\/code>, and not with <code>branch 1<\/code> (unless the tip of <code>branch 1<\/code> is currently pointing to <code>commit 1<\/code>).<\/p>\n<p>Using <code>dvc exp branch<\/code> just creates a new git branch that contains the contents of the experiment. It does not move\/re-associate the experiment itself to be under the new git branch. If you are using <code>dvc exp branch<\/code>, the expectation is that afterwards you will just use that git branch (instead of using the experiment ref).<\/p>\n<p>This is why you get:<\/p>\n<blockquote>\n<ul>\n<li>If I run dvc exp show -a, it shows &lt;branch 1&gt; and &lt;branch 2&gt;, but no experiment<\/li>\n<li>If I run dvc exp show -A, it shows the experiment under &lt;commit 1&gt;<\/li>\n<\/ul>\n<\/blockquote>\n<p><code>branch 1<\/code>'s HEAD has moved due to <code>In the mean time, I have more commits on &lt;branch 1&gt;<\/code>, so <code>exp show -a<\/code> does not include <code>commit 1<\/code> in the table. <code>branch 2<\/code> is an entire new git branch, and is no longer considered to be a child experiment.<\/p>. <p>Hi <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a>, <a class=\"mention\" href=\"\/u\/pmrowla\">@pmrowla<\/a>,<br>\nThanks for your replies. <a class=\"mention\" href=\"\/u\/pmrowla\">@pmrowla<\/a> thanks for your detailed explanation, which would be great to have in the dvc documentation! <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a> it only shows <code>workspace<\/code> and <code>branch 2<\/code>, but no experiments under <code>branch 2<\/code> which seems to be the intended behavior after reading <a class=\"mention\" href=\"\/u\/pmrowla\">@pmrowla<\/a> 's answer.<\/p>\n<p>Now I see that my approach is not supported by dvc, or at least not as straightforward as I was thinking. To get to a solution of the issue: Should I change the way I organize experiments and if so, how? Or is my approach ok but I am not using the dvc commands correctly?<\/p>. <p>Do you actually want to sort or group by branches? Could you provide more info about how you want to group the experiments? Like you suggested, it seems like there is simply a misunderstanding or disconnect about expected workflows.<\/p>\n<p>Some possible workflows depending on what you want:<\/p>\n<ul>\n<li>Leave your setup as is. Once you do <code>dvc exp branch<\/code>, the result in <code>dvc exp show -a<\/code> for branch 2 is the exact same as the experiment you see under commit 1. You could create as many branches as you want like this, but it could get messy to manage that many branches.<\/li>\n<li>Remove the <code>dvc exp branch<\/code> step. You can push experiments as is (and pull them locally) and see them all with <code>dvc exp show<\/code> without needing to create new branches or commits. This is a more typical workflow, in which case you could later use <code>dvc exp branch<\/code> (or <code>dvc exp apply<\/code>) to bring the best experiment into your regular Git workflow.<\/li>\n<li>Checkout a different branch before running the experiment so that the experiment is based on the tip of that branch. This could be useful if you actually need to group bunches of experiments by branches.<\/li>\n<\/ul>. <p>Hi <a class=\"mention\" href=\"\/u\/dberenbaum\">@dberenbaum<\/a>,<br>\nthanks for your answer! I tried all your suggestions, but still experiments do not show up in dvc exp show. I can only see my HEAD commit there but without experiment name. When I use -A I can see all commits with their results but then it is pretty messy and I cannot sort by any of the metrics.<\/p>\n<p>After trying out a  lot of stuff, I think my major misconception is that dvc experiments are meant to be multiple experiments per git commit and sorting only those. Is that correct? My use case is that I have at max 1 experiment per git commit and I want to sort them e.g. per branch.<\/p>\n<p>So I could of course write my own dvc exp show replacement, but I am asking myself: Am I using dvc wrong?<\/p>. <blockquote>\n<p>I think my major misconception is that dvc experiments are meant to be multiple experiments per git commit and sorting only those.<\/p>\n<\/blockquote>\n<p>Right, makes sense. Now, the table is only sorting within a commit, but you want to sort between all commits or branches, right? Please feel free to add a feature request to <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/new\" class=\"inline-onebox\">Sign in to GitHub \u00b7 GitHub<\/a>, or I can copy the relevant points there if you would prefer.<\/p>\n<blockquote>\n<p>My use case is that I have at max 1 experiment per git commit and I want to sort them e.g. per branch.<\/p>\n<\/blockquote>\n<p>Do you mind explaining why you want to have 1 experiment per commit? What differs between each commit? This sort of defeats the point of dvc experiments, which is to keep track of those differences for you so that you don\u2019t need so many commits\/branches.<\/p>\n<p>If you have long gaps with lots of commits between every experiment, and you don\u2019t think you have any use for running multiple experiments based on a single commit, then I think your workflow is fine, although you could probably accomplish the same without the <code>dvc exp<\/code> commands (other than <code>dvc exp show<\/code>).<\/p>. <blockquote>\n<p>Do you mind explaining why you want to have 1 experiment per commit? What differs between each commit? This sort of defeats the point of dvc experiments, which is to keep track of those differences for you so that you don\u2019t need so many commits\/branches.<\/p>\n<\/blockquote>\n<p>I am doing AI research and mostly the difference between experiments is in code. So I want to change code, then of course commit with git and run an experiment on top of it. Of course, sometimes I also just change parameters, where experiments would come in handy. But (perhaps that\u2019s a personal thing) I prefer to have a completely defined state within git when running an experiment, therefore, I would rather update and commit my params.yaml file than running an experiment with parameters in the arguments.<\/p>\n<blockquote>\n<p>[\u2026] although you could probably accomplish the same without the <code>dvc exp<\/code> commands [\u2026]<\/p>\n<\/blockquote>\n<p>Yes, that\u2019s what I am thinking as well. Just use <code>dvc repro<\/code>. TBH, the <code>exp<\/code> stuff of dvc is adding a whole bunch of complexity to the workflow for a (at least for me) quite limited benefit. I think working with experiment names and hashes in addition to all the git stuff introduces many new ways to make errors.<\/p>\n<p>Btw, so I don\u2019t just grumble here: I love all the rest of DVC so far <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=11\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> Great tool!<\/p>. <blockquote>\n<p>I am doing AI research and mostly the difference between experiments is in code. So I want to change code, then of course commit with git and run an experiment on top of it.<\/p>\n<\/blockquote>\n<p>One thing to note here would be that DVC experiments will include uncommitted code changes (they aren\u2019t meant to only support parameter changes). So rather than \u201ccommit with git and then run an experiment on top of it\u201d, you could just make your code changes and then do <code>dvc exp run<\/code> (without modified parameters and without <code>git commit<\/code>'ing anything), to see what you would get before committing those code changes (with the results and those code changes stored in the DVC experiment).<\/p>\n<p>You can also retrieve your changes from an experiment using <code>dvc exp apply<\/code>. So you can do things like:<\/p>\n<pre><code class=\"lang-auto\"># edit some code\n$ vim src\/file.py\n$ dvc exp run -n exp-a\n# edit code again (to test some alternate version of the changes)\n$ vim src\/file.py\n$ dvc exp run -n exp-b\n# compare the results of code changes in A vs code changes in B\n$ dvc exp show\n# decide that I want to keep the changes+results from A and discard everything else\n$ git reset --hard\n$ dvc exp apply exp-a\n$ git add .\n$ git commit -m 'this commit contains the code changes and result of A'\n<\/code><\/pre>. <p>I see, thank you for pointing that out. However, because I have to run my experiments on specific remote machines, this workflow will not work for me. I have to commit and push any changes so that the remote machine can pick up the desired state and run from there.<br>\nAlso, by doing so, I will use a mix of git and dvc for code version control. This feels very error prone to me.<\/p>. <blockquote>\n<p>Also, by doing so, I will use a mix of git and dvc for code version control. This feels very error prone to me.<\/p>\n<\/blockquote>\n<p>Your explanation about why this workflow doesn\u2019t fit your use case makes sense, but just to clarify this, internally DVC experiments are just git commits, so for all intents and purposes you would still only be using git for code version control.<\/p>\n<p>Named experiments are just git refs (like branches or tags) stored in a separate <code>refs\/exps<\/code> namespace, and you can use experiment refs or SHAs in <code>git<\/code> commands like any other commit SHA or git ref. You can read more about the implementation here: <a href=\"https:\/\/dvc.org\/blog\/experiment-refs\">https:\/\/dvc.org\/blog\/experiment-refs<\/a><\/p>. <p>Hello <a class=\"mention\" href=\"\/u\/dberenbaum\">@dberenbaum<\/a> <a class=\"mention\" href=\"\/u\/pmrowla\">@pmrowla<\/a>,<\/p>\n<p>I have been experimenting with DVC (version <code>2.18.1<\/code>) the past week and am trying to establish a workflow for our ML experiments. I am trying to use a workflow where experiments are grouped in separate branches and I am using the data and project structure provided by this tutorial <a href=\"https:\/\/realpython.com\/python-data-version-control\/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Data Version Control With Python and DVC \u2013 Real Python<\/a>, I forked the repo, which you can see here: <a href=\"https:\/\/github.com\/ChristianP45\/real-python-dvc\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - ChristianP45\/real-python-dvc<\/a>.<\/p>\n<p>So for example, I would like to train a simple SGD classifier, so I create a branch called <code>sgd<\/code> so that I can group all SGD related experiments in that branch. I ran 2 experiments using a different number of iterations for each (i.e. baseline and 500-iters). It all looks good up until the point where I actually commit and push the changes  (FYI I installed the DVC git hooks)<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/5dec75dadf08d502acd83bee51e4031402794d9e.jpeg\" data-download-href=\"\/uploads\/short-url\/doT1a731hsdznHMD31TEI01Ey18.jpeg?dl=1\" title=\"dvc_debug\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/5dec75dadf08d502acd83bee51e4031402794d9e_2_619x500.jpeg\" alt=\"dvc_debug\" data-base62-sha1=\"doT1a731hsdznHMD31TEI01Ey18\" width=\"619\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/5dec75dadf08d502acd83bee51e4031402794d9e_2_619x500.jpeg, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/5dec75dadf08d502acd83bee51e4031402794d9e_2_928x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/5dec75dadf08d502acd83bee51e4031402794d9e_2_1238x1000.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/5dec75dadf08d502acd83bee51e4031402794d9e_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">dvc_debug<\/span><span class=\"informations\">1920\u00d71549 184 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>As you can see, the experiments then appear under <code>master<\/code> branch after commiting and pushing the changes for some reason? And I cannot figure out why. Is this expected? Am I missing something? It would be really helpful if guys could assist as I am running out of ideas.<\/p>\n<p>Thanks in advance <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>Cheers,<br>\nChris<\/p>. <p>Hi <a class=\"mention\" href=\"\/u\/chrisp45\">@chrisP45<\/a> . This is not expected, appears to be a bug in <code>dvc exp show -a<\/code>. I will take a look<\/p>. <blockquote>\n<p>As you can see, the experiments then appear under <code>master<\/code> branch after commiting and pushing the changes for some reason? And I cannot figure out why.<\/p>\n<\/blockquote>\n<p>I took a look.<br>\nThis only happens when <code>sgd<\/code> branch doesn\u2019t contain any commits so it points to the same commit as <code>master<\/code> (let\u2019s call it <code>master-commit-0<\/code>).<\/p>\n<p>When you run the experiments in the <code>sgd<\/code> branch, they are actually derived from the <code> master-commit-0<\/code>.<\/p>\n<p>So, when you run <code>git add<\/code> and <code>git commit<\/code>, you are introducing a new commit in <code>sgd<\/code> branch and that\u2019s why the default <code>exp show<\/code> doesn\u2019t show the experiments. By default, only shows experiments derived from the current commit (you could see the experiments if you ran <code>dvc exp show -n 2<\/code>).<\/p>\n<p>However, <code>dvc exp show -a<\/code> looks to all branches and it finds that <code>master<\/code> points to <code>master-commit-0<\/code>, which has 2 experiments derived.<\/p>\n<p>You can use the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/exp\/gc\" rel=\"noopener nofollow ugc\"><code>dvc exp gc -w<\/code><\/a> (or with other flags) to clean the dvc experiments.<\/p>\n<p>I think the problem here is that <code>git add<\/code> and <code>git commit<\/code> is used as an alternative to the commands we provide for <a href=\"https:\/\/dvc.org\/doc\/user-guide\/experiment-management\/persisting-experiments\" rel=\"noopener nofollow ugc\">persisting experiments<\/a> and it doesn\u2019t take care of cleaning experiments, causing the confusion.<\/p>\n<p><a class=\"mention\" href=\"\/u\/chrisp45\">@chrisP45<\/a> does this solve the question?<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-exp-show-experiment-not-showing-wrong-position\/1019",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-10T11:39:18.370Z",
                "Answer_body":"<p>Hi again,<\/p>\n<p>so after stepping through the dvc code to see what is happening, I realized I must have misunderstood the concept of experiments.<\/p>\n<p>What I would like to have is an overview of all my experiments, grouped by the branches they are in and sorted by metrics, parameters or whatever. Apparently, dvc exp show is not the right tool to do so. Does someone have a suggestion how I can achieve what I want?<\/p>\n<p>I am also very open to suggestions on how a better workflow would look like. My \u201cspecial requirement\u201d is that I need to run experiements on another machine inside a docker container (Which I tried to do by doing inside the container: git clone, dvc exp run, [dvc exp branch], dvc exp push, git push).<br>\nThis requirement brings another requirement, i.e. that all experiments must be pushed to git automatically after experiment run and I need to somehow remove unwanted experiments later.<\/p>\n<p>I tried to solve this by having each experiment create its own branch, then either merging the branch into my main or feature branch afterwards or deleting the branch. When I could then exp show all experiments for some branch, I\u2019d be fine. But this does not seem to be the dvc way.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-10T12:30:44.478Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/gugar\">@gugar<\/a> ,<\/p>\n<p>And what does plain <code>dvc exp show<\/code> show if you <code>git checkout<\/code> to <code>branch 2<\/code>?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-11T05:18:06.186Z",
                "Answer_body":"<p>DVC experiments are always associated with parent commits (and not branches). <code>dvc exp show -a\/--all-branches<\/code> shows your current branch heads, and experiments associated with those branch head commits. So in your scenario, your experiment will always be associated with <code>commit 1<\/code>, and not with <code>branch 1<\/code> (unless the tip of <code>branch 1<\/code> is currently pointing to <code>commit 1<\/code>).<\/p>\n<p>Using <code>dvc exp branch<\/code> just creates a new git branch that contains the contents of the experiment. It does not move\/re-associate the experiment itself to be under the new git branch. If you are using <code>dvc exp branch<\/code>, the expectation is that afterwards you will just use that git branch (instead of using the experiment ref).<\/p>\n<p>This is why you get:<\/p>\n<blockquote>\n<ul>\n<li>If I run dvc exp show -a, it shows &lt;branch 1&gt; and &lt;branch 2&gt;, but no experiment<\/li>\n<li>If I run dvc exp show -A, it shows the experiment under &lt;commit 1&gt;<\/li>\n<\/ul>\n<\/blockquote>\n<p><code>branch 1<\/code>'s HEAD has moved due to <code>In the mean time, I have more commits on &lt;branch 1&gt;<\/code>, so <code>exp show -a<\/code> does not include <code>commit 1<\/code> in the table. <code>branch 2<\/code> is an entire new git branch, and is no longer considered to be a child experiment.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-11T12:30:29.562Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a>, <a class=\"mention\" href=\"\/u\/pmrowla\">@pmrowla<\/a>,<br>\nThanks for your replies. <a class=\"mention\" href=\"\/u\/pmrowla\">@pmrowla<\/a> thanks for your detailed explanation, which would be great to have in the dvc documentation! <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a> it only shows <code>workspace<\/code> and <code>branch 2<\/code>, but no experiments under <code>branch 2<\/code> which seems to be the intended behavior after reading <a class=\"mention\" href=\"\/u\/pmrowla\">@pmrowla<\/a> 's answer.<\/p>\n<p>Now I see that my approach is not supported by dvc, or at least not as straightforward as I was thinking. To get to a solution of the issue: Should I change the way I organize experiments and if so, how? Or is my approach ok but I am not using the dvc commands correctly?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-11T15:27:31.752Z",
                "Answer_body":"<p>Do you actually want to sort or group by branches? Could you provide more info about how you want to group the experiments? Like you suggested, it seems like there is simply a misunderstanding or disconnect about expected workflows.<\/p>\n<p>Some possible workflows depending on what you want:<\/p>\n<ul>\n<li>Leave your setup as is. Once you do <code>dvc exp branch<\/code>, the result in <code>dvc exp show -a<\/code> for branch 2 is the exact same as the experiment you see under commit 1. You could create as many branches as you want like this, but it could get messy to manage that many branches.<\/li>\n<li>Remove the <code>dvc exp branch<\/code> step. You can push experiments as is (and pull them locally) and see them all with <code>dvc exp show<\/code> without needing to create new branches or commits. This is a more typical workflow, in which case you could later use <code>dvc exp branch<\/code> (or <code>dvc exp apply<\/code>) to bring the best experiment into your regular Git workflow.<\/li>\n<li>Checkout a different branch before running the experiment so that the experiment is based on the tip of that branch. This could be useful if you actually need to group bunches of experiments by branches.<\/li>\n<\/ul>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-12T16:09:40.034Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dberenbaum\">@dberenbaum<\/a>,<br>\nthanks for your answer! I tried all your suggestions, but still experiments do not show up in dvc exp show. I can only see my HEAD commit there but without experiment name. When I use -A I can see all commits with their results but then it is pretty messy and I cannot sort by any of the metrics.<\/p>\n<p>After trying out a  lot of stuff, I think my major misconception is that dvc experiments are meant to be multiple experiments per git commit and sorting only those. Is that correct? My use case is that I have at max 1 experiment per git commit and I want to sort them e.g. per branch.<\/p>\n<p>So I could of course write my own dvc exp show replacement, but I am asking myself: Am I using dvc wrong?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-12T17:31:56.116Z",
                "Answer_body":"<blockquote>\n<p>I think my major misconception is that dvc experiments are meant to be multiple experiments per git commit and sorting only those.<\/p>\n<\/blockquote>\n<p>Right, makes sense. Now, the table is only sorting within a commit, but you want to sort between all commits or branches, right? Please feel free to add a feature request to <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/new\" class=\"inline-onebox\">Sign in to GitHub \u00b7 GitHub<\/a>, or I can copy the relevant points there if you would prefer.<\/p>\n<blockquote>\n<p>My use case is that I have at max 1 experiment per git commit and I want to sort them e.g. per branch.<\/p>\n<\/blockquote>\n<p>Do you mind explaining why you want to have 1 experiment per commit? What differs between each commit? This sort of defeats the point of dvc experiments, which is to keep track of those differences for you so that you don\u2019t need so many commits\/branches.<\/p>\n<p>If you have long gaps with lots of commits between every experiment, and you don\u2019t think you have any use for running multiple experiments based on a single commit, then I think your workflow is fine, although you could probably accomplish the same without the <code>dvc exp<\/code> commands (other than <code>dvc exp show<\/code>).<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-13T07:33:30.726Z",
                "Answer_body":"<blockquote>\n<p>Do you mind explaining why you want to have 1 experiment per commit? What differs between each commit? This sort of defeats the point of dvc experiments, which is to keep track of those differences for you so that you don\u2019t need so many commits\/branches.<\/p>\n<\/blockquote>\n<p>I am doing AI research and mostly the difference between experiments is in code. So I want to change code, then of course commit with git and run an experiment on top of it. Of course, sometimes I also just change parameters, where experiments would come in handy. But (perhaps that\u2019s a personal thing) I prefer to have a completely defined state within git when running an experiment, therefore, I would rather update and commit my params.yaml file than running an experiment with parameters in the arguments.<\/p>\n<blockquote>\n<p>[\u2026] although you could probably accomplish the same without the <code>dvc exp<\/code> commands [\u2026]<\/p>\n<\/blockquote>\n<p>Yes, that\u2019s what I am thinking as well. Just use <code>dvc repro<\/code>. TBH, the <code>exp<\/code> stuff of dvc is adding a whole bunch of complexity to the workflow for a (at least for me) quite limited benefit. I think working with experiment names and hashes in addition to all the git stuff introduces many new ways to make errors.<\/p>\n<p>Btw, so I don\u2019t just grumble here: I love all the rest of DVC so far <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=11\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> Great tool!<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-13T07:49:28.324Z",
                "Answer_body":"<blockquote>\n<p>I am doing AI research and mostly the difference between experiments is in code. So I want to change code, then of course commit with git and run an experiment on top of it.<\/p>\n<\/blockquote>\n<p>One thing to note here would be that DVC experiments will include uncommitted code changes (they aren\u2019t meant to only support parameter changes). So rather than \u201ccommit with git and then run an experiment on top of it\u201d, you could just make your code changes and then do <code>dvc exp run<\/code> (without modified parameters and without <code>git commit<\/code>'ing anything), to see what you would get before committing those code changes (with the results and those code changes stored in the DVC experiment).<\/p>\n<p>You can also retrieve your changes from an experiment using <code>dvc exp apply<\/code>. So you can do things like:<\/p>\n<pre><code class=\"lang-auto\"># edit some code\n$ vim src\/file.py\n$ dvc exp run -n exp-a\n# edit code again (to test some alternate version of the changes)\n$ vim src\/file.py\n$ dvc exp run -n exp-b\n# compare the results of code changes in A vs code changes in B\n$ dvc exp show\n# decide that I want to keep the changes+results from A and discard everything else\n$ git reset --hard\n$ dvc exp apply exp-a\n$ git add .\n$ git commit -m 'this commit contains the code changes and result of A'\n<\/code><\/pre>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-13T10:11:01.833Z",
                "Answer_body":"<p>I see, thank you for pointing that out. However, because I have to run my experiments on specific remote machines, this workflow will not work for me. I have to commit and push any changes so that the remote machine can pick up the desired state and run from there.<br>\nAlso, by doing so, I will use a mix of git and dvc for code version control. This feels very error prone to me.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-13T10:19:57.733Z",
                "Answer_body":"<blockquote>\n<p>Also, by doing so, I will use a mix of git and dvc for code version control. This feels very error prone to me.<\/p>\n<\/blockquote>\n<p>Your explanation about why this workflow doesn\u2019t fit your use case makes sense, but just to clarify this, internally DVC experiments are just git commits, so for all intents and purposes you would still only be using git for code version control.<\/p>\n<p>Named experiments are just git refs (like branches or tags) stored in a separate <code>refs\/exps<\/code> namespace, and you can use experiment refs or SHAs in <code>git<\/code> commands like any other commit SHA or git ref. You can read more about the implementation here: <a href=\"https:\/\/dvc.org\/blog\/experiment-refs\">https:\/\/dvc.org\/blog\/experiment-refs<\/a><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-17T14:03:21.538Z",
                "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/dberenbaum\">@dberenbaum<\/a> <a class=\"mention\" href=\"\/u\/pmrowla\">@pmrowla<\/a>,<\/p>\n<p>I have been experimenting with DVC (version <code>2.18.1<\/code>) the past week and am trying to establish a workflow for our ML experiments. I am trying to use a workflow where experiments are grouped in separate branches and I am using the data and project structure provided by this tutorial <a href=\"https:\/\/realpython.com\/python-data-version-control\/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Data Version Control With Python and DVC \u2013 Real Python<\/a>, I forked the repo, which you can see here: <a href=\"https:\/\/github.com\/ChristianP45\/real-python-dvc\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - ChristianP45\/real-python-dvc<\/a>.<\/p>\n<p>So for example, I would like to train a simple SGD classifier, so I create a branch called <code>sgd<\/code> so that I can group all SGD related experiments in that branch. I ran 2 experiments using a different number of iterations for each (i.e. baseline and 500-iters). It all looks good up until the point where I actually commit and push the changes  (FYI I installed the DVC git hooks)<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/5dec75dadf08d502acd83bee51e4031402794d9e.jpeg\" data-download-href=\"\/uploads\/short-url\/doT1a731hsdznHMD31TEI01Ey18.jpeg?dl=1\" title=\"dvc_debug\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/5dec75dadf08d502acd83bee51e4031402794d9e_2_619x500.jpeg\" alt=\"dvc_debug\" data-base62-sha1=\"doT1a731hsdznHMD31TEI01Ey18\" width=\"619\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/5dec75dadf08d502acd83bee51e4031402794d9e_2_619x500.jpeg, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/5dec75dadf08d502acd83bee51e4031402794d9e_2_928x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/5dec75dadf08d502acd83bee51e4031402794d9e_2_1238x1000.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/5dec75dadf08d502acd83bee51e4031402794d9e_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">dvc_debug<\/span><span class=\"informations\">1920\u00d71549 184 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>As you can see, the experiments then appear under <code>master<\/code> branch after commiting and pushing the changes for some reason? And I cannot figure out why. Is this expected? Am I missing something? It would be really helpful if guys could assist as I am running out of ideas.<\/p>\n<p>Thanks in advance <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>Cheers,<br>\nChris<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-17T14:31:12.409Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/chrisp45\">@chrisP45<\/a> . This is not expected, appears to be a bug in <code>dvc exp show -a<\/code>. I will take a look<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-17T15:00:23.980Z",
                "Answer_body":"<blockquote>\n<p>As you can see, the experiments then appear under <code>master<\/code> branch after commiting and pushing the changes for some reason? And I cannot figure out why.<\/p>\n<\/blockquote>\n<p>I took a look.<br>\nThis only happens when <code>sgd<\/code> branch doesn\u2019t contain any commits so it points to the same commit as <code>master<\/code> (let\u2019s call it <code>master-commit-0<\/code>).<\/p>\n<p>When you run the experiments in the <code>sgd<\/code> branch, they are actually derived from the <code> master-commit-0<\/code>.<\/p>\n<p>So, when you run <code>git add<\/code> and <code>git commit<\/code>, you are introducing a new commit in <code>sgd<\/code> branch and that\u2019s why the default <code>exp show<\/code> doesn\u2019t show the experiments. By default, only shows experiments derived from the current commit (you could see the experiments if you ran <code>dvc exp show -n 2<\/code>).<\/p>\n<p>However, <code>dvc exp show -a<\/code> looks to all branches and it finds that <code>master<\/code> points to <code>master-commit-0<\/code>, which has 2 experiments derived.<\/p>\n<p>You can use the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/exp\/gc\" rel=\"noopener nofollow ugc\"><code>dvc exp gc -w<\/code><\/a> (or with other flags) to clean the dvc experiments.<\/p>\n<p>I think the problem here is that <code>git add<\/code> and <code>git commit<\/code> is used as an alternative to the commands we provide for <a href=\"https:\/\/dvc.org\/doc\/user-guide\/experiment-management\/persisting-experiments\" rel=\"noopener nofollow ugc\">persisting experiments<\/a> and it doesn\u2019t take care of cleaning experiments, causing the confusion.<\/p>\n<p><a class=\"mention\" href=\"\/u\/chrisp45\">@chrisP45<\/a> does this solve the question?<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"exp experi show wrong posit try dai question exp setup git head commit run experi remot machin checkout pull exp run creat new branch exp branch creat push experi exp push local machin fetch experi exp pull mean time commit exp behav unexpectedli run exp show experi run exp show experi expect behavior run exp show experi hope got wrong",
        "Question_preprocessed_content":"exp experi show wrong posit try dai question exp setup git head commit run experi remot machin creat new branch creat push experi local machin fetch experi mean time commit exp behav unexpectedli run exp show experi run exp show experi expect behavior run exp show experi hope got wrong",
        "Question_gpt_summary_original":"The user is encountering challenges with the dvc exp show command. Despite running an experiment and pushing it to a new branch, the experiment is not showing up when using the dvc exp show command with the -a option. Instead, it only shows the two branches. However, when using the -A option, the experiment shows up under the original commit. The user is expecting the experiment to show up under the new branch when using the -a option.",
        "Question_gpt_summary":"user encount challeng exp command despit run experi push new branch experi show exp command option instead show branch option experi show origin commit user expect experi new branch option",
        "Answer_original_content":"step code happen realiz misunderstood concept experi like overview experi group branch sort metric paramet appar exp right tool suggest achiev want open suggest better workflow look like special requir need run experi machin insid docker contain tri insid contain git clone exp run exp branch exp push git push requir bring requir experi push git automat experi run need remov unwant experi later tri solv have experi creat branch merg branch main featur branch delet branch exp experi branch fine wai gugar plain exp git checkout branch experi associ parent commit branch exp branch show current branch head experi associ branch head commit scenario experi associ commit branch tip branch current point commit exp branch creat new git branch contain content experi associ experi new git branch exp branch expect us git branch instead experi ref run exp show experi run exp show experi branch head move mean time commit exp includ commit tabl branch entir new git branch longer consid child experi kuprus pmrowla thank repli pmrowla thank detail explan great document kuprus show workspac branch experi branch intend behavior read pmrowla answer approach support straightforward think solut issu chang wai organ experi approach command correctli actual want sort group branch provid info want group experi like suggest like simpli misunderstand disconnect expect workflow possibl workflow depend want leav setup exp branch result exp branch exact experi commit creat branch want like messi manag branch remov exp branch step push experi pull local exp need creat new branch commit typic workflow case later us exp branch exp appli bring best experi regular git workflow checkout differ branch run experi experi base tip branch us actual need group bunch experi branch dberenbaum thank answer tri suggest experi exp head commit experi us commit result pretti messi sort metric try lot stuff think major misconcept experi meant multipl experi git commit sort correct us case max experi git commit want sort branch cours write exp replac ask wrong think major misconcept experi meant multipl experi git commit sort right make sens tabl sort commit want sort commit branch right feel free add featur request sign github github copi relev point prefer us case max experi git commit want sort branch mind explain want experi commit differ commit sort defeat point experi track differ dont need commit branch long gap lot commit experi dont think us run multipl experi base singl commit think workflow fine probabl accomplish exp command exp mind explain want experi commit differ commit sort defeat point experi track differ dont need commit branch research differ experi code want chang code cours commit git run experi cours chang paramet experi come handi that person thing prefer complet defin state git run experi updat commit param yaml file run experi paramet argument probabl accomplish exp command ye that think us repro tbh exp stuff ad bunch complex workflow limit benefit think work experi name hash addit git stuff introduc new wai error btw dont grumbl love rest far great tool research differ experi code want chang code cours commit git run experi thing note experi includ uncommit code chang arent meant support paramet chang commit git run experi code chang exp run modifi paramet git commit ing commit code chang result code chang store experi retriev chang experi exp appli thing like edit code vim src file exp run exp edit code test altern version chang vim src file exp run exp compar result code chang code chang exp decid want chang result discard git reset hard exp appli exp git add git commit commit contain code chang result thank point run experi specif remot machin workflow work commit push chang remot machin pick desir state run us mix git code version control feel error prone us mix git code version control feel error prone explan workflow doesnt fit us case make sens clarifi intern experi git commit intent purpos git code version control name experi git ref like branch tag store separ ref exp namespac us experi ref sha git command like commit sha git ref read implement http org blog experi ref hello dberenbaum pmrowla experi version past week try establish workflow experi try us workflow experi group separ branch data project structur provid tutori data version control python real python fork repo github christianp real python exampl like train simpl sgd classifi creat branch call sgd group sgd relat experi branch ran experi differ number iter baselin iter look good point actual commit push chang fyi instal git hook debug experi appear master branch commit push chang reason figur expect miss help gui assist run idea thank advanc cheer chri chrisp expect appear bug exp look experi appear master branch commit push chang reason figur took look happen sgd branch doesnt contain commit point commit master let master commit run experi sgd branch actual deriv master commit run git add git commit introduc new commit sgd branch that default exp doesnt experi default show experi deriv current commit experi ran exp exp look branch find master point master commit experi deriv us exp flag clean experi think problem git add git commit altern command provid persist experi doesnt care clean experi caus confus chrisp solv question",
        "Answer_preprocessed_content":"step code happen realiz misunderstood concept experi like overview experi group branch sort metric paramet appar exp right tool suggest achiev want open suggest better workflow look like special requir need run experi machin insid docker contain requir bring requir experi push git automat experi run need remov unwant experi later tri solv have experi creat branch merg branch main featur branch delet branch exp experi branch fine plain experi associ parent commit show current branch head experi associ branch head commit scenario experi associ creat new git branch contain content experi experi new git branch expect us git branch run exp show experi run exp show experi head move includ tabl entir new git branch longer consid child thank repli thank detail explan great document show experi intend behavior read answer approach support straightforward think solut issu chang wai organ experi approach command correctli actual want sort group branch provid info want group experi like suggest like simpli misunderstand disconnect expect workflow possibl workflow depend want leav setup result branch exact experi commit creat branch want like messi manag branch remov step push experi need creat new branch commit typic workflow case later us bring best experi regular git workflow checkout differ branch run experi experi base tip branch us actual need group bunch experi branch thank answer tri suggest experi exp head commit experi us commit result pretti messi sort metric try lot stuff think major misconcept experi meant multipl experi git commit sort correct us case max experi git commit want sort branch cours write exp replac ask wrong think major misconcept experi meant multipl experi git commit sort right make sens tabl sort commit want sort commit branch right feel free add featur request sign github github copi relev point prefer us case max experi git commit want sort branch mind explain want experi commit differ commit sort defeat point experi track differ dont need long gap lot commit experi dont think us run multipl experi base singl commit think workflow fine probabl accomplish command mind explain want experi commit differ commit sort defeat point experi track differ dont need research differ experi code want chang code cours commit git run experi cours chang paramet experi come handi prefer complet defin state git run experi updat commit file run experi paramet argument probabl accomplish command ye that think us tbh stuff ad bunch complex workflow limit benefit think work experi name hash addit git stuff introduc new wai error btw dont grumbl love rest far great tool research differ experi code want chang code cours commit git run experi thing note experi includ uncommit code chang commit git run experi code chang commit code chang retriev chang experi thing like thank point run experi specif remot machin workflow work commit push chang remot machin pick desir state run us mix git code version control feel error prone us mix git code version control feel error prone explan workflow doesnt fit us case make sens clarifi intern experi git commit intent purpos git code version control name experi git ref store separ namespac us experi ref sha command like commit sha git ref read implement hello experi past week try establish workflow experi try us workflow experi group separ branch data project structur provid tutori data version control python real python fork repo github exampl like train simpl sgd classifi creat branch call group sgd relat experi branch ran experi differ number iter look good point actual commit push chang experi appear branch commit push chang reason figur expect miss help gui assist run idea thank advanc cheer chri expect appear bug look experi appear branch commit push chang reason figur took look happen branch doesnt contain commit point commit run experi branch actual deriv run introduc new commit branch that default doesnt experi default show experi deriv current commit look branch find point experi deriv us clean experi think problem altern command provid persist experi doesnt care clean experi caus confus solv question",
        "Answer_gpt_summary_original":"the user is encountering challenges with the 'exp show' command, where the experiment is not showing or is in the wrong position. the answer suggests different possible workflows depending on what the user wants, such as leaving the setup as is, removing the exp branch step, or checking out a different branch before running the experiment. the answer also clarifies that experiments are just git commits and that named experiments are just git refs stored in a separate refs\/exps namespace. additionally, the answer addresses a bug in exp show -a where experiments appear under the wrong branch and suggests using exp gc -w to clean the experiments.",
        "Answer_gpt_summary":"user encount challeng exp command experi show wrong posit answer suggest differ possibl workflow depend user want leav setup remov exp branch step check differ branch run experi answer clarifi experi git commit name experi git ref store separ ref exp namespac addition answer address bug exp experi appear wrong branch suggest exp clean experi"
    },
    {
        "Question_title":"XGBoost Model in AWS-Sagemaker Fails with no error message",
        "Question_body":"<p>I'm trying to get a model using the XGBoost classifier in AWS-Sagemaker.  I'm following the abalone example, but when I run it to build the training job it states InProgress 3 times and then just states Failed.  Where do I go to find why it failed?  <\/p>\n\n<p>I've double checked the parameters and made sure the input and output files and directories in S3 were correct.  I know there is permission to read and write because when setting up the data for train\/validate\/test I read and write to S3 with no problems.<\/p>\n\n<pre><code>print(status)\nwhile status !='Completed' and status!='Failed':\n    time.sleep(60)\n    status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n    print(status)\n<\/code><\/pre>\n\n<p>That is the code where the print statements come from.  Is there something I can add to receive a better error message?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1562946273023,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":204.0,
        "Answer_body":"<p>The problem occurred was that the file sent for predictions was csv but the XGBoost settings were set to receive libsvm.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57010184",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1564494294316,
        "Question_original_content":"xgboost model aw fail error messag try model xgboost classifi aw follow abalon exampl run build train job state inprogress time state fail fail doubl check paramet sure input output file directori correct know permiss read write set data train valid test read write problem print statu statu complet statu fail time sleep statu client train job trainingjobnam job trainingjobstatu print statu code print statement come add receiv better error messag",
        "Question_preprocessed_content":"xgboost model aw fail error messag try model xgboost classifi follow abalon exampl run build train job state inprogress time state fail fail doubl check paramet sure input output file directori correct know permiss read write set data read write problem code print statement come add receiv better error messag",
        "Question_gpt_summary_original":"The user is facing challenges while trying to build a training job using the XGBoost classifier in AWS-Sagemaker. The job fails without any error message, and the user is unable to determine the reason for the failure. The user has checked the parameters and ensured that the input and output files and directories in S3 are correct. The user is also able to read and write to S3 without any problems. The user is seeking help to receive a better error message.",
        "Question_gpt_summary":"user face challeng try build train job xgboost classifi aw job fail error messag user unabl determin reason failur user check paramet ensur input output file directori correct user abl read write problem user seek help receiv better error messag",
        "Answer_original_content":"problem occur file sent predict csv xgboost set set receiv libsvm",
        "Answer_preprocessed_content":"problem occur file sent predict csv xgboost set set receiv libsvm",
        "Answer_gpt_summary_original":"Solution: The issue was identified as a mismatch between the file format sent for predictions and the XGBoost settings. The XGBoost classifier was set to receive libsvm format, but the file sent was in csv format. The solution is to ensure that the file format matches the XGBoost settings.",
        "Answer_gpt_summary":"solut issu identifi mismatch file format sent predict xgboost set xgboost classifi set receiv libsvm format file sent csv format solut ensur file format match xgboost set"
    },
    {
        "Question_title":"Remotely execute ClearML task using local-only repo",
        "Question_body":"<p>I want to execute ClearML task remotely. According to docs there are 2 options: 1) execute single python file; 2) ClearML would identify that script is part of repo, that repo will be cloned and installed into docker and executed on the worker.<\/p>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<p>I think it is somewhat extending scenario 1, where not a single file is passed for execution but whole directory with file in it.<\/p>\n<p>PS: i understand reproducibility concerns that arise, but repo is really not accessible from worker :(<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653499480970,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":25.0,
        "Answer_body":"<p>Disclaimer: I'm a team members of ClearML<\/p>\n<blockquote>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<\/blockquote>\n<p>well, no :( if your code is a single script, then yes ClearML would store the entire script, then the worker will reproduce it on the remote machine. But if your code base is composed of more than a single file, then why not use git? it is free hosted by GitHub, Bitbucket, GitLab etc.<\/p>\n<p>In theory this is doable and if you feel the need, I urge you to PR this feature. Basically you would store the entire folder as an artifact (ClearML will auto zip it for you), then the agent needs to unzip the artifact and run it. The main issue would be that cloning the Task will not clone the artifact...<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72381916",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1659992618003,
        "Question_original_content":"remot execut task local repo want execut task remot accord doc option execut singl python file identifi script repo repo clone instal docker execut worker second scenario assum repo remot url access worker isn case possibl pack local repo send remot execut think somewhat extend scenario singl file pass execut directori file understand reproduc concern aris repo access worker thank advanc",
        "Question_preprocessed_content":"remot execut task repo want execut task remot accord doc option execut singl python file identifi script repo repo clone instal docker execut worker second scenario assum repo remot url access worker isn case possibl pack local repo send remot execut think somewhat extend scenario singl file pass execut directori file understand reproduc concern aris repo access worker thank advanc",
        "Question_gpt_summary_original":"The user wants to execute a ClearML task remotely, but faces a challenge when the repo is not accessible by the worker. The two options provided by ClearML are to execute a single python file or to clone and install the repo into docker and execute it on the worker. However, the user wonders if it is possible to pack the local repo and send it for remote execution, similar to passing a directory with a file for execution. The user acknowledges the reproducibility concerns but states that the repo is not accessible from the worker.",
        "Question_gpt_summary":"user want execut task remot face challeng repo access worker option provid execut singl python file clone instal repo docker execut worker user wonder possibl pack local repo send remot execut similar pass directori file execut user acknowledg reproduc concern state repo access worker",
        "Answer_original_content":"disclaim team member second scenario assum repo remot url access worker isn case possibl pack local repo send remot execut code singl script ye store entir script worker reproduc remot machin code base compos singl file us git free host github bitbucket gitlab theori doabl feel need urg featur basic store entir folder artifact auto zip agent need unzip artifact run main issu clone task clone artifact",
        "Answer_preprocessed_content":"disclaim team member second scenario assum repo remot url access worker isn case possibl pack local repo send remot execut code singl script ye store entir script worker reproduc remot machin code base compos singl file us git free host github bitbucket gitlab theori doabl feel need urg featur basic store entir folder artifact agent need unzip artifact run main issu clone task clone",
        "Answer_gpt_summary_original":"Solution: The discussion mentions two possible solutions. The first solution is to execute a single python file or to clone and install the repo into docker and execute it on the worker, which are the options provided by ClearML. The second solution is to use git to store the entire code base, which is free and hosted by GitHub, Bitbucket, GitLab, etc. However, if the user wants to pack the local repo and send it for remote execution, it is not currently possible with ClearML. Although, in theory, it is doable and the user can submit a feature request to ClearML to store the entire folder as an artifact, which will be auto-zipped by ClearML, and then the agent needs to unzip the artifact and",
        "Answer_gpt_summary":"solut discuss mention possibl solut solut execut singl python file clone instal repo docker execut worker option provid second solut us git store entir code base free host github bitbucket gitlab user want pack local repo send remot execut current possibl theori doabl user submit featur request store entir folder artifact auto zip agent need unzip artifact"
    },
    {
        "Question_title":"How to pass environment variables in sagemaker tuner job",
        "Question_body":"Sagemaker training jobs support setting environment variables on-the-fly in the training job:\n\n \"Environment\": { \n      \"string\" : \"string\" \n   },\n\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html\n\nI did not find an equivalent for the tuner jobs:\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateHyperParameterTuningJob.html\n\nAccording to my testing, the SagemakerTuner in the python SDK simply ignores the environment variables set in the passed estimator.\n\nIs there any way to pass environment variables to the training jobs started by a tuner job programmatically, or is that currently unsupported?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1669725280762,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":9.0,
        "Answer_body":"Thanks for raising this. Yes, as you point out the Environment collection is not supported in the underlying CreateHyperparameterTuningJob API and therefore the SageMaker Python SDK can't make use of it when running a tuner.\n\nAs discussed on the SM Py SDK GitHub issue here, you might consider using hyperparameters instead to pass parameters through to the job?\n\nIf you specifically need environment variables for some other process\/library, you could also explore setting the variables from your Python script (perhaps to map from hyperparam to env var?).\n\nOr another option could be to customize your container image to bake in the variable via the ENV command? For example to customize an existing AWS Deep Learning Container (framework container), you could:\n\nUse sagemaker.image_uris.retrieve(...) to find the base image URI for your given framework, version, region, etc. You'll need to authenticate Docker to this registry as well as your own Amazon ECR account.\nCreate a Dockerfile that takes this base image URI as an arg and builds FROM it, something like this example\nAdd the required ENV commands to bake in the (static) environment variables you need\ndocker build your custom container (passing in the base image URI as a --build-arg), upload it to Amazon ECR, and use in your SageMaker training job.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5aMFxhnLQeqMY39mlmYHjA\/how-to-pass-environment-variables-in-sagemaker-tuner-job",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-29T15:42:27.973Z",
                "Answer_score":1,
                "Answer_body":"Thanks for raising this. Yes, as you point out the Environment collection is not supported in the underlying CreateHyperparameterTuningJob API and therefore the SageMaker Python SDK can't make use of it when running a tuner.\n\nAs discussed on the SM Py SDK GitHub issue here, you might consider using hyperparameters instead to pass parameters through to the job?\n\nIf you specifically need environment variables for some other process\/library, you could also explore setting the variables from your Python script (perhaps to map from hyperparam to env var?).\n\nOr another option could be to customize your container image to bake in the variable via the ENV command? For example to customize an existing AWS Deep Learning Container (framework container), you could:\n\nUse sagemaker.image_uris.retrieve(...) to find the base image URI for your given framework, version, region, etc. You'll need to authenticate Docker to this registry as well as your own Amazon ECR account.\nCreate a Dockerfile that takes this base image URI as an arg and builds FROM it, something like this example\nAdd the required ENV commands to bake in the (static) environment variables you need\ndocker build your custom container (passing in the base image URI as a --build-arg), upload it to Amazon ECR, and use in your SageMaker training job.",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1669736547972,
        "Question_original_content":"pass environ variabl tuner job train job support set environ variabl fly train job environ string string http doc aw amazon com latest apirefer api createtrainingjob html equival tuner job http doc aw amazon com latest apirefer api createhyperparametertuningjob html accord test tuner python sdk simpli ignor environ variabl set pass estim wai pass environ variabl train job start tuner job programmat current unsupport",
        "Question_preprocessed_content":"pass environ variabl tuner job train job support set environ variabl train job environ equival tuner job accord test tuner python sdk simpli ignor environ variabl set pass estim wai pass environ variabl train job start tuner job programmat current unsupport",
        "Question_gpt_summary_original":"The user is facing a challenge in passing environment variables to Sagemaker tuner jobs programmatically. While Sagemaker training jobs support setting environment variables on-the-fly, the user did not find an equivalent for the tuner jobs. The SagemakerTuner in the python SDK also ignores the environment variables set in the passed estimator. The user is seeking a solution to pass environment variables to the training jobs started by a tuner job programmatically.",
        "Question_gpt_summary":"user face challeng pass environ variabl tuner job programmat train job support set environ variabl fly user equival tuner job tuner python sdk ignor environ variabl set pass estim user seek solut pass environ variabl train job start tuner job programmat",
        "Answer_original_content":"thank rais ye point environ collect support underli createhyperparametertuningjob api python sdk us run tuner discuss sdk github issu consid hyperparamet instead pass paramet job specif need environ variabl process librari explor set variabl python script map hyperparam env var option custom contain imag bake variabl env command exampl custom exist aw deep learn contain framework contain us imag uri retriev base imag uri given framework version region need authent docker registri amazon ecr account creat dockerfil take base imag uri arg build like exampl add requir env command bake static environ variabl need docker build custom contain pass base imag uri build arg upload amazon ecr us train job",
        "Answer_preprocessed_content":"thank rais ye point environ collect support underli createhyperparametertuningjob api python sdk us run tuner discuss sdk github issu consid hyperparamet instead pass paramet job specif need environ variabl explor set variabl python script option custom contain imag bake variabl env command exampl custom exist aw deep learn contain us base imag uri given framework version region need authent docker registri amazon ecr account creat dockerfil take base imag uri arg build like exampl add requir env command bake environ variabl need docker build custom contain upload amazon ecr us train job",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Use hyperparameters instead of environment variables to pass parameters through to the job.\n2. Set the environment variables from the Python script.\n3. Customize the container image to bake in the variable via the ENV command.\n\nNo personal opinions or biases are included in the response.",
        "Answer_gpt_summary":"possibl solut mention discuss us hyperparamet instead environ variabl pass paramet job set environ variabl python script custom contain imag bake variabl env command person opinion bias includ respons"
    },
    {
        "Question_title":"How to visualize charts in Visual Studio and Azure ML through R script?",
        "Question_body":"<p>I have seen on various examples (even in Azure ML) that you are able to create appealing charts using R in Visual Studio (not R Studio!), but I have no clue how they did it. I am experienced with R, but if someone could point me in the right direction of how to visualize data sets in Visual Studio and Azure ML; I would really appreciate it.\nHere is an example I would like to duplicate (in both Azure ML and Visual Studio): <a href=\"http:\/\/i.stack.imgur.com\/2aoGB.jpg\" rel=\"nofollow\">Visual studio chart<\/a><\/p>\n\n<p>Image source: <a href=\"https:\/\/regmedia.co.uk\/2016\/03\/09\/r_vis_studio_plot.jpg?x=648&amp;y=348&amp;crop=1\" rel=\"nofollow\">https:\/\/regmedia.co.uk\/2016\/03\/09\/r_vis_studio_plot.jpg?x=648&amp;y=348&amp;crop=1<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1465389537790,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1485875676343,
        "Question_score":2.0,
        "Question_view_count":1548.0,
        "Answer_body":"<p>You can install ggplot2 in your solution in the Visual Studio extension Open R (<a href=\"https:\/\/www.visualstudio.com\/en-us\/features\/rtvs-vs.aspx\" rel=\"nofollow noreferrer\">https:\/\/www.visualstudio.com\/en-us\/features\/rtvs-vs.aspx<\/a>) through this line of code and visualize it within the R Plot window in Visual Studio after creating your R-project: <\/p>\n\n<pre><code>install.packages('ggplot2', dep = TRUE)\n\nlibrary(ggplot2)\n<\/code><\/pre>\n\n<p>The reason I have \u00ablibrary(ggplot2)\u00bb is to check if the package got successfully installed, else you would get an error like this: <strong>Error in library(ggplot2) : there is no package called \u2018ggplot2\u2019<\/strong><\/p>\n\n<p>So if you don\u2019t get that error; you should be good to go.<\/p>\n\n<p>For your question about how to output charts; you simply have to populate the ggplot2 charts from a datasource, like in my example below (csv-file):<\/p>\n\n<pre><code>dataset1 &lt;- read.csv(\"Adult Census Income Binary Classification dataset.csv\", header = TRUE, sep = \",\", quote = \"\", fill = TRUE, comment.char = \"\")\n\nhead(dataset1)\n\ninstall.packages('ggplot2', dep = TRUE)\n\nlibrary(ggplot2)\n\nnames(dataset1) &lt;- sub(pattern = ',', replacement = '.', x = names(dataset1))\n\nfoo = qplot(age, data = dataset1, geom = \"histogram\", fill = income, position = \"dodge\");\n\nprint(foo)\n\nbar = qplot(age, data = dataset1, geom = \"density\", alpha = 1, fill = income);\n\nprint(bar)\n<\/code><\/pre>\n\n<p>Here you can see that I create two charts, one histogram and one density-chart.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/sLxMN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sLxMN.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>In Azure ML, the same charts (this time I included a histogram for Relationships as well), would look like this:<\/p>\n\n<pre><code>\/\/ Map 1-based optional input ports to variables\n\ndataset1 &lt;- maml.mapInputPort(1) # class: data.frame\n\nlibrary(ggplot2)\n\nlibrary(data.table)\n\nnames(dataset1) &lt;- sub(pattern=',', replacement='.', x=names(dataset1))\n\n\/\/ This time we need to specify the X to be sex; which we didn\u2019t need in Visual Studio\n\nfoo = qplot(x=sex, data=dataset1, geom=\"histogram\", fill=income, position=\"dodge\");\n\nprint(foo)\n\nfoo = qplot(x=relationship, data=dataset1, geom=\"histogram\", fill=income, position=\"dodge\");\n\nprint(foo)\n\nfoo = qplot(x=age, data=dataset1, geom=\"density\", alpha=0.5, fill=income);\n\nprint(foo)\n\n\/\/ Select data.frame to be sent to the output Dataset port maml.mapOutputPort(\"dataset1\");\n<\/code><\/pre>\n\n<p>Remember to put all of this in a \u201cExecute R Script module\u201d in order to run it correctly. After that, you can right lick the module and visualize the result.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/2vTlD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2vTlD.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37702759",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1465396070168,
        "Question_original_content":"visual chart visual studio script seen exampl abl creat appeal chart visual studio studio clue experienc point right direct visual data set visual studio appreci exampl like duplic visual studio visual studio chart imag sourc http regmedia vi studio plot jpg crop",
        "Question_preprocessed_content":"visual chart visual studio script seen exampl abl creat appeal chart visual studio clue experienc point right direct visual data set visual studio appreci exampl like duplic visual studio chart imag sourc",
        "Question_gpt_summary_original":"The user is experienced with R but is facing challenges in creating appealing charts using R in Visual Studio and Azure ML. They are seeking guidance on how to visualize data sets in both platforms and would like to duplicate a specific example.",
        "Question_gpt_summary":"user experienc face challeng creat appeal chart visual studio seek guidanc visual data set platform like duplic specif exampl",
        "Answer_original_content":"instal ggplot solut visual studio extens open http visualstudio com featur rtv aspx line code visual plot window visual studio creat project instal packag ggplot dep true librari ggplot reason librari ggplot check packag got successfulli instal error like error librari ggplot packag call ggplot dont error good question output chart simpli popul ggplot chart datasourc like exampl csv file dataset read csv adult censu incom binari classif dataset csv header true sep quot true comment char head dataset instal packag ggplot dep true librari ggplot name dataset sub pattern replac name dataset foo qplot ag data dataset geom histogram incom posit dodg print foo bar qplot ag data dataset geom densiti alpha incom print bar creat chart histogram densiti chart chart time includ histogram relationship look like map base option input port variabl dataset maml mapinputport class data frame librari ggplot librari data tabl name dataset sub pattern replac name dataset time need specifi sex didnt need visual studio foo qplot sex data dataset geom histogram incom posit dodg print foo foo qplot relationship data dataset geom histogram incom posit dodg print foo foo qplot ag data dataset geom densiti alpha incom print foo select data frame sent output dataset port maml mapoutputport dataset rememb execut script modul order run correctli right lick modul visual result",
        "Answer_preprocessed_content":"instal ggplot solut visual studio extens open line code visual plot window visual studio creat reason librari check packag got successfulli instal error like error librari packag call ggplot dont error good question output chart simpli popul ggplot chart datasourc like exampl creat chart histogram chart look like rememb execut script modul order run correctli right lick modul visual result",
        "Answer_gpt_summary_original":"The solution provided is to install ggplot2 in Visual Studio and Azure ML using the code \"install.packages('ggplot2', dep = TRUE)\" and then visualize the data sets in R Plot window in Visual Studio or in \"Execute R Script module\" in Azure ML. The example provided shows how to create a histogram and density chart using ggplot2 in both platforms.",
        "Answer_gpt_summary":"solut provid instal ggplot visual studio code instal packag ggplot dep true visual data set plot window visual studio execut script modul exampl provid show creat histogram densiti chart ggplot platform"
    },
    {
        "Question_title":"Keep track of all the parameters of spark-submit",
        "Question_body":"<p>I have a team where many member has permission to submit Spark tasks to YARN (the resource management) by command line. It's hard to track who is using how much cores, who is using how much memory...e.g. Now I'm looking for a software, framework or something could help me monitor the parameters that each member used. It will be a bridge between client and YARN. Then I could used it to filter the submit commands.<\/p>\n\n<p>I did take a look at <a href=\"http:\/\/www.mlflow.org\" rel=\"nofollow noreferrer\">mlflow<\/a> and I really like the MLFlow Tracking but it was designed for ML training process. I wonder if there is an alternative for my purpose? Or there is any other solution for the problem.<\/p>\n\n<p>Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1562750084187,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":93.0,
        "Answer_body":"<p>My recommendation would be to build such a tool yourself as its not too complicated,\nhave a wrapper script to spark submit which logs the usage in a DB and after the spark job finishes the wrapper will know to release information. could be done really easily.\nIn addition you can even block new spark submits if your team already asked for too much information.<\/p>\n\n<p>And as you build it your self its really flexible as you can even create \"sub teams\" or anything you want.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56967364",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1562766864887,
        "Question_original_content":"track paramet spark submit team member permiss submit spark task yarn resourc manag command line hard track core memori look softwar framework help monitor paramet member bridg client yarn filter submit command look like track design train process wonder altern purpos solut problem thank",
        "Question_preprocessed_content":"track paramet team member permiss submit spark task yarn command line hard track core look softwar framework help monitor paramet member bridg client yarn filter submit command look like track design train process wonder altern purpos solut problem thank",
        "Question_gpt_summary_original":"The user is facing challenges in tracking the usage of cores and memory by team members who submit Spark tasks to YARN via command line. They are looking for a software or framework that can help monitor the parameters used by each member and act as a bridge between the client and YARN. The user has explored MLFlow Tracking but is seeking alternatives or other solutions to address the problem.",
        "Question_gpt_summary":"user face challeng track usag core memori team member submit spark task yarn command line look softwar framework help monitor paramet member act bridg client yarn user explor track seek altern solut address problem",
        "Answer_original_content":"recommend build tool complic wrapper script spark submit log usag spark job finish wrapper know releas inform easili addit block new spark submit team ask inform build self flexibl creat sub team want",
        "Answer_preprocessed_content":"recommend build tool complic wrapper script spark submit log usag spark job finish wrapper know releas inform easili addit block new spark submit team ask inform build self flexibl creat sub team want",
        "Answer_gpt_summary_original":"Solution: One solution suggested in the discussion is to build a tool that can act as a bridge between the client and YARN. The tool can be built by creating a wrapper script to spark submit which logs the usage in a database and releases information after the spark job finishes. This tool can also block new spark submits if the team has already asked for too much information. Building the tool provides flexibility to create \"sub teams\" or any other customization required. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut solut suggest discuss build tool act bridg client yarn tool built creat wrapper script spark submit log usag databas releas inform spark job finish tool block new spark submit team ask inform build tool provid flexibl creat sub team custom requir solut mention discuss"
    },
    {
        "Question_title":"How to checkpoint SageMaker model artifact during a training job?",
        "Question_body":"Hi,\n\nIs there a way to regularly checkpoint model artifact in a SageMaker training job for BYO training container?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586331915000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":182.0,
        "Answer_body":"If you specify a checkpoint configuration (regardless of managed spot training) when starting a training job, checkpointing will work. You can provide a local path and S3 path as follows (API reference):\n\n\"CheckpointConfig\": { \n  \"LocalPath\": \"string\",\n  \"S3Uri\": \"string\"\n}\n\n\nThe local path defaults to \/opt\/ml\/checkpoints\/, and then you specify the target path in S3 with S3Uri.\n\nGiven this configuration, SageMaker will configure an output channel with Continuous upload mode to Amazon S3. At the time being, this results in running an agent on the hosts that watches the file system and continuously uploads data to Amazon S3. Similar behavior is applied when debugging is enabled, for delivering tensor data to Amazon S3.\n\nAs commented, sagemaker-containers implements its own code to save intermediate outputs and watching files on the file system, but I would rather rely on the functionality offered by the service to avoid dependencies on specific libraries where possible.\n\nNote: when using SageMaker Processing, which in my view can be considered an abstraction over training or, from another perspective, the foundation for training, you can configure an output channel to use continuous upload mode; further info here.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrXX2MIygS5igas27GrAhHw\/how-to-checkpoint-sage-maker-model-artifact-during-a-training-job",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-04-08T15:22:36.000Z",
                "Answer_score":0,
                "Answer_body":"If you specify a checkpoint configuration (regardless of managed spot training) when starting a training job, checkpointing will work. You can provide a local path and S3 path as follows (API reference):\n\n\"CheckpointConfig\": { \n  \"LocalPath\": \"string\",\n  \"S3Uri\": \"string\"\n}\n\n\nThe local path defaults to \/opt\/ml\/checkpoints\/, and then you specify the target path in S3 with S3Uri.\n\nGiven this configuration, SageMaker will configure an output channel with Continuous upload mode to Amazon S3. At the time being, this results in running an agent on the hosts that watches the file system and continuously uploads data to Amazon S3. Similar behavior is applied when debugging is enabled, for delivering tensor data to Amazon S3.\n\nAs commented, sagemaker-containers implements its own code to save intermediate outputs and watching files on the file system, but I would rather rely on the functionality offered by the service to avoid dependencies on specific libraries where possible.\n\nNote: when using SageMaker Processing, which in my view can be considered an abstraction over training or, from another perspective, the foundation for training, you can configure an output channel to use continuous upload mode; further info here.",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1586359356000,
        "Question_original_content":"checkpoint model artifact train job wai regularli checkpoint model artifact train job byo train contain",
        "Question_preprocessed_content":"checkpoint model artifact train job wai regularli checkpoint model artifact train job byo train contain",
        "Question_gpt_summary_original":"The user is seeking information on how to regularly checkpoint model artifacts during a SageMaker training job for a BYO training container.",
        "Question_gpt_summary":"user seek inform regularli checkpoint model artifact train job byo train contain",
        "Answer_original_content":"specifi checkpoint configur regardless manag spot train start train job checkpoint work provid local path path follow api refer checkpointconfig localpath string suri string local path default opt checkpoint specifi target path suri given configur configur output channel continu upload mode amazon time result run agent host watch file continu upload data amazon similar behavior appli debug enabl deliv tensor data amazon comment contain implement code save intermedi output watch file file reli function offer servic avoid depend specif librari possibl note process view consid abstract train perspect foundat train configur output channel us continu upload mode info",
        "Answer_preprocessed_content":"specifi checkpoint configur start train job checkpoint work provid local path path follow checkpointconfig local path default specifi target path uri given configur configur output channel continu upload mode amazon time result run agent host watch file continu upload data amazon similar behavior appli debug enabl deliv tensor data amazon comment contain implement code save intermedi output watch file file reli function offer servic avoid depend specif librari possibl note process view consid abstract train perspect foundat train configur output channel us continu upload mode info",
        "Answer_gpt_summary_original":"Solution: The user can specify a checkpoint configuration when starting a training job in SageMaker. This can be done by providing a local path and S3 path in the checkpoint configuration. SageMaker will then configure an output channel with Continuous upload mode to Amazon S3, which will continuously upload data to S3. When using SageMaker Processing, the user can configure an output channel to use continuous upload mode.",
        "Answer_gpt_summary":"solut user specifi checkpoint configur start train job provid local path path checkpoint configur configur output channel continu upload mode amazon continu upload data process user configur output channel us continu upload mode"
    },
    {
        "Question_title":"Can I pass arguments to the entrypoint of a SageMaker estimator?",
        "Question_body":"<p>I'm using the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html#sagemaker.estimator.Framework\" rel=\"noreferrer\">SageMaker python sdk<\/a> and was hoping to pass in some arguments to be used by my entrypoint, I'm not seeing how to do this.<\/p>\n\n<pre><code>from sagemaker.sklearn.estimator import SKLearn  # sagemaker python sdk\n\nentrypoint = 'entrypoint_script.py'\n\nsklearn = SKLearn(entry_point=entrypoint,  # &lt;-- need to pass args to this\n                  train_instance_type=instance_class,\n                  role=role,\n                  sagemaker_session=sm)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1556867880610,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1556899526172,
        "Question_score":5.0,
        "Question_view_count":2780.0,
        "Answer_body":"<p>The answer is no as there is no parameter on the Estimator base class, or the fit method, that accepts arguments to pass to the entrypoint.<\/p>\n\n<p>I resolved this by passing the parameter as part of the hyperparameter dictionary. This gets passed to the entrypoint as arguments.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1578346552883,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55964972",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1556949846448,
        "Question_original_content":"pass argument entrypoint estim python sdk hope pass argument entrypoint see sklearn estim import sklearn python sdk entrypoint entrypoint script sklearn sklearn entri point entrypoint need pass arg train instanc type instanc class role role session",
        "Question_preprocessed_content":"pass argument entrypoint estim python sdk hope pass argument entrypoint see",
        "Question_gpt_summary_original":"The user is trying to pass arguments to the entrypoint of a SageMaker estimator using the SageMaker python sdk, but is unable to find a way to do so. They have provided a code snippet where they are using the SKLearn estimator and need to pass arguments to the entry_point parameter.",
        "Question_gpt_summary":"user try pass argument entrypoint estim python sdk unabl wai provid code snippet sklearn estim need pass argument entri point paramet",
        "Answer_original_content":"answer paramet estim base class fit method accept argument pass entrypoint resolv pass paramet hyperparamet dictionari get pass entrypoint argument",
        "Answer_preprocessed_content":"answer paramet estim base class fit method accept argument pass entrypoint resolv pass paramet hyperparamet dictionari get pass entrypoint argument",
        "Answer_gpt_summary_original":"Solution: One solution mentioned in the discussion is to pass the parameter as part of the hyperparameter dictionary, which gets passed to the entrypoint as arguments.",
        "Answer_gpt_summary":"solut solut mention discuss pass paramet hyperparamet dictionari get pass entrypoint argument"
    },
    {
        "Question_title":"Sagemaker doesn't inference in an async manner",
        "Question_body":"<p>I've deployed a custom model with an async endpoint. I want to process video files with it because videos can have ~5-10 minutes I can't load all frames to memory. Of course, I want to make an inference on each frame.\nI've written<br \/>\n<code>input_fn<\/code> - download video file from s3 using boto and creates generator which loads video frames with a given batch size - return a generator - written with OpenCV<br \/>\n<code>predict_fn<\/code> - iterate over generator batched frames and generate prediction using model - save prediction in list<br \/>\n<code>output_fn<\/code> - transform prediction into json format, gzip all to reduce the size<\/p>\n<p>Endpoint works well, but the problem is concurrency. The sagemaker endpoint processes request after request (from cloudwatch and s3 save file time). I don't know why this happens.\nmax_concurrent_invocations_per_instance is set to 1000. Other settings from PyTorch serving are as follows:<\/p>\n<pre><code>SAGEMAKER_MODEL_SERVER_TIMEOUT: 100000\nSAGEMAKER_TS_MAX_BATCH_DELAY: 10000\nSAGEMAKER_TS_BATCH_SIZE: 1000\nSAGEMAKER_TS_MAX_WORKERS: 4\nSAGEMAKER_TS_RESPONSE_TIMEOUT: 100000\n<\/code><\/pre>\n<p>And still, it doesn't work. So how can I create an async inference endpoint with PyTorch to get concurrency?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653569251380,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":109.0,
        "Answer_body":"<p>The concurrency settings for TorchServe DLC are controlled by such mechanisms as # of workers, which can be set by defining the appropriate variables, such as <code>SAGEMAKER_TS_*<\/code>, and <code>SAGEMAKER_MODEL_*<\/code> (see, e.g., <a href=\"https:\/\/github.com\/pytorch\/serve\/blob\/master\/docs\/configuration.md\" rel=\"nofollow noreferrer\">this page<\/a> for details on their meaning and implications).<\/p>\n<p>While the latter are agnostic to any particular serving stack and are defined in the <a href=\"https:\/\/github.com\/aws\/sagemaker-inference-toolkit\" rel=\"nofollow noreferrer\">SageMaker Inference Toolkit<\/a>, the former are TorchServe-specific and are defined in <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\" rel=\"nofollow noreferrer\">TorchServe Inference Toolkit<\/a>. Moreover, since the TorchServe Inference Toolkit is built on top of the SageMaker Inference Toolkit, there is a non-trivial interplay between these two sets of params.<\/p>\n<p>Thus you may also want to experiment with such params as, e.g., <code>SAGEMAKER_MODEL_SERVER_WORKERS<\/code> to properly set up the concurrency setting of the SageMaker Async Endpoint.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72392070",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1653766025390,
        "Question_original_content":"infer async manner deploi custom model async endpoint want process video file video minut load frame memori cours want infer frame written input download video file boto creat gener load video frame given batch size return gener written opencv predict iter gener batch frame gener predict model save predict list output transform predict json format gzip reduc size endpoint work problem concurr endpoint process request request cloudwatch save file time know happen max concurr invoc instanc set set pytorch serv follow model server timeout max batch delai batch size max worker respons timeout work creat async infer endpoint pytorch concurr",
        "Question_preprocessed_content":"infer async manner deploi custom model async endpoint want process video file video minut load frame memori cours want infer frame written download video file boto creat gener load video frame given batch size return gener written opencv iter gener batch frame gener predict model save predict list transform predict json format gzip reduc size endpoint work problem concurr endpoint process request request know happen set set pytorch serv follow work creat async infer endpoint pytorch concurr",
        "Question_gpt_summary_original":"The user has deployed a custom model with an async endpoint on Sagemaker to process video files with multiple frames. However, the Sagemaker endpoint processes requests one after the other, causing concurrency issues. The user has tried adjusting the settings, but the problem persists. The user is seeking a solution to create an async inference endpoint with PyTorch to achieve concurrency.",
        "Question_gpt_summary":"user deploi custom model async endpoint process video file multipl frame endpoint process request caus concurr issu user tri adjust set problem persist user seek solut creat async infer endpoint pytorch achiev concurr",
        "Answer_original_content":"concurr set torchserv dlc control mechan worker set defin appropri variabl model page detail mean implic agnost particular serv stack defin infer toolkit torchserv specif defin torchserv infer toolkit torchserv infer toolkit built infer toolkit non trivial interplai set param want experi param model server worker properli set concurr set async endpoint",
        "Answer_preprocessed_content":"concurr set torchserv dlc control mechan worker set defin appropri variabl agnost particular serv stack defin infer toolkit defin torchserv infer toolkit torchserv infer toolkit built infer toolkit interplai set param want experi param properli set concurr set async endpoint",
        "Answer_gpt_summary_original":"Solution: The user can experiment with adjusting the concurrency settings for TorchServe DLC by defining appropriate variables such as <code>SAGEMAKER_TS_*<\/code> and <code>SAGEMAKER_MODEL_*<\/code>. Additionally, the user can experiment with adjusting the <code>SAGEMAKER_MODEL_SERVER_WORKERS<\/code> parameter to properly set up the concurrency setting of the SageMaker Async Endpoint.",
        "Answer_gpt_summary":"solut user experi adjust concurr set torchserv dlc defin appropri variabl model addition user experi adjust model server worker paramet properli set concurr set async endpoint"
    },
    {
        "Question_title":"How to custom code an inference pipeline in AWS sagemaker?",
        "Question_body":"<p>I am building a time series usecase to automate the preprocess and retrain tasks.At first the data is preprocessed using numpy, pandas, statsmodels etc &amp; later a machine learning algorithm is applied to make predictions.\nThe reason for using inference pipeline is that it reuses the same preprocess code for training and inference. I have checked the examples given by AWS sagemaker team with spark and sci-kit learn. In both the examples they use a sci-kit learn container to fit &amp; transform their preprocess code. Should I also have to create a container which is not needed in my use case as I am not using any sci-kit-learn code? <\/p>\n\n<p>Can someone give me a custom example of using these pipelines? Any help is appreciated!<\/p>\n\n<p><strong>Sources looked into:<\/strong><\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline<\/a>\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/inference_pipeline_sparkml_blazingtext_dbpedia\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/inference_pipeline_sparkml_blazingtext_dbpedia<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1574408498230,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1186.0,
        "Answer_body":"<p>Apologies for the late response.<\/p>\n\n<p>Below is some documentation on inference pipelines:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html<\/a>\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-real-time.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-real-time.html<\/a><\/p>\n\n<blockquote>\n  <p>Should I also have to create a container which is not needed in my use case as I am not using any sci-kit-learn code?<\/p>\n<\/blockquote>\n\n<p>Your container is an encapsulation of the environment needed for your custom code needed to run properly. Based on the requirements listed above, <code>numpy, pandas, statsmodels etc &amp; later a machine learning algorithm<\/code>, I would create a container if you wish to isolate your dependencies or modify an existing predefined SageMaker container, such as the scikit-learn one, and add your dependencies into that.<\/p>\n\n<blockquote>\n  <p>Can someone give me a custom example of using these pipelines? Any help is appreciated!<\/p>\n<\/blockquote>\n\n<p>Unfortunately, the two example notebooks referenced above are the only examples utilizing inference pipelines. The biggest hurdle most likely is creating containers that fulfill the preprocessing and prediction task you are seeking and then combining those two together into the inference pipeline.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1579974118672,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58989610",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1575505485728,
        "Question_original_content":"custom code infer pipelin build time seri usecas autom preprocess retrain task data preprocess numpi panda statsmodel later machin learn algorithm appli predict reason infer pipelin reus preprocess code train infer check exampl given team spark sci kit learn exampl us sci kit learn contain fit transform preprocess code creat contain need us case sci kit learn code custom exampl pipelin help appreci sourc look http github com awslab amazon exampl tree master python sdk scikit learn infer pipelin http github com awslab amazon exampl tree master advanc function infer pipelin sparkml blazingtext dbpedia",
        "Question_preprocessed_content":"custom code infer pipelin build time seri usecas autom preprocess retrain data preprocess numpi panda statsmodel later machin learn algorithm appli predict reason infer pipelin reus preprocess code train infer check exampl given team spark learn exampl us learn contain fit transform preprocess code creat contain need us case code custom exampl pipelin help appreci sourc look",
        "Question_gpt_summary_original":"The user is facing a challenge in custom coding an inference pipeline in AWS Sagemaker for a time series use case. They have checked examples provided by AWS Sagemaker team but are unsure if they need to create a container as they are not using any sci-kit-learn code. The user is seeking a custom example of using these pipelines.",
        "Question_gpt_summary":"user face challeng custom code infer pipelin time seri us case check exampl provid team unsur need creat contain sci kit learn code user seek custom exampl pipelin",
        "Answer_original_content":"apolog late respons document infer pipelin http doc aw amazon com latest infer pipelin html http doc aw amazon com latest infer pipelin real time html creat contain need us case sci kit learn code contain encapsul environ need custom code need run properli base requir list numpi panda statsmodel later machin learn algorithm creat contain wish isol depend modifi exist predefin contain scikit learn add depend custom exampl pipelin help appreci unfortun exampl notebook referenc exampl util infer pipelin biggest hurdl like creat contain fulfil preprocess predict task seek combin infer pipelin",
        "Answer_preprocessed_content":"apolog late respons document infer pipelin creat contain need us case code contain encapsul environ need custom code need run properli base requir list creat contain wish isol depend modifi exist predefin contain add depend custom exampl pipelin help appreci unfortun exampl notebook referenc exampl util infer pipelin biggest hurdl like creat contain fulfil preprocess predict task seek combin infer pipelin",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- The user can refer to the AWS Sagemaker documentation on inference pipelines to understand how to create them for their time series use case.\n- The user may need to create a container to encapsulate the environment needed for their custom code to run properly, even if they are not using any sci-kit-learn code.\n- The user can modify an existing predefined SageMaker container, such as the scikit-learn one, and add their dependencies into that.\n- Unfortunately, no custom example of using these pipelines is provided in the discussion.",
        "Answer_gpt_summary":"possibl solut mention discuss user refer document infer pipelin understand creat time seri us case user need creat contain encapsul environ need custom code run properli sci kit learn code user modifi exist predefin contain scikit learn add depend unfortun custom exampl pipelin provid discuss"
    },
    {
        "Question_title":"Cannot execute AWS Sagemaker Notebook",
        "Question_body":"<p>I cannot execute sagemaker notebook anymore.<br>\nThe following error occurs.<\/p>\n\n<pre><code>Failed to start kernel\nAn error occurred (ThrottlingException) when calling the CreateApp operation (reached max retries: 4): \nRate exceeded\n<\/code><\/pre>\n\n<p>I checked my app list and there are only two.\nOne app is trying to delete but never stops, this could be one of the problem.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/M0iqo.png\" rel=\"nofollow noreferrer\">image<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1590373205720,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":965.0,
        "Answer_body":"<p>Happened to me too. Contact support and ask them to delete the kernel behind the scenes.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61994821",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1590563891208,
        "Question_original_content":"execut notebook execut notebook anymor follow error occur fail start kernel error occur throttlingexcept call createapp oper reach max retri rate exceed check app list app try delet stop problem imag",
        "Question_preprocessed_content":"execut notebook execut notebook anymor follow error occur check app list app try delet stop problem imag",
        "Question_gpt_summary_original":"The user is facing challenges in executing AWS Sagemaker Notebook due to a ThrottlingException error that occurs when calling the CreateApp operation. The user suspects that the issue could be related to an app that is trying to delete but never stops.",
        "Question_gpt_summary":"user face challeng execut notebook throttlingexcept error occur call createapp oper user suspect issu relat app try delet stop",
        "Answer_original_content":"happen contact support ask delet kernel scene",
        "Answer_preprocessed_content":"happen contact support ask delet kernel scene",
        "Answer_gpt_summary_original":"Solution: Contact AWS support and request them to delete the kernel behind the scenes.",
        "Answer_gpt_summary":"solut contact aw support request delet kernel scene"
    },
    {
        "Question_title":"\"The timestamp column must have valid timestamp entries.\" error when using `timestamp_split_column_name` arg in `AutoMLTabularTrainingJob.run`",
        "Question_body":"<p>From <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform.html#google.cloud.aiplatform.AutoMLTabularTrainingJob.run\" rel=\"nofollow noreferrer\">the docs<\/a> it says that<\/p>\n<blockquote>\n<p>The value of the key values of the key (the values in the column) must be in RFC 3339 date-time format, where time-offset = \u201cZ\u201d (e.g. 1985-04-12T23:20:50.52Z)<\/p>\n<\/blockquote>\n<p>The dataset that I'm pointing to is a CSV in cloud storage, where the data is in the format suggested by the docs:<\/p>\n<pre><code>$ gsutil cat gs:\/\/my-data.csv | head | xsv select TS_SPLIT_COL\nTS_SPLIT_COL\n2021-01-18T00:00:00.00Z\n2021-01-18T00:00:00.00Z\n2021-01-04T00:00:00.00Z\n2021-03-06T00:00:00.00Z\n2021-01-15T00:00:00.00Z\n2021-02-11T00:00:00.00Z\n2021-02-05T00:00:00.00Z\n2021-05-20T00:00:00.00Z\n2021-01-05T00:00:00.00Z\n<\/code><\/pre>\n<p>But I receive a <code>Training pipeline failed with error message: The timestamp column must have valid timestamp entries.<\/code> error when I try to run a training job<\/p>\n<p>EDIT: this should hopefully make it more reproducible<\/p>\n<p>data: <a href=\"https:\/\/pastebin.com\/qEDqvzX6\" rel=\"nofollow noreferrer\">https:\/\/pastebin.com\/qEDqvzX6<\/a><\/p>\n<p>Code I'm running:<\/p>\n<pre><code>from google.cloud import aiplatform\n\nPROJECT = &quot;my-project&quot;\nDATASET_ID = &quot;dataset-id&quot;  # points to CSV \n\naiplatform.init(project=PROJECT)\n\ndataset = aiplatform.TabularDataset(DATASET_ID)\n\njob = aiplatform.AutoMLTabularTrainingJob(\n    display_name=&quot;so-58454722&quot;,\n    optimization_prediction_type=&quot;classification&quot;,\n    optimization_objective=&quot;maximize-au-roc&quot;,\n)\n\nmodel = job.run(\n    dataset=dataset,\n    model_display_name=&quot;so-58454722&quot;,\n    target_column=&quot;Y&quot;,\n    training_fraction_split=0.8,\n    validation_fraction_split=0.1,\n    test_fraction_split=0.1,\n    timestamp_split_column_name=&quot;TS_SPLIT_COL&quot;,\n)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1647473614997,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1647534964220,
        "Question_score":0.0,
        "Question_view_count":157.0,
        "Answer_body":"<p>Try this timestamp format instead:<\/p>\n<p><code>2022-03-18T01:23:45.123456+00:00<\/code><\/p>\n<p>It uses <code>+00:00<\/code> instead of <code>Z<\/code> to specify timezone.<\/p>\n<p>This change will eliminate the &quot;The timestamp column must have valid timestamp entries.&quot; error<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71505415",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1647602028112,
        "Question_original_content":"timestamp column valid timestamp entri error timestamp split column arg automltabulartrainingjob run doc sai valu kei valu kei valu column rfc date time format time offset dataset point csv cloud storag data format suggest doc gsutil cat data csv head xsv select split col split col receiv train pipelin fail error messag timestamp column valid timestamp entri error try run train job edit hopefulli reproduc data http pastebin com qedqvzx code run googl cloud import aiplatform project project dataset dataset point csv aiplatform init project project dataset aiplatform tabulardataset dataset job aiplatform automltabulartrainingjob displai optim predict type classif optim object maxim roc model job run dataset dataset model displai target column train fraction split valid fraction split test fraction split timestamp split column split col",
        "Question_preprocessed_content":"timestamp column valid timestamp error arg doc sai valu kei valu kei rfc format dataset point csv cloud storag data format suggest doc receiv error try run train job edit hopefulli reproduc data code run",
        "Question_gpt_summary_original":"The user is encountering an error when running a training job using `AutoMLTabularTrainingJob.run` with the `timestamp_split_column_name` argument. The error message states that the timestamp column must have valid timestamp entries in RFC 3339 date-time format with time-offset = \u201cZ\u201d. The user's dataset is in the correct format, but they are still receiving the error.",
        "Question_gpt_summary":"user encount error run train job automltabulartrainingjob run timestamp split column argument error messag state timestamp column valid timestamp entri rfc date time format time offset user dataset correct format receiv error",
        "Answer_original_content":"try timestamp format instead us instead specifi timezon chang elimin timestamp column valid timestamp entri error",
        "Answer_preprocessed_content":"try timestamp format instead us instead specifi timezon chang elimin timestamp column valid timestamp error",
        "Answer_gpt_summary_original":"Solution: The discussion suggests using a different timestamp format that includes the timezone offset in the format of `2022-03-18T01:23:45.123456+00:00` instead of `Z`. This solution is expected to eliminate the error message and allow the user to run the training job successfully.",
        "Answer_gpt_summary":"solut discuss suggest differ timestamp format includ timezon offset format instead solut expect elimin error messag allow user run train job successfulli"
    },
    {
        "Question_title":"How to deploy R script web service via Azure CLI",
        "Question_body":"Hello everyone,\n\nI am tring to deploy R script as a web service using Azure Machine Learning. I created pipeline as below.\n\nI can deploy the model and endpoint from [Deploy] button but I cannot control some properties: i.e. resource name, dns name.\n\nIt seems that the az ml model deploy command can be used to deploy the endpoint.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-container-instance#using-the-azure-cli\n\nI have no information for inferenceconfig.json. How to write score.py to execute R script? Is it any example?",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1635213453407,
        "Question_favorite_count":9.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, the following document describes how to define an inference configuration.\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/603664\/how-to-deploy-r-script-web-service-via-azure-cli.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-26T21:18:18.257Z",
                "Answer_score":0,
                "Answer_body":"Hi, the following document describes how to define an inference configuration.\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1635283098256,
        "Question_original_content":"deploi script web servic azur cli hello tring deploi script web servic creat pipelin deploi model endpoint deploi button control properti resourc dn model deploi command deploi endpoint http doc microsoft com azur machin learn deploi azur contain instanc azur cli inform inferenceconfig json write score execut script exampl",
        "Question_preprocessed_content":"deploi script web servic azur cli hello tring deploi script web servic creat pipelin deploi model endpoint button control properti resourc dn model deploi command deploi endpoint inform write execut script exampl",
        "Question_gpt_summary_original":"The user is facing challenges in deploying an R script as a web service using Azure Machine Learning. They are unable to control certain properties such as resource name and DNS name when deploying the model and endpoint from the \"Deploy\" button. The user is seeking guidance on how to use the \"az ml model deploy\" command to deploy the endpoint and how to write the score.py to execute the R script, as they have no information on the inferenceconfig.json.",
        "Question_gpt_summary":"user face challeng deploi script web servic unabl control certain properti resourc dn deploi model endpoint deploi button user seek guidanc us model deploi command deploi endpoint write score execut script inform inferenceconfig json",
        "Answer_original_content":"follow document describ defin infer configur kindli accept answer inform help thank",
        "Answer_preprocessed_content":"follow document describ defin infer configur kindli accept answer inform help thank",
        "Answer_gpt_summary_original":"Solution provided: The discussion participant suggests referring to a document that describes how to define an inference configuration. However, no specific solution is mentioned regarding the user's challenge of controlling certain properties when deploying the model and endpoint using the \"Deploy\" button.",
        "Answer_gpt_summary":"solut provid discuss particip suggest refer document describ defin infer configur specif solut mention user challeng control certain properti deploi model endpoint deploi button"
    },
    {
        "Question_title":"Python: How to retrive the best model from Optuna LightGBM study?",
        "Question_body":"<p>I would like to get the best model to use later in the notebook to predict using a different test batch.<\/p>\n\n<p>reproducible example (taken from Optuna Github) :<\/p>\n\n<pre><code>import lightgbm as lgb\nimport numpy as np\nimport sklearn.datasets\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\n\nimport optuna\n\n\n# FYI: Objective functions can take additional arguments\n# (https:\/\/optuna.readthedocs.io\/en\/stable\/faq.html#objective-func-additional-args).\ndef objective(trial):\n    data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n    dtrain = lgb.Dataset(train_x, label=train_y)\n    dvalid = lgb.Dataset(valid_x, label=valid_y)\n\n    param = {\n        \"objective\": \"binary\",\n        \"metric\": \"auc\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n\n    # Add a callback for pruning.\n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"auc\")\n    gbm = lgb.train(\n        param, dtrain, valid_sets=[dvalid], verbose_eval=False, callbacks=[pruning_callback]\n    )\n\n    preds = gbm.predict(valid_x)\n    pred_labels = np.rint(preds)\n    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n    return accuracy\n\n<\/code><\/pre>\n\n<p>my understanding is that the study below will tune for accuracy. I would like to somehow retrieve the best model from the study (not just the parameters) without saving it as a pickle, I just want to use the model somewhere else in my notebook. <\/p>\n\n<pre><code>\nif __name__ == \"__main__\":\n    study = optuna.create_study(\n        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"maximize\"\n    )\n    study.optimize(objective, n_trials=100)\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n\n<\/code><\/pre>\n\n<p>desired output would be <\/p>\n\n<pre><code>best_model = ~model from above~\nnew_target_pred = best_model.predict(new_data_test)\nmetrics.accuracy_score(new_target_test, new__target_pred)\n\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1591072505577,
        "Question_favorite_count":9.0,
        "Question_last_edit_time":1591072893872,
        "Question_score":14.0,
        "Question_view_count":8921.0,
        "Answer_body":"<p>I think you can use the <code>callback<\/code> argument of <a href=\"https:\/\/optuna.readthedocs.io\/en\/latest\/reference\/study.html#optuna.study.Study.optimize\" rel=\"noreferrer\"><code>Study.optimize<\/code><\/a> to save the best model. In the following code example, the callback checks if a given trial is corresponding to the best trial and saves the model as a global variable <code>best_booster<\/code>.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>best_booster = None\ngbm = None\n\ndef objective(trial):\n    global gbm\n    # ...\n\ndef callback(study, trial):\n    global best_booster\n    if study.best_trial == trial:\n        best_booster = gbm\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(\n        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"maximize\"\n    )\n    study.optimize(objective, n_trials=100, callbacks=[callback])\n\n<\/code><\/pre>\n\n<p>If you define your objective function as a class, you can remove the global variables. I created a notebook as a code example. Please take a look at it:\n<a href=\"https:\/\/colab.research.google.com\/drive\/1ssjXp74bJ8bCAbvXFOC4EIycBto_ONp_?usp=sharing\" rel=\"noreferrer\">https:\/\/colab.research.google.com\/drive\/1ssjXp74bJ8bCAbvXFOC4EIycBto_ONp_?usp=sharing<\/a><\/p>\n\n<blockquote>\n  <p>I would like to somehow retrieve the best model from the study (not just the parameters) without saving it as a pickle<\/p>\n<\/blockquote>\n\n<p>FYI, if you can pickle the boosters, I think you can make the code simple by following <a href=\"https:\/\/optuna.readthedocs.io\/en\/latest\/faq.html#how-to-save-machine-learning-models-trained-in-objective-functions\" rel=\"noreferrer\">this FAQ<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":8.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62144904",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1591153873300,
        "Question_original_content":"python retriv best model lightgbm studi like best model us later notebook predict differ test batch reproduc exampl taken github import lightgbm lgb import numpi import sklearn dataset import sklearn metric sklearn model select import train test split import fyi object function addit argument http readthedoc stabl faq html object func addit arg def object trial data target sklearn dataset load breast cancer return true train valid train valid train test split data target test size dtrain lgb dataset train label train dvalid lgb dataset valid label valid param object binari metric auc verbos boost type gbdt lambda trial suggest loguniform lambda lambda trial suggest loguniform lambda num leav trial suggest int num leav featur fraction trial suggest uniform featur fraction bag fraction trial suggest uniform bag fraction bag freq trial suggest int bag freq min child sampl trial suggest int min child sampl add callback prune prune callback integr lightgbmpruningcallback trial auc gbm lgb train param dtrain valid set dvalid verbos eval fals callback prune callback pred gbm predict valid pred label rint pred accuraci sklearn metric accuraci score valid pred label return accuraci understand studi tune accuraci like retriev best model studi paramet save pickl want us model notebook main studi creat studi pruner pruner medianprun warmup step direct maxim studi optim object trial print best trial trial studi best trial print param kei valu trial param item print format kei valu desir output best model model new target pred best model predict new data test metric accuraci score new target test new target pred",
        "Question_preprocessed_content":"python retriv best model lightgbm studi like best model us later notebook predict differ test batch reproduc exampl understand studi tune accuraci like retriev best model studi save pickl want us model notebook desir output",
        "Question_gpt_summary_original":"The user wants to retrieve the best model from an Optuna LightGBM study to use it later in the notebook to predict using a different test batch. The study tunes for accuracy, and the user wants to retrieve the best model without saving it as a pickle. The desired output is the best model and its accuracy score on new test data.",
        "Question_gpt_summary":"user want retriev best model lightgbm studi us later notebook predict differ test batch studi tune accuraci user want retriev best model save pickl desir output best model accuraci score new test data",
        "Answer_original_content":"think us callback argument studi optim save best model follow code exampl callback check given trial correspond best trial save model global variabl best booster best booster gbm def object trial global gbm def callback studi trial global best booster studi best trial trial best booster gbm main studi creat studi pruner pruner medianprun warmup step direct maxim studi optim object trial callback callback defin object function class remov global variabl creat notebook code exampl look http colab research googl com drive ssjxpbjbcabvxfoceiycbto onp usp share like retriev best model studi paramet save pickl fyi pickl booster think code simpl follow faq",
        "Answer_preprocessed_content":"think us argument save best model follow code exampl callback check given trial correspond best trial save model global variabl defin object function class remov global variabl creat notebook code exampl look like retriev best model studi save pickl fyi pickl booster think code simpl follow faq",
        "Answer_gpt_summary_original":"Solution:\n- Use the `callback` argument of `Study.optimize` to save the best model. The callback checks if a given trial is corresponding to the best trial and saves the model as a global variable `best_booster`.\n- If you define your objective function as a class, you can remove the global variables.\n- If you can pickle the boosters, you can make the code simple by following the FAQ provided in the discussion.",
        "Answer_gpt_summary":"solut us callback argument studi optim save best model callback check given trial correspond best trial save model global variabl best booster defin object function class remov global variabl pickl booster code simpl follow faq provid discuss"
    },
    {
        "Question_title":"How to get the date of a dvc remote pushed",
        "Question_body":"<p>Hi<br>\nwhen I do a dvc pull the files pulled get the current date.<\/p>\n<p>I wonder how to get the date when the files were pushed.<\/p>\n<p>With git I can get this information with <em>git log my_file<\/em>.<br>\nHow to do this kind of information with dvc?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1611757273971,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":223.0,
        "Answer_body":"<blockquote>\n<p>I wonder how to get the date when the files were pushed.<\/p>\n<\/blockquote>\n<p>You can track the dates regarding changes on your dvc-tracked files with <code>git log<\/code>, but not <code>pull<\/code>\/<code>push<\/code>s. When you <code>push<\/code> something to remote (or anyone who works with your project), it actually don\u2019t adds any field to your file regarding the push date.<\/p>\n<p>The best you can do to obtain this information is checking out the provider\u2019s interface (like if it is s3, you can see the last changed date in the listing section).<\/p>. <p>For dvc files such as my_file.dvc sure I can do a git log on the .dvc file.<\/p>\n<p>But we use stages and our dvc.yaml and dvc.lock files may contain several dependencies that could have been pushed at different time points. So the date of those dvc files may not be informative.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-get-the-date-of-a-dvc-remote-pushed\/640",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-27T14:25:51.087Z",
                "Answer_body":"<blockquote>\n<p>I wonder how to get the date when the files were pushed.<\/p>\n<\/blockquote>\n<p>You can track the dates regarding changes on your dvc-tracked files with <code>git log<\/code>, but not <code>pull<\/code>\/<code>push<\/code>s. When you <code>push<\/code> something to remote (or anyone who works with your project), it actually don\u2019t adds any field to your file regarding the push date.<\/p>\n<p>The best you can do to obtain this information is checking out the provider\u2019s interface (like if it is s3, you can see the last changed date in the listing section).<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-27T16:20:14.292Z",
                "Answer_body":"<p>For dvc files such as my_file.dvc sure I can do a git log on the .dvc file.<\/p>\n<p>But we use stages and our dvc.yaml and dvc.lock files may contain several dependencies that could have been pushed at different time points. So the date of those dvc files may not be informative.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"date remot push pull file pull current date wonder date file push git inform git log file kind inform",
        "Question_preprocessed_content":"date remot push pull file pull current date wonder date file push git inform git log kind inform",
        "Question_gpt_summary_original":"The user is facing a challenge in getting the date when files were pushed to a dvc remote. They are able to get the current date when pulling files, but are unsure how to retrieve the date of the push. They are seeking guidance on how to obtain this information, similar to how it can be done with git using \"git log my_file\".",
        "Question_gpt_summary":"user face challeng get date file push remot abl current date pull file unsur retriev date push seek guidanc obtain inform similar git git log file",
        "Answer_original_content":"wonder date file push track date chang track file git log pull push push remot work project actual dont add field file push date best obtain inform check provid interfac like chang date list section file file sure git log file us stage yaml lock file contain depend push differ time point date file inform",
        "Answer_preprocessed_content":"wonder date file push track date chang track file remot actual dont add field file push date best obtain inform check provid interfac file sure git log file us stage yaml lock file contain depend push differ time point date file inform",
        "Answer_gpt_summary_original":"Solutions provided:\n- There is no direct way to obtain the date of push for dvc-tracked files.\n- One can check the provider's interface (e.g. s3) to see the last changed date in the listing section.\n- For dvc files such as my_file.dvc, one can do a git log on the .dvc file.\n- However, for dvc files that contain several dependencies pushed at different time points, the date of those dvc files may not be informative.",
        "Answer_gpt_summary":"solut provid direct wai obtain date push track file check provid interfac chang date list section file file git log file file contain depend push differ time point date file inform"
    },
    {
        "Question_title":"Why does Azure ML Studio (classic) take additional time to execute Python Scripts?",
        "Question_body":"<p>I have been working with ML Studio (classic) and facing a problem with &quot;Execute Python&quot; scripts. I have noticed that it takes additional time to perform some internal tasks after which it starts executing the actual Python code in ML Studio. This delay has caused an increased time of 40-60 seconds per module which is aggregating and causing a delay of 400-500 seconds per execution when consumed through Batch Execution System or on running the experiments manually. (I've multiple Modules of &quot;Execute Python&quot; scripts)<\/p>\n<p>For instance - If I run a code in my local system, suppose it takes 2-3 seconds. The same would consume 50-60 seconds in Azure ML Studio.<\/p>\n<p>Can you please help understand the reason behind this or any optimization that can be done?<\/p>\n<p>Regards,\nAnant<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593694819477,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":166.0,
        "Answer_body":"<p>The known limitations of Machine Learning Studio (classic) are:<\/p>\n<p>The Python runtime is sandboxed and does not allow access to the network or to the local file system in a persistent manner.<\/p>\n<p>All files saved locally are isolated and deleted once the module finishes. The Python code cannot access most directories on the machine it runs on, the exception being the current directory and its subdirectories.<\/p>\n<p>When you provide a zipped file as a resource, the files are copied from your workspace to the experiment execution space, unpacked, and then used. Copying and unpacking resources can consume memory.<\/p>\n<p>The module can output a single data frame. It's not possible to return arbitrary Python objects such as trained models directly back to the Studio (classic) runtime. However, you can write objects to storage or to the workspace. Another option is to use pickle to serialize multiple objects into a byte array and then return the array inside a data frame.<\/p>\n<p>Hope this helps!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62696966",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1593695267950,
        "Question_original_content":"studio classic addit time execut python script work studio classic face problem execut python script notic take addit time perform intern task start execut actual python code studio delai caus increas time second modul aggreg caus delai second execut consum batch execut run experi manual multipl modul execut python script instanc run code local suppos take second consum second studio help understand reason optim regard anant",
        "Question_preprocessed_content":"studio addit time execut python script work studio face problem execut python script notic take addit time perform intern task start execut actual python code studio delai caus increas time second modul aggreg caus delai second execut consum batch execut run experi manual instanc run code local suppos take second consum second studio help understand reason optim regard anant",
        "Question_gpt_summary_original":"The user is facing a challenge with Azure ML Studio (classic) where it takes additional time to execute Python scripts due to internal tasks, causing a delay of 40-60 seconds per module and 400-500 seconds per execution. The user is seeking help to understand the reason behind this delay and any optimization that can be done.",
        "Question_gpt_summary":"user face challeng studio classic take addit time execut python script intern task caus delai second modul second execut user seek help understand reason delai optim",
        "Answer_original_content":"known limit machin learn studio classic python runtim sandbox allow access network local file persist manner file save local isol delet modul finish python code access directori machin run except current directori subdirectori provid zip file resourc file copi workspac experi execut space unpack copi unpack resourc consum memori modul output singl data frame possibl return arbitrari python object train model directli studio classic runtim write object storag workspac option us pickl serial multipl object byte arrai return arrai insid data frame hope help",
        "Answer_preprocessed_content":"known limit machin learn studio python runtim sandbox allow access network local file persist manner file save local isol delet modul finish python code access directori machin run except current directori subdirectori provid zip file resourc file copi workspac experi execut space unpack copi unpack resourc consum memori modul output singl data frame possibl return arbitrari python object train model directli studio runtim write object storag workspac option us pickl serial multipl object byte arrai return arrai insid data frame hope help",
        "Answer_gpt_summary_original":"No solutions are provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"DVC Status in vscode",
        "Question_body":"<p>I\u2019m using vscode with DVC. I\u2019m confused by what\u2019s being displayed in the SOURCE CONTROL sidebar<\/p>\n<p>There\u2019s the standard git section which I understand, and below that, there\u2019s a section for DVC<\/p>\n<p>It\u2019s showing all my model artifacts in a section saying Committed with D (deleted) beside them after I\u2019ve run and experiment and done a dvc push. The same files are also showing as deleted when I use the explorer sidebar (except they\u2019re there as symlinks to the cache)<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/6b6250df837965815ecd3785e216f17801e166a9.png\" alt=\"image\" data-base62-sha1=\"fjXJgUhnFGtJVWSdWqlGFuW8nuF\" width=\"369\" height=\"156\"><\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1676010980563,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":46.0,
        "Answer_body":"<p>This is the vscode file explorer view<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/98bb6e8db79488bd73d2d1aee919917838723268.png\" alt=\"image\" data-base62-sha1=\"lN894JXpwTZGr34C8SMLYjy4j8c\" width=\"361\" height=\"182\"><\/p>. <p>Do those files exist locally? Could you please post <code>dvc doctor<\/code> output and <code>dvc data status --granular --untracked-files --unchanged -vv<\/code>? Thanks.<\/p>. <p>Yes they exist locally, ie they can be opened in vscode and are valid symlinks to files in the dvc cache.<\/p>\n<p>dvc status is showing as deleted. This happened around the time I updated dvc (via apt) to the latest version, and added a stage to my dvc.yaml. Originally I only had one stage producing models\/model.pt, models\/text_transform.pt and models\/index_to_name.json. I then added preceding stage to create models\/tokenizer.json and models\/char_tokenizer.json in the same folder)<\/p>\n<p><code>&gt; dvc data status --granular --untracked-files --unchanged -vv<\/code><\/p>\n<pre><code class=\"lang-auto\">...\nDVC committed changes:\n  (git commit the corresponding dvc files to update the repo)\n        deleted: models\/model.pt\n        deleted: models\/tokenizer.json\n        deleted: models\/char_tokenizer.json\n        deleted: models\/text_transform.pt\n        deleted: models\/index_to_name.json\n\nDVC unchanged files:\n   ...\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">&gt; dvc doctor\n\nDVC version: 2.44.0 (deb)\n-------------------------\nPlatform: Python 3.10.8 on Linux-5.15.0-60-generic-x86_64-with-glibc2.35\nSubprojects:\n\nSupports:\n        azure (adlfs = 2023.1.0, knack = 0.10.1, azure-identity = 1.12.0),\n        gdrive (pydrive2 = 1.15.0),\n        gs (gcsfs = 2023.1.0),\n        hdfs (fsspec = 2023.1.0, pyarrow = 11.0.0),\n        http (aiohttp = 3.8.3, aiohttp-retry = 2.8.3),\n        https (aiohttp = 3.8.3, aiohttp-retry = 2.8.3),\n        oss (ossfs = 2021.8.0),\n        s3 (s3fs = 2023.1.0, boto3 = 1.24.59),\n        ssh (sshfs = 2023.1.0),\n        webdav (webdav4 = 0.9.8),\n        webdavs (webdav4 = 0.9.8),\n        webhdfs (fsspec = 2023.1.0)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/nvme0n1p2\nCaches: local\nRemotes: s3\nWorkspace directory: ext4 on \/dev\/nvme0n1p2\nRepo: dvc, git\n<\/code><\/pre>. <p>Also on my second machine, I did a git sync and then dvc pull, it\u2019s now showing the same thing, i.e.<\/p>\n<p>$ dvc pull<\/p>\n<p>M       data\/datasets\/examples\/<br>\nA       models\/tokenizer.json<br>\nA       models\/char_tokenizer.json<br>\nA       models\/model.pt<br>\nA       models\/text_transform.pt<br>\nA       models\/index_to_name.json<br>\nA       models\/checkpoints\/<br>\nD       plots\/<br>\nD       models\/<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/ab5c05f3eab0a6350790a0b3a799ac29d97c6ce4.png\" data-download-href=\"\/uploads\/short-url\/orUNZDwH1MSfhGLJSnvyYT92pbS.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/ab5c05f3eab0a6350790a0b3a799ac29d97c6ce4_2_690x283.png\" alt=\"image\" data-base62-sha1=\"orUNZDwH1MSfhGLJSnvyYT92pbS\" width=\"690\" height=\"283\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/ab5c05f3eab0a6350790a0b3a799ac29d97c6ce4_2_690x283.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/ab5c05f3eab0a6350790a0b3a799ac29d97c6ce4.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/ab5c05f3eab0a6350790a0b3a799ac29d97c6ce4.png 2x\" data-dominant-color=\"242128\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">760\u00d7312 21.4 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Of note is it pulled the latest model files (they\u2019re now on the local machine, in particular, the tokenizer files were never on this machine as I only just added that stage) but then marked them as deleted. Note: deletion of plots is correct - I did remove that folder on the other machine.<\/p>\n<p>I\u2019m not sure how these files are tracked. They\u2019re model artifacts, there are no models.dvc file like the input dataset etc.<\/p>. <p>Where are they tracked? Are those tracked in <code>.dvc<\/code> files or <code>dvc.yaml<\/code>\/<code>dvc.lock<\/code> file? This may happen if <code>.dvc<\/code> or <code>dvc.lock<\/code> files are deleted. Is that the case?<\/p>\n<p>What does <code>git status<\/code> show?<\/p>. <p>These files are not tracked by a <code>.dvc<\/code> file, they\u2019re in the <code>dvc.yaml<\/code> \/<code>dvc.lock<\/code> file.<\/p>\n<p>One thing I noticed is when I tried to commit these files it says \"ERROR: failed to commit - unable to commit changed stage: \u2018train\u2019.<\/p>\n<p>When I use the <code>force<\/code> option I noticed that it\u2019s modified one of the deps for the train stage, specifically the folder that contains a python package.<\/p>\n<pre><code>- path: src\/multiclass_classifier\n  md5: 6e7721a0f99f31a34f8ff87509039e37.dir\n  size: 53640\n  nfiles: 39\n\n- path: src\/multiclass_classifier\n  md5: d81406ab21bfec100d1669268bc31122.dir\n  size: 52541\n  nfiles: 38\n<\/code><\/pre>\n<p>I\u2019m not sure what file(s) have changed - it doesn\u2019t seem to be a source file or I git would show a change.<\/p>\n<p>Is there a better way of setting a stage dependency to a package (installed using pip -e .)?<\/p>. <p>Also I decided to recreate my <code>dvc.ymal'<\/code> file using <code>dvc stage add<\/code><\/p>\n<p>One thing I noticed was when I tried to recreate the first stage<\/p>\n<pre><code class=\"lang-auto\">$ dvc stage add -n tokenizer -d data\/datasets\/raw\/dataset.csv -p tokenizer.dataset -p tokenizer.vocab_size -o models\/tokenizer.json -o models\/char_tokenizer.json src\/scripts\/tokenizer.py\n\nERROR: Output(s) outside of DVC project: models\/tokenizer.json, models\/char_tokenizer.json. See &lt;https:\/\/dvc.org\/doc\/user-guide\/managing-external-data&gt; for more info.\n<\/code><\/pre>\n<p>I ended up creating a new folder called <code>model<\/code> instead. I then got warned that metrics.json couldn\u2019t be added to the train stage as it was tracked by git. I fixed this.<\/p>\n<p>I\u2019m still seeing messages that files are deleted - it\u2019s not the same files now. It\u2019s the outputs from stage 1 (which are dependencies of stage 2). All of these files are being saved into the same folder (models).<\/p>\n<p>It\u2019s also now reporting that metrics.json is deleted<\/p>. <p>I decided to start again, I removed the .dvc folder and created a new s3 bucket for the remote and re-initialised the repo and ran dvc add again.<\/p>\n<p>I did notice that at least one .dvc file changed when I ran dvc add - implying that they were somehow out of sync. In particular, the actual number of tracked files changed when I ran dvc add and compared the old and new .dvc file<\/p>\n<p>Seems to be working now<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-status-in-vscode\/1511",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-10T06:36:50.239Z",
                "Answer_body":"<p>This is the vscode file explorer view<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/98bb6e8db79488bd73d2d1aee919917838723268.png\" alt=\"image\" data-base62-sha1=\"lN894JXpwTZGr34C8SMLYjy4j8c\" width=\"361\" height=\"182\"><\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-10T11:52:30.181Z",
                "Answer_body":"<p>Do those files exist locally? Could you please post <code>dvc doctor<\/code> output and <code>dvc data status --granular --untracked-files --unchanged -vv<\/code>? Thanks.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-10T22:24:33.294Z",
                "Answer_body":"<p>Yes they exist locally, ie they can be opened in vscode and are valid symlinks to files in the dvc cache.<\/p>\n<p>dvc status is showing as deleted. This happened around the time I updated dvc (via apt) to the latest version, and added a stage to my dvc.yaml. Originally I only had one stage producing models\/model.pt, models\/text_transform.pt and models\/index_to_name.json. I then added preceding stage to create models\/tokenizer.json and models\/char_tokenizer.json in the same folder)<\/p>\n<p><code>&gt; dvc data status --granular --untracked-files --unchanged -vv<\/code><\/p>\n<pre><code class=\"lang-auto\">...\nDVC committed changes:\n  (git commit the corresponding dvc files to update the repo)\n        deleted: models\/model.pt\n        deleted: models\/tokenizer.json\n        deleted: models\/char_tokenizer.json\n        deleted: models\/text_transform.pt\n        deleted: models\/index_to_name.json\n\nDVC unchanged files:\n   ...\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">&gt; dvc doctor\n\nDVC version: 2.44.0 (deb)\n-------------------------\nPlatform: Python 3.10.8 on Linux-5.15.0-60-generic-x86_64-with-glibc2.35\nSubprojects:\n\nSupports:\n        azure (adlfs = 2023.1.0, knack = 0.10.1, azure-identity = 1.12.0),\n        gdrive (pydrive2 = 1.15.0),\n        gs (gcsfs = 2023.1.0),\n        hdfs (fsspec = 2023.1.0, pyarrow = 11.0.0),\n        http (aiohttp = 3.8.3, aiohttp-retry = 2.8.3),\n        https (aiohttp = 3.8.3, aiohttp-retry = 2.8.3),\n        oss (ossfs = 2021.8.0),\n        s3 (s3fs = 2023.1.0, boto3 = 1.24.59),\n        ssh (sshfs = 2023.1.0),\n        webdav (webdav4 = 0.9.8),\n        webdavs (webdav4 = 0.9.8),\n        webhdfs (fsspec = 2023.1.0)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/nvme0n1p2\nCaches: local\nRemotes: s3\nWorkspace directory: ext4 on \/dev\/nvme0n1p2\nRepo: dvc, git\n<\/code><\/pre>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-12T21:33:14.941Z",
                "Answer_body":"<p>Also on my second machine, I did a git sync and then dvc pull, it\u2019s now showing the same thing, i.e.<\/p>\n<p>$ dvc pull<\/p>\n<p>M       data\/datasets\/examples\/<br>\nA       models\/tokenizer.json<br>\nA       models\/char_tokenizer.json<br>\nA       models\/model.pt<br>\nA       models\/text_transform.pt<br>\nA       models\/index_to_name.json<br>\nA       models\/checkpoints\/<br>\nD       plots\/<br>\nD       models\/<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/ab5c05f3eab0a6350790a0b3a799ac29d97c6ce4.png\" data-download-href=\"\/uploads\/short-url\/orUNZDwH1MSfhGLJSnvyYT92pbS.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/ab5c05f3eab0a6350790a0b3a799ac29d97c6ce4_2_690x283.png\" alt=\"image\" data-base62-sha1=\"orUNZDwH1MSfhGLJSnvyYT92pbS\" width=\"690\" height=\"283\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/ab5c05f3eab0a6350790a0b3a799ac29d97c6ce4_2_690x283.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/ab5c05f3eab0a6350790a0b3a799ac29d97c6ce4.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/ab5c05f3eab0a6350790a0b3a799ac29d97c6ce4.png 2x\" data-dominant-color=\"242128\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">760\u00d7312 21.4 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Of note is it pulled the latest model files (they\u2019re now on the local machine, in particular, the tokenizer files were never on this machine as I only just added that stage) but then marked them as deleted. Note: deletion of plots is correct - I did remove that folder on the other machine.<\/p>\n<p>I\u2019m not sure how these files are tracked. They\u2019re model artifacts, there are no models.dvc file like the input dataset etc.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-13T09:50:11.353Z",
                "Answer_body":"<p>Where are they tracked? Are those tracked in <code>.dvc<\/code> files or <code>dvc.yaml<\/code>\/<code>dvc.lock<\/code> file? This may happen if <code>.dvc<\/code> or <code>dvc.lock<\/code> files are deleted. Is that the case?<\/p>\n<p>What does <code>git status<\/code> show?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-13T21:55:51.437Z",
                "Answer_body":"<p>These files are not tracked by a <code>.dvc<\/code> file, they\u2019re in the <code>dvc.yaml<\/code> \/<code>dvc.lock<\/code> file.<\/p>\n<p>One thing I noticed is when I tried to commit these files it says \"ERROR: failed to commit - unable to commit changed stage: \u2018train\u2019.<\/p>\n<p>When I use the <code>force<\/code> option I noticed that it\u2019s modified one of the deps for the train stage, specifically the folder that contains a python package.<\/p>\n<pre><code>- path: src\/multiclass_classifier\n  md5: 6e7721a0f99f31a34f8ff87509039e37.dir\n  size: 53640\n  nfiles: 39\n\n- path: src\/multiclass_classifier\n  md5: d81406ab21bfec100d1669268bc31122.dir\n  size: 52541\n  nfiles: 38\n<\/code><\/pre>\n<p>I\u2019m not sure what file(s) have changed - it doesn\u2019t seem to be a source file or I git would show a change.<\/p>\n<p>Is there a better way of setting a stage dependency to a package (installed using pip -e .)?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-14T01:49:48.722Z",
                "Answer_body":"<p>Also I decided to recreate my <code>dvc.ymal'<\/code> file using <code>dvc stage add<\/code><\/p>\n<p>One thing I noticed was when I tried to recreate the first stage<\/p>\n<pre><code class=\"lang-auto\">$ dvc stage add -n tokenizer -d data\/datasets\/raw\/dataset.csv -p tokenizer.dataset -p tokenizer.vocab_size -o models\/tokenizer.json -o models\/char_tokenizer.json src\/scripts\/tokenizer.py\n\nERROR: Output(s) outside of DVC project: models\/tokenizer.json, models\/char_tokenizer.json. See &lt;https:\/\/dvc.org\/doc\/user-guide\/managing-external-data&gt; for more info.\n<\/code><\/pre>\n<p>I ended up creating a new folder called <code>model<\/code> instead. I then got warned that metrics.json couldn\u2019t be added to the train stage as it was tracked by git. I fixed this.<\/p>\n<p>I\u2019m still seeing messages that files are deleted - it\u2019s not the same files now. It\u2019s the outputs from stage 1 (which are dependencies of stage 2). All of these files are being saved into the same folder (models).<\/p>\n<p>It\u2019s also now reporting that metrics.json is deleted<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-17T07:39:45.537Z",
                "Answer_body":"<p>I decided to start again, I removed the .dvc folder and created a new s3 bucket for the remote and re-initialised the repo and ran dvc add again.<\/p>\n<p>I did notice that at least one .dvc file changed when I ran dvc add - implying that they were somehow out of sync. In particular, the actual number of tracked files changed when I ran dvc add and compared the old and new .dvc file<\/p>\n<p>Seems to be working now<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"statu vscode vscode confus what displai sourc control sidebar there standard git section understand there section show model artifact section sai commit delet iv run experi push file show delet us explor sidebar theyr symlink cach",
        "Question_preprocessed_content":"statu vscode vscode confus what displai sourc control sidebar there standard git section understand there section show model artifact section sai commit iv run experi push file show delet us explor sidebar",
        "Question_gpt_summary_original":"The user is facing challenges with understanding the display in the SOURCE CONTROL sidebar of vscode while using DVC. The git section is clear, but the DVC section is showing all model artifacts as Committed with D (deleted) after running an experiment and doing a dvc push. The same files are also showing as deleted in the explorer sidebar, but they are present as symlinks to the cache.",
        "Question_gpt_summary":"user face challeng understand displai sourc control sidebar vscode git section clear section show model artifact commit delet run experi push file show delet explor sidebar present symlink cach",
        "Answer_original_content":"vscode file explor view file exist local post doctor output data statu granular untrack file unchang thank ye exist local open vscode valid symlink file cach statu show delet happen time updat apt latest version ad stage yaml origin stage produc model model model text transform model index json ad preced stage creat model token json model char token json folder data statu granular untrack file unchang commit chang git commit correspond file updat repo delet model model delet model token json delet model char token json delet model text transform delet model index json unchang file doctor version deb platform python linux gener glibc subproject support azur adlf knack azur ident gdrive pydriv gcsf hdf fsspec pyarrow http aiohttp aiohttp retri http aiohttp aiohttp retri oss ossf sf boto ssh sshf webdav webdav webdav webdav webhdf fsspec cach type hardlink symlink cach directori ext dev nvmenp cach local remot workspac directori ext dev nvmenp repo git second machin git sync pull show thing pull data dataset exampl model token json model char token json model model model text transform model index json model checkpoint plot model imag note pull latest model file theyr local machin particular token file machin ad stage mark delet note delet plot correct remov folder machin sure file track theyr model artifact model file like input dataset track track file yaml lock file happen lock file delet case git statu file track file theyr yaml lock file thing notic tri commit file sai error fail commit unabl commit chang stage train us forc option notic modifi dep train stage specif folder contain python packag path src multiclass classifi eaffafff dir size nfile path src multiclass classifi dabbfecdbc dir size nfile sure file chang doesnt sourc file git chang better wai set stage depend packag instal pip decid recreat ymal file stage add thing notic tri recreat stage stage add token data dataset raw dataset csv token dataset token vocab size model token json model char token json src script token error output outsid project model token json model char token json info end creat new folder call model instead got warn metric json ad train stage track git fix see messag file delet file output stage depend stage file save folder model report metric json delet decid start remov folder creat new bucket remot initialis repo ran add notic file chang ran add impli sync particular actual number track file chang ran add compar old new file work",
        "Answer_preprocessed_content":"vscode file explor view file exist local post output ye exist local open vscode valid symlink file cach statu show delet happen time updat latest version ad stage yaml origin stage produc ad preced stage creat folder second machin git sync pull show thing pull plot model imag note pull latest model file mark delet note delet plot correct remov folder machin sure file track theyr model artifact model file like input dataset track track file file happen file delet case file track file theyr file thing notic tri commit file sai error fail commit unabl commit chang stage train us option notic modifi dep train stage specif folder contain python packag sure file chang doesnt sourc file git chang better wai set stage depend packag decid recreat file thing notic tri recreat stage end creat new folder call instead got warn ad train stage track git fix see messag file delet file output stage file save folder report delet decid start remov folder creat new bucket remot repo ran add notic file chang ran add impli sync particular actual number track file chang ran add compar old new file work",
        "Answer_gpt_summary_original":"Solutions provided:\n- The user was asked to provide the output of `dvc doctor` and `dvc data status --granular --untracked-files --unchanged -vv`.\n- The files are not tracked by a `.dvc` file, they\u2019re in the `dvc.yaml`\/`dvc.lock` file.\n- The user was asked to check if the `.dvc` or `dvc.lock` files were deleted.\n- The user was asked to check the output of `git status`.\n- The user recreated the `dvc.yaml` file using `dvc stage add`.\n- The user removed the `.dvc` folder and created a new S3 bucket for the remote and re-initialized the repo and",
        "Answer_gpt_summary":"solut provid user ask provid output doctor data statu granular untrack file unchang file track file theyr yaml lock file user ask check lock file delet user ask check output git statu user recreat yaml file stage add user remov folder creat new bucket remot initi repo"
    },
    {
        "Question_title":"Allowing users to view GPU utilization in GCP Vertex AI training jobs",
        "Question_body":"<p>I am running custom training jobs using Google cloud Vertex AI. But when I enter a custom training job page, the GPU utilization display is not shown, instead, there is a message saying &quot;you don't have access to this data.&quot;\n<a href=\"https:\/\/i.stack.imgur.com\/t0D2M.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/t0D2M.jpg\" alt=\"enter image description here\" \/><\/a>\nI would appreciate help finding the right IAM role which will allow me to view the GPU utilization.\nThanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660033318073,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":40.0,
        "Answer_body":"<p>You can use <a href=\"https:\/\/cloud.google.com\/monitoring\/access-control#mon_roles_desc\" rel=\"nofollow noreferrer\"><code>monitoring.viewer<\/code><\/a> IAM role to display both CPU and GPU utilization in GCP Vertex AI training jobs on top of <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/access-control#predefined-roles\" rel=\"nofollow noreferrer\"><code>aiplatform.viewer<\/code><\/a> IAM role.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jbqyl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jbqyl.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73288631",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1660101916403,
        "Question_original_content":"allow user view gpu util gcp train job run custom train job googl cloud enter custom train job page gpu util displai shown instead messag sai access data appreci help find right iam role allow view gpu util thank",
        "Question_preprocessed_content":"allow user view gpu util gcp train job run custom train job googl cloud enter custom train job page gpu util displai shown instead messag sai access appreci help find right iam role allow view gpu util thank",
        "Question_gpt_summary_original":"The user is facing challenges in viewing GPU utilization while running custom training jobs using Google cloud Vertex AI. The GPU utilization display is not shown and the user is unable to access the data. The user is seeking help in finding the right IAM role to view the GPU utilization.",
        "Question_gpt_summary":"user face challeng view gpu util run custom train job googl cloud gpu util displai shown user unabl access data user seek help find right iam role view gpu util",
        "Answer_original_content":"us monitor viewer iam role displai cpu gpu util gcp train job aiplatform viewer iam role",
        "Answer_preprocessed_content":"us iam role displai cpu gpu util gcp train job iam role",
        "Answer_gpt_summary_original":"Solution: The user can use the \"monitoring.viewer\" IAM role along with the \"aiplatform.viewer\" IAM role to display both CPU and GPU utilization in GCP Vertex AI training jobs.",
        "Answer_gpt_summary":"solut user us monitor viewer iam role aiplatform viewer iam role displai cpu gpu util gcp train job"
    },
    {
        "Question_title":"How can I save more metadata on an MLFlow model",
        "Question_body":"<p>I am trying to save a model to MLFlow, but as I have a custom prediction pipeline to retrieve data, I need to save extra metadata into the model.<\/p>\n<p>I tried using my custom signature class, which It does the job correctly and saves the model with the extra metadata in the MLModel file (YAML format). But when want to load the model from the MLFlow registry, the signature is not easy accesible.<\/p>\n<pre><code>mlflow.sklearn.log_model(model, &quot;model&quot;, signature = signature)\n<\/code><\/pre>\n<p>I've also tried to save an extra dictionary at the log_model function, but it saves it in the conda.yaml file:<\/p>\n<pre><code>mlflow.sklearn.log_model(model, &quot;model&quot;, {&quot;metadata1&quot;:&quot;value1&quot;, &quot;metadata2&quot;:&quot;value2&quot;})\n<\/code><\/pre>\n<p>Should I make my own flavour? Or my own Model inheritance? I've seen <a href=\"https:\/\/github1s.com\/mlflow\/mlflow\/blob\/HEAD\/mlflow\/pyfunc\/__init__.py\" rel=\"nofollow noreferrer\">here<\/a> that the PyFuncModel recieves some metadata class and an implementation to solve this, but I don't know where should I pass my own implementations to PyFuncModel on an experiment script. Here's a minimal example:<\/p>\n<pre><code>import mlflow\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\nmetadata_dic = {&quot;metadata1&quot;: &quot;value1&quot;, \n                &quot;metadata2&quot;: &quot;value2&quot;}\n\nX = np.array([[-2, -1, 0, 1, 2, 1],[-2, -1, 0, 1, 2, 1]]).T\ny = np.array([0, 0, 1, 1, 1, 0])\n\nX = pd.DataFrame(X, columns=[&quot;X1&quot;, &quot;X2&quot;])\ny = pd.DataFrame(y, columns=[&quot;y&quot;])\n\n\nmodel = LogisticRegression()\nmodel.fit(X, y)\n\nmlflow.sklearn.log_model(model, &quot;model&quot;)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1638276045390,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":323.0,
        "Answer_body":"<p>Finally, I made a class that contains every metadata and saved it as an model argument:<\/p>\n<pre><code>model = LogisticRegression()\nmodel.fit(X, y)\nmodel.metadata = ModelMetadata(**metadata_dic)\nmlflow.sklearn.log_model(model, &quot;model&quot;)\n<\/code><\/pre>\n<p>Here I lost the customizable <code>predict<\/code> process, but after reading the <code>MLFlow<\/code> documentation is not very clear how to proceed.<\/p>\n<p>If anyone finds a good approach It would be very appreciated.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70169519",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1638361888372,
        "Question_original_content":"save metadata model try save model custom predict pipelin retriev data need save extra metadata model tri custom signatur class job correctli save model extra metadata mlmodel file yaml format want load model registri signatur easi acces sklearn log model model model signatur signatur tri save extra dictionari log model function save conda yaml file sklearn log model model model metadata valu metadata valu flavour model inherit seen pyfuncmodel reciev metadata class implement solv know pass implement pyfuncmodel experi script minim exampl import import numpi import panda sklearn linear model import logisticregress metadata dic metadata valu metadata valu arrai arrai datafram column datafram column model logisticregress model fit sklearn log model model model",
        "Question_preprocessed_content":"save metadata model try save model custom predict pipelin retriev data need save extra metadata model tri custom signatur class job correctli save model extra metadata mlmodel file want load model registri signatur easi acces tri save extra dictionari function save file flavour model inherit seen pyfuncmodel reciev metadata class implement solv know pass implement pyfuncmodel experi script minim exampl",
        "Question_gpt_summary_original":"The user is facing challenges in saving extra metadata into an MLFlow model due to a custom prediction pipeline. The user has tried using a custom signature class and saving an extra dictionary at the log_model function, but the signature is not easily accessible when loading the model from the MLFlow registry. The user is considering making their own flavor or model inheritance and is seeking guidance on how to pass their own implementations to PyFuncModel on an experiment script.",
        "Question_gpt_summary":"user face challeng save extra metadata model custom predict pipelin user tri custom signatur class save extra dictionari log model function signatur easili access load model registri user consid make flavor model inherit seek guidanc pass implement pyfuncmodel experi script",
        "Answer_original_content":"final class contain metadata save model argument model logisticregress model fit model metadata modelmetadata metadata dic sklearn log model model model lost customiz predict process read document clear proce find good approach appreci",
        "Answer_preprocessed_content":"final class contain metadata save model argument lost customiz process read document clear proce find good approach appreci",
        "Answer_gpt_summary_original":"Solution: One solution mentioned in the discussion is to create a class that contains all the metadata and save it as a model argument. However, this approach may result in losing the customizable predict process. The user is seeking guidance on how to proceed with their own implementations to PyFuncModel on an experiment script. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut solut mention discuss creat class contain metadata save model argument approach result lose customiz predict process user seek guidanc proce implement pyfuncmodel experi script solut mention discuss"
    },
    {
        "Question_title":"How to add r2 and adj r2 metric in linear regression model - AzureML Studio?",
        "Question_body":"I have trained a linear regression model in AzureML studio which was created in designer as pipeline.\n\nI could not able to see R square and adj-R square metric in Evaluate Model step.\n\nCould any throw thoughts how can I add these 2 metrics to my trained model\n\n\n\n\n\n\n\n\n\nThanks\nBhaskar",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1618843927707,
        "Question_favorite_count":8.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello,\n\nSorry for the confusing. Actually, Coefficient of determination, often referred to as R2, represents the predictive power of the model as a value between 0 and 1. Zero means the model is random (explains nothing); 1 means there is a perfect fit. However, caution should be used in interpreting R2 values, as low values can be entirely normal and high values can be suspect in Azure Machine Learning Designer.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/evaluate-model#metrics-for-regression-models\n\nRegards,\nYutong",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/362850\/how-to-add-r2-metric-in-linear-regression-model-az.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-20T00:19:19.74Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nSorry for the confusing. Actually, Coefficient of determination, often referred to as R2, represents the predictive power of the model as a value between 0 and 1. Zero means the model is random (explains nothing); 1 means there is a perfect fit. However, caution should be used in interpreting R2 values, as low values can be entirely normal and high values can be suspect in Azure Machine Learning Designer.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/evaluate-model#metrics-for-regression-models\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1618877959740,
        "Question_original_content":"add adj metric linear regress model studio train linear regress model studio creat design pipelin abl squar adj squar metric evalu model step throw thought add metric train model thank bhaskar",
        "Question_preprocessed_content":"add adj metric linear regress model studio train linear regress model studio creat design pipelin abl squar squar metric evalu model step throw thought add metric train model thank bhaskar",
        "Question_gpt_summary_original":"The user is facing a challenge in adding R square and adj-R square metric to their trained linear regression model in AzureML Studio. They are seeking advice on how to add these metrics to their model.",
        "Question_gpt_summary":"user face challeng ad squar adj squar metric train linear regress model studio seek advic add metric model",
        "Answer_original_content":"hello sorri confus actual coeffici determin refer repres predict power model valu zero mean model random explain mean perfect fit caution interpret valu low valu entir normal high valu suspect design http doc microsoft com azur machin learn algorithm modul refer evalu model metric regress model regard yutong",
        "Answer_preprocessed_content":"hello sorri confus actual coeffici determin refer repres predict power model valu zero mean model random mean perfect fit caution interpret valu low valu entir normal high valu suspect design regard yutong",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to the challenge by suggesting the use of Coefficient of determination, also known as R2, to represent the predictive power of the model. The user is advised to refer to the Azure Machine Learning Designer documentation for more information on how to add this metric to their trained linear regression model.",
        "Answer_gpt_summary":"solut discuss provid solut challeng suggest us coeffici determin known repres predict power model user advis refer design document inform add metric train linear regress model"
    },
    {
        "Question_title":"Update offline run before syncing",
        "Question_body":"<p>Hi! During training, my script crashed unexpectedly and did not save the latest epoch information.  I restarted training without being aware of it, and now my epochs are offset by a large number.<\/p>\n<p>Is it possible to edit the epoch number (index) and add a certain value to each entry? I have tried opening the \u201crun_name.wandb\u201d file and I can already see the \u2018_step\u2019 variable for each entry, but I was wondering if there is a cleaner way to perform such an update.<\/p>\n<p>Thank you in advance for your help!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1658741360433,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":122.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/vandrew\">@vandrew<\/a> , I understand what you are attempting to achieve now. At this time our API doesn\u2019t support offline mode to access local log files. We do have this planned as a future feature but I can\u2019t speak to a specific timeline. At this time you will have to sync your runs first in online mode, then update metrics using the API.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/update-offline-run-before-syncing\/2794",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-25T20:38:33.756Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/vandrew\">@vandrew<\/a> , you can currently update a <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/run\">run<\/a> after it has logged using our <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#update-metrics-for-a-run-after-the-run-has-finished\">API<\/a> . Would this functionality work for your intended use case?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-27T13:28:45.321Z",
                "Answer_body":"<p>Thanks for the response, <a class=\"mention\" href=\"\/u\/mohammadbakir\">@mohammadbakir<\/a> ! Sorry for not mentioning this yet, but the run I am trying to update is an offline run. The issue I am facing is not being able to upload it to wandb before updating it, as the two resulting wandb runs have conflicting step numbers. This would lead to overwriting some data that was logged.<\/p>\n<p>The <code>wandb.Api().run()<\/code> command seems to only take as an argument a path in the form <code>&lt;entity&gt;\/&lt;project&gt;\/&lt;run_id&gt;<\/code>, so this does not seem to help with offline runs. I have also tried initializing an empty run with <code>run = wandb.init()<\/code>, after which I tried changing the run directory with <code>run.dir = \"path_to_old_run\"<\/code>, but this was not successful.<\/p>\n<p>I assume there is currently no functionality to achieve this?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-28T22:36:27.071Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/vandrew\">@vandrew<\/a> , I understand what you are attempting to achieve now. At this time our API doesn\u2019t support offline mode to access local log files. We do have this planned as a future feature but I can\u2019t speak to a specific timeline. At this time you will have to sync your runs first in online mode, then update metrics using the API.<\/p>",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-09-26T22:37:14.338Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":1659047787071,
        "Question_original_content":"updat offlin run sync train script crash unexpectedli save latest epoch inform restart train awar epoch offset larg number possibl edit epoch number index add certain valu entri tri open run file step variabl entri wonder cleaner wai perform updat thank advanc help",
        "Question_preprocessed_content":"updat offlin run sync train script crash unexpectedli save latest epoch inform restart train awar epoch offset larg number possibl edit epoch number add certain valu entri tri open file variabl entri wonder cleaner wai perform updat thank advanc help",
        "Question_gpt_summary_original":"The user's script crashed during training and did not save the latest epoch information, resulting in a large offset in epoch numbers. The user is seeking a way to edit the epoch number and add a certain value to each entry, and has attempted to do so by opening the \"run_name.wandb\" file.",
        "Question_gpt_summary":"user script crash train save latest epoch inform result larg offset epoch number user seek wai edit epoch number add certain valu entri attempt open run file",
        "Answer_original_content":"vandrew understand attempt achiev time api doesnt support offlin mode access local log file plan futur featur speak specif timelin time sync run onlin mode updat metric api",
        "Answer_preprocessed_content":"understand attempt achiev time api doesnt support offlin mode access local log file plan futur featur speak specif timelin time sync run onlin mode updat metric api",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Add'ed files are duplicated in the cache, no links",
        "Question_body":"<p>I have started using dvc by running <code>dvc init<\/code> and then <code>dvc add<\/code> for two directories and one file.<\/p>\n<p>All worked fine, all filed have been copied to the cache but I noted that the total disk usage has doubled as the original files remained in addition to the cached copies.<\/p>\n<p>The original files do not show any signs of being reflinks.<\/p>\n<p>Is this right?<\/p>\n<p>I am using Ubuntu 16.04.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1566732869315,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":671.0,
        "Answer_body":"<p><a class=\"mention\" href=\"\/u\/byoussin\">@byoussin<\/a> Does your fs support reflinks? You could check it with <code>dvc version<\/code> command. If it does not, then copies are a fallback that we use when reflinks are not supported. You could also use hardlinks or symlinks, as described in <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/config#cache\" rel=\"nofollow noopener\">https:\/\/dvc.org\/doc\/commands-reference\/config#cache<\/a> .<\/p>. <p><a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a> Thanks!<br>\nIndeed, <code>dvc version<\/code> says:<\/p>\n<blockquote>\n<p>Cache: reflink - False, hardlink - True, symlink - True<\/p>\n<\/blockquote>\n<p>It appears that normally ubuntu filesystems do not support reflinks, although some do:<br>\n<a href=\"https:\/\/techsparx.com\/linux\/disks\/ubuntu-xfs.html\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https:\/\/techsparx.com\/linux\/disks\/ubuntu-xfs.html<\/a><\/p>\n<p>Mine apparently does not.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/added-files-are-duplicated-in-the-cache-no-links\/209",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-08-26T07:32:27.897Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/byoussin\">@byoussin<\/a> Does your fs support reflinks? You could check it with <code>dvc version<\/code> command. If it does not, then copies are a fallback that we use when reflinks are not supported. You could also use hardlinks or symlinks, as described in <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/config#cache\" rel=\"nofollow noopener\">https:\/\/dvc.org\/doc\/commands-reference\/config#cache<\/a> .<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-08-26T12:26:25.844Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a> Thanks!<br>\nIndeed, <code>dvc version<\/code> says:<\/p>\n<blockquote>\n<p>Cache: reflink - False, hardlink - True, symlink - True<\/p>\n<\/blockquote>\n<p>It appears that normally ubuntu filesystems do not support reflinks, although some do:<br>\n<a href=\"https:\/\/techsparx.com\/linux\/disks\/ubuntu-xfs.html\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https:\/\/techsparx.com\/linux\/disks\/ubuntu-xfs.html<\/a><\/p>\n<p>Mine apparently does not.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"add file duplic cach link start run init add directori file work fine file copi cach note total disk usag doubl origin file remain addit cach copi origin file sign reflink right ubuntu",
        "Question_preprocessed_content":"add file duplic cach link start run directori file work fine file copi cach note total disk usag doubl origin file remain addit cach copi origin file sign reflink right ubuntu",
        "Question_gpt_summary_original":"The user has encountered a challenge where the files added through dvc are being duplicated in the cache, resulting in a doubling of disk usage. The original files do not appear to be reflinks, and the user is unsure if this is expected behavior. The user is using Ubuntu 16.04.",
        "Question_gpt_summary":"user encount challeng file ad duplic cach result doubl disk usag origin file appear reflink user unsur expect behavior user ubuntu",
        "Answer_original_content":"byoussin support reflink check version command copi fallback us reflink support us hardlink symlink describ http org doc command refer config cach kuprus thank version sai cach reflink fals hardlink true symlink true appear normal ubuntu filesystem support reflink http techsparx com linux disk ubuntu xf html appar",
        "Answer_preprocessed_content":"support reflink check command copi fallback us reflink support us hardlink symlink describ thank sai cach reflink fals hardlink true symlink true appear normal ubuntu filesystem support reflink appar",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are to use hardlinks or symlinks instead of copies when the filesystem does not support reflinks. The user can check if their filesystem supports reflinks by using the \"dvc version\" command. No personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss us hardlink symlink instead copi filesystem support reflink user check filesystem support reflink version command person opinion bias includ summari"
    },
    {
        "Question_title":"How to assign two or more time series identifier columns in Vertex AI Tabular Forecasting",
        "Question_body":"<p>I was wondering if it is possible to have more than one time series identifier column in the model? Let's assume I'd like to create a forecast at a product and store level (which the documentation suggests should be possible).<\/p>\n<p>If I select product as the series identifier, the only options I have left for store is either a covariate or an attribute and neither is applicable in this scenario.<\/p>\n<p>Would concatenating product and store and using the individual product and store code values for that concatenated ID as attributes be a solution? It doesn't feel right, but I can't see any other option - am I missing something?<\/p>\n<p>Note: I understand that this feature of Vertex AI is currently in preview and that because of that the options may be limited.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1632228068913,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":269.0,
        "Answer_body":"<p>There isn't an alternate way to assign 2 or more <strong>Time Series Identifiers<\/strong> in the <strong>Forecasting Model<\/strong> on <strong>Vertex AI<\/strong>. The &quot;<strong>Forecasting model<\/strong>&quot; is in the &quot;<strong>Preview<\/strong>&quot; <a href=\"https:\/\/cloud.google.com\/products#product-launch-stages\" rel=\"nofollow noreferrer\">Product launch stage<\/a>, as you are aware, with all consequences of that fact the options are limited. Please refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/bp-tabular#data_preparation_best_practices\" rel=\"nofollow noreferrer\">doc<\/a> for more information about the best practices for data preparation to train the forecasting model.<\/p>\n<p>As a workaround, the two columns can be concatenated and assigned a Time Series Identifier on that concatenated column, as you have mentioned in the question. This way, the concatenated column carries more contextual information into the training of the model.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1632482492492,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69269073",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1632376096768,
        "Question_original_content":"assign time seri identifi column tabular forecast wonder possibl time seri identifi column model let assum like creat forecast product store level document suggest possibl select product seri identifi option left store covari attribut applic scenario concaten product store individu product store code valu concaten attribut solut feel right option miss note understand featur current preview option limit",
        "Question_preprocessed_content":"assign time seri identifi column tabular forecast wonder possibl time seri identifi column model let assum like creat forecast product store level select product seri identifi option left store covari attribut applic scenario concaten product store individu product store code valu concaten attribut solut feel right option miss note understand featur current preview option limit",
        "Question_gpt_summary_original":"The user is facing a challenge in assigning two or more time series identifier columns in Vertex AI Tabular Forecasting. They want to create a forecast at a product and store level, but the only options available for store are either a covariate or an attribute, which are not applicable in this scenario. The user is considering concatenating product and store and using the individual product and store code values for that concatenated ID as attributes, but they are unsure if this is the right solution. The user acknowledges that this feature is currently in preview and the options may be limited.",
        "Question_gpt_summary":"user face challeng assign time seri identifi column tabular forecast want creat forecast product store level option avail store covari attribut applic scenario user consid concaten product store individu product store code valu concaten attribut unsur right solut user acknowledg featur current preview option limit",
        "Answer_original_content":"isn altern wai assign time seri identifi forecast model forecast model preview product launch stage awar consequ fact option limit refer doc inform best practic data prepar train forecast model workaround column concaten assign time seri identifi concaten column mention question wai concaten column carri contextu inform train model",
        "Answer_preprocessed_content":"isn altern wai assign time seri identifi forecast model forecast model preview product launch stage awar consequ fact option limit refer doc inform best practic data prepar train forecast model workaround column concaten assign time seri identifi concaten column mention question wai concaten column carri contextu inform train model",
        "Answer_gpt_summary_original":"Solution: The only solution mentioned in the discussion is to concatenate the two columns and assign a Time Series Identifier on that concatenated column. This workaround can carry more contextual information into the training of the model. However, it is acknowledged that this feature is currently in preview and the options may be limited.",
        "Answer_gpt_summary":"solut solut mention discuss concaten column assign time seri identifi concaten column workaround carri contextu inform train model acknowledg featur current preview option limit"
    },
    {
        "Question_title":"MLFlow Get Run information in R API and NGINX issue",
        "Question_body":"Hi all,\n\n\nGreat work on mlflow, we have been using it to run many experiments.\n\n\nI am trying to get a list of runs for a specific experiment. I am hosting mlflow in a container running on an EC2 instance in AWS.\u00a0\n\n\n\nI am using the R API.\n\n\n# set tracking URI\nmlflow_set_tracking_uri(ML_FLOW_TRACKING_URI)\n\n\n# init MLflow client object\nclient <- mlflow:::mlflow_client(ML_FLOW_TRACKING_URI)\n\n\n# extract experimment information\nd_exp <- mlflow:::mlflow_client_get_experiment(client, EXPERIMENT_ID)\n\n\n\nNow when I use this on experiments with few runs (<30) I get back the data in a named list with the data being stores in d_exp$runs\n\n\nHowever, for experiments with many runs I receive a cURL error as follow:\n\n\nError in curl::curl_fetch_memory(url, handle = handle) :\u00a0\n\u00a0 Timeout was reached: Operation timed out after 1003 milliseconds with 0 bytes received\n\n\nHas anyone had a similar\u00a0problem and happened to fix it? Or is this an issue with the API?\n\n\nNGINX Issue\n\n\nAlso another issue I have is sending requests to mlflow when I secure it with SSL certs using nGINX (with authentication) as a reverse proxy.\n\n\nIs there an example implementation on how to get this setup and working? I saw that there is a HostCreds class in the source code, but how do I initialise this?\n\n\nCurrent fix has been to whitelist the IP address, however this is obviously not a long-term solution.\n\n\nCheers,\n--\n\nVivek Katial\nData Scientist\n\nLevel 1, 155 Karangahape Road, Auckland Central,\u00a01010\nvivek....@quantiful.co.nz | \u00a00210435892\nwww.quantiful.co.nz",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1541534149000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":31.0,
        "Answer_body":"It looks like the default timeout in the R API is 1 second (per\u00a0this code). This seems relatively aggressive, especially for a remote server. I think you can change this timeout by using\n\n\noptions(\"mlflow.rest.timeout\" = 60)\n\n\nto set it to 60 seconds, for example.\n\n\nRegarding the TLS stuff, let me add Tomas -- I know he was working on improving the R client support for authenticating against remote servers.\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CABGg2YG0C8uPjWmg6G2B2rU3AZeJpM%2B-i5QGc4XfA7y1o_1NuQ%40mail.gmail.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/6nWaATd4uTw",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-11-08T13:13:45",
                "Answer_body":"It looks like the default timeout in the R API is 1 second (per\u00a0this code). This seems relatively aggressive, especially for a remote server. I think you can change this timeout by using\n\n\noptions(\"mlflow.rest.timeout\" = 60)\n\n\nto set it to 60 seconds, for example.\n\n\nRegarding the TLS stuff, let me add Tomas -- I know he was working on improving the R client support for authenticating against remote servers.\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CABGg2YG0C8uPjWmg6G2B2rU3AZeJpM%2B-i5QGc4XfA7y1o_1NuQ%40mail.gmail.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"run inform api nginx issu great work run experi try list run specif experi host contain run instanc aw api set track uri set track uri flow track uri init client object client client flow track uri extract experim inform exp client experi client experi us experi run data name list data store exp run experi run receiv curl error follow error curl curl fetch memori url handl handl timeout reach oper time millisecond byte receiv similarproblem happen fix issu api nginx issu issu send request secur ssl cert nginx authent revers proxi exampl implement setup work saw hostcr class sourc code initialis current fix whitelist address obvious long term solut cheer vivek katial data scientist level karangahap road auckland central vivek quanti quanti",
        "Question_preprocessed_content":"run inform api nginx issu great work run experi try list run specif experi host contain run instanc aw api set track uri init client object client extract experim inform us experi run data name list data store experi run receiv curl error follow error handl handl timeout reach oper time millisecond byte receiv similarproblem happen fix issu api nginx issu issu send request secur ssl cert nginx revers proxi exampl implement setup work saw hostcr class sourc code initialis current fix whitelist address obvious solut cheer vivek katial data scientist level karangahap road auckland central",
        "Question_gpt_summary_original":"The user is encountering two challenges with MLFlow. Firstly, when using the R API to get a list of runs for an experiment with many runs, they receive a cURL error due to a timeout. Secondly, when securing MLFlow with SSL certs using NGINX as a reverse proxy, they are unable to send requests and are seeking an example implementation to resolve the issue. The user has temporarily fixed the issue by whitelisting the IP address, but this is not a long-term solution.",
        "Question_gpt_summary":"user encount challeng firstli api list run experi run receiv curl error timeout secondli secur ssl cert nginx revers proxi unabl send request seek exampl implement resolv issu user temporarili fix issu whitelist address long term solut",
        "Answer_original_content":"look like default timeout api second perthi code rel aggress especi remot server think chang timeout option rest timeout set second exampl tl stuff let add toma know work improv client support authent remot server receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user cabggygcupjwmggbruazejpm iqgcxfayo nuq mail gmail com option visit http group googl com optout",
        "Answer_preprocessed_content":"look like default timeout api second rel aggress especi remot server think chang timeout set second exampl tl stuff let add toma know work improv client support authent remot server receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit",
        "Answer_gpt_summary_original":"Solution 1: The user can change the timeout in the R API by using \"options(\"mlflow.rest.timeout\" = 60)\" to set it to 60 seconds, for example.\n\nSolution 2: Tomas is working on improving the R client support for authenticating against remote servers, which may help resolve the issue with securing MLFlow with SSL certs using NGINX as a reverse proxy. No example implementation is provided in the discussion.",
        "Answer_gpt_summary":"solut user chang timeout api option rest timeout set second exampl solut toma work improv client support authent remot server help resolv issu secur ssl cert nginx revers proxi exampl implement provid discuss"
    },
    {
        "Question_title":"Sagemaker batch transform job failure for 'batchStrategy: MultiRecord' along with data processing",
        "Question_body":"<p>We are using SageMaker Batch Transform job and to fit as many records in a mini-batch as can fit within the <code>MaxPayloadInMB<\/code> limit, we are setting <code>BatchStrategy<\/code> to <code>MultiRecord<\/code> and <code>SplitType<\/code> to <code>Line<\/code>.<\/p>\n<p>Input to the SageMaker batch transform job is:<\/p>\n<pre><code>{&quot;requestBody&quot;: {&quot;data&quot;: {&quot;Age&quot;: 90, &quot;Experience&quot;: 26, &quot;Income&quot;: 30, &quot;Family&quot;: 3, &quot;CCAvg&quot;: 1}}, &quot;mName&quot;: &quot;loanprediction&quot;, &quot;mVersion&quot;: &quot;1&quot;, &quot;testFlag&quot;: &quot;false&quot;, &quot;environment&quot;: &quot;DEV&quot;, &quot;transactionId&quot;: &quot;5-687sdf87-0bc7e3cb3454dbf261ed1353&quot;, &quot;timestamp&quot;: &quot;2022-01-15T01:45:32.955Z&quot;}\n{&quot;requestBody&quot;: {&quot;data&quot;: {&quot;Age&quot;: 55, &quot;Experience&quot;: 26, &quot;Income&quot;: 450, &quot;Family&quot;: 3, &quot;CCAvg&quot;: 1}}, &quot;mName&quot;: &quot;loanprediction&quot;, &quot;mVersion&quot;: &quot;1&quot;, &quot;testFlag&quot;: &quot;false&quot;, &quot;environment&quot;: &quot;DEV&quot;, &quot;transactionId&quot;: &quot;5-69e22778-594916685f4ceca66c08bfbc&quot;, &quot;timestamp&quot;: &quot;2022-01-15T01:46:32.386Z&quot;}\n<\/code><\/pre>\n<p>This is the SageMaker batch transform job config:<\/p>\n<pre><code>apiVersion: sagemaker.aws.amazon.com\/v1\nkind: BatchTransformJob\nmetadata:\n        generateName: '...-batchtransform'\nspec:\n        batchStrategy: MultiRecord\n        dataProcessing:\n                JoinSource: Input\n                OutputFilter: $\n                inputFilter: $.requestBody\n        modelClientConfig:\n                invocationsMaxRetries: 0\n                invocationsTimeoutInSeconds: 3\n        mName: '..'\n        region: us-west-2\n        transformInput:\n                contentType: application\/json\n                dataSource:\n                        s3DataSource:\n                                s3DataType: S3Prefix\n                                s3Uri: s3:\/\/......\/part-\n                splitType: Line\n        transformOutput:\n                accept: application\/json\n                assembleWith: Line\n                kmsKeyId: '....'\n                s3OutputPath: s3:\/\/....\/batch_output\n        transformResources:\n                instanceCount: ..\n                instanceType: '..'\n<\/code><\/pre>\n<p>The SageMaker batch transform job fails with:<\/p>\n<p>Error in batch transform data-log -<\/p>\n<blockquote>\n<p>2022-01-27T00:55:39.781:[sagemaker logs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:<\/p>\n\n<p>2022-01-27T00:55:39.781:[sagemaker logs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:<\/p>\n400 Bad Request 2022-01-27T00:55:39.781:[sagemaker\nlogs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:\n<p>Failed to decode JSON object: Extra data: line 2 column 1 (char\n163)<\/p>\n<\/blockquote>\n<p><strong>Observation:<\/strong>\nThis issue occurs when we provide <code>batchStrategy: MultiRecord<\/code> in the manifest along with these data processing configs:<\/p>\n<pre><code>dataProcessing:\n        JoinSource: Input\n        OutputFilter: $\n        inputFilter: $.requestBody\n<\/code><\/pre>\n<p><strong>NOTE:<\/strong> If we put <code>batchStrategy: SingleRecord<\/code> along with the aforementioned data processing configs, it just works fine (job succeeds)!<\/p>\n<p><strong>Question:<\/strong> How can we achieve successful run with <code>batchStrategy: MultiRecord<\/code> along with the aforementioned data processing config?<\/p>\n<p>A successful output with <code>batchStrategy: SingleRecord<\/code> looks like this:<\/p>\n<blockquote>\n<p>{&quot;SageMakerOutput&quot;:{&quot;prediction&quot;:0},&quot;environment&quot;:&quot;DEV&quot;,&quot;transactionId&quot;:&quot;5-687sdf87-0bc7e3cb3454dbf261ed1353&quot;,&quot;mName&quot;:&quot;loanprediction&quot;,&quot;mVersion&quot;:&quot;1&quot;,&quot;requestBody&quot;:{&quot;data&quot;:{&quot;Age&quot;:90,&quot;CCAvg&quot;:1,&quot;Experience&quot;:26,&quot;Family&quot;:3,&quot;Income&quot;:30}},&quot;testFlag&quot;:&quot;false&quot;,&quot;timestamp&quot;:&quot;2022-01-15T01:45:32.955Z&quot;}\n{&quot;SageMakerOutput&quot;:{&quot;prediction&quot;:0},&quot;environment&quot;:&quot;DEV&quot;,&quot;transactionId&quot;:&quot;5-69e22778-594916685f4ceca66c08bfbc&quot;,&quot;mName&quot;:&quot;loanprediction&quot;,&quot;mVersion&quot;:&quot;1&quot;,&quot;requestBody&quot;:{&quot;data&quot;:{&quot;Age&quot;:55,&quot;CCAvg&quot;:1,&quot;Experience&quot;:26,&quot;Family&quot;:3,&quot;Income&quot;:450}},&quot;testFlag&quot;:&quot;false&quot;,&quot;timestamp&quot;:&quot;2022-01-15T01:46:32.386Z&quot;}\nRegion name \u2013 optional: Relevant resource ARN \u2013 optional:\narn:aws:sagemaker:us-west-2:435945521637:transform-job\/my-pipeline-9v28r-bat-e548fbfb125946528957e0f123456789<\/p>\n<\/blockquote>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1643344870580,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1643607224392,
        "Question_score":0.0,
        "Question_view_count":659.0,
        "Answer_body":"<p>When your input data is in JSON line format and you choose a SingleRecord BatchStrategy, your container will receive a single JSON payload body like below<\/p>\n<pre><code>{ &lt;some JSON data&gt; }\n<\/code><\/pre>\n<p>However, if you use MultiRecord, Batch transform will split your JSON line input (which might contain 100 lines for example) into multiple records (say 10 records) all sent at once to your container as shown below:<\/p>\n<pre><code>{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n.\n.\n.\n{ &lt;some JSON data&gt; }\n<\/code><\/pre>\n<p>Therefore your container should be able to handle such input for it to work. However, from the error message, I can see it is complaining about invalid JSON format as it reads the second row of the request.<\/p>\n<p>I also noticed that you have supplied <code>ContentType<\/code> and <code>AcceptType<\/code> as <code>application\/json<\/code> but instead should be <code>application\/jsonlines<\/code><\/p>\n<p>Could you please test your container to see if it can handle multiple JSON line records per single invocation.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70888883",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1643938529208,
        "Question_original_content":"batch transform job failur batchstrategi multirecord data process batch transform job fit record mini batch fit maxpayloadinmb limit set batchstrategi multirecord splittyp line input batch transform job requestbodi data ag experi incom famili ccavg mname loanpredict mversion testflag fals environ dev transactionid sdf bcecbdbf timestamp requestbodi data ag experi incom famili ccavg mname loanpredict mversion testflag fals environ dev transactionid fcecacbfbc timestamp batch transform job config apivers aw amazon com kind batchtransformjob metadata generatenam batchtransform spec batchstrategi multirecord dataprocess joinsourc input outputfilt inputfilt requestbodi modelclientconfig invocationsmaxretri invocationstimeoutinsecond mname region west transforminput contenttyp applic json datasourc sdatasourc sdatatyp sprefix suri splittyp line transformoutput accept applic json assemblewith line kmskeyid soutputpath batch output transformresourc instancecount instancetyp batch transform job fail error batch transform data log log ephemer dev loanpredict usw dev loanpredict pipelin fbb bcaaff txt log ephemer dev loanpredict usw dev loanpredict pipelin fbb bcaaff txt bad request log ephemer dev loanpredict usw dev loanpredict pipelin fbb bcaaff txt fail decod json object extra data line column char observ issu occur provid batchstrategi multirecord manifest data process config dataprocess joinsourc input outputfilt inputfilt requestbodi note batchstrategi singlerecord aforement data process config work fine job succe question achiev success run batchstrategi multirecord aforement data process config success output batchstrategi singlerecord look like output predict environ dev transactionid sdf bcecbdbf mname loanpredict mversion requestbodi data ag ccavg experi famili incom testflag fals timestamp output predict environ dev transactionid fcecacbfbc mname loanpredict mversion requestbodi data ag ccavg experi famili incom testflag fals timestamp region option relev resourc arn option arn aw west transform job pipelin bat efbfbef",
        "Question_preprocessed_content":"batch transform job failur batchstrategi multirecord data process batch transform job fit record fit limit set input batch transform job batch transform job config batch transform job fail error batch transform log log bad request log fail decod json object extra data line column observ issu occur provid manifest data process config note aforement data process config work fine question achiev success run aforement data process config success output look like region option relev resourc arn option",
        "Question_gpt_summary_original":"The user is encountering a failure in their SageMaker batch transform job when using the <code>batchStrategy: MultiRecord<\/code> configuration along with certain data processing configurations. The error message indicates a failure to decode a JSON object due to extra data. The user is seeking a solution to successfully run the job with the <code>batchStrategy: MultiRecord<\/code> configuration and the aforementioned data processing configurations. The job runs successfully when using the <code>batchStrategy: SingleRecord<\/code> configuration.",
        "Question_gpt_summary":"user encount failur batch transform job batchstrategi multirecord configur certain data process configur error messag indic failur decod json object extra data user seek solut successfulli run job batchstrategi multirecord configur aforement data process configur job run successfulli batchstrategi singlerecord configur",
        "Answer_original_content":"input data json line format choos singlerecord batchstrategi contain receiv singl json payload bodi like us multirecord batch transform split json line input contain line exampl multipl record record sent contain shown contain abl handl input work error messag complain invalid json format read second row request notic suppli contenttyp accepttyp applic json instead applic jsonlin test contain handl multipl json line record singl invoc",
        "Answer_preprocessed_content":"input data json line format choos singlerecord batchstrategi contain receiv singl json payload bodi like us multirecord batch transform split json line input multipl record sent contain shown contain abl handl input work error messag complain invalid json format read second row request notic suppli instead test contain handl multipl json line record singl invoc",
        "Answer_gpt_summary_original":"Solution:\n- The user should ensure that their container can handle multiple JSON line records per single invocation when using the <code>batchStrategy: MultiRecord<\/code> configuration.\n- The user should also ensure that they are using the correct <code>ContentType<\/code> and <code>AcceptType<\/code> which should be <code>application\/jsonlines<\/code> instead of <code>application\/json<\/code>.",
        "Answer_gpt_summary":"solut user ensur contain handl multipl json line record singl invoc batchstrategi multirecord configur user ensur correct contenttyp accepttyp applic jsonlin instead applic json"
    },
    {
        "Question_title":"Azure ML workspace blob structure \/ Can I safely delete these blobs?",
        "Question_body":"Hello,\n\nI am trying to figure out the folder structure of Azure ML workspace in my storage account.\nI want to be able to delete old pipeline runs and experiments that have piled up in my workspace directly from Azure Storage Explorer without breaking the system.\nMy datastores and folder structure are as follows:\n\nDatastore: workspaceartifactstore\nBlob container: azureml\nFolder structure:\n\u251c\u2500 ComputeRecord\n\u251c\u2500 Dataset\n\u251c\u2500 ExperimentRun\n\u251c\u2500 LocalUpload\n\nDatastore: workspaceblobstore (Default)\nBlob container: azureml-blobstore-(a series of numbers)\nFolder structure:\n\u251c\u2500 azureml\n\u2502 \u251c\u2500\u2500 (a series of numbers)-setup\n\u2502 \u2502 \u251c\u2500\u2500 _tracer.py\n\u2502 \u2502 \u251c\u2500\u2500 azureml_globals.py\n\u2502 \u2502 \u251c\u2500\u2500 context_managers.py\n\u2502 \u2502 \u251c\u2500\u2500 job_prep.py\n\u2502 \u2502 \u251c\u2500\u2500 log_history_status.py\n\u2502 \u2502 \u251c\u2500\u2500 request_utilities.py\n\u2502 \u2502 \u251c\u2500\u2500 run_token_provider.py\n\u2502 \u2502 \u251c\u2500\u2500 utility_context_managers.py\n\u2502 \u251c\u2500\u2500 (another series of numbers)-setup\n\u2502 \u2502 \u251c\u2500\u2500 sames files as above\n\nIt would help if I understood what does each of these containers actually store.\nI already tried to delete all blobs stored in 'workspaceblobstore', but it didn't remove any pipeline or experiment from ML Studio.\nI have a few datasets registered in my workspace, and I don't want to delete them (nor unregister them).\n\nCan I set a data retention policy on both containers in order to delete old blobs?\nCan I safely delete the blobs (folders) stored in 'workspaceartifactstore' too? Will they be recreated automatically when I run a new experiment?\nWhy are there two separate 'azureml' and 'azureml-blobstore-(a series of numbers)' containers? Is it possible to merge them?\n\nThanks.\n\nThank you.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647500925610,
        "Question_favorite_count":9.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, thanks for reaching out. I've worked on a similar inquiry and the advise is to not delete data stored in default datastore to avoid weird errors. The option to easily delete experiment runs is on the roadmap. Here's a similar thread. Feel free to raise and track feature request on ideas portal.\n\nAccording to documentation, when you create a workspace, an Azure blob container and an Azure file share are automatically registered as datastores to the workspace. They're named workspaceblobstore and workspacefilestore, respectively. The workspaceblobstore is used to store workspace artifacts and your machine learning experiment logs. It's also set as the default datastore and can't be deleted from the workspace. The workspacefilestore is used to store notebooks and R scripts authorized via compute instance.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/775834\/azure-ml-workspace-blob-structure-can-i-safely-del.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-17T14:52:56.467Z",
                "Answer_score":0,
                "Answer_body":"Hi, thanks for reaching out. I've worked on a similar inquiry and the advise is to not delete data stored in default datastore to avoid weird errors. The option to easily delete experiment runs is on the roadmap. Here's a similar thread. Feel free to raise and track feature request on ideas portal.\n\nAccording to documentation, when you create a workspace, an Azure blob container and an Azure file share are automatically registered as datastores to the workspace. They're named workspaceblobstore and workspacefilestore, respectively. The workspaceblobstore is used to store workspace artifacts and your machine learning experiment logs. It's also set as the default datastore and can't be deleted from the workspace. The workspacefilestore is used to store notebooks and R scripts authorized via compute instance.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1647528776467,
        "Question_original_content":"workspac blob structur safe delet blob hello try figur folder structur workspac storag account want abl delet old pipelin run experi pile workspac directli azur storag explor break datastor folder structur follow datastor workspaceartifactstor blob contain folder structur computerecord dataset experimentrun localupload datastor workspaceblobstor default blob contain blobstor seri number folder structur seri number setup tracer global context manag job prep log histori statu request util run token provid util context manag seri number setup same file help understood contain actual store tri delet blob store workspaceblobstor remov pipelin experi studio dataset regist workspac want delet unregist set data retent polici contain order delet old blob safe delet blob folder store workspaceartifactstor recreat automat run new experi separ blobstor seri number contain possibl merg thank thank",
        "Question_preprocessed_content":"workspac blob structur safe delet blob hello try figur folder structur workspac storag account want abl delet old pipelin run experi pile workspac directli azur storag explor break datastor folder structur follow datastor workspaceartifactstor blob contain folder structur computerecord dataset experimentrun localupload datastor workspaceblobstor blob contain seri number folder structur setup setup same file help understood contain actual store tri delet blob store workspaceblobstor remov pipelin experi studio dataset regist workspac want delet set data retent polici contain order delet old blob safe delet blob store workspaceartifactstor recreat automat run new experi separ seri number contain possibl merg thank thank",
        "Question_gpt_summary_original":"The user is facing challenges in understanding the folder structure of Azure ML workspace in their storage account and wants to delete old pipeline runs and experiments without breaking the system. They have two datastores with different blob containers and folder structures, and they are unsure of what each container stores. The user has tried deleting all blobs in one container but it did not remove any pipeline or experiment from ML Studio. They are also unsure if they can safely delete the blobs stored in 'workspaceartifactstore' and if they can merge the two separate 'azureml' and 'azureml-blobstore-(a series of numbers)' containers. The user is seeking advice on setting a data retention policy to delete old blobs.",
        "Question_gpt_summary":"user face challeng understand folder structur workspac storag account want delet old pipelin run experi break datastor differ blob contain folder structur unsur contain store user tri delet blob contain remov pipelin experi studio unsur safe delet blob store workspaceartifactstor merg separ blobstor seri number contain user seek advic set data retent polici delet old blob",
        "Answer_original_content":"thank reach work similar inquiri advis delet data store default datastor avoid weird error option easili delet experi run roadmap similar thread feel free rais track featur request idea portal accord document creat workspac azur blob contain azur file share automat regist datastor workspac name workspaceblobstor workspacefilestor respect workspaceblobstor store workspac artifact machin learn experi log set default datastor delet workspac workspacefilestor store notebook script author comput instanc",
        "Answer_preprocessed_content":"thank reach work similar inquiri advis delet data store default datastor avoid weird error option easili delet experi run roadmap similar thread feel free rais track featur request idea portal accord document creat workspac azur blob contain azur file share automat regist datastor workspac name workspaceblobstor workspacefilestor respect workspaceblobstor store workspac artifact machin learn experi log set default datastor delet workspac workspacefilestor store notebook script author comput instanc",
        "Answer_gpt_summary_original":"Solutions provided in the discussion include not deleting data stored in the default datastore to avoid errors, waiting for the option to easily delete experiment runs which is on the roadmap, and raising a feature request on the ideas portal. The documentation suggests that the workspaceblobstore is used to store workspace artifacts and machine learning experiment logs, and is set as the default datastore which cannot be deleted from the workspace. The workspacefilestore is used to store authorized notebooks and R scripts. No solution is provided for merging the two separate 'azureml' and 'azureml-blobstore-(a series of numbers)' containers or setting a data retention policy to delete old blobs.",
        "Answer_gpt_summary":"solut provid discuss includ delet data store default datastor avoid error wait option easili delet experi run roadmap rais featur request idea portal document suggest workspaceblobstor store workspac artifact machin learn experi log set default datastor delet workspac workspacefilestor store author notebook script solut provid merg separ blobstor seri number contain set data retent polici delet old blob"
    },
    {
        "Question_title":"What could I be doing wrong to get this result from Azure AutoML timeseries forecasting?",
        "Question_body":"I'm experimenting with Azure AutoML for timeseries forecasting. I have a simple two column training dataset with two years of data at hourly intervals. Column 1 is Date\/Time Column 2 is the variable I want to predict. I've done several runs of Azure AutoML and it seems to complete successfully. However, when I do a forecast and graph it something is obviously wrong. It looks like the forecast is being quantised somehow. The graph below is for the 7 days after the training set. Blue is actual and red is the forecast. This is obviously not right.\n\n\n\n\nHere is my configuration for the training (python):\n\n lags = [1,24,168]\n forecast_horizon = 7 * 24 # 7 days of hourly data\n forecasting_parameters = ForecastingParameters(\n     time_column_name=\"DateTime\",\n     forecast_horizon=forecast_horizon,\n     target_lags=lags,\n     country_or_region_for_holidays='NZ',\n     freq='H',\n     use_stl='season',\n     seasonality='auto'\n )\n automl_config = AutoMLConfig(task='forecasting',\n                              debug_log='automl_forecasting_function.log',\n                              primary_metric='normalized_root_mean_squared_error',\n                              experiment_timeout_hours=1,\n                              experiment_exit_score=0.05, \n                              enable_early_stopping=True,\n                              training_data=train_df,\n                              compute_target=compute,\n                              n_cross_validations=10,\n                              verbosity = logging.INFO,\n                              max_concurrent_iterations=19,\n                              max_cores_per_iteration=19,\n                              label_column_name=\"Output\",\n                              forecasting_parameters=forecasting_parameters,\n                              featurization=\"auto\",\n                              enable_dnn=False)\n\n\n\nThe best model from the run is a VotingEnsemble:\n\n ForecastingPipelineWrapper(pipeline=Pipeline(\n   memory=None,\n   steps=[('timeseriestransformer',\n   TimeSeriesTransformer(\n     featurization_config=None,\n     pipeline_type=<TimeSeriesPipelineType.FULL: 1>)),\n   ('prefittedsoftvotingregressor',\n   PreFittedSoftVotingRegressor(estimators=[('7',\n   Pipeline(memory=None,\n   steps=[('minmaxscaler',\n   MinMaxScaler(copy=True,\n   feature_range=(0,\n   1))...\n   DecisionTreeRegressor(ccp_alpha=0.0,\n   criterion='mse',\n   max_depth=None,\n   max_features=0.5,\n   max_leaf_nodes=None,\n   min_impurity_decrease=0.0,\n   min_impurity_split=None,\n   min_samples_leaf=0.00218714609400816,\n   min_samples_split=0.00630957344480193,\n   min_weight_fraction_leaf=0.0,\n   presort='deprecated',\n   random_state=None,\n   splitter='best'))],\n   verbose=False))],\n   weights=[0.5,\n   0.5]))],\n   verbose=False),\n   stddev=None)",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1606359622230,
        "Question_favorite_count":4.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"I tried again after turning off early stopping and letting it run for the two hours.... and got this",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/176361\/what-could-i-be-doing-wrong-to-get-this-result-fro.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-26T20:37:59.827Z",
                "Answer_score":0,
                "Answer_body":"I tried again after turning off early stopping and letting it run for the two hours.... and got this",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1606423079827,
        "Question_original_content":"wrong result azur automl timeseri forecast experi azur automl timeseri forecast simpl column train dataset year data hourli interv column date time column variabl want predict run azur automl complet successfulli forecast graph obvious wrong look like forecast quantis graph dai train set blue actual red forecast obvious right configur train python lag forecast horizon dai hourli data forecast paramet forecastingparamet time column datetim forecast horizon forecast horizon target lag lag countri region holidai freq us stl season season auto automl config automlconfig task forecast debug log automl forecast function log primari metric normal root mean squar error experi timeout hour experi exit score enabl earli stop true train data train comput target comput cross valid verbos log info max concurr iter max core iter label column output forecast paramet forecast paramet featur auto enabl dnn fals best model run votingensembl forecastingpipelinewrapp pipelin pipelin memori step timeseriestransform timeseriestransform featur config pipelin type prefittedsoftvotingregressor prefittedsoftvotingregressor estim pipelin memori step minmaxscal minmaxscal copi true featur rang decisiontreeregressor ccp alpha criterion mse max depth max featur max leaf node min impur decreas min impur split min sampl leaf min sampl split min weight fraction leaf presort deprec random state splitter best verbos fals weight verbos fals stddev",
        "Question_preprocessed_content":"wrong result azur automl timeseri forecast experi azur automl timeseri forecast simpl column train dataset year data hourli interv column column variabl want predict run azur automl complet successfulli forecast graph obvious wrong look like forecast quantis graph dai train set blue actual red forecast obvious right configur train lag dai hourli data forecastingparamet automlconfig best model run votingensembl forecastingpipelinewrapp criterion mse presort deprec splitter best verbos fals verbos fals stddev",
        "Question_gpt_summary_original":"The user is facing challenges with Azure AutoML for timeseries forecasting. Despite successful completion of several runs, the forecast graph is not accurate and appears to be quantized. The user has provided the configuration for the training, which includes a VotingEnsemble model as the best model from the run.",
        "Question_gpt_summary":"user face challeng azur automl timeseri forecast despit success complet run forecast graph accur appear quantiz user provid configur train includ votingensembl model best model run",
        "Answer_original_content":"tri turn earli stop let run hour got",
        "Answer_preprocessed_content":"tri turn earli stop let run got",
        "Answer_gpt_summary_original":"Solution: No solution is mentioned in the discussion.",
        "Answer_gpt_summary":"solut solut mention discuss"
    },
    {
        "Question_title":"Passing data between AzureML pipeline steps with OutputFileDatasetConfig: difference between 'inputs\/outputs' and 'arguments'?",
        "Question_body":"Hello,\n\nI have been successfully building and operating machine learning pipelines with Azure SDK, but there is something I fail to fully understand, and I'm wondering if my code can be simplified in some way.\n\nLet's say I have a simple pipeline with two steps: the first step processes data located at 'training_data_path' in Blob storage and then saves it to the same location, and the second step reads that processed data to do something else. So my code is as follows:\n\n def_data_store = ws.get_default_datastore()\n training_data_path = (def_data_store, 'training_data')\n    \n step_1_config = OutputFileDatasetConfig(destination = training_data_path)\n step_2_config = OutputFileDatasetConfig(destination = training_data_path)\n    \n step_1 = PythonScriptStep(\n     name=\"Step 1\",\n     script_name=\"step_1.py\",\n     source_directory=\".\/\",\n     outputs=[step_1_config],\n     arguments = [\n         \"--training-data-path\", step_1_config\n         ],    \n     compute_target=compute_target,\n     runconfig=aml_run_config,\n     allow_reuse=False\n )\n    \n step_2 = PythonScriptStep(\n     name=\"Step 2\",\n     script_name=\"step_2.py\",\n     source_directory=\".\/\",\n     inputs=[step_1_config.as_input('training_data')],\n     arguments = [\n         \"--training-data-path\", step_2_config\n         ],    \n     compute_target=compute_target,\n     runconfig=aml_run_config,\n     allow_reuse=False\n )\n\n\n\n\nI have two questions about that:\n\n1) Even though the path to the data is the same in each step, it seems like I have to create a separate OutputFileDatasetConfig object for each step. So if my pipeline has 10 steps, I will create step_1_config, step_2_config, step_3_config... Isn't there a way to reuse the same OutputFileDatasetConfig object for multiple steps?\n\n2) As far as I know, in step 2, I could delete the 'inputs' parameter and modify the 'arguments' parameter as follows, the result would be the same.\n\n step_2 = PythonScriptStep(\n     name=\"Step 2\",\n     script_name=\"step_2.py\",\n     source_directory=\".\/\",\n     arguments = [\n         \"--training-data-path\", step_1_config.as_input('training_data')\n         ],    \n     compute_target=compute_target,\n     runconfig=aml_run_config,\n     allow_reuse=False\n )\n\n\n\nMy question is: is there any difference when specifying the input using both the 'inputs' and 'arguments' parameters Vs. using only the 'arguments' parameter?\n\nThanks.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654156643450,
        "Question_favorite_count":11.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@ThierryL-3166 I think the recommendation to use separate OutputFileDatasetConfig objects for different steps is to avoid concurrent writes to a single object. As stated in a note in documentation:\n\n Concurrent writes to a OutputFileDatasetConfig will fail. Do not attempt to use a single OutputFileDatasetConfig concurrently. Do not share a single OutputFileDatasetConfig in a multiprocessing situation, such as when using distributed training.\n\nIf your steps do not run in parallel then you can try to use a single object and check though.\n\nWith respect to using inputs or arguments, If you are using the same for the same operation then arguments would pass the same as input to the script used in the same step and you would need to use an argparser to retrieve the value in the script. Whereas, inputs would provide the same value as the run objects context in the same script. The section access datasets within script provides an example here for a train and test dataset where train is passed with arguments and test with inputs.\n\n smaller_dataset = iris_dataset.take_sample(0.1, seed=seed) # 10%\n train, test = smaller_dataset.random_split(percentage=0.8, seed=seed)\n    \n # In pipeline definition script:\n # Code for demonstration only: It would be very confusing to split datasets between `arguments` and `inputs`\n train_step = PythonScriptStep(\n     name=\"train_data\",\n     script_name=\"train.py\",\n     compute_target=cluster,\n     arguments=['--training-folder', train.as_named_input('train').as_download()],\n     inputs=[test.as_named_input('test').as_download()]\n )\n    \n # In pipeline script\n parser = argparse.ArgumentParser()\n parser.add_argument('--training-folder', type=str, dest='train_folder', help='training data folder mounting point')\n args = parser.parse_args()\n training_data_folder = args.train_folder\n    \n testing_data_folder = Run.get_context().input_datasets['test']",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/874019\/passing-data-between-azureml-pipeline-steps-with-o.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-03T13:02:24.427Z",
                "Answer_score":0,
                "Answer_body":"@ThierryL-3166 I think the recommendation to use separate OutputFileDatasetConfig objects for different steps is to avoid concurrent writes to a single object. As stated in a note in documentation:\n\n Concurrent writes to a OutputFileDatasetConfig will fail. Do not attempt to use a single OutputFileDatasetConfig concurrently. Do not share a single OutputFileDatasetConfig in a multiprocessing situation, such as when using distributed training.\n\nIf your steps do not run in parallel then you can try to use a single object and check though.\n\nWith respect to using inputs or arguments, If you are using the same for the same operation then arguments would pass the same as input to the script used in the same step and you would need to use an argparser to retrieve the value in the script. Whereas, inputs would provide the same value as the run objects context in the same script. The section access datasets within script provides an example here for a train and test dataset where train is passed with arguments and test with inputs.\n\n smaller_dataset = iris_dataset.take_sample(0.1, seed=seed) # 10%\n train, test = smaller_dataset.random_split(percentage=0.8, seed=seed)\n    \n # In pipeline definition script:\n # Code for demonstration only: It would be very confusing to split datasets between `arguments` and `inputs`\n train_step = PythonScriptStep(\n     name=\"train_data\",\n     script_name=\"train.py\",\n     compute_target=cluster,\n     arguments=['--training-folder', train.as_named_input('train').as_download()],\n     inputs=[test.as_named_input('test').as_download()]\n )\n    \n # In pipeline script\n parser = argparse.ArgumentParser()\n parser.add_argument('--training-folder', type=str, dest='train_folder', help='training data folder mounting point')\n args = parser.parse_args()\n training_data_folder = args.train_folder\n    \n testing_data_folder = Run.get_context().input_datasets['test']",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1654261344427,
        "Question_original_content":"pass data pipelin step outputfiledatasetconfig differ input output argument hello successfulli build oper machin learn pipelin azur sdk fail fulli understand wonder code simplifi wai let simpl pipelin step step process data locat train data path blob storag save locat second step read process data code follow def data store default datastor train data path def data store train data step config outputfiledatasetconfig destin train data path step config outputfiledatasetconfig destin train data path step pythonscriptstep step script step sourc directori output step config argument train data path step config comput target comput target runconfig aml run config allow reus fals step pythonscriptstep step script step sourc directori input step config input train data argument train data path step config comput target comput target runconfig aml run config allow reus fals question path data step like creat separ outputfiledatasetconfig object step pipelin step creat step config step config step config isn wai reus outputfiledatasetconfig object multipl step far know step delet input paramet modifi argument paramet follow result step pythonscriptstep step script step sourc directori argument train data path step config input train data comput target comput target runconfig aml run config allow reus fals question differ specifi input input argument paramet argument paramet thank",
        "Question_preprocessed_content":"pass data pipelin step outputfiledatasetconfig differ argument hello successfulli build oper machin learn pipelin azur sdk fail fulli understand wonder code simplifi wai let simpl pipelin step step process data locat blob storag save locat second step read process data code follow outputfiledatasetconfig outputfiledatasetconfig pythonscriptstep pythonscriptstep argument question path data step like creat separ outputfiledatasetconfig object step pipelin step creat isn wai reus outputfiledatasetconfig object multipl step far know step delet input paramet modifi argument paramet follow result pythonscriptstep question differ specifi input input argument paramet argument paramet thank",
        "Question_gpt_summary_original":"The user is building machine learning pipelines with Azure SDK and has encountered two challenges. Firstly, they have to create a separate OutputFileDatasetConfig object for each step, even though the path to the data is the same in each step. Secondly, they are unsure if there is any difference when specifying the input using both the 'inputs' and 'arguments' parameters Vs. using only the 'arguments' parameter.",
        "Question_gpt_summary":"user build machin learn pipelin azur sdk encount challeng firstli creat separ outputfiledatasetconfig object step path data step secondli unsur differ specifi input input argument paramet argument paramet",
        "Answer_original_content":"thierryl think recommend us separ outputfiledatasetconfig object differ step avoid concurr write singl object state note document concurr write outputfiledatasetconfig fail attempt us singl outputfiledatasetconfig concurr share singl outputfiledatasetconfig multiprocess situat distribut train step run parallel try us singl object check respect input argument oper argument pass input script step need us argpars retriev valu script input provid valu run object context script section access dataset script provid exampl train test dataset train pass argument test input smaller dataset iri dataset sampl seed seed train test smaller dataset random split percentag seed seed pipelin definit script code demonstr confus split dataset argument input train step pythonscriptstep train data script train comput target cluster argument train folder train name input train download input test name input test download pipelin script parser argpars argumentpars parser add argument train folder type str dest train folder help train data folder mount point arg parser pars arg train data folder arg train folder test data folder run context input dataset test",
        "Answer_preprocessed_content":"think recommend us separ outputfiledatasetconfig object differ step avoid concurr write singl object state note document concurr write outputfiledatasetconfig fail attempt us singl outputfiledatasetconfig concurr share singl outputfiledatasetconfig multiprocess situat distribut train step run parallel try us singl object check respect input argument oper argument pass input script step need us argpars retriev valu script input provid valu run object context script section access dataset script provid exampl train test dataset train pass argument test input seed seed train test seed seed pipelin definit script code demonstr confus split dataset pythonscriptstep pipelin script parser type str help train data folder mount point arg",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Create separate OutputFileDatasetConfig objects for different steps to avoid concurrent writes to a single object. However, if the steps do not run in parallel, a single object can be used.\n\n2. When using the same input for the same operation, arguments would pass the same as input to the script used in the same step and an argparser would be needed to retrieve the value in the script. Whereas, inputs would provide the same value as the run objects context in the same script. An example is provided in the discussion for accessing train and test datasets within a script.",
        "Answer_gpt_summary":"possibl solut mention discuss creat separ outputfiledatasetconfig object differ step avoid concurr write singl object step run parallel singl object input oper argument pass input script step argpars need retriev valu script input provid valu run object context script exampl provid discuss access train test dataset script"
    },
    {
        "Question_title":"Multiple pipelines with single metric file",
        "Question_body":"<p>I am trying to define multiple pipelines sharing a single metrics file and this does not seem possible currently. I get an error when trying to add the same metric file to more than one pipeline.<\/p>\n<p>My use-case is, I am doing some ML experiments and would like to have multiple models that I work on in parallel, for example a SVM and a Naive Bayes model (maybe more models in future), both sharing the same data and much of the pre-processing steps. Then I have 2 pipelines, which are very similar, sharing many stages: model_svm.dvc and model_nb.dvc. So they are in the same repository and I would like that they both use the same metric file so that when I call \u201cdvc metrics show -T\u201d it will show past results from both models.<\/p>\n<p>Is this currently possible and if so how? Or am I trying to use the system in a way that was not intended, and if so could you recommend how I could restructure my project so that it fits better with how DVC was intended to be used?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1569495407825,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":767.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tania\">@tania<\/a> !<\/p>\n<p>Indeed, dvc doesn\u2019t allow multiple dvcfiles to have the same output, as it creates conflicts on <code>dvc checkout<\/code>. Are you caching that metric file with dvc? I.e. are you using <code>-m<\/code> or <code>-M<\/code> option in the respective <code>dvc run<\/code>'s for it?<\/p>\n<p>Also, note that for <code>dvc metrics show<\/code>, you don\u2019t have to use 1 metric file, you can totally have multiple of them and <code>dvc metrics show<\/code> will show both.<\/p>. <p>Thanks <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a>, I didn\u2019t realise that metrics show shows all the metrics files. This solves my problem.<\/p>\n<p>I am using -m to add the metrics files.<\/p>. <p>Glad it solves it! <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> Be sure to let us know if you have any questions.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/multiple-pipelines-with-single-metric-file\/233",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-09-26T11:40:20.925Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tania\">@tania<\/a> !<\/p>\n<p>Indeed, dvc doesn\u2019t allow multiple dvcfiles to have the same output, as it creates conflicts on <code>dvc checkout<\/code>. Are you caching that metric file with dvc? I.e. are you using <code>-m<\/code> or <code>-M<\/code> option in the respective <code>dvc run<\/code>'s for it?<\/p>\n<p>Also, note that for <code>dvc metrics show<\/code>, you don\u2019t have to use 1 metric file, you can totally have multiple of them and <code>dvc metrics show<\/code> will show both.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-09-26T11:51:06.137Z",
                "Answer_body":"<p>Thanks <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a>, I didn\u2019t realise that metrics show shows all the metrics files. This solves my problem.<\/p>\n<p>I am using -m to add the metrics files.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-09-26T12:05:22.722Z",
                "Answer_body":"<p>Glad it solves it! <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> Be sure to let us know if you have any questions.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"multipl pipelin singl metric file try defin multipl pipelin share singl metric file possibl current error try add metric file pipelin us case experi like multipl model work parallel exampl svm naiv bay model mayb model futur share data pre process step pipelin similar share stage model svm model repositori like us metric file metric past result model current possibl try us wai intend recommend restructur project fit better intend",
        "Question_preprocessed_content":"multipl pipelin singl metric file try defin multipl pipelin share singl metric file possibl current error try add metric file pipelin experi like multipl model work parallel exampl svm naiv bay model share data step pipelin similar share stage repositori like us metric file metric past result model current possibl try us wai intend recommend restructur project fit better intend",
        "Question_gpt_summary_original":"The user is facing a challenge in defining multiple pipelines that share a single metrics file. They are attempting to work on multiple ML models in parallel, with two pipelines sharing many stages. However, they are unable to add the same metric file to more than one pipeline and are seeking advice on how to restructure their project to better fit with DVC's intended use.",
        "Question_gpt_summary":"user face challeng defin multipl pipelin share singl metric file attempt work multipl model parallel pipelin share stage unabl add metric file pipelin seek advic restructur project better fit intend us",
        "Answer_original_content":"tania doesnt allow multipl file output creat conflict checkout cach metric file option respect run note metric dont us metric file total multipl metric thank kuprus didnt realis metric show metric file solv problem add metric file glad solv sure let know question",
        "Answer_preprocessed_content":"doesnt allow multipl file output creat conflict cach metric file option respect note dont us metric file total multipl thank didnt realis metric show metric file solv problem add metric glad solv sure let know question",
        "Answer_gpt_summary_original":"Solution:\n- The user can use multiple metrics files with the `dvc metrics show` command.\n- The user should cache the metric file with DVC by using the `-m` or `-M` option in the respective `dvc run` commands.",
        "Answer_gpt_summary":"solut user us multipl metric file metric command user cach metric file option respect run command"
    },
    {
        "Question_title":"Azure Machine Learning - Memory Error while creating dataframe",
        "Question_body":"<p>I am getting memory error while creating simple dataframe read from CSV file on Azure Machine Learning using notebook VM as compute instance. The VM has config of DS 13 56gb RAM, 8vcpu, 112gb storage on Ubuntu (Linux (ubuntu 16.04). CSV file is 5gb file. <\/p>\n\n<pre><code>blob_service = BlockBlobService(account_name,account_key)\nblobstring = blob_service.get_blob_to_text(container,filepath).content\ndffinaldata = pd.read_csv(StringIO(blobstring), sep=',')\n<\/code><\/pre>\n\n<p>What I am doing wrong here ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579544749467,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1579556126092,
        "Question_score":1.0,
        "Question_view_count":507.0,
        "Answer_body":"<p>you need to provide the right encoding when calling get_blob_to_text, please refer to the <a href=\"https:\/\/github.com\/Azure\/azure-storage-python\/blob\/master\/samples\/blob\/block_blob_usage.py#L390\" rel=\"nofollow noreferrer\">sample<\/a>.<\/p>\n\n<p>The code below is what  normally use for reading data file in blob storages. Basically, you can use blob\u2019s url along with sas token and use a request method. However, You might want to edit the \u2018for loop\u2019 depending what types of data you have (e.g. csv, jpg, and etc).<\/p>\n\n<p>-- Python code below --<\/p>\n\n<pre><code>import requests\nfrom azure.storage.blob import BlockBlobService, BlobPermissions\nfrom azure.storage.blob.baseblobservice import BaseBlobService\nfrom datetime import datetime, timedelta\n\naccount_name = '&lt;account_name&gt;'\naccount_key = '&lt;account_key&gt;'\ncontainer_name = '&lt;container_name&gt;'\n\nblob_service=BlockBlobService(account_name,account_key)\ngenerator = blob_service.list_blobs(container_name)\n\nfor blob in generator:\n    url = f\"https:\/\/{account_name}.blob.core.windows.net\/{container_name}\"\n    service = BaseBlobService(account_name=account_name, account_key=account_key)\n    token = service.generate_blob_shared_access_signature(container_name, img_name, permission=BlobPermissions.READ, expiry=datetime.utcnow() + timedelta(hours=1),)\n    url_with_sas = f\"{url}?{token}\"\n    response = requests.get(url_with_sas)\n<\/code><\/pre>\n\n<p>Please follow the below link to read data on Azure Blob Storage.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1579586200472,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59829017",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1579583849896,
        "Question_original_content":"memori error creat datafram get memori error creat simpl datafram read csv file notebook comput instanc config ram vcpu storag ubuntu linux ubuntu csv file file blob servic blockblobservic account account kei blobstr blob servic blob text contain filepath content dffinaldata read csv stringio blobstr sep wrong",
        "Question_preprocessed_content":"memori error creat datafram get memori error creat simpl datafram read csv file notebook comput instanc config ram vcpu storag ubuntu csv file file wrong",
        "Question_gpt_summary_original":"The user is encountering a memory error while creating a dataframe from a 5GB CSV file on Azure Machine Learning using a notebook VM as a compute instance with a configuration of DS 13 56gb RAM, 8vcpu, 112gb storage on Ubuntu (Linux (ubuntu 16.04). The user is seeking assistance in identifying the issue.",
        "Question_gpt_summary":"user encount memori error creat datafram csv file notebook comput instanc configur ram vcpu storag ubuntu linux ubuntu user seek assist identifi issu",
        "Answer_original_content":"need provid right encod call blob text refer sampl code normal us read data file blob storag basic us blob url sa token us request method want edit loop depend type data csv jpg python code import request azur storag blob import blockblobservic blobpermiss azur storag blob baseblobservic import baseblobservic datetim import datetim timedelta account account kei contain blob servic blockblobservic account account kei gener blob servic list blob contain blob gener url http account blob core window net contain servic baseblobservic account account account kei account kei token servic gener blob share access signatur contain img permiss blobpermiss read expiri datetim utcnow timedelta hour url sa url token respons request url sa follow link read data azur blob storag http doc microsoft com azur machin learn access data",
        "Answer_preprocessed_content":"need provid right encod call refer sampl code normal us read data file blob storag basic us blob url sa token us request method want edit loop depend type data python code follow link read data azur blob storag",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to the challenge by suggesting that the user needs to provide the right encoding when calling get_blob_to_text. The discussion also provides a Python code that can be used for reading data files in blob storages. The code uses blob's URL along with SAS token and a request method. However, the user might need to edit the 'for loop' depending on the types of data they have. The discussion also provides a link to the Microsoft documentation on how to access data on Azure Blob Storage.",
        "Answer_gpt_summary":"solut discuss provid solut challeng suggest user need provid right encod call blob text discuss provid python code read data file blob storag code us blob url sa token request method user need edit loop depend type data discuss provid link microsoft document access data azur blob storag"
    },
    {
        "Question_title":"Vertex AI batch predictions from file-list",
        "Question_body":"<p>I want to submit batch prediction job for a custom model (in my case it is torch model, but I think this is irrelevant in this case). So I read the documentation:\n<a href=\"https:\/\/i.stack.imgur.com\/WGa7T.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WGa7T.png\" alt=\"batch prediction from file-list\" \/><\/a><\/p>\n<p>But as there are no examples I cannot be sure what the schema of the json object which vertex ai will send to my model will be. Does someone have made this work ?<\/p>\n<p>My best guess is that the request will be with the following body:<\/p>\n<pre><code>{'instance' : &lt;b64-encoded-content-of-the-file&gt;}\n<\/code><\/pre>\n<p>But when I read the documentation (for other 'features' of vertex ai) I could imagine the following body as well:<\/p>\n<pre><code>{'instance': {'b64' : &lt;b64-encoded-content-of-the-file&gt;}}\n<\/code><\/pre>\n<p>Does somebody actually know ?<\/p>\n<p>Another thing I did is to make a 'fake-model' which returns the request it gets ... when I submit the batch-prediction job it actually finishes successfully but when I check the output file it is empty ... so ... I actually need help\/more time to think of other ways to decipher vertex ai docs.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625948141620,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":611.0,
        "Answer_body":"<p>Vertex AI <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#request_requirements\" rel=\"nofollow noreferrer\">custom container<\/a> should wrap a service with an endpoint (predict) for receiving a list of instances, each is a json serializable object<\/p>\n<pre><code>{'instances': [{'b64' : &lt;b64-encoded-content-of-the-file1&gt;}, {'b64' : &lt;b64-encoded-content-of-the-file1&gt;}, ...]}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68331232",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1626781802423,
        "Question_original_content":"batch predict file list want submit batch predict job custom model case torch model think irrelev case read document exampl sure schema json object send model work best guess request follow bodi instanc read document featur imagin follow bodi instanc somebodi actual know thing fake model return request get submit batch predict job actual finish successfulli check output file actual need help time think wai deciph doc thank advanc",
        "Question_preprocessed_content":"batch predict want submit batch predict job custom model read document exampl sure schema json object send model work best guess request follow bodi read document imagin follow bodi somebodi actual know thing return request get submit job actual finish successfulli check output file actual need time think wai deciph doc thank advanc",
        "Question_gpt_summary_original":"The user is facing challenges in submitting a batch prediction job for a custom model in Vertex AI due to the lack of examples and uncertainty about the schema of the JSON object that Vertex AI will send to the model. The user's best guess is that the request will be with the body containing the b64-encoded content of the file, but there is also a possibility that the body will contain a nested JSON object. The user attempted to create a fake model to decipher the Vertex AI docs but encountered an issue where the output file was empty.",
        "Question_gpt_summary":"user face challeng submit batch predict job custom model lack exampl uncertainti schema json object send model user best guess request bodi contain encod content file possibl bodi contain nest json object user attempt creat fake model deciph doc encount issu output file",
        "Answer_original_content":"custom contain wrap servic endpoint predict receiv list instanc json serializ object instanc",
        "Answer_preprocessed_content":"custom contain wrap servic endpoint receiv list instanc json serializ object",
        "Answer_gpt_summary_original":"Solution: The solution provided in the discussion is that the custom container in Vertex AI should wrap a service with an endpoint (predict) for receiving a list of instances, each of which is a JSON serializable object. The JSON object should contain a list of instances, and each instance should contain the b64-encoded content of the file.",
        "Answer_gpt_summary":"solut solut provid discuss custom contain wrap servic endpoint predict receiv list instanc json serializ object json object contain list instanc instanc contain encod content file"
    },
    {
        "Question_title":"Automated machine learning model deployment issue",
        "Question_body":"So I'm having an issue with setting up an endpoint for a machine learning model which was trained using Azure AutoML. When I try to test the deployed model, I get an error saying that the service is temporarily unavailable. After looking online, I found that this might happen because of an error in the run() function in the entry script.\n\nWhen I try to test the entry script on a notebook in Azure ML studio, on a fresh compute instance, there are two problems:\nFirst I get the error: AttributeError: 'MSIAuthentication' object has no attribute 'get_token'\nWhich is solved by running: pip install azureml-core\n\nThen I get the error: ModuleNotFoundError: No module named 'azureml.automl.runtime'\nWhich I try to solve using: pip install azureml-automl-runtime\nBut this throws a lot of incompatibility errors during the installation. When I then try to run the entry script I get an error with the message: \"Failed while applying learned transformations.\"\n\nSo I setup a new virtual environment on my local machine in which I only installed azure-automl-runtime. Using that setup the entry script works perfectly fine. So I created a custom environment in Azure ML studio using the conda file of that local virtual environment. Unfortunatly I still get the error \"service temporarily unavailable\" when trying to test the endpoint.\n\nI have a feeling the default Azure ML containers are incompatible with azureml-automl-runtime, since installing this on a ML studio notebook also throws a lot of errors.\n\nI feel like there should be an elegant way to deploy an AutoML model, am I doing something wrong here?\n\n\n\n\n\nUpdate: I found out I didn't change the environment for the endpoint, so that is why I was getting the same error probably. When using the custom environment I got errors from gunicorn, so I also added that package to the environment. Now I get the following error:\n\n       File \"\/var\/azureml-server\/entry.py\", line 1, in <module>\n     import create_app\n   File \"\/var\/azureml-server\/create_app.py\", line 4, in <module>\n     from routes_common import main\n   File \"\/var\/azureml-server\/routes_common.py\", line 39, in <module>\n     from azure.ml.api.exceptions.ClientSideException import ClientSideException\n ModuleNotFoundError: No module named 'azure.ml'\n\n\n\n\nSo what do I install to fix this? Is there a list somewhere of required packages for an ML model endpoint?",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1627371604967,
        "Question_favorite_count":8.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"I managed to fix the issue with the environment by just adding everything that would throw an error. Then I found out the return value has to be a json\/dict object, which if not done throws the exact same 'service temporarily unavailable' error.\n\nBut my issue with the confusing curated environments and azureml-automl-runtime in ML studio notebooks remain. Maybe this is worth looking into @ramr-msft .",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/490809\/automated-machine-learning-model-deployment-issue.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-07-28T09:19:56.087Z",
                "Answer_score":1,
                "Answer_body":"I managed to fix the issue with the environment by just adding everything that would throw an error. Then I found out the return value has to be a json\/dict object, which if not done throws the exact same 'service temporarily unavailable' error.\n\nBut my issue with the confusing curated environments and azureml-automl-runtime in ML studio notebooks remain. Maybe this is worth looking into @ramr-msft .",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-07-27T13:52:34.27Z",
                "Answer_score":0,
                "Answer_body":"@Hidde-5466 Thanks, Can you try this notebook for deployment and if that works for you (it should), compare with your code?\nhttps:\/\/github.com\/CESARDELATORRE\/Easy-AutoML-MLOps\/blob\/master\/notebooks\/5-automl-model-service-deployment-and-inference\/automl-model-service-deployment-and-inference-safe-driver-classifier.ipynb\n\nYou\u2019ll first need to train and register the model with this previous notebook using a pipeline:\nhttps:\/\/github.com\/CESARDELATORRE\/Easy-AutoML-MLOps\/blob\/master\/notebooks\/4-automlstep-pipeline-run\/automlstep-pipeline-run-safe-driver-classifier.ipynb\n\nYou can also use the notebook with a simple AutoML remote run, but you might need to change the name of the model when registering it in the Workspace since it\u2019s a different name to what the deployment notebook is using:\nhttps:\/\/github.com\/CESARDELATORRE\/Easy-AutoML-MLOps\/blob\/master\/notebooks\/3-automl-remote-compute-run\/automl-remote-compute-run-safe-driver-classifier.ipynb",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":1627463996087,
        "Question_original_content":"autom machin learn model deploy issu have issu set endpoint machin learn model train azur automl try test deploi model error sai servic temporarili unavail look onlin happen error run function entri script try test entri script notebook studio fresh comput instanc problem error attributeerror msiauthent object attribut token solv run pip instal core error modulenotfounderror modul name automl runtim try solv pip instal automl runtim throw lot incompat error instal try run entri script error messag fail appli learn transform setup new virtual environ local machin instal azur automl runtim setup entri script work perfectli fine creat custom environ studio conda file local virtual environ unfortunatli error servic temporarili unavail try test endpoint feel default contain incompat automl runtim instal studio notebook throw lot error feel like eleg wai deploi automl model wrong updat chang environ endpoint get error probabl custom environ got error gunicorn ad packag environ follow error file var server entri line import creat app file var server creat app line rout common import main file var server rout common line azur api except clientsideexcept import clientsideexcept modulenotfounderror modul name azur instal fix list requir packag model endpoint",
        "Question_preprocessed_content":"autom machin learn model deploy issu have issu set endpoint machin learn model train azur automl try test deploi model error sai servic temporarili unavail look onlin happen error run function entri script try test entri script notebook studio fresh comput instanc problem error attributeerror msiauthent object attribut solv run pip instal core error modulenotfounderror modul name try solv pip instal throw lot incompat error instal try run entri script error messag fail appli learn setup new virtual environ local machin instal setup entri script work perfectli fine creat custom environ studio conda file local virtual environ unfortunatli error servic temporarili unavail try test endpoint feel default contain incompat instal studio notebook throw lot error feel like eleg wai deploi automl model wrong updat chang environ endpoint get error probabl custom environ got error gunicorn ad packag environ follow error file line import file line import main file line import clientsideexcept modulenotfounderror modul name instal fix list requir packag model endpoint",
        "Question_gpt_summary_original":"The user is facing challenges in setting up an endpoint for a machine learning model trained using Azure AutoML. The deployed model is showing a \"service temporarily unavailable\" error, which may be due to an error in the run() function in the entry script. The user encountered errors while testing the entry script on a notebook in Azure ML studio, which were solved by installing azureml-core and azureml-automl-runtime. However, the user still faced compatibility issues and errors while testing the endpoint. The user is unsure if they are doing something wrong and is seeking guidance on the required packages for an ML model endpoint.",
        "Question_gpt_summary":"user face challeng set endpoint machin learn model train azur automl deploi model show servic temporarili unavail error error run function entri script user encount error test entri script notebook studio solv instal core automl runtim user face compat issu error test endpoint user unsur wrong seek guidanc requir packag model endpoint",
        "Answer_original_content":"manag fix issu environ ad throw error return valu json dict object throw exact servic temporarili unavail error issu confus curat environ automl runtim studio notebook remain mayb worth look ramr msft",
        "Answer_preprocessed_content":"manag fix issu environ ad throw error return valu object throw exact servic temporarili unavail error issu confus curat environ studio notebook remain mayb worth look",
        "Answer_gpt_summary_original":"Solution: The user fixed the issue with the environment by adding all the packages that would throw an error. They also found out that the return value has to be a json\/dict object, which if not done throws the same 'service temporarily unavailable' error. However, there were no solutions provided for the confusion with curated environments and azureml-automl-runtime in ML studio notebooks.",
        "Answer_gpt_summary":"solut user fix issu environ ad packag throw error return valu json dict object throw servic temporarili unavail error solut provid confus curat environ automl runtim studio notebook"
    },
    {
        "Question_title":"Separate data in data explorer and use as datastore",
        "Question_body":"Hello,\n\nWe are sending data from IoT Central to Event Hubs and then to Data Explorer, with the hopes of then sending the data to Azure Machine Learning.\n\nIn order to send data from Event Hubs to Data Explorer it needs a data ingestion into a table on data explorer.\n\nFor this data ingestion, it needs a json mapping.\n\nWe could ingest the data, but the message from the iot central data goes to event hubs that goes to data explorer carries the telemetry data as a dynamic type (a json inside a json).\n\n (\"telemetry\":{\"Temp:\"37\",\"Vol\":\"97\"})\n\n\n\n\nWe want to separate the telemetry data in different columns.\n\nSo Temp will have one column and Vol another.\n\nI am wondering how that can be done?\n\nAnd additionally, since we would like to send the data to ML, can data explorer be used as a datastore in ML?\n\nThanks!!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619036349177,
        "Question_favorite_count":12.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @yjay-4307,\n\nYou can use parse operator - Evaluates a string expression and parses its value into one or more calculated columns. The calculated columns will have nulls, for unsuccessfully parsed strings.\n\nFor more details, refer SO thread addressing similar issue.\n\nUnfortuantely, Azure Data Explorer is not a supported storage solution with Azure Machine Learning.\n\nDatastores currently support storing connection information to the storage services listed in the following matrix.\n\nFor unsupported storage solutions, and to save data egress cost during ML experiments, move your data to a supported Azure storage solution.\n\nReference: Connect to storage services on Azure - Azure Machine Learning.\n\nHope this helps. Do let us know if you any further queries.\n\nPlease don\u2019t forget to Accept Answer and Up-Vote wherever the information provided helps you, this can be beneficial to other community members.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/366532\/separate-data-in-data-explorer-and-use-as-datastor.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-22T08:36:44.29Z",
                "Answer_score":1,
                "Answer_body":"Hello @yjay-4307,\n\nYou can use parse operator - Evaluates a string expression and parses its value into one or more calculated columns. The calculated columns will have nulls, for unsuccessfully parsed strings.\n\nFor more details, refer SO thread addressing similar issue.\n\nUnfortuantely, Azure Data Explorer is not a supported storage solution with Azure Machine Learning.\n\nDatastores currently support storing connection information to the storage services listed in the following matrix.\n\nFor unsupported storage solutions, and to save data egress cost during ML experiments, move your data to a supported Azure storage solution.\n\nReference: Connect to storage services on Azure - Azure Machine Learning.\n\nHope this helps. Do let us know if you any further queries.\n\nPlease don\u2019t forget to Accept Answer and Up-Vote wherever the information provided helps you, this can be beneficial to other community members.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1619080604288,
        "Question_original_content":"separ data data explor us datastor hello send data iot central event hub data explor hope send data order send data event hub data explor need data ingest tabl data explor data ingest need json map ingest data messag iot central data goe event hub goe data explor carri telemetri data dynam type json insid json telemetri temp vol want separ telemetri data differ column temp column vol wonder addition like send data data explor datastor thank",
        "Question_preprocessed_content":"separ data data explor us datastor hello send data iot central event hub data explor hope send data order send data event hub data explor need data ingest tabl data explor data ingest need json map ingest data messag iot central data goe event hub goe data explor carri telemetri data dynam type want separ telemetri data differ column temp column vol wonder addition like send data data explor datastor thank",
        "Question_gpt_summary_original":"The user is facing challenges in separating telemetry data in different columns in Data Explorer. The data is being sent from IoT Central to Event Hubs and then to Data Explorer, but the telemetry data is in a dynamic type (a json inside a json). The user needs to ingest the data into a table on Data Explorer with a json mapping. Additionally, the user is unsure if Data Explorer can be used as a datastore in Azure Machine Learning.",
        "Question_gpt_summary":"user face challeng separ telemetri data differ column data explor data sent iot central event hub data explor telemetri data dynam type json insid json user need ingest data tabl data explor json map addition user unsur data explor datastor",
        "Answer_original_content":"hello yjai us pars oper evalu string express pars valu calcul column calcul column null unsuccessfulli pars string detail refer thread address similar issu unfortuant azur data explor support storag solut datastor current support store connect inform storag servic list follow matrix unsupport storag solut save data egress cost experi data support azur storag solut refer connect storag servic azur hope help let know queri dont forget accept answer vote inform provid help benefici commun member",
        "Answer_preprocessed_content":"hello us pars oper evalu string express pars valu calcul column calcul column null unsuccessfulli pars string detail refer thread address similar issu unfortuant azur data explor support storag solut datastor current support store connect inform storag servic list follow matrix unsupport storag solut save data egress cost experi data support azur storag solut refer connect storag servic azur hope help let know queri dont forget accept answer inform provid help benefici commun member",
        "Answer_gpt_summary_original":"Solution:\n- The user can use the parse operator to separate telemetry data in different columns in Data Explorer.\n- Azure Data Explorer is not a supported storage solution with Azure Machine Learning. The user needs to move their data to a supported Azure storage solution.",
        "Answer_gpt_summary":"solut user us pars oper separ telemetri data differ column data explor azur data explor support storag solut user need data support azur storag solut"
    },
    {
        "Question_title":"Save the result of a query in a BigQuery Table, in Cloud Storage",
        "Question_body":"<p>I would like to know what is the OPTIMAL way to store the result of a Google BigQuery table query, to Google Cloud storage. My code, which is currently being run in some Jupyter Notebook (in Vertex AI Workbench, same project than both the BigQuery data source, as well as the Cloud Storage destination), looks as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># CELL 1 OF 2\n\nfrom google.cloud import bigquery\nbqclient = bigquery.Client()\n\n# The query string can vary:\nquery_string = &quot;&quot;&quot;\n        SELECT *  \n        FROM `my_project-name.my_db.my_table` \n        LIMIT 2000000\n        &quot;&quot;&quot;\n\ndataframe = (\n    bqclient.query(query_string)\n    .result()\n    .to_dataframe(\n        create_bqstorage_client=True,\n    )\n)\nprint(&quot;Dataframe shape: &quot;, dataframe.shape)\n\n# CELL 2 OF 2:\n\nimport pandas as pd\ndataframe.to_csv('gs:\/\/my_bucket\/test_file.csv', index=False)\n<\/code><\/pre>\n<p>This code takes around 7.5 minutes to successfully complete.<\/p>\n<p><strong>Is there a more OPTIMAL way to achive what was done above?<\/strong> (It would mean <em>faster<\/em>, but maybe something else could be improved).<\/p>\n<p>Some additional notes:<\/p>\n<ol>\n<li>I want to run it &quot;via a Jupyter Notebook&quot; (in Vertex AI Workbench), because sometimes some data preprocessing, or special filtering must be done, which cannot be easily accomplished via SQL queries.<\/li>\n<li>For the first part of the code, I have discarded <a href=\"https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_gbq.html\" rel=\"nofollow noreferrer\">pandas.read_gbq<\/a>, as it was giving me some weird EOF errors, when (experimentally) &quot;storing as .CSV and reading back&quot;.<\/li>\n<li>Intuitively, I would focus the optimization efforts in the second half of the code (<code>CELL 2 OF 2<\/code>), as the first one was borrowed from <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/bigquery-storage-python-pandas\" rel=\"nofollow noreferrer\">the official Google documentation<\/a>. I have tried <a href=\"https:\/\/stackoverflow.com\/a\/57404119\/16706763\">this<\/a> but it does not work, however in the same thread <a href=\"https:\/\/stackoverflow.com\/a\/60644694\/16706763\">this<\/a> option worked OK.<\/li>\n<li>It is likley that this code will be included in some Docker image afterwards, so &quot;as little libraries as possible&quot; must be used.<\/li>\n<\/ol>\n<p>Thank you.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_time":1651600671333,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1409.0,
        "Answer_body":"<p>After some experiments, I think I have got to a solution for my original post. First, the updated code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd  # Just one library is imported this time\n\n# This SQL query can vary, modify it to match your needs\nquery_string = &quot;&quot;&quot;\nSELECT *\nFROM `my_project.my_db.my_table`\nLIMIT 2000000\n&quot;&quot;&quot;\n\n# One liner to query BigQuery data.\ndownloaded_dataframe = pd.read_gbq(query_string, dialect='standard', use_bqstorage_api=True)\n\n# Data processing (OPTIONAL, modify it to match your needs)\n# I won't do anything this time, just upload the previously queried data\n\n# Data store in GCS\ndownloaded_dataframe.to_csv('gs:\/\/my_bucket\/uploaded_data.csv', index=False)\n<\/code><\/pre>\n<p>Some final notes:<\/p>\n<ol>\n<li>I have not done an &quot;in-depth research&quot; about the processing speed VS the number of rows existing in a BigQuery table, however I saw that the processing time with the updated code and the original query, now takes ~6 minutes; that's enough for the time being. <em>This answer might have some room for further improvements<\/em> therefore, but it's better than the original situation.<\/li>\n<li>The EOF error I mentioned in  my original post was: <code>ParserError: Error tokenizing data. C error: EOF inside string starting at row 70198<\/code>. In the end I got to realize that it did not have anything to do with <a href=\"https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_gbq.html\" rel=\"nofollow noreferrer\">pandas_gbq<\/a> function, but with &quot;how I was saving the data&quot;. See, <em>I was 'experimentally' storing the .csv file in the Vertex AI Workbench local storage, then downloading it to my local device, and when trying to open that data from my local device, I kept stumbling upon that error, however not getting the same when downloading the .csv data from Cloud Storage<\/em> ... Why? Well, it happens that if you download the .csv data &quot;very quickly&quot; after &quot;it gets generated&quot; (i.e., after few seconds), from Vertex AI Workbench local storage, the data is simply still incomplete, but it does not give any error or warning message: it will simply &quot;let you start with the download&quot;. For this reason, I think it is safer to export your data to Cloud Storage, and then download safely from there. This behaviour is more noticeable on large files (i.e. my own generated file, which had ~3.1GB in size).<\/li>\n<\/ol>\n<p>Hope this helps.<\/p>\n<p>Thank you.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1651696486067,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72103557",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1651612810316,
        "Question_original_content":"save result queri bigqueri tabl cloud storag like know optim wai store result googl bigqueri tabl queri googl cloud storag code current run jupyt notebook workbench project bigqueri data sourc cloud storag destin look follow cell googl cloud import bigqueri bqclient bigqueri client queri string vari queri string select project tabl limit datafram bqclient queri queri string result datafram creat bqstorag client true print datafram shape datafram shape cell import panda datafram csv bucket test file csv index fals code take minut successfulli complet optim wai achiv mean faster mayb improv addit note want run jupyt notebook workbench data preprocess special filter easili accomplish sql queri code discard panda read gbq give weird eof error experiment store csv read intuit focu optim effort second half code cell borrow offici googl document tri work thread option work liklei code includ docker imag littl librari possibl thank",
        "Question_preprocessed_content":"save result queri bigqueri tabl cloud storag like know optim wai store result googl bigqueri tabl queri googl cloud storag code current run jupyt notebook look follow code take minut successfulli complet optim wai achiv addit note want run jupyt notebook data preprocess special filter easili accomplish sql queri code discard give weird eof error store csv read intuit focu optim effort second half code borrow offici googl document tri work thread option work liklei code includ docker imag littl librari possibl thank",
        "Question_gpt_summary_original":"The user is seeking advice on the optimal way to store the result of a Google BigQuery table query to Google Cloud storage. They are currently using a Jupyter Notebook in Vertex AI Workbench and have encountered issues with pandas.read_gbq. They believe that the optimization efforts should focus on the second half of the code and want to use as few libraries as possible as the code will be included in a Docker image.",
        "Question_gpt_summary":"user seek advic optim wai store result googl bigqueri tabl queri googl cloud storag current jupyt notebook workbench encount issu panda read gbq believ optim effort focu second half code want us librari possibl code includ docker imag",
        "Answer_original_content":"experi think got solut origin post updat code import panda librari import time sql queri vari modifi match need queri string select project tabl limit liner queri bigqueri data download datafram read gbq queri string dialect standard us bqstorag api true data process option modifi match need won time upload previous queri data data store gc download datafram csv bucket upload data csv index fals final note depth research process speed number row exist bigqueri tabl saw process time updat code origin queri take minut time answer room improv better origin situat eof error mention origin post parsererror error token data error eof insid string start row end got realiz panda gbq function save data experiment store csv file workbench local storag download local devic try open data local devic kept stumbl error get download csv data cloud storag happen download csv data quickli get gener second workbench local storag data simpli incomplet error warn messag simpli let start download reason think safer export data cloud storag download safe behaviour notic larg file gener file size hope help thank",
        "Answer_preprocessed_content":"experi think got solut origin post updat code final note research process speed number row exist bigqueri tabl saw process time updat code origin queri take minut time answer room improv better origin situat eof error mention origin post end got realiz function save data experiment store csv file workbench local storag download local devic try open data local devic kept stumbl error get download csv data cloud storag happen download csv data quickli get gener workbench local storag data simpli incomplet error warn messag simpli let start download reason think safer export data cloud storag download safe behaviour notic larg file hope help thank",
        "Answer_gpt_summary_original":"Solution:\n- The user updated their code to use only one library, pandas, to query BigQuery data and store it in Google Cloud Storage.\n- They also recommend exporting data to Cloud Storage instead of downloading it from Vertex AI Workbench local storage to avoid incomplete data issues.",
        "Answer_gpt_summary":"solut user updat code us librari panda queri bigqueri data store googl cloud storag recommend export data cloud storag instead download workbench local storag avoid incomplet data issu"
    },
    {
        "Question_title":"Why did it take so long to create endpoint with AWS Sagemaker using Boto3?",
        "Question_body":"<p>It took 45 minutes to create my endpoint from the stored endpoint configuration. (I tested it and it works too). This is the first time that I've used boto3 to do this, whereas previously I just used the Sagemaker web GUI to create an endpoint from endpoint configuration.  Suggestions to my code are appreciated:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nsagemaker_client = boto3.client('sagemaker')\n\nresponse = sagemaker_client.create_endpoint(\n    EndpointName='sagemaker-tensorflow-x',\n    EndpointConfigName='sagemaker-tensorflow-x'\n)\n<\/code><\/pre>\n<p>Note: I've replaced the last part of my endpoint name with <code>x<\/code>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1598917151350,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1598923606240,
        "Question_score":2.0,
        "Question_view_count":373.0,
        "Answer_body":"<p>AWS has currently <a href=\"https:\/\/status.aws.amazon.com\/\" rel=\"nofollow noreferrer\">issues<\/a> with Sagemaker:<\/p>\n<blockquote>\n<p>Increased Error Rates and Latencies for Multiple API operations<\/p>\n<\/blockquote>\n<blockquote>\n<p>5:33 PM PDT We are investigating increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. Previously created jobs and endpoints are unaffected.<\/p>\n<\/blockquote>\n<blockquote>\n<p>6:04 PM PDT We are continuing to investigate increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. Previously created jobs and endpoints are unaffected.<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rQHQC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rQHQC.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63679503",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1598923278260,
        "Question_original_content":"long creat endpoint boto took minut creat endpoint store endpoint configur test work time boto previous web gui creat endpoint endpoint configur suggest code appreci import boto client boto client respons client creat endpoint endpointnam tensorflow endpointconfignam tensorflow note replac endpoint",
        "Question_preprocessed_content":"long creat endpoint boto took minut creat endpoint store endpoint configur time boto previous web gui creat endpoint endpoint configur suggest code appreci note replac endpoint",
        "Question_gpt_summary_original":"The user encountered a challenge in creating an endpoint with AWS Sagemaker using Boto3, which took 45 minutes to complete. The user had previously used the Sagemaker web GUI to create an endpoint from endpoint configuration. The user is seeking suggestions to improve their code.",
        "Question_gpt_summary":"user encount challeng creat endpoint boto took minut complet user previous web gui creat endpoint endpoint configur user seek suggest improv code",
        "Answer_original_content":"aw current issu increas error rate latenc multipl api oper pdt investig increas error rate latenc createtrainingjob createhyperparametertuningjob createendpoint api oper east region previous creat job endpoint unaffect pdt continu investig increas error rate latenc createtrainingjob createhyperparametertuningjob createendpoint api oper east region previous creat job endpoint unaffect",
        "Answer_preprocessed_content":"aw current issu increas error rate latenc multipl api oper pdt investig increas error rate latenc createtrainingjob createhyperparametertuningjob createendpoint api oper region previous creat job endpoint unaffect pdt continu investig increas error rate latenc createtrainingjob createhyperparametertuningjob createendpoint api oper region previous creat job endpoint unaffect",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"How to change metrics computation and recompute",
        "Question_body":"<p>Hi all,<br>\nI trained several models and computed a simple metrics summary for each of them. Now, those models (and their corresponding metrics.json files) are versioned by dvc and git and there is a git tag associated with each model.<br>\nHaving the models trained and stored, I would like to enrich the metrics summary and go back and recompute it for each model \u2013 and store it in git as with the simple summary. This means the last part of the pipeline, say compute_metrics.py, will be changed and I would like to run dvc repro again. Is there a simple way of recomputing the metrics for all models (that is for all tags)?<\/p>\n<p>Thank you,<br>\nMichal<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1575979302220,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":420.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/michal.najman\">@michal.najman<\/a><br>\nSo what you are looking for is probably <code>dvc repro<\/code><br>\n<a href=\"https:\/\/dvc.org\/doc\/command-reference\/repro#repro\">docs<\/a><br>\nIf you have only modified the code responsible for metrics calculation, dvc should detect that and recalculate only metrics. If you want to be sure, you can explicitly tell dvc to reproduce metrics stage only with <code>--single-item<\/code> (eg <code>dvc repro --single-item metrics_calc.dvc<\/code>) param. I am afraid that currently, we do not support <code>repro<\/code> for multiple git revisions, which would probably have to be done manually. Maybe you want to create feature request on GitHub, with a short description how would you expect that to work?<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-change-metrics-computation-and-recompute\/278",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-12-10T17:46:20.045Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/michal.najman\">@michal.najman<\/a><br>\nSo what you are looking for is probably <code>dvc repro<\/code><br>\n<a href=\"https:\/\/dvc.org\/doc\/command-reference\/repro#repro\">docs<\/a><br>\nIf you have only modified the code responsible for metrics calculation, dvc should detect that and recalculate only metrics. If you want to be sure, you can explicitly tell dvc to reproduce metrics stage only with <code>--single-item<\/code> (eg <code>dvc repro --single-item metrics_calc.dvc<\/code>) param. I am afraid that currently, we do not support <code>repro<\/code> for multiple git revisions, which would probably have to be done manually. Maybe you want to create feature request on GitHub, with a short description how would you expect that to work?<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"chang metric comput recomput train model comput simpl metric summari model correspond metric json file version git git tag associ model have model train store like enrich metric summari recomput model store git simpl summari mean pipelin comput metric chang like run repro simpl wai recomput metric model tag thank michal",
        "Question_preprocessed_content":"chang metric comput recomput train model comput simpl metric summari model version git git tag associ model have model train store like enrich metric summari recomput model store git simpl summari mean pipelin chang like run repro simpl wai recomput metric model thank michal",
        "Question_gpt_summary_original":"The user has trained several models and computed a simple metrics summary for each of them. The models and their corresponding metrics.json files are versioned by dvc and git, and there is a git tag associated with each model. The user wants to enrich the metrics summary and recompute it for each model, which requires changing the last part of the pipeline and running dvc repro again. The user is seeking a simple way to recomputed the metrics for all models (that is for all tags).",
        "Question_gpt_summary":"user train model comput simpl metric summari model correspond metric json file version git git tag associ model user want enrich metric summari recomput model requir chang pipelin run repro user seek simpl wai recomput metric model tag",
        "Answer_original_content":"michal najman look probabl repro doc modifi code respons metric calcul detect recalcul metric want sure explicitli tell reproduc metric stage singl item repro singl item metric calc param afraid current support repro multipl git revis probabl manual mayb want creat featur request github short descript expect work",
        "Answer_preprocessed_content":"look probabl doc modifi code respons metric calcul detect recalcul metric want sure explicitli tell reproduc metric stage param afraid current support multipl git revis probabl manual mayb want creat featur request github short descript expect work",
        "Answer_gpt_summary_original":"Solutions provided: \n- The user can use `dvc repro` to recompute the metrics for all models. If only the code responsible for metrics calculation has been modified, dvc should detect that and recalculate only metrics. \n- The user can explicitly tell dvc to reproduce metrics stage only with `--single-item` parameter. \n- Currently, `repro` for multiple git revisions is not supported, and it would have to be done manually. The user is advised to create a feature request on GitHub with a short description of how they would expect it to work.",
        "Answer_gpt_summary":"solut provid user us repro recomput metric model code respons metric calcul modifi detect recalcul metric user explicitli tell reproduc metric stage singl item paramet current repro multipl git revis support manual user advis creat featur request github short descript expect work"
    },
    {
        "Question_title":"AWS SageMaker SparkML Schema Eroor: member.environment' failed to satisfy constraint",
        "Question_body":"<p>I am deploying a model onto AWS via Sagemaker:<\/p>\n\n<p>I set up my JSON schema as follow:<\/p>\n\n<pre><code>import json\nschema = {\n    \"input\": [\n        {\n            \"name\": \"V1\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V2\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V3\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V4\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V5\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V6\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V7\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V8\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V9\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V10\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V11\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V12\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V13\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V14\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V15\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V16\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V17\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V18\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V19\",\n            \"type\": \"double\"\n        }, \n                {\n            \"name\": \"V20\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V21\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V22\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V23\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V24\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V25\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V26\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V27\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V28\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"Amount\",\n            \"type\": \"double\"\n        },         \n    ],\n    \"output\": \n        {\n            \"name\": \"features\",\n            \"type\": \"double\",\n            \"struct\": \"vector\"\n        }\n}\nschema_json = json.dumps(schema)\nprint(schema_json)\n<\/code><\/pre>\n\n<p>And deployed as:<\/p>\n\n<pre><code>from sagemaker.model import Model\nfrom sagemaker.pipeline import PipelineModel\nfrom sagemaker.sparkml.model import SparkMLModel\n\nsparkml_data = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\n# passing the schema defined above by using an environment variable that sagemaker-sparkml-serving understands\nsparkml_model = SparkMLModel(model_data=sparkml_data, env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json})\nxgb_model = Model(model_data=xgb_model.model_data, image=training_image)\n\nmodel_name = 'inference-pipeline-' + timestamp_prefix\nsm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, xgb_model])\n\n    endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)\n<\/code><\/pre>\n\n<p>I got the error as below:<\/p>\n\n<p>ClientError: An error occurred (ValidationException) when calling the CreateModel operation: 1 validation error detected: Value '{SAGEMAKER_SPARKML_SCHEMA={\"input\": [{\"type\": \"double\", \"name\": \"V1\"}, {\"type\": \"double\", \"name\": \"V2\"}, {\"type\": \"double\", \"name\": \"V3\"}, {\"type\": \"double\", \"name\": \"V4\"}, {\"type\": \"double\", \"name\": \"V5\"}, {\"type\": \"double\", \"name\": \"V6\"}, {\"type\": \"double\", \"name\": \"V7\"}, {\"type\": \"double\", \"name\": \"V8\"}, {\"type\": \"double\", \"name\": \"V9\"}, {\"type\": \"double\", \"name\": \"V10\"}, {\"type\": \"double\", \"name\": \"V11\"}, {\"type\": \"double\", \"name\": \"V12\"}, {\"type\": \"double\", \"name\": \"V13\"}, {\"type\": \"double\", \"name\": \"V14\"}, {\"type\": \"double\", \"name\": \"V15\"}, {\"type\": \"double\", \"name\": \"V16\"}, {\"type\": \"double\", \"name\": \"V17\"}, {\"type\": \"double\", \"name\": \"V18\"}, {\"type\": \"double\", \"name\": \"V19\"}, {\"type\": \"double\", \"name\": \"V20\"}, {\"type\": \"double\", \"name\": \"V21\"}, {\"type\": \"double\", \"name\": \"V22\"}, {\"type\": \"double\", \"name\": \"V23\"}, {\"type\": \"double\", \"name\": \"V24\"}, {\"type\": \"double\", \"name\": \"V25\"}, {\"type\": \"double\", \"name\": \"V26\"}, {\"type\": \"double\", \"name\": \"V27\"}, {\"type\": \"double\", \"name\": \"V28\"}, {\"type\": \"double\", \"name\": \"Amount\"}], \"output\": {\"type\": \"double\", \"name\": \"features\", \"struct\": \"vector\"}}}' at 'containers.1**.<strong>member.environment' failed to satisfy constraint: Map value must satisfy constraint: [Member must have length less than or equal to 1024<\/strong>,** Member must have length greater than or equal to 0, Member must satisfy regular expression pattern: [\\S\\s]*]<\/p>\n\n<p>I try to reduce my features to 20 and it able to deploy. Just wondering how can I Pass the schema with 29 attributes?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1590448983843,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":226.0,
        "Answer_body":"<p>I do not think the environment length of 1024 limit will be increased in a short time. To work around this, you could try to rebuild the spark ml container with the <code>SAGEMAKER_SPARKML_SCHEMA<\/code> env var:<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62012264",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1600448991412,
        "Question_original_content":"sparkml schema eroor member environ fail satisfi constraint deploi model aw set json schema follow import json schema input type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl output featur type doubl struct vector schema json json dump schema print schema json deploi model import model pipelin import pipelinemodel sparkml model import sparkmlmodel sparkml data format model bucket model kei prefix model tar pass schema defin environ variabl sparkml serv understand sparkml model sparkmlmodel model data sparkml data env sparkml schema schema json xgb model model model data xgb model model data imag train imag model infer pipelin timestamp prefix model pipelinemodel model role role model sparkml model xgb model endpoint infer pipelin timestamp prefix model deploi initi instanc count instanc type xlarg endpoint endpoint got error clienterror error occur validationexcept call createmodel oper valid error detect valu sparkml schema input type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl type doubl output type doubl featur struct vector contain member environ fail satisfi constraint map valu satisfi constraint member length equal member length greater equal member satisfi regular express pattern try reduc featur abl deploi wonder pass schema attribut",
        "Question_preprocessed_content":"sparkml schema eroor fail satisfi constraint deploi model aw set json schema follow deploi got error clienterror error occur call createmodel oper valid error detect valu output fail satisfi constraint map valu satisfi constraint try reduc featur abl deploi wonder pass schema attribut",
        "Question_gpt_summary_original":"The user encountered an error while deploying a model onto AWS via Sagemaker due to a validation error. The error occurred because the member.environment failed to satisfy the constraint, which requires the map value to have a length less than or equal to 1024. The user was able to deploy the model successfully after reducing the features to 20, but is unsure how to pass the schema with 29 attributes.",
        "Question_gpt_summary":"user encount error deploi model aw valid error error occur member environ fail satisfi constraint requir map valu length equal user abl deploi model successfulli reduc featur unsur pass schema attribut",
        "Answer_original_content":"think environ length limit increas short time work try rebuild spark contain sparkml schema env var http github com aw sparkml serv contain blob master readm run imag local",
        "Answer_preprocessed_content":"think environ length limit increas short time work try rebuild spark contain env var",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to rebuild the spark ml container with the SAGEMAKER_SPARKML_SCHEMA env var. However, there are no other solutions provided.",
        "Answer_gpt_summary":"solut possibl solut mention discuss rebuild spark contain sparkml schema env var solut provid"
    },
    {
        "Question_title":"Refactor columns and features in Azure Machine Learning",
        "Question_body":"<p>Is there any way I can make my dataset features in Azure ML into something else than what it already is? <\/p>\n\n<p>I found a dataset of the Titanic ship in the sample datasets which I would like to work with but all of my columns are either a numeric feature or string feature, but I would like to categorize these. Also is there any possibility to rename the columns within my model so it\u2019s more descriptive than what I initially got? I have no clue what SibSp means for instance.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1465481281330,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1465977179120,
        "Question_score":0.0,
        "Question_view_count":297.0,
        "Answer_body":"<p>What you are doing is essentially recreating this experiment made by Raja Iqbal for the Titanic dataset. I recommend you check that out here: <a href=\"http:\/\/gallery.cortanaintelligence.com\/Experiment\/Tutorial-Building-a-classification-model-in-Azure-ML-8?share=1\" rel=\"nofollow noreferrer\">http:\/\/gallery.cortanaintelligence.com\/Experiment\/Tutorial-Building-a-classification-model-in-Azure-ML-8?share=1<\/a><\/p>\n\n<p>To answer your question, the module you can drag to your canvas in order to make the features into categories; is the Edit Metadata module where you select the columns you want and change the \u201cunchanged\u201d into \u201cMake categorical\u201d within the Categorical-properties pane like in the image below:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/2NDht.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2NDht.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>You can also use the same module to make better sense from your columns by giving them a different column name. SibSp means SiblingSpouse like I have renamed it to in the image below:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Gm9Rr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Gm9Rr.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And at last you can assign the targeted value (survived) and make the field into a label for ease of use.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/LyN0j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LyN0j.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37728314",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1465484184003,
        "Question_original_content":"refactor column featur wai dataset featur dataset titan ship sampl dataset like work column numer featur string featur like categor possibl renam column model descript initi got clue sibsp mean instanc",
        "Question_preprocessed_content":"refactor column featur wai dataset featur dataset titan ship sampl dataset like work column numer featur string featur like categor possibl renam column model descript initi got clue sibsp mean instanc",
        "Question_gpt_summary_original":"The user is facing challenges in Azure Machine Learning related to the dataset features. They want to categorize the numeric and string features and rename the columns to make them more descriptive. The user is also unsure about the meaning of some of the column names.",
        "Question_gpt_summary":"user face challeng relat dataset featur want categor numer string featur renam column descript user unsur mean column name",
        "Answer_original_content":"essenti recreat experi raja iqbal titan dataset recommend check http galleri cortanaintellig com experi tutori build classif model azur share answer question modul drag canva order featur categori edit metadata modul select column want chang unchang categor categor properti pane like imag us modul better sens column give differ column sibsp mean siblingspous like renam imag assign target valu surviv field label eas us",
        "Answer_preprocessed_content":"essenti recreat experi raja iqbal titan dataset recommend check answer question modul drag canva order featur categori edit metadata modul select column want chang unchang categor pane like imag us modul better sens column give differ column sibsp mean siblingspous like renam imag assign target valu field label eas us",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Use the Edit Metadata module to categorize the features by selecting the columns and changing the \"unchanged\" into \"Make categorical\" within the Categorical-properties pane.\n- Use the same module to rename the columns to make them more descriptive.\n- Assign the targeted value (survived) and make the field into a label for ease of use.\n\nNo personal opinions or biases were included in the response.",
        "Answer_gpt_summary":"possibl solut mention discuss us edit metadata modul categor featur select column chang unchang categor categor properti pane us modul renam column descript assign target valu surviv field label eas us person opinion bias includ respons"
    },
    {
        "Question_title":"Train an already trained model in Sagemaker and Huggingface without re-initialising",
        "Question_body":"<p>Let's say I have successfully trained a model on some training data for 10 epochs. How can I then access the very same model and train for a further 10 epochs?<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-checkpoints.html\" rel=\"nofollow noreferrer\">In the docs<\/a> it suggests &quot;you need to specify a checkpoint output path through hyperparameters&quot; --&gt; how?<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># define my estimator the standard way\nhuggingface_estimator = HuggingFace(\n    entry_point='train.py',\n    source_dir='.\/scripts',\n    instance_type='ml.p3.2xlarge',\n    instance_count=1,\n    role=role,\n    transformers_version='4.10',\n    pytorch_version='1.9',\n    py_version='py38',\n    hyperparameters = hyperparameters,\n    metric_definitions=metric_definitions\n)\n\n# train the model\nhuggingface_estimator.fit(\n    {'train': training_input_path, 'test': test_input_path}\n)\n<\/code><\/pre>\n<p>If I run <code>huggingface_estimator.fit<\/code> again it will just start the whole thing over again and overwrite my previous training.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649787701300,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":110.0,
        "Answer_body":"<p>You can find the relevant checkpoint save\/load code in <a href=\"https:\/\/github.com\/huggingface\/notebooks\/blob\/main\/sagemaker\/05_spot_instances\/sagemaker-notebook.ipynb\" rel=\"nofollow noreferrer\">Spot Instances - Amazon SageMaker x Hugging Face Transformers<\/a>.<br \/>\n(The example enables Spot instances, but you can use on-demand).<\/p>\n<ol>\n<li>In hyperparameters you set: <code>'output_dir':'\/opt\/ml\/checkpoints'<\/code>.<\/li>\n<li>You define a <code>checkpoint_s3_uri<\/code> in the Estimator (which is unique to the series of jobs you'll run).<\/li>\n<li>You add code for train.py to support checkpointing:<\/li>\n<\/ol>\n<blockquote>\n<pre><code>from transformers.trainer_utils import get_last_checkpoint\n\n# check if checkpoint existing if so continue training\nif get_last_checkpoint(args.output_dir) is not None:\n    logger.info(&quot;***** continue training *****&quot;)\n    last_checkpoint = get_last_checkpoint(args.output_dir)\n    trainer.train(resume_from_checkpoint=last_checkpoint)\nelse:\n    trainer.train()\n<\/code><\/pre>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71847442",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1649831911830,
        "Question_original_content":"train train model huggingfac initialis let successfulli train model train data epoch access model train epoch doc suggest need specifi checkpoint output path hyperparamet defin estim standard wai huggingfac estim huggingfac entri point train sourc dir script instanc type xlarg instanc count role role transform version pytorch version version hyperparamet hyperparamet metric definit metric definit train model huggingfac estim fit train train input path test test input path run huggingfac estim fit start thing overwrit previou train",
        "Question_preprocessed_content":"train train model huggingfac let successfulli train model train data epoch access model train epoch doc suggest need specifi checkpoint output path hyperparamet run start thing overwrit previou train",
        "Question_gpt_summary_original":"The user wants to know how to access a previously trained model and train it for further epochs without re-initializing it. The user has found documentation suggesting the use of a checkpoint output path through hyperparameters, but is unsure how to implement it. The user is concerned that running the training process again will overwrite the previous training.",
        "Question_gpt_summary":"user want know access previous train model train epoch initi user document suggest us checkpoint output path hyperparamet unsur implement user concern run train process overwrit previou train",
        "Answer_original_content":"relev checkpoint save load code spot instanc hug face transform exampl enabl spot instanc us demand hyperparamet set output dir opt checkpoint defin checkpoint uri estim uniqu seri job run add code train support checkpoint transform trainer util import checkpoint check checkpoint exist continu train checkpoint arg output dir logger info continu train checkpoint checkpoint arg output dir trainer train resum checkpoint checkpoint trainer train",
        "Answer_preprocessed_content":"relev checkpoint code spot instanc hug face transform exampl enabl spot instanc us hyperparamet set defin estim add code support checkpoint",
        "Answer_gpt_summary_original":"Solution:\n- Set the output directory for checkpoints in hyperparameters.\n- Define a unique checkpoint S3 URI in the Estimator.\n- Add code to train.py to support checkpointing and continue training from the last checkpoint if it exists.",
        "Answer_gpt_summary":"solut set output directori checkpoint hyperparamet defin uniqu checkpoint uri estim add code train support checkpoint continu train checkpoint exist"
    },
    {
        "Question_title":"Run Sagemaker notebook instance and be able to close tab",
        "Question_body":"<p>I'm currently using Sagemaker notebook instance (not from Sagemaker Studio), and I want to run a notebook that is expected to take around 8 hours to finish. I want to leave it overnight, and see the output from each cell, the output is a combination of print statements and plots.<\/p>\n<p>Howevever, when I start running the notebook and make sure the initial cells run, I close the Jupyterlab tab in my browser, and some minutes after, I open it again to see how is it going, but the notebook is stopped.<\/p>\n<p>Is there any way where I can still use my notebook as it is, see the output from each cell (prints and plots) and do not have to keep the Jupyterlab tab open (turn my laptop off, etc)?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1646922683383,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1154.0,
        "Answer_body":"<p>Answering my own question.<\/p>\n<p>I ended up using Sagemaker Processing jobs for this. As initially suggested by the other answer. I found this library developed a few months ago: <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-run-notebook\" rel=\"nofollow noreferrer\">Sagemaker run notebook<\/a>, which helped still keep my notebook structure and cells as I had them, and be able to run it using Sagemaker run notebook using a bigger instance, and modifying the notebook in a smaller one.<\/p>\n<p>The output of each cell was saved, along the plots I had, in S3 as a jupyter notebook.<\/p>\n<p>I see that no constant support is given to the library, but you can fork it and make changes to it, and use it as per your requirements. For example, creating a docker container based on your needs.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71425842",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1662537228663,
        "Question_original_content":"run notebook instanc abl close tab current notebook instanc studio want run notebook expect hour finish want leav overnight output cell output combin print statement plot howevev start run notebook sure initi cell run close jupyterlab tab browser minut open go notebook stop wai us notebook output cell print plot jupyterlab tab open turn laptop",
        "Question_preprocessed_content":"run notebook instanc abl close tab current notebook instanc want run notebook expect hour finish want leav overnight output cell output combin print statement plot howevev start run notebook sure initi cell run close jupyterlab tab browser minut open go notebook stop wai us notebook output cell jupyterlab tab open",
        "Question_gpt_summary_original":"The user is facing challenges in running a Sagemaker notebook instance and leaving it to run overnight. The user wants to be able to see the output from each cell, which includes print statements and plots, without having to keep the Jupyterlab tab open. However, when the user closes the tab and opens it again later, the notebook is stopped. The user is seeking a solution to be able to use the notebook as intended without having to keep the tab open.",
        "Question_gpt_summary":"user face challeng run notebook instanc leav run overnight user want abl output cell includ print statement plot have jupyterlab tab open user close tab open later notebook stop user seek solut abl us notebook intend have tab open",
        "Answer_original_content":"answer question end process job initi suggest answer librari develop month ago run notebook help notebook structur cell abl run run notebook bigger instanc modifi notebook smaller output cell save plot jupyt notebook constant support given librari fork chang us requir exampl creat docker contain base need",
        "Answer_preprocessed_content":"answer question end process job initi suggest answer librari develop month ago run notebook help notebook structur cell abl run run notebook bigger instanc modifi notebook smaller output cell save plot jupyt notebook constant support given librari fork chang us requir exampl creat docker contain base need",
        "Answer_gpt_summary_original":"Solution: The user found a solution by using Sagemaker Processing jobs and a library called Sagemaker run notebook. The library helped the user to keep the notebook structure and cells as they were and run it using a bigger instance, and modifying the notebook in a smaller one. The output of each cell was saved, along with the plots, in S3 as a Jupyter notebook. The user mentioned that no constant support is given to the library, but it can be forked and modified based on the user's requirements, such as creating a docker container based on their needs.",
        "Answer_gpt_summary":"solut user solut process job librari call run notebook librari help user notebook structur cell run bigger instanc modifi notebook smaller output cell save plot jupyt notebook user mention constant support given librari fork modifi base user requir creat docker contain base need"
    },
    {
        "Question_title":"Single cache or multiple caches in NAS with External Data",
        "Question_body":"<p>We have a large NAS with large datasets both in number of files and file size. Those data sets need to be used by several gitlab projects (data wrangling, data processing, modelling, making plots, etc).<\/p>\n<p>We want to have some kind of version control of the data (mainly, say \u201cthis experiment was run with these data\u201d and check that it hasn\u2019t changed), so I have made some simple tests with DVC, but I\u2019d like the opinion of people with more insight. My main question is whether I should set up a single shared cache for all projects, or independent caches for each project (and whether my proposed solution is sound at all). A secondary question is whether the setup will allow to run experiments on the cloud down the line. Here are the details of the problem and my attempted solutions:<\/p>\n<p>Main constraints:<\/p>\n<ul>\n<li>we cannot migrate the data to a flat directory structure where files get renamed with their hash, so the data has to co-exist with the cache. That is, we need to maintain the original directory structure and filenames in the NAS as they are<\/li>\n<li>the NAS data needs to be accessed and processed by several people from several machines (the NAS is an ext2 filesystem that gets mounted with NFS on those machines), typically by cloning the gitlab project to their own \/home\/john_doe\/Software\/my_cats_projects local directory.<\/li>\n<li>we cannot have duplicates of the data, e.g. the one on the NAS and then on each machine that needs to process it<\/li>\n<\/ul>\n<p>The solution I have come up with, and that seems to work is to combine DVC\u2019s \u201cManaging External Data\u201d (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/user-guide\/managing-external-data<\/a>) and \u201cLarge Dataset Optimization\u201d (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization<\/a>).<\/p>\n<p>We can have gitlab projects with a softlink to the external data in the NAS, e.g.<\/p>\n<p>my_cats_project<br>\n\u251c\u2500\u2500 bin<br>\n\u2502       \u2514\u2500\u2500 process_some_data.sh<br>\n\u251c\u2500\u2500 data<br>\n\u2502       \u2514\u2500\u2500 some_small_local_data.csv<br>\n\u251c\u2500\u2500 external_data \u2192 \/nas_server\/laboratory_1\/big_dataset_with_cats\/<br>\n|               \u251c\u2500\u2500  cat_movie_001.mov<br>\n|               \u2514\u2500\u2500  cat_movie_002.mov<br>\n\u251c\u2500\u2500 README.md<br>\n\u2514\u2500\u2500 src<br>\n\u2514\u2500\u2500 python_code.py<\/p>\n<p>We initialise dvc in the gitlab project<\/p>\n<p>$ cd my_data_project<br>\n$ dvc init<\/p>\n<p>We then tell dvc to use an external cache on the NAS. This is the main part of my question, whether it\u2019s better to use a single external cache living on the NAS for all datasets and projects,<\/p>\n<p>$ dvc cache dir \/nas_server\/common_dvc_cache<\/p>\n<p>or have separate caches for each project,<\/p>\n<p>$ dvc cache dir \/nas_server\/projects_dvc_cache\/my_cats_project_dvc_cache<\/p>\n<p>Either way, the cache or caches will live on the NAS, avoiding transferring large data files to \/home directories.<\/p>\n<p>To avoid data duplication, we configure the cache to use hardlinks (as reflinks are not available on ext2)<\/p>\n<p>$ dvc config cache.type hardlink<\/p>\n<p>This way, the original data files (cat_movie_001.mov) and their cache \u201ccopies\u201d (asd9890w908ad9sfasd9fasdf980asfd) will point to the same inode.<\/p>\n<p>In this set up, data cannot be overwritten or updates. For example, if we want new versions of the cat movies above, we\u2019ll have to create new directories for those, but that\u2019s fine in this case.<\/p>\n<p>DVC will make the original files non-writable to protect the cache from being corrupted. But we can still move the original data files on the NAS using \u201cdvc move\u201d, right?<\/p>\n<p>Finally, I\u2019d be grateful for any insights on whether this setup would allow to then run projects on cloud service providers supported by DVC.<\/p>\n<p>Thanks a lot!<\/p>\n<p>Ramon.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1648633274669,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":588.0,
        "Answer_body":"<blockquote>\n<ul>\n<li>we cannot migrate the data to a flat directory structure where files get renamed with their hash, so the data has to co-exist with the cache. That is, we need to maintain the original directory structure and filenames in the NAS as they are<\/li>\n<\/ul>\n<\/blockquote>\n<p>Can you elaborate on this requirement?<\/p>\n<p>Using external data is generally not recommended and is only intended as a workaround for very specific use cases (such as where data absolutely cannot be moved\/reorganized due to security or compliance reasons).<\/p>\n<p>The way that DVC works is that the \u201coriginal directory structure and filenames\u201d are kept in a Git\/DVC repository (where DVC keeps track of the mapping between the original structure and the actual file hashes).<\/p>\n<blockquote>\n<p>Finally, I\u2019d be grateful for any insights on whether this setup would allow to then run projects on cloud service providers supported by DVC.<\/p>\n<\/blockquote>\n<p>Can you clarify what you mean by \u201crun projects on cloud service providers\u201d here?<\/p>\n<p>If you are asking about DVC remotes, the answer is no, If you are using the external data feature, it cannot be pushed\/pulled to\/from DVC cloud remotes.<\/p>\n<hr>\n<p>I think the typical solution for your problem would be to set up a main\/central DVC repository as a <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries#data-registries\">data registry<\/a>. The data registry repo would replace your existing setup (so that the data registry repo would follow the \u201coriginal directory structure and filenames in the NAS\u201d, and it would handle keeping track of the mapping from the original structure to the DVC content-addressable file hash storage structure).<\/p>\n<p>Then, all of your other projects would import the data from the data registry as needed.<\/p>\n<p>With regard to keeping data only on the NAS, all of your projects (including the data registry itself) could be configured to use a shared DVC cache which is kept on the NAS (as described in the large dataset optimization docs).<\/p>\n<p>So the end result here would be:<\/p>\n<ul>\n<li>All actual data is only stored on the NAS (in the DVC cache structure), and only a single copy of each file exists on the NAS<\/li>\n<li>The original file\/directory structure is managed\/tracked by the data registry project.\n<ul>\n<li>With regard to managing the data registry repo itself, a \u201clocal\u201d copy of that repo could be kept anywhere, whether it that is on the NAS itself (and configured to use hardlinks) or elsewhere (and configured to use symlinks)<\/li>\n<\/ul>\n<\/li>\n<li>For each user\u2019s individual projects, they would have their own local clone of the gitlab project configured to use symlinks (user\u2019s local machines would not store any of the actual data, they would only have symlinks pointing to the NAS)<\/li>\n<\/ul>\n<p>This setup would also allow you to push\/pull data from the shared cache on the NAS to DVC cloud remotes.<\/p>. <p>Hi pmrowla,<\/p>\n<p>Thanks for your insightful reply, and sorry it has taken me a while to implement the ideas. I think I have it working now (details below, in case you\u2019d like to check).<\/p>\n<blockquote>\n<p>Can you elaborate on this requirement?<\/p>\n<\/blockquote>\n<p>What I meant by \u201cwe cannot migrate the data to a flat directory structure where files get renamed with their hash\u201d is that the data files still need to exist and be accessible to users who have no git\/dvc knowledge and will use Mac Finder \/ Windows File Explorer \/ Linux Nautilus to copy and browse files on the NAS without using DVC. Your solution enables this, so that\u2019s great.<\/p>\n<p>We then want to be able to put some of those data sets under version control. Your solution of the Data Registry allows that too.<\/p>\n<p>I configured the Data Registry with<\/p>\n<pre data-code-wrap=\"shell\"><code class=\"lang-nohighlight\">dvc cache dir ..\/dvc_cache\/\ndvc config cache.shared group\ndvc config cache.type hardlink\n<\/code><\/pre>\n<p>so I expected that directories that were be added with <code>dvc add<\/code> would be write-protected, as the documentation says. However, I can still see them as writable<\/p>\n<pre><code class=\"lang-auto\">drwxr-xr-x. 4 user group 4.0K May 24 10:09 foo_dataset\n-rw-r--r--. 1 user group   93 May 25 14:47 foo_dataset.dvc\n<\/code><\/pre>\n<blockquote>\n<p>Can you clarify what you mean by \u201crun projects on cloud service providers\u201d here?<\/p>\n<\/blockquote>\n<p>What I meant, now updated with your Data Registry suggestion, is that we have our local NAS with datasets and the Data Registry, and then we have software experiments on another local GPU server, namely we write a python script to process a dataset. We put that dataset under version control in the Data Registry following your advice. Now, we can import the dataset into the experiment to benefit from data version control using <code>dvc import<\/code>.<\/p>\n<p>But, after doing the prep work locally, we want to run this on AWS. The NAS already gets copied regularly to AWS anyway, so the data will be there, we want to avoid having to copy it to S3 buckets every time we run new experiments.<\/p>\n<p>I need to think a bit more about this after having implemented your Data Registry proposal (I\u2019ll  probably come back with more questions!)<\/p>\n<h2>\n<a name=\"details-about-our-current-implementation-after-your-advice-1\" class=\"anchor\" href=\"#details-about-our-current-implementation-after-your-advice-1\"><\/a>Details about our current implementation, after your advice:<\/h2>\n<p>The NAS has a <code>datasets<\/code> directory, with subdirectories that contain data sets with data files, e.g.<\/p>\n<pre><code class=\"lang-auto\">datasets\/\n\u2514\u2500\u2500 foo_dataset\n    \u251c\u2500\u2500 donor1\n    \u2502   \u251c\u2500\u2500 file1.jpg\n    \u2502   \u251c\u2500\u2500 file2.jpg\n    \u2502   \u2514\u2500\u2500 file3.jpg\n    \u2514\u2500\u2500 donor2\n        \u251c\u2500\u2500 file4.jpg\n        \u251c\u2500\u2500 file5.jpg\n        \u2514\u2500\u2500 file6.jpg\n<\/code><\/pre>\n<p>I created a Gitlab repository <code>data_registry<\/code>, and pulled it into <code>datasets<\/code>.<\/p>\n<p>I created a directory <code>dvc_cache<\/code> for the shared cache, at the same level as <code>datasets<\/code>.<\/p>\n<p>I then configured the DVC cache in <code>datasets<\/code><\/p>\n<pre data-code-wrap=\"shell\"><code class=\"lang-nohighlight\">cd datasets\/\ndvc cache dir ..\/dvc_cache\/\ndvc config cache.shared group\ndvc config cache.type hardlink\ndvc config core.autostage true\ngit commit .dvc\/config -m \"Configure external shared cache in ..\/dvc_cache\"\ngit push\n<\/code><\/pre>\n<p>Now we can put selected data sets under version control with<\/p>\n<pre data-code-wrap=\"shell\"><code class=\"lang-nohighlight\">cd datasets\/\ndvc add foo_dataset\ngit commit foo_dataset.dvc .gitignore -m \"Put foo_dataset under data version control\"\ngit push\n<\/code><\/pre>\n<p>To import a data set into an experiment that lives on another server, a server that mounts the NAS on \/mnt\/nas, first we configure DVC for the experiment<\/p>\n<pre><code class=\"lang-auto\">cd ~\/Software\/myexperiment\n# initialise DVC\ndvc init\n# tell the experiment to use the Data Registry shared cache as its own DVC cache\ndvc cache dir \/mnt\/nas\/dvc_cache\/\ndvc config cache.shared group\n# if the experiment directory is in the same filesystem as the Data Registry (the Data \n# Registry lives on NAS2), then DVC will create hardlinks to the data files. If they are \n# in different filesystems (e.g. if the experiment is on NAS1), it will create softlinks\ndvc config cache.type hardlink,symlink\n# make \"dvc add\" automatically do \"git add\" of the files it creates\/modifies\/deletes. This\n# stages those files so they are ready to commit\ndvc config core.autostage true\n# commit the changes\ngit commit .dvc .dvcignore .gitignore -m \"Configure DVC for experiment\"\n<\/code><\/pre>\n<p>then, import the data set into the experiment<\/p>\n<pre data-code-wrap=\"shell\"><code class=\"lang-nohighlight\">dvc import GITLAB_SERVER_URL\/data_registry foo_dataset\ngit commit .gitignore foo_dataset.dvc -m \"Import dataset foo_dataset\"\ngit push\n<\/code><\/pre>\n<p>This creates a directory <code>foo_dataset\/<\/code> with the data set files and a pointer file <code>foo_dataset.dvc<\/code>.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/single-cache-or-multiple-caches-in-nas-with-external-data\/1136",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-31T06:05:31.395Z",
                "Answer_body":"<blockquote>\n<ul>\n<li>we cannot migrate the data to a flat directory structure where files get renamed with their hash, so the data has to co-exist with the cache. That is, we need to maintain the original directory structure and filenames in the NAS as they are<\/li>\n<\/ul>\n<\/blockquote>\n<p>Can you elaborate on this requirement?<\/p>\n<p>Using external data is generally not recommended and is only intended as a workaround for very specific use cases (such as where data absolutely cannot be moved\/reorganized due to security or compliance reasons).<\/p>\n<p>The way that DVC works is that the \u201coriginal directory structure and filenames\u201d are kept in a Git\/DVC repository (where DVC keeps track of the mapping between the original structure and the actual file hashes).<\/p>\n<blockquote>\n<p>Finally, I\u2019d be grateful for any insights on whether this setup would allow to then run projects on cloud service providers supported by DVC.<\/p>\n<\/blockquote>\n<p>Can you clarify what you mean by \u201crun projects on cloud service providers\u201d here?<\/p>\n<p>If you are asking about DVC remotes, the answer is no, If you are using the external data feature, it cannot be pushed\/pulled to\/from DVC cloud remotes.<\/p>\n<hr>\n<p>I think the typical solution for your problem would be to set up a main\/central DVC repository as a <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries#data-registries\">data registry<\/a>. The data registry repo would replace your existing setup (so that the data registry repo would follow the \u201coriginal directory structure and filenames in the NAS\u201d, and it would handle keeping track of the mapping from the original structure to the DVC content-addressable file hash storage structure).<\/p>\n<p>Then, all of your other projects would import the data from the data registry as needed.<\/p>\n<p>With regard to keeping data only on the NAS, all of your projects (including the data registry itself) could be configured to use a shared DVC cache which is kept on the NAS (as described in the large dataset optimization docs).<\/p>\n<p>So the end result here would be:<\/p>\n<ul>\n<li>All actual data is only stored on the NAS (in the DVC cache structure), and only a single copy of each file exists on the NAS<\/li>\n<li>The original file\/directory structure is managed\/tracked by the data registry project.\n<ul>\n<li>With regard to managing the data registry repo itself, a \u201clocal\u201d copy of that repo could be kept anywhere, whether it that is on the NAS itself (and configured to use hardlinks) or elsewhere (and configured to use symlinks)<\/li>\n<\/ul>\n<\/li>\n<li>For each user\u2019s individual projects, they would have their own local clone of the gitlab project configured to use symlinks (user\u2019s local machines would not store any of the actual data, they would only have symlinks pointing to the NAS)<\/li>\n<\/ul>\n<p>This setup would also allow you to push\/pull data from the shared cache on the NAS to DVC cloud remotes.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-26T10:20:17.820Z",
                "Answer_body":"<p>Hi pmrowla,<\/p>\n<p>Thanks for your insightful reply, and sorry it has taken me a while to implement the ideas. I think I have it working now (details below, in case you\u2019d like to check).<\/p>\n<blockquote>\n<p>Can you elaborate on this requirement?<\/p>\n<\/blockquote>\n<p>What I meant by \u201cwe cannot migrate the data to a flat directory structure where files get renamed with their hash\u201d is that the data files still need to exist and be accessible to users who have no git\/dvc knowledge and will use Mac Finder \/ Windows File Explorer \/ Linux Nautilus to copy and browse files on the NAS without using DVC. Your solution enables this, so that\u2019s great.<\/p>\n<p>We then want to be able to put some of those data sets under version control. Your solution of the Data Registry allows that too.<\/p>\n<p>I configured the Data Registry with<\/p>\n<pre data-code-wrap=\"shell\"><code class=\"lang-nohighlight\">dvc cache dir ..\/dvc_cache\/\ndvc config cache.shared group\ndvc config cache.type hardlink\n<\/code><\/pre>\n<p>so I expected that directories that were be added with <code>dvc add<\/code> would be write-protected, as the documentation says. However, I can still see them as writable<\/p>\n<pre><code class=\"lang-auto\">drwxr-xr-x. 4 user group 4.0K May 24 10:09 foo_dataset\n-rw-r--r--. 1 user group   93 May 25 14:47 foo_dataset.dvc\n<\/code><\/pre>\n<blockquote>\n<p>Can you clarify what you mean by \u201crun projects on cloud service providers\u201d here?<\/p>\n<\/blockquote>\n<p>What I meant, now updated with your Data Registry suggestion, is that we have our local NAS with datasets and the Data Registry, and then we have software experiments on another local GPU server, namely we write a python script to process a dataset. We put that dataset under version control in the Data Registry following your advice. Now, we can import the dataset into the experiment to benefit from data version control using <code>dvc import<\/code>.<\/p>\n<p>But, after doing the prep work locally, we want to run this on AWS. The NAS already gets copied regularly to AWS anyway, so the data will be there, we want to avoid having to copy it to S3 buckets every time we run new experiments.<\/p>\n<p>I need to think a bit more about this after having implemented your Data Registry proposal (I\u2019ll  probably come back with more questions!)<\/p>\n<h2>\n<a name=\"details-about-our-current-implementation-after-your-advice-1\" class=\"anchor\" href=\"#details-about-our-current-implementation-after-your-advice-1\"><\/a>Details about our current implementation, after your advice:<\/h2>\n<p>The NAS has a <code>datasets<\/code> directory, with subdirectories that contain data sets with data files, e.g.<\/p>\n<pre><code class=\"lang-auto\">datasets\/\n\u2514\u2500\u2500 foo_dataset\n    \u251c\u2500\u2500 donor1\n    \u2502   \u251c\u2500\u2500 file1.jpg\n    \u2502   \u251c\u2500\u2500 file2.jpg\n    \u2502   \u2514\u2500\u2500 file3.jpg\n    \u2514\u2500\u2500 donor2\n        \u251c\u2500\u2500 file4.jpg\n        \u251c\u2500\u2500 file5.jpg\n        \u2514\u2500\u2500 file6.jpg\n<\/code><\/pre>\n<p>I created a Gitlab repository <code>data_registry<\/code>, and pulled it into <code>datasets<\/code>.<\/p>\n<p>I created a directory <code>dvc_cache<\/code> for the shared cache, at the same level as <code>datasets<\/code>.<\/p>\n<p>I then configured the DVC cache in <code>datasets<\/code><\/p>\n<pre data-code-wrap=\"shell\"><code class=\"lang-nohighlight\">cd datasets\/\ndvc cache dir ..\/dvc_cache\/\ndvc config cache.shared group\ndvc config cache.type hardlink\ndvc config core.autostage true\ngit commit .dvc\/config -m \"Configure external shared cache in ..\/dvc_cache\"\ngit push\n<\/code><\/pre>\n<p>Now we can put selected data sets under version control with<\/p>\n<pre data-code-wrap=\"shell\"><code class=\"lang-nohighlight\">cd datasets\/\ndvc add foo_dataset\ngit commit foo_dataset.dvc .gitignore -m \"Put foo_dataset under data version control\"\ngit push\n<\/code><\/pre>\n<p>To import a data set into an experiment that lives on another server, a server that mounts the NAS on \/mnt\/nas, first we configure DVC for the experiment<\/p>\n<pre><code class=\"lang-auto\">cd ~\/Software\/myexperiment\n# initialise DVC\ndvc init\n# tell the experiment to use the Data Registry shared cache as its own DVC cache\ndvc cache dir \/mnt\/nas\/dvc_cache\/\ndvc config cache.shared group\n# if the experiment directory is in the same filesystem as the Data Registry (the Data \n# Registry lives on NAS2), then DVC will create hardlinks to the data files. If they are \n# in different filesystems (e.g. if the experiment is on NAS1), it will create softlinks\ndvc config cache.type hardlink,symlink\n# make \"dvc add\" automatically do \"git add\" of the files it creates\/modifies\/deletes. This\n# stages those files so they are ready to commit\ndvc config core.autostage true\n# commit the changes\ngit commit .dvc .dvcignore .gitignore -m \"Configure DVC for experiment\"\n<\/code><\/pre>\n<p>then, import the data set into the experiment<\/p>\n<pre data-code-wrap=\"shell\"><code class=\"lang-nohighlight\">dvc import GITLAB_SERVER_URL\/data_registry foo_dataset\ngit commit .gitignore foo_dataset.dvc -m \"Import dataset foo_dataset\"\ngit push\n<\/code><\/pre>\n<p>This creates a directory <code>foo_dataset\/<\/code> with the data set files and a pointer file <code>foo_dataset.dvc<\/code>.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"singl cach multipl cach na extern data larg na larg dataset number file file size data set need gitlab project data wrangl data process model make plot want kind version control data mainli experi run data check chang simpl test like opinion peopl insight main question set singl share cach project independ cach project propos solut sound secondari question setup allow run experi cloud line detail problem attempt solut main constraint migrat data flat directori structur file renam hash data exist cach need maintain origin directori structur filenam na na data need access process peopl machin na ext filesystem get mount nf machin typic clone gitlab project home john doe softwar cat project local directori duplic data na machin need process solut come work combin manag extern data http org doc user guid manag extern data larg dataset optim http org doc user guid larg dataset optim gitlab project softlink extern data na cat project bin process data data small local data csv extern data na server laboratori big dataset cat cat movi mov cat movi mov readm src python code initialis gitlab project data project init tell us extern cach na main question better us singl extern cach live na dataset project cach dir na server common cach separ cach project cach dir na server project cach cat project cach wai cach cach live na avoid transfer larg data file home directori avoid data duplic configur cach us hardlink reflink avail ext config cach type hardlink wai origin data file cat movi mov cach copi asdwadsfasdfasdfasfd point inod set data overwritten updat exampl want new version cat movi creat new directori that fine case origin file non writabl protect cach corrupt origin data file na right final grate insight setup allow run project cloud servic provid support thank lot ramon",
        "Question_preprocessed_content":"singl cach multipl cach na extern data larg na larg dataset number file file size data set need gitlab project want kind version control data simpl test like opinion peopl insight main question set singl share cach project independ cach project secondari question setup allow run experi cloud line detail problem attempt solut main constraint migrat data flat directori structur file renam hash data cach need maintain origin directori structur filenam na na data need access process peopl machin typic clone gitlab project local directori duplic data na machin need process solut come work combin manag extern data larg dataset optim gitlab project softlink extern data na bin data src initialis gitlab project init tell us extern cach na main question better us singl extern cach live na dataset project cach dir separ cach project cach dir wai cach cach live na avoid transfer larg data file directori avoid data duplic configur cach us hardlink config hardlink wai origin data file cach copi point inod set data overwritten updat exampl want new version cat movi creat new directori that fine case origin file protect cach corrupt origin data file na right final grate insight setup allow run project cloud servic provid support thank lot ramon",
        "Question_gpt_summary_original":"The user has a large NAS with large datasets that need to be used by several gitlab projects. They want to have version control of the data and are considering whether to set up a single shared cache for all projects or independent caches for each project. The main constraints are that the data cannot be migrated to a flat directory structure, needs to be accessed and processed by several people from several machines, and cannot have duplicates. The user has come up with a solution using DVC's \"Managing External Data\" and \"Large Dataset Optimization\" and is seeking opinions on whether to use a single external cache or separate caches for each project. The cache or caches will live on the NAS, avoiding transferring large data files to \/home directories. The user has configured the cache to use hardlinks to avoid data duplication and protect the cache from being corrupted. The user is also seeking insights on whether this setup would allow running projects on cloud service providers supported by DVC.",
        "Question_gpt_summary":"user larg na larg dataset need gitlab project want version control data consid set singl share cach project independ cach project main constraint data migrat flat directori structur need access process peopl machin duplic user come solut manag extern data larg dataset optim seek opinion us singl extern cach separ cach project cach cach live na avoid transfer larg data file home directori user configur cach us hardlink avoid data duplic protect cach corrupt user seek insight setup allow run project cloud servic provid support",
        "Answer_original_content":"migrat data flat directori structur file renam hash data exist cach need maintain origin directori structur filenam na elabor requir extern data gener recommend intend workaround specif us case data absolut move reorgan secur complianc reason wai work origin directori structur filenam kept git repositori keep track map origin structur actual file hash final grate insight setup allow run project cloud servic provid support clarifi mean run project cloud servic provid ask remot answer extern data featur push pull cloud remot think typic solut problem set main central repositori data registri data registri repo replac exist setup data registri repo follow origin directori structur filenam na handl keep track map origin structur content address file hash storag structur project import data data registri need regard keep data na project includ data registri configur us share cach kept na describ larg dataset optim doc end result actual data store na cach structur singl copi file exist na origin file directori structur manag track data registri project regard manag data registri repo local copi repo kept na configur us hardlink configur us symlink user individu project local clone gitlab project configur us symlink user local machin store actual data symlink point na setup allow push pull data share cach na cloud remot pmrowla thank insight repli sorri taken implement idea think work detail case youd like check elabor requir meant migrat data flat directori structur file renam hash data file need exist access user git knowledg us mac finder window file explor linux nautilu copi brows file na solut enabl that great want abl data set version control solut data registri allow configur data registri cach dir cach config cach share group config cach type hardlink expect directori ad add write protect document sai writabl drwxr user group foo dataset user group foo dataset clarifi mean run project cloud servic provid meant updat data registri suggest local na dataset data registri softwar experi local gpu server write python script process dataset dataset version control data registri follow advic import dataset experi benefit data version control import prep work local want run aw na get copi regularli aw data want avoid have copi bucket time run new experi need think bit have implement data registri propos ill probabl come question detail current implement advic na dataset directori subdirectori contain data set data file dataset foo dataset donor file jpg file jpg file jpg donor file jpg file jpg file jpg creat gitlab repositori data registri pull dataset creat directori cach share cach level dataset configur cach dataset dataset cach dir cach config cach share group config cach type hardlink config core autostag true git commit config configur extern share cach cach git push select data set version control dataset add foo dataset git commit foo dataset gitignor foo dataset data version control git push import data set experi live server server mount na mnt na configur experi softwar myexperi initialis init tell experi us data registri share cach cach cach dir mnt na cach config cach share group experi directori filesystem data registri data registri live na creat hardlink data file differ filesystem experi na creat softlink config cach type hardlink symlink add automat git add file creat modifi delet stage file readi commit config core autostag true commit chang git commit ignor gitignor configur experi import data set experi import gitlab server url data registri foo dataset git commit gitignor foo dataset import dataset foo dataset git push creat directori foo dataset data set file pointer file foo dataset",
        "Answer_preprocessed_content":"migrat data flat directori structur file renam hash data cach need maintain origin directori structur filenam na elabor requir extern data gener recommend intend workaround specif us case wai work origin directori structur filenam kept git repositori final grate insight setup allow run project cloud servic provid support clarifi mean run project cloud servic provid ask remot answer extern data featur cloud remot think typic solut problem set repositori data registri data registri repo replac exist setup project import data data registri need regard keep data na project configur us share cach kept na end result actual data store na singl copi file exist na origin structur data registri project regard manag data registri repo local copi repo kept na user individu project local clone gitlab project configur us symlink setup allow data share cach na cloud pmrowla thank insight repli sorri taken implement idea think work elabor requir meant migrat data flat directori structur file renam hash data file need exist access user git knowledg us mac finder window file explor linux nautilu copi brows file na solut enabl that great want abl data set version control solut data registri allow configur data registri expect directori ad document sai writabl clarifi mean run project cloud servic provid meant updat data registri suggest local na dataset data registri softwar experi local gpu server write python script process dataset dataset version control data registri follow advic import dataset experi benefit data version control prep work local want run aw na get copi regularli aw data want avoid have copi bucket time run new experi need think bit have implement data registri propos detail current implement advic na directori subdirectori contain data set data file creat gitlab repositori pull creat directori share cach level configur cach select data set version control import data set experi live server server mount na configur experi import data set experi creat directori data set file pointer file",
        "Answer_gpt_summary_original":"Solution:\n- Set up a main\/central DVC repository as a data registry to manage the original file\/directory structure and track the mapping from the original structure to the DVC content-addressable file hash storage structure.\n- Import data from the data registry as needed for each project.\n- Configure all projects to use a shared DVC cache which is kept on the NAS to avoid transferring large data files to \/home directories.\n- Use hardlinks to avoid data duplication and protect the cache from being corrupted.\n- This setup would allow pushing\/pulling data from the shared cache on the NAS to DVC cloud remotes.\n- If the experiment directory is in the same filesystem as the Data Registry, then DVC will create hardlinks to the data files",
        "Answer_gpt_summary":"solut set main central repositori data registri manag origin file directori structur track map origin structur content address file hash storag structur import data data registri need project configur project us share cach kept na avoid transfer larg data file home directori us hardlink avoid data duplic protect cach corrupt setup allow push pull data share cach na cloud remot experi directori filesystem data registri creat hardlink data file"
    },
    {
        "Question_title":"Use an existing Gateway with Azure Machine Learning?",
        "Question_body":"<p>We want to access an onprem SQL database with an existing Gateway, is that possible in AML?  The tool only seems to allow creating new gateways.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1482505382510,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":39.0,
        "Answer_body":"<p>Confirmed that this is not possible, AML only allows use of AML-created gateways.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41303697",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1488397699940,
        "Question_original_content":"us exist gatewai want access onprem sql databas exist gatewai possibl aml tool allow creat new gatewai",
        "Question_preprocessed_content":"us exist gatewai want access onprem sql databas exist gatewai possibl aml tool allow creat new gatewai",
        "Question_gpt_summary_original":"The user is facing a challenge in accessing an on-premises SQL database with an existing Gateway in Azure Machine Learning. The tool only allows creating new gateways, and the user is unsure if it is possible to use an existing one.",
        "Question_gpt_summary":"user face challeng access premis sql databas exist gatewai tool allow creat new gatewai user unsur possibl us exist",
        "Answer_original_content":"confirm possibl aml allow us aml creat gatewai",
        "Answer_preprocessed_content":"confirm possibl aml allow us gatewai",
        "Answer_gpt_summary_original":"Solution: No solution provided. It is confirmed that it is not possible to use an existing gateway in Azure Machine Learning, and the tool only allows creating new gateways.",
        "Answer_gpt_summary":"solut solut provid confirm possibl us exist gatewai tool allow creat new gatewai"
    },
    {
        "Question_title":"Sagemaker export and load model to memory",
        "Question_body":"<p>I have created a model using sagemaker (on aws ml notebook). \nI then exported that model to s3 and a <code>.tar.gz<\/code> file was created there.<\/p>\n\n<p>Im trying to find a way to load the model object to memory in my code (without using AWS docker images and deployment) and run a prediction on it.<\/p>\n\n<p>I looked for functions to do that in the model section of the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html#\" rel=\"nofollow noreferrer\">sagemaker docs<\/a>, but everything there is tightly coupled to the AWS docker images.<\/p>\n\n<p>I then tried opening the file with <code>tarfile<\/code> and <code>shutil<\/code> packages but that was useless.<\/p>\n\n<p>Any ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1562675762410,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1838.0,
        "Answer_body":"<p>With the exception of XGBoost, built-in algorithms are implemented with Apache MXNet, so simply extract the model from the .tar.gz file and load it with MXNet: load_checkpoint() is the API to use.<\/p>\n\n<p>XGBoost models are just pickled objects. Unpickle and load in sklearn:<\/p>\n\n<pre><code>$ python3\n&gt;&gt;&gt; import sklearn, pickle\n&gt;&gt;&gt; model = pickle.load(open(\"xgboost-model\", \"rb\"))\n&gt;&gt;&gt; type(model)\n&lt;class 'xgboost.core.Booster'&gt;\n<\/code><\/pre>\n\n<p>Models trained with built-in library (Tensorflow, MXNet, Pytorch, etc.) are vanilla models that can be loaded as-is with the correct library.<\/p>\n\n<p>Hope this helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1562845984060,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56952741",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1562768843267,
        "Question_original_content":"export load model memori creat model aw notebook export model tar file creat try wai load model object memori code aw docker imag deploy run predict look function model section doc tightli coupl aw docker imag tri open file tarfil shutil packag useless idea",
        "Question_preprocessed_content":"export load model memori creat model export model file creat try wai load model object memori code run predict look function model section doc tightli coupl aw docker imag tri open file packag useless idea",
        "Question_gpt_summary_original":"The user has created a model using Sagemaker on AWS ML Notebook and exported it to S3 as a .tar.gz file. They are trying to load the model object to memory in their code without using AWS docker images and deployment, but the functions in the Sagemaker docs are tightly coupled to AWS docker images. The user has tried opening the file with tarfile and shutil packages but has been unsuccessful. They are seeking ideas for a solution.",
        "Question_gpt_summary":"user creat model aw notebook export tar file try load model object memori code aw docker imag deploy function doc tightli coupl aw docker imag user tri open file tarfil shutil packag unsuccess seek idea solut",
        "Answer_original_content":"except xgboost built algorithm implement apach mxnet simpli extract model tar file load mxnet load checkpoint api us xgboost model pickl object unpickl load sklearn python import sklearn pickl model pickl load open xgboost model type model model train built librari tensorflow mxnet pytorch vanilla model load correct librari hope help",
        "Answer_preprocessed_content":"except xgboost algorithm implement apach mxnet simpli extract model file load mxnet api us xgboost model pickl object unpickl load sklearn model train librari vanilla model load correct librari hope help",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- For built-in algorithms except XGBoost, extract the model from the .tar.gz file and load it with MXNet using the load_checkpoint() API.\n- For XGBoost models, unpickle and load in sklearn using the pickle.load() function.\n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss built algorithm xgboost extract model tar file load mxnet load checkpoint api xgboost model unpickl load sklearn pickl load function person opinion bias includ summari"
    },
    {
        "Question_title":"Create a predictor from an endpoint in a different region",
        "Question_body":"<p>I have created an endpoint on us-east-1. try to create a predictor:<\/p>\n\n<pre><code>In [106]: sagemaker.predictor.RealTimePredictor(&lt;endpoint name&gt;)\n<\/code><\/pre>\n\n<p>and get<\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the DescribeEndpoint operation: \nCould not find endpoint \"arn:aws:sagemaker:us-east-2:&lt;account number&gt;:endpoint\/&lt;endpoint name&gt;\".\n<\/code><\/pre>\n\n<p>which is perfectly correct, since the endpoint is on us-east-1.  Probably I could change some defaults, but I'd rather not - I work on us-east-2 99% of the time.<\/p>\n\n<p>So, how can I set a different region when initializing the predictor?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1573497062720,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":478.0,
        "Answer_body":"<p>The (python) <code>Predictors<\/code> <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\" rel=\"nofollow noreferrer\">documentation<\/a> shows that you can pass a <code>Session<\/code> object. In turn, the <code>Session<\/code> can be <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.Session\" rel=\"nofollow noreferrer\">initialized<\/a> with a <em>client<\/em> and a <em>runtime client<\/em> - the former does everything except endpoint invocations, the latter does... endpoint invocations.<\/p>\n\n<p>Those clients are tied to specific regions. It seems like you should be able to set the runtime client region to match your endpoint, by manually instantiating it, while leaving the regular client alone (disclaimer here: I haven't tried this - if you do, let me\/us know how it goes :)).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58806807",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1573497881147,
        "Question_original_content":"creat predictor endpoint differ region creat endpoint east try creat predictor predictor realtimepredictor clienterror error occur validationexcept call describeendpoint oper endpoint arn aw east endpoint perfectli correct endpoint east probabl chang default work east time set differ region initi predictor",
        "Question_preprocessed_content":"creat predictor endpoint differ region creat endpoint try creat predictor perfectli correct endpoint probabl chang default work time set differ region initi predictor",
        "Question_gpt_summary_original":"The user is facing a challenge in creating a predictor from an endpoint in a different region. They have created an endpoint on us-east-1 but are unable to create a predictor on us-east-2 due to a validation error. The user is unsure how to set a different region when initializing the predictor.",
        "Question_gpt_summary":"user face challeng creat predictor endpoint differ region creat endpoint east unabl creat predictor east valid error user unsur set differ region initi predictor",
        "Answer_original_content":"python predictor document show pass session object turn session initi client runtim client endpoint invoc endpoint invoc client ti specif region like abl set runtim client region match endpoint manual instanti leav regular client disclaim haven tri let know goe",
        "Answer_preprocessed_content":"document show pass object turn initi client runtim client endpoint invoc endpoint invoc client ti specif region like abl set runtim client region match endpoint manual instanti leav regular client",
        "Answer_gpt_summary_original":"Solution: The user can pass a Session object to the Predictors documentation and initialize it with a client and a runtime client. The clients are tied to specific regions, so the user can set the runtime client region to match their endpoint by manually instantiating it while leaving the regular client alone. However, it is not clear if this solution has been tried and tested.",
        "Answer_gpt_summary":"solut user pass session object predictor document initi client runtim client client ti specif region user set runtim client region match endpoint manual instanti leav regular client clear solut tri test"
    },
    {
        "Question_title":"Reloading from checkpoing during AWS Sagemaker Training",
        "Question_body":"<p>Sagemaker is a great tool to train your models, and we save some money by using AWS spot instances. However, training jobs sometimes get stopped in the middle. We are using some mechanisms to continue from the latest checkpoint after a restart. See also the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-checkpoints.html\" rel=\"nofollow noreferrer\">docs<\/a>.<\/p>\n<p>Still, how do you efficiently test such a mechanism? Can you trigger it yourself? Otherwise you have to wait until the spot instance actually \u00eds restarted.<\/p>\n<p>Also, are you expected to use the linked <code>checkpoint_s3_uri<\/code> argument or the <code>model_dir<\/code> for this? E.g. the <code>TensorFlow<\/code> estimator <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/sagemaker.tensorflow.html#tensorflow-estimator\" rel=\"nofollow noreferrer\">docs<\/a> seem to suggest something <code>model_dir<\/code>for checkpoints.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616592352363,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1616592807080,
        "Question_score":0.0,
        "Question_view_count":286.0,
        "Answer_body":"<p>Since you can't manually terminate a sagemaker instance, run an Amazon SageMaker Managed Spot training for a small number of epochs, Amazon SageMaker would have backed up your checkpoint files to S3. Check that checkpoints are there. Now run a second training run, but this time provide the first jobs\u2019 checkpoint location to <code>checkpoint_s3_uri<\/code>. Reference is <a href=\"https:\/\/towardsdatascience.com\/a-quick-guide-to-using-spot-instances-with-amazon-sagemaker-b9cfb3a44a68\" rel=\"nofollow noreferrer\">here<\/a>, this also answer your second question.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66782040",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1616662041463,
        "Question_original_content":"reload checkpo train great tool train model save monei aw spot instanc train job stop middl mechan continu latest checkpoint restart doc effici test mechan trigger wait spot instanc actual restart expect us link checkpoint uri argument model dir tensorflow estim doc suggest model dirfor checkpoint",
        "Question_preprocessed_content":"reload checkpo train great tool train model save monei aw spot instanc train job stop middl mechan continu latest checkpoint restart doc effici test mechan trigger wait spot instanc actual restart expect us link argument estim doc suggest checkpoint",
        "Question_gpt_summary_original":"The user is facing challenges with training jobs getting stopped in the middle while using AWS Sagemaker and spot instances. They are using mechanisms to continue from the latest checkpoint after a restart, but are unsure how to efficiently test this mechanism and whether to use the checkpoint_s3_uri or model_dir argument.",
        "Question_gpt_summary":"user face challeng train job get stop middl spot instanc mechan continu latest checkpoint restart unsur effici test mechan us checkpoint uri model dir argument",
        "Answer_original_content":"manual termin instanc run manag spot train small number epoch back checkpoint file check checkpoint run second train run time provid job checkpoint locat checkpoint uri refer answer second question",
        "Answer_preprocessed_content":"manual termin instanc run manag spot train small number epoch back checkpoint file check checkpoint run second train run time provid job checkpoint locat refer answer second question",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to run an Amazon SageMaker Managed Spot training for a small number of epochs and check if the checkpoint files are backed up to S3. Then, run a second training run and provide the first job's checkpoint location to the checkpoint_s3_uri argument. This solution can help test the mechanism of continuing from the latest checkpoint after a restart and also answers the question of whether to use the checkpoint_s3_uri or model_dir argument.",
        "Answer_gpt_summary":"solut possibl solut mention discuss run manag spot train small number epoch check checkpoint file back run second train run provid job checkpoint locat checkpoint uri argument solut help test mechan continu latest checkpoint restart answer question us checkpoint uri model dir argument"
    },
    {
        "Question_title":"Can you load a SageMaker trained model into Keras?",
        "Question_body":"<p>I have followed the AWS tutorial(<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\/object_detection_image_json_format.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\/object_detection_image_json_format.ipynb<\/a>) and trained my first model using SageMaker.<\/p>\n\n<p>The end result is an archive containing the following files:\n- hyperparams.json\n- model_algo_1-0000.params\n- model_algo_1-symbol.json<\/p>\n\n<p>I am not familiar with this format, and was not able to load it into Keras via keras.models.model_from_json()<\/p>\n\n<p>I am assuming this is a different format or an AWS proprietary one.<\/p>\n\n<p>Can you please help me identify the format?\nIs it possible to load this into a Keras model and do inference without an EC2 instance(locally)?<\/p>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1580061744520,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":212.0,
        "Answer_body":"<p>Built-in algorithms are implemented with Apache MXNet, so that's how you'd load the model locally. load_checkpoint() is the appropriate API: <a href=\"https:\/\/mxnet.apache.org\/api\/python\/docs\/api\/mxnet\/model\/index.html#mxnet.model.load_checkpoint\" rel=\"nofollow noreferrer\">https:\/\/mxnet.apache.org\/api\/python\/docs\/api\/mxnet\/model\/index.html#mxnet.model.load_checkpoint<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59921196",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1580119883430,
        "Question_original_content":"load train model kera follow aw tutori http github com awslab amazon exampl blob master introduct amazon algorithm object detect pascalvoc coco object detect imag json format ipynb train model end result archiv contain follow file hyperparam json model algo param model algo symbol json familiar format abl load kera kera model model json assum differ format aw proprietari help identifi format possibl load kera model infer instanc local thank",
        "Question_preprocessed_content":"load train model kera follow aw train model end result archiv contain follow file familiar format abl load kera assum differ format aw proprietari help identifi format possibl load kera model infer instanc thank",
        "Question_gpt_summary_original":"The user has encountered a challenge in loading a SageMaker trained model into Keras. They have attempted to use keras.models.model_from_json() but were unsuccessful. The user is unsure of the format and suspects it may be an AWS proprietary one. They are seeking assistance in identifying the format and determining if it is possible to do inference without an EC2 instance.",
        "Question_gpt_summary":"user encount challeng load train model kera attempt us kera model model json unsuccess user unsur format suspect aw proprietari seek assist identifi format determin possibl infer instanc",
        "Answer_original_content":"built algorithm implement apach mxnet load model local load checkpoint appropri api http mxnet apach org api python doc api mxnet model index html mxnet model load checkpoint",
        "Answer_preprocessed_content":"algorithm implement apach mxnet load model local appropri api",
        "Answer_gpt_summary_original":"Solution: The user can load the SageMaker trained model into Keras using Apache MXNet. They can use the load_checkpoint() API to load the model locally.",
        "Answer_gpt_summary":"solut user load train model kera apach mxnet us load checkpoint api load model local"
    },
    {
        "Question_title":"Custom Container deployment in vertex ai",
        "Question_body":"<p>I am trying to deploy my custom container in vertex ai endpoint for predictions. The contents of the application are as follows.<\/p>\n<ol>\n<li>Flask - app.py<\/li>\n<\/ol>\n<pre><code>import pandas as pd\nfrom flask import Flask, jsonify,request\nimport tensorflow\nimport pre_process\nimport post_process\n\n\napp = Flask(__name__)\n\n\n@app.route('\/predict',methods=['POST'])\ndef predict():\n    req = request.json.get('instances')\n    \n    input_data = req[0]['email']\n\n    #preprocessing\n    text = pre_process.preprocess(input_data)\n    vector = pre_process.preprocess_tokenizing(text)\n\n    model = tensorflow.keras.models.load_model('model')\n\n    #predict\n    prediction = model.predict(vector)\n\n    #postprocessing\n    value = post_process.postprocess(list(prediction[0])) \n    \n    return jsonify({'output':{'doc_class':value}})\n\n\nif __name__=='__main__':\n    app.run(host='0.0.0.0')\n<\/code><\/pre>\n<ol start=\"2\">\n<li>Dockerfile<\/li>\n<\/ol>\n<pre><code>FROM python:3.7\n\nWORKDIR \/app\n\nCOPY . \/app\n\nRUN pip install --trusted-host pypi.python.org -r requirements.txt \n\n\nCMD [&quot;gunicorn&quot;, &quot;--bind&quot;, &quot;0.0.0.0:5000&quot;, &quot;app:app&quot;]\n\nEXPOSE 5050\n<\/code><\/pre>\n<ol start=\"3\">\n<li>pre_process.py<\/li>\n<\/ol>\n<pre><code>#import \nimport pandas as pd\nimport pickle\nimport re\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\ndef preprocess(text):\n    &quot;&quot;&quot;Do all the Preprocessing as shown above and\n    return a tuple contain preprocess_email,preprocess_subject,preprocess_text for that Text_data&quot;&quot;&quot;\n         \n    \n    #After you store it in the list, Replace those sentances in original text by space.\n    text = re.sub(&quot;(Subject:).+&quot;,&quot; &quot;,text,re.I)\n    \n    #Delete all the sentances where sentence starts with &quot;Write to:&quot; or &quot;From:&quot;.\n    text = re.sub(&quot;((Write to:)|(From:)).+&quot;,&quot;&quot;,text,re.I)\n    \n    #Delete all the tags like &quot;&lt; anyword &gt;&quot;\n    text = re.sub(&quot;&lt;[^&gt;&lt;]+&gt;&quot;,&quot;&quot;,text)\n    \n    #Delete all the data which are present in the brackets.\n    text = re.sub(&quot;\\([^()]+\\)&quot;,&quot;&quot;,text)\n    \n    #Remove all the newlines('\\n'), tabs('\\t'), &quot;-&quot;, &quot;&quot;.\n    text = re.sub(&quot;[\\n\\t\\\\-]+&quot;,&quot;&quot;,text)\n    \n    #Remove all the words which ends with &quot;:&quot;.\n    text = re.sub(&quot;(\\w+:)&quot;,&quot;&quot;,text)\n    \n    #Decontractions, replace words like below to full words.\n\n    lines = re.sub(r&quot;n\\'t&quot;, &quot; not&quot;, text)\n    lines = re.sub(r&quot;\\'re&quot;, &quot; are&quot;, lines)\n    lines = re.sub(r&quot;\\'s&quot;, &quot; is&quot;, lines)\n    lines = re.sub(r&quot;\\'d&quot;, &quot; would&quot;, lines)\n    lines = re.sub(r&quot;\\'ll&quot;, &quot; will&quot;, lines)\n    lines = re.sub(r&quot;\\'t&quot;, &quot; not&quot;, lines)\n    lines = re.sub(r&quot;\\'ve&quot;, &quot; have&quot;, lines)\n    lines = re.sub(r&quot;\\'m&quot;, &quot; am&quot;, lines)\n    text = lines\n    \n        #replace numbers with spaces\n    text = re.sub(&quot;\\d+&quot;,&quot; &quot;,text)\n    \n        # remove _ from the words starting and\/or ending with _\n    text = re.sub(&quot;(\\s_)|(_\\s)&quot;,&quot; &quot;,text)\n    \n        #remove 1 or 2 letter word before _\n    text = re.sub(&quot;\\w{1,2}_&quot;,&quot;&quot;,text)\n    \n        #convert all letters to lowercase and remove the words which are greater \n        #than or equal to 15 or less than or equal to 2.\n    text = text.lower()\n    \n    text =&quot; &quot;.join([i for i in text.split() if len(i)&lt;15 and len(i)&gt;2])\n    \n    #replace all letters except A-Z,a-z,_ with space\n    preprocessed_text = re.sub(&quot;\\W+&quot;,&quot; &quot;,text)\n\n    return preprocessed_text\n\ndef preprocess_tokenizing(text):\n        \n    #from tf.keras.preprocessing.text import Tokenizer\n    #from tf.keras.preprocessing.sequence import pad_sequences\n    \n    tokenizer = pickle.load(open('tokenizer.pkl','rb'))\n\n    max_length = 1019\n    tokenizer.fit_on_texts([text])\n    encoded_docs = tokenizer.texts_to_sequences([text])\n    text_padded = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n    \n    return text_padded\n<\/code><\/pre>\n<ol start=\"4\">\n<li>post_process.py<\/li>\n<\/ol>\n<pre><code>def postprocess(vector):\n    index = vector.index(max(vector))\n    classes = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n    return classes[index]\n<\/code><\/pre>\n<ol start=\"4\">\n<li>requirements.txt<\/li>\n<\/ol>\n<pre><code>gunicorn\npandas==1.3.3\nnumpy==1.19.5\nflask\nflask-cors\nh5py==3.1.0\nscikit-learn==0.24.2\ntensorflow==2.6.0\n\n<\/code><\/pre>\n<ol start=\"5\">\n<li><p>model<\/p>\n<\/li>\n<li><p>tokenizer.pkl<\/p>\n<\/li>\n<\/ol>\n<p>I am following this blog <a href=\"https:\/\/medium.com\/mlearning-ai\/serverless-prediction-at-scale-part-2-custom-container-deployment-on-vertex-ai-103a43d0a290\" rel=\"nofollow noreferrer\">vertex ai deployment<\/a> for gcloud console commands to containerise and deploy the model to endpoint.But the model is taking forever to get deployed and ultimately fails to get deployed.<\/p>\n<p>After running the container in local host, it runs as expected but it is not getting deployed into vertex ai endpoint. I don't understand whether the problem is in flask app.py or Dockerfile or whether the problem lies somewhere else.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1632490921653,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":629.0,
        "Answer_body":"<p>I was able to resolve this issue by adding health route to http server. I added the following piece of code in my flask app.<\/p>\n<pre><code>@app.route('\/healthz')\ndef healthz():\n    return &quot;OK&quot;\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1632810969572,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69316032",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1632806171387,
        "Question_original_content":"custom contain deploy try deploi custom contain endpoint predict content applic follow flask app import panda flask import flask jsonifi request import tensorflow import pre process import post process app flask app rout predict method post def predict req request json instanc input data req email preprocess text pre process preprocess input data vector pre process preprocess token text model tensorflow kera model load model model predict predict model predict vector postprocess valu post process postprocess list predict return jsonifi output doc class valu main app run host dockerfil python workdir app copi app run pip instal trust host pypi python org requir txt cmd gunicorn bind app app expos pre process import import panda import pickl import import tensorflow tensorflow kera preprocess sequenc import pad sequenc def preprocess text preprocess shown return tupl contain preprocess email preprocess subject preprocess text text data store list replac sentanc origin text space text sub subject text delet sentanc sentenc start write text sub write text delet tag like text sub text delet data present bracket text sub text remov newlin tab text sub text remov word end text sub text decontract replac word like word line sub text line sub line line sub line line sub line line sub line line sub line line sub line line sub line text line replac number space text sub text remov word start end text sub text remov letter word text sub text convert letter lowercas remov word greater equal equal text text lower text join text split len replac letter space preprocess text sub text return preprocess text def preprocess token text kera preprocess text import token kera preprocess sequenc import pad sequenc token pickl load open token pkl max length token fit text text encod doc token text sequenc text text pad pad sequenc encod doc maxlen max length pad post return text pad post process def postprocess vector index vector index max vector class return class index requir txt gunicorn panda numpi flask flask cor hpy scikit learn tensorflow model token pkl follow blog deploy gcloud consol command containeris deploi model endpoint model take forev deploi ultim fail deploi run contain local host run expect get deploi endpoint understand problem flask app dockerfil problem li",
        "Question_preprocessed_content":"custom contain deploy try deploi custom contain endpoint predict content applic follow flask dockerfil model follow blog deploy gcloud consol command containeris deploi model model take forev deploi ultim fail deploi run contain local host run expect get deploi endpoint understand problem flask dockerfil problem li",
        "Question_gpt_summary_original":"The user is facing challenges in deploying a custom container in Vertex AI endpoint for predictions. The user has followed a blog for GCloud console commands to containerize and deploy the model to the endpoint, but the deployment is taking too long and ultimately failing. The user is unsure whether the problem lies in the Flask app.py or Dockerfile or somewhere else.",
        "Question_gpt_summary":"user face challeng deploi custom contain endpoint predict user follow blog gcloud consol command container deploi model endpoint deploy take long ultim fail user unsur problem li flask app dockerfil",
        "Answer_original_content":"abl resolv issu ad health rout http server ad follow piec code flask app app rout healthz def healthz return",
        "Answer_preprocessed_content":"abl resolv issu ad health rout http server ad follow piec code flask app",
        "Answer_gpt_summary_original":"Solution: One solution mentioned in the discussion is to add a health route to the Flask app by adding a piece of code to the app.py file. This solution helped the user to resolve the issue of the custom container deployment taking too long and ultimately failing.",
        "Answer_gpt_summary":"solut solut mention discuss add health rout flask app ad piec code app file solut help user resolv issu custom contain deploy take long ultim fail"
    },
    {
        "Question_title":"SigOpt run improvement",
        "Question_body":"<p>In the \u201ctrials vs best value\u201d visualization I see a lot of points with the same value. Does this mean that SigOpt is not finding any improvement for those runs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645558490644,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":149.0,
        "Answer_body":"<p>What that particular graph is showing is the best seen value, or trace, so far for all the completed iterations. This means that the graph will only go up when a new and better run has been found.<\/p>\n<p>The fact that there can be multiple runs in between improvements might seem a bit counter intuitive. However, this is a product of Bayesian optimization relaying on both exploration and exploitation to find a good optimum.<\/p>\n<p>To be the most efficient at finding a good optimum the optimization algorithms both need to explorer the search space, while also exploiting the optimums that we already know.<\/p>\n<p>So whenever we are not seeing a better run for a given iteration, so does not mean that the iteration has been wasted, this just means that we have learned that a given run is not a better optimum - this information is just as valuable for the algorithm.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.sigopt.com\/t\/sigopt-run-improvement\/94",
        "Tool":"SigOpt",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-31T12:05:48.236Z",
                "Answer_body":"<p>What that particular graph is showing is the best seen value, or trace, so far for all the completed iterations. This means that the graph will only go up when a new and better run has been found.<\/p>\n<p>The fact that there can be multiple runs in between improvements might seem a bit counter intuitive. However, this is a product of Bayesian optimization relaying on both exploration and exploitation to find a good optimum.<\/p>\n<p>To be the most efficient at finding a good optimum the optimization algorithms both need to explorer the search space, while also exploiting the optimums that we already know.<\/p>\n<p>So whenever we are not seeing a better run for a given iteration, so does not mean that the iteration has been wasted, this just means that we have learned that a given run is not a better optimum - this information is just as valuable for the algorithm.<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"run improv trial best valu visual lot point valu mean find improv run",
        "Question_preprocessed_content":"run improv trial best valu visual lot point valu mean find improv run",
        "Question_gpt_summary_original":"The user is questioning whether SigOpt is not finding any improvement for certain runs, as they see many points with the same value in the \"trials vs best value\" visualization.",
        "Question_gpt_summary":"user question find improv certain run point valu trial best valu visual",
        "Answer_original_content":"particular graph show best seen valu trace far complet iter mean graph new better run fact multipl run improv bit counter intuit product bayesian optim relai explor exploit good optimum effici find good optimum optim algorithm need explor search space exploit optimum know see better run given iter mean iter wast mean learn given run better optimum inform valuabl algorithm",
        "Answer_preprocessed_content":"particular graph show best seen valu trace far complet iter mean graph new better run fact multipl run improv bit counter intuit product bayesian optim relai explor exploit good optimum effici find good optimum optim algorithm need explor search space exploit optimum know see better run given iter mean iter wast mean learn given run better optimum inform valuabl algorithm",
        "Answer_gpt_summary_original":"Solution: The discussion explains that the \"trials vs best value\" visualization shows the best seen value so far for all completed iterations. The optimization algorithm relies on both exploration and exploitation to find a good optimum, so there may be multiple runs between improvements. Not seeing a better run for a given iteration does not mean that the iteration has been wasted, as this information is valuable for the algorithm. Therefore, the solution is to continue running the optimization algorithm and trust the process.",
        "Answer_gpt_summary":"solut discuss explain trial best valu visual show best seen valu far complet iter optim algorithm reli explor exploit good optimum multipl run improv see better run given iter mean iter wast inform valuabl algorithm solut continu run optim algorithm trust process"
    },
    {
        "Question_title":"DVC with bitbucket",
        "Question_body":"<p>I want to setup DVC on top of bitbucket instead of git. How can i do it. Can DVC integrate with bitbucket.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1564573379958,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":2091.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/writetoneeraj\">@writetoneeraj<\/a> !<\/p>\n<p>Bitbucket is also using git though. For dvc it doesn\u2019t matter which git server you are using to store your git project, it could be gitlab, github, bitbucket, even your own git server or no git server at all. Mind that dvc is using git to store tiny meta-files alongside your code, and the data files themselves are stored outside of git, so if you want to share them, you\u2019ll need to set up a dvc remote ,which could be anything from ssh server to hdfs, s3, gs, azure, etc. You might find this [1] article useful <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>[1] <a href=\"https:\/\/dvc.org\/doc\/use-cases\/share-data-and-model-files\" rel=\"nofollow noopener\">https:\/\/dvc.org\/doc\/use-cases\/share-data-and-model-files<\/a><\/p>\n<p>Thanks,<br>\nRuslan<\/p>. <aside class=\"quote no-group\" data-username=\"writetoneeraj\" data-post=\"1\" data-topic=\"192\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/w\/dec6dc\/40.png\" class=\"avatar\"> writetoneeraj:<\/div>\n<blockquote>\n<p>Can DVC integrate with bitbucket.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Hello,<\/p>\n<p>While Git is used to storing and version code, DVC does the same for data and model files. Git can store code locally and also on a hosting service like GitHub, <a href=\"https:\/\/mindmajix.com\/bitbucket-training\" rel=\"noopener nofollow ugc\">Bitbucket<\/a>, or GitLab. Likewise, DVC uses a remote repository to store all your data and models.<\/p>\n<p>Thanks &amp; Regards,<br>\nAnita<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-with-bitbucket\/192",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-07-31T12:51:43.230Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/writetoneeraj\">@writetoneeraj<\/a> !<\/p>\n<p>Bitbucket is also using git though. For dvc it doesn\u2019t matter which git server you are using to store your git project, it could be gitlab, github, bitbucket, even your own git server or no git server at all. Mind that dvc is using git to store tiny meta-files alongside your code, and the data files themselves are stored outside of git, so if you want to share them, you\u2019ll need to set up a dvc remote ,which could be anything from ssh server to hdfs, s3, gs, azure, etc. You might find this [1] article useful <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>[1] <a href=\"https:\/\/dvc.org\/doc\/use-cases\/share-data-and-model-files\" rel=\"nofollow noopener\">https:\/\/dvc.org\/doc\/use-cases\/share-data-and-model-files<\/a><\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-31T12:46:51.588Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"writetoneeraj\" data-post=\"1\" data-topic=\"192\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/w\/dec6dc\/40.png\" class=\"avatar\"> writetoneeraj:<\/div>\n<blockquote>\n<p>Can DVC integrate with bitbucket.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Hello,<\/p>\n<p>While Git is used to storing and version code, DVC does the same for data and model files. Git can store code locally and also on a hosting service like GitHub, <a href=\"https:\/\/mindmajix.com\/bitbucket-training\" rel=\"noopener nofollow ugc\">Bitbucket<\/a>, or GitLab. Likewise, DVC uses a remote repository to store all your data and models.<\/p>\n<p>Thanks &amp; Regards,<br>\nAnita<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"bitbucket want setup bitbucket instead git integr bitbucket",
        "Question_preprocessed_content":"bitbucket want setup bitbucket instead git integr bitbucket",
        "Question_gpt_summary_original":"The user is seeking guidance on how to set up DVC on top of Bitbucket instead of Git and is unsure if DVC can integrate with Bitbucket.",
        "Question_gpt_summary":"user seek guidanc set bitbucket instead git unsur integr bitbucket",
        "Answer_original_content":"writetoneeraj bitbucket git doesnt matter git server store git project gitlab github bitbucket git server git server mind git store tini meta file alongsid code data file store outsid git want share youll need set remot ssh server hdf azur articl us http org doc us case share data model file thank ruslan writetoneeraj integr bitbucket hello git store version code data model file git store code local host servic like github bitbucket gitlab likewis us remot repositori store data model thank regard anita",
        "Answer_preprocessed_content":"bitbucket git doesnt matter git server store git project gitlab github bitbucket git server git server mind git store tini alongsid code data file store outsid git want share youll need set remot ssh server hdf azur articl us thank ruslan writetoneeraj integr bitbucket hello git store version code data model file git store code local host servic like github bitbucket gitlab likewis us remot repositori store data model thank regard anita",
        "Answer_gpt_summary_original":"Solution:\n- DVC can integrate with Bitbucket as it doesn't matter which git server is used to store the git project.\n- DVC uses git to store tiny meta-files alongside the code, and the data files themselves are stored outside of git. To share them, a DVC remote needs to be set up, which could be anything from ssh server to hdfs, s3, gs, azure, etc.",
        "Answer_gpt_summary":"solut integr bitbucket matter git server store git project us git store tini meta file alongsid code data file store outsid git share remot need set ssh server hdf azur"
    },
    {
        "Question_title":"Tracking data and code dependencies",
        "Question_body":"<p>Hello!<br>\nI am currently evaluating tools for keeping track of data in our workflow, and I am looking at how DVC works. First off, thanks for developing it, Dmitry, as it is a very nice tool.<\/p>\n<p>I have read the tutorial on your blog and there are a couple of question I have, though, which I suspect are quite critical.<\/p>\n<p>First: the use of hard links is a great idea when it comes to speed, but what happens if one changes the data file? I tried and the <code>dvc status<\/code> reports \u201ccorrupted cache data\u201d, as the new hash is different from the saved one. Apparently, there is no way of going back, is dvc assuming that data are immutable? Because it might be in some use cases, but not in general. Also, mistakes might happen.<\/p>\n<p>Second, the run functionality is extremely useful, but it appears to be as much fragile. Apparently, dvc is not tied to git when it comes to tracking which files are used for a specific run: I have to specify the dependency to source code using the -d flag to make runs reproducible. What happens if I do not specify a file that is actually being used? What happens if that file changes and a new run is executed? Apparently, dvc does not check for differences and no message or warning is raised. repro can be forced, and results change, but no warning is raised even in this case.<\/p>\n<p>So, I am interested into knowing if I am using the tool in an inappropriate way: am I missing something in DVC philosophy?<\/p>\n<p>Thanks!<br>\n~Alessandro<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1526466665911,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":1491.0,
        "Answer_body":"<p>Hi Alessandro!<\/p>\n<p>What we really want are reflinks(essentially CoW files), but they are not available on every fs as of right now. In dvc 0.9.7(which is going to be released this week) dvc tries all possibilities: reflink\/hardlink\/symlink\/copy until it finds something that works. Yes, when hardlinks are used(or symlinks), dvc assumes that you won\u2019t modify data by-hand without re-adding it, so if it detects that linked cache file was suddenly changed, it just removes it and tells user that there was a corrupted cache detected. Unfortunately there is no efficient way around it, except using reflinks, which are fortunately coming to every modern fs, so we will be able to use them exclusively in the future.<\/p>\n<p>We previously had an auto-magical system in place, that tracked new files after the run and automatically added them as outputs to dvc file. Unfortunately, that system was proven to be too implicit, which resulted in often unexpected behavior, as only the user truly knows which files he considers as outputs and which he considers as dependencies. Thus we implemented the current explicit system, so that users know for sure which files are dependencies and which are outputs for their stage.<\/p>\n<p>If you don\u2019t specify any dependencies, dvc will always treat your stage as changed and will always reproduce it. If you don\u2019t specify particular dependency, then you will have it hidden from dvc, so dvc will not be able to track it and reproduce your pipeline when that dependency is changed. Dvc can\u2019t know which files you want to track(or be warned about) if you don\u2019t specify them, so \u2018repro\u2019 and especially \u2018repro --force\u2019 behavior is totally normal <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=5\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>It definitely seems like dvc is an appropriate tool for your project. Maybe you could elaborate on your scenario, so we could suggest you some tips and tricks?<\/p>\n<p>Cheers,<br>\nRuslan<\/p>. <p>Hello Ruslan,<br>\nthank you for your wonderful answer!<\/p>\n<p>First, it\u2019s wonderful to know that reflinks are coming, but I wonder if our platforms are supported. I searched on the Github issue tracker to read about it (Issue <span class=\"hashtag\">#280<\/span>) but it is not clear to me if copy can be used instead (by default). I\u2019d rather trade speed for safety on this, so it would be great if DVC could be configured to prefer reflink over copy over hard\/softlink.<\/p>\n<p>Regarding the auto-magical system to track new files: yes, I read about that somewhere and I agree the current system is better. Explicit is better than implicit <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=5\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>On this, I must admit that, in some ways, I prefer the approach that Pachyderm is using: having a virtual filesystem where data is exposed, and applications are free to read\/write from there to have automatic tracking of outputs. With DVC, docker is not necessary (as it is in Pachyderm) so a thin virtual filesystem layer would be sufficient, but I don\u2019t know if this is portable (FUSE on Linux, BSD and Mac, but I don\u2019t know what\u2019s available on Windows).<br>\nI think that would be a nice alternative to how DVC is currently operating, but I understand it might take too much dev effort.<\/p>\n<p>Regarding the tracking: thanks for letting me know. I\u2019ll grok this, then ask again if something is still unclear <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=5\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>Regarding our use case: it\u2019s very similar to <a href=\"https:\/\/discuss.dvc.org\/t\/does-dvc-fit-in-a-local-area-network-infrastucture-where-git-repos-are-not-in-the-computing-server\/24\">this<\/a>, but it\u2019s kind of complex as we have some requirements regarding authentication and data access. I think it\u2019s better to start a new conversation, but I\u2019ll take some time to learn more about DVC before doing that.<\/p>\n<p>Thanks!<br>\n~Alessandro<\/p>. <p>And thank you for the feedback, we really appreciate it!<\/p>\n<p>Your safety concerns are totally understandable and starting from 0.9.7 you will be able to select default type of links in the config. E.g. to select \u2018copy\u2019 you would need to to run <code>dvc config cache.type copy<\/code>. Unfortunately, as of 0.9.7, you will not be able to alter the order of preferred types, but I totally agree that it is indeed a great idea and we will definitely consider adding support for it in 0.9.8(<a href=\"https:\/\/github.com\/dataversioncontrol\/dvc\/issues\/709\">https:\/\/github.com\/dataversioncontrol\/dvc\/issues\/709<\/a>).<\/p>\n<p>We have actually considered adding docker and\/or FUSE support to provide another level of isolation and reproducibility, but just didn\u2019t have time to get to it. But we will definitely look into it in the future!<\/p>\n<p>Sure, feel free to ping us, we are happy to help <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=5\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>Cheers,<br>\nRuslan<\/p>. <p>Alessandro, thank you for the questions! Ruslan, thanks for the answers!<\/p>\n<p>You are right about immutable data files. The new <code>copy<\/code> semantic can solve the issue with some negative impact on performance (you will feel that with 5Gb+ file size). And, as Ruslan described, <code>Reflink<\/code> is a way to solve the issue without the impact on performance.<\/p>\n<p>This feature was just released: <a href=\"https:\/\/discuss.dvc.org\/t\/dvc-0-9-7-release\/29\" class=\"inline-onebox\">DVC 0.9.7 release<\/a><\/p>\n<p>Thanks,<br>\nDmitry<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/tracking-data-and-code-dependencies\/28",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-05-16T11:10:52.736Z",
                "Answer_body":"<p>Hi Alessandro!<\/p>\n<p>What we really want are reflinks(essentially CoW files), but they are not available on every fs as of right now. In dvc 0.9.7(which is going to be released this week) dvc tries all possibilities: reflink\/hardlink\/symlink\/copy until it finds something that works. Yes, when hardlinks are used(or symlinks), dvc assumes that you won\u2019t modify data by-hand without re-adding it, so if it detects that linked cache file was suddenly changed, it just removes it and tells user that there was a corrupted cache detected. Unfortunately there is no efficient way around it, except using reflinks, which are fortunately coming to every modern fs, so we will be able to use them exclusively in the future.<\/p>\n<p>We previously had an auto-magical system in place, that tracked new files after the run and automatically added them as outputs to dvc file. Unfortunately, that system was proven to be too implicit, which resulted in often unexpected behavior, as only the user truly knows which files he considers as outputs and which he considers as dependencies. Thus we implemented the current explicit system, so that users know for sure which files are dependencies and which are outputs for their stage.<\/p>\n<p>If you don\u2019t specify any dependencies, dvc will always treat your stage as changed and will always reproduce it. If you don\u2019t specify particular dependency, then you will have it hidden from dvc, so dvc will not be able to track it and reproduce your pipeline when that dependency is changed. Dvc can\u2019t know which files you want to track(or be warned about) if you don\u2019t specify them, so \u2018repro\u2019 and especially \u2018repro --force\u2019 behavior is totally normal <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=5\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>It definitely seems like dvc is an appropriate tool for your project. Maybe you could elaborate on your scenario, so we could suggest you some tips and tricks?<\/p>\n<p>Cheers,<br>\nRuslan<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-05-16T11:53:41.431Z",
                "Answer_body":"<p>Hello Ruslan,<br>\nthank you for your wonderful answer!<\/p>\n<p>First, it\u2019s wonderful to know that reflinks are coming, but I wonder if our platforms are supported. I searched on the Github issue tracker to read about it (Issue <span class=\"hashtag\">#280<\/span>) but it is not clear to me if copy can be used instead (by default). I\u2019d rather trade speed for safety on this, so it would be great if DVC could be configured to prefer reflink over copy over hard\/softlink.<\/p>\n<p>Regarding the auto-magical system to track new files: yes, I read about that somewhere and I agree the current system is better. Explicit is better than implicit <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=5\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>On this, I must admit that, in some ways, I prefer the approach that Pachyderm is using: having a virtual filesystem where data is exposed, and applications are free to read\/write from there to have automatic tracking of outputs. With DVC, docker is not necessary (as it is in Pachyderm) so a thin virtual filesystem layer would be sufficient, but I don\u2019t know if this is portable (FUSE on Linux, BSD and Mac, but I don\u2019t know what\u2019s available on Windows).<br>\nI think that would be a nice alternative to how DVC is currently operating, but I understand it might take too much dev effort.<\/p>\n<p>Regarding the tracking: thanks for letting me know. I\u2019ll grok this, then ask again if something is still unclear <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=5\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>Regarding our use case: it\u2019s very similar to <a href=\"https:\/\/discuss.dvc.org\/t\/does-dvc-fit-in-a-local-area-network-infrastucture-where-git-repos-are-not-in-the-computing-server\/24\">this<\/a>, but it\u2019s kind of complex as we have some requirements regarding authentication and data access. I think it\u2019s better to start a new conversation, but I\u2019ll take some time to learn more about DVC before doing that.<\/p>\n<p>Thanks!<br>\n~Alessandro<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-05-16T12:34:58.534Z",
                "Answer_body":"<p>And thank you for the feedback, we really appreciate it!<\/p>\n<p>Your safety concerns are totally understandable and starting from 0.9.7 you will be able to select default type of links in the config. E.g. to select \u2018copy\u2019 you would need to to run <code>dvc config cache.type copy<\/code>. Unfortunately, as of 0.9.7, you will not be able to alter the order of preferred types, but I totally agree that it is indeed a great idea and we will definitely consider adding support for it in 0.9.8(<a href=\"https:\/\/github.com\/dataversioncontrol\/dvc\/issues\/709\">https:\/\/github.com\/dataversioncontrol\/dvc\/issues\/709<\/a>).<\/p>\n<p>We have actually considered adding docker and\/or FUSE support to provide another level of isolation and reproducibility, but just didn\u2019t have time to get to it. But we will definitely look into it in the future!<\/p>\n<p>Sure, feel free to ping us, we are happy to help <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=5\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>Cheers,<br>\nRuslan<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-05-18T00:11:47.713Z",
                "Answer_body":"<p>Alessandro, thank you for the questions! Ruslan, thanks for the answers!<\/p>\n<p>You are right about immutable data files. The new <code>copy<\/code> semantic can solve the issue with some negative impact on performance (you will feel that with 5Gb+ file size). And, as Ruslan described, <code>Reflink<\/code> is a way to solve the issue without the impact on performance.<\/p>\n<p>This feature was just released: <a href=\"https:\/\/discuss.dvc.org\/t\/dvc-0-9-7-release\/29\" class=\"inline-onebox\">DVC 0.9.7 release<\/a><\/p>\n<p>Thanks,<br>\nDmitry<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"track data code depend hello current evalu tool keep track data workflow look work thank develop dmitri nice tool read tutori blog coupl question suspect critic us hard link great idea come speed happen chang data file tri statu report corrupt cach data new hash differ save appar wai go assum data immut us case gener mistak happen second run function extrem us appear fragil appar ti git come track file specif run specifi depend sourc code flag run reproduc happen specifi file actual happen file chang new run execut appar check differ messag warn rais repro forc result chang warn rais case interest know tool inappropri wai miss philosophi thank alessandro",
        "Question_preprocessed_content":"track data code depend hello current evalu tool keep track data workflow look work thank develop dmitri nice tool read tutori blog coupl question suspect critic us hard link great idea come speed happen chang data file tri report corrupt cach data new hash differ save appar wai go assum data immut us case gener mistak happen second run function extrem us appear fragil appar ti git come track file specif run specifi depend sourc code flag run reproduc happen specifi file actual happen file chang new run execut appar check differ messag warn rais repro forc result chang warn rais case interest know tool inappropri wai miss philosophi thank alessandro",
        "Question_gpt_summary_original":"The user is evaluating DVC for tracking data in their workflow and has encountered two critical challenges. Firstly, DVC assumes that data is immutable and does not allow going back if the data file is changed. Secondly, the run functionality is fragile and not tied to git, which may result in runs being non-reproducible if a file is not specified or changes without any warning. The user is seeking clarification on whether they are using the tool inappropriately and if they are missing something in DVC philosophy.",
        "Question_gpt_summary":"user evalu track data workflow encount critic challeng firstli assum data immut allow go data file chang secondli run function fragil ti git result run non reproduc file specifi chang warn user seek clarif tool inappropri miss philosophi",
        "Answer_original_content":"alessandro want reflink essenti cow file avail right go releas week tri possibl reflink hardlink symlink copi find work ye hardlink symlink assum wont modifi data hand ad detect link cach file suddenli chang remov tell user corrupt cach detect unfortun effici wai reflink fortun come modern abl us exclus futur previous auto magic place track new file run automat ad output file unfortun proven implicit result unexpect behavior user truli know file consid output consid depend implement current explicit user know sure file depend output stage dont specifi depend treat stage chang reproduc dont specifi particular depend hidden abl track reproduc pipelin depend chang know file want track warn dont specifi repro especi repro forc behavior total normal definit like appropri tool project mayb elabor scenario suggest tip trick cheer ruslan hello ruslan thank wonder answer wonder know reflink come wonder platform support search github issu tracker read issu clear copi instead default trade speed safeti great configur prefer reflink copi hard softlink auto magic track new file ye read agre current better explicit better implicit admit wai prefer approach pachyderm have virtual filesystem data expos applic free read write automat track output docker necessari pachyderm virtual filesystem layer suffici dont know portabl fuse linux bsd mac dont know what avail window think nice altern current oper understand dev effort track thank let know ill grok ask unclear us case similar kind complex requir authent data access think better start new convers ill time learn thank alessandro thank feedback appreci safeti concern total understand start abl select default type link config select copi need run config cach type copi unfortun abl alter order prefer type total agre great idea definit consid ad support http github com dataversioncontrol issu actual consid ad docker fuse support provid level isol reproduc didnt time definit look futur sure feel free ping happi help cheer ruslan alessandro thank question ruslan thank answer right immut data file new copi semant solv issu neg impact perform feel file size ruslan describ reflink wai solv issu impact perform featur releas releas thank dmitri",
        "Answer_preprocessed_content":"alessandro want reflink avail right go releas week tri possibl find work ye hardlink assum wont modifi data detect link cach file suddenli chang remov tell user corrupt cach detect unfortun effici wai reflink fortun come modern abl us exclus futur previous place track new file run automat ad output file unfortun proven implicit result unexpect behavior user truli know file consid output consid depend implement current explicit user know sure file depend output stage dont specifi depend treat stage chang reproduc dont specifi particular depend hidden abl track reproduc pipelin depend chang know file want track dont specifi repro especi repro behavior total normal definit like appropri tool project mayb elabor scenario suggest tip trick cheer ruslan hello ruslan thank wonder answer wonder know reflink come wonder platform support search github issu tracker read clear copi instead trade speed safeti great configur prefer reflink copi track new file ye read agre current better explicit better implicit admit wai prefer approach pachyderm have virtual filesystem data expos applic free automat track output docker necessari virtual filesystem layer suffici dont know portabl think nice altern current oper understand dev effort track thank let know ill grok ask unclear us case similar kind complex requir authent data access think better start new convers ill time learn thank alessandro thank feedback appreci safeti concern total understand start abl select default type link config select copi need run unfortun abl alter order prefer type total agre great idea definit consid ad support actual consid ad docker fuse support provid level isol reproduc didnt time definit look futur sure feel free ping happi help cheer ruslan alessandro thank question ruslan thank answer right immut data file new semant solv issu neg impact perform ruslan describ wai solv issu impact perform featur releas releas thank dmitri",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- DVC 0.9.7 tries all possibilities (reflink\/hardlink\/symlink\/copy) until it finds something that works. When hardlinks are used (or symlinks), DVC assumes that the data won't be modified by-hand without re-adding it. If it detects that the linked cache file was suddenly changed, it removes it and tells the user that there was a corrupted cache detected.\n- DVC can't know which files the user wants to track (or be warned about) if they don't specify them, so 'repro' and especially 'repro --force' behavior is totally normal.\n- Starting from 0.9.7, the",
        "Answer_gpt_summary":"possibl solut mention discuss tri possibl reflink hardlink symlink copi find work hardlink symlink assum data won modifi hand ad detect link cach file suddenli chang remov tell user corrupt cach detect know file user want track warn specifi repro especi repro forc behavior total normal start"
    },
    {
        "Question_title":"Embarrassingly parallel hyperparameter search via Azure + DataBricks + MLFlow",
        "Question_body":"<p>Conceptual question.  My company is pushing Azure + DataBricks.  I am trying to understand where this can take us.<\/p>\n<p>I am porting some work I've done locally to the Azure + Databricks platform.  I want to run an experiment with a large number of hyperparameter combinations using Azure + Databricks + MLfLow.  I am using PyTorch to implement my models.<\/p>\n<p>I have a cluster with 8 nodes.  I want to kick off the parameter search across all of the nodes in an embarrassingly parallel manner (one run per node, running independently).  Is this as simple as creating a MLflow project and then using the mlflow.projects.run command for each hyperparameter combination and Databricks + MLflow will take care of the rest?<\/p>\n<p>Is this technology capable of this?  I'm looking for some references I could use to make this happen.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1594133526057,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1594134725200,
        "Question_score":0.0,
        "Question_view_count":262.0,
        "Answer_body":"<p>The short answer is yes, it's possible, but won't be exactly as easy as running a single mlflow command. You can paralelize single-node workflows using spark Python UDFs, a good example of this is this <a href=\"https:\/\/pages.databricks.com\/rs\/094-YMS-629\/images\/Fine-Grained-Time-Series-Forecasting.html?_ga=2.64430959.1760852900.1593769579-972789996.1561118598\" rel=\"nofollow noreferrer\">notebook<\/a><\/p>\n<p>I'm not sure if this will work with pytorch, but there is hyperopt library that lets you parallelize search across parameters using Spark - it's integrated with mlflow and available in databricks ML runtime. I've been using it only with scikit-learn, but it may be <a href=\"https:\/\/docs.databricks.com\/applications\/machine-learning\/automl\/hyperopt\/hyperopt-model-selection.html\" rel=\"nofollow noreferrer\">worth checking out<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62778020",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1594986774772,
        "Question_original_content":"embarrassingli parallel hyperparamet search azur databrick conceptu question compani push azur databrick try understand port work local azur databrick platform want run experi larg number hyperparamet combin azur databrick pytorch implement model cluster node want kick paramet search node embarrassingli parallel manner run node run independ simpl creat project project run command hyperparamet combin databrick care rest technolog capabl look refer us happen",
        "Question_preprocessed_content":"embarrassingli parallel hyperparamet search azur databrick conceptu question compani push azur databrick try understand port work local azur databrick platform want run experi larg number hyperparamet combin azur databrick pytorch implement model cluster node want kick paramet search node embarrassingli parallel manner simpl creat project command hyperparamet combin databrick care rest technolog capabl look refer us happen",
        "Question_gpt_summary_original":"The user is facing the challenge of understanding how to use Azure + DataBricks + MLFlow to run an experiment with a large number of hyperparameter combinations in an embarrassingly parallel manner using PyTorch. They are unsure if creating an MLflow project and using the mlflow.projects.run command for each hyperparameter combination will work and are looking for references to help them achieve this.",
        "Question_gpt_summary":"user face challeng understand us azur databrick run experi larg number hyperparamet combin embarrassingli parallel manner pytorch unsur creat project project run command hyperparamet combin work look refer help achiev",
        "Answer_original_content":"short answer ye possibl won exactli easi run singl command paralel singl node workflow spark python udf good exampl notebook sure work pytorch hyperopt librari let parallel search paramet spark integr avail databrick runtim scikit learn worth check",
        "Answer_preprocessed_content":"short answer ye possibl won exactli easi run singl command paralel workflow spark python udf good exampl notebook sure work pytorch hyperopt librari let parallel search paramet spark integr avail databrick runtim worth check",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion include parallelizing single-node workflows using Spark Python UDFs and using the hyperopt library to parallelize search across parameters using Spark, which is integrated with MLflow and available in Databricks ML runtime. However, it is not clear if these solutions will work with PyTorch.",
        "Answer_gpt_summary":"possibl solut mention discuss includ parallel singl node workflow spark python udf hyperopt librari parallel search paramet spark integr avail databrick runtim clear solut work pytorch"
    },
    {
        "Question_title":"I have added my google drive as a remote storage",
        "Question_body":"<p>\u2026but when I do a dvc push, i get the below error. Any idea how to resolve this. i am following the getting started sessions to get myself familiar with the product<\/p>\n<p>ERROR: unexpected error - : &lt;HttpError 404 when requesting <a href=\"https:\/\/www.googleapis.com\/drive\/v2\/files\/16uQro82U1-Ox3ABLZ0Oa7uNUJr0T9BD-?fields=driveId&amp;supportsAllDrives=true&amp;alt=json\" rel=\"noopener nofollow ugc\">https:\/\/www.googleapis.com\/drive\/v2\/files\/16uQro82U1-Ox3ABLZ0Oa7uNUJr0T9BD-?fields=driveId&amp;supportsAllDrives=true&amp;alt=json<\/a> returned \u201cFile not found: 16uQro82U1-Ox3ABLZ0Oa7uNUJr0T9BD-\u201d. Details: \u201c[{\u2018domain\u2019: \u2018global\u2019, \u2018reason\u2019: \u2018notFound\u2019, \u2018message\u2019: \u2018File not found: 16uQro82U1-Ox3ABLZ0Oa7uNUJr0T9BD-\u2019, \u2018locationType\u2019: \u2018other\u2019, \u2018location\u2019: \u2018file\u2019}]\u201d&gt;<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_creation_time":1644813615375,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1287.0,
        "Answer_body":"<p>Hi, could you please share the output of <code>dvc push -v<\/code> and <code>dvc doctor<\/code>? Thanks.<\/p>. <p>It is showing a permission error but I have already given the permission<\/p>\n<p>Traceback (most recent call last):<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/fs\/spec.py\u201d, line 121, in _gdrive_shared_drive_id<br>\nitem.FetchMetadata(\u201cdriveId\u201d)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/auth.py\u201d, line 84, in _decorated<br>\nreturn decoratee(self, *args, **kwargs)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/files.py\u201d, line 448, in FetchMetadata<br>\nraise ApiRequestError(error)<br>\npydrive2.files.ApiRequestError: &lt;HttpError 404 when requesting <a href=\"https:\/\/www.googleapis.com\/drive\/v2\/files\/16uQro82U1-Ox3ABLZ0Oa7uNUJr0T9BD-?fields=driveId&amp;supportsAllDrives=true&amp;alt=json\" rel=\"noopener nofollow ugc\">https:\/\/www.googleapis.com\/drive\/v2\/files\/16uQro82U1-Ox3ABLZ0Oa7uNUJr0T9BD-?fields=driveId&amp;supportsAllDrives=true&amp;alt=json<\/a> returned \u201cFile not found: 16uQro82U1-Ox3ABLZ0Oa7uNUJr0T9BD-\u201d. Details: \u201c[{\u2018domain\u2019: \u2018global\u2019, \u2018reason\u2019: \u2018notFound\u2019, \u2018message\u2019: \u2018File not found: 16uQro82U1-Ox3ABLZ0Oa7uNUJr0T9BD-\u2019, \u2018locationType\u2019: \u2018other\u2019, \u2018location\u2019: \u2018file\u2019}]\u201d&gt;<\/p>\n<p>The above exception was the direct cause of the following exception:<\/p>\n<p>Traceback (most recent call last):<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/main.py\u201d, line 55, in main<br>\nret = cmd.do_run()<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/command\/base.py\u201d, line 45, in do_run<br>\nreturn self.run()<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/command\/data_sync.py\u201d, line 57, in run<br>\nprocessed_files_count = self.repo.push(<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/repo\/<strong>init<\/strong>.py\u201d, line 49, in wrapper<br>\nreturn f(repo, *args, **kwargs)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/repo\/push.py\u201d, line 56, in push<br>\npushed += self.cloud.push(<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/data_cloud.py\u201d, line 85, in push<br>\nreturn transfer(<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/objects\/transfer.py\u201d, line 153, in transfer<br>\nstatus = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/objects\/status.py\u201d, line 158, in compare_status<br>\ndest_exists, dest_missing = status(<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/objects\/status.py\u201d, line 131, in status<br>\nexists.update(odb.hashes_exist(hashes, name=odb.fs_path, **kwargs))<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/objects\/db\/base.py\u201d, line 470, in hashes_exist<br>\nremote_hashes = self.list_hashes_exists(hashes, jobs, name)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/objects\/db\/base.py\u201d, line 421, in list_hashes_exists<br>\nret = list(itertools.compress(hashes, in_remote))<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/concurrent\/futures\/_base.py\u201d, line 608, in result_iterator<br>\nyield fs.pop().result()<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/concurrent\/futures\/_base.py\u201d, line 445, in result<br>\nreturn self.__get_result()<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/concurrent\/futures\/_base.py\u201d, line 390, in __get_result<br>\nraise self._exception<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/concurrent\/futures\/thread.py\u201d, line 52, in run<br>\nresult = self.fn(*self.args, **self.kwargs)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/objects\/db\/base.py\u201d, line 412, in exists_with_progress<br>\nret = self.fs.exists(fs_path)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/fs\/fsspec_wrapper.py\u201d, line 91, in exists<br>\nreturn self.fs.exists(path)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/fs\/spec.py\u201d, line 225, in exists<br>\nself._get_item_id(path)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/fs\/spec.py\u201d, line 192, in _get_item_id<br>\nitem_ids = self._path_to_item_ids(base, create, use_cache)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/fs\/spec.py\u201d, line 174, in _path_to_item_ids<br>\nitem_ids = self._get_cached_item_ids(path, use_cache)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/fs\/spec.py\u201d, line 170, in _get_cached_item_ids<br>\nreturn self._ids_cache[\u201cdirs\u201d].get(path, [])<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/funcy\/objects.py\u201d, line 50, in <strong>get<\/strong><br>\nreturn prop.<strong>get<\/strong>(instance, type)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/funcy\/objects.py\u201d, line 28, in <strong>get<\/strong><br>\nres = instance.<strong>dict<\/strong>[self.fget.<strong>name<\/strong>] = self.fget(instance)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/fs\/spec.py\u201d, line 82, in _ids_cache<br>\nfor item in self._gdrive_list(<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/fs\/spec.py\u201d, line 132, in _gdrive_list<br>\nparam.update(self._list_params)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/funcy\/objects.py\u201d, line 28, in <strong>get<\/strong><br>\nres = instance.<strong>dict<\/strong>[self.fget.<strong>name<\/strong>] = self.fget(instance)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/fs\/spec.py\u201d, line 100, in _list_params<br>\ndrive_id = self._gdrive_shared_drive_id(self.root)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/funcy\/decorators.py\u201d, line 45, in wrapper<br>\nreturn deco(call, *dargs, **dkwargs)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/funcy\/flow.py\u201d, line 127, in retry<br>\nreturn call()<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/funcy\/decorators.py\u201d, line 66, in <strong>call<\/strong><br>\nreturn self._func(*self._args, **self._kwargs)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/fs\/spec.py\u201d, line 125, in _gdrive_shared_drive_id<br>\nraise PermissionError from exc<br>\nPermissionError<\/p>. <p>How did you setup gdrive? Did you follow this <a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote\">setup guide<\/a>?<\/p>. <p>Hi, <a class=\"mention\" href=\"\/u\/skshetry\">@skshetry<\/a> not sure if this was solved but I encountered the same issue. I followed along with the remote storage setup. However, one difference is that I did not use dvc add because I had written the dependencies and outputs directly in the dvc.yaml file.  Secondly, I only allowed dvc access \/ permission to see, create, and delete its own configuration data. If I need to change the dvc permissons, how do I do that after the initial setup?<\/p>\n<p>\u2013solved\u2013<br>\nSo I guess dvc needs access to everything. The authorization section was helpful, <a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote?_ga=2.39431651.1183132044.1651410796-478586346.1651410796#authorization\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote?_ga=2.39431651.1183132044.1651410796-478586346.1651410796#authorization<\/a><br>\njust delete the user credentials in the .dvc\/tmp file and try again.<\/p>. <p>Hello if anyone is willing to help I have tried the above steps and to my surprise it worked on my college id when I tried to push but on my personal gmail its not working.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/cefc83e22d7923b9685b91b2e0d617acb928dfe0.jpeg\" data-download-href=\"\/uploads\/short-url\/tx5mWVDv30aQc6VHkGRpMi28E4E.jpeg?dl=1\" title=\"329180134ee2fb6417bf6b250e5ddb4a-1\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/cefc83e22d7923b9685b91b2e0d617acb928dfe0_2_386x500.jpeg\" alt=\"329180134ee2fb6417bf6b250e5ddb4a-1\" data-base62-sha1=\"tx5mWVDv30aQc6VHkGRpMi28E4E\" width=\"386\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/cefc83e22d7923b9685b91b2e0d617acb928dfe0_2_386x500.jpeg, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/cefc83e22d7923b9685b91b2e0d617acb928dfe0_2_579x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/cefc83e22d7923b9685b91b2e0d617acb928dfe0_2_772x1000.jpeg 2x\" data-dominant-color=\"F5F5F5\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">329180134ee2fb6417bf6b250e5ddb4a-1<\/span><span class=\"informations\">1700\u00d72200 341 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nThanks in advance<\/p>. <p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/960fac6ee53992d911bcd56315271a6fb8d6487c.jpeg\" data-download-href=\"\/uploads\/short-url\/lpvcsL5JQ0hAzKNo2TdyO3Utjek.jpeg?dl=1\" title=\"329180134ee2fb6417bf6b250e5ddb4a-2\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/960fac6ee53992d911bcd56315271a6fb8d6487c_2_386x500.jpeg\" alt=\"329180134ee2fb6417bf6b250e5ddb4a-2\" data-base62-sha1=\"lpvcsL5JQ0hAzKNo2TdyO3Utjek\" width=\"386\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/960fac6ee53992d911bcd56315271a6fb8d6487c_2_386x500.jpeg, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/960fac6ee53992d911bcd56315271a6fb8d6487c_2_579x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/960fac6ee53992d911bcd56315271a6fb8d6487c_2_772x1000.jpeg 2x\" data-dominant-color=\"F5F5F5\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">329180134ee2fb6417bf6b250e5ddb4a-2<\/span><span class=\"informations\">1700\u00d72200 340 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Another Pic for permission encountered error<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/i-have-added-my-google-drive-as-a-remote-storage\/1061",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-14T13:08:21.763Z",
                "Answer_body":"<p>Hi, could you please share the output of <code>dvc push -v<\/code> and <code>dvc doctor<\/code>? Thanks.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-15T01:37:57.987Z",
                "Answer_body":"<p>It is showing a permission error but I have already given the permission<\/p>\n<p>Traceback (most recent call last):<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/fs\/spec.py\u201d, line 121, in _gdrive_shared_drive_id<br>\nitem.FetchMetadata(\u201cdriveId\u201d)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/auth.py\u201d, line 84, in _decorated<br>\nreturn decoratee(self, *args, **kwargs)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/files.py\u201d, line 448, in FetchMetadata<br>\nraise ApiRequestError(error)<br>\npydrive2.files.ApiRequestError: &lt;HttpError 404 when requesting <a href=\"https:\/\/www.googleapis.com\/drive\/v2\/files\/16uQro82U1-Ox3ABLZ0Oa7uNUJr0T9BD-?fields=driveId&amp;supportsAllDrives=true&amp;alt=json\" rel=\"noopener nofollow ugc\">https:\/\/www.googleapis.com\/drive\/v2\/files\/16uQro82U1-Ox3ABLZ0Oa7uNUJr0T9BD-?fields=driveId&amp;supportsAllDrives=true&amp;alt=json<\/a> returned \u201cFile not found: 16uQro82U1-Ox3ABLZ0Oa7uNUJr0T9BD-\u201d. Details: \u201c[{\u2018domain\u2019: \u2018global\u2019, \u2018reason\u2019: \u2018notFound\u2019, \u2018message\u2019: \u2018File not found: 16uQro82U1-Ox3ABLZ0Oa7uNUJr0T9BD-\u2019, \u2018locationType\u2019: \u2018other\u2019, \u2018location\u2019: \u2018file\u2019}]\u201d&gt;<\/p>\n<p>The above exception was the direct cause of the following exception:<\/p>\n<p>Traceback (most recent call last):<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/main.py\u201d, line 55, in main<br>\nret = cmd.do_run()<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/command\/base.py\u201d, line 45, in do_run<br>\nreturn self.run()<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/command\/data_sync.py\u201d, line 57, in run<br>\nprocessed_files_count = self.repo.push(<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/repo\/<strong>init<\/strong>.py\u201d, line 49, in wrapper<br>\nreturn f(repo, *args, **kwargs)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/repo\/push.py\u201d, line 56, in push<br>\npushed += self.cloud.push(<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/data_cloud.py\u201d, line 85, in push<br>\nreturn transfer(<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/objects\/transfer.py\u201d, line 153, in transfer<br>\nstatus = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/objects\/status.py\u201d, line 158, in compare_status<br>\ndest_exists, dest_missing = status(<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/objects\/status.py\u201d, line 131, in status<br>\nexists.update(odb.hashes_exist(hashes, name=odb.fs_path, **kwargs))<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/objects\/db\/base.py\u201d, line 470, in hashes_exist<br>\nremote_hashes = self.list_hashes_exists(hashes, jobs, name)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/objects\/db\/base.py\u201d, line 421, in list_hashes_exists<br>\nret = list(itertools.compress(hashes, in_remote))<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/concurrent\/futures\/_base.py\u201d, line 608, in result_iterator<br>\nyield fs.pop().result()<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/concurrent\/futures\/_base.py\u201d, line 445, in result<br>\nreturn self.__get_result()<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/concurrent\/futures\/_base.py\u201d, line 390, in __get_result<br>\nraise self._exception<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/concurrent\/futures\/thread.py\u201d, line 52, in run<br>\nresult = self.fn(*self.args, **self.kwargs)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/objects\/db\/base.py\u201d, line 412, in exists_with_progress<br>\nret = self.fs.exists(fs_path)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/dvc\/fs\/fsspec_wrapper.py\u201d, line 91, in exists<br>\nreturn self.fs.exists(path)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/fs\/spec.py\u201d, line 225, in exists<br>\nself._get_item_id(path)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/fs\/spec.py\u201d, line 192, in _get_item_id<br>\nitem_ids = self._path_to_item_ids(base, create, use_cache)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/fs\/spec.py\u201d, line 174, in _path_to_item_ids<br>\nitem_ids = self._get_cached_item_ids(path, use_cache)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/fs\/spec.py\u201d, line 170, in _get_cached_item_ids<br>\nreturn self._ids_cache[\u201cdirs\u201d].get(path, [])<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/funcy\/objects.py\u201d, line 50, in <strong>get<\/strong><br>\nreturn prop.<strong>get<\/strong>(instance, type)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/funcy\/objects.py\u201d, line 28, in <strong>get<\/strong><br>\nres = instance.<strong>dict<\/strong>[self.fget.<strong>name<\/strong>] = self.fget(instance)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/fs\/spec.py\u201d, line 82, in _ids_cache<br>\nfor item in self._gdrive_list(<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/fs\/spec.py\u201d, line 132, in _gdrive_list<br>\nparam.update(self._list_params)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/funcy\/objects.py\u201d, line 28, in <strong>get<\/strong><br>\nres = instance.<strong>dict<\/strong>[self.fget.<strong>name<\/strong>] = self.fget(instance)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/fs\/spec.py\u201d, line 100, in _list_params<br>\ndrive_id = self._gdrive_shared_drive_id(self.root)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/funcy\/decorators.py\u201d, line 45, in wrapper<br>\nreturn deco(call, *dargs, **dkwargs)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/funcy\/flow.py\u201d, line 127, in retry<br>\nreturn call()<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/funcy\/decorators.py\u201d, line 66, in <strong>call<\/strong><br>\nreturn self._func(*self._args, **self._kwargs)<br>\nFile \u201c\/opt\/miniconda3\/lib\/python3.9\/site-packages\/pydrive2\/fs\/spec.py\u201d, line 125, in _gdrive_shared_drive_id<br>\nraise PermissionError from exc<br>\nPermissionError<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-15T08:29:14.143Z",
                "Answer_body":"<p>How did you setup gdrive? Did you follow this <a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote\">setup guide<\/a>?<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-01T13:26:47.604Z",
                "Answer_body":"<p>Hi, <a class=\"mention\" href=\"\/u\/skshetry\">@skshetry<\/a> not sure if this was solved but I encountered the same issue. I followed along with the remote storage setup. However, one difference is that I did not use dvc add because I had written the dependencies and outputs directly in the dvc.yaml file.  Secondly, I only allowed dvc access \/ permission to see, create, and delete its own configuration data. If I need to change the dvc permissons, how do I do that after the initial setup?<\/p>\n<p>\u2013solved\u2013<br>\nSo I guess dvc needs access to everything. The authorization section was helpful, <a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote?_ga=2.39431651.1183132044.1651410796-478586346.1651410796#authorization\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote?_ga=2.39431651.1183132044.1651410796-478586346.1651410796#authorization<\/a><br>\njust delete the user credentials in the .dvc\/tmp file and try again.<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-17T15:58:33.676Z",
                "Answer_body":"<p>Hello if anyone is willing to help I have tried the above steps and to my surprise it worked on my college id when I tried to push but on my personal gmail its not working.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/cefc83e22d7923b9685b91b2e0d617acb928dfe0.jpeg\" data-download-href=\"\/uploads\/short-url\/tx5mWVDv30aQc6VHkGRpMi28E4E.jpeg?dl=1\" title=\"329180134ee2fb6417bf6b250e5ddb4a-1\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/cefc83e22d7923b9685b91b2e0d617acb928dfe0_2_386x500.jpeg\" alt=\"329180134ee2fb6417bf6b250e5ddb4a-1\" data-base62-sha1=\"tx5mWVDv30aQc6VHkGRpMi28E4E\" width=\"386\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/cefc83e22d7923b9685b91b2e0d617acb928dfe0_2_386x500.jpeg, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/cefc83e22d7923b9685b91b2e0d617acb928dfe0_2_579x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/cefc83e22d7923b9685b91b2e0d617acb928dfe0_2_772x1000.jpeg 2x\" data-dominant-color=\"F5F5F5\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">329180134ee2fb6417bf6b250e5ddb4a-1<\/span><span class=\"informations\">1700\u00d72200 341 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nThanks in advance<\/p>",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-17T15:59:22.019Z",
                "Answer_body":"<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/960fac6ee53992d911bcd56315271a6fb8d6487c.jpeg\" data-download-href=\"\/uploads\/short-url\/lpvcsL5JQ0hAzKNo2TdyO3Utjek.jpeg?dl=1\" title=\"329180134ee2fb6417bf6b250e5ddb4a-2\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/960fac6ee53992d911bcd56315271a6fb8d6487c_2_386x500.jpeg\" alt=\"329180134ee2fb6417bf6b250e5ddb4a-2\" data-base62-sha1=\"lpvcsL5JQ0hAzKNo2TdyO3Utjek\" width=\"386\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/960fac6ee53992d911bcd56315271a6fb8d6487c_2_386x500.jpeg, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/960fac6ee53992d911bcd56315271a6fb8d6487c_2_579x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/960fac6ee53992d911bcd56315271a6fb8d6487c_2_772x1000.jpeg 2x\" data-dominant-color=\"F5F5F5\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">329180134ee2fb6417bf6b250e5ddb4a-2<\/span><span class=\"informations\">1700\u00d72200 340 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Another Pic for permission encountered error<\/p>",
                "Answer_has_accepted":false
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"ad googl drive remot storag push error idea resolv follow get start session familiar product error unexpect error",
        "Question_preprocessed_content":"ad googl drive remot storag push error idea resolv follow get start session familiar product error unexpect error",
        "Question_gpt_summary_original":"The user is encountering an error when trying to do a dvc push after adding their Google Drive as a remote storage. The error message indicates that the file cannot be found, and the user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount error try push ad googl drive remot storag error messag indic file user seek help resolv issu",
        "Answer_original_content":"share output push doctor thank show permiss error given permiss traceback recent file opt miniconda lib python site packag pydriv spec line gdrive share drive item fetchmetadata driveid file opt miniconda lib python site packag pydriv auth line decor return decorate self arg kwarg file opt miniconda lib python site packag pydriv file line fetchmetadata rais apirequesterror error pydriv file apirequesterror except direct caus follow except traceback recent file opt miniconda lib python site packag main line main ret cmd run file opt miniconda lib python site packag command base line run return self run file opt miniconda lib python site packag command data sync line run process file count self repo push file opt miniconda lib python site packag repo init line wrapper return repo arg kwarg file opt miniconda lib python site packag repo push line push push self cloud push file opt miniconda lib python site packag data cloud line push return transfer file opt miniconda lib python site packag object transfer line transfer statu compar statu src dest obj id check delet fals kwarg file opt miniconda lib python site packag object statu line compar statu dest exist dest miss statu file opt miniconda lib python site packag object statu line statu exist updat odb hash exist hash odb path kwarg file opt miniconda lib python site packag object base line hash exist remot hash self list hash exist hash job file opt miniconda lib python site packag object base line list hash exist ret list itertool compress hash remot file opt miniconda lib python concurr futur base line result iter yield pop result file opt miniconda lib python concurr futur base line result return self result file opt miniconda lib python concurr futur base line result rais self except file opt miniconda lib python concurr futur thread line run result self self arg self kwarg file opt miniconda lib python site packag object base line exist progress ret self exist path file opt miniconda lib python site packag fsspec wrapper line exist return self exist path file opt miniconda lib python site packag pydriv spec line exist self item path file opt miniconda lib python site packag pydriv spec line item item id self path item id base creat us cach file opt miniconda lib python site packag pydriv spec line path item id item id self cach item id path us cach file opt miniconda lib python site packag pydriv spec line cach item id return self id cach dir path file opt miniconda lib python site packag funci object line return prop instanc type file opt miniconda lib python site packag funci object line re instanc dict self fget self fget instanc file opt miniconda lib python site packag pydriv spec line id cach item self gdrive list file opt miniconda lib python site packag pydriv spec line gdrive list param updat self list param file opt miniconda lib python site packag funci object line re instanc dict self fget self fget instanc file opt miniconda lib python site packag pydriv spec line list param drive self gdrive share drive self root file opt miniconda lib python site packag funci decor line wrapper return deco darg dkwarg file opt miniconda lib python site packag funci flow line retri return file opt miniconda lib python site packag funci decor line return self func self arg self kwarg file opt miniconda lib python site packag pydriv spec line gdrive share drive rais permissionerror exc permissionerror setup gdrive follow setup guid skshetri sure solv encount issu follow remot storag setup differ us add written depend output directli yaml file secondli allow access permiss creat delet configur data need chang permisson initi setup solv guess need access author section help http org doc user guid setup googl drive remot author delet user credenti tmp file try hello will help tri step surpris work colleg tri push person gmail work eefbbfbeddba thank advanc eefbbfbeddba pic permiss encount error",
        "Answer_preprocessed_content":"share output show permiss error given permiss traceback file line file line return decorate file line fetchmetadata rais apirequesterror except direct caus follow except traceback file line main ret file line return file line run file line wrapper return file line push push file line push return transfer file line statu file line job file line ret file line yield file line result return file line rais file line run result file line ret file line exist return file line exist file line creat file line file line return file line return type file line re file line item file line file line re file line file line wrapper return deco file line retri return file line return file line rais permissionerror exc permissionerror setup gdrive follow setup guid sure solv encount issu follow remot storag setup differ us add written depend output directli yaml file secondli allow access permiss creat delet configur data need chang permisson initi setup solv guess need access author section help delet user credenti file try hello will help tri step surpris work colleg tri push person gmail work thank advanc pic permiss encount error",
        "Answer_gpt_summary_original":"Solutions provided:\n- One user suggested checking the output of `dvc push -v` and `dvc doctor`.\n- Another user suggested that dvc needs access to everything and shared a link to the authorization section of the setup guide.\n- A user mentioned that they encountered the same issue and solved it by allowing dvc access to see, create, and delete its own configuration data. They also suggested deleting the user credentials in the .dvc\/tmp file and trying again.\n- No solution was provided for the user who encountered the permission error on their personal Gmail account.",
        "Answer_gpt_summary":"solut provid user suggest check output push doctor user suggest need access share link author section setup guid user mention encount issu solv allow access creat delet configur data suggest delet user credenti tmp file try solut provid user encount permiss error person gmail account"
    },
    {
        "Question_title":"How to enable authentication for an ACI webservice in Azure Machine Learning service?",
        "Question_body":"<p>I am able to deploy a Azure Machine learning prediction service in my workspace <code>ws<\/code> using the syntax<\/p>\n\n<pre><code>aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                                               memory_gb=8, \n                                               tags={\"method\" : \"some method\"}, \n                                               description='Predict something')\n<\/code><\/pre>\n\n<p>and then<\/p>\n\n<pre><code>service = Webservice.deploy_from_image(deployment_config = aciconfig,\n                                       image = image,\n                                       name = service_name,\n                                       workspace = ws)\n<\/code><\/pre>\n\n<p>as described in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#aci\" rel=\"nofollow noreferrer\">documentation<\/a>.<br>\nHowever, this exposes a service publicly and this is not really optimal.<\/p>\n\n<p>What's the easiest way to shield the ACI service? I understand that passing an <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.aciwebservice?view=azure-ml-py#deploy-configuration-cpu-cores-none--memory-gb-none--tags-none--properties-none--description-none--location-none--auth-enabled-none--ssl-enabled-none--enable-app-insights-none--ssl-cert-pem-file-none--ssl-key-pem-file-none--ssl-cname-none-\" rel=\"nofollow noreferrer\"><code>auth_enabled=True<\/code><\/a> parameter may do the job, but then how can I instruct a client (say, using <code>curl<\/code> or Postman) to use the service afterwards? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1556135731340,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1556186547292,
        "Question_score":0.0,
        "Question_view_count":676.0,
        "Answer_body":"<p>See <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service#call-the-service-c\" rel=\"nofollow noreferrer\">here<\/a> for an example (in C#). When you enable auth, you will need to send the API key in the \"Authorization\" header in the HTTP request:<\/p>\n\n<pre><code>client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", authKey);\n<\/code><\/pre>\n\n<p>See <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service#authentication-key\" rel=\"nofollow noreferrer\">here<\/a> how to retrieve the key.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":1556183204092,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55837639",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1556182170523,
        "Question_original_content":"enabl authent aci webservic servic abl deploi predict servic workspac syntax aciconfig aciwebservic deploi configur cpu core memori tag method method descript predict servic webservic deploi imag deploy config aciconfig imag imag servic workspac describ document expos servic publicli optim easiest wai shield aci servic understand pass auth enabl true paramet job instruct client curl postman us servic",
        "Question_preprocessed_content":"enabl authent aci webservic servic abl deploi predict servic workspac syntax describ document expos servic publicli optim easiest wai shield aci servic understand pass paramet job instruct client us servic",
        "Question_gpt_summary_original":"The user has successfully deployed an Azure Machine Learning prediction service in their workspace, but they are concerned about the service being exposed publicly. They are looking for a way to shield the ACI service and have considered using the \"auth_enabled=True\" parameter, but they are unsure how to instruct a client to use the service afterwards.",
        "Question_gpt_summary":"user successfulli deploi predict servic workspac concern servic expos publicli look wai shield aci servic consid auth enabl true paramet unsur instruct client us servic",
        "Answer_original_content":"exampl enabl auth need send api kei author header http request client defaultrequesthead author new authenticationheadervalu bearer authkei retriev kei",
        "Answer_preprocessed_content":"exampl enabl auth need send api kei author header http request retriev kei",
        "Answer_gpt_summary_original":"Solution: The user can shield the ACI service by using the \"auth_enabled=True\" parameter and sending the API key in the \"Authorization\" header in the HTTP request. The user can retrieve the key by following the instructions provided in the documentation. An example in C# is also provided in the discussion.",
        "Answer_gpt_summary":"solut user shield aci servic auth enabl true paramet send api kei author header http request user retriev kei follow instruct provid document exampl provid discuss"
    },
    {
        "Question_title":"windows compatibility",
        "Question_body":"Hello,\n\n\nI am not sure if this is the right place to ask this:\nI have seen the PR of the mlflow which replaces Gunicorn by Waitress has been approved and merged in master, but when will this versioned released on Pypi?\n\n\nThanks a lot!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1562794385000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":8.0,
        "Answer_body":"It will be in the next release (1.1), which should hopefully come in the next 1-2 weeks. You can also build an installable version of MLflow from the Git version as described here if you want to use that:\u00a0https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst#building-a-distributable-artifact.\n\n\nMatei\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/c27fe5b9-ba0b-4843-985d-8edce9acdf1e%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/mNJwqIkvMR8",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-07-14T13:39:37",
                "Answer_body":"It will be in the next release (1.1), which should hopefully come in the next 1-2 weeks. You can also build an installable version of MLflow from the Git version as described here if you want to use that:\u00a0https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst#building-a-distributable-artifact.\n\n\nMatei\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/c27fe5b9-ba0b-4843-985d-8edce9acdf1e%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            }
        ],
        "Question_closed_time":null,
        "Question_original_content":"window compat hello sure right place ask seen replac gunicorn waitress approv merg master version releas pypi thank lot",
        "Question_preprocessed_content":"window compat hello sure right place ask seen replac gunicorn waitress approv merg master version releas pypi thank lot",
        "Question_gpt_summary_original":"The user is inquiring about the release date of a new version of mlflow on Pypi that replaces Gunicorn with Waitress.",
        "Question_gpt_summary":"user inquir releas date new version pypi replac gunicorn waitress",
        "Answer_original_content":"releas hopefulli come week build instal version git version describ want us http github com blob master contribut rst build distribut artifact matei receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user cfeb bab edceacdf googlegroup com option visit http group googl com optout",
        "Answer_preprocessed_content":"releas hopefulli come week build instal version git version describ want us matei receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit",
        "Answer_gpt_summary_original":"Solution: The new version of mlflow on Pypi that replaces Gunicorn with Waitress will be released in the next release (1.1), which is expected to come in the next 1-2 weeks. Alternatively, the user can build an installable version of MLflow from the Git version as described in the link provided.",
        "Answer_gpt_summary":"solut new version pypi replac gunicorn waitress releas releas expect come week altern user build instal version git version describ link provid"
    },
    {
        "Question_title":"Please validate: SageMaker Endpoint URL Authentication\/Authorization",
        "Question_body":"Need validation:\n\nOnce the SageMaker endpoint is deployed. It can be invoked with the Sagemaker Runtime API InvokeEndpoint OR it can be invoked using the endpoint URL+HTTP AZ headers (below).\n\nSuccessful deployment also exposes a URL (on the console) that has the format:\n\nhttps:\/\/runtime.sagemaker.us-east-1.amazonaws.com\/endpoints\/ENDPOINT-NAME\/invocations\n\nWhat is the purpose of this URL (shown on console)?\n\nIn my understanding this URL Cannot be invoked w\/o appropriate headers as then there will be a need to have globally unique endpoint name!! THAT IS to invoke this URL it needs to have the \"HTTP Authorization headers\" (refer: https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/API\/sig-v4-authenticating-requests.html)\n\nI have a customer who is concerned that anyone can invoke the URL even from the internet. Tried to do it and received the <MissingTokenException> so I know it can't be done but just want to ensure I have the right explanation. (Test with HTTP\/AZ headers pending)",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1602169065000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":282.0,
        "Answer_body":"Your understanding is correct. From the docs:\n\nAmazon SageMaker strips all POST headers except those supported by the API. Amazon SageMaker might add additional headers. You should not rely on the behavior of headers outside those enumerated in the request syntax.\n\nCalls to InvokeEndpoint are authenticated by using AWS Signature Version 4.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFlHNZ7JxTFGIkPHQ75u44w\/please-validate-sage-maker-endpoint-url-authentication-authorization",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-08T15:38:33.000Z",
                "Answer_score":0,
                "Answer_body":"Your understanding is correct. From the docs:\n\nAmazon SageMaker strips all POST headers except those supported by the API. Amazon SageMaker might add additional headers. You should not rely on the behavior of headers outside those enumerated in the request syntax.\n\nCalls to InvokeEndpoint are authenticated by using AWS Signature Version 4.",
                "Answer_has_accepted":true
            }
        ],
        "Question_closed_time":1602171513000,
        "Question_original_content":"valid endpoint url authent author need valid endpoint deploi invok runtim api invokeendpoint invok endpoint url http header success deploy expos url consol format http runtim east amazonaw com endpoint endpoint invoc purpos url shown consol understand url invok appropri header need global uniqu endpoint invok url need http author header refer http doc aw amazon com amazon latest api sig authent request html custom concern invok url internet tri receiv know want ensur right explan test http header pend",
        "Question_preprocessed_content":"valid endpoint url need valid endpoint deploi invok runtim api invokeendpoint invok endpoint url http header success deploy expos url format purpos url understand url invok appropri header need global uniqu endpoint invok url need http author header custom concern invok url internet tri receiv know want ensur right explan",
        "Question_gpt_summary_original":"The user is seeking validation regarding the purpose of the URL exposed after successful deployment of SageMaker endpoint. They believe that the URL cannot be invoked without appropriate headers and a globally unique endpoint name. However, a customer is concerned that anyone can invoke the URL from the internet, but the user has tested it and received a <MissingTokenException>, indicating that it cannot be done. The user plans to test it with HTTP\/AZ headers to confirm their explanation.",
        "Question_gpt_summary":"user seek valid purpos url expos success deploy endpoint believ url invok appropri header global uniqu endpoint custom concern invok url internet user test receiv indic user plan test http header confirm explan",
        "Answer_original_content":"understand correct doc strip post header support api add addit header reli behavior header outsid enumer request syntax call invokeendpoint authent aw signatur version",
        "Answer_preprocessed_content":"understand correct doc strip post header support api add addit header reli behavior header outsid enumer request syntax call invokeendpoint authent aw signatur version",
        "Answer_gpt_summary_original":"Solution: The user's understanding is correct. The URL exposed after successful deployment of SageMaker endpoint cannot be invoked without appropriate headers and a globally unique endpoint name. Calls to InvokeEndpoint are authenticated by using AWS Signature Version 4.",
        "Answer_gpt_summary":"solut user understand correct url expos success deploy endpoint invok appropri header global uniqu endpoint call invokeendpoint authent aw signatur version"
    },
    {
        "Question_title":"`Model.deploy()` failing for AutoML with `400 'automatic_resources' is not supported for Model`",
        "Question_body":"<p>Trying to use the python SDK to deploy an AutoML model, but am recieving the error<\/p>\n<pre><code>google.api_core.exceptions.InvalidArgument: 400 'automatic_resources' is not supported for Model\n<\/code><\/pre>\n<p>Here's what I'm running:<\/p>\n<pre><code>from google.cloud import aiplatform\n\naiplatform.init(project=&quot;MY_PROJECT&quot;)\n\n# model is an AutoML tabular model\nmodel = aiplatform.Model(&quot;MY_MODEL_ID&quot;)\n\n# this fails\nmodel.deploy()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643912020147,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":149.0,
        "Answer_body":"<p>You encounter the <code>automatic_resources<\/code> because deploying models for AutoML Tables requires the user to define the machine type at model deployment. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#specify\" rel=\"nofollow noreferrer\">configure compute resources for prediction docs<\/a> for more information.<\/p>\n<blockquote>\n<p>If you want to use a custom-trained model or an AutoML tabular model\nto serve online predictions, you must specify a machine type when you\ndeploy the Model resource as a DeployedModel to an Endpoint.<\/p>\n<\/blockquote>\n<p>You should add defining of resources to your code for you to continue the deployment. Adding <code>machine_type<\/code> parameter should suffice. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#machine-types\" rel=\"nofollow noreferrer\">supported machine types docs<\/a> for complete list.<\/p>\n<p>See code below:<\/p>\n<pre><code>from google.cloud import aiplatform\n\nproject = &quot;your-project-id&quot;\nlocation = &quot;us-central1&quot;\nmodel_id = &quot;99999999&quot;\n\naiplatform.init(project=project, location=location)\nmodel_name = (f&quot;projects\/{project}\/locations\/{location}\/models\/{model_id}&quot;)\n\nmodel = aiplatform.Model(model_name=model_name)\n\nmodel.deploy(machine_type=&quot;n1-standard-4&quot;)\n\nmodel.wait()\nprint(model.display_name)\n<\/code><\/pre>\n<p>Output:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/NHpJF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NHpJF.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70976273",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1643942120972,
        "Question_original_content":"model deploi fail automl automat resourc support model try us python sdk deploi automl model reciev error googl api core except invalidargu automat resourc support model run googl cloud import aiplatform aiplatform init project project model automl tabular model model aiplatform model model fail model deploi",
        "Question_preprocessed_content":"fail automl try us python sdk deploi automl model reciev error run",
        "Question_gpt_summary_original":"The user is encountering an error while trying to deploy an AutoML model using the python SDK. The error message states that \"automatic_resources\" is not supported for the model, resulting in a 400 InvalidArgument error. The user has provided the code they are running, which includes initializing the project and model, but the deployment fails.",
        "Question_gpt_summary":"user encount error try deploi automl model python sdk error messag state automat resourc support model result invalidargu error user provid code run includ initi project model deploy fail",
        "Answer_original_content":"encount automat resourc deploi model automl tabl requir user defin machin type model deploy configur comput resourc predict doc inform want us custom train model automl tabular model serv onlin predict specifi machin type deploi model resourc deployedmodel endpoint add defin resourc code continu deploy ad machin type paramet suffic support machin type doc complet list code googl cloud import aiplatform project project locat central model aiplatform init project project locat locat model project project locat locat model model model aiplatform model model model model deploi machin type standard model wait print model displai output",
        "Answer_preprocessed_content":"encount deploi model automl tabl requir user defin machin type model deploy configur comput resourc predict doc inform want us model automl tabular model serv onlin predict specifi machin type deploi model resourc deployedmodel endpoint add defin resourc code continu deploy ad paramet suffic support machin type doc complet list code output",
        "Answer_gpt_summary_original":"Solution: The error message encountered by the user while deploying an AutoML model using the python SDK is due to the absence of defining machine type at model deployment. The solution is to add the `machine_type` parameter to the code and specify a supported machine type. The user can refer to the supported machine types documentation for a complete list. The code snippet provided in the discussion includes the necessary changes to the code.",
        "Answer_gpt_summary":"solut error messag encount user deploi automl model python sdk absenc defin machin type model deploy solut add machin type paramet code specifi support machin type user refer support machin type document complet list code snippet provid discuss includ necessari chang code"
    },
    {
        "Question_title":"AWS CLI: How to use variable to filter EFS",
        "Question_body":"<p>I want to use the value of the <code>DOMAIN_ID<\/code> variable to filter the EFS to get a FileSystemId. I used the commands below. The first command works and it stores the domain ID. The second one returns an empty list, even though the <code>DOMAIN_ID<\/code> variable is present.<\/p>\n<pre><code>DOMAIN_ID=$(aws sagemaker list-domains --query 'Domains[0].DomainId')\naws efs describe-file-systems --query 'FileSystems[?CreationToken==`$DOMAIN_ID`].FileSystemId'\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>[]\n<\/code><\/pre>\n<p>Expected output:<\/p>\n<pre><code>&lt;Some EFS identifier&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658155362930,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1658167919760,
        "Question_score":0.0,
        "Question_view_count":64.0,
        "Answer_body":"<p>This works (escaping backticks) -<\/p>\n<pre><code>aws efs describe-file-systems --query &quot;FileSystems[?CreationToken==\\`$DOMAIN_ID\\`].FileSystemId&quot;\n<\/code><\/pre>\n<p>You can also use describe-domain command instead -<\/p>\n<pre><code>$ DOMAIN_ID=$(aws sagemaker list-domains --query 'Domains[0].DomainId' | tr -d '&quot;')\n$ aws sagemaker describe-domain --domain-id $DOMAIN_ID --query 'HomeEfsFileSystemId'\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73024189",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1658177973060,
        "Question_original_content":"aw cli us variabl filter ef want us valu domain variabl filter ef filesystemid command command work store domain second return list domain variabl present domain list domain queri domain domainid aw ef file system queri filesystem creationtoken domain filesystemid output expect output",
        "Question_preprocessed_content":"aw cli us variabl filter ef want us valu variabl filter ef filesystemid command command work store domain second return list variabl present output expect output",
        "Question_gpt_summary_original":"The user is facing a challenge in using a variable to filter EFS and get a FileSystemId. The first command works and stores the domain ID, but the second command returns an empty list even though the DOMAIN_ID variable is present. The expected output is a specific EFS identifier.",
        "Question_gpt_summary":"user face challeng variabl filter ef filesystemid command work store domain second command return list domain variabl present expect output specif ef identifi",
        "Answer_original_content":"work escap backtick aw ef file system queri filesystem creationtoken domain filesystemid us domain command instead domain list domain queri domain domainid domain domain domain queri homeefsfilesystemid",
        "Answer_preprocessed_content":"work us command instead",
        "Answer_gpt_summary_original":"Two solutions were provided in the discussion. The first solution involves using the `aws efs describe-file-systems` command with a query to filter the EFS and get the FileSystemId. The second solution involves using the `aws sagemaker describe-domain` command with the `--domain-id` option to get the HomeEfsFileSystemId.",
        "Answer_gpt_summary":"solut provid discuss solut involv aw ef file system command queri filter ef filesystemid second solut involv domain command domain option homeefsfilesystemid"
    },
    {
        "Question_title":"How to invoke Sagemaker XGBoost endpoint post model creation?",
        "Question_body":"<p>I have been following along with this really helpful XGBoost tutorial on Medium (code used towards bottom of article): <a href=\"https:\/\/medium.com\/analytics-vidhya\/random-forest-and-xgboost-on-amazon-sagemaker-and-aws-lambda-29abd9467795\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/analytics-vidhya\/random-forest-and-xgboost-on-amazon-sagemaker-and-aws-lambda-29abd9467795<\/a>.<\/p>\n<p>To-date, I've been able to get data appropriately formatted for ML purposes, a model created based on training data, and then test data fed through the model to give useful results.<\/p>\n<p>Whenever I leave and come back to work more on the model or feed in new test data however, I find I need to re-run all model creation steps in order to make any further predictions. Instead I would like to just call my already created model endpoint based on the Image_URI and feed in new data.<\/p>\n<p>Current steps performed:<\/p>\n<p>Model Training<\/p>\n<pre><code>xgb = sagemaker.estimator.Estimator(containers[my_region],\n                                    role, \n                                    train_instance_count=1, \n                                    train_instance_type='ml.m4.xlarge',\n                                    output_path='s3:\/\/{}\/{}\/output'.format(bucket_name, prefix),\n                                    sagemaker_session=sess)\nxgb.set_hyperparameters(eta=0.06,\n                        alpha=0.8,\n                        lambda_bias=0.8,\n                        gamma=50,\n                        min_child_weight=6,\n                        subsample=0.5,\n                        silent=0,\n                        early_stopping_rounds=5,\n                        objective='reg:linear',\n                        num_round=1000)\n\nxgb.fit({'train': s3_input_train})\n\nxgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n<p>Evaluation<\/p>\n<pre><code>test_data_array = test_data.drop([ 'price','id','sqft_above','date'], axis=1).values #load the data into an array\n\nxgb_predictor.serializer = csv_serializer # set the serializer type\n\npredictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\npredictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array\nprint(predictions_array.shape)\n\nfrom sklearn.metrics import r2_score\nprint(&quot;R2 score : %.2f&quot; % r2_score(test_data['price'],predictions_array))\n<\/code><\/pre>\n<p>It seems that this particular line:<\/p>\n<pre><code>predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n<\/code><\/pre>\n<p>needs to be re-written in order to not reference xgb.predictor but instead reference the model location.<\/p>\n<p>I have tried the following<\/p>\n<pre><code>trained_model = sagemaker.model.Model(\n    model_data='s3:\/\/{}\/{}\/output\/xgboost-2020-11-10-00-00\/output\/model.tar.gz'.format(bucket_name, prefix),\n    image_uri='XXXXXXXXXX.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest',\n    role=role)  # your role here; could be different name\n\ntrained_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n<p>and then replaced<\/p>\n<pre><code>xgb_predictor.serializer = csv_serializer # set the serializer type\npredictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n<\/code><\/pre>\n<p>with<\/p>\n<pre><code>trained_model.serializer = csv_serializer # set the serializer type\npredictions = trained_model.predict(test_data_array).decode('utf-8') # predict!\n<\/code><\/pre>\n<p>but I get the following error:<\/p>\n<pre><code>AttributeError: 'Model' object has no attribute 'predict'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1605059871707,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1242.0,
        "Answer_body":"<p>that's a good question :) I agree, many of the official tutorials tend to show the full train-to-invoke pipeline and don't emphasize enough that each step can be done separately. In your specific case, when you want to invoke an already-deployed endpoint, you can either: (A) use the invoke API call in one of the numerous SDKs (example in <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker-runtime\/invoke-endpoint.html\" rel=\"nofollow noreferrer\">CLI<\/a>, <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker-runtime.html#SageMakerRuntime.Client.invoke_endpoint\" rel=\"nofollow noreferrer\">boto3<\/a>) or (B) or instantiate a <code>predictor<\/code> with the high-level Python SDK, either the generic <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"nofollow noreferrer\"><code>sagemaker.model.Model<\/code><\/a> class or its XGBoost-specific child: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/xgboost\/xgboost.html#sagemaker.xgboost.model.XGBoostPredictor\" rel=\"nofollow noreferrer\"><code>sagemaker.xgboost.model.XGBoostPredictor<\/code><\/a> as illustrated below:<\/p>\n<pre><code>from sagemaker.xgboost.model import XGBoostPredictor\n    \npredictor = XGBoostPredictor(endpoint_name='your-endpoint')\npredictor.predict('&lt;payload&gt;')\n<\/code><\/pre>\n<p>similar question <a href=\"https:\/\/stackoverflow.com\/questions\/56255154\/how-to-use-a-pretrained-model-from-s3-to-predict-some-data\/56277411#56277411\">How to use a pretrained model from s3 to predict some data?<\/a><\/p>\n<p>Note:<\/p>\n<ul>\n<li>If you want the <code>model.deploy()<\/code> call to return a predictor, your model must be instantiated with a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"nofollow noreferrer\"><code>predictor_cls<\/code><\/a>. This is optional, you can also first deploy a model, and then invoke it as a separate step with the above technique<\/li>\n<li>Endpoints create charges even if you don't invoke them; they are charged per uptime. So if you don't need an always-on endpoint, don't hesitate to shut it down to minimize costs.<\/li>\n<\/ul>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64779388",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_closed_time":1605108362620,
        "Question_original_content":"invok xgboost endpoint post model creation follow help xgboost tutori medium code articl http medium com analyt vidhya random forest xgboost amazon aw lambda abd date abl data appropri format purpos model creat base train data test data fed model us result leav come work model feed new test data need run model creation step order predict instead like creat model endpoint base imag uri feed new data current step perform model train xgb estim estim contain region role train instanc count train instanc type xlarg output path output format bucket prefix session sess xgb set hyperparamet eta alpha lambda bia gamma min child weight subsampl silent earli stop round object reg linear num round xgb fit train input train xgb predictor xgb deploi initi instanc count instanc type xlarg evalu test data arrai test data drop price sqft date axi valu load data arrai xgb predictor serial csv serial set serial type predict xgb predictor predict test data arrai decod utf predict predict arrai fromstr predict sep turn predict arrai print predict arrai shape sklearn metric import score print score score test data price predict arrai particular line predict xgb predictor predict test data arrai decod utf predict need written order refer xgb predictor instead refer model locat tri follow train model model model model data output xgboost output model tar format bucket prefix imag uri dkr ecr east amazonaw com xgboost latest role role role differ train model deploi initi instanc count instanc type xlarg replac xgb predictor serial csv serial set serial type predict xgb predictor predict test data arrai decod utf predict train model serial csv serial set serial type predict train model predict test data arrai decod utf predict follow error attributeerror model object attribut predict",
        "Question_preprocessed_content":"invok xgboost endpoint post model creation follow help xgboost tutori medium abl data appropri format purpos model creat base train data test data fed model us result leav come work model feed new test data need model creation step order predict instead like creat model endpoint base feed new data current step perform model train evalu particular line need order refer instead refer model locat tri follow replac follow error",
        "Question_gpt_summary_original":"The user is facing a challenge in invoking Sagemaker XGBoost endpoint post model creation. They have been able to create a model based on training data and feed test data through the model to get useful results. However, whenever they leave and come back to work on the model or feed in new test data, they need to re-run all model creation steps to make any further predictions. The user wants to call their already created model endpoint based on the Image_URI and feed in new data, but they are facing an error when trying to reference the model location instead of xgb.predictor.",
        "Question_gpt_summary":"user face challeng invok xgboost endpoint post model creation abl creat model base train data feed test data model us result leav come work model feed new test data need run model creation step predict user want creat model endpoint base imag uri feed new data face error try refer model locat instead xgb predictor",
        "Answer_original_content":"good question agre offici tutori tend train invok pipelin emphas step separ specif case want invok deploi endpoint us invok api numer sdk exampl cli boto instanti predictor high level python sdk gener model model class xgboost specif child xgboost model xgboostpredictor illustr xgboost model import xgboostpredictor predictor xgboostpredictor endpoint endpoint predictor predict similar question us pretrain model predict data note want model deploi return predictor model instanti predictor cl option deploi model invok separ step techniqu endpoint creat charg invok charg uptim need endpoint hesit shut minim cost",
        "Answer_preprocessed_content":"good question agre offici tutori tend pipelin emphas step separ specif case want invok endpoint us invok api numer sdk instanti python sdk gener class child illustr similar question us pretrain model predict data note want return predictor model instanti option deploi model invok separ step techniqu endpoint creat charg invok charg uptim need endpoint hesit shut minim cost",
        "Answer_gpt_summary_original":"The discussion suggests two possible solutions for the challenge. The first solution is to use the invoke API call in one of the numerous SDKs or instantiate a predictor with the high-level Python SDK. The second solution is to use a pre-trained model from S3 to predict new data. The note mentions that the model must be instantiated with a predictor_cls if the model.deploy() call is to return a predictor. Additionally, the discussion advises shutting down endpoints when not in use to minimize costs.",
        "Answer_gpt_summary":"discuss suggest possibl solut challeng solut us invok api numer sdk instanti predictor high level python sdk second solut us pre train model predict new data note mention model instanti predictor cl model deploi return predictor addition discuss advis shut endpoint us minim cost"
    }
]